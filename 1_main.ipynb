{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(filename='error.log', level=logging.ERROR)\n",
    "from datetime import datetime\n",
    "# datetime.now().strftime(\"%d/%m/%y %H:%M:%S\")\n",
    "FILES = [\n",
    "    \"grandmaster_nl_pl_only_plot.json\",\n",
    "    \"master_nl_pl_only_plot.json\",\n",
    "    \"expert_nl_pl_only_plot.json\",\n",
    "]\n",
    "\n",
    "CLASS_NAME = {\n",
    "    \"grandmaster\": \"GrandMasterCode\",\n",
    "    \"master\": \"MasterCode\",\n",
    "    \"expert\": \"ExpertCode\",\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Weaviate instance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "{\"deprecations\": null, \"objects\": [{\"class\": \"GrandMasterCode\", \"creationTimeUnix\": 1680794367435, \"id\": \"002bd101-b16d-48c7-a401-44c673520d1a\", \"lastUpdateTimeUnix\": 1680794367435, \"properties\": {\"code\": \"# Reference: https://www.kaggle.com/code/tarunpaparaju/champs-competition-chemistry-background-and-eda\\n\\nplt.figure(figsize=(26, 24))\\nfor i, col in enumerate(typelist):\\n    plt.subplot(4,2, i + 1)\\n    sns.distplot(dipole_moments[train['type']==col]['X'],color = 'orange', kde=False)\\n    sns.distplot(dipole_moments[train['type']==col]['Y'],color = 'red', kde=False)\\n    sns.distplot(dipole_moments[train['type']==col]['Z'],color = 'blue', kde=False)\\n    plt.title(col)\"}, \"vectorWeights\": null}, {\"class\": \"GrandMasterCode\", \"creationTimeUnix\": 1680792753015, \"id\": \"00300e91-5c0a-4237-9312-85a5c14ea286\", \"lastUpdateTimeUnix\": 1680792753015, \"properties\": {\"code\": \"import pandas as pd\\nimport matplotlib.pyplot as plt\\nimport networkx as nx\\nimport nltk\\nfrom nltk.util import ngrams\\n%matplotlib inline\\ntrain = pd.read_csv(\\\"../input/train.csv\\\")\"}, \"vectorWeights\": null}, {\"class\": \"GrandMasterCode\", \"creationTimeUnix\": 1680793377324, \"id\": \"003d5cca-3c64-4a24-808b-131706668200\", \"lastUpdateTimeUnix\": 1680793377324, \"properties\": {\"code\": \"# Reference: https://www.kaggle.com/code/gpreda/santander-value-prediction-extensive-eda\\n\\nnon_zeros = (train_df.ne(0).sum(axis=0))\\n\\nplt.figure(figsize=(10,6))\\nplt.title(\\\"Distribution of log(number of non-zeros per column) - train set\\\")\\nsns.distplot(np.log1p(non_zeros),color=\\\"darkblue\\\", kde=True,bins=100)\\nplt.show()\"}, \"vectorWeights\": null}, {\"class\": \"GrandMasterCode\", \"creationTimeUnix\": 1680794428129, \"id\": \"00838b9c-b1e4-42e1-af56-e3a599715cbb\", \"lastUpdateTimeUnix\": 1680794428129, \"properties\": {\"code\": \"# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-notebook-v3-0\\n\\ntest = pd.read_csv(test_file, usecols=['age'])\\ntest['age'] = test['age'].replace(to_replace=[' NA'], value=np.nan)\\ntest['age'] = test['age'].astype('float64')\\n\\nage_series = test.age.value_counts()\\nplt.figure(figsize=(12,4))\\nsns.barplot(age_series.index.astype('int'), age_series.values, alpha=0.8)\\nplt.ylabel('Number of Occurrences of the customer', fontsize=12)\\nplt.xlabel('Age', fontsize=12)\\nplt.xticks(rotation='vertical')\\nplt.show()\"}, \"vectorWeights\": null}, {\"class\": \"GrandMasterCode\", \"creationTimeUnix\": 1680794643438, \"id\": \"0086e5b6-7f41-436c-b626-e3f2c1bd6dfa\", \"lastUpdateTimeUnix\": 1680794643438, \"properties\": {\"code\": \"# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-notebook\\n\\nbio = pd.read_csv(\\\"../input/biology.csv\\\")\\ntitle = np.array(bio['title'])\\ncontent = np.array(bio['content'])\\ntags = np.array(bio['tags'])\\n\\n# wordcloud for tags #\\ntext = ''\\nfor ind, tag in enumerate(tags):\\n    text = \\\" \\\".join([text, tag])\\ntext = text.strip()\\n\\nwordcloud = WordCloud(background_color='white', width=600, height=300, max_font_size=50, max_words=80).generate(text)\\nwordcloud.recolor(random_state=218)\\nplt.imshow(wordcloud)\\nplt.axis(\\\"off\\\")\\nplt.title(\\\"Wordcloud on 'tags' for biology \\\")\\nplt.show()\\n\\n# wordcloud for title #\\ntext = ''\\nfor ind, tag in enumerate(title):\\n    text = \\\" \\\".join([text, tag])\\ntext = text.strip()\\n\\nstop_words = set(stopwords.words('english') + ['sas', 'ss', 'fas', 'des', 'les', 'ess'])\\nwordcloud = WordCloud(background_color='white', width=600, height=300, stopwords=stop_words, max_font_size=50, max_words=80).generate(text)\\nwordcloud.recolor(random_state=218)\\nplt.imshow(wordcloud)\\nplt.axis(\\\"off\\\")\\nplt.title(\\\"Wordcloud on 'title' for biology \\\")\\nplt.show()\\n\\n### Commenting this out for now as it throws error while rendering and not while running it at the backend ###\\n## wordcloud for content #\\n#text = ''\\n#for ind, tag in enumerate(content):\\n#    text = \\\" \\\".join([text, tag])\\n#text = text.strip()\\n\\n#stop_words = set(stopwords.words('english') + ['rbs', 'sas', 'ss', 'fas', 'des', 'ess', 'les', 'bas', 'poses', 'los', 'ros', 'cs'])\\n#wordcloud = WordCloud(background_color='white', width=600, height=300, stopwords=stop_words, max_font_size=50, max_words=80).generate(text)\\n#wordcloud.recolor(random_state=218)\\n#plt.imshow(wordcloud)\\n#plt.axis(\\\"off\\\")\\n#plt.title(\\\"Wordcloud on 'content' for biology \\\")\\n#plt.show()\"}, \"vectorWeights\": null}, {\"class\": \"GrandMasterCode\", \"creationTimeUnix\": 1680793745190, \"id\": \"009cba7d-ffb1-4d2e-81e0-4aa7455861b4\", \"lastUpdateTimeUnix\": 1680793745190, \"properties\": {\"code\": \"# Reference: https://www.kaggle.com/code/cdeotte/top-solutions-ensemble-0-947\\n\\nsub.open_channels.value_counts()\\nres=40\\nplt.figure(figsize=(20,5))\\nplt.plot(sub.time[::res],sub.open_channels[::res])\\nplt.show()\"}, \"vectorWeights\": null}, {\"class\": \"GrandMasterCode\", \"creationTimeUnix\": 1680792967878, \"id\": \"00c586a2-2426-49c1-be82-e4c3e1d75af4\", \"lastUpdateTimeUnix\": 1680792967878, \"properties\": {\"code\": \"# Reference: https://www.kaggle.com/code/gpreda/siim-isic-melanoma-classification-eda\\n\\ndef show_dicom_images(data):\\n    img_data = list(data.T.to_dict().values())\\n    f, ax = plt.subplots(3,3, figsize=(16,18))\\n    for i,data_row in enumerate(img_data):\\n        patientImage = data_row['image_name']+'.dcm'\\n        imagePath = os.path.join(PATH,\\\"train/\\\",patientImage)\\n        data_row_img_data = dcm.read_file(imagePath)\\n        modality = data_row_img_data.Modality\\n        age = data_row_img_data.PatientAge\\n        sex = data_row_img_data.PatientSex\\n        data_row_img = dcm.dcmread(imagePath)\\n        ax[i//3, i%3].imshow(data_row_img.pixel_array, cmap=plt.cm.gray) \\n        ax[i//3, i%3].axis('off')\\n        ax[i//3, i%3].set_title(f\\\"ID: {data_row['image_name']}\\\\nModality: {modality} Age: {age} Sex: {sex}\\\\nDiagnosis: {data_row['diagnosis']}\\\")\\n    plt.show()\\nshow_dicom_images(train_df[train_df['target']==1].sample(9))\"}, \"vectorWeights\": null}, {\"class\": \"GrandMasterCode\", \"creationTimeUnix\": 1680793161762, \"id\": \"010daf3c-3e4b-424a-afeb-7270470a3e32\", \"lastUpdateTimeUnix\": 1680793161762, \"properties\": {\"code\": \"# Reference: https://www.kaggle.com/code/allunia/molecules-eda\\n\\nmolecules_size = structures.groupby(\\\"molecule_name\\\").atom.size()\\nmean_size = molecules_size.rolling(window=500).mean()\\nplt.figure(figsize=(20,5))\\nplt.plot(molecules_size.values, '+')\\nplt.plot(np.arange(250, len(molecules_size)-249), mean_size.dropna().values, '-')\\nplt.xlabel(\\\"Molecule name number\\\")\\nplt.ylabel(\\\"Number of atoms\\\");\"}, \"vectorWeights\": null}, {\"class\": \"GrandMasterCode\", \"creationTimeUnix\": 1680795111872, \"id\": \"011128ea-d532-4c33-b446-60f3a37a5a3c\", \"lastUpdateTimeUnix\": 1680795111872, \"properties\": {\"code\": \"# Reference: https://www.kaggle.com/code/robikscube/movie-db-box-office-prediction-intro-and-eda\\n\\nimport warnings\\nwarnings.simplefilter(action='ignore', category=FutureWarning)\\n\\nsns.jointplot('budget_log', 'revenue_log', train.loc[train['budget_log'] > 1], kind='reg')\\nplt.show()\"}, \"vectorWeights\": null}, {\"class\": \"GrandMasterCode\", \"creationTimeUnix\": 1680795199294, \"id\": \"014d55e9-84d1-4e6b-a723-3ddc422f5ac7\", \"lastUpdateTimeUnix\": 1680795199294, \"properties\": {\"code\": \"# Reference: https://www.kaggle.com/code/artgor/text-modelling-in-pytorch-v2\\n\\npuncts = [',', '.', '\\\"', ':', ')', '(', '-', '!', '?', '|', ';', \\\"'\\\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\\\\\', '\\u2022',  '~', '@', '\\u00a3', \\n '\\u00b7', '_', '{', '}', '\\u00a9', '^', '\\u00ae', '`',  '<', '\\u2192', '\\u00b0', '\\u20ac', '\\u2122', '\\u203a',  '\\u2665', '\\u2190', '\\u00d7', '\\u00a7', '\\u2033', '\\u2032', '\\u00c2', '\\u2588', '\\u00bd', '\\u00e0', '\\u2026', \\n '\\u201c', '\\u2605', '\\u201d', '\\u2013', '\\u25cf', '\\u00e2', '\\u25ba', '\\u2212', '\\u00a2', '\\u00b2', '\\u00ac', '\\u2591', '\\u00b6', '\\u2191', '\\u00b1', '\\u00bf', '\\u25be', '\\u2550', '\\u00a6', '\\u2551', '\\u2015', '\\u00a5', '\\u2593', '\\u2014', '\\u2039', '\\u2500', \\n '\\u2592', '\\uff1a', '\\u00bc', '\\u2295', '\\u25bc', '\\u25aa', '\\u2020', '\\u25a0', '\\u2019', '\\u2580', '\\u00a8', '\\u2584', '\\u266b', '\\u2606', '\\u00e9', '\\u00af', '\\u2666', '\\u00a4', '\\u25b2', '\\u00e8', '\\u00b8', '\\u00be', '\\u00c3', '\\u22c5', '\\u2018', '\\u221e', \\n '\\u2219', '\\uff09', '\\u2193', '\\u3001', '\\u2502', '\\uff08', '\\u00bb', '\\uff0c', '\\u266a', '\\u2569', '\\u255a', '\\u00b3', '\\u30fb', '\\u2566', '\\u2563', '\\u2554', '\\u2557', '\\u25ac', '\\u2764', '\\u00ef', '\\u00d8', '\\u00b9', '\\u2264', '\\u2021', '\\u221a', ]\\n\\ndef clean_text(x):\\n    x = str(x)\\n    for punct in puncts:\\n        x = x.replace(punct, f' {punct} ')\\n    return x\\n\\ndef clean_numbers(x):\\n    if bool(re.search(r'\\\\d', x)):\\n        x = re.sub('[0-9]{5,}', '#####', x)\\n        x = re.sub('[0-9]{4}', '####', x)\\n        x = re.sub('[0-9]{3}', '###', x)\\n        x = re.sub('[0-9]{2}', '##', x)\\n    return x\\n\\nmispell_dict = {\\\"aren't\\\" : \\\"are not\\\",\\n                \\\"can't\\\" : \\\"cannot\\\",\\n                \\\"couldn't\\\" : \\\"could not\\\",\\n                \\\"didn't\\\" : \\\"did not\\\",\\n                \\\"doesn't\\\" : \\\"does not\\\",\\n                \\\"don't\\\" : \\\"do not\\\",\\n                \\\"hadn't\\\" : \\\"had not\\\",\\n                \\\"hasn't\\\" : \\\"has not\\\",\\n                \\\"haven't\\\" : \\\"have not\\\",\\n                \\\"he'd\\\" : \\\"he would\\\",\\n                \\\"he'll\\\" : \\\"he will\\\",\\n                \\\"he's\\\" : \\\"he is\\\",\\n                \\\"i'd\\\" : \\\"I would\\\",\\n                \\\"i'd\\\" : \\\"I had\\\",\\n                \\\"i'll\\\" : \\\"I will\\\",\\n                \\\"i'm\\\" : \\\"I am\\\",\\n                \\\"isn't\\\" : \\\"is not\\\",\\n                \\\"it's\\\" : \\\"it is\\\",\\n                \\\"it'll\\\":\\\"it will\\\",\\n                \\\"i've\\\" : \\\"I have\\\",\\n                \\\"let's\\\" : \\\"let us\\\",\\n                \\\"mightn't\\\" : \\\"might not\\\",\\n                \\\"mustn't\\\" : \\\"must not\\\",\\n                \\\"shan't\\\" : \\\"shall not\\\",\\n                \\\"she'd\\\" : \\\"she would\\\",\\n                \\\"she'll\\\" : \\\"she will\\\",\\n                \\\"she's\\\" : \\\"she is\\\",\\n                \\\"shouldn't\\\" : \\\"should not\\\",\\n                \\\"that's\\\" : \\\"that is\\\",\\n                \\\"there's\\\" : \\\"there is\\\",\\n                \\\"they'd\\\" : \\\"they would\\\",\\n                \\\"they'll\\\" : \\\"they will\\\",\\n                \\\"they're\\\" : \\\"they are\\\",\\n                \\\"they've\\\" : \\\"they have\\\",\\n                \\\"we'd\\\" : \\\"we would\\\",\\n                \\\"we're\\\" : \\\"we are\\\",\\n                \\\"weren't\\\" : \\\"were not\\\",\\n                \\\"we've\\\" : \\\"we have\\\",\\n                \\\"what'll\\\" : \\\"what will\\\",\\n                \\\"what're\\\" : \\\"what are\\\",\\n                \\\"what's\\\" : \\\"what is\\\",\\n                \\\"what've\\\" : \\\"what have\\\",\\n                \\\"where's\\\" : \\\"where is\\\",\\n                \\\"who'd\\\" : \\\"who would\\\",\\n                \\\"who'll\\\" : \\\"who will\\\",\\n                \\\"who're\\\" : \\\"who are\\\",\\n                \\\"who's\\\" : \\\"who is\\\",\\n                \\\"who've\\\" : \\\"who have\\\",\\n                \\\"won't\\\" : \\\"will not\\\",\\n                \\\"wouldn't\\\" : \\\"would not\\\",\\n                \\\"you'd\\\" : \\\"you would\\\",\\n                \\\"you'll\\\" : \\\"you will\\\",\\n                \\\"you're\\\" : \\\"you are\\\",\\n                \\\"you've\\\" : \\\"you have\\\",\\n                \\\"'re\\\": \\\" are\\\",\\n                \\\"wasn't\\\": \\\"was not\\\",\\n                \\\"we'll\\\":\\\" will\\\",\\n                \\\"didn't\\\": \\\"did not\\\",\\n                \\\"tryin'\\\":\\\"trying\\\",\\n               '\\\\u200b': '',\\n                '\\u2026': '',\\n                '\\\\ufeff': '',\\n                '\\u0915\\u0930\\u0928\\u093e': '',\\n                '\\u0939\\u0948': ''}\\n\\nfor coin in ['Litecoin', 'altcoin', 'altcoins', 'coinbase', 'litecoin', 'Unocoin', 'Dogecoin', 'cryptocoin', 'Altcoins', 'filecoin', 'Altcoin', 'cryptocoins',\\n             'Altacoin', 'Dentacoin', 'Bytecoin', 'Siacoin', 'Onecoin', 'dogecoin', 'unocoin', 'siacoin', 'litecoins', 'Filecoin', 'Buyucoin', 'Litecoins',\\n             'Laxmicoin', 'shtcoins', 'Sweatcoin', 'Skycoin', 'vitrocoin', 'Monacoin', 'Litcoin', 'reddcoin', 'freebitcoin', 'Namecoin', 'plexcoin', 'Onecoins',\\n             'daikicoin', 'Gainbitcoin', 'Gatecoin', 'Plexcoin', 'peercoin', 'coinsecure', 'dogecoins', 'cointries', 'Zcoin', 'genxcoin', 'Frazcoin', 'frazcoin',\\n             'coinify', 'Nagricoin', 'OKcoin', 'Presscoins', 'Dagcoin', 'batcoin', 'Spectrocoin', 'Travelflexcoin', 'ecoin', 'Minexcoin', 'Kashhcoin', 'coinone',\\n             'octacoin', 'coinsides', 'zabercoin', 'ADZcoin', 'cyptocoin', 'bitecoin', 'Bitecoin', 'Emercoin', 'tegcoin', 'flipcoin', 'Gridcoin', 'Facecoin',\\n             'Ravencoins', 'digicoin', 'bitcoincash', 'Vitrocoin', 'Livecoin', 'dashcoin', 'Fedcoin', 'litcoins', 'Webcoin', 'coinspot', 'bitoxycoin', 'peercoins',\\n             'Ucoin', 'ALTcoins', 'coincidece', 'dagcoin', 'Giracoin', 'coincheck', 'Swisscoin', 'butcoin', 'neocoin', 'mintcoin', 'Myriadcoin', 'Viacoin', 'jiocoin',\\n             'Potcoin', 'bibitcoin', 'gainbitcoin', 'altercoins', 'coinburn', 'Kodakcoin', 'Bcoin', 'Kucoin', 'Operacoin', 'Lomocoin', 'dentacoin', 'Nyancoin',\\n             'Jiocoin', 'Indicoin', 'coinsidered', 'Vertcoin', 'Maidsafecoin', 'coindelta', 'coinfirm', 'coinvest', 'bixcoin', 'litcoin', 'Dogecoins', 'Unicoin',\\n             'Rothscoin', 'localbitcoins', 'groestlcoin', 'sibcoin', 'Travelercoin', 'Vericoin', 'bytecoin', 'Bananacoin', 'PACcoin']:\\n    mispell_dict[coin] = 'bitcoin'\\n\\ndef _get_mispell(mispell_dict):\\n    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\\n    return mispell_dict, mispell_re\\n\\nmispellings, mispellings_re = _get_mispell(mispell_dict)\\ndef replace_typical_misspell(text):\\n    def replace(match):\\n        return mispellings[match.group(0)]\\n    return mispellings_re.sub(replace, text)\\n\\n# Clean the text\\ntrain[\\\"question_text\\\"] = train[\\\"question_text\\\"].apply(lambda x: clean_text(x.lower()))\\ntest[\\\"question_text\\\"] = test[\\\"question_text\\\"].apply(lambda x: clean_text(x.lower()))\\n\\n# Clean numbers\\ntrain[\\\"question_text\\\"] = train[\\\"question_text\\\"].apply(lambda x: clean_numbers(x))\\ntest[\\\"question_text\\\"] = test[\\\"question_text\\\"].apply(lambda x: clean_numbers(x))\\n\\n# Clean speelings\\ntrain[\\\"question_text\\\"] = train[\\\"question_text\\\"].apply(lambda x: replace_typical_misspell(x))\\ntest[\\\"question_text\\\"] = test[\\\"question_text\\\"].apply(lambda x: replace_typical_misspell(x))\\nmax_features = 120000\\ntk = Tokenizer(lower = True, filters='', num_words=max_features)\\nfull_text = list(train['question_text'].values) + list(test['question_text'].values)\\ntk.fit_on_texts(full_text)\\ntrain_tokenized = tk.texts_to_sequences(train['question_text'].fillna('missing'))\\ntest_tokenized = tk.texts_to_sequences(test['question_text'].fillna('missing'))\\ntrain['question_text'].apply(lambda x: len(x.split())).plot(kind='hist');\\nplt.yscale('log');\\nplt.title('Distribution of question text length in characters');\"}, \"vectorWeights\": null}, {\"class\": \"GrandMasterCode\", \"creationTimeUnix\": 1680793399316, \"id\": \"01691d53-994a-4c32-a2f8-cc7722ffd4d9\", \"lastUpdateTimeUnix\": 1680793399316, \"properties\": {\"code\": \"# Reference: https://www.kaggle.com/code/allunia/protein-atlas-exploration-and-baseline\\n\\nfeature = \\\"Cytosol\\\"\\nplt.figure(figsize=(20,5))\\nsns.distplot(baseline_proba_predictions[feature].values[0:-10], color=\\\"Purple\\\")\\nplt.xlabel(\\\"Predicted probabilites of {}\\\".format(feature))\\nplt.ylabel(\\\"Density\\\")\\nplt.xlim([0,1])\"}, \"vectorWeights\": null}, {\"class\": \"GrandMasterCode\", \"creationTimeUnix\": 1680795008734, \"id\": \"016b4323-b126-4c47-bc85-d1f33154893d\", \"lastUpdateTimeUnix\": 1680795008734, \"properties\": {\"code\": \"# Reference: https://www.kaggle.com/code/artgor/exploration-of-data-step-by-step\\n\\nsentiment_dict = {}\\nfor filename in os.listdir('../input/train_sentiment/'):\\n    with open('../input/train_sentiment/' + filename, 'r') as f:\\n        sentiment = json.load(f)\\n    pet_id = filename.split('.')[0]\\n    sentiment_dict[pet_id] = {}\\n    sentiment_dict[pet_id]['magnitude'] = sentiment['documentSentiment']['magnitude']\\n    sentiment_dict[pet_id]['score'] = sentiment['documentSentiment']['score']\\n    sentiment_dict[pet_id]['language'] = sentiment['language']\\n\\nfor filename in os.listdir('../input/test_sentiment/'):\\n    with open('../input/test_sentiment/' + filename, 'r') as f:\\n        sentiment = json.load(f)\\n    pet_id = filename.split('.')[0]\\n    sentiment_dict[pet_id] = {}\\n    sentiment_dict[pet_id]['magnitude'] = sentiment['documentSentiment']['magnitude']\\n    sentiment_dict[pet_id]['score'] = sentiment['documentSentiment']['score']\\n    sentiment_dict[pet_id]['language'] = sentiment['language']\\ntrain['lang'] = train['PetID'].apply(lambda x: sentiment_dict[x]['language'] if x in sentiment_dict else 'no')\\ntrain['magnitude'] = train['PetID'].apply(lambda x: sentiment_dict[x]['magnitude'] if x in sentiment_dict else 0)\\ntrain['score'] = train['PetID'].apply(lambda x: sentiment_dict[x]['score'] if x in sentiment_dict else 0)\\n\\ntest['lang'] = test['PetID'].apply(lambda x: sentiment_dict[x]['language'] if x in sentiment_dict else 'no')\\ntest['magnitude'] = test['PetID'].apply(lambda x: sentiment_dict[x]['magnitude'] if x in sentiment_dict else 0)\\ntest['score'] = test['PetID'].apply(lambda x: sentiment_dict[x]['score'] if x in sentiment_dict else 0)\\n\\nall_data['lang'] = all_data['PetID'].apply(lambda x: sentiment_dict[x]['language'] if x in sentiment_dict else 'no')\\nall_data['magnitude'] = all_data['PetID'].apply(lambda x: sentiment_dict[x]['magnitude'] if x in sentiment_dict else 0)\\nall_data['score'] = all_data['PetID'].apply(lambda x: sentiment_dict[x]['score'] if x in sentiment_dict else 0)\\nplot_four_graphs(col='lang', main_title='lang', dataset_title='Number of pets by lang in train and test data')\"}, \"vectorWeights\": null}, {\"class\": \"GrandMasterCode\", \"creationTimeUnix\": 1680793247933, \"id\": \"018866ad-b40f-4c4b-8647-8c8281a6dd41\", \"lastUpdateTimeUnix\": 1680793247933, \"properties\": {\"code\": \"fit_gaussians = False\\nuse_plotly=True\"}, \"vectorWeights\": null}, {\"class\": \"GrandMasterCode\", \"creationTimeUnix\": 1680793702465, \"id\": \"01be989e-c69e-4f89-8264-eb20703bfb9a\", \"lastUpdateTimeUnix\": 1680793702465, \"properties\": {\"code\": \"# Reference: https://www.kaggle.com/code/iafoss/postprocessing-for-hypercolumns-kernel\\n\\nnoise_th = 75.0*255*(sz/128.0)**2 #threshold for the number of predicted pixels\\n\\ngc.collect();\\ntorch.cuda.empty_cache()\\npreds = preds0.clone()\\npreds[preds.view(preds.shape[0],-1).float().sum(-1) < noise_th,...] = 0.0\\n\\ndices = []\\n#here evrything is multiplied by 255, so 0.2 threshold corresponds to 51\\nthrs = np.arange(40, 60, 1)\\nfor th in progress_bar(thrs):\\n    preds_m = (preds>th).byte()\\n    dices.append(dice_overall(preds_m, ys).mean())\\ndices = np.array(dices)    \\n\\nbest_dice = dices.max()\\nbest_thr = thrs[dices.argmax()]\\n\\nplt.figure(figsize=(8,4))\\nplt.plot(thrs, dices)\\nplt.vlines(x=best_thr, ymin=dices.min(), ymax=dices.max())\\nplt.text(best_thr, best_dice, f'DICE = {best_dice:.3f}', fontsize=14);\\nplt.show()\"}, \"vectorWeights\": null}, {\"class\": \"GrandMasterCode\", \"creationTimeUnix\": 1680793356344, \"id\": \"01c3a6fc-780e-4251-83bc-666819ac548d\", \"lastUpdateTimeUnix\": 1680793356344, \"properties\": {\"code\": \"# Reference: https://www.kaggle.com/code/gpreda/santander-value-prediction-extensive-eda\\n\\nnon_zeros = (train_df.ne(0).sum(axis=1))\\n\\nplt.figure(figsize=(10,6))\\nplt.title(\\\"Distribution of log(number of non-zeros per row) - train set\\\")\\nsns.distplot(np.log1p(non_zeros),color=\\\"red\\\", kde=True,bins=100)\\nplt.show()\\n\"}, \"vectorWeights\": null}, {\"class\": \"GrandMasterCode\", \"creationTimeUnix\": 1680793507193, \"id\": \"01c50350-331a-4ceb-9a16-892130522c23\", \"lastUpdateTimeUnix\": 1680793507193, \"properties\": {\"code\": \"# Reference: https://www.kaggle.com/code/robikscube/rob-s-covid19-week-2-submission-less-aggressive\\n\\ndat.iloc[-10:]\\nfor myplace in ['Iran']:\\n\\n    # Confirmed Cases\\n    fig, axs = plt.subplots(1, 2, figsize=(15, 3))\\n    dat = tt.query('Place == @myplace and Days_Since_Ten_Cases >= 0')[['Days_Since_Ten_Cases','ConfirmedCases']].dropna()\\n    dat = dat.iloc[-10:]\\n    X = dat['Days_Since_Ten_Cases']\\n    y = dat['ConfirmedCases']\\n    y = y.cummax()\\n    dat_all = tt.query('Place == @myplace and Days_Since_Ten_Cases >= 0')[['Days_Since_Ten_Cases','ConfirmedCases']]\\n    X_pred = dat_all['Days_Since_Ten_Cases']\\n    en = ElasticNet()\\n    en.fit(X.values.reshape(-1, 1), y.values)\\n    preds = en.predict(X_pred.values.reshape(-1, 1))\\n    tt.loc[(tt['Place'] == myplace) & (tt['Days_Since_Ten_Cases'] >= 0), 'ConfirmedCases_Pred1'] = preds\\n    # Cap at 10 % Population\\n    pop_myplace = tt.query('Place == @myplace')['Population_final'].values[0]\\n    tt.loc[(tt['Place'] == myplace) & (tt['ConfirmedCases_Pred1'] > (0.1 * pop_myplace)), 'ConfirmedCases_Pred1'] = (0.1 * pop_myplace)\\n    ax = tt.query('Place == @myplace').set_index('Date')[['ConfirmedCases','ConfirmedCases_Pred1']].plot(figsize=(15, 5), title=myplace, ax=axs[0])\\n    # Fatalities\\n    # If low count then do percent of confirmed:\\n    dat = tt.query('Place == @myplace and Days_Since_Ten_Cases >= 0')[['Days_Since_Ten_Cases','Fatalities']].dropna()\\n    dat = dat.iloc[-10:]\\n    if len(dat) < 5:\\n        tt.loc[(tt['Place'] == myplace), 'Fatalities_Pred1'] = tt.loc[(tt['Place'] == myplace)]['ConfirmedCases_Pred1'] * 0.001\\n    elif tt.query('Place == @myplace')['Fatalities'].max() < 5:\\n        tt.loc[(tt['Place'] == myplace), 'Fatalities_Pred1'] = tt.loc[(tt['Place'] == myplace)]['ConfirmedCases_Pred1'] * 0.001\\n    else:\\n        X = dat['Days_Since_Ten_Cases']\\n        y = dat['Fatalities']\\n        y = y.cummax()\\n        dat_all = tt.query('Place == @myplace and Days_Since_Ten_Cases >= 0')[['Days_Since_Ten_Cases','Fatalities']]\\n        X_pred = dat_all['Days_Since_Ten_Cases']\\n        en = ElasticNet()\\n        en.fit(X.values.reshape(-1, 1), y.values)\\n        preds = en.predict(X_pred.values.reshape(-1, 1))\\n        tt.loc[(tt['Place'] == myplace) & (tt['Days_Since_Ten_Cases'] >= 0), 'Fatalities_Pred1'] = preds\\n\\n        # Cap at 0.0001 Population\\n        pop_myplace = tt.query('Place == @myplace')['Population_final'].values[0]\\n        tt.loc[(tt['Place'] == myplace) & (tt['Fatalities_Pred1'] > (0.0005 * pop_myplace)), 'Fatalities_Pred1'] = (0.0005 * pop_myplace)\\n\\n    ax = tt.query('Place == @myplace').set_index('Date')[['Fatalities','Fatalities_Pred1']].plot(figsize=(15, 5), title=myplace, ax=axs[1])\\n    plt.show()\\n# Clean Up any time the actual is less than the real\\ntt['ConfirmedCases_Pred1'] = tt[['ConfirmedCases','ConfirmedCases_Pred1']].max(axis=1)\\ntt['Fatalities_Pred1'] = tt[['Fatalities','Fatalities_Pred1']].max(axis=1)\\n\\ntt['ConfirmedCases_Pred1'] = tt['ConfirmedCases_Pred1'].fillna(0)\\ntt['Fatalities_Pred1'] = tt['Fatalities_Pred1'].fillna(0)\\n\\n# Fill pred with\\ntt.loc[~tt['ConfirmedCases'].isna(), 'ConfirmedCases_Pred1'] = tt.loc[~tt['ConfirmedCases'].isna()]['ConfirmedCases']\\ntt.loc[~tt['Fatalities'].isna(), 'Fatalities_Pred1'] = tt.loc[~tt['Fatalities'].isna()]['Fatalities']\\n\\ntt['ConfirmedCases_Pred1'] = tt.groupby('Place')['ConfirmedCases_Pred1'].transform('cummax')\\ntt['Fatalities_Pred1'] = tt.groupby('Place')['Fatalities_Pred1'].transform('cummax')\\n# Questionable numbers\\ntt.query('Place == \\\"Iran\\\"').set_index('Date')[['ConfirmedCases',\\n                                               'ConfirmedCases_Pred1',]].plot(figsize=(15, 5))\\n# Make Iran's Predictions Linear\\n\\ntt.query('Place == \\\"Iran\\\"').set_index('Date')[['Fatalities','Fatalities_Pred1']].plot(figsize=(15, 5))\"}, \"vectorWeights\": null}, {\"class\": \"GrandMasterCode\", \"creationTimeUnix\": 1680792989710, \"id\": \"01c64396-5dcb-45b4-84ff-5c4fbf898918\", \"lastUpdateTimeUnix\": 1680792989710, \"properties\": {\"code\": \"# Reference: https://www.kaggle.com/code/tunguz/cats-and-dogs-with-rapids-t-sne\\n\\ntrain = np.load('../input/cats-and-dogs-embedded-data/cats_and_dogs_1/train_ResNet50.npy')\\n%%time\\ntsne = TSNE(n_components=2)\\ntrain_2D = tsne.fit_transform(train)\\nplt.scatter(train_2D[:,0], train_2D[:,1], c = target, s = 0.5)\"}, \"vectorWeights\": null}, {\"class\": \"GrandMasterCode\", \"creationTimeUnix\": 1680794046360, \"id\": \"01ea6cf7-613f-4c7d-b1b3-37f9fe90b8af\", \"lastUpdateTimeUnix\": 1680794046360, \"properties\": {\"code\": \"# Reference: https://www.kaggle.com/code/kabure/extensive-eda-of-deal-probability\\n\\ndf_train['count_word_log'] = np.log(df_train['count_word'])\\n\\n(sns\\n  .FacetGrid(df_train, \\n             hue='deal_prob_cat', \\n             size=5, aspect=2)\\n  .map(sns.kdeplot, 'count_word_log', shade=True)\\n .add_legend()\\n)\\nplt.show()\"}, \"vectorWeights\": null}, {\"class\": \"GrandMasterCode\", \"creationTimeUnix\": 1680794282392, \"id\": \"02161632-f0b3-455f-a04d-9d1d5dd509da\", \"lastUpdateTimeUnix\": 1680794282392, \"properties\": {\"code\": \"# Reference: https://www.kaggle.com/code/allunia/don-t-turn-into-a-smoothie-after-the-shake-up\\n\\npatient_gender_train = train_info.groupby(\\\"patient_id\\\").sex.unique().apply(lambda l: l[0])\\npatient_gender_test = test_info.groupby(\\\"patient_id\\\").sex.unique().apply(lambda l: l[0])\\n\\ntrain_patients = pd.DataFrame(index=patient_gender_train.index.values, data=patient_gender_train.values, columns=[\\\"sex\\\"])\\ntest_patients = pd.DataFrame(index=patient_gender_test.index.values, data=patient_gender_test.values, columns=[\\\"sex\\\"])\\n\\ntrain_patients.loc[:, \\\"num_images\\\"] = train_info.groupby(\\\"patient_id\\\").size()\\ntest_patients.loc[:, \\\"num_images\\\"] = test_info.groupby(\\\"patient_id\\\").size()\\n\\ntrain_patients.loc[:, \\\"min_age\\\"] = train_info.groupby(\\\"patient_id\\\").age_approx.min()\\ntrain_patients.loc[:, \\\"max_age\\\"] = train_info.groupby(\\\"patient_id\\\").age_approx.max()\\ntest_patients.loc[:, \\\"min_age\\\"] = test_info.groupby(\\\"patient_id\\\").age_approx.min()\\ntest_patients.loc[:, \\\"max_age\\\"] = test_info.groupby(\\\"patient_id\\\").age_approx.max()\\n\\ntrain_patients.loc[:, \\\"age_span\\\"] = train_patients[\\\"max_age\\\"] - train_patients[\\\"min_age\\\"]\\ntest_patients.loc[:, \\\"age_span\\\"] = test_patients[\\\"max_age\\\"] - test_patients[\\\"min_age\\\"]\\n\\ntrain_patients.loc[:, \\\"benign_cases\\\"] = train_info.groupby([\\\"patient_id\\\", \\\"benign_malignant\\\"]).size().loc[:, \\\"benign\\\"]\\ntrain_patients.loc[:, \\\"malignant_cases\\\"] = train_info.groupby([\\\"patient_id\\\", \\\"benign_malignant\\\"]).size().loc[:, \\\"malignant\\\"]\\ntrain_patients[\\\"min_age_malignant\\\"] = train_info.groupby([\\\"patient_id\\\", \\\"benign_malignant\\\"]).age_approx.min().loc[:, \\\"malignant\\\"]\\ntrain_patients[\\\"max_age_malignant\\\"] = train_info.groupby([\\\"patient_id\\\", \\\"benign_malignant\\\"]).age_approx.max().loc[:, \\\"malignant\\\"]\\ntrain_patients.sort_values(by=\\\"malignant_cases\\\", ascending=False).head()\\nfig, ax = plt.subplots(2,2,figsize=(20,12))\\nsns.countplot(train_patients.sex, ax=ax[0,0], palette=\\\"Reds\\\")\\nax[0,0].set_title(\\\"Gender counts with unique patient ids in train\\\")\\nsns.countplot(test_patients.sex, ax=ax[0,1], palette=\\\"Blues\\\");\\nax[0,1].set_title(\\\"Gender counts with unique patient ids in test\\\");\\n\\ntrain_age_span_perc = train_patients.age_span.value_counts() / train_patients.shape[0] * 100\\ntest_age_span_perc = test_patients.age_span.value_counts() / test_patients.shape[0] * 100\\n\\nsns.barplot(train_age_span_perc.index, train_age_span_perc.values, ax=ax[1,0], color=\\\"Orangered\\\");\\nsns.barplot(test_age_span_perc.index, test_age_span_perc.values, ax=ax[1,1], color=\\\"Lightseagreen\\\");\\nax[1,0].set_title(\\\"Patients age span in train\\\")\\nax[1,1].set_title(\\\"Patients age span in test\\\")\\nfor n in range(2):\\n    ax[1,n].set_ylabel(\\\"% in data\\\")\\n    ax[1,n].set_xlabel(\\\"age span\\\");\"}, \"vectorWeights\": null}, {\"class\": \"GrandMasterCode\", \"creationTimeUnix\": 1680795349425, \"id\": \"0224f746-0a17-4eaa-9c5b-b7377492adb7\", \"lastUpdateTimeUnix\": 1680795349425, \"properties\": {\"code\": \"# Reference: https://www.kaggle.com/code/artgor/eda-feature-engineering-and-xgb-lgb\\n\\nfig, ax1 = plt.subplots(figsize=(16, 8))\\nplt.title(\\\"Project count and approval rate by day of week.\\\")\\nsns.countplot(x='weekday', data=train, ax=ax1)\\nax1.set_ylabel('Projects count', color='b')\\nplt.legend(['Projects count'])\\nax2 = ax1.twinx()\\nsns.pointplot(x=\\\"weekday\\\", y=\\\"project_is_approved\\\", data=train, ci=99, ax=ax2, color='black')\\nax2.set_ylabel('Approval rate', color='g')\\nplt.legend(['Approval rate'], loc=(0.875, 0.9))\\nplt.grid(False)\"}, \"vectorWeights\": null}, {\"class\": \"GrandMasterCode\", \"creationTimeUnix\": 1680794707574, \"id\": \"022e09e8-c2e2-47d1-8829-ba0daad5fbc1\", \"lastUpdateTimeUnix\": 1680794707574, \"properties\": {\"code\": \"# Reference: https://www.kaggle.com/code/mobassir/jigsaw-google-q-a-eda\\n\\navg_word_len = train['answer'].apply(lambda x: 1.0*len(''.join(x.split()))/(len(x) - len(''.join(x.split())) + 1))\\ntrain['avg_word_len'] = avg_word_len\\navg_word_len = train.loc[train['avg_word_len']<10]['avg_word_len']\\nsns.distplot(avg_word_len, color='g')\\nplt.show()\"}, \"vectorWeights\": null}, {\"class\": \"GrandMasterCode\", \"creationTimeUnix\": 1680793507193, \"id\": \"023eb31e-e47d-42a9-9c10-a3bd7c2b3d70\", \"lastUpdateTimeUnix\": 1680793507193, \"properties\": {\"code\": \"import numpy as np # linear algebra\\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\\nimport matplotlib.pylab as plt\\nimport seaborn as sns\\nimport os, gc, pickle, copy, datetime, warnings\\nimport pycountry\\n\\npd.set_option('max_columns', 500)\\npd.set_option('max_rows', 500)\\npd.options.display.float_format = '{:.2f}'.format\\n!ls -l ../input/covid19-global-forecasting-week-2/\"}, \"vectorWeights\": null}, {\"class\": \"GrandMasterCode\", \"creationTimeUnix\": 1680794814657, \"id\": \"0260d95e-b97d-477b-9f27-bd6de84d0792\", \"lastUpdateTimeUnix\": 1680794814657, \"properties\": {\"code\": \"# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-notebook-instacart\\n\\norder_products_train_df = pd.merge(order_products_train_df, orders_df, on='order_id', how='left')\\ngrouped_df = order_products_train_df.groupby([\\\"order_dow\\\"])[\\\"reordered\\\"].aggregate(\\\"mean\\\").reset_index()\\n\\nplt.figure(figsize=(12,8))\\nsns.barplot(grouped_df['order_dow'].values, grouped_df['reordered'].values, alpha=0.8, color=color[3])\\nplt.ylabel('Reorder ratio', fontsize=12)\\nplt.xlabel('Day of week', fontsize=12)\\nplt.title(\\\"Reorder ratio across day of week\\\", fontsize=15)\\nplt.xticks(rotation='vertical')\\nplt.ylim(0.5, 0.7)\\nplt.show()\\ngrouped_df = order_products_train_df.groupby([\\\"order_hour_of_day\\\"])[\\\"reordered\\\"].aggregate(\\\"mean\\\").reset_index()\\n\\nplt.figure(figsize=(12,8))\\nsns.barplot(grouped_df['order_hour_of_day'].values, grouped_df['reordered'].values, alpha=0.8, color=color[4])\\nplt.ylabel('Reorder ratio', fontsize=12)\\nplt.xlabel('Hour of day', fontsize=12)\\nplt.title(\\\"Reorder ratio across hour of day\\\", fontsize=15)\\nplt.xticks(rotation='vertical')\\nplt.ylim(0.5, 0.7)\\nplt.show()\\n\\ngrouped_df = order_products_train_df.groupby([\\\"order_dow\\\", \\\"order_hour_of_day\\\"])[\\\"reordered\\\"].aggregate(\\\"mean\\\").reset_index()\\ngrouped_df = grouped_df.pivot('order_dow', 'order_hour_of_day', 'reordered')\\n\\nplt.figure(figsize=(12,6))\\nsns.heatmap(grouped_df)\\nplt.title(\\\"Reorder ratio of Day of week Vs Hour of day\\\")\\nplt.show()\"}, \"vectorWeights\": null}, {\"class\": \"GrandMasterCode\", \"creationTimeUnix\": 1680795455959, \"id\": \"0287b90a-4c94-4d74-a102-91a80bd75877\", \"lastUpdateTimeUnix\": 1680795455959, \"properties\": {\"code\": \"# Reference: https://www.kaggle.com/code/robikscube/big-data-bowl-comprehensive-eda-with-pandas\\n\\ntrain.groupby('PlayId').first()['Yards'] \\\\\\n    .plot(kind='hist',\\n          figsize=(15, 5),\\n          bins=50,\\n          title='Distribution of Yards Gained (Target)')\\nplt.show()\"}, \"vectorWeights\": null}, {\"class\": \"GrandMasterCode\", \"creationTimeUnix\": 1680793226280, \"id\": \"029ae5cc-e289-4b52-9be9-d5d273321e64\", \"lastUpdateTimeUnix\": 1680793226280, \"properties\": {\"code\": \"# Reference: https://www.kaggle.com/code/shivamb/imaterialist-fashion-eda-object-detection-colors\\n\\ntrain_labels = Counter(train_annotations)\\n\\nxvalues = list(train_labels.keys())\\nyvalues = list(train_labels.values())\\n\\ntrace1 = go.Bar(x=xvalues, y=yvalues, opacity=0.8, name=\\\"year count\\\", marker=dict(color='rgba(20, 20, 20, 1)'))\\nlayout = dict(width=800, title='Distribution of different labels in the train dataset', legend=dict(orientation=\\\"h\\\"));\\n\\nfig = go.Figure(data=[trace1], layout=layout);\\niplot(fig);\\nvalid_labels = Counter(valid_annotations)\\n\\nxvalues = list(valid_labels.keys())\\nyvalues = list(valid_labels.values())\\n\\ntrace1 = go.Bar(x=xvalues, y=yvalues, opacity=0.8, name=\\\"year count\\\", marker=dict(color='rgba(20, 20, 20, 1)'))\\nlayout = dict(width=800, title='Distribution of different labels in the valid dataset', legend=dict(orientation=\\\"h\\\"));\\n\\nfig = go.Figure(data=[trace1], layout=layout);\\niplot(fig);\\ndef get_images_for_labels(labellist, data):\\n    image_ids = []\\n    for each in data['annotations']:\\n        if all(x in each['labelId'] for x in labellist):\\n            image_ids.append(each['imageId'])\\n            if len(image_ids) == 2:\\n                break\\n    image_urls = []\\n    for each in data['images']:\\n        if each['imageId'] in image_ids:\\n            image_urls.append(each['url'])\\n    return image_urls\\n# most common labels \\n\\ntemps = train_labels.most_common(10)\\nlabels_tr = [\\\"Label-\\\"+str(x[0]) for x in temps]\\nvalues = [x[1] for x in temps]\\n\\ntrace1 = go.Bar(x=labels_tr, y=values, opacity=0.7, name=\\\"year count\\\", marker=dict(color='rgba(120, 120, 120, 0.8)'))\\nlayout = dict(height=400, title='Top 10 Labels in the train dataset', legend=dict(orientation=\\\"h\\\"));\\n\\nfig = go.Figure(data=[trace1], layout=layout);\\niplot(fig);\"}, \"vectorWeights\": null}], \"totalResults\": 25}\n"
     ]
    }
   ],
   "source": [
    "import weaviate\n",
    "import json\n",
    "\n",
    "\n",
    "weaviate_client = weaviate.Client(\"http://202.151.177.149:81\")  # Replace with your endpoint\n",
    "some_objects = weaviate_client.data_object.get()\n",
    "if (json.dumps(some_objects)):\n",
    "    print(True)\n",
    "else:\n",
    "    print(False)\n",
    "\n",
    "print(json.dumps(some_objects))\n",
    "\n",
    "from elasticsearch import Elasticsearch\n",
    "from json import loads\n",
    "\n",
    "elastic_client = Elasticsearch(\"http://202.151.177.154:9200\")\n",
    "\n",
    "response = str(elastic_client.info())\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Obj\n",
    "# {\n",
    "#     \"markdown\": [Array<Str>],\n",
    "#     \"processed\": [Array<Str>],\n",
    "#     \"code\": Str\n",
    "# }\n",
    "\n",
    "# Example Elasticsearch data tier\n",
    "# 1. grandmaster\n",
    "# 2. master\n",
    "# 3. expert\n",
    "\n",
    "# Example Weaviate Classname\n",
    "# 1. GrandMasterCode\n",
    "# 2. MasterCode\n",
    "# 3. ExpertCode\n",
    "\n",
    "# ElasticSearch Query Example\n",
    "# baseQuery = {\n",
    "#     \"query\": {\n",
    "#         \"match\": {\n",
    "#             \"markdown\": {\n",
    "#                 \"query\": markdown_desc\n",
    "#             }\n",
    "#         }\n",
    "#     }\n",
    "# }\n",
    "# processedQuery = {\n",
    "#     \"query\": {\n",
    "#         \"match\": {\n",
    "#             \"processed\": {\n",
    "#                 \"query\": markdown_desc\n",
    "#             }\n",
    "#         }\n",
    "#     }\n",
    "# }\n",
    "\n",
    "def getMLRecommendation(text: str, target_class: str) -> dict:\n",
    "    # md_text = obj['markdown']\n",
    "    # cur_class = \"grandmaster\"\n",
    "    near_text = {\"concepts\": [text]}\n",
    "    fetched = (weaviate_client.query\n",
    "                      .get(CLASS_NAME[target_class], [\"code\"])\n",
    "                      .with_near_text(near_text)\n",
    "                      .with_limit(1)\n",
    "                      .do()\n",
    "                      )\n",
    "    data = fetched['data']['Get'][CLASS_NAME[target_class]]\n",
    "    return data[0]\n",
    "\n",
    "def getElasticQuery(query, type=[\"base\", \"processed\"]):\n",
    "    if (type == \"processed\"):\n",
    "        result = {\n",
    "        \"query\": {\n",
    "            \"match\": {\n",
    "                \"processed\": {\n",
    "                    \"query\": query\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    else:\n",
    "        result = {\n",
    "        \"query\": {\n",
    "            \"match\": {\n",
    "                \"markdown\": {\n",
    "                    \"query\": query\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    return result \n",
    "\n",
    "def getElasticRecommendation(index, queryBody):\n",
    "    response = elastic_client.search(index=index, body=queryBody)\n",
    "\n",
    "    if response and response[\"hits\"][\"hits\"]:\n",
    "        result = response[\"hits\"][\"hits\"][0][\"_source\"]\n",
    "        return result['code']\n",
    "    return []\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_name in FILES:\n",
    "    with open(f'data/{file_name}', 'r') as file:\n",
    "        data = json.load(file)\n",
    "        data_rank = file_name[:-21]\n",
    "\n",
    "        data_length = len(data)\n",
    "        testing_accumulate = []\n",
    "        count = 0\n",
    "        for row in data:\n",
    "            markdown = \"\".join(row['markdown'])\n",
    "            \n",
    "            try:\n",
    "                temp_result = {\n",
    "                    \"original_md\": markdown,\n",
    "                    \"original_code\": row['code'],\n",
    "                    \"count\": count,\n",
    "                    \"data_rank\": data_rank\n",
    "                }\n",
    "\n",
    "                # Get ML Recommendation\n",
    "                recommended_code_ml = getMLRecommendation(markdown, data_rank)\n",
    "                temp_result[\"recommended_code_ml\"] = recommended_code_ml['code']\n",
    "                # temp_result[\"model\"] = \"machine-learning\"\n",
    "\n",
    "                # Get Elastic Recommendation\n",
    "                query_base = getElasticQuery(markdown)\n",
    "                query_processed = getElasticQuery(markdown, \"processed\")\n",
    "                recommended_code_es_base = getElasticRecommendation(data_rank, query_base)\n",
    "                recommended_code_es_processed = getElasticRecommendation(data_rank, query_processed)\n",
    "                temp_result['recommended_code_es_base'] = recommended_code_es_base\n",
    "                temp_result['recommended_code_es_processed'] = recommended_code_es_processed\n",
    "\n",
    "                # Append to the collection\n",
    "                testing_accumulate.append(temp_result)\n",
    "\n",
    "                print(f'Count: {count}/{data_length}')\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f'[{datetime.now().strftime(\"%d/%m/%y %H:%M:%S\")}] Skipping item #{count} due to an error.')\n",
    "                logging.error(f\"Error occurred for item: {markdown}\\nError message: {str(e)}\\n>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\\n\")\n",
    "                continue\n",
    "            count =  count + 1\n",
    "        \n",
    "        with open(f\"recommendation result/{data_rank}_result.json\", \"w\") as file:\n",
    "            json.dump(testing_accumulate, file)\n",
    "            print(f\"successfully write to: {data_rank}_result.json \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "senior-jupyter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
