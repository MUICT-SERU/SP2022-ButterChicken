{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILES = [\n",
    "    \"grandmaster_nl_pl_only_plot.json\",\n",
    "    # \"master_nl_pl_only_plot.json\",\n",
    "    # \"expert_nl_pl_only_plot.json\",\n",
    "]\n",
    "\n",
    "CLASS_NAME = {\n",
    "    \"grandmaster\": \"GrandMasterCode\",\n",
    "    \"master\": \"MasterCode\",\n",
    "    \"expert\": \"ExpertCode\",\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Weaviate instance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "{\"deprecations\": null, \"objects\": [{\"class\": \"GrandMasterCode\", \"creationTimeUnix\": 1680794367435, \"id\": \"002bd101-b16d-48c7-a401-44c673520d1a\", \"lastUpdateTimeUnix\": 1680794367435, \"properties\": {\"code\": \"# Reference: https://www.kaggle.com/code/tarunpaparaju/champs-competition-chemistry-background-and-eda\\n\\nplt.figure(figsize=(26, 24))\\nfor i, col in enumerate(typelist):\\n    plt.subplot(4,2, i + 1)\\n    sns.distplot(dipole_moments[train['type']==col]['X'],color = 'orange', kde=False)\\n    sns.distplot(dipole_moments[train['type']==col]['Y'],color = 'red', kde=False)\\n    sns.distplot(dipole_moments[train['type']==col]['Z'],color = 'blue', kde=False)\\n    plt.title(col)\"}, \"vectorWeights\": null}, {\"class\": \"GrandMasterCode\", \"creationTimeUnix\": 1680792753015, \"id\": \"00300e91-5c0a-4237-9312-85a5c14ea286\", \"lastUpdateTimeUnix\": 1680792753015, \"properties\": {\"code\": \"import pandas as pd\\nimport matplotlib.pyplot as plt\\nimport networkx as nx\\nimport nltk\\nfrom nltk.util import ngrams\\n%matplotlib inline\\ntrain = pd.read_csv(\\\"../input/train.csv\\\")\"}, \"vectorWeights\": null}, {\"class\": \"GrandMasterCode\", \"creationTimeUnix\": 1680793377324, \"id\": \"003d5cca-3c64-4a24-808b-131706668200\", \"lastUpdateTimeUnix\": 1680793377324, \"properties\": {\"code\": \"# Reference: https://www.kaggle.com/code/gpreda/santander-value-prediction-extensive-eda\\n\\nnon_zeros = (train_df.ne(0).sum(axis=0))\\n\\nplt.figure(figsize=(10,6))\\nplt.title(\\\"Distribution of log(number of non-zeros per column) - train set\\\")\\nsns.distplot(np.log1p(non_zeros),color=\\\"darkblue\\\", kde=True,bins=100)\\nplt.show()\"}, \"vectorWeights\": null}, {\"class\": \"GrandMasterCode\", \"creationTimeUnix\": 1680794428129, \"id\": \"00838b9c-b1e4-42e1-af56-e3a599715cbb\", \"lastUpdateTimeUnix\": 1680794428129, \"properties\": {\"code\": \"# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-notebook-v3-0\\n\\ntest = pd.read_csv(test_file, usecols=['age'])\\ntest['age'] = test['age'].replace(to_replace=[' NA'], value=np.nan)\\ntest['age'] = test['age'].astype('float64')\\n\\nage_series = test.age.value_counts()\\nplt.figure(figsize=(12,4))\\nsns.barplot(age_series.index.astype('int'), age_series.values, alpha=0.8)\\nplt.ylabel('Number of Occurrences of the customer', fontsize=12)\\nplt.xlabel('Age', fontsize=12)\\nplt.xticks(rotation='vertical')\\nplt.show()\"}, \"vectorWeights\": null}, {\"class\": \"GrandMasterCode\", \"creationTimeUnix\": 1680794643438, \"id\": \"0086e5b6-7f41-436c-b626-e3f2c1bd6dfa\", \"lastUpdateTimeUnix\": 1680794643438, \"properties\": {\"code\": \"# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-notebook\\n\\nbio = pd.read_csv(\\\"../input/biology.csv\\\")\\ntitle = np.array(bio['title'])\\ncontent = np.array(bio['content'])\\ntags = np.array(bio['tags'])\\n\\n# wordcloud for tags #\\ntext = ''\\nfor ind, tag in enumerate(tags):\\n    text = \\\" \\\".join([text, tag])\\ntext = text.strip()\\n\\nwordcloud = WordCloud(background_color='white', width=600, height=300, max_font_size=50, max_words=80).generate(text)\\nwordcloud.recolor(random_state=218)\\nplt.imshow(wordcloud)\\nplt.axis(\\\"off\\\")\\nplt.title(\\\"Wordcloud on 'tags' for biology \\\")\\nplt.show()\\n\\n# wordcloud for title #\\ntext = ''\\nfor ind, tag in enumerate(title):\\n    text = \\\" \\\".join([text, tag])\\ntext = text.strip()\\n\\nstop_words = set(stopwords.words('english') + ['sas', 'ss', 'fas', 'des', 'les', 'ess'])\\nwordcloud = WordCloud(background_color='white', width=600, height=300, stopwords=stop_words, max_font_size=50, max_words=80).generate(text)\\nwordcloud.recolor(random_state=218)\\nplt.imshow(wordcloud)\\nplt.axis(\\\"off\\\")\\nplt.title(\\\"Wordcloud on 'title' for biology \\\")\\nplt.show()\\n\\n### Commenting this out for now as it throws error while rendering and not while running it at the backend ###\\n## wordcloud for content #\\n#text = ''\\n#for ind, tag in enumerate(content):\\n#    text = \\\" \\\".join([text, tag])\\n#text = text.strip()\\n\\n#stop_words = set(stopwords.words('english') + ['rbs', 'sas', 'ss', 'fas', 'des', 'ess', 'les', 'bas', 'poses', 'los', 'ros', 'cs'])\\n#wordcloud = WordCloud(background_color='white', width=600, height=300, stopwords=stop_words, max_font_size=50, max_words=80).generate(text)\\n#wordcloud.recolor(random_state=218)\\n#plt.imshow(wordcloud)\\n#plt.axis(\\\"off\\\")\\n#plt.title(\\\"Wordcloud on 'content' for biology \\\")\\n#plt.show()\"}, \"vectorWeights\": null}, {\"class\": \"GrandMasterCode\", \"creationTimeUnix\": 1680793745190, \"id\": \"009cba7d-ffb1-4d2e-81e0-4aa7455861b4\", \"lastUpdateTimeUnix\": 1680793745190, \"properties\": {\"code\": \"# Reference: https://www.kaggle.com/code/cdeotte/top-solutions-ensemble-0-947\\n\\nsub.open_channels.value_counts()\\nres=40\\nplt.figure(figsize=(20,5))\\nplt.plot(sub.time[::res],sub.open_channels[::res])\\nplt.show()\"}, \"vectorWeights\": null}, {\"class\": \"GrandMasterCode\", \"creationTimeUnix\": 1680792967878, \"id\": \"00c586a2-2426-49c1-be82-e4c3e1d75af4\", \"lastUpdateTimeUnix\": 1680792967878, \"properties\": {\"code\": \"# Reference: https://www.kaggle.com/code/gpreda/siim-isic-melanoma-classification-eda\\n\\ndef show_dicom_images(data):\\n    img_data = list(data.T.to_dict().values())\\n    f, ax = plt.subplots(3,3, figsize=(16,18))\\n    for i,data_row in enumerate(img_data):\\n        patientImage = data_row['image_name']+'.dcm'\\n        imagePath = os.path.join(PATH,\\\"train/\\\",patientImage)\\n        data_row_img_data = dcm.read_file(imagePath)\\n        modality = data_row_img_data.Modality\\n        age = data_row_img_data.PatientAge\\n        sex = data_row_img_data.PatientSex\\n        data_row_img = dcm.dcmread(imagePath)\\n        ax[i//3, i%3].imshow(data_row_img.pixel_array, cmap=plt.cm.gray) \\n        ax[i//3, i%3].axis('off')\\n        ax[i//3, i%3].set_title(f\\\"ID: {data_row['image_name']}\\\\nModality: {modality} Age: {age} Sex: {sex}\\\\nDiagnosis: {data_row['diagnosis']}\\\")\\n    plt.show()\\nshow_dicom_images(train_df[train_df['target']==1].sample(9))\"}, \"vectorWeights\": null}, {\"class\": \"GrandMasterCode\", \"creationTimeUnix\": 1680793161762, \"id\": \"010daf3c-3e4b-424a-afeb-7270470a3e32\", \"lastUpdateTimeUnix\": 1680793161762, \"properties\": {\"code\": \"# Reference: https://www.kaggle.com/code/allunia/molecules-eda\\n\\nmolecules_size = structures.groupby(\\\"molecule_name\\\").atom.size()\\nmean_size = molecules_size.rolling(window=500).mean()\\nplt.figure(figsize=(20,5))\\nplt.plot(molecules_size.values, '+')\\nplt.plot(np.arange(250, len(molecules_size)-249), mean_size.dropna().values, '-')\\nplt.xlabel(\\\"Molecule name number\\\")\\nplt.ylabel(\\\"Number of atoms\\\");\"}, \"vectorWeights\": null}, {\"class\": \"GrandMasterCode\", \"creationTimeUnix\": 1680795111872, \"id\": \"011128ea-d532-4c33-b446-60f3a37a5a3c\", \"lastUpdateTimeUnix\": 1680795111872, \"properties\": {\"code\": \"# Reference: https://www.kaggle.com/code/robikscube/movie-db-box-office-prediction-intro-and-eda\\n\\nimport warnings\\nwarnings.simplefilter(action='ignore', category=FutureWarning)\\n\\nsns.jointplot('budget_log', 'revenue_log', train.loc[train['budget_log'] > 1], kind='reg')\\nplt.show()\"}, \"vectorWeights\": null}, {\"class\": \"GrandMasterCode\", \"creationTimeUnix\": 1680795199294, \"id\": \"014d55e9-84d1-4e6b-a723-3ddc422f5ac7\", \"lastUpdateTimeUnix\": 1680795199294, \"properties\": {\"code\": \"# Reference: https://www.kaggle.com/code/artgor/text-modelling-in-pytorch-v2\\n\\npuncts = [',', '.', '\\\"', ':', ')', '(', '-', '!', '?', '|', ';', \\\"'\\\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\\\\\', '\\u2022',  '~', '@', '\\u00a3', \\n '\\u00b7', '_', '{', '}', '\\u00a9', '^', '\\u00ae', '`',  '<', '\\u2192', '\\u00b0', '\\u20ac', '\\u2122', '\\u203a',  '\\u2665', '\\u2190', '\\u00d7', '\\u00a7', '\\u2033', '\\u2032', '\\u00c2', '\\u2588', '\\u00bd', '\\u00e0', '\\u2026', \\n '\\u201c', '\\u2605', '\\u201d', '\\u2013', '\\u25cf', '\\u00e2', '\\u25ba', '\\u2212', '\\u00a2', '\\u00b2', '\\u00ac', '\\u2591', '\\u00b6', '\\u2191', '\\u00b1', '\\u00bf', '\\u25be', '\\u2550', '\\u00a6', '\\u2551', '\\u2015', '\\u00a5', '\\u2593', '\\u2014', '\\u2039', '\\u2500', \\n '\\u2592', '\\uff1a', '\\u00bc', '\\u2295', '\\u25bc', '\\u25aa', '\\u2020', '\\u25a0', '\\u2019', '\\u2580', '\\u00a8', '\\u2584', '\\u266b', '\\u2606', '\\u00e9', '\\u00af', '\\u2666', '\\u00a4', '\\u25b2', '\\u00e8', '\\u00b8', '\\u00be', '\\u00c3', '\\u22c5', '\\u2018', '\\u221e', \\n '\\u2219', '\\uff09', '\\u2193', '\\u3001', '\\u2502', '\\uff08', '\\u00bb', '\\uff0c', '\\u266a', '\\u2569', '\\u255a', '\\u00b3', '\\u30fb', '\\u2566', '\\u2563', '\\u2554', '\\u2557', '\\u25ac', '\\u2764', '\\u00ef', '\\u00d8', '\\u00b9', '\\u2264', '\\u2021', '\\u221a', ]\\n\\ndef clean_text(x):\\n    x = str(x)\\n    for punct in puncts:\\n        x = x.replace(punct, f' {punct} ')\\n    return x\\n\\ndef clean_numbers(x):\\n    if bool(re.search(r'\\\\d', x)):\\n        x = re.sub('[0-9]{5,}', '#####', x)\\n        x = re.sub('[0-9]{4}', '####', x)\\n        x = re.sub('[0-9]{3}', '###', x)\\n        x = re.sub('[0-9]{2}', '##', x)\\n    return x\\n\\nmispell_dict = {\\\"aren't\\\" : \\\"are not\\\",\\n                \\\"can't\\\" : \\\"cannot\\\",\\n                \\\"couldn't\\\" : \\\"could not\\\",\\n                \\\"didn't\\\" : \\\"did not\\\",\\n                \\\"doesn't\\\" : \\\"does not\\\",\\n                \\\"don't\\\" : \\\"do not\\\",\\n                \\\"hadn't\\\" : \\\"had not\\\",\\n                \\\"hasn't\\\" : \\\"has not\\\",\\n                \\\"haven't\\\" : \\\"have not\\\",\\n                \\\"he'd\\\" : \\\"he would\\\",\\n                \\\"he'll\\\" : \\\"he will\\\",\\n                \\\"he's\\\" : \\\"he is\\\",\\n                \\\"i'd\\\" : \\\"I would\\\",\\n                \\\"i'd\\\" : \\\"I had\\\",\\n                \\\"i'll\\\" : \\\"I will\\\",\\n                \\\"i'm\\\" : \\\"I am\\\",\\n                \\\"isn't\\\" : \\\"is not\\\",\\n                \\\"it's\\\" : \\\"it is\\\",\\n                \\\"it'll\\\":\\\"it will\\\",\\n                \\\"i've\\\" : \\\"I have\\\",\\n                \\\"let's\\\" : \\\"let us\\\",\\n                \\\"mightn't\\\" : \\\"might not\\\",\\n                \\\"mustn't\\\" : \\\"must not\\\",\\n                \\\"shan't\\\" : \\\"shall not\\\",\\n                \\\"she'd\\\" : \\\"she would\\\",\\n                \\\"she'll\\\" : \\\"she will\\\",\\n                \\\"she's\\\" : \\\"she is\\\",\\n                \\\"shouldn't\\\" : \\\"should not\\\",\\n                \\\"that's\\\" : \\\"that is\\\",\\n                \\\"there's\\\" : \\\"there is\\\",\\n                \\\"they'd\\\" : \\\"they would\\\",\\n                \\\"they'll\\\" : \\\"they will\\\",\\n                \\\"they're\\\" : \\\"they are\\\",\\n                \\\"they've\\\" : \\\"they have\\\",\\n                \\\"we'd\\\" : \\\"we would\\\",\\n                \\\"we're\\\" : \\\"we are\\\",\\n                \\\"weren't\\\" : \\\"were not\\\",\\n                \\\"we've\\\" : \\\"we have\\\",\\n                \\\"what'll\\\" : \\\"what will\\\",\\n                \\\"what're\\\" : \\\"what are\\\",\\n                \\\"what's\\\" : \\\"what is\\\",\\n                \\\"what've\\\" : \\\"what have\\\",\\n                \\\"where's\\\" : \\\"where is\\\",\\n                \\\"who'd\\\" : \\\"who would\\\",\\n                \\\"who'll\\\" : \\\"who will\\\",\\n                \\\"who're\\\" : \\\"who are\\\",\\n                \\\"who's\\\" : \\\"who is\\\",\\n                \\\"who've\\\" : \\\"who have\\\",\\n                \\\"won't\\\" : \\\"will not\\\",\\n                \\\"wouldn't\\\" : \\\"would not\\\",\\n                \\\"you'd\\\" : \\\"you would\\\",\\n                \\\"you'll\\\" : \\\"you will\\\",\\n                \\\"you're\\\" : \\\"you are\\\",\\n                \\\"you've\\\" : \\\"you have\\\",\\n                \\\"'re\\\": \\\" are\\\",\\n                \\\"wasn't\\\": \\\"was not\\\",\\n                \\\"we'll\\\":\\\" will\\\",\\n                \\\"didn't\\\": \\\"did not\\\",\\n                \\\"tryin'\\\":\\\"trying\\\",\\n               '\\\\u200b': '',\\n                '\\u2026': '',\\n                '\\\\ufeff': '',\\n                '\\u0915\\u0930\\u0928\\u093e': '',\\n                '\\u0939\\u0948': ''}\\n\\nfor coin in ['Litecoin', 'altcoin', 'altcoins', 'coinbase', 'litecoin', 'Unocoin', 'Dogecoin', 'cryptocoin', 'Altcoins', 'filecoin', 'Altcoin', 'cryptocoins',\\n             'Altacoin', 'Dentacoin', 'Bytecoin', 'Siacoin', 'Onecoin', 'dogecoin', 'unocoin', 'siacoin', 'litecoins', 'Filecoin', 'Buyucoin', 'Litecoins',\\n             'Laxmicoin', 'shtcoins', 'Sweatcoin', 'Skycoin', 'vitrocoin', 'Monacoin', 'Litcoin', 'reddcoin', 'freebitcoin', 'Namecoin', 'plexcoin', 'Onecoins',\\n             'daikicoin', 'Gainbitcoin', 'Gatecoin', 'Plexcoin', 'peercoin', 'coinsecure', 'dogecoins', 'cointries', 'Zcoin', 'genxcoin', 'Frazcoin', 'frazcoin',\\n             'coinify', 'Nagricoin', 'OKcoin', 'Presscoins', 'Dagcoin', 'batcoin', 'Spectrocoin', 'Travelflexcoin', 'ecoin', 'Minexcoin', 'Kashhcoin', 'coinone',\\n             'octacoin', 'coinsides', 'zabercoin', 'ADZcoin', 'cyptocoin', 'bitecoin', 'Bitecoin', 'Emercoin', 'tegcoin', 'flipcoin', 'Gridcoin', 'Facecoin',\\n             'Ravencoins', 'digicoin', 'bitcoincash', 'Vitrocoin', 'Livecoin', 'dashcoin', 'Fedcoin', 'litcoins', 'Webcoin', 'coinspot', 'bitoxycoin', 'peercoins',\\n             'Ucoin', 'ALTcoins', 'coincidece', 'dagcoin', 'Giracoin', 'coincheck', 'Swisscoin', 'butcoin', 'neocoin', 'mintcoin', 'Myriadcoin', 'Viacoin', 'jiocoin',\\n             'Potcoin', 'bibitcoin', 'gainbitcoin', 'altercoins', 'coinburn', 'Kodakcoin', 'Bcoin', 'Kucoin', 'Operacoin', 'Lomocoin', 'dentacoin', 'Nyancoin',\\n             'Jiocoin', 'Indicoin', 'coinsidered', 'Vertcoin', 'Maidsafecoin', 'coindelta', 'coinfirm', 'coinvest', 'bixcoin', 'litcoin', 'Dogecoins', 'Unicoin',\\n             'Rothscoin', 'localbitcoins', 'groestlcoin', 'sibcoin', 'Travelercoin', 'Vericoin', 'bytecoin', 'Bananacoin', 'PACcoin']:\\n    mispell_dict[coin] = 'bitcoin'\\n\\ndef _get_mispell(mispell_dict):\\n    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\\n    return mispell_dict, mispell_re\\n\\nmispellings, mispellings_re = _get_mispell(mispell_dict)\\ndef replace_typical_misspell(text):\\n    def replace(match):\\n        return mispellings[match.group(0)]\\n    return mispellings_re.sub(replace, text)\\n\\n# Clean the text\\ntrain[\\\"question_text\\\"] = train[\\\"question_text\\\"].apply(lambda x: clean_text(x.lower()))\\ntest[\\\"question_text\\\"] = test[\\\"question_text\\\"].apply(lambda x: clean_text(x.lower()))\\n\\n# Clean numbers\\ntrain[\\\"question_text\\\"] = train[\\\"question_text\\\"].apply(lambda x: clean_numbers(x))\\ntest[\\\"question_text\\\"] = test[\\\"question_text\\\"].apply(lambda x: clean_numbers(x))\\n\\n# Clean speelings\\ntrain[\\\"question_text\\\"] = train[\\\"question_text\\\"].apply(lambda x: replace_typical_misspell(x))\\ntest[\\\"question_text\\\"] = test[\\\"question_text\\\"].apply(lambda x: replace_typical_misspell(x))\\nmax_features = 120000\\ntk = Tokenizer(lower = True, filters='', num_words=max_features)\\nfull_text = list(train['question_text'].values) + list(test['question_text'].values)\\ntk.fit_on_texts(full_text)\\ntrain_tokenized = tk.texts_to_sequences(train['question_text'].fillna('missing'))\\ntest_tokenized = tk.texts_to_sequences(test['question_text'].fillna('missing'))\\ntrain['question_text'].apply(lambda x: len(x.split())).plot(kind='hist');\\nplt.yscale('log');\\nplt.title('Distribution of question text length in characters');\"}, \"vectorWeights\": null}, {\"class\": \"GrandMasterCode\", \"creationTimeUnix\": 1680793399316, \"id\": \"01691d53-994a-4c32-a2f8-cc7722ffd4d9\", \"lastUpdateTimeUnix\": 1680793399316, \"properties\": {\"code\": \"# Reference: https://www.kaggle.com/code/allunia/protein-atlas-exploration-and-baseline\\n\\nfeature = \\\"Cytosol\\\"\\nplt.figure(figsize=(20,5))\\nsns.distplot(baseline_proba_predictions[feature].values[0:-10], color=\\\"Purple\\\")\\nplt.xlabel(\\\"Predicted probabilites of {}\\\".format(feature))\\nplt.ylabel(\\\"Density\\\")\\nplt.xlim([0,1])\"}, \"vectorWeights\": null}, {\"class\": \"GrandMasterCode\", \"creationTimeUnix\": 1680795008734, \"id\": \"016b4323-b126-4c47-bc85-d1f33154893d\", \"lastUpdateTimeUnix\": 1680795008734, \"properties\": {\"code\": \"# Reference: https://www.kaggle.com/code/artgor/exploration-of-data-step-by-step\\n\\nsentiment_dict = {}\\nfor filename in os.listdir('../input/train_sentiment/'):\\n    with open('../input/train_sentiment/' + filename, 'r') as f:\\n        sentiment = json.load(f)\\n    pet_id = filename.split('.')[0]\\n    sentiment_dict[pet_id] = {}\\n    sentiment_dict[pet_id]['magnitude'] = sentiment['documentSentiment']['magnitude']\\n    sentiment_dict[pet_id]['score'] = sentiment['documentSentiment']['score']\\n    sentiment_dict[pet_id]['language'] = sentiment['language']\\n\\nfor filename in os.listdir('../input/test_sentiment/'):\\n    with open('../input/test_sentiment/' + filename, 'r') as f:\\n        sentiment = json.load(f)\\n    pet_id = filename.split('.')[0]\\n    sentiment_dict[pet_id] = {}\\n    sentiment_dict[pet_id]['magnitude'] = sentiment['documentSentiment']['magnitude']\\n    sentiment_dict[pet_id]['score'] = sentiment['documentSentiment']['score']\\n    sentiment_dict[pet_id]['language'] = sentiment['language']\\ntrain['lang'] = train['PetID'].apply(lambda x: sentiment_dict[x]['language'] if x in sentiment_dict else 'no')\\ntrain['magnitude'] = train['PetID'].apply(lambda x: sentiment_dict[x]['magnitude'] if x in sentiment_dict else 0)\\ntrain['score'] = train['PetID'].apply(lambda x: sentiment_dict[x]['score'] if x in sentiment_dict else 0)\\n\\ntest['lang'] = test['PetID'].apply(lambda x: sentiment_dict[x]['language'] if x in sentiment_dict else 'no')\\ntest['magnitude'] = test['PetID'].apply(lambda x: sentiment_dict[x]['magnitude'] if x in sentiment_dict else 0)\\ntest['score'] = test['PetID'].apply(lambda x: sentiment_dict[x]['score'] if x in sentiment_dict else 0)\\n\\nall_data['lang'] = all_data['PetID'].apply(lambda x: sentiment_dict[x]['language'] if x in sentiment_dict else 'no')\\nall_data['magnitude'] = all_data['PetID'].apply(lambda x: sentiment_dict[x]['magnitude'] if x in sentiment_dict else 0)\\nall_data['score'] = all_data['PetID'].apply(lambda x: sentiment_dict[x]['score'] if x in sentiment_dict else 0)\\nplot_four_graphs(col='lang', main_title='lang', dataset_title='Number of pets by lang in train and test data')\"}, \"vectorWeights\": null}, {\"class\": \"GrandMasterCode\", \"creationTimeUnix\": 1680793247933, \"id\": \"018866ad-b40f-4c4b-8647-8c8281a6dd41\", \"lastUpdateTimeUnix\": 1680793247933, \"properties\": {\"code\": \"fit_gaussians = False\\nuse_plotly=True\"}, \"vectorWeights\": null}, {\"class\": \"GrandMasterCode\", \"creationTimeUnix\": 1680793702465, \"id\": \"01be989e-c69e-4f89-8264-eb20703bfb9a\", \"lastUpdateTimeUnix\": 1680793702465, \"properties\": {\"code\": \"# Reference: https://www.kaggle.com/code/iafoss/postprocessing-for-hypercolumns-kernel\\n\\nnoise_th = 75.0*255*(sz/128.0)**2 #threshold for the number of predicted pixels\\n\\ngc.collect();\\ntorch.cuda.empty_cache()\\npreds = preds0.clone()\\npreds[preds.view(preds.shape[0],-1).float().sum(-1) < noise_th,...] = 0.0\\n\\ndices = []\\n#here evrything is multiplied by 255, so 0.2 threshold corresponds to 51\\nthrs = np.arange(40, 60, 1)\\nfor th in progress_bar(thrs):\\n    preds_m = (preds>th).byte()\\n    dices.append(dice_overall(preds_m, ys).mean())\\ndices = np.array(dices)    \\n\\nbest_dice = dices.max()\\nbest_thr = thrs[dices.argmax()]\\n\\nplt.figure(figsize=(8,4))\\nplt.plot(thrs, dices)\\nplt.vlines(x=best_thr, ymin=dices.min(), ymax=dices.max())\\nplt.text(best_thr, best_dice, f'DICE = {best_dice:.3f}', fontsize=14);\\nplt.show()\"}, \"vectorWeights\": null}, {\"class\": \"GrandMasterCode\", \"creationTimeUnix\": 1680793356344, \"id\": \"01c3a6fc-780e-4251-83bc-666819ac548d\", \"lastUpdateTimeUnix\": 1680793356344, \"properties\": {\"code\": \"# Reference: https://www.kaggle.com/code/gpreda/santander-value-prediction-extensive-eda\\n\\nnon_zeros = (train_df.ne(0).sum(axis=1))\\n\\nplt.figure(figsize=(10,6))\\nplt.title(\\\"Distribution of log(number of non-zeros per row) - train set\\\")\\nsns.distplot(np.log1p(non_zeros),color=\\\"red\\\", kde=True,bins=100)\\nplt.show()\\n\"}, \"vectorWeights\": null}, {\"class\": \"GrandMasterCode\", \"creationTimeUnix\": 1680793507193, \"id\": \"01c50350-331a-4ceb-9a16-892130522c23\", \"lastUpdateTimeUnix\": 1680793507193, \"properties\": {\"code\": \"# Reference: https://www.kaggle.com/code/robikscube/rob-s-covid19-week-2-submission-less-aggressive\\n\\ndat.iloc[-10:]\\nfor myplace in ['Iran']:\\n\\n    # Confirmed Cases\\n    fig, axs = plt.subplots(1, 2, figsize=(15, 3))\\n    dat = tt.query('Place == @myplace and Days_Since_Ten_Cases >= 0')[['Days_Since_Ten_Cases','ConfirmedCases']].dropna()\\n    dat = dat.iloc[-10:]\\n    X = dat['Days_Since_Ten_Cases']\\n    y = dat['ConfirmedCases']\\n    y = y.cummax()\\n    dat_all = tt.query('Place == @myplace and Days_Since_Ten_Cases >= 0')[['Days_Since_Ten_Cases','ConfirmedCases']]\\n    X_pred = dat_all['Days_Since_Ten_Cases']\\n    en = ElasticNet()\\n    en.fit(X.values.reshape(-1, 1), y.values)\\n    preds = en.predict(X_pred.values.reshape(-1, 1))\\n    tt.loc[(tt['Place'] == myplace) & (tt['Days_Since_Ten_Cases'] >= 0), 'ConfirmedCases_Pred1'] = preds\\n    # Cap at 10 % Population\\n    pop_myplace = tt.query('Place == @myplace')['Population_final'].values[0]\\n    tt.loc[(tt['Place'] == myplace) & (tt['ConfirmedCases_Pred1'] > (0.1 * pop_myplace)), 'ConfirmedCases_Pred1'] = (0.1 * pop_myplace)\\n    ax = tt.query('Place == @myplace').set_index('Date')[['ConfirmedCases','ConfirmedCases_Pred1']].plot(figsize=(15, 5), title=myplace, ax=axs[0])\\n    # Fatalities\\n    # If low count then do percent of confirmed:\\n    dat = tt.query('Place == @myplace and Days_Since_Ten_Cases >= 0')[['Days_Since_Ten_Cases','Fatalities']].dropna()\\n    dat = dat.iloc[-10:]\\n    if len(dat) < 5:\\n        tt.loc[(tt['Place'] == myplace), 'Fatalities_Pred1'] = tt.loc[(tt['Place'] == myplace)]['ConfirmedCases_Pred1'] * 0.001\\n    elif tt.query('Place == @myplace')['Fatalities'].max() < 5:\\n        tt.loc[(tt['Place'] == myplace), 'Fatalities_Pred1'] = tt.loc[(tt['Place'] == myplace)]['ConfirmedCases_Pred1'] * 0.001\\n    else:\\n        X = dat['Days_Since_Ten_Cases']\\n        y = dat['Fatalities']\\n        y = y.cummax()\\n        dat_all = tt.query('Place == @myplace and Days_Since_Ten_Cases >= 0')[['Days_Since_Ten_Cases','Fatalities']]\\n        X_pred = dat_all['Days_Since_Ten_Cases']\\n        en = ElasticNet()\\n        en.fit(X.values.reshape(-1, 1), y.values)\\n        preds = en.predict(X_pred.values.reshape(-1, 1))\\n        tt.loc[(tt['Place'] == myplace) & (tt['Days_Since_Ten_Cases'] >= 0), 'Fatalities_Pred1'] = preds\\n\\n        # Cap at 0.0001 Population\\n        pop_myplace = tt.query('Place == @myplace')['Population_final'].values[0]\\n        tt.loc[(tt['Place'] == myplace) & (tt['Fatalities_Pred1'] > (0.0005 * pop_myplace)), 'Fatalities_Pred1'] = (0.0005 * pop_myplace)\\n\\n    ax = tt.query('Place == @myplace').set_index('Date')[['Fatalities','Fatalities_Pred1']].plot(figsize=(15, 5), title=myplace, ax=axs[1])\\n    plt.show()\\n# Clean Up any time the actual is less than the real\\ntt['ConfirmedCases_Pred1'] = tt[['ConfirmedCases','ConfirmedCases_Pred1']].max(axis=1)\\ntt['Fatalities_Pred1'] = tt[['Fatalities','Fatalities_Pred1']].max(axis=1)\\n\\ntt['ConfirmedCases_Pred1'] = tt['ConfirmedCases_Pred1'].fillna(0)\\ntt['Fatalities_Pred1'] = tt['Fatalities_Pred1'].fillna(0)\\n\\n# Fill pred with\\ntt.loc[~tt['ConfirmedCases'].isna(), 'ConfirmedCases_Pred1'] = tt.loc[~tt['ConfirmedCases'].isna()]['ConfirmedCases']\\ntt.loc[~tt['Fatalities'].isna(), 'Fatalities_Pred1'] = tt.loc[~tt['Fatalities'].isna()]['Fatalities']\\n\\ntt['ConfirmedCases_Pred1'] = tt.groupby('Place')['ConfirmedCases_Pred1'].transform('cummax')\\ntt['Fatalities_Pred1'] = tt.groupby('Place')['Fatalities_Pred1'].transform('cummax')\\n# Questionable numbers\\ntt.query('Place == \\\"Iran\\\"').set_index('Date')[['ConfirmedCases',\\n                                               'ConfirmedCases_Pred1',]].plot(figsize=(15, 5))\\n# Make Iran's Predictions Linear\\n\\ntt.query('Place == \\\"Iran\\\"').set_index('Date')[['Fatalities','Fatalities_Pred1']].plot(figsize=(15, 5))\"}, \"vectorWeights\": null}, {\"class\": \"GrandMasterCode\", \"creationTimeUnix\": 1680792989710, \"id\": \"01c64396-5dcb-45b4-84ff-5c4fbf898918\", \"lastUpdateTimeUnix\": 1680792989710, \"properties\": {\"code\": \"# Reference: https://www.kaggle.com/code/tunguz/cats-and-dogs-with-rapids-t-sne\\n\\ntrain = np.load('../input/cats-and-dogs-embedded-data/cats_and_dogs_1/train_ResNet50.npy')\\n%%time\\ntsne = TSNE(n_components=2)\\ntrain_2D = tsne.fit_transform(train)\\nplt.scatter(train_2D[:,0], train_2D[:,1], c = target, s = 0.5)\"}, \"vectorWeights\": null}, {\"class\": \"GrandMasterCode\", \"creationTimeUnix\": 1680794046360, \"id\": \"01ea6cf7-613f-4c7d-b1b3-37f9fe90b8af\", \"lastUpdateTimeUnix\": 1680794046360, \"properties\": {\"code\": \"# Reference: https://www.kaggle.com/code/kabure/extensive-eda-of-deal-probability\\n\\ndf_train['count_word_log'] = np.log(df_train['count_word'])\\n\\n(sns\\n  .FacetGrid(df_train, \\n             hue='deal_prob_cat', \\n             size=5, aspect=2)\\n  .map(sns.kdeplot, 'count_word_log', shade=True)\\n .add_legend()\\n)\\nplt.show()\"}, \"vectorWeights\": null}, {\"class\": \"GrandMasterCode\", \"creationTimeUnix\": 1680794282392, \"id\": \"02161632-f0b3-455f-a04d-9d1d5dd509da\", \"lastUpdateTimeUnix\": 1680794282392, \"properties\": {\"code\": \"# Reference: https://www.kaggle.com/code/allunia/don-t-turn-into-a-smoothie-after-the-shake-up\\n\\npatient_gender_train = train_info.groupby(\\\"patient_id\\\").sex.unique().apply(lambda l: l[0])\\npatient_gender_test = test_info.groupby(\\\"patient_id\\\").sex.unique().apply(lambda l: l[0])\\n\\ntrain_patients = pd.DataFrame(index=patient_gender_train.index.values, data=patient_gender_train.values, columns=[\\\"sex\\\"])\\ntest_patients = pd.DataFrame(index=patient_gender_test.index.values, data=patient_gender_test.values, columns=[\\\"sex\\\"])\\n\\ntrain_patients.loc[:, \\\"num_images\\\"] = train_info.groupby(\\\"patient_id\\\").size()\\ntest_patients.loc[:, \\\"num_images\\\"] = test_info.groupby(\\\"patient_id\\\").size()\\n\\ntrain_patients.loc[:, \\\"min_age\\\"] = train_info.groupby(\\\"patient_id\\\").age_approx.min()\\ntrain_patients.loc[:, \\\"max_age\\\"] = train_info.groupby(\\\"patient_id\\\").age_approx.max()\\ntest_patients.loc[:, \\\"min_age\\\"] = test_info.groupby(\\\"patient_id\\\").age_approx.min()\\ntest_patients.loc[:, \\\"max_age\\\"] = test_info.groupby(\\\"patient_id\\\").age_approx.max()\\n\\ntrain_patients.loc[:, \\\"age_span\\\"] = train_patients[\\\"max_age\\\"] - train_patients[\\\"min_age\\\"]\\ntest_patients.loc[:, \\\"age_span\\\"] = test_patients[\\\"max_age\\\"] - test_patients[\\\"min_age\\\"]\\n\\ntrain_patients.loc[:, \\\"benign_cases\\\"] = train_info.groupby([\\\"patient_id\\\", \\\"benign_malignant\\\"]).size().loc[:, \\\"benign\\\"]\\ntrain_patients.loc[:, \\\"malignant_cases\\\"] = train_info.groupby([\\\"patient_id\\\", \\\"benign_malignant\\\"]).size().loc[:, \\\"malignant\\\"]\\ntrain_patients[\\\"min_age_malignant\\\"] = train_info.groupby([\\\"patient_id\\\", \\\"benign_malignant\\\"]).age_approx.min().loc[:, \\\"malignant\\\"]\\ntrain_patients[\\\"max_age_malignant\\\"] = train_info.groupby([\\\"patient_id\\\", \\\"benign_malignant\\\"]).age_approx.max().loc[:, \\\"malignant\\\"]\\ntrain_patients.sort_values(by=\\\"malignant_cases\\\", ascending=False).head()\\nfig, ax = plt.subplots(2,2,figsize=(20,12))\\nsns.countplot(train_patients.sex, ax=ax[0,0], palette=\\\"Reds\\\")\\nax[0,0].set_title(\\\"Gender counts with unique patient ids in train\\\")\\nsns.countplot(test_patients.sex, ax=ax[0,1], palette=\\\"Blues\\\");\\nax[0,1].set_title(\\\"Gender counts with unique patient ids in test\\\");\\n\\ntrain_age_span_perc = train_patients.age_span.value_counts() / train_patients.shape[0] * 100\\ntest_age_span_perc = test_patients.age_span.value_counts() / test_patients.shape[0] * 100\\n\\nsns.barplot(train_age_span_perc.index, train_age_span_perc.values, ax=ax[1,0], color=\\\"Orangered\\\");\\nsns.barplot(test_age_span_perc.index, test_age_span_perc.values, ax=ax[1,1], color=\\\"Lightseagreen\\\");\\nax[1,0].set_title(\\\"Patients age span in train\\\")\\nax[1,1].set_title(\\\"Patients age span in test\\\")\\nfor n in range(2):\\n    ax[1,n].set_ylabel(\\\"% in data\\\")\\n    ax[1,n].set_xlabel(\\\"age span\\\");\"}, \"vectorWeights\": null}, {\"class\": \"GrandMasterCode\", \"creationTimeUnix\": 1680795349425, \"id\": \"0224f746-0a17-4eaa-9c5b-b7377492adb7\", \"lastUpdateTimeUnix\": 1680795349425, \"properties\": {\"code\": \"# Reference: https://www.kaggle.com/code/artgor/eda-feature-engineering-and-xgb-lgb\\n\\nfig, ax1 = plt.subplots(figsize=(16, 8))\\nplt.title(\\\"Project count and approval rate by day of week.\\\")\\nsns.countplot(x='weekday', data=train, ax=ax1)\\nax1.set_ylabel('Projects count', color='b')\\nplt.legend(['Projects count'])\\nax2 = ax1.twinx()\\nsns.pointplot(x=\\\"weekday\\\", y=\\\"project_is_approved\\\", data=train, ci=99, ax=ax2, color='black')\\nax2.set_ylabel('Approval rate', color='g')\\nplt.legend(['Approval rate'], loc=(0.875, 0.9))\\nplt.grid(False)\"}, \"vectorWeights\": null}, {\"class\": \"GrandMasterCode\", \"creationTimeUnix\": 1680794707574, \"id\": \"022e09e8-c2e2-47d1-8829-ba0daad5fbc1\", \"lastUpdateTimeUnix\": 1680794707574, \"properties\": {\"code\": \"# Reference: https://www.kaggle.com/code/mobassir/jigsaw-google-q-a-eda\\n\\navg_word_len = train['answer'].apply(lambda x: 1.0*len(''.join(x.split()))/(len(x) - len(''.join(x.split())) + 1))\\ntrain['avg_word_len'] = avg_word_len\\navg_word_len = train.loc[train['avg_word_len']<10]['avg_word_len']\\nsns.distplot(avg_word_len, color='g')\\nplt.show()\"}, \"vectorWeights\": null}, {\"class\": \"GrandMasterCode\", \"creationTimeUnix\": 1680793507193, \"id\": \"023eb31e-e47d-42a9-9c10-a3bd7c2b3d70\", \"lastUpdateTimeUnix\": 1680793507193, \"properties\": {\"code\": \"import numpy as np # linear algebra\\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\\nimport matplotlib.pylab as plt\\nimport seaborn as sns\\nimport os, gc, pickle, copy, datetime, warnings\\nimport pycountry\\n\\npd.set_option('max_columns', 500)\\npd.set_option('max_rows', 500)\\npd.options.display.float_format = '{:.2f}'.format\\n!ls -l ../input/covid19-global-forecasting-week-2/\"}, \"vectorWeights\": null}, {\"class\": \"GrandMasterCode\", \"creationTimeUnix\": 1680794814657, \"id\": \"0260d95e-b97d-477b-9f27-bd6de84d0792\", \"lastUpdateTimeUnix\": 1680794814657, \"properties\": {\"code\": \"# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-notebook-instacart\\n\\norder_products_train_df = pd.merge(order_products_train_df, orders_df, on='order_id', how='left')\\ngrouped_df = order_products_train_df.groupby([\\\"order_dow\\\"])[\\\"reordered\\\"].aggregate(\\\"mean\\\").reset_index()\\n\\nplt.figure(figsize=(12,8))\\nsns.barplot(grouped_df['order_dow'].values, grouped_df['reordered'].values, alpha=0.8, color=color[3])\\nplt.ylabel('Reorder ratio', fontsize=12)\\nplt.xlabel('Day of week', fontsize=12)\\nplt.title(\\\"Reorder ratio across day of week\\\", fontsize=15)\\nplt.xticks(rotation='vertical')\\nplt.ylim(0.5, 0.7)\\nplt.show()\\ngrouped_df = order_products_train_df.groupby([\\\"order_hour_of_day\\\"])[\\\"reordered\\\"].aggregate(\\\"mean\\\").reset_index()\\n\\nplt.figure(figsize=(12,8))\\nsns.barplot(grouped_df['order_hour_of_day'].values, grouped_df['reordered'].values, alpha=0.8, color=color[4])\\nplt.ylabel('Reorder ratio', fontsize=12)\\nplt.xlabel('Hour of day', fontsize=12)\\nplt.title(\\\"Reorder ratio across hour of day\\\", fontsize=15)\\nplt.xticks(rotation='vertical')\\nplt.ylim(0.5, 0.7)\\nplt.show()\\n\\ngrouped_df = order_products_train_df.groupby([\\\"order_dow\\\", \\\"order_hour_of_day\\\"])[\\\"reordered\\\"].aggregate(\\\"mean\\\").reset_index()\\ngrouped_df = grouped_df.pivot('order_dow', 'order_hour_of_day', 'reordered')\\n\\nplt.figure(figsize=(12,6))\\nsns.heatmap(grouped_df)\\nplt.title(\\\"Reorder ratio of Day of week Vs Hour of day\\\")\\nplt.show()\"}, \"vectorWeights\": null}, {\"class\": \"GrandMasterCode\", \"creationTimeUnix\": 1680795455959, \"id\": \"0287b90a-4c94-4d74-a102-91a80bd75877\", \"lastUpdateTimeUnix\": 1680795455959, \"properties\": {\"code\": \"# Reference: https://www.kaggle.com/code/robikscube/big-data-bowl-comprehensive-eda-with-pandas\\n\\ntrain.groupby('PlayId').first()['Yards'] \\\\\\n    .plot(kind='hist',\\n          figsize=(15, 5),\\n          bins=50,\\n          title='Distribution of Yards Gained (Target)')\\nplt.show()\"}, \"vectorWeights\": null}, {\"class\": \"GrandMasterCode\", \"creationTimeUnix\": 1680793226280, \"id\": \"029ae5cc-e289-4b52-9be9-d5d273321e64\", \"lastUpdateTimeUnix\": 1680793226280, \"properties\": {\"code\": \"# Reference: https://www.kaggle.com/code/shivamb/imaterialist-fashion-eda-object-detection-colors\\n\\ntrain_labels = Counter(train_annotations)\\n\\nxvalues = list(train_labels.keys())\\nyvalues = list(train_labels.values())\\n\\ntrace1 = go.Bar(x=xvalues, y=yvalues, opacity=0.8, name=\\\"year count\\\", marker=dict(color='rgba(20, 20, 20, 1)'))\\nlayout = dict(width=800, title='Distribution of different labels in the train dataset', legend=dict(orientation=\\\"h\\\"));\\n\\nfig = go.Figure(data=[trace1], layout=layout);\\niplot(fig);\\nvalid_labels = Counter(valid_annotations)\\n\\nxvalues = list(valid_labels.keys())\\nyvalues = list(valid_labels.values())\\n\\ntrace1 = go.Bar(x=xvalues, y=yvalues, opacity=0.8, name=\\\"year count\\\", marker=dict(color='rgba(20, 20, 20, 1)'))\\nlayout = dict(width=800, title='Distribution of different labels in the valid dataset', legend=dict(orientation=\\\"h\\\"));\\n\\nfig = go.Figure(data=[trace1], layout=layout);\\niplot(fig);\\ndef get_images_for_labels(labellist, data):\\n    image_ids = []\\n    for each in data['annotations']:\\n        if all(x in each['labelId'] for x in labellist):\\n            image_ids.append(each['imageId'])\\n            if len(image_ids) == 2:\\n                break\\n    image_urls = []\\n    for each in data['images']:\\n        if each['imageId'] in image_ids:\\n            image_urls.append(each['url'])\\n    return image_urls\\n# most common labels \\n\\ntemps = train_labels.most_common(10)\\nlabels_tr = [\\\"Label-\\\"+str(x[0]) for x in temps]\\nvalues = [x[1] for x in temps]\\n\\ntrace1 = go.Bar(x=labels_tr, y=values, opacity=0.7, name=\\\"year count\\\", marker=dict(color='rgba(120, 120, 120, 0.8)'))\\nlayout = dict(height=400, title='Top 10 Labels in the train dataset', legend=dict(orientation=\\\"h\\\"));\\n\\nfig = go.Figure(data=[trace1], layout=layout);\\niplot(fig);\"}, \"vectorWeights\": null}], \"totalResults\": 25}\n"
     ]
    }
   ],
   "source": [
    "import weaviate\n",
    "import json\n",
    "\n",
    "\n",
    "weaviate_client = weaviate.Client(\"http://202.151.177.149:81\")  # Replace with your endpoint\n",
    "some_objects = weaviate_client.data_object.get()\n",
    "if (json.dumps(some_objects)):\n",
    "    print(True)\n",
    "else:\n",
    "    print(False)\n",
    "\n",
    "print(json.dumps(some_objects))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Elasticsearch Instance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'node40937-typhon-server.th1.proen.cloud', 'cluster_name': 'typhon-ir-cluster', 'cluster_uuid': 'VE76ILFVQhuoZlkd6KnPbw', 'version': {'number': '8.6.0', 'build_flavor': 'default', 'build_type': 'docker', 'build_hash': 'f67ef2df40237445caa70e2fef79471cc608d70d', 'build_date': '2023-01-04T09:35:21.782467981Z', 'build_snapshot': False, 'lucene_version': '9.4.2', 'minimum_wire_compatibility_version': '7.17.0', 'minimum_index_compatibility_version': '7.0.0'}, 'tagline': 'You Know, for Search'}\n"
     ]
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from json import loads\n",
    "\n",
    "elastic_client = Elasticsearch(\"http://202.151.177.154:9200\")\n",
    "\n",
    "response = str(elastic_client.info())\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Obj\n",
    "# {\n",
    "#     \"markdown\": [Array<Str>],\n",
    "#     \"processed\": [Array<Str>],\n",
    "#     \"code\": Str\n",
    "# }\n",
    "\n",
    "# Example Weaviate Classname\n",
    "# 1. GrandMasterCode\n",
    "# 2. MasterCode\n",
    "# 3. ExpertCode\n",
    "\n",
    "# ElasticSearch Query Example\n",
    "# baseQuery = {\n",
    "#     \"query\": {\n",
    "#         \"match\": {\n",
    "#             \"markdown\": {\n",
    "#                 \"query\": markdown_desc\n",
    "#             }\n",
    "#         }\n",
    "#     }\n",
    "# }\n",
    "# processedQuery = {\n",
    "#     \"query\": {\n",
    "#         \"match\": {\n",
    "#             \"processed\": {\n",
    "#                 \"query\": markdown_desc\n",
    "#             }\n",
    "#         }\n",
    "#     }\n",
    "# }\n",
    "\n",
    "def getMLRecommendation(text: str) -> dict:\n",
    "    # md_text = obj['markdown']\n",
    "    cur_class = \"grandmaster\"\n",
    "    near_text = {\"concepts\": [text]}\n",
    "    fetched = (weaviate_client.query\n",
    "                      .get(CLASS_NAME[cur_class], [\"code\"])\n",
    "                      .with_near_text(near_text)\n",
    "                      .with_limit(1)\n",
    "                      .do()\n",
    "                      )\n",
    "    data = fetched['data']['Get'][CLASS_NAME[cur_class]]\n",
    "    return data[0]\n",
    "\n",
    "def getElasticQuery(query, type=[\"base\", \"processed\"]):\n",
    "    if (type == \"processed\"):\n",
    "        result = {\n",
    "        \"query\": {\n",
    "            \"match\": {\n",
    "                \"processed\": {\n",
    "                    \"query\": query\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    else:\n",
    "        result = {\n",
    "        \"query\": {\n",
    "            \"match\": {\n",
    "                \"markdown\": {\n",
    "                    \"query\": query\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    return result \n",
    "\n",
    "def getElasticRecommendation(index, queryBody):\n",
    "    response = elastic_client.search(index=index, body=queryBody)\n",
    "\n",
    "    if response and response[\"hits\"][\"hits\"]:\n",
    "        result = response[\"hits\"][\"hits\"][0][\"_source\"]\n",
    "        return result['code']\n",
    "    return []\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"grandmaster_nl_pl_only_plot.json\") as f:\n",
    "    grand_master_json = loads(f.read())\n",
    "\n",
    "# markdown_desc = \"create a scatter plot\"\n",
    "i = 1\n",
    "\n",
    "for pair in grand_master_json:\n",
    "    markdown_desc = \"\".join(pair[\"markdown\"])\n",
    "\n",
    "\n",
    "    response = elastic_client.search(index=\"grandmaster\", body=baseQuery)\n",
    "\n",
    "    if response and response[\"hits\"][\"hits\"]:\n",
    "        a = response[\"hits\"][\"hits\"][0]\n",
    "        # print(json.dumps(a, indent=4))\n",
    "        res = response[\"hits\"][\"hits\"][0][\"_source\"]\n",
    "\n",
    "    print(\"<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\")\n",
    "    print(json.dumps(res, indent=4))\n",
    "\n",
    "    i = i + 1\n",
    "    if (i == 10):\n",
    "        i = 1\n",
    "        break\n",
    "# print(\"Hello World\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_accumulate = []\n",
    "# i = 0\n",
    "for file_name in FILES:\n",
    "    with open(file_name, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "        data_length = len(data)\n",
    "        data_count = 1\n",
    "\n",
    "        for row in data:\n",
    "            rows_length = len(row['markdown'])\n",
    "            row_count = 1\n",
    "\n",
    "            for markdown in row['markdown']:\n",
    "                temp_result = {\n",
    "                    \"original_md\": markdown,\n",
    "                    \"original_code\": row['code'],\n",
    "                    \"count\": f'{data_count}-{row_count}/{rows_length}',\n",
    "                    \"data_rank\": file_name[:-21],\n",
    "                }\n",
    "                \n",
    "                # Get ML Recommendation\n",
    "                recommended_code_ml = getMLRecommendation(markdown)\n",
    "                temp_result[\"recommended_code\"] = recommended_code_ml['code']\n",
    "                temp_result[\"model\"] = \"machine-learning\"\n",
    "\n",
    "                # Get Elastic Recommendation\n",
    "                query_base = getElasticQuery(markdown)\n",
    "                query_base = getElasticQuery(markdown, \"processed\")\n",
    "                recommended_code_es = getElasticRecommendation()\n",
    "\n",
    "                # Append to the collection\n",
    "                testing_accumulate.append(temp_result)\n",
    "\n",
    "                print(f'Row Count: {row_count}/{rows_length}')\n",
    "                row_count = row_count + 1\n",
    "\n",
    "            print(f'Data Count: {data_count}/{data_length}')\n",
    "            data_count = data_count + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"original_md\": \"# Acknowledgements\\n\\n1. [M5 Forecasting - Starter Data Exploration](https://www.kaggle.com/robikscube/m5-forecasting-starter-data-exploration) ~ by Rob Mulla\\n2. [EDA and Baseline Model](https://www.kaggle.com/rdizzl3/eda-and-baseline-model) ~ by RDizzl3\\n3. [How to Create an ARIMA Model for Time Series Forecasting in Python](https://machinelearningmastery.com/arima-for-time-series-forecasting-with-python/) ~ by Machine Learning Mastery\\n4. [7 methods to perform Time Series forecasting (with Python codes)](https://www.analyticsvidhya.com/blog/2018/02/time-series-forecasting-methods/) ~ by Analytics Vidhya\\n5. [Economics for the IB Diploma](https://www.cambridge.org/core/books/economics-for-the-ib-diploma/1918CF16A8FC979AAB19951A487DCB1C) ~ by Ellie Tragakes\\n6. [Prophet](https://facebook.github.io/prophet/) ~ by Facebook\",\n",
      "        \"recommended_code\": \"import numpy as np\\nimport pandas as pd\\nimport matplotlib.pylab as plt\\nimport cv2\\nimport seaborn as sns\\nfrom sklearn.metrics import log_loss\\nXCEPTION_MODEL = '../input/deepfakemodelspackages/xception-b5690688.pth'\\n%%time\\n# Install packages\\n!pip install ../input/deepfakemodelspackages/Pillow-6.2.1-cp36-cp36m-manylinux1_x86_64.whl -f ./ --no-index\\n!pip install ../input/deepfakemodelspackages/munch-2.5.0-py2.py3-none-any.whl -f ./ --no-index\\n!pip install ../input/deepfakemodelspackages/numpy-1.17.4-cp36-cp36m-manylinux1_x86_64.whl -f ./ --no-index\\n!pip install ../input/deepfakemodelspackages/pretrainedmodels-0.7.4/pretrainedmodels-0.7.4/ -f ./ --no-index\\n!pip install ../input/deepfakemodelspackages/six-1.13.0-py2.py3-none-any.whl -f ./ --no-index\\n!pip install ../input/deepfakemodelspackages/torchvision-0.4.2-cp36-cp36m-manylinux1_x86_64.whl -f ./ --no-index\\n!pip install ../input/deepfakemodelspackages/tqdm-4.40.2-py2.py3-none-any.whl -f ./ --no-index\",\n",
      "        \"original_code\": \"# Reference: https://www.kaggle.com/code/tarunpaparaju/m5-competition-eda-models\\n\\nimport os\\nimport gc\\nimport time\\nimport math\\nimport datetime\\nfrom math import log, floor\\nfrom sklearn.neighbors import KDTree\\n\\nimport numpy as np\\nimport pandas as pd\\nfrom pathlib import Path\\nfrom sklearn.utils import shuffle\\nfrom tqdm.notebook import tqdm as tqdm\\n\\nimport seaborn as sns\\nfrom matplotlib import colors\\nimport matplotlib.pyplot as plt\\nfrom matplotlib.colors import Normalize\\n\\nimport plotly.express as px\\nimport plotly.graph_objects as go\\nimport plotly.figure_factory as ff\\nfrom plotly.subplots import make_subplots\\n\\nimport pywt\\nfrom statsmodels.robust import mad\\n\\nimport scipy\\nimport statsmodels\\nfrom scipy import signal\\nimport statsmodels.api as sm\\nfrom fbprophet import Prophet\\nfrom scipy.signal import butter, deconvolve\\nfrom statsmodels.tsa.arima_model import ARIMA\\nfrom statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing, Holt\\n\\nimport warnings\\nwarnings.filterwarnings(\\\"ignore\\\")\",\n",
      "        \"count\": \"1-1\",\n",
      "        \"rank\": \"grandmaster\"\n",
      "    },\n",
      "    {\n",
      "        \"original_md\": \"# Contents\\n\\n* [<font size=4>The dataset</font>](#1)\\n\\n\\n* [<font size=4>EDA</font>](#2)\\n    * [Preparing the ground](#2.1)\\n    * [Sales data](#2.2)\\n    * [Denoising](#2.3)\\n    * [Stores and sales](#2.4)\\n\\n    \\n* [<font size=4>Modeling</font>](#3)\\n    * [Train/Val split](#3.1)\\n    * [Naive approach](#3.2)\\n    * [Moving average](#3.3)\\n    * [Holt linear](#3.4)\\n    * [Exponential smoothing](#3.5)\\n    * [ARIMA](#3.6)\\n    * [Prophet](#3.7)\\n    * [Loss for each model](#3.8)\\n\\n\\n* [<font size=4>Takeaways</font>](#4)\\n\\n\\n* [<font size=4>Ending Note</font>](#5)\",\n",
      "        \"recommended_code\": \"!pip install fastai==0.7.0 --no-deps\\n!pip install torch==0.4.1 torchvision==0.2.1\\nfrom fastai.conv_learner import *\\nfrom fastai.dataset import *\\n\\nimport pandas as pd\\nimport numpy as np\\nimport os\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.metrics import f1_score\\nimport scipy.optimize as opt\\nimport matplotlib.pyplot as plt\\nPATH = './'\\nTRAIN = '../input/human-protein-atlas-image-classification/train/'\\nTEST = '../input/human-protein-atlas-image-classification/test/'\\nLABELS = '../input/human-protein-atlas-image-classification/train.csv'\\nSPLIT = '../input/protein-trainval-split/'\\nnw = 2   #number of workers for data loader\\nname_label_dict = {\\n0:  'Nucleoplasm',\\n1:  'Nuclear membrane',\\n2:  'Nucleoli',   \\n3:  'Nucleoli fibrillar center',\\n4:  'Nuclear speckles',\\n5:  'Nuclear bodies',\\n6:  'Endoplasmic reticulum',   \\n7:  'Golgi apparatus',\\n8:  'Peroxisomes',\\n9:  'Endosomes',\\n10:  'Lysosomes',\\n11:  'Intermediate filaments',\\n12:  'Actin filaments',\\n13:  'Focal adhesion sites',   \\n14:  'Microtubules',\\n15:  'Microtubule ends',  \\n16:  'Cytokinetic bridge',   \\n17:  'Mitotic spindle',\\n18:  'Microtubule organizing center',  \\n19:  'Centrosome',\\n20:  'Lipid droplets',\\n21:  'Plasma membrane',   \\n22:  'Cell junctions', \\n23:  'Mitochondria',\\n24:  'Aggresome',\\n25:  'Cytosol',\\n26:  'Cytoplasmic bodies',   \\n27:  'Rods & rings' }\",\n",
      "        \"original_code\": \"# Reference: https://www.kaggle.com/code/tarunpaparaju/m5-competition-eda-models\\n\\nimport os\\nimport gc\\nimport time\\nimport math\\nimport datetime\\nfrom math import log, floor\\nfrom sklearn.neighbors import KDTree\\n\\nimport numpy as np\\nimport pandas as pd\\nfrom pathlib import Path\\nfrom sklearn.utils import shuffle\\nfrom tqdm.notebook import tqdm as tqdm\\n\\nimport seaborn as sns\\nfrom matplotlib import colors\\nimport matplotlib.pyplot as plt\\nfrom matplotlib.colors import Normalize\\n\\nimport plotly.express as px\\nimport plotly.graph_objects as go\\nimport plotly.figure_factory as ff\\nfrom plotly.subplots import make_subplots\\n\\nimport pywt\\nfrom statsmodels.robust import mad\\n\\nimport scipy\\nimport statsmodels\\nfrom scipy import signal\\nimport statsmodels.api as sm\\nfrom fbprophet import Prophet\\nfrom scipy.signal import butter, deconvolve\\nfrom statsmodels.tsa.arima_model import ARIMA\\nfrom statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing, Holt\\n\\nimport warnings\\nwarnings.filterwarnings(\\\"ignore\\\")\",\n",
      "        \"count\": \"1-2\",\n",
      "        \"rank\": \"grandmaster\"\n",
      "    },\n",
      "    {\n",
      "        \"original_md\": \"# The dataset <a id=\\\"1\\\"></a>\\n\\nThe dataset consists of five .csv files.\\n\\n* <code>calendar.csv</code> - Contains the dates on which products are sold. The dates are in a <code>yyyy/dd/mm</code> format.\\n\\n* <code>sales_train_validation.csv</code> - Contains the historical daily unit sales data per product and store <code>[d_1 - d_1913]</code>.\\n\\n* <code>submission.csv</code> - Demonstrates the correct format for submission to the competition.\\n\\n* <code>sell_prices.csv</code> - Contains information about the price of the products sold per store and date.\\n\\n* <code>sales_train_evaluation.csv</code> - Available one month before the competition deadline. It will include sales for <code>[d_1 - d_1941]</code>.\\n\\nIn this competition, we need to forecast the sales for <code>[d_1942 - d_1969]</code>. These rows form the evaluation set. The rows <code>[d_1914 - d_1941]</code> form the validation set, and the remaining rows form the training set. Now, since we understand the dataset and know what to predict, let us visualize the dataset.\",\n",
      "        \"recommended_code\": \"# Reference: https://www.kaggle.com/code/allunia/m5-sales-uncertainty-prediction\\n\\ntrain_timeseries = timeseries[0:-28]\\neval_timeseries = timeseries[-28::]\\nprint(len(train_timeseries), len(eval_timeseries))\\ndays = np.arange(1, len(series_cols)+1)\\nplt.figure(figsize=(20,5))\\nplt.plot(days[0:-28], train_timeseries, label=\\\"train\\\")\\nplt.plot(days[-28::], eval_timeseries, label=\\\"validation\\\")\\nplt.title(\\\"Top-Level-1: Summed product sales of all stores and states\\\");\\nplt.legend()\\nplt.xlabel(\\\"Day\\\")\\nplt.ylabel(\\\"Unit sales\\\");\",\n",
      "        \"original_code\": \"# Reference: https://www.kaggle.com/code/tarunpaparaju/m5-competition-eda-models\\n\\nimport os\\nimport gc\\nimport time\\nimport math\\nimport datetime\\nfrom math import log, floor\\nfrom sklearn.neighbors import KDTree\\n\\nimport numpy as np\\nimport pandas as pd\\nfrom pathlib import Path\\nfrom sklearn.utils import shuffle\\nfrom tqdm.notebook import tqdm as tqdm\\n\\nimport seaborn as sns\\nfrom matplotlib import colors\\nimport matplotlib.pyplot as plt\\nfrom matplotlib.colors import Normalize\\n\\nimport plotly.express as px\\nimport plotly.graph_objects as go\\nimport plotly.figure_factory as ff\\nfrom plotly.subplots import make_subplots\\n\\nimport pywt\\nfrom statsmodels.robust import mad\\n\\nimport scipy\\nimport statsmodels\\nfrom scipy import signal\\nimport statsmodels.api as sm\\nfrom fbprophet import Prophet\\nfrom scipy.signal import butter, deconvolve\\nfrom statsmodels.tsa.arima_model import ARIMA\\nfrom statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing, Holt\\n\\nimport warnings\\nwarnings.filterwarnings(\\\"ignore\\\")\",\n",
      "        \"count\": \"1-3\",\n",
      "        \"rank\": \"grandmaster\"\n",
      "    },\n",
      "    {\n",
      "        \"original_md\": \"# EDA <a id=\\\"2\\\"></a>\\n\\nNow, I will try to visualize the sales data and gain some insights from it.\",\n",
      "        \"recommended_code\": \"# Reference: https://www.kaggle.com/code/gpreda/elo-eda-and-prediction\\n\\nplot_target_distribution('feature_2')\",\n",
      "        \"original_code\": \"# Reference: https://www.kaggle.com/code/tarunpaparaju/m5-competition-eda-models\\n\\nimport os\\nimport gc\\nimport time\\nimport math\\nimport datetime\\nfrom math import log, floor\\nfrom sklearn.neighbors import KDTree\\n\\nimport numpy as np\\nimport pandas as pd\\nfrom pathlib import Path\\nfrom sklearn.utils import shuffle\\nfrom tqdm.notebook import tqdm as tqdm\\n\\nimport seaborn as sns\\nfrom matplotlib import colors\\nimport matplotlib.pyplot as plt\\nfrom matplotlib.colors import Normalize\\n\\nimport plotly.express as px\\nimport plotly.graph_objects as go\\nimport plotly.figure_factory as ff\\nfrom plotly.subplots import make_subplots\\n\\nimport pywt\\nfrom statsmodels.robust import mad\\n\\nimport scipy\\nimport statsmodels\\nfrom scipy import signal\\nimport statsmodels.api as sm\\nfrom fbprophet import Prophet\\nfrom scipy.signal import butter, deconvolve\\nfrom statsmodels.tsa.arima_model import ARIMA\\nfrom statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing, Holt\\n\\nimport warnings\\nwarnings.filterwarnings(\\\"ignore\\\")\",\n",
      "        \"count\": \"1-4\",\n",
      "        \"rank\": \"grandmaster\"\n",
      "    },\n",
      "    {\n",
      "        \"original_md\": \"## Preparing the ground <a id=\\\"2.1\\\"></a>\",\n",
      "        \"recommended_code\": \"# Reference: https://www.kaggle.com/code/gpreda/panda-challenge-starting-eda\\n\\nplot_count(train_df, 'isup_grade','ISUP grade - data count and percent', size=3)\",\n",
      "        \"original_code\": \"# Reference: https://www.kaggle.com/code/tarunpaparaju/m5-competition-eda-models\\n\\nimport os\\nimport gc\\nimport time\\nimport math\\nimport datetime\\nfrom math import log, floor\\nfrom sklearn.neighbors import KDTree\\n\\nimport numpy as np\\nimport pandas as pd\\nfrom pathlib import Path\\nfrom sklearn.utils import shuffle\\nfrom tqdm.notebook import tqdm as tqdm\\n\\nimport seaborn as sns\\nfrom matplotlib import colors\\nimport matplotlib.pyplot as plt\\nfrom matplotlib.colors import Normalize\\n\\nimport plotly.express as px\\nimport plotly.graph_objects as go\\nimport plotly.figure_factory as ff\\nfrom plotly.subplots import make_subplots\\n\\nimport pywt\\nfrom statsmodels.robust import mad\\n\\nimport scipy\\nimport statsmodels\\nfrom scipy import signal\\nimport statsmodels.api as sm\\nfrom fbprophet import Prophet\\nfrom scipy.signal import butter, deconvolve\\nfrom statsmodels.tsa.arima_model import ARIMA\\nfrom statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing, Holt\\n\\nimport warnings\\nwarnings.filterwarnings(\\\"ignore\\\")\",\n",
      "        \"count\": \"1-5\",\n",
      "        \"rank\": \"grandmaster\"\n",
      "    },\n",
      "    {\n",
      "        \"original_md\": \"### Import libraries\",\n",
      "        \"recommended_code\": \"# Necessary librarys\\nimport os # it's a operational system library, to set some informations\\nimport random # random is to generate random values\\n\\nimport pandas as pd # to manipulate data frames \\nimport numpy as np # to work with matrix\\nfrom scipy.stats import kurtosis, skew # it's to explore some statistics of numerical values\\n\\nimport matplotlib.pyplot as plt # to graphics plot\\nimport seaborn as sns # a good library to graphic plots\\nimport squarify # to better understand proportion of categorys - it's a treemap layout algorithm\\n\\n# Importing librarys to use on interactive graphs\\nfrom plotly.offline import init_notebook_mode, iplot, plot \\nimport plotly.graph_objs as go \\n\\nimport json # to convert json in df\\nfrom pandas.io.json import json_normalize # to normalize the json file\\n\\n# to set a style to all graphs\\nplt.style.use('fivethirtyeight')\\ninit_notebook_mode(connected=True)\",\n",
      "        \"original_code\": \"# Reference: https://www.kaggle.com/code/tarunpaparaju/m5-competition-eda-models\\n\\nimport os\\nimport gc\\nimport time\\nimport math\\nimport datetime\\nfrom math import log, floor\\nfrom sklearn.neighbors import KDTree\\n\\nimport numpy as np\\nimport pandas as pd\\nfrom pathlib import Path\\nfrom sklearn.utils import shuffle\\nfrom tqdm.notebook import tqdm as tqdm\\n\\nimport seaborn as sns\\nfrom matplotlib import colors\\nimport matplotlib.pyplot as plt\\nfrom matplotlib.colors import Normalize\\n\\nimport plotly.express as px\\nimport plotly.graph_objects as go\\nimport plotly.figure_factory as ff\\nfrom plotly.subplots import make_subplots\\n\\nimport pywt\\nfrom statsmodels.robust import mad\\n\\nimport scipy\\nimport statsmodels\\nfrom scipy import signal\\nimport statsmodels.api as sm\\nfrom fbprophet import Prophet\\nfrom scipy.signal import butter, deconvolve\\nfrom statsmodels.tsa.arima_model import ARIMA\\nfrom statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing, Holt\\n\\nimport warnings\\nwarnings.filterwarnings(\\\"ignore\\\")\",\n",
      "        \"count\": \"1-6\",\n",
      "        \"rank\": \"grandmaster\"\n",
      "    },\n",
      "    {\n",
      "        \"original_md\": \"## Sales data <a id=\\\"2.2\\\"></a>\\n\\n\\n### Sample sales data\",\n",
      "        \"recommended_code\": \"# Reference: https://www.kaggle.com/code/gpreda/elo-eda-and-prediction\\n\\nplot_two_bar(get_categories(train_df,'feature_2'), get_categories(test_df,'feature_2'), \\n             'Train data', 'Test data',\\n             'Feature 2', 'Number of records')\",\n",
      "        \"original_code\": \"# Reference: https://www.kaggle.com/code/tarunpaparaju/m5-competition-eda-models\\n\\nids = sorted(list(set(sales_train_val['id'])))\\nd_cols = [c for c in sales_train_val.columns if 'd_' in c]\\nx_1 = sales_train_val.loc[sales_train_val['id'] == ids[2]].set_index('id')[d_cols].values[0]\\nx_2 = sales_train_val.loc[sales_train_val['id'] == ids[1]].set_index('id')[d_cols].values[0]\\nx_3 = sales_train_val.loc[sales_train_val['id'] == ids[17]].set_index('id')[d_cols].values[0]\\n\\nfig = make_subplots(rows=3, cols=1)\\n\\nfig.add_trace(go.Scatter(x=np.arange(len(x_1)), y=x_1, showlegend=False,\\n                    mode='lines', name=\\\"First sample\\\",\\n                         marker=dict(color=\\\"mediumseagreen\\\")),\\n             row=1, col=1)\\n\\nfig.add_trace(go.Scatter(x=np.arange(len(x_2)), y=x_2, showlegend=False,\\n                    mode='lines', name=\\\"Second sample\\\",\\n                         marker=dict(color=\\\"violet\\\")),\\n             row=2, col=1)\\n\\nfig.add_trace(go.Scatter(x=np.arange(len(x_3)), y=x_3, showlegend=False,\\n                    mode='lines', name=\\\"Third sample\\\",\\n                         marker=dict(color=\\\"dodgerblue\\\")),\\n             row=3, col=1)\\n\\nfig.update_layout(height=1200, width=800, title_text=\\\"Sample sales\\\")\\nfig.show()\",\n",
      "        \"count\": \"2-1\",\n",
      "        \"rank\": \"grandmaster\"\n",
      "    },\n",
      "    {\n",
      "        \"original_md\": \"These are sales data from randomly selected stores in the dataset. As expected, the sales data is very erratic, owing to the fact that so many factors affect the sales on a given day. On certain days, the sales quantity is zero, which indicates that a certain product may not be available on that day (as noted by Rob in his kernel).\",\n",
      "        \"recommended_code\": \"# Reference: https://www.kaggle.com/code/robikscube/fun-eda-with-fake-names-and-lots-of-plots\\n\\ngrouped = train.groupby(by=['item_name'])\\nfor i, d in grouped:\\n    myplot = d.set_index('date').groupby('store_name')['sales'] \\\\\\n        .plot(figsize=(15,2), style='.', title=str(i), legend=False)\\n    plt.show()\",\n",
      "        \"original_code\": \"# Reference: https://www.kaggle.com/code/tarunpaparaju/m5-competition-eda-models\\n\\nids = sorted(list(set(sales_train_val['id'])))\\nd_cols = [c for c in sales_train_val.columns if 'd_' in c]\\nx_1 = sales_train_val.loc[sales_train_val['id'] == ids[0]].set_index('id')[d_cols].values[0][:90]\\nx_2 = sales_train_val.loc[sales_train_val['id'] == ids[4]].set_index('id')[d_cols].values[0][1300:1400]\\nx_3 = sales_train_val.loc[sales_train_val['id'] == ids[65]].set_index('id')[d_cols].values[0][350:450]\\nfig = make_subplots(rows=3, cols=1)\\n\\nfig.add_trace(go.Scatter(x=np.arange(len(x_1)), y=x_1, showlegend=False,\\n                    mode='lines+markers', name=\\\"First sample\\\",\\n                         marker=dict(color=\\\"mediumseagreen\\\")),\\n             row=1, col=1)\\n\\nfig.add_trace(go.Scatter(x=np.arange(len(x_2)), y=x_2, showlegend=False,\\n                    mode='lines+markers', name=\\\"Second sample\\\",\\n                         marker=dict(color=\\\"violet\\\")),\\n             row=2, col=1)\\n\\nfig.add_trace(go.Scatter(x=np.arange(len(x_3)), y=x_3, showlegend=False,\\n                    mode='lines+markers', name=\\\"Third sample\\\",\\n                         marker=dict(color=\\\"dodgerblue\\\")),\\n             row=3, col=1)\\n\\nfig.update_layout(height=1200, width=800, title_text=\\\"Sample sales snippets\\\")\\nfig.show()\",\n",
      "        \"count\": \"3-1\",\n",
      "        \"rank\": \"grandmaster\"\n",
      "    },\n",
      "    {\n",
      "        \"original_md\": \"### Sample sales snippets\",\n",
      "        \"recommended_code\": \"# Reference: https://www.kaggle.com/code/artgor/it-is-time-for-m5-going-step-by-step\\n\\nplt.figure(figsize=(12, 4))\\nfor i in range(10):\\n    plt.plot(train_sales.loc[train_sales['item_id'] == 'HOBBIES_1_002'].iloc[i, 6:].values,\\n             label=train_sales.loc[train_sales['item_id'] == 'HOBBIES_1_002'].iloc[i, 5]);\\nplt.title('HOBBIES_1_002 sales')\\nplt.legend();\",\n",
      "        \"original_code\": \"# Reference: https://www.kaggle.com/code/tarunpaparaju/m5-competition-eda-models\\n\\nids = sorted(list(set(sales_train_val['id'])))\\nd_cols = [c for c in sales_train_val.columns if 'd_' in c]\\nx_1 = sales_train_val.loc[sales_train_val['id'] == ids[0]].set_index('id')[d_cols].values[0][:90]\\nx_2 = sales_train_val.loc[sales_train_val['id'] == ids[4]].set_index('id')[d_cols].values[0][1300:1400]\\nx_3 = sales_train_val.loc[sales_train_val['id'] == ids[65]].set_index('id')[d_cols].values[0][350:450]\\nfig = make_subplots(rows=3, cols=1)\\n\\nfig.add_trace(go.Scatter(x=np.arange(len(x_1)), y=x_1, showlegend=False,\\n                    mode='lines+markers', name=\\\"First sample\\\",\\n                         marker=dict(color=\\\"mediumseagreen\\\")),\\n             row=1, col=1)\\n\\nfig.add_trace(go.Scatter(x=np.arange(len(x_2)), y=x_2, showlegend=False,\\n                    mode='lines+markers', name=\\\"Second sample\\\",\\n                         marker=dict(color=\\\"violet\\\")),\\n             row=2, col=1)\\n\\nfig.add_trace(go.Scatter(x=np.arange(len(x_3)), y=x_3, showlegend=False,\\n                    mode='lines+markers', name=\\\"Third sample\\\",\\n                         marker=dict(color=\\\"dodgerblue\\\")),\\n             row=3, col=1)\\n\\nfig.update_layout(height=1200, width=800, title_text=\\\"Sample sales snippets\\\")\\nfig.show()\",\n",
      "        \"count\": \"3-2\",\n",
      "        \"rank\": \"grandmaster\"\n",
      "    },\n",
      "    {\n",
      "        \"original_md\": \"In the above plots, I simply zoom in to sample snippets in the sales data. As stated earlier, we can clearly see that the sales data is very erratic and volatile. Sometimes, the sales are zero for a few days in a row, and at other times, it remains at its peak value for a few days. Therefore, we need some sort of \\\"denoising\\\" techniques to find the underlying trends in the sales data and make forecasts.\",\n",
      "        \"recommended_code\": \"# Reference: https://www.kaggle.com/code/ogrellier/median-rental-prices-matter\\n\\nplt.figure(figsize=(11,10))\\nsns.boxplot(x=\\\"bedrooms\\\", y=\\\"price\\\", hue=\\\"interest_level\\\", data=usable_train, palette=palette)\",\n",
      "        \"original_code\": \"# Reference: https://www.kaggle.com/code/tarunpaparaju/m5-competition-eda-models\\n\\ndef maddest(d, axis=None):\\n    return np.mean(np.absolute(d - np.mean(d, axis)), axis)\\n\\ndef denoise_signal(x, wavelet='db4', level=1):\\n    coeff = pywt.wavedec(x, wavelet, mode=\\\"per\\\")\\n    sigma = (1/0.6745) * maddest(coeff[-level])\\n\\n    uthresh = sigma * np.sqrt(2*np.log(len(x)))\\n    coeff[1:] = (pywt.threshold(i, value=uthresh, mode='hard') for i in coeff[1:])\\n\\n    return pywt.waverec(coeff, wavelet, mode='per')\\ny_w1 = denoise_signal(x_1)\\ny_w2 = denoise_signal(x_2)\\ny_w3 = denoise_signal(x_3)\\n\\n\\nfig = make_subplots(rows=3, cols=1)\\n\\nfig.add_trace(\\n    go.Scatter(x=np.arange(len(x_1)), mode='lines+markers', y=x_1, marker=dict(color=\\\"mediumaquamarine\\\"), showlegend=False,\\n               name=\\\"Original signal\\\"),\\n    row=1, col=1\\n)\\n\\nfig.add_trace(\\n    go.Scatter(x=np.arange(len(x_1)), y=y_w1, mode='lines', marker=dict(color=\\\"darkgreen\\\"), showlegend=False,\\n               name=\\\"Denoised signal\\\"),\\n    row=1, col=1\\n)\\n\\nfig.add_trace(\\n    go.Scatter(x=np.arange(len(x_2)), mode='lines+markers', y=x_2, marker=dict(color=\\\"thistle\\\"), showlegend=False),\\n    row=2, col=1\\n)\\n\\nfig.add_trace(\\n    go.Scatter(x=np.arange(len(x_2)), y=y_w2, mode='lines', marker=dict(color=\\\"purple\\\"), showlegend=False),\\n    row=2, col=1\\n)\\n\\nfig.add_trace(\\n    go.Scatter(x=np.arange(len(x_3)), mode='lines+markers', y=x_3, marker=dict(color=\\\"lightskyblue\\\"), showlegend=False),\\n    row=3, col=1\\n)\\n\\nfig.add_trace(\\n    go.Scatter(x=np.arange(len(x_3)), y=y_w3, mode='lines', marker=dict(color=\\\"navy\\\"), showlegend=False),\\n    row=3, col=1\\n)\\n\\nfig.update_layout(height=1200, width=800, title_text=\\\"Original (pale) vs. Denoised (dark) sales\\\")\\nfig.show()\",\n",
      "        \"count\": \"4-1\",\n",
      "        \"rank\": \"grandmaster\"\n",
      "    },\n",
      "    {\n",
      "        \"original_md\": \"## Denoising <a id=\\\"2.3\\\"></a>\\n\\nNow, I will show how these volatile sales prices can be denoised in order to extract underlying trends. This method may lose some information from the original time series, but it may be useful in extracting certain features regarding the trends in the time series.\",\n",
      "        \"recommended_code\": \"# Reference: https://www.kaggle.com/code/artgor/extensive-poverty-eda-feature-engineering-and-lgb\\n\\nsns.boxplot(x='Target', y='escolari', data = train);\\nplt.title('Years of schooling per household poverty level.')\",\n",
      "        \"original_code\": \"# Reference: https://www.kaggle.com/code/tarunpaparaju/m5-competition-eda-models\\n\\ndef maddest(d, axis=None):\\n    return np.mean(np.absolute(d - np.mean(d, axis)), axis)\\n\\ndef denoise_signal(x, wavelet='db4', level=1):\\n    coeff = pywt.wavedec(x, wavelet, mode=\\\"per\\\")\\n    sigma = (1/0.6745) * maddest(coeff[-level])\\n\\n    uthresh = sigma * np.sqrt(2*np.log(len(x)))\\n    coeff[1:] = (pywt.threshold(i, value=uthresh, mode='hard') for i in coeff[1:])\\n\\n    return pywt.waverec(coeff, wavelet, mode='per')\\ny_w1 = denoise_signal(x_1)\\ny_w2 = denoise_signal(x_2)\\ny_w3 = denoise_signal(x_3)\\n\\n\\nfig = make_subplots(rows=3, cols=1)\\n\\nfig.add_trace(\\n    go.Scatter(x=np.arange(len(x_1)), mode='lines+markers', y=x_1, marker=dict(color=\\\"mediumaquamarine\\\"), showlegend=False,\\n               name=\\\"Original signal\\\"),\\n    row=1, col=1\\n)\\n\\nfig.add_trace(\\n    go.Scatter(x=np.arange(len(x_1)), y=y_w1, mode='lines', marker=dict(color=\\\"darkgreen\\\"), showlegend=False,\\n               name=\\\"Denoised signal\\\"),\\n    row=1, col=1\\n)\\n\\nfig.add_trace(\\n    go.Scatter(x=np.arange(len(x_2)), mode='lines+markers', y=x_2, marker=dict(color=\\\"thistle\\\"), showlegend=False),\\n    row=2, col=1\\n)\\n\\nfig.add_trace(\\n    go.Scatter(x=np.arange(len(x_2)), y=y_w2, mode='lines', marker=dict(color=\\\"purple\\\"), showlegend=False),\\n    row=2, col=1\\n)\\n\\nfig.add_trace(\\n    go.Scatter(x=np.arange(len(x_3)), mode='lines+markers', y=x_3, marker=dict(color=\\\"lightskyblue\\\"), showlegend=False),\\n    row=3, col=1\\n)\\n\\nfig.add_trace(\\n    go.Scatter(x=np.arange(len(x_3)), y=y_w3, mode='lines', marker=dict(color=\\\"navy\\\"), showlegend=False),\\n    row=3, col=1\\n)\\n\\nfig.update_layout(height=1200, width=800, title_text=\\\"Original (pale) vs. Denoised (dark) sales\\\")\\nfig.show()\",\n",
      "        \"count\": \"4-2\",\n",
      "        \"rank\": \"grandmaster\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(testing_accumulate, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.get(\"/\", async (req, res) => {\n",
    "    const grandMasterJson = JSON.parse(\n",
    "    await readFile('path/to/file.json')\n",
    "    )\n",
    "\n",
    "    grandMasterJson.map(async (pair) => {\n",
    "        const markdownDesc = pair.markdown.reduce((acc, cur) => acc + cur, \"\");\n",
    "        const response = await fetch(URL + \"/api/v1/ir/base\", {\n",
    "            method: \"POST\", \n",
    "            headers: {\n",
    "                \"Content-Type\": \"application/json\",\n",
    "            },\n",
    "            body: JSON.stringify({\n",
    "                query: markdownDesc,\n",
    "                index: \"grandmaster\",\n",
    "            }),\n",
    "        });\n",
    "\n",
    "        const responseJson = await response.json();\n",
    "\n",
    "        if(responseJson && responseJson.hits.hits.length > 0) {\n",
    "            console.log(responseJson.hits.hits[0]);\n",
    "        }\n",
    "        console.log(responseJson)\n",
    "    });\n",
    "\n",
    "    res.status(200).send(\"Hello World\")\n",
    "})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create a scatter plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: https://www.kaggle.com/code/arthurtok/interactive-porto-insights-a-plot-ly-tutorial\n",
    "\n",
    "# Scatter plot \n",
    "trace = go.Scatter(\n",
    "    y = rf.feature_importances_,\n",
    "    x = features,\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        sizemode = 'diameter',\n",
    "        sizeref = 1,\n",
    "        size = 13,\n",
    "        #size= rf.feature_importances_,\n",
    "        #color = np.random.randn(500), #set color equal to a variable\n",
    "        color = rf.feature_importances_,\n",
    "        colorscale='Portland',\n",
    "        showscale=True\n",
    "    ),\n",
    "    text = features\n",
    ")\n",
    "data = [trace]\n",
    "\n",
    "layout= go.Layout(\n",
    "    autosize= True,\n",
    "    title= 'Random Forest Feature Importance',\n",
    "    hovermode= 'closest',\n",
    "     xaxis= dict(\n",
    "         ticklen= 5,\n",
    "         showgrid=False,\n",
    "        zeroline=False,\n",
    "        showline=False\n",
    "     ),\n",
    "    yaxis=dict(\n",
    "        title= 'Feature Importance',\n",
    "        showgrid=False,\n",
    "        zeroline=False,\n",
    "        ticklen= 5,\n",
    "        gridwidth= 2\n",
    "    ),\n",
    "    showlegend= False\n",
    ")\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.iplot(fig,filename='scatter2010')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "senior-jupyter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
