[{"markdown": ["In the above graphs, the dark lineplots represent the denoised sales and the light lineplots represent the original sales. We can see that Wavelet denoising is able to successfully find the \"general trend\" in the sales data without getting distracted by the noise. Finding these high- trends or patterns in the sales may be useful in generating features to train a model.", "The below diagram illustrates these graphs side-by-side. Red graphs represent original sales and green graphs represent denoised sales."], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/m5-competition-eda-models\n\nfig, ax = plt.subplots(nrows=3, ncols=2, figsize=(30, 20))\n\nax[0, 0].plot(x_1, color='seagreen', marker='o') \nax[0, 0].set_title('Original Sales', fontsize=24)\nax[0, 1].plot(y_w1, color='red', marker='.') \nax[0, 1].set_title('After Wavelet Denoising', fontsize=24)\n\nax[1, 0].plot(x_2, color='seagreen', marker='o') \nax[1, 0].set_title('Original Sales', fontsize=24)\nax[1, 1].plot(y_w2, color='red', marker='.') \nax[1, 1].set_title('After Wavelet Denoising', fontsize=24)\n\nax[2, 0].plot(x_3, color='seagreen', marker='o') \nax[2, 0].set_title('Original Sales', fontsize=24)\nax[2, 1].plot(y_w3, color='red', marker='.') \nax[2, 1].set_title('After Wavelet Denoising', fontsize=24)\n\nplt.show()", "processed": ["graph dark lineplot repres denois sale light lineplot repres origin sale see wavelet denois abl success find gener trend sale data without get distract nois find high trend pattern sale may use gener featur train model", "diagram illustr graph side side red graph repres origin sale green graph repres denois sale"]}, {"markdown": ["In the above graphs, the dark lineplots represent the denoised sales and the light lineplots represent the original sales. We can see that average smoothing is not as effective as Wavelet denoising at finding macroscopic trends and pattersns in the data. A lot of the noise in the original sales persists even after denoising. Therefore, wavelet denoising is clearly more effective at finding trends in the sales data. Nonetheless, average smoothing or \"rolling mean\" can also be used to calculate useful features for modeling.", "The below diagram illustrates these graphs side-by-side. Red graphs represent original sales and green graphs represent denoised sales."], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/m5-competition-eda-models\n\nfig, ax = plt.subplots(nrows=3, ncols=2, figsize=(30, 20))\n\nax[0, 0].plot(x_1, color='seagreen', marker='o') \nax[0, 0].set_title('Original Sales', fontsize=24)\nax[0, 1].plot(y_a1, color='red', marker='.') \nax[0, 1].set_title('After Wavelet Denoising', fontsize=24)\n\nax[1, 0].plot(x_2, color='seagreen', marker='o') \nax[1, 0].set_title('Original Sales', fontsize=24)\nax[1, 1].plot(y_a2, color='red', marker='.') \nax[1, 1].set_title('After Wavelet Denoising', fontsize=24)\n\nax[2, 0].plot(x_3, color='seagreen', marker='o') \nax[2, 0].set_title('Original Sales', fontsize=24)\nax[2, 1].plot(y_a3, color='red', marker='.') \nax[2, 1].set_title('After Wavelet Denoising', fontsize=24)\n\nplt.show()", "processed": ["graph dark lineplot repres denois sale light lineplot repres origin sale see averag smooth effect wavelet denois find macroscop trend pattersn data lot nois origin sale persist even denois therefor wavelet denois clearli effect find trend sale data nonetheless averag smooth roll mean also use calcul use featur model", "diagram illustr graph side side red graph repres origin sale green graph repres denois sale"]}, {"markdown": ["## <a id=\"34\">Values distribution</a>  ", "We define a function to show the number and percent of each category in the current selected feature."], "code": "# Reference: https://www.kaggle.com/code/gpreda/2019-data-science-bowl-eda\n\ndef plot_count(feature, title, df, size=1):\n    f, ax = plt.subplots(1,1, figsize=(4*size,4))\n    total = float(len(df))\n    g = sns.countplot(df[feature], order = df[feature].value_counts().index[:20], palette='Set3')\n    g.set_title(\"Number and percentage of {}\".format(title))\n    if(size > 2):\n        plt.xticks(rotation=90, size=8)\n    for p in ax.patches:\n        height = p.get_height()\n        ax.text(p.get_x()+p.get_width()/2.,\n                height + 3,\n                '{:1.2f}%'.format(100*height/total),\n                ha=\"center\") \n    plt.show()    \nplot_count('title', 'title (first most frequent 20 values - train)', train_df, size=4)\nplot_count('title', 'title (first most frequent 20 values - test)', test_df, size=4)\nprint(f\"Title values (train): {train_df.title.nunique()}\")\nprint(f\"Title values (test): {test_df.title.nunique()}\")\nplot_count('type', 'type - train', train_df, size=2)\nplot_count('type', 'type - test', test_df, size=2)\nplot_count('world', 'world - train', train_df, size=2)\nplot_count('world', 'world - test', test_df, size=2)\nplot_count('event_code', 'event_code - test', train_df, size=4)\nplot_count('event_code', 'event_code - test', test_df, size=4)", "processed": ["id 34 valu distribut", "defin function show number percent categori current select featur"]}, {"markdown": ["Let's look to the first 40 values, ordered by percent of existing data (descending)."], "code": "# Reference: https://www.kaggle.com/code/gpreda/2019-data-science-bowl-eda\n\nplt.figure(figsize=(10, 10))\nsns.set(style='whitegrid')\nax = sns.barplot(x='Percent', y='index', data=stat_event_data.head(40), color='blue')\nplt.title('Most frequent features in event data')\nplt.ylabel('Features')\nstat_event_data[['index', 'Percent']].head(20)", "processed": ["let look first 40 valu order percent exist data descend"]}, {"markdown": ["Let's see the distribution of the number of arguments for each `event_id`."], "code": "# Reference: https://www.kaggle.com/code/gpreda/2019-data-science-bowl-eda\n\ntmp = specs_args_extracted.groupby(['event_id'])['info'].count()\ndf = pd.DataFrame({'event_id':tmp.index, 'count': tmp.values})\nplt.figure(figsize=(6,4))\nsns.set(style='whitegrid')\nax = sns.distplot(df['count'],kde=True,hist=False, bins=40)\nplt.title('Distribution of number of arguments per event_id')\nplt.xlabel('Number of arguments'); plt.ylabel('Density'); plt.show()\nplot_count('args_name', 'args_name (first 20 most frequent values) - specs', specs_args_extracted, size=4)\nplot_count('args_type', 'args_type - specs', specs_args_extracted, size=3)\nplot_count('args_info', 'args_info (first 20 most frequent values) - specs', specs_args_extracted, size=4)", "processed": ["let see distribut number argument event id"]}, {"markdown": ["Then, we calculate the compacted form of train, by merging the aggregated numerical features from train with the dataset with unique `installation_id`."], "code": "# Reference: https://www.kaggle.com/code/gpreda/2019-data-science-bowl-eda\n\nfor i in numerical_columns:\n    comp_train_df = comp_train_df.merge(get_numeric_columns(train_df, i), left_index = True, right_index = True)\nprint(f\"comp_train shape: {comp_train_df.shape}\")\ncomp_train_df.head()\n# get the mode of the title\nlabels_map = dict(train_labels_df.groupby('title')['accuracy_group'].agg(lambda x:x.value_counts().index[0]))\n# merge target\nlabels = train_labels_df[['installation_id', 'title', 'accuracy_group']]\n# replace title with the mode\nlabels['title'] = labels['title'].map(labels_map)\n# join train with labels\ncomp_train_df = labels.merge(comp_train_df, on = 'installation_id', how = 'left')\nprint('We have {} training rows'.format(comp_train_df.shape[0]))\ncomp_train_df.head()\nprint(f\"comp_train_df shape: {comp_train_df.shape}\")\nfor feature in comp_train_df.columns.values[3:20]:\n    print(f\"{feature} unique values: {comp_train_df[feature].nunique()}\")\nplot_count('title', 'title - compound train', comp_train_df)\nplot_count('accuracy_group', 'accuracy_group - compound train', comp_train_df, size=2)\nplt.figure(figsize=(16,6))\n_titles = comp_train_df.title.unique()\nplt.title(\"Distribution of log(`game time mean`) values (grouped by title) in the comp train\")\nfor _title in _titles:\n    red_comp_train_df = comp_train_df.loc[comp_train_df.title == _title]\n    sns.distplot(np.log(red_comp_train_df['game_time_mean']), kde=True, label=f'title: {_title}')\nplt.legend()\nplt.show()\nplt.figure(figsize=(16,6))\n_titles = comp_train_df.title.unique()\nplt.title(\"Distribution of log(`game time std`) values (grouped by title) in the comp train\")\nfor _title in _titles:\n    red_comp_train_df = comp_train_df.loc[comp_train_df.title == _title]\n    sns.distplot(np.log(red_comp_train_df['game_time_std']), kde=True, label=f'title: {_title}')\nplt.legend()\nplt.show()\nplt.figure(figsize=(16,6))\n_titles = comp_train_df.title.unique()\nplt.title(\"Distribution of `game time skew` values (grouped by title) in the comp train\")\nfor _title in _titles:\n    red_comp_train_df = comp_train_df.loc[comp_train_df.title == _title]\n    sns.distplot(red_comp_train_df['game_time_skew'], kde=True, label=f'title: {_title}')\nplt.legend()\nplt.show()\nplt.figure(figsize=(16,6))\n_titles = comp_train_df.title.unique()\nplt.title(\"Distribution of `hour mean` values (grouped by title) in the comp train\")\nfor _title in _titles:\n    red_comp_train_df = comp_train_df.loc[comp_train_df.title == _title]\n    sns.distplot(red_comp_train_df['hour_mean'], kde=True, label=f'title: {_title}')\nplt.legend()\nplt.show()\nplt.figure(figsize=(16,6))\n_titles = comp_train_df.title.unique()\nplt.title(\"Distribution of `hour std` values (grouped by title) in the comp train\")\nfor _title in _titles:\n    red_comp_train_df = comp_train_df.loc[comp_train_df.title == _title]\n    sns.distplot(red_comp_train_df['hour_std'], kde=True, label=f'title: {_title}')\nplt.legend()\nplt.show()\nplt.figure(figsize=(16,6))\n_titles = comp_train_df.title.unique()\nplt.title(\"Distribution of `hour skew` values (grouped by title) in the comp train\")\nfor _title in _titles:\n    red_comp_train_df = comp_train_df.loc[comp_train_df.title == _title]\n    sns.distplot(red_comp_train_df['hour_skew'], kde=True, label=f'title: {_title}')\nplt.legend()\nplt.show()\nplt.figure(figsize=(16,6))\n_titles = comp_train_df.title.unique()\nplt.title(\"Distribution of `month mean` values (grouped by title) in the comp train\")\nfor _title in _titles:\n    red_comp_train_df = comp_train_df.loc[comp_train_df.title == _title]\n    sns.distplot(red_comp_train_df['month_mean'], kde=True, label=f'title: {_title}')\nplt.legend()\nplt.show()\nplt.figure(figsize=(16,6))\n_titles = comp_train_df.title.unique()\nplt.title(\"Distribution of `month std` values (grouped by title) in the comp train\")\nfor _title in _titles:\n    red_comp_train_df = comp_train_df.loc[comp_train_df.title == _title]\n    sns.distplot(red_comp_train_df['month_std'], kde=True, label=f'title: {_title}')\nplt.legend()\nplt.show()\nplt.figure(figsize=(16,6))\n_titles = comp_train_df.title.unique()\nplt.title(\"Distribution of `month skew` values (grouped by title) in the comp train\")\nfor _title in _titles:\n    red_comp_train_df = comp_train_df.loc[comp_train_df.title == _title]\n    sns.distplot(red_comp_train_df['month_skew'], kde=True, label=f'title: {_title}')\nplt.legend()\nplt.show()\nplt.figure(figsize=(16,6))\n_accuracy_groups = comp_train_df.accuracy_group.unique()\nplt.title(\"Distribution of log(`game time mean`) values (grouped by accuracy group) in the comp train\")\nfor _accuracy_group in _accuracy_groups:\n    red_comp_train_df = comp_train_df.loc[comp_train_df.accuracy_group == _accuracy_group]\n    sns.distplot(np.log(red_comp_train_df['game_time_mean']), kde=True, label=f'accuracy group= {_accuracy_group}')\nplt.legend()\nplt.show()\nplt.figure(figsize=(16,6))\n_accuracy_groups = comp_train_df.accuracy_group.unique()\nplt.title(\"Distribution of log(`game time std`) values (grouped by accuracy group) in the comp train\")\nfor _accuracy_group in _accuracy_groups:\n    red_comp_train_df = comp_train_df.loc[comp_train_df.accuracy_group == _accuracy_group]\n    sns.distplot(np.log(red_comp_train_df['game_time_std']), kde=True, label=f'accuracy group= {_accuracy_group}')\nplt.legend()\nplt.show()\nplt.figure(figsize=(16,6))\n_accuracy_groups = comp_train_df.accuracy_group.unique()\nplt.title(\"Distribution of `game time skew` values (grouped by accuracy group) in the comp train\")\nfor _accuracy_group in _accuracy_groups:\n    red_comp_train_df = comp_train_df.loc[comp_train_df.accuracy_group == _accuracy_group]\n    sns.distplot(red_comp_train_df['game_time_skew'], kde=True, label=f'accuracy group= {_accuracy_group}')\nplt.legend()\nplt.show()\nplt.figure(figsize=(16,6))\n_accuracy_groups = comp_train_df.accuracy_group.unique()\nplt.title(\"Distribution of `hour mean` values (grouped by accuracy group) in the comp train\")\nfor _accuracy_group in _accuracy_groups:\n    red_comp_train_df = comp_train_df.loc[comp_train_df.accuracy_group == _accuracy_group]\n    sns.distplot(red_comp_train_df['hour_mean'], kde=True, label=f'accuracy group= {_accuracy_group}')\nplt.legend()\nplt.show()\nplt.figure(figsize=(16,6))\n_accuracy_groups = comp_train_df.accuracy_group.unique()\nplt.title(\"Distribution of `hour std` values (grouped by accuracy group) in the comp train\")\nfor _accuracy_group in _accuracy_groups:\n    red_comp_train_df = comp_train_df.loc[comp_train_df.accuracy_group == _accuracy_group]\n    sns.distplot(red_comp_train_df['hour_std'], kde=True, label=f'accuracy group= {_accuracy_group}')\nplt.legend()\nplt.show()\nplt.figure(figsize=(16,6))\n_accuracy_groups = comp_train_df.accuracy_group.unique()\nplt.title(\"Distribution of `hour skew` values (grouped by accuracy group) in the comp train\")\nfor _accuracy_group in _accuracy_groups:\n    red_comp_train_df = comp_train_df.loc[comp_train_df.accuracy_group == _accuracy_group]\n    sns.distplot(red_comp_train_df['hour_skew'], kde=True, label=f'accuracy group= {_accuracy_group}')\nplt.legend()\nplt.show()\nplt.figure(figsize=(16,6))\n_accuracy_groups = comp_train_df.accuracy_group.unique()\nplt.title(\"Distribution of `month mean` values (grouped by accuracy group) in the comp train\")\nfor _accuracy_group in _accuracy_groups:\n    red_comp_train_df = comp_train_df.loc[comp_train_df.accuracy_group == _accuracy_group]\n    sns.distplot(red_comp_train_df['month_mean'], kde=True, label=f'accuracy group= {_accuracy_group}')\nplt.legend()\nplt.show()\nplt.figure(figsize=(16,6))\n_accuracy_groups = comp_train_df.accuracy_group.unique()\nplt.title(\"Distribution of `month std` values (grouped by accuracy group) in the comp train\")\nfor _accuracy_group in _accuracy_groups:\n    red_comp_train_df = comp_train_df.loc[comp_train_df.accuracy_group == _accuracy_group]\n    sns.distplot(red_comp_train_df['month_std'], kde=True, label=f'accuracy group= {_accuracy_group}')\nplt.legend()\nplt.show()\nplt.figure(figsize=(16,6))\n_accuracy_groups = comp_train_df.accuracy_group.unique()\nplt.title(\"Distribution of `month skew` values (grouped by accuracy group) in the comp train\")\nfor _accuracy_group in _accuracy_groups:\n    red_comp_train_df = comp_train_df.loc[comp_train_df.accuracy_group == _accuracy_group]\n    sns.distplot(red_comp_train_df['month_skew'], kde=True, label=f'accuracy group= {_accuracy_group}')\nplt.legend()\nplt.show()", "processed": ["calcul compact form train merg aggreg numer featur train dataset uniqu instal id"]}, {"markdown": ["# Data Preprocessing\nhttps://www.kaggle.com/osciiart/covid19-lightgbm"], "code": "# Reference: https://www.kaggle.com/code/robikscube/rob-s-covid19-week-3-submission\n\n# Read in data\ntrain = pd.read_csv(\"../input/covid19-global-forecasting-week-3/train.csv\")\ntest = pd.read_csv(\"../input/covid19-global-forecasting-week-3/test.csv\")\n\ntt = pd.concat([train, test], sort=False)\ntt = train.merge(test, on=['Province_State','Country_Region','Date'], how='outer')\n\n# concat Country/Region and Province/State\ndef name_place(x):\n    try:\n        x_new = x['Country_Region'] + \"_\" + x['Province_State']\n    except:\n        x_new = x['Country_Region']\n    return x_new\ntt['Place'] = tt.apply(lambda x: name_place(x), axis=1)\n# tt = tt.drop(['Province_State','Country_Region'], axis=1)\ntt['Date'] = pd.to_datetime(tt['Date'])\ntt['doy'] = tt['Date'].dt.dayofyear\ntt['dow'] = tt['Date'].dt.dayofweek\ntt['hasProvidence'] = ~tt['Province_State'].isna()\n\n\ncountry_meta = pd.read_csv('../input/covid19-forecasting-metadata/region_metadata.csv')\ntt = tt.merge(country_meta, how='left')\n\ncountry_date_meta = pd.read_csv('../input/covid19-forecasting-metadata/region_date_metadata.csv')\n#tt = tt.merge(country_meta, how='left')\n\ntt['HasFatality'] = tt.groupby('Place')['Fatalities'].transform(lambda x: x.max() > 0)\ntt['HasCases'] = tt.groupby('Place')['ConfirmedCases'].transform(lambda x: x.max() > 0)\n\nfirst_case_date = tt.query('ConfirmedCases >= 1').groupby('Place')['Date'].min().to_dict()\nten_case_date = tt.query('ConfirmedCases >= 10').groupby('Place')['Date'].min().to_dict()\nhundred_case_date = tt.query('ConfirmedCases >= 100').groupby('Place')['Date'].min().to_dict()\nfirst_fatal_date = tt.query('Fatalities >= 1').groupby('Place')['Date'].min().to_dict()\nten_fatal_date = tt.query('Fatalities >= 10').groupby('Place')['Date'].min().to_dict()\nhundred_fatal_date = tt.query('Fatalities >= 100').groupby('Place')['Date'].min().to_dict()\n\ntt['First_Case_Date'] = tt['Place'].map(first_case_date)\ntt['Ten_Case_Date'] = tt['Place'].map(ten_case_date)\ntt['Hundred_Case_Date'] = tt['Place'].map(hundred_case_date)\ntt['First_Fatal_Date'] = tt['Place'].map(first_fatal_date)\ntt['Ten_Fatal_Date'] = tt['Place'].map(ten_fatal_date)\ntt['Hundred_Fatal_Date'] = tt['Place'].map(hundred_fatal_date)\n\ntt['Days_Since_First_Case'] = (tt['Date'] - tt['First_Case_Date']).dt.days\ntt['Days_Since_Ten_Cases'] = (tt['Date'] - tt['Ten_Case_Date']).dt.days\ntt['Days_Since_Hundred_Cases'] = (tt['Date'] - tt['Hundred_Case_Date']).dt.days\ntt['Days_Since_First_Fatal'] = (tt['Date'] - tt['First_Fatal_Date']).dt.days\ntt['Days_Since_Ten_Fatal'] = (tt['Date'] - tt['Ten_Fatal_Date']).dt.days\ntt['Days_Since_Hundred_Fatal'] = (tt['Date'] - tt['Hundred_Fatal_Date']).dt.days\n\n# Merge smoking data\nsmoking = pd.read_csv(\"../input/smokingstats/share-of-adults-who-smoke.csv\")\nsmoking = smoking.rename(columns={'Smoking prevalence, total (ages 15+) (% of adults)': 'Smoking_Rate'})\nsmoking_dict = smoking.groupby('Entity')['Year'].max().to_dict()\nsmoking['LastYear'] = smoking['Entity'].map(smoking_dict)\nsmoking = smoking.query('Year == LastYear').reset_index()\nsmoking['Entity'] = smoking['Entity'].str.replace('United States', 'US')\n\ntt = tt.merge(smoking[['Entity','Smoking_Rate']],\n         left_on='Country_Region',\n         right_on='Entity',\n         how='left',\n         validate='m:1') \\\n    .drop('Entity', axis=1)\n\n# Country data\ncountry_info = pd.read_csv('../input/countryinfo/covid19countryinfo.csv')\n\n\ntt = tt.merge(country_info, left_on=['Country_Region','Province_State'],\n              right_on=['country','region'],\n              how='left',\n              validate='m:1')\n\n# State info from wikipedia\nus_state_info = pd.read_html('https://simple.wikipedia.org/wiki/List_of_U.S._states_by_population')[0] \\\n    [['State','Population estimate, July 1, 2019[2]']] \\\n    .rename(columns={'Population estimate, July 1, 2019[2]' : 'Population'})\n#us_state_info['2019 population'] = pd.to_numeric(us_state_info['2019 population'].str.replace('[note 1]','').replace('[]',''))\n\ntt = tt.merge(us_state_info[['State','Population']],\n         left_on='Province_State',\n         right_on='State',\n         how='left')\n\ntt['pop'] = pd.to_numeric(tt['pop'].str.replace(',',''))\ntt['pop'] = tt['pop'].fillna(tt['Population'])\ntt['pop'] = pd.to_numeric(tt['pop'])\n\ntt['pop_diff'] = tt['pop'] - tt['Population']\ntt['Population_final'] = tt['Population']\ntt.loc[~tt['hasProvidence'], 'Population_final'] = tt.loc[~tt['hasProvidence']]['pop']\n\ntt['Confirmed_Cases_Diff'] = tt.groupby('Place')['ConfirmedCases'].diff()\ntt['Fatailities_Diff'] = tt.groupby('Place')['Fatalities'].diff()\nmax_date = tt.dropna(subset=['ConfirmedCases'])['Date'].max()\ntt['gdp2019'] = pd.to_numeric(tt['gdp2019'].str.replace(',',''))\n# Correcting population for missing countries\n# Googled their names and copied the numbers here\npop_dict = {'Angola': int(29.78 * 10**6),\n            'Australia_Australian Capital Territory': 423_800,\n            'Australia_New South Wales': int(7.544 * 10**6),\n            'Australia_Northern Territory': 244_300,\n            'Australia_Queensland' : int(5.071 * 10**6),\n            'Australia_South Australia' : int(1.677 * 10**6),\n            'Australia_Tasmania': 515_000,\n            'Australia_Victoria': int(6.359 * 10**6),\n            'Australia_Western Australia': int(2.589 * 10**6),\n            'Brazil': int(209.3 * 10**6),\n            'Canada_Alberta' : int(4.371 * 10**6),\n            'Canada_British Columbia' : int(5.071 * 10**6),\n            'Canada_Manitoba' : int(1.369 * 10**6),\n            'Canada_New Brunswick' : 776_827,\n            'Canada_Newfoundland and Labrador' : 521_542,\n            'Canada_Nova Scotia' : 971_395,\n            'Canada_Ontario' : int(14.57 * 10**6),\n            'Canada_Prince Edward Island' : 156_947,\n            'Canada_Quebec' : int(8.485 * 10**6),\n            'Canada_Saskatchewan': int(1.174 * 10**6),\n            'China_Anhui': int(62 * 10**6),\n            'China_Beijing': int(21.54 * 10**6),\n            'China_Chongqing': int(30.48 * 10**6),\n            'China_Fujian' :  int(38.56 * 10**6),\n            'China_Gansu' : int(25.58 * 10**6),\n            'China_Guangdong' : int(113.46 * 10**6),\n            'China_Guangxi' : int(48.38 * 10**6),\n            'China_Guizhou' : int(34.75 * 10**6),\n            'China_Hainan' : int(9.258 * 10**6),\n            'China_Hebei' : int(74.7 * 10**6),\n            'China_Heilongjiang' : int(38.31 * 10**6),\n            'China_Henan' : int(94 * 10**6),\n            'China_Hong Kong' : int(7.392 * 10**6),\n            'China_Hubei' : int(58.5 * 10**6),\n            'China_Hunan' : int(67.37 * 10**6),\n            'China_Inner Mongolia' :  int(24.71 * 10**6),\n            'China_Jiangsu' : int(80.4 * 10**6),\n            'China_Jiangxi' : int(45.2 * 10**6),\n            'China_Jilin' : int(27.3 * 10**6),\n            'China_Liaoning' : int(43.9 * 10**6),\n            'China_Macau' : 622_567,\n            'China_Ningxia' : int(6.301 * 10**6),\n            'China_Qinghai' : int(5.627 * 10**6),\n            'China_Shaanxi' : int(37.33 * 10**6),\n            'China_Shandong' : int(92.48 * 10**6),\n            'China_Shanghai' : int(24.28 * 10**6),\n            'China_Shanxi' : int(36.5 * 10**6),\n            'China_Sichuan' : int(81.1 * 10**6),\n            'China_Tianjin' : int(15 * 10**6),\n            'China_Tibet' : int(3.18 * 10**6),\n            'China_Xinjiang' : int(21.81 * 10**6),\n            'China_Yunnan' : int(45.97 * 10**6),\n            'China_Zhejiang' : int(57.37 * 10**6),\n            'Denmark_Faroe Islands' : 51_783,\n            'Denmark_Greenland' : 56_171,\n            'France_French Guiana' : 290_691,\n            'France_French Polynesia' : 283_007,\n            'France_Guadeloupe' : 395_700,\n            'France_Martinique' : 376_480,\n            'France_Mayotte' : 270_372,\n            'France_New Caledonia' : 99_926,\n            'France_Reunion' : 859_959,\n            'France_Saint Barthelemy' : 9_131,\n            'France_St Martin' : 32_125,\n            'Netherlands_Aruba' : 105_264,\n            'Netherlands_Curacao' : 161_014,\n            'Netherlands_Sint Maarten' : 41_109,\n            'Papua New Guinea' : int(8.251 * 10**6),\n            'US_Guam' : 164_229,\n            'US_Virgin Islands' : 107_268,\n            'United Kingdom_Bermuda' : 65_441,\n            'United Kingdom_Cayman Islands' : 61_559,\n            'United Kingdom_Channel Islands' : 170_499,\n            'United Kingdom_Gibraltar' : 34_571,\n            'United Kingdom_Isle of Man' : 84_287,\n            'United Kingdom_Montserrat' : 4_922,\n            'Botswana' : int(2.292 * 10**6),\n            'Burma' : int(53.37 * 10**6),\n            'Burundi': int(10.86 * 10**6),\n            'Canada' : int(37.59 * 10**6),\n            'MS Zaandam' : 1_829,\n            'Sierra Leone': int(7.557 * 10**6),\n            'United Kingdom' : int(66.65 * 10**6),\n            'West Bank and Gaza' : int(4.685 * 10**6),\n            'Canada_Northwest Territories': 44_826,\n            'Canada_Yukon' : 35_874,\n            'United Kingdom_Anguilla' : 15_094,\n            'United Kingdom_British Virgin Islands' : 35_802,\n            'United Kingdom_Turks and Caicos Islands' : 31_458\n           }\n\ntt['Population_final'] = tt['Population_final'].fillna(tt['Place'].map(pop_dict))\n\ntt.loc[tt['Place'] == 'Diamond Princess', 'Population final'] = 2_670\n\ntt['ConfirmedCases_Log'] = tt['ConfirmedCases'].apply(np.log1p)\ntt['Fatalities_Log'] = tt['Fatalities'].apply(np.log1p)\n\ntt['Population_final'] = tt['Population_final'].astype('int')\ntt['Cases_Per_100kPop'] = (tt['ConfirmedCases'] / tt['Population_final']) * 100000\ntt['Fatalities_Per_100kPop'] = (tt['Fatalities'] / tt['Population_final']) * 100000\n\ntt['Cases_Percent_Pop'] = ((tt['ConfirmedCases'] / tt['Population_final']) * 100)\ntt['Fatalities_Percent_Pop'] = ((tt['Fatalities'] / tt['Population_final']) * 100)\n\ntt['Cases_Log_Percent_Pop'] = ((tt['ConfirmedCases'] / tt['Population_final']) * 100).apply(np.log1p)\ntt['Fatalities_Log_Percent_Pop'] = ((tt['Fatalities'] / tt['Population_final']) * 100).apply(np.log1p)\n\n\ntt['Max_Confirmed_Cases'] = tt.groupby('Place')['ConfirmedCases'].transform(max)\ntt['Max_Fatalities'] = tt.groupby('Place')['Fatalities'].transform(max)\n\ntt['Max_Cases_Per_100kPop'] = tt.groupby('Place')['Cases_Per_100kPop'].transform(max)\ntt['Max_Fatalities_Per_100kPop'] = tt.groupby('Place')['Fatalities_Per_100kPop'].transform(max)\ntt.query('Date == @max_date') \\\n    .query('Place != \"Diamond Princess\"') \\\n    .query('Cases_Log_Percent_Pop > -10000') \\\n    ['Cases_Log_Percent_Pop'].plot(kind='hist', bins=500)\nplt.show()\nfig, ax1 = plt.subplots(figsize=(15, 5))\n\ntt.query('Days_Since_Ten_Cases > 0') \\\n    .query('Place != \"Diamond Princess\"') \\\n    .dropna(subset=['Cases_Percent_Pop']) \\\n    .query('Days_Since_Ten_Cases < 40') \\\n    .groupby('Place') \\\n    .plot(x='Days_Since_Ten_Cases',\n          y='Cases_Log_Percent_Pop',\n          style='.-',\n          figsize=(15, 5),\n          alpha=0.2,\n          ax=ax1,\n         title='Days since 10 Cases by Percent of Population with Cases')\nax1.get_legend().remove()\nplt.show()\n\nfig, ax2 = plt.subplots(figsize=(15, 5))\ntt.query('Days_Since_Ten_Fatal > 0') \\\n    .query('Place != \"Diamond Princess\"') \\\n    .dropna(subset=['Cases_Percent_Pop']) \\\n    .query('Days_Since_Ten_Fatal < 100') \\\n    .groupby('Place') \\\n    .plot(x='Days_Since_Ten_Fatal',\n          y='Cases_Log_Percent_Pop',\n          style='.-',\n          figsize=(15, 5),\n          alpha=0.2,\n         title='Days since 10 Fatailites by Percent of Population with Cases',\n         ax=ax2)\nax2.get_legend().remove()\nplt.show()\nPLOT = False\nif PLOT:\n    for x in tt['Place'].unique():\n        try:\n            fig, ax = plt.subplots(1, 4, figsize=(15, 2))\n            tt.query('Place == @x') \\\n                .query('ConfirmedCases > 0') \\\n                .set_index('Date')['Cases_Log_Percent_Pop'] \\\n                .plot(title=f'{x} confirmed log pct pop', ax=ax[0])\n            tt.query('Place == @x') \\\n                .query('ConfirmedCases > 0') \\\n                .set_index('Date')['Cases_Percent_Pop'] \\\n                .plot(title=f'{x} confirmed cases', ax=ax[1])\n            tt.query('Place == @x') \\\n                .query('Fatalities > 0') \\\n                .set_index('Date')['Fatalities_Log_Percent_Pop'] \\\n                .plot(title=f'{x} confirmed log pct pop', ax=ax[2])\n            tt.query('Place == @x') \\\n                .query('Fatalities > 0') \\\n                .set_index('Date')['Fatalities_Percent_Pop'] \\\n                .plot(title=f'{x} confirmed cases', ax=ax[3])\n        except:\n            pass\n        plt.show()\ntt.query('Date == @max_date')[['Place','Max_Cases_Per_100kPop',\n                               'Max_Fatalities_Per_100kPop','Max_Confirmed_Cases',\n                               'Population_final',\n                              'Days_Since_First_Case',\n                              'Confirmed_Cases_Diff']] \\\n    .drop_duplicates() \\\n    .sort_values('Max_Cases_Per_100kPop', ascending=False)", "processed": ["data preprocess http www kaggl com osciiart covid19 lightgbm"]}, {"markdown": ["# Features about cases since first case/ fatality"], "code": "# Reference: https://www.kaggle.com/code/robikscube/rob-s-covid19-week-3-submission\n\ntt['Past_7Days_ConfirmedCases_Std'] = tt['Place'].map(tt.dropna(subset=['ConfirmedCases']).query('Date >= 20200324').groupby('Place')['ConfirmedCases'].std().to_dict())\ntt['Past_7Days_Fatalities_Std'] = tt['Place'].map(tt.dropna(subset=['ConfirmedCases']).query('Date >= 20200324').groupby('Place')['Fatalities'].std().to_dict())\n\ntt['Past_7Days_ConfirmedCases_Min'] = tt['Place'].map(tt.dropna(subset=['ConfirmedCases']).query('Date >= 20200324').groupby('Place')['ConfirmedCases'].min().to_dict())\ntt['Past_7Days_Fatalities_Min'] = tt['Place'].map(tt.dropna(subset=['Fatalities']).query('Date >= 20200324').groupby('Place')['Fatalities'].min().to_dict())\n\ntt['Past_7Days_ConfirmedCases_Max'] = tt['Place'].map(tt.dropna(subset=['ConfirmedCases']).query('Date >= 20200324').groupby('Place')['ConfirmedCases'].max().to_dict())\ntt['Past_7Days_Fatalities_Max'] = tt['Place'].map(tt.dropna(subset=['Fatalities']).query('Date >= 20200324').groupby('Place')['Fatalities'].max().to_dict())\n\ntt['Past_7Days_Confirmed_Change_of_Total'] = (tt['Past_7Days_ConfirmedCases_Max'] - tt['Past_7Days_ConfirmedCases_Min']) / (tt['Past_7Days_ConfirmedCases_Max'])\ntt['Past_7Days_Fatalities_Change_of_Total'] = (tt['Past_7Days_Fatalities_Max'] - tt['Past_7Days_Fatalities_Min']) / (tt['Past_7Days_Fatalities_Max'])\ntt['Past_21Days_ConfirmedCases_Std'] = tt['Place'].map(tt.dropna(subset=['ConfirmedCases']).query('Date >= 20200310').groupby('Place')['ConfirmedCases'].std().to_dict())\ntt['Past_21Days_Fatalities_Std'] = tt['Place'].map(tt.dropna(subset=['ConfirmedCases']).query('Date >= 20200310').groupby('Place')['Fatalities'].std().to_dict())\n\ntt['Past_21Days_ConfirmedCases_Min'] = tt['Place'].map(tt.dropna(subset=['ConfirmedCases']).query('Date >= 20200310').groupby('Place')['ConfirmedCases'].min().to_dict())\ntt['Past_21Days_Fatalities_Min'] = tt['Place'].map(tt.dropna(subset=['Fatalities']).query('Date >= 20200324').groupby('Place')['Fatalities'].min().to_dict())\n\ntt['Past_21Days_ConfirmedCases_Max'] = tt['Place'].map(tt.dropna(subset=['ConfirmedCases']).query('Date >= 20200310').groupby('Place')['ConfirmedCases'].max().to_dict())\ntt['Past_21Days_Fatalities_Max'] = tt['Place'].map(tt.dropna(subset=['Fatalities']).query('Date >= 20200310').groupby('Place')['Fatalities'].max().to_dict())\n\ntt['Past_21Days_Confirmed_Change_of_Total'] = (tt['Past_21Days_ConfirmedCases_Max'] - tt['Past_21Days_ConfirmedCases_Min']) / (tt['Past_21Days_ConfirmedCases_Max'])\ntt['Past_21Days_Fatalities_Change_of_Total'] = (tt['Past_21Days_Fatalities_Max'] - tt['Past_21Days_Fatalities_Min']) / (tt['Past_21Days_Fatalities_Max'])\n\ntt['Past_7Days_Fatalities_Change_of_Total'] = tt['Past_7Days_Fatalities_Change_of_Total'].fillna(0)\ntt['Past_21Days_Fatalities_Change_of_Total'] = tt['Past_21Days_Fatalities_Change_of_Total'].fillna(0)\ntt['Date_7Days_Since_First_Case'] = tt['Place'].map(tt.loc[tt['Days_Since_First_Case'] == 7] \\\n    .set_index('Place')['Date'] \\\n    .to_dict())\ntt['Date_14Days_Since_First_Case'] = tt['Place'].map(tt.loc[tt['Days_Since_First_Case'] == 14] \\\n    .set_index('Place')['Date'] \\\n    .to_dict())\ntt['Date_21Days_Since_First_Case'] = tt['Place'].map(tt.loc[tt['Days_Since_First_Case'] == 21] \\\n    .set_index('Place')['Date'] \\\n    .to_dict())\ntt['Date_28Days_Since_First_Case'] = tt['Place'].map(tt.loc[tt['Days_Since_First_Case'] == 28] \\\n    .set_index('Place')['Date'] \\\n    .to_dict())\ntt['Date_35Days_Since_First_Case'] = tt['Place'].map(tt.loc[tt['Days_Since_First_Case'] == 35] \\\n    .set_index('Place')['Date'] \\\n    .to_dict())\ntt['Date_60Days_Since_First_Case'] = tt['Place'].map(tt.loc[tt['Days_Since_First_Case'] == 60] \\\n    .set_index('Place')['Date'] \\\n    .to_dict())\n\ntt['Date_7Days_Since_Ten_Cases'] = tt['Place'].map(tt.loc[tt['Days_Since_Ten_Cases'] == 7] \\\n    .set_index('Place')['Date'] \\\n    .to_dict())\ntt['Date_14Days_Since_Ten_Cases'] = tt['Place'].map(tt.loc[tt['Days_Since_Ten_Cases'] == 14] \\\n    .set_index('Place')['Date'] \\\n    .to_dict())\ntt['Date_21Days_Since_Ten_Cases'] = tt['Place'].map(tt.loc[tt['Days_Since_Ten_Cases'] == 21] \\\n    .set_index('Place')['Date'] \\\n    .to_dict())\ntt['Date_28Days_Since_Ten_Cases'] = tt['Place'].map(tt.loc[tt['Days_Since_Ten_Cases'] == 28] \\\n    .set_index('Place')['Date'] \\\n    .to_dict())\ntt['Date_35Days_Since_Ten_Cases'] = tt['Place'].map(tt.loc[tt['Days_Since_Ten_Cases'] == 35] \\\n    .set_index('Place')['Date'] \\\n    .to_dict())\ntt['Date_60Days_Since_Ten_Cases'] = tt['Place'].map(tt.loc[tt['Days_Since_Ten_Cases'] == 60] \\\n    .set_index('Place')['Date'] \\\n    .to_dict())\n\n\ntt['Date_7Days_Since_First_Fatal'] = tt['Place'].map(tt.loc[tt['Days_Since_First_Fatal'] == 7] \\\n    .set_index('Place')['Date'] \\\n    .to_dict())\ntt['Date_14Days_Since_First_Fatal'] = tt['Place'].map(tt.loc[tt['Days_Since_First_Fatal'] == 14] \\\n    .set_index('Place')['Date'] \\\n    .to_dict())\ntt['Date_21Days_Since_First_Fatal'] = tt['Place'].map(tt.loc[tt['Days_Since_First_Fatal'] == 21] \\\n    .set_index('Place')['Date'] \\\n    .to_dict())\ntt['Date_28Days_Since_First_Fatal'] = tt['Place'].map(tt.loc[tt['Days_Since_First_Fatal'] == 28] \\\n    .set_index('Place')['Date'] \\\n    .to_dict())\ntt['Date_35Days_Since_First_Fatal'] = tt['Place'].map(tt.loc[tt['Days_Since_First_Fatal'] == 35] \\\n    .set_index('Place')['Date'] \\\n    .to_dict())\ntt['Date_60Days_Since_First_Fatal'] = tt['Place'].map(tt.loc[tt['Days_Since_First_Fatal'] == 60] \\\n    .set_index('Place')['Date'] \\\n    .to_dict())\ntt['CC_7D_1stCase'] = tt.loc[tt['Date_7Days_Since_First_Case'] == tt['Date']]['ConfirmedCases']\ntt['CC_14D_1stCase'] = tt.loc[tt['Date_14Days_Since_First_Case'] == tt['Date']]['ConfirmedCases']\ntt['CC_21D_1stCase'] = tt.loc[tt['Date_21Days_Since_First_Case'] == tt['Date']]['ConfirmedCases']\ntt['CC_28D_1stCase'] = tt.loc[tt['Date_28Days_Since_First_Case'] == tt['Date']]['ConfirmedCases']\ntt['CC_35D_1stCase'] = tt.loc[tt['Date_35Days_Since_First_Case'] == tt['Date']]['ConfirmedCases']\ntt['CC_60D_1stCase'] = tt.loc[tt['Date_60Days_Since_First_Case'] == tt['Date']]['ConfirmedCases']\n\ntt['F_7D_1stCase'] = tt.loc[tt['Date_7Days_Since_First_Case'] == tt['Date']]['Fatalities']\ntt['F_14D_1stCase'] = tt.loc[tt['Date_14Days_Since_First_Case'] == tt['Date']]['Fatalities']\ntt['F_21D_1stCase'] = tt.loc[tt['Date_21Days_Since_First_Case'] == tt['Date']]['Fatalities']\ntt['F_28D_1stCase'] = tt.loc[tt['Date_28Days_Since_First_Case'] == tt['Date']]['Fatalities']\ntt['F_35D_1stCase'] = tt.loc[tt['Date_35Days_Since_First_Case'] == tt['Date']]['Fatalities']\ntt['F_60D_1stCase'] = tt.loc[tt['Date_60Days_Since_First_Case'] == tt['Date']]['Fatalities']\n\ntt['CC_7D_10Case'] = tt.loc[tt['Date_7Days_Since_Ten_Cases'] == tt['Date']]['ConfirmedCases']\ntt['CC_14D_10Case'] = tt.loc[tt['Date_14Days_Since_Ten_Cases'] == tt['Date']]['ConfirmedCases']\ntt['CC_21D_10Case'] = tt.loc[tt['Date_21Days_Since_Ten_Cases'] == tt['Date']]['ConfirmedCases']\ntt['CC_28D_10Case'] = tt.loc[tt['Date_28Days_Since_Ten_Cases'] == tt['Date']]['ConfirmedCases']\ntt['CC_35D_10Case'] = tt.loc[tt['Date_35Days_Since_Ten_Cases'] == tt['Date']]['ConfirmedCases']\ntt['CC_60D_10Case'] = tt.loc[tt['Date_60Days_Since_Ten_Cases'] == tt['Date']]['ConfirmedCases']\n\ntt['F_7D_10Case'] = tt.loc[tt['Date_7Days_Since_Ten_Cases'] == tt['Date']]['Fatalities']\ntt['F_14D_10Case'] = tt.loc[tt['Date_14Days_Since_Ten_Cases'] == tt['Date']]['Fatalities']\ntt['F_21D_10Case'] = tt.loc[tt['Date_21Days_Since_Ten_Cases'] == tt['Date']]['Fatalities']\ntt['F_28D_10Case'] = tt.loc[tt['Date_28Days_Since_Ten_Cases'] == tt['Date']]['Fatalities']\ntt['F_35D_10Case'] = tt.loc[tt['Date_35Days_Since_Ten_Cases'] == tt['Date']]['Fatalities']\ntt['F_60D_10Case'] = tt.loc[tt['Date_60Days_Since_Ten_Cases'] == tt['Date']]['Fatalities']\n\ntt['CC_7D_1Fatal'] = tt.loc[tt['Date_7Days_Since_First_Fatal'] == tt['Date']]['ConfirmedCases']\ntt['CC_14D_1Fatal'] = tt.loc[tt['Date_14Days_Since_First_Fatal'] == tt['Date']]['ConfirmedCases']\ntt['CC_21D_1Fatal'] = tt.loc[tt['Date_21Days_Since_First_Fatal'] == tt['Date']]['ConfirmedCases']\ntt['CC_28D_1Fatal'] = tt.loc[tt['Date_28Days_Since_First_Fatal'] == tt['Date']]['ConfirmedCases']\ntt['CC_35D_1Fatal'] = tt.loc[tt['Date_35Days_Since_First_Fatal'] == tt['Date']]['ConfirmedCases']\ntt['CC_60D_1Fatal'] = tt.loc[tt['Date_60Days_Since_First_Fatal'] == tt['Date']]['ConfirmedCases']\n\ntt['F_7D_1Fatal'] = tt.loc[tt['Date_7Days_Since_First_Fatal'] == tt['Date']]['Fatalities']\ntt['F_14D_1Fatal'] = tt.loc[tt['Date_14Days_Since_First_Fatal'] == tt['Date']]['Fatalities']\ntt['F_21D_1Fatal'] = tt.loc[tt['Date_21Days_Since_First_Fatal'] == tt['Date']]['Fatalities']\ntt['F_28D_1Fatal'] = tt.loc[tt['Date_28Days_Since_First_Fatal'] == tt['Date']]['Fatalities']\ntt['F_35D_1Fatal'] = tt.loc[tt['Date_35Days_Since_First_Fatal'] == tt['Date']]['Fatalities']\ntt['F_60D_1Fatal'] = tt.loc[tt['Date_60Days_Since_First_Fatal'] == tt['Date']]['Fatalities']\nfig, axs = plt.subplots(1, 2, figsize=(15, 5))\ntt[['Place','Past_7Days_Confirmed_Change_of_Total','Past_7Days_Fatalities_Change_of_Total',\n    'Past_7Days_ConfirmedCases_Max','Past_7Days_ConfirmedCases_Min',\n   'Past_7Days_Fatalities_Max','Past_7Days_Fatalities_Min']] \\\n    .drop_duplicates() \\\n    .sort_values('Past_7Days_Confirmed_Change_of_Total')['Past_7Days_Confirmed_Change_of_Total'] \\\n    .plot(kind='hist', bins=50, title='Distribution of Pct change confirmed past 7 days', ax=axs[0])\ntt[['Place','Past_21Days_Confirmed_Change_of_Total','Past_21Days_Fatalities_Change_of_Total',\n    'Past_21Days_ConfirmedCases_Max','Past_21Days_ConfirmedCases_Min',\n   'Past_21Days_Fatalities_Max','Past_21Days_Fatalities_Min']] \\\n    .drop_duplicates() \\\n    .sort_values('Past_21Days_Confirmed_Change_of_Total')['Past_21Days_Confirmed_Change_of_Total'] \\\n    .plot(kind='hist', bins=50, title='Distribution of Pct change confirmed past 21 days', ax=axs[1])\nplt.show()\n\nfig, axs = plt.subplots(1, 2, figsize=(15, 5))\ntt[['Place','Past_7Days_Fatalities_Change_of_Total']] \\\n    .drop_duplicates()['Past_7Days_Fatalities_Change_of_Total'] \\\n    .plot(kind='hist', bins=50, title='Distribution of Pct change confirmed past 7 days', ax=axs[0])\ntt[['Place', 'Past_21Days_Fatalities_Change_of_Total']] \\\n    .drop_duplicates()['Past_21Days_Fatalities_Change_of_Total'] \\\n    .plot(kind='hist', bins=50, title='Distribution of Pct change confirmed past 21 days', ax=axs[1])\nplt.show()\ntt.head()\n# Example of flat prop\ntt.query(\"Place == 'China_Chongqing'\").set_index('Date')['ConfirmedCases'].dropna().plot(figsize=(15, 5))\nplt.show()", "processed": ["featur case sinc first case fatal"]}, {"markdown": ["# Other Data Prep"], "code": "# Reference: https://www.kaggle.com/code/robikscube/rob-s-covid19-week-3-submission\n\ntt.query(\"Place == 'Italy'\").set_index('Date')[['ConfirmedCases']] \\\n    .dropna().plot(figsize=(15, 5), title='Italy Confirmed Cases')\nplt.show()\ntt.query(\"Place == 'Italy'\").set_index('Date')[['ConfirmedCases_Log']] \\\n    .dropna().plot(figsize=(15, 5), title='Italy Fatalities')\nplt.show()\nlatest_summary_stats = tt.query('Date == @max_date') \\\n    [['Country_Region',\n      'Place',\n      'Max_Cases_Per_100kPop',\n      'Max_Fatalities_Per_100kPop',\n      'Max_Confirmed_Cases',\n      'Population_final',\n      'Days_Since_First_Case',\n      'Days_Since_Ten_Cases']] \\\n    .drop_duplicates()\ntt.query('Province_State == \"Maryland\"').set_index('Date') \\\n    [['ConfirmedCases','Confirmed_Cases_Diff']].plot(figsize=(15,5 ))\ntt['ConfirmedCasesRolling2'] = tt.groupby('Place')['ConfirmedCases'].rolling(2, center=True).mean().values\ntt['FatalitiesRolling2'] = tt.groupby('Place')['Fatalities'].rolling(2, center=True).mean().values\ntrain = tt.loc[~tt['ConfirmedCases'].isna()].query('Days_Since_First_Case > 0')\n\nTARGET = 'ConfirmedCasesRolling2'\n\n# LightGBM is no bueno\n\n# import lightgbm as lgb\n\n# SEED = 529\n# params = {'num_leaves': 8,\n#           'min_data_in_leaf': 5,  # 42,\n#           'objective': 'regression',\n#           'max_depth': 2,\n#           'learning_rate': 0.02,\n# #           'boosting': 'gbdt',\n#           'bagging_freq': 5,  # 5\n#           'bagging_fraction': 0.8,  # 0.5,\n#           'feature_fraction': 0.82,\n#           'bagging_seed': SEED,\n#           'reg_alpha': 1,  # 1.728910519108444,\n#           'reg_lambda': 4.98,\n#           'random_state': SEED,\n#           'metric': 'mse',\n#           'verbosity': 100,\n#           'min_gain_to_split': 0.02,  # 0.01077313523861969,\n#           'min_child_weight': 5,  # 19.428902804238373,\n#           'num_threads': 6,\n#           }\n\n# model = lgb.LGBMRegressor(**params, n_estimators=5000)\n# model.fit(train[FEATURES],\n#           train[TARGET])\n# model.feature_importances_\ntt['Date'].min()\ntt['doy'] = tt['Date'].dt.dayofyear\n\n# test = tt.loc[~tt['ForecastId'].isna()]\n# preds = model.predict(test[FEATURES])\n# tt.loc[~tt['ForecastId'].isna(),\n#        'Confirmed_Cases_Diff_Pred'] = preds\n# # tt['ConfirmedCases_Pred'] = tt['ConfirmedCases_Log_Pred'].apply(np.expm1)", "processed": ["data prep"]}, {"markdown": ["# Sigmoid"], "code": "# Reference: https://www.kaggle.com/code/robikscube/rob-s-covid19-week-3-submission\n\nimport scipy.optimize as opt\n\ndef sigmoid(t, M, beta, alpha):\n    return M / (1 + np.exp(-beta * (t - alpha)))\n\n\nfor myplace in tt['Place'].unique():\n\n\n    pop = tt.loc[tt['Place'] == myplace]['Population_final'].values[0]\n\n    BOUNDS=(0, [pop, 2.0, 100])\n\n    xin = tt.query('Place == @myplace').dropna(subset=['ConfirmedCases'])['doy'].values\n    yin = tt.query('Place == @myplace').dropna(subset=['ConfirmedCases'])['ConfirmedCases'].values\n    popt, pcov = opt.curve_fit(sigmoid,\n                               xin,\n                               yin,\n                               bounds=BOUNDS)\n\n    M, beta, alpha = popt\n    print(M, beta, alpha)\n    x = tt.loc[tt['Place'] == myplace]['doy'].values\n    tt.loc[tt['Place'] == myplace, 'ConfirmedCases_forecast'] = sigmoid(x, M, beta, alpha)\n\n    xin = tt.query('Place == @myplace').dropna(subset=['Fatalities'])['doy'].values\n    yin = tt.query('Place == @myplace').dropna(subset=['Fatalities'])['Fatalities'].values\n    popt, pcov = opt.curve_fit(sigmoid,\n                               xin,\n                               yin,\n                               bounds=BOUNDS)\n\n    M, beta, alpha = popt\n    print(M, beta, alpha)\n    x = tt.loc[tt['Place'] == myplace]['doy'].values\n    tt.loc[tt['Place'] == myplace, 'Fatalities_forecast'] = sigmoid(x, M, beta, alpha)\n\n    fig, axs = plt.subplots(1, 2, figsize=(15, 3))\n    ax = tt.query('Place == @myplace').set_index('Date')[['ConfirmedCases','ConfirmedCases_forecast']].plot(title=myplace, ax=axs[0])\n    ax = tt.query('Place == @myplace').set_index('Date')[['Fatalities','Fatalities_forecast']].plot(title=myplace, ax=axs[1])\n    plt.show()", "processed": ["sigmoid"]}, {"markdown": ["# Linear Regression"], "code": "# Reference: https://www.kaggle.com/code/robikscube/rob-s-covid19-week-3-submission\n\nfrom sklearn.linear_model import LinearRegression, ElasticNet\n\nfor myplace in tt['Place'].unique():\n    try:\n        # Confirmed Cases\n        fig, axs = plt.subplots(1, 2, figsize=(15, 3))\n        dat = tt.query('Place == @myplace and Days_Since_Ten_Cases >= 0')[['Days_Since_Ten_Cases','ConfirmedCases_Log']].dropna()\n        X = dat['Days_Since_Ten_Cases']\n        y = dat['ConfirmedCases_Log']\n        y = y.cummax()\n        dat_all = tt.query('Place == @myplace and Days_Since_Ten_Cases >= 0')[['Days_Since_Ten_Cases','ConfirmedCases_Log']]\n        X_pred = dat_all['Days_Since_Ten_Cases']\n        en = ElasticNet()\n        en.fit(X.values.reshape(-1, 1), y.values)\n        preds = en.predict(X_pred.values.reshape(-1, 1))\n        tt.loc[(tt['Place'] == myplace) & (tt['Days_Since_Ten_Cases'] >= 0), 'ConfirmedCases_Log_Pred1'] = preds\n        tt.loc[(tt['Place'] == myplace), 'ConfirmedCases_Pred1'] = tt['ConfirmedCases_Log_Pred1'].apply(np.expm1)\n        # Cap at 10 % Population\n        pop_myplace = tt.query('Place == @myplace')['Population_final'].values[0]\n        tt.loc[(tt['Place'] == myplace) & (tt['ConfirmedCases_Pred1'] > (0.05 * pop_myplace)), 'ConfirmedCases_Pred1'] = (0.05 * pop_myplace)\n        ax = tt.query('Place == @myplace').set_index('Date')[['ConfirmedCases','ConfirmedCases_Pred1']].plot(figsize=(15, 5), title=myplace, ax=axs[0])\n        # Fatalities\n        # If low count then do percent of confirmed:\n        dat = tt.query('Place == @myplace and Days_Since_Ten_Cases >= 0')[['Days_Since_Ten_Cases','Fatalities_Log']].dropna()\n        if len(dat) < 5:\n            tt.loc[(tt['Place'] == myplace), 'Fatalities_Pred1'] = tt.loc[(tt['Place'] == myplace)]['ConfirmedCases_Pred1'] * 0.0001\n        elif tt.query('Place == @myplace')['Fatalities'].max() < 5:\n            tt.loc[(tt['Place'] == myplace), 'Fatalities_Pred1'] = tt.loc[(tt['Place'] == myplace)]['ConfirmedCases_Pred1'] * 0.0001\n        else:\n            X = dat['Days_Since_Ten_Cases']\n            y = dat['Fatalities_Log']\n            y = y.cummax()\n            dat_all = tt.query('Place == @myplace and Days_Since_Ten_Cases >= 0')[['Days_Since_Ten_Cases','Fatalities_Log']]\n            X_pred = dat_all['Days_Since_Ten_Cases']\n            en = ElasticNet()\n            en.fit(X.values.reshape(-1, 1), y.values)\n            preds = en.predict(X_pred.values.reshape(-1, 1))\n            tt.loc[(tt['Place'] == myplace) & (tt['Days_Since_Ten_Cases'] >= 0), 'Fatalities_Log_Pred1'] = preds\n            tt.loc[(tt['Place'] == myplace), 'Fatalities_Pred1'] = tt['Fatalities_Log_Pred1'].apply(np.expm1)\n\n            # Cap at 0.0001 Population\n            pop_myplace = tt.query('Place == @myplace')['Population_final'].values[0]\n            tt.loc[(tt['Place'] == myplace) & (tt['Fatalities_Pred1'] > (0.0001 * pop_myplace)), 'Fatalities_Pred1'] = (0.0001 * pop_myplace)\n\n        ax = tt.query('Place == @myplace').set_index('Date')[['Fatalities','Fatalities_Pred1']].plot(figsize=(15, 5), title=myplace, ax=axs[1])\n        plt.show()\n    except:\n        print(f'============= FAILED FOR {myplace} =============')\n", "processed": ["linear regress"]}, {"markdown": ["# Deal with Flattened location"], "code": "# Reference: https://www.kaggle.com/code/robikscube/rob-s-covid19-week-3-submission\n\nconstant_fatal_places\ntt.loc[tt['Place'].isin(constant_fatal_places), 'ConfirmedCases_Pred1'] = tt.loc[tt['Place'].isin(constant_fatal_places)]['Place'].map(tt.loc[tt['Place'].isin(constant_fatal_places)].groupby('Place')['ConfirmedCases'].max())\ntt.loc[tt['Place'].isin(constant_fatal_places), 'Fatalities_Pred1'] = tt.loc[tt['Place'].isin(constant_fatal_places)]['Place'].map(tt.loc[tt['Place'].isin(constant_fatal_places)].groupby('Place')['Fatalities'].max())\nfor myplace in constant_fatal_places:\n    fig, axs = plt.subplots(1, 2, figsize=(15, 3))\n    ax = tt.query('Place == @myplace').set_index('Date')[['ConfirmedCases','ConfirmedCases_Pred1']].plot(figsize=(15, 5), title=myplace, ax=axs[0])\n    ax = tt.query('Place == @myplace').set_index('Date')[['Fatalities','Fatalities_Pred1']].plot(figsize=(15, 5), title=myplace, ax=axs[1])\n    plt.show()", "processed": ["deal flatten locat"]}, {"markdown": ["# Plot them all!!"], "code": "# Reference: https://www.kaggle.com/code/robikscube/rob-s-covid19-week-3-submission\n\nfor myplace in tt['Place'].unique():\n    try:\n        # Confirmed Cases\n        fig, axs = plt.subplots(1, 2, figsize=(15, 3))\n        ax = tt.query('Place == @myplace').set_index('Date')[['ConfirmedCases','ConfirmedCases_Pred']].plot(title=myplace, ax=axs[0])\n        ax = tt.query('Place == @myplace').set_index('Date')[['Fatalities','Fatalities_Pred']].plot(title=myplace, ax=axs[1])\n        plt.show()\n    except:\n        print(f'============= FAILED FOR {myplace} =============')\ntt.groupby('Place')['Fatalities_Pred'].max().sort_values()", "processed": ["plot"]}, {"markdown": ["# <a id=\"4\">Data exploration</a>\n\n\n## <a id=\"41\">Class distribution</a>\n\nLet's inspect the train data to check the **cat**/**dog** distribution.   We show first the split in the reduced train data."], "code": "# Reference: https://www.kaggle.com/code/gpreda/cats-or-dogs-using-cnn-with-transfer-learning\n\ndef plot_image_list_count(data_image_list):\n    labels = []\n    for img in data_image_list:\n        labels.append(img.split('.')[-3])\n    sns.countplot(labels)\n    plt.title('Cats and Dogs')\n    \nplot_image_list_count(train_image_list)", "processed": ["id 4 data explor id 41 class distribut let inspect train data check cat dog distribut show first split reduc train data"]}, {"markdown": ["## <a id=\"53\">Validation accuracy and loss</a>\n\nLet's show the train and validation accuracy on the same plot. As well, we will represent the train and validation loss on the same graph."], "code": "# Reference: https://www.kaggle.com/code/gpreda/cats-or-dogs-using-cnn-with-transfer-learning\n\ndef plot_accuracy_and_loss(train_model):\n    hist = train_model.history\n    acc = hist['acc']\n    val_acc = hist['val_acc']\n    loss = hist['loss']\n    val_loss = hist['val_loss']\n    epochs = range(len(acc))\n    f, ax = plt.subplots(1,2, figsize=(14,6))\n    ax[0].plot(epochs, acc, 'g', label='Training accuracy')\n    ax[0].plot(epochs, val_acc, 'r', label='Validation accuracy')\n    ax[0].set_title('Training and validation accuracy')\n    ax[0].legend()\n    ax[1].plot(epochs, loss, 'g', label='Training loss')\n    ax[1].plot(epochs, val_loss, 'r', label='Validation loss')\n    ax[1].set_title('Training and validation loss')\n    ax[1].legend()\n    plt.show()\nplot_accuracy_and_loss(train_model)", "processed": ["id 53 valid accuraci loss let show train valid accuraci plot well repres train valid loss graph"]}, {"markdown": ["# Create the Fastai resnet50 model\n- Make sure you have GPU turned on or this will be very slow.\n- We are using mean_absolute_error as our evaluation metric"], "code": "# Reference: https://www.kaggle.com/code/robikscube/lanl-earthquake-melspectrogram-images-fastai-nn\n\ndef mean_absolute_error(pred:Tensor, targ:Tensor)->Rank0Tensor:\n    \"Mean absolute error between `pred` and `targ`.\"\n    pred,targ = flatten_check(pred,targ)\n    return torch.abs(targ - pred).mean()\n\nlearn = cnn_learner(data, models.resnet50, metrics=mean_absolute_error)\nlearn.fit_one_cycle(4, 0.01)\n# Plot train vs valid loss\nfig = learn.recorder.plot_losses(return_fig=True)\nfig.set_size_inches(15,5)\n# Unfreeze the model and search for a good learning rate\nlearn.unfreeze()\nlearn.lr_find()\nfig = learn.recorder.plot(return_fig=True)\nfig.set_size_inches(15,5)\nlearn.fit_one_cycle(2, slice(1e-6, 3e-3/10))\nlearn.save('cnn-step1')\n# Export the model\nlearn.export()\n# We can see there is now an export.pkl file that we've saved\n!ls -l", "processed": ["creat fastai resnet50 model make sure gpu turn slow use mean absolut error evalu metric"]}, {"markdown": ["Validation MAE ~ 2.08 isn't that great and we are starting to overfit. Using a better cross validation technique may be necessary.\n\nI'm sure you can do better than I have here, but this should be enough to get you started.", "# Predicting on the test set\n- We load our sample submission file and create an imagelist based off of the seg_id\n- We point this imagelist at the images we've creates for the test set.\n- Load the trained model and call the prediction method on this imagelist."], "code": "# Reference: https://www.kaggle.com/code/robikscube/lanl-earthquake-melspectrogram-images-fastai-nn\n\nss = pd.read_csv('../input/LANL-Earthquake-Prediction/sample_submission.csv')\ntest = ImageList.from_df(ss, '../input/lanl-earthquake-spectrogram-images/test_images/test_images_v3/', cols='seg_id', suffix='.png')\nlearn = load_learner('./', test=test)\nlearn.load('cnn-step1')\npreds = learn.get_preds(ds_type=DatasetType.Test)\n# Save the time to failure\nss['time_to_failure'] = [float(x) for x in preds[0]]\nss.head()\n# Cap the minimum and maximum time to failure values\nss.loc[ss['time_to_failure'] < 0, 'time_to_failure'] = 0\nss.loc[ss['time_to_failure'] > 12, 'time_to_failure'] = 12\nss.plot(kind='hist', bins=100, figsize=(15, 5), title='Distribution of predictions on the Test Set')\nplt.show()\n# Save our predictions\nss.to_csv('submission.csv', index=False)", "processed": ["valid mae 2 08 great start overfit use better cross valid techniqu may necessari sure better enough get start", "predict test set load sampl submiss file creat imagelist base seg id point imagelist imag creat test set load train model call predict method imagelist"]}, {"markdown": ["# Gold Medal Paths\n(Missing the team \"Young for you\" because their name must have changed from the data I have.\n\nUpdating to incluse the paths of the gold medal \ud83e\udd47 teams on the private leaderboard!"], "code": "# Reference: https://www.kaggle.com/code/robikscube/the-race-to-detect-fraud-final-lb\n\n# Missing \"Young for you\"\ngold_teams = ['FraudSquad','2 uncles and 3 puppies','T.U.V', 'Lions','The Zoo',\n              'Grand Rookie(Done!Good luck to everyone!)\uff09', 'S.A.R.M.A', 'AlKo', 'M5',\n              'Flying Whales','\u90a6\u76db\u79d1\u6280\u5c0f\u5206\u961f','YuyaYamamoto','MaYa','Mr Lonely \u266c','spongebob',\n              'Taemyung Heo','conundrum.ai & Evgeny', '\u30af\u30bd\u30b6\u30b3\u3061\u3083\u3046\u306d\u3093','Our AUC says nothing to us',\n              'bird and fish', '\u5929\u884c\u5065,\u541b\u5b50\u4ee5\u81ea\u5f3a\u4e0d\u606f \u5730\u52bf\u5764,\u541b\u5b50\u4ee5\u539a\u5fb7\u8f7d\u7269']\n\ngold_df = df[gold_teams]\ngold_scores = [0.945884, 0.944210, 0.942580, 0.942453, 0.942391, 0.942314, 0.942268, 0.942129, 0.941750,\n              0.941638, 0.941413, 0.941338, 0.941153, 0.941096, 0.941011, 0.940934, 0.940756, 0.940730,\n              0.940526, 0.940250, 0.940076]\n\ngold_scores_df = pd.DataFrame(index=gold_teams,\n                             data=gold_scores,\n                             columns=['Private Score'])\n# Interative Plotly\nmypal = cl.scales['9']['div']['Spectral']\ncolors = cl.interp( mypal, 21 )\nannotations = []\ninit_notebook_mode(connected=True)\nTOP_TEAMS = df.max().loc[df.max() > FIFTEENTH_SCORE].index.values\ndf_filtered = gold_df.ffill()\ndf_filtered = df_filtered.iloc[df_filtered.index >= '08-01-2019']\nteam_ordered = df_filtered.max(axis=0) \\\n    .sort_values(ascending=False).index.tolist()\n\ndata = []\ni = 0\nfor col in df_filtered[team_ordered].columns:\n    data.append(go.Scatter(\n                        x = df_filtered.index,\n                        y = df_filtered[col],\n                        name=col,\n                        line=dict(color=colors[i], width=2),)\n               )\n    i += 1\n\nannotations.append(dict(xref='paper', yref='paper', x=0.0, y=1.05,\n                              xanchor='left', yanchor='bottom',\n                              text='Gold Medal Teams Private LB Journey',\n                              font=dict(family='Arial',\n                                        size=30,\n                                        color='rgb(37,37,37)'),\n                              showarrow=False))\n\nlayout = go.Layout(yaxis=dict(range=[0.945, TOP_SCORE+0.001]),\n                   hovermode='x',\n                   plot_bgcolor='white',\n                  annotations=annotations,\n                  )\nfig = go.Figure(data=data, layout=layout)\nfig.update_layout(\n    legend=go.layout.Legend(\n        traceorder=\"normal\",\n        font=dict(\n            family=\"sans-serif\",\n            size=12,\n            color=\"black\"\n        ),\n        bgcolor=\"LightSteelBlue\",\n        bordercolor=\"Black\",\n        borderwidth=2,\n    )\n)\n\nfig.update_layout(legend_orientation=\"h\")\nfig.update_layout(template=\"plotly_white\")\n#fig.update_yaxes(showgrid=True, gridwidth=0.5, gridcolor='LightGrey')\nfig.update_xaxes(showgrid=False)\n\niplot(fig)\ngold_scores_df.sort_values('Private Score', ascending=True). \\\n    plot(kind='barh',\n         xlim=(0.94, 0.946),\n         figsize=(15, 10),\n         title='Final Private Board Scores of Gold Teams',\n         color='lightgoldenrodyellow')\nplt.show()", "processed": ["gold medal path miss team young name must chang data updat inclus path gold medal team privat leaderboard"]}, {"markdown": ["# Top Public LB Scores over time"], "code": "# Reference: https://www.kaggle.com/code/robikscube/the-race-to-detect-fraud-final-lb\n\n# Interative Plotly\nmypal = cl.scales['9']['div']['Spectral']\ncolors = cl.interp( mypal, 15 )\nannotations = []\ninit_notebook_mode(connected=True)\nTOP_TEAMS = df.max().loc[df.max() > FIFTEENTH_SCORE].index.values\ndf_filtered = df[TOP_TEAMS].ffill()\ndf_filtered = df_filtered.iloc[df_filtered.index >= '08-01-2019']\nteam_ordered = df_filtered.max(axis=0) \\\n    .sort_values(ascending=False).index.tolist()\n\ndata = []\ni = 0\nfor col in df_filtered[team_ordered].columns:\n    data.append(go.Scatter(\n                        x = df_filtered.index,\n                        y = df_filtered[col],\n                        name=col,\n                        line=dict(color=colors[i], width=2),)\n               )\n    i += 1\n\nannotations.append(dict(xref='paper', yref='paper', x=0.0, y=1.05,\n                              xanchor='left', yanchor='bottom',\n                              text='IEEE Fraud Detection Leaderboard Tracking',\n                              font=dict(family='Arial',\n                                        size=30,\n                                        color='rgb(37,37,37)'),\n                              showarrow=False))\n\nlayout = go.Layout(yaxis=dict(range=[0.945, TOP_SCORE+0.001]),\n                   hovermode='x',\n                   plot_bgcolor='white',\n                  annotations=annotations,\n                  )\nfig = go.Figure(data=data, layout=layout)\nfig.update_layout(\n    legend=go.layout.Legend(\n        traceorder=\"normal\",\n        font=dict(\n            family=\"sans-serif\",\n            size=12,\n            color=\"black\"\n        ),\n        bgcolor=\"LightSteelBlue\",\n        bordercolor=\"Black\",\n        borderwidth=2,\n    )\n)\n\nfig.update_layout(legend_orientation=\"h\")\nfig.update_layout(template=\"plotly_white\")\n#fig.update_yaxes(showgrid=True, gridwidth=0.5, gridcolor='LightGrey')\nfig.update_xaxes(showgrid=False)\n\niplot(fig)", "processed": ["top public lb score time"]}, {"markdown": ["# All competitors LB Position over Time\n(Kernel keeps breaking so I subsample to 1000 random teams)"], "code": "# Reference: https://www.kaggle.com/code/robikscube/the-race-to-detect-fraud-final-lb\n\n# Scores of top teams over time\nplt.rcParams[\"font.size\"] = \"12\"\nALL_TEAMS = df.columns.values\ndf_ffill = df[ALL_TEAMS].ffill()\n\n# This is broken\ndf_ffill.T.sample(1000).T.plot(figsize=(20, 10),\n                           color=color_pal[0],\n                           legend=False,\n                           alpha=0.05,\n                           ylim=(0.925, TOP_SCORE+0.001),\n                           title='All Teams Public LB Scores over Time')\n\ndf.ffill().max(axis=1).plot(color=color_pal[1], label='1st Place Public LB', legend=True)\nplt.show()", "processed": ["competitor lb posit time kernel keep break subsampl 1000 random team"]}, {"markdown": ["# Number of Teams by Date"], "code": "# Reference: https://www.kaggle.com/code/robikscube/the-race-to-detect-fraud-final-lb\n\nplt.rcParams[\"font.size\"] = \"12\"\nax =df.ffill() \\\n    .count(axis=1) \\\n    .plot(figsize=(20, 8),\n          title='Number of Teams in the Competition by Date',\n         color=color_pal[5], lw=5)\nax.set_ylabel('Number of Teams')\nplt.axvline('09-23-2019', color='orange', linestyle='-.')\nplt.text('09-23-2019', 4000,'Merger Deadline',rotation=-90)\nplt.axvline('10-1-2019', color='orange', linestyle='-.')\nplt.text('10-1-2019', 4000,'Original Deadline',rotation=-90)\nplt.axvline('10-3-2019', color='orange', linestyle='-.')\nplt.text('10-3-2019', 4000,'Extended Deadline',rotation=-90)\nplt.show()\n# plt.style.use('ggplot')\n# team_over_time = df.ffill() \\\n#     .count(axis=1)\n\n# lr = LinearRegression()\n# _ = lr.fit(np.array(pd.to_numeric(team_over_time.index).tolist()).reshape(-1, 1),\n#            team_over_time.values)\n\n# teamcount_df = pd.DataFrame(team_over_time)\n\n# teamcount_pred_df = pd.DataFrame(index=pd.date_range('07-15-2019','10-05-2019'))\n# teamcount_pred_df['Forecast Using All Data'] = lr.predict(np.array(pd.to_numeric(teamcount_pred_df.index).tolist()).reshape(-1, 1))\n\n# lr = LinearRegression()\n# _ = lr.fit(np.array(pd.to_numeric(team_over_time[-5000:].index).tolist()).reshape(-1, 1),\n#            team_over_time[-5000:].values)\n\n# teamcount_pred_df['Forecast Using Recent Data'] = lr.predict(np.array(pd.to_numeric(teamcount_pred_df.index).tolist()).reshape(-1, 1))\n\n# plt.rcParams[\"font.size\"] = \"12\"\n# ax =df.ffill() \\\n#     .count(axis=1) \\\n#     .plot(figsize=(20, 8),\n#           title='Forecasting the Final Number of Teams',\n#          color=color_pal[5], lw=5,\n#          xlim=('07-13-2019','10-02-2019'))\n# teamcount_pred_df['Forecast Using All Data'].plot(ax=ax, style='.-.', alpha=0.5, label='Regression Using All Data')\n# teamcount_pred_df['Forecast Using Recent Data'].plot(ax=ax, style='.-.', alpha=0.5, label='Regression Using last 1000 observations')\n# ax.set_ylabel('Number of Teams')\n# teamcount_pred_df.plot(ax=ax, style='.-.', alpha=0.5)\n# plt.axvline('09-23-2019', color='orange', linestyle='-.')\n# plt.text('09-23-2019', 4000,'Merger Deadline',rotation=-90)\n# plt.axvline('10-1-2019', color='orange', linestyle='-.')\n# plt.text('10-1-2019', 4000,'Original Deadline',rotation=-90)\n# plt.axvline('10-3-2019', color='orange', linestyle='-.')\n# plt.text('10-3-2019', 4000,'Extended Deadline',rotation=-90)\n# plt.show()", "processed": ["number team date"]}, {"markdown": ["# Top LB Scores"], "code": "# Reference: https://www.kaggle.com/code/robikscube/the-race-to-detect-fraud-final-lb\n\nplt.rcParams[\"font.size\"] = \"12\"\n# Create Top Teams List\nTOP_TEAMS = df.max().loc[df.max() > FIFTYTH_SCORE].index.values\ndf[TOP_TEAMS].max().sort_values(ascending=True).plot(kind='barh',\n                                       xlim=(FIFTYTH_SCORE-0.001,TOP_SCORE+0.001),\n                                       title='Top 50 Public LB Teams',\n                                       figsize=(12, 15),\n                                       color=color_pal[3])\nplt.show()", "processed": ["top lb score"]}, {"markdown": ["# Count of LB Submissions with Improved Score"], "code": "# Reference: https://www.kaggle.com/code/robikscube/the-race-to-detect-fraud-final-lb\n\nplt.rcParams[\"font.size\"] = \"12\"\ndf[TOP_TEAMS].nunique().sort_values().plot(kind='barh',\n                                           figsize=(12, 15),\n                                           color=color_pal[1],\n                                           title='Count of Submissions improving LB score by Team')\nplt.show()", "processed": ["count lb submiss improv score"]}, {"markdown": ["We see that most frequent **label** is `FAKE` (80.75%), `meawmsgiti.mp4` is the most frequent **original** (6 samples).", "Let's do now some data distribution visualizations."], "code": "# Reference: https://www.kaggle.com/code/gpreda/deepfake-starter-kit\n\ndef plot_count(feature, title, df, size=1):\n    '''\n    Plot count of classes / feature\n    param: feature - the feature to analyze\n    param: title - title to add to the graph\n    param: df - dataframe from which we plot feature's classes distribution \n    param: size - default 1.\n    '''\n    f, ax = plt.subplots(1,1, figsize=(4*size,4))\n    total = float(len(df))\n    g = sns.countplot(df[feature], order = df[feature].value_counts().index[:20], palette='Set3')\n    g.set_title(\"Number and percentage of {}\".format(title))\n    if(size > 2):\n        plt.xticks(rotation=90, size=8)\n    for p in ax.patches:\n        height = p.get_height()\n        ax.text(p.get_x()+p.get_width()/2.,\n                height + 3,\n                '{:1.2f}%'.format(100*height/total),\n                ha=\"center\") \n    plt.show()    \nplot_count('split', 'split (train)', meta_train_df)\nplot_count('label', 'label (train)', meta_train_df)", "processed": ["see frequent label fake 80 75 meawmsgiti mp4 frequent origin 6 sampl", "let data distribut visual"]}, {"markdown": ["From [4] ([Basic EDA Face Detection, split video, ROI](https://www.kaggle.com/marcovasquez/basic-eda-face-detection-split-video-roi)) we modified a function for displaying a selected image from a video."], "code": "# Reference: https://www.kaggle.com/code/gpreda/deepfake-starter-kit\n\ndef display_image_from_video(video_path):\n    '''\n    input: video_path - path for video\n    process:\n    1. perform a video capture from the video\n    2. read the image\n    3. display the image\n    '''\n    capture_image = cv.VideoCapture(video_path) \n    ret, frame = capture_image.read()\n    fig = plt.figure(figsize=(10,10))\n    ax = fig.add_subplot(111)\n    frame = cv.cvtColor(frame, cv.COLOR_BGR2RGB)\n    ax.imshow(frame)\nfor video_file in fake_train_sample_video:\n    display_image_from_video(os.path.join(DATA_FOLDER, TRAIN_SAMPLE_FOLDER, video_file))", "processed": ["4 basic eda face detect split video roi http www kaggl com marcovasquez basic eda face detect split video roi modifi function display select imag video"]}, {"markdown": ["We also define a function for detection and display of all these specific objects.  \n\nThe function call the **detect** method of the **ObjectDetector** object. For each object we are using a different shape and color, as following:\n* Frontal face: green rectangle;  \n* Eye: red circle;  \n* Smile: red rectangle;  \n* Profile face: blue rectangle.  \n\nNote: due to a huge amount of false positive, we deactivate for now the smile detector."], "code": "# Reference: https://www.kaggle.com/code/gpreda/deepfake-starter-kit\n\ndef detect_objects(image, scale_factor, min_neighbors, min_size):\n    '''\n    Objects detection function\n    Identify frontal face, eyes, smile and profile face and display the detected objects over the image\n    param: image - the image extracted from the video\n    param: scale_factor - scale factor parameter for `detect` function of ObjectDetector object\n    param: min_neighbors - min neighbors parameter for `detect` function of ObjectDetector object\n    param: min_size - minimum size parameter for f`detect` function of ObjectDetector object\n    '''\n    \n    image_gray=cv.cvtColor(image, cv.COLOR_BGR2GRAY)\n\n\n    eyes=ed.detect(image_gray,\n                   scale_factor=scale_factor,\n                   min_neighbors=min_neighbors,\n                   min_size=(int(min_size[0]/2), int(min_size[1]/2)))\n\n    for x, y, w, h in eyes:\n        #detected eyes shown in color image\n        cv.circle(image,(int(x+w/2),int(y+h/2)),(int((w + h)/4)),(0, 0,255),3)\n \n    # deactivated due to many false positive\n    #smiles=sd.detect(image_gray,\n    #               scale_factor=scale_factor,\n    #               min_neighbors=min_neighbors,\n    #               min_size=(int(min_size[0]/2), int(min_size[1]/2)))\n\n    #for x, y, w, h in smiles:\n    #    #detected smiles shown in color image\n    #    cv.rectangle(image,(x,y),(x+w, y+h),(0, 0,255),3)\n\n\n    profiles=pd.detect(image_gray,\n                   scale_factor=scale_factor,\n                   min_neighbors=min_neighbors,\n                   min_size=min_size)\n\n    for x, y, w, h in profiles:\n        #detected profiles shown in color image\n        cv.rectangle(image,(x,y),(x+w, y+h),(255, 0,0),3)\n\n    faces=fd.detect(image_gray,\n                   scale_factor=scale_factor,\n                   min_neighbors=min_neighbors,\n                   min_size=min_size)\n\n    for x, y, w, h in faces:\n        #detected faces shown in color image\n        cv.rectangle(image,(x,y),(x+w, y+h),(0, 255,0),3)\n\n    # image\n    fig = plt.figure(figsize=(10,10))\n    ax = fig.add_subplot(111)\n    image = cv.cvtColor(image, cv.COLOR_BGR2RGB)\n    ax.imshow(image)", "processed": ["also defin function detect display specif object function call detect method objectdetector object object use differ shape color follow frontal face green rectangl eye red circl smile red rectangl profil face blue rectangl note due huge amount fals posit deactiv smile detector"]}, {"markdown": ["## Transpose Training Set and Order by the mean feature\n- Plot 500 examples - maybe we can see a pattern.\n"], "code": "# Reference: https://www.kaggle.com/code/robikscube/brainstorming-santander-features\n\nfor row in tqdm(range(0, 500)):\n    target_of_this = train.loc[row]['target']\n    if target_of_this == 0:\n        train_T.sort_values('feat_mean')[row].plot(figsize=(15, 5), color=colors[0], alpha=0.2)\n    elif target_of_this == 1:\n        train_T.sort_values('feat_mean')[row].plot(figsize=(15, 5), color=colors[2], alpha=0.6)", "processed": ["transpos train set order mean featur plot 500 exampl mayb see pattern"]}, {"markdown": ["## Lets try that again but plot an equal distribution of positive/negative target"], "code": "# Reference: https://www.kaggle.com/code/robikscube/brainstorming-santander-features\n\n# Identify the index of postitive and negative observations\ntrain_T.index = [x for x in range(0,200)]\nnegative_index = train.loc[train['target'] == 0].index.values\npositive_index = train.loc[train['target'] == 1].index.values\n\n# plot each one\nfor plotcount in tqdm(range(0, 5)):\n    ax = train_T.sort_values('feat_mean')[negative_index[plotcount]].reset_index(drop=True).plot(figsize=(15, 5),\n                                                                                                 color=colors[0],\n                                                                                                 alpha=0.5)\n    train_T.sort_values('feat_mean')[positive_index[plotcount]].reset_index(drop=True).plot(figsize=(15, 5),\n                                                                                            color=colors[2],\n                                                                                            alpha=0.5)\n    ax.tick_params(which='minor', length=4, color='r')", "processed": ["let tri plot equal distribut posit neg target"]}, {"markdown": ["## Within each observation (row) sort the scaled columns in ascending order\n- This could just be a data property but it appears that the positive/negative observations diverge at the ends."], "code": "# Reference: https://www.kaggle.com/code/robikscube/brainstorming-santander-features\n\nfor plotcount in tqdm(range(0, 5000)):\n    scaleddf_T[negative_index[plotcount]].sort_values().reset_index(drop=True).plot(figsize=(15, 15), color=colors[0], alpha=0.2)\n    scaleddf_T[positive_index[plotcount]].sort_values().reset_index(drop=True).plot(figsize=(15, 15), color=colors[2], alpha=0.2)", "processed": ["within observ row sort scale column ascend order could data properti appear posit neg observ diverg end"]}, {"markdown": ["AUC of 0.52 is actually pretty decent. Seems that at least at this level of analysis there is not much difference between the train and test sets. ", "AUC of 0.56 reveals some dicrepancy between the train and test sets. Let's take a look at the feature importances:"], "code": "# Reference: https://www.kaggle.com/code/tunguz/adversarial-ashrae\n\nfeature_imp = pd.DataFrame(sorted(zip(clf.feature_importance(),columns)), columns=['Value','Feature'])\n\nplt.figure(figsize=(20, 10))\nsns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False).head(20))\nplt.title('LightGBM Features')\nplt.tight_layout()\nplt.show()\nplt.savefig('lgbm_importances-01.png')\n", "processed": ["auc 0 52 actual pretti decent seem least level analysi much differ train test set", "auc 0 56 reveal dicrep train test set let take look featur import"]}, {"markdown": ["## 5. Tuning models and building the feature importance diagrams<a class=\"anchor\" id=\"5\"></a>\n\n[Back to Table of Contents](#0.1)", "Technology for feature importance building from the my kernel [Feature importance - xgb, lgbm, logreg, linreg](https://www.kaggle.com/vbmokin/feature-importance-xgb-lgbm-logreg-linreg)", "### 5.1 LGBM <a class=\"anchor\" id=\"5.1\"></a>\n\n[Back to Table of Contents](#0.1)"], "code": "# Reference: https://www.kaggle.com/code/vbmokin/nfl-feature-importance-xgb-lgbm-linreg\n\nX = train\nz = target\n#%% split training set to validation set\nXtrain, Xval, Ztrain, Zval = train_test_split(X, z, test_size=0.2, random_state=0)\ntrain_set = lgbm.Dataset(Xtrain, Ztrain, silent=False)\nvalid_set = lgbm.Dataset(Xval, Zval, silent=False)\nparams = {\n        'boosting_type':'gbdt',\n        'objective': 'regression',\n        'num_leaves': 31,\n        'learning_rate': 0.05,\n        'max_depth': -1,\n        'subsample': 0.8,\n        'bagging_fraction' : 1,\n        'max_bin' : 5000 ,\n        'bagging_freq': 20,\n        'colsample_bytree': 0.6,\n        'metric': 'rmse',\n        'min_split_gain': 0.5,\n        'min_child_weight': 1,\n        'min_child_samples': 10,\n        'scale_pos_weight':1,\n        'zero_as_missing': True,\n        'seed':0,        \n    }\n\nmodelL = lgbm.train(params, train_set = train_set, num_boost_round=1000,\n                   early_stopping_rounds=50,verbose_eval=10, valid_sets=valid_set)\nfig =  plt.figure(figsize = (25,30))\naxes = fig.add_subplot(111)\nlgbm.plot_importance(modelL,ax = axes,height = 0.5)\nplt.show();plt.close()\nfeature_score = pd.DataFrame(train.columns, columns = ['feature']) \nfeature_score['score_lgb'] = modelL.feature_importance()", "processed": ["5 tune model build featur import diagram class anchor id 5 back tabl content 0 1", "technolog featur import build kernel featur import xgb lgbm logreg linreg http www kaggl com vbmokin featur import xgb lgbm logreg linreg", "5 1 lgbm class anchor id 5 1 back tabl content 0 1"]}, {"markdown": ["### 5.2 XGB<a class=\"anchor\" id=\"5.2\"></a>\n\n[Back to Table of Contents](#0.1)"], "code": "# Reference: https://www.kaggle.com/code/vbmokin/nfl-feature-importance-xgb-lgbm-linreg\n\n#%% split training set to validation set \ndata_tr  = xgb.DMatrix(Xtrain, label=Ztrain)\ndata_cv  = xgb.DMatrix(Xval   , label=Zval)\nevallist = [(data_tr, 'train'), (data_cv, 'valid')]\nparms = {'max_depth':8, #maximum depth of a tree\n         'objective':'reg:squarederror',\n         'eta'      :0.3,\n         'subsample':0.8,#SGD will use this percentage of data\n         'lambda '  :4, #L2 regularization term,>1 more conservative \n         'colsample_bytree ':0.9,\n         'colsample_bylevel':1,\n         'min_child_weight': 10}\nmodelx = xgb.train(parms, data_tr, num_boost_round=200, evals = evallist,\n                  early_stopping_rounds=30, maximize=False, \n                  verbose_eval=10)\n\nprint('score = %1.5f, n_boost_round =%d.'%(modelx.best_score,modelx.best_iteration))\nfig =  plt.figure(figsize = (15,30))\naxes = fig.add_subplot(111)\nxgb.plot_importance(modelx,ax = axes,height = 0.5)\nplt.show();plt.close()\nfeature_score['score_xgb'] = feature_score['feature'].map(modelx.get_score(importance_type='weight'))\nfeature_score", "processed": ["5 2 xgb class anchor id 5 2 back tabl content 0 1"]}, {"markdown": ["## Check generalization\nHere we \n1. split the dataset in 2 equal parts at each iteration. \n2. We run an LGBM on the first part with early stopping using the 2nd part to get the optimal round\n3. We then run a 5 fold CV with early stopping on the 1st part and keep all folds best round\n4. We compare mean, max and min to the optimum\n5. Do the same flipping part 1 and 2 roles\n6. Use a new seed"], "code": "# Reference: https://www.kaggle.com/code/ogrellier/can-early-stopping-generalize\n\nfrom sklearn.model_selection import StratifiedKFold\nimport time\nnb_seed = 20\nvalues = np.zeros((2 * nb_seed, 4))\nscores = np.zeros((2 * nb_seed, 4))\ni = 0\nstart = time.time()\nfor seed in range(nb_seed):\n    fold_lev1 = StratifiedKFold(n_splits=2, shuffle=True, random_state=seed)\n    for trn_l1_idx, val_l1_idx in fold_lev1.split(target, target):\n        # Split level1 data\n        trn_l1_x, trn_l1_y = trn.iloc[trn_l1_idx], target.iloc[trn_l1_idx]\n        val_l1_x, val_l1_y = trn.iloc[val_l1_idx], target.iloc[val_l1_idx]\n        # Compute Optimal l1 round\n        opt_l1_rnd, opt_score = compute_optimal_round(trn_l1_x, trn_l1_y, val_l1_x, val_l1_y)\n        # print(\"opt_l1_rnd : \", opt_l1_rnd)\n        # Split level2 data\n        opt_l2_rnd = []\n        fold_lev2 = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n        for trn_l2_idx, val_l2_idx in fold_lev2.split(trn_l1_y, trn_l1_y):\n            trn_l2_x, trn_l2_y = trn_l1_x.iloc[trn_l2_idx], trn_l1_y.iloc[trn_l2_idx]\n            val_l2_x, val_l2_y = trn_l1_x.iloc[val_l2_idx], trn_l1_y.iloc[val_l2_idx]\n            # Compute optimal round for current fold\n            opt_fold_round, _ = compute_optimal_round(trn_l2_x, trn_l2_y, val_l2_x, val_l2_y)\n            opt_l2_rnd.append(opt_fold_round)\n        # Print rounds\n        values[i, :] = [opt_l1_rnd, np.mean(opt_l2_rnd), np.min(opt_l2_rnd), np.max(opt_l2_rnd)]\n        elapsed = (time.time() - start) / 60\n        score_mean, score_min, score_max = compute_score(\n            trn_l1_x, trn_l1_y, val_l1_x, val_l1_y, \n            rounds = [np.mean(opt_l2_rnd),  \n                      np.min(opt_l2_rnd), \n                      np.max(opt_l2_rnd)])\n        scores[i, :] = [opt_score, score_mean, score_min, score_max]\n        \n        print(\"Opt round %5d / Mean round %5d / Min round %5d / Max round %5d [in %5.1f min]\"\n              % (values[i, 0], values[i, 1], values[i, 2], values[i, 3], elapsed))\n        print(\"Opt score %.3f / Mean score %.3f / Min score %.3f / Max score %.3f [in %5.1f min]\"\n              % (scores[i, 0], scores[i, 1], scores[i, 2], scores[i, 3], elapsed))\n        \n        i += 1\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nplt.figure(figsize=(10,10))\nsns.distplot(100 * (values[:, 1] - values[:, 0]) / values[:, 0], label=\"Mean CV rounds - Optimum round\")\nsns.distplot(100 * (values[:, 2] - values[:, 0]) / values[:, 0], label=\"Min CV rounds - Optimum round\")\nsns.distplot(100 * (values[:, 3] - values[:, 0]) / values[:, 0], label=\"Max CV rounds - Optimum round\")\nplt.legend(loc=\"upper right\")\nplt.title(\"Error in Optimum round estimation using 5-CV best rounds (in %)\")\n", "processed": ["check gener 1 split dataset 2 equal part iter 2 run lgbm first part earli stop use 2nd part get optim round 3 run 5 fold cv earli stop 1st part keep fold best round 4 compar mean max min optimum 5 flip part 1 2 role 6 use new seed"]}, {"markdown": ["On average the mean of fold rounds gives a good estimate but variance is a big issue here.\n\nBut What about scores ?"], "code": "# Reference: https://www.kaggle.com/code/ogrellier/can-early-stopping-generalize\n\nplt.figure(figsize=(10,10))\nsns.distplot(100 * (scores[:, 1] - scores[:, 0]) / scores[:, 0], label=\"Mean CV rounds - Optimum round\")\nsns.distplot(100 * (scores[:, 2] - scores[:, 0]) / scores[:, 0], label=\"Min CV rounds - Optimum round\")\nsns.distplot(100 * (scores[:, 3] - scores[:, 0]) / scores[:, 0], label=\"Max CV rounds - Optimum round\")\nplt.legend(loc=\"upper right\")\nplt.title(\"Error in Optimum score estimation using 5-CV best rounds (in %)\")", "processed": ["averag mean fold round give good estim varianc big issu score"]}, {"markdown": ["As expected, this distribution looks much more, ahem, normal. This is probably one of the main reasons why the metric that we are trying to optimize for this competition is RMSLE - root mean square logarithmic error.\n\nAnother way of looking at the same distribution is with the help of violinplot."], "code": "# Reference: https://www.kaggle.com/code/tunguz/yaeda-yet-another-eda\n\nsns.set_style(\"whitegrid\")\nax = sns.violinplot(x=np.log(1+train_df.target.values))\nplt.show()", "processed": ["expect distribut look much ahem normal probabl one main reason metric tri optim competit rmsle root mean squar logarithm error anoth way look distribut help violinplot"]}, {"markdown": ["Only marginal improvement - there is a verly small bump close to 15.\n\nCan the violin plot help?"], "code": "# Reference: https://www.kaggle.com/code/tunguz/yaeda-yet-another-eda\n\nsns.set_style(\"whitegrid\")\nax = sns.violinplot(x=np.log(train_df[columns_to_use].values.flatten()+1))\nplt.show()", "processed": ["margin improv verli small bump close 15 violin plot help"]}, {"markdown": ["Not really - the plot looks nicer, but the overall shape is pretty much the same. \n\nOK, let's take a look at the distribution of non-zero values."], "code": "# Reference: https://www.kaggle.com/code/tunguz/yaeda-yet-another-eda\n\ntrain_nz = np.log(train_df[columns_to_use].values.flatten()+1)\ntrain_nz = train_nz[np.nonzero(train_nz)]\nplt.figure(figsize=(12, 5))\nplt.hist(train_nz, bins=50)\nplt.title('Log Histogram nonzero train counts')\nplt.xlabel('Count')\nplt.ylabel('Log value')\nplt.show()\nsns.set_style(\"whitegrid\")\nax = sns.violinplot(x=train_nz)\nplt.show()\ndescribe(train_nz)", "processed": ["realli plot look nicer overal shape pretti much ok let take look distribut non zero valu"]}, {"markdown": ["OK, that's much more interesting. \n\nLet's do the same thing with the test data."], "code": "# Reference: https://www.kaggle.com/code/tunguz/yaeda-yet-another-eda\n\ntest_nz = np.log(test_df[columns_to_use].values.flatten()+1)\ntest_nz = test_nz[np.nonzero(test_nz)]\nplt.figure(figsize=(12, 5))\nplt.hist(test_nz, bins=50)\nplt.title('Log Histogram nonzero test counts')\nplt.xlabel('Count')\nplt.ylabel('Log value')\nplt.show()\nsns.set_style(\"whitegrid\")\nax = sns.violinplot(x=test_nz)\nplt.show()\ndescribe(test_nz)", "processed": ["ok much interest let thing test data"]}, {"markdown": ["## Exploration"], "code": "# Reference: https://www.kaggle.com/code/xhlulu/reducing-image-sizes-to-32x32\n\nlabel_df = pd.read_csv('../input/train.csv')\nsubmission_df = pd.read_csv('../input/sample_submission.csv')\nlabel_df.head()\nlabel_df['category_id'].value_counts()[1:16].plot(kind='bar')\ndef display_samples(df, columns=4, rows=3):\n    fig=plt.figure(figsize=(5*columns, 3*rows))\n\n    for i in range(columns*rows):\n        image_path = df.loc[i,'file_name']\n        image_id = df.loc[i,'category_id']\n        img = cv2.imread(f'../input/train_images/{image_path}')\n        fig.add_subplot(rows, columns, i+1)\n        plt.title(image_id)\n        plt.imshow(img)\n\ndisplay_samples(label_df)", "processed": ["explor"]}, {"markdown": ["Seems like a single data point is well above the rest. \n\nNow let us plot the distribution graph."], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-notebook-mercedes\n\nulimit = 180\ntrain_df['y'].ix[train_df['y']>ulimit] = ulimit\n\nplt.figure(figsize=(12,8))\nsns.distplot(train_df.y.values, bins=50, kde=False)\nplt.xlabel('y value', fontsize=12)\nplt.show()", "processed": ["seem like singl data point well rest let u plot distribut graph"]}, {"markdown": ["So all the integer columns are binary with some columns have only one unique value 0. Possibly we could exclude those columns in our modeling activity.\n\nNow let us explore the categorical columns present in the dataset."], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-notebook-mercedes\n\nvar_name = \"X0\"\ncol_order = np.sort(train_df[var_name].unique()).tolist()\nplt.figure(figsize=(12,6))\nsns.stripplot(x=var_name, y='y', data=train_df, order=col_order)\nplt.xlabel(var_name, fontsize=12)\nplt.ylabel('y', fontsize=12)\nplt.title(\"Distribution of y variable with \"+var_name, fontsize=15)\nplt.show()\nvar_name = \"X1\"\ncol_order = np.sort(train_df[var_name].unique()).tolist()\nplt.figure(figsize=(12,6))\nsns.stripplot(x=var_name, y='y', data=train_df, order=col_order)\nplt.xlabel(var_name, fontsize=12)\nplt.ylabel('y', fontsize=12)\nplt.title(\"Distribution of y variable with \"+var_name, fontsize=15)\nplt.show()\nvar_name = \"X2\"\ncol_order = np.sort(train_df[var_name].unique()).tolist()\nplt.figure(figsize=(12,6))\nsns.boxplot(x=var_name, y='y', data=train_df, order=col_order)\nplt.xlabel(var_name, fontsize=12)\nplt.ylabel('y', fontsize=12)\nplt.title(\"Distribution of y variable with \"+var_name, fontsize=15)\nplt.show()\nvar_name = \"X3\"\ncol_order = np.sort(train_df[var_name].unique()).tolist()\nplt.figure(figsize=(12,6))\nsns.violinplot(x=var_name, y='y', data=train_df, order=col_order)\nplt.xlabel(var_name, fontsize=12)\nplt.ylabel('y', fontsize=12)\nplt.title(\"Distribution of y variable with \"+var_name, fontsize=15)\nplt.show()\nvar_name = \"X4\"\ncol_order = np.sort(train_df[var_name].unique()).tolist()\nplt.figure(figsize=(12,6))\nsns.violinplot(x=var_name, y='y', data=train_df, order=col_order)\nplt.xlabel(var_name, fontsize=12)\nplt.ylabel('y', fontsize=12)\nplt.title(\"Distribution of y variable with \"+var_name, fontsize=15)\nplt.show()\nvar_name = \"X5\"\ncol_order = np.sort(train_df[var_name].unique()).tolist()\nplt.figure(figsize=(12,6))\nsns.boxplot(x=var_name, y='y', data=train_df, order=col_order)\nplt.xlabel(var_name, fontsize=12)\nplt.ylabel('y', fontsize=12)\nplt.title(\"Distribution of y variable with \"+var_name, fontsize=15)\nplt.show()\nvar_name = \"X6\"\ncol_order = np.sort(train_df[var_name].unique()).tolist()\nplt.figure(figsize=(12,6))\nsns.boxplot(x=var_name, y='y', data=train_df, order=col_order)\nplt.xlabel(var_name, fontsize=12)\nplt.ylabel('y', fontsize=12)\nplt.title(\"Distribution of y variable with \"+var_name, fontsize=15)\nplt.show()\nvar_name = \"X8\"\ncol_order = np.sort(train_df[var_name].unique()).tolist()\nplt.figure(figsize=(12,6))\nsns.boxplot(x=var_name, y='y', data=train_df, order=col_order)\nplt.xlabel(var_name, fontsize=12)\nplt.ylabel('y', fontsize=12)\nplt.title(\"Distribution of y variable with \"+var_name, fontsize=15)\nplt.show()", "processed": ["integ column binari column one uniqu valu 0 possibl could exclud column model activ let u explor categor column present dataset"]}, {"markdown": ["Binary variables which shows a good color difference in the above graphs between 0 and 1 are likely to be more predictive given the the count distribution is also good between both the classes (can be seen from the previous graph). We will dive more into the important variables in the later part of the notebook.\n\n**ID variable:**\n\nOne more important thing we need to look at it is ID variable. This will give an idea of how the splits are done across train and test (random or id based) and also to help see if ID has some potential prediction capability (probably not so useful for business)\n\nLet us first see how the 'y' variable changes with ID variable."], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-notebook-mercedes\n\nvar_name = \"ID\"\nplt.figure(figsize=(12,6))\nsns.regplot(x=var_name, y='y', data=train_df, scatter_kws={'alpha':0.5, 's':30})\nplt.xlabel(var_name, fontsize=12)\nplt.ylabel('y', fontsize=12)\nplt.title(\"Distribution of y variable with \"+var_name, fontsize=15)\nplt.show()", "processed": ["binari variabl show good color differ graph 0 1 like predict given count distribut also good class seen previou graph dive import variabl later part notebook id variabl one import thing need look id variabl give idea split done across train test random id base also help see id potenti predict capabl probabl use busi let u first see variabl chang id variabl"]}, {"markdown": ["There seems to be a slight decreasing trend with respect to ID variable. Now let us see how the IDs are distributed across train and test."], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-notebook-mercedes\n\nplt.figure(figsize=(6,10))\ntrain_df['eval_set'] = \"train\"\ntest_df['eval_set'] = \"test\"\nfull_df = pd.concat([train_df[[\"ID\",\"eval_set\"]], test_df[[\"ID\",\"eval_set\"]]], axis=0)\n\nplt.figure(figsize=(12,6))\nsns.violinplot(x=\"eval_set\", y='ID', data=full_df)\nplt.xlabel(\"eval_set\", fontsize=12)\nplt.ylabel('y', fontsize=12)\nplt.title(\"Distribution of ID variable with evaluation set\", fontsize=15)\nplt.show()", "processed": ["seem slight decreas trend respect id variabl let u see id distribut across train test"]}, {"markdown": ["### All channel values"], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/plant-pathology-2020-eda-models\n\nfig = ff.create_distplot([values], group_labels=[\"Channels\"], colors=[\"purple\"])\nfig.update_layout(showlegend=False, template=\"simple_white\")\nfig.update_layout(title_text=\"Distribution of channel values\")\nfig.data[0].marker.line.color = 'rgb(0, 0, 0)'\nfig.data[0].marker.line.width = 0.5\nfig", "processed": ["channel valu"]}, {"markdown": ["The channel values seem to have a roughly normal distribution centered around 105. The maximum channel activation is 255. This means that the average channel value is less than half the maximum value, which indicates that channels are minimally activated most of the time.", "### Red channel values"], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/plant-pathology-2020-eda-models\n\nfig = ff.create_distplot([red_values], group_labels=[\"R\"], colors=[\"red\"])\nfig.update_layout(showlegend=False, template=\"simple_white\")\nfig.update_layout(title_text=\"Distribution of red channel values\")\nfig.data[0].marker.line.color = 'rgb(0, 0, 0)'\nfig.data[0].marker.line.width = 0.5\nfig", "processed": ["channel valu seem roughli normal distribut center around 105 maximum channel activ 255 mean averag channel valu le half maximum valu indic channel minim activ time", "red channel valu"]}, {"markdown": ["The red channel values seem to roughly normal distribution, but with a slight rightward (positive skew). This indicates that the red channel tends to be more concentrated at lower values, at around 100. There is large variation in average red values across images.", "### Green channel values"], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/plant-pathology-2020-eda-models\n\nfig = ff.create_distplot([green_values], group_labels=[\"G\"], colors=[\"green\"])\nfig.update_layout(showlegend=False, template=\"simple_white\")\nfig.update_layout(title_text=\"Distribution of green channel values\")\nfig.data[0].marker.line.color = 'rgb(0, 0, 0)'\nfig.data[0].marker.line.width = 0.5\nfig", "processed": ["red channel valu seem roughli normal distribut slight rightward posit skew indic red channel tend concentr lower valu around 100 larg variat averag red valu across imag", "green channel valu"]}, {"markdown": ["The green channel values have a more uniform distribution than the red channel values, with a smaller peak. The distribution also has a leftward skew (in contrast to red) and a larger mode of around 140. This indicates that green is more pronounced in these images than red, which makes sense, because these are images of leaves!", "### Blue channel values"], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/plant-pathology-2020-eda-models\n\nfig = ff.create_distplot([blue_values], group_labels=[\"B\"], colors=[\"blue\"])\nfig.update_layout(showlegend=False, template=\"simple_white\")\nfig.update_layout(title_text=\"Distribution of blue channel values\")\nfig.data[0].marker.line.color = 'rgb(0, 0, 0)'\nfig.data[0].marker.line.width = 0.5\nfig", "processed": ["green channel valu uniform distribut red channel valu smaller peak distribut also leftward skew contrast red larger mode around 140 indic green pronounc imag red make sen imag leav", "blue channel valu"]}, {"markdown": ["The blue channel has the most uniform distribution out of the three color channels, with minimal skew (slight leftward skew). The blue channel shows great variation across images in the dataset.", "### All channel values (together)"], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/plant-pathology-2020-eda-models\n\nfig = go.Figure()\n\nfor idx, values in enumerate([red_values, green_values, blue_values]):\n    if idx == 0:\n        color = \"Red\"\n    if idx == 1:\n        color = \"Green\"\n    if idx == 2:\n        color = \"Blue\"\n    fig.add_trace(go.Box(x=[color]*len(values), y=values, name=color, marker=dict(color=color.lower())))\n    \nfig.update_layout(yaxis_title=\"Mean value\", xaxis_title=\"Color channel\",\n                  title=\"Mean value vs. Color channel\", template=\"plotly_white\")\nfig = ff.create_distplot([red_values, green_values, blue_values],\n                         group_labels=[\"R\", \"G\", \"B\"],\n                         colors=[\"red\", \"green\", \"blue\"])\nfig.update_layout(title_text=\"Distribution of red channel values\", template=\"simple_white\")\nfig.data[0].marker.line.color = 'rgb(0, 0, 0)'\nfig.data[0].marker.line.width = 0.5\nfig.data[1].marker.line.color = 'rgb(0, 0, 0)'\nfig.data[1].marker.line.width = 0.5\nfig.data[2].marker.line.color = 'rgb(0, 0, 0)'\nfig.data[2].marker.line.width = 0.5\nfig", "processed": ["blue channel uniform distribut three color channel minim skew slight leftward skew blue channel show great variat across imag dataset", "channel valu togeth"]}, {"markdown": ["**Target Variable**\n\nBefore delving more into the features, let us first have a look at the target variable 'interest level'"], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-notebook-2-connect\n\nint_level = train_df['interest_level'].value_counts()\n\nplt.figure(figsize=(8,4))\nsns.barplot(int_level.index, int_level.values, alpha=0.8, color=color[1])\nplt.ylabel('Number of Occurrences', fontsize=12)\nplt.xlabel('Interest level', fontsize=12)\nplt.show()", "processed": ["target variabl delv featur let u first look target variabl interest level"]}, {"markdown": ["Interest level is low for most of the cases followed by medium and then high which makes sense.\n\nNow let us start looking into the numerical features present in the dataset. Numerical features are\n\n - bathrooms\n - bedrooms\n - price\n - latitude\n - longitude\n\nThe last two are actually not numerical variables, but for now let us just consider it to be numerical.\n\n**Bathrooms:**\n\nLet us first start with bathrooms."], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-notebook-2-connect\n\ncnt_srs = train_df['bathrooms'].value_counts()\n\nplt.figure(figsize=(8,4))\nsns.barplot(cnt_srs.index, cnt_srs.values, alpha=0.8, color=color[0])\nplt.ylabel('Number of Occurrences', fontsize=12)\nplt.xlabel('bathrooms', fontsize=12)\nplt.show()\ntrain_df['bathrooms'].ix[train_df['bathrooms']>3] = 3\nplt.figure(figsize=(8,4))\nsns.violinplot(x='interest_level', y='bathrooms', data=train_df)\nplt.xlabel('Interest level', fontsize=12)\nplt.ylabel('bathrooms', fontsize=12)\nplt.show()", "processed": ["interest level low case follow medium high make sen let u start look numer featur present dataset numer featur bathroom bedroom price latitud longitud last two actual numer variabl let u consid numer bathroom let u first start bathroom"]}, {"markdown": ["Looks like evenly distributed across the interest levels. Now let us look at the next feature 'bedrooms'.\n\n**Bedrooms:**"], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-notebook-2-connect\n\ncnt_srs = train_df['bedrooms'].value_counts()\n\nplt.figure(figsize=(8,4))\nsns.barplot(cnt_srs.index, cnt_srs.values, alpha=0.8, color=color[2])\nplt.ylabel('Number of Occurrences', fontsize=12)\nplt.xlabel('bedrooms', fontsize=12)\nplt.show()\nplt.figure(figsize=(8,6))\nsns.countplot(x='bedrooms', hue='interest_level', data=train_df)\nplt.ylabel('Number of Occurrences', fontsize=12)\nplt.xlabel('bedrooms', fontsize=12)\nplt.show()", "processed": ["look like evenli distribut across interest level let u look next featur bedroom bedroom"]}, {"markdown": ["Looks like there are some outliers in this feature. So let us remove them and then plot again."], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-notebook-2-connect\n\nulimit = np.percentile(train_df.price.values, 99)\ntrain_df['price'].ix[train_df['price']>ulimit] = ulimit\n\nplt.figure(figsize=(8,6))\nsns.distplot(train_df.price.values, bins=50, kde=True)\nplt.xlabel('price', fontsize=12)\nplt.show()", "processed": ["look like outlier featur let u remov plot"]}, {"markdown": ["The distribution is right skewed as we can see.\n\nNow let us look at the latitude and longitude variables.\n\n**Latitude & Longitude:**"], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-notebook-2-connect\n\nllimit = np.percentile(train_df.latitude.values, 1)\nulimit = np.percentile(train_df.latitude.values, 99)\ntrain_df['latitude'].ix[train_df['latitude']<llimit] = llimit\ntrain_df['latitude'].ix[train_df['latitude']>ulimit] = ulimit\n\nplt.figure(figsize=(8,6))\nsns.distplot(train_df.latitude.values, bins=50, kde=False)\nplt.xlabel('latitude', fontsize=12)\nplt.show()", "processed": ["distribut right skew see let u look latitud longitud variabl latitud longitud"]}, {"markdown": ["So the latitude values are primarily between 40.6 and 40.9. Now let us look at the longitude values."], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-notebook-2-connect\n\nllimit = np.percentile(train_df.longitude.values, 1)\nulimit = np.percentile(train_df.longitude.values, 99)\ntrain_df['longitude'].ix[train_df['longitude']<llimit] = llimit\ntrain_df['longitude'].ix[train_df['longitude']>ulimit] = ulimit\n\nplt.figure(figsize=(8,6))\nsns.distplot(train_df.longitude.values, bins=50, kde=False)\nplt.xlabel('longitude', fontsize=12)\nplt.show()", "processed": ["latitud valu primarili 40 6 40 9 let u look longitud valu"]}, {"markdown": ["The longitude values range between -73.8 and -74.02. So the data corresponds to the **New York City**.\n\nNow let us plot the same in a map. Thanks to this [kernel][1] by Dotman.\n\n\n  [1]: https://www.kaggle.com/dotman/d/fivethirtyeight/uber-pickups-in-new-york-city/data-exploration-and-visualization"], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-notebook-2-connect\n\nfrom mpl_toolkits.basemap import Basemap\nfrom matplotlib import cm\n\nwest, south, east, north = -74.02, 40.64, -73.85, 40.86\n\nfig = plt.figure(figsize=(14,10))\nax = fig.add_subplot(111)\nm = Basemap(projection='merc', llcrnrlat=south, urcrnrlat=north,\n            llcrnrlon=west, urcrnrlon=east, lat_ts=south, resolution='i')\nx, y = m(train_df['longitude'].values, train_df['latitude'].values)\nm.hexbin(x, y, gridsize=200,\n         bins='log', cmap=cm.YlOrRd_r);", "processed": ["longitud valu rang 73 8 74 02 data correspond new york citi let u plot map thank kernel 1 dotman 1 http www kaggl com dotman fivethirtyeight uber pickup new york citi data explor visual"]}, {"markdown": ["**Created:**\n\nNow let us look at the date column 'created' "], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-notebook-2-connect\n\ntrain_df[\"created\"] = pd.to_datetime(train_df[\"created\"])\ntrain_df[\"date_created\"] = train_df[\"created\"].dt.date\ncnt_srs = train_df['date_created'].value_counts()\n\n\nplt.figure(figsize=(12,4))\nax = plt.subplot(111)\nax.bar(cnt_srs.index, cnt_srs.values, alpha=0.8)\nax.xaxis_date()\nplt.xticks(rotation='vertical')\nplt.show()", "processed": ["creat let u look date column creat"]}, {"markdown": ["So we have data from April to June 2016 in our train set. Now let us look at the test set as well and see if they are also from the same date range. "], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-notebook-2-connect\n\ntest_df[\"created\"] = pd.to_datetime(test_df[\"created\"])\ntest_df[\"date_created\"] = test_df[\"created\"].dt.date\ncnt_srs = test_df['date_created'].value_counts()\n\nplt.figure(figsize=(12,4))\nax = plt.subplot(111)\nax.bar(cnt_srs.index, cnt_srs.values, alpha=0.8)\nax.xaxis_date()\nplt.xticks(rotation='vertical')\nplt.show()", "processed": ["data april june 2016 train set let u look test set well see also date rang"]}, {"markdown": ["Looks very similar to the train set dates and so we are good to go.!\n\nWe shall also look at the hour-wise listing trend (Just for fun)"], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-notebook-2-connect\n\ntrain_df[\"hour_created\"] = train_df[\"created\"].dt.hour\ncnt_srs = train_df['hour_created'].value_counts()\n\nplt.figure(figsize=(12,6))\nsns.barplot(cnt_srs.index, cnt_srs.values, alpha=0.8, color=color[3])\nplt.xticks(rotation='vertical')\nplt.show()", "processed": ["look similar train set date good go shall also look hour wise list trend fun"]}, {"markdown": ["Most of the display addresses occur less than 100 times in the given dataset. None of the display address occur more than 500 times.\n\n**Number of Photos:**\n\nThis competition also has a huge database of photos of the listings. To start with, let us look at the number of photos given for listings."], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-notebook-2-connect\n\ntrain_df[\"num_photos\"] = train_df[\"photos\"].apply(len)\ncnt_srs = train_df['num_photos'].value_counts()\n\nplt.figure(figsize=(12,6))\nsns.barplot(cnt_srs.index, cnt_srs.values, alpha=0.8)\nplt.xlabel('Number of Photos', fontsize=12)\nplt.ylabel('Number of Occurrences', fontsize=12)\nplt.show()\ntrain_df['num_photos'].ix[train_df['num_photos']>12] = 12\nplt.figure(figsize=(12,6))\nsns.violinplot(x=\"num_photos\", y=\"interest_level\", data=train_df, order =['low','medium','high'])\nplt.xlabel('Number of Photos', fontsize=12)\nplt.ylabel('Interest Level', fontsize=12)\nplt.show()", "processed": ["display address occur le 100 time given dataset none display address occur 500 time number photo competit also huge databas photo list start let u look number photo given list"]}, {"markdown": ["Let us now look at the number of features variable and see its distribution.\n\n**Number of features:**"], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-notebook-2-connect\n\ntrain_df[\"num_features\"] = train_df[\"features\"].apply(len)\ncnt_srs = train_df['num_features'].value_counts()\n\nplt.figure(figsize=(12,6))\nsns.barplot(cnt_srs.index, cnt_srs.values, alpha=0.8)\nplt.ylabel('Number of Occurrences', fontsize=12)\nplt.xlabel('Number of features', fontsize=12)\nplt.show()\ntrain_df['num_features'].ix[train_df['num_features']>17] = 17\nplt.figure(figsize=(12,10))\nsns.violinplot(y=\"num_features\", x=\"interest_level\", data=train_df, order =['low','medium','high'])\nplt.xlabel('Interest Level', fontsize=12)\nplt.ylabel('Number of features', fontsize=12)\nplt.show()", "processed": ["let u look number featur variabl see distribut number featur"]}, {"markdown": ["And here are some nice sketches..."], "code": "# Reference: https://www.kaggle.com/code/jpmiller/image-based-cnn\n\nevens = range(0,11,2)\nodds = range(1,12, 2)\ndf1 = draw_df[draw_df.index.isin(evens)]\ndf2 = draw_df[draw_df.index.isin(odds)]\n\nexample1s = [ast.literal_eval(pts) for pts in df1.drawing.values]\nexample2s = [ast.literal_eval(pts) for pts in df2.drawing.values]\nlabels = df2.word.tolist()\nfor i, example in enumerate(example1s):\n    plt.figure(figsize=(6,3))\n    \n    for x,y in example:\n        plt.subplot(1,2,1)\n        plt.plot(x, y, marker='.')\n        plt.axis('off')\n\n    for x,y, in example2s[i]:\n        plt.subplot(1,2,2)\n        plt.plot(x, y, marker='.')\n        plt.axis('off')\n        label = labels[i]\n        plt.title(label, fontsize=10)\n\n    plt.show()  ", "processed": ["nice sketch"]}, {"markdown": ["There is the same number of series in X_train and y_train, numbered from 0 to 3809 (total 3810). Each series have 128 measurements.   \nEach series in train dataset is part of a group (numbered from 0 to 72).  \nThe number of rows in X_train and X_test differs with 6 x 128, 128 being the number of measurements for each group.  ", "## <a id='32'>Distribution of target feature - surface</a> \n"], "code": "# Reference: https://www.kaggle.com/code/gpreda/robots-need-help\n\nf, ax = plt.subplots(1,1, figsize=(16,4))\ntotal = float(len(y_train))\ng = sns.countplot(y_train['surface'], order = y_train['surface'].value_counts().index, palette='Set3')\ng.set_title(\"Number and percentage of labels for each class\")\nfor p in ax.patches:\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()/2.,\n            height + 3,\n            '{:1.2f}%'.format(100*height/total),\n            ha=\"center\") \nplt.show()    ", "processed": ["number seri x train train number 0 3809 total 3810 seri 128 measur seri train dataset part group number 0 72 number row x train x test differ 6 x 128 128 number measur group", "id 32 distribut target featur surfac"]}, {"markdown": ["## <a id='33'>Distribution of group_id</a>  "], "code": "# Reference: https://www.kaggle.com/code/gpreda/robots-need-help\n\nf, ax = plt.subplots(1,1, figsize=(18,8))\ntotal = float(len(y_train))\ng = sns.countplot(y_train['group_id'], order = y_train['group_id'].value_counts().index, palette='Set3')\ng.set_title(\"Number and percentage of group_id\")\nfor p in ax.patches:\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()/2.,\n            height + 3,\n            '{:1.1f}%'.format(100*height/total),\n            ha=\"center\", rotation='90') \nplt.show()    ", "processed": ["id 33 distribut group id"]}, {"markdown": ["## <a id='34'>Density plots of features</a>  \n\nLet's show now the density plot of variables in train and test dataset. \n\nWe represent with different colors the distribution for values with different values of **surface**.\n\nWe introduce two utility functions for plotting."], "code": "# Reference: https://www.kaggle.com/code/gpreda/robots-need-help\n\ndef plot_feature_distribution(df1, df2, label1, label2, features):\n    i = 0\n    sns.set_style('whitegrid')\n    plt.figure()\n    fig, ax = plt.subplots(2,5,figsize=(16,8))\n\n    for feature in features:\n        i += 1\n        plt.subplot(2,5,i)\n        sns.distplot(df1[feature], hist=False, label=label1)\n        sns.distplot(df2[feature], hist=False, label=label2)\n        plt.xlabel(feature, fontsize=9)\n        locs, labels = plt.xticks()\n        plt.tick_params(axis='x', which='major', labelsize=8)\n        plt.tick_params(axis='y', which='major', labelsize=8)\n    plt.show();\nfeatures = X_train.columns.values[3:13]\nplot_feature_distribution(X_train, X_test, 'train', 'test', features)\ndef plot_feature_class_distribution(classes,tt, features):\n    i = 0\n    sns.set_style('whitegrid')\n    plt.figure()\n    fig, ax = plt.subplots(5,2,figsize=(16,24))\n\n    for feature in features:\n        i += 1\n        plt.subplot(5,2,i)\n        for clas in classes:\n            ttc = tt[tt['surface']==clas]\n            sns.distplot(ttc[feature], hist=False,label=clas)\n        plt.xlabel(feature, fontsize=9)\n        locs, labels = plt.xticks()\n        plt.tick_params(axis='x', which='major', labelsize=8)\n        plt.tick_params(axis='y', which='major', labelsize=8)\n    plt.show();\nclasses = (y_train['surface'].value_counts()).index\ntt = X_train.merge(y_train, on='series_id', how='inner')\nplot_feature_class_distribution(classes, tt, features)", "processed": ["id 34 densiti plot featur let show densiti plot variabl train test dataset repres differ color distribut valu differ valu surfac introduc two util function plot"]}, {"markdown": ["Let's have a look at the images and the masks."], "code": "# Reference: https://www.kaggle.com/code/artgor/segmentation-in-pytorch-using-convenient-tools\n\nfig = plt.figure(figsize=(25, 16))\nfor j, im_id in enumerate(np.random.choice(train['im_id'].unique(), 4)):\n    for i, (idx, row) in enumerate(train.loc[train['im_id'] == im_id].iterrows()):\n        ax = fig.add_subplot(5, 4, j * 4 + i + 1, xticks=[], yticks=[])\n        im = Image.open(f\"{path}/train_images/{row['Image_Label'].split('_')[0]}\")\n        plt.imshow(im)\n        mask_rle = row['EncodedPixels']\n        try: # label might not be there!\n            mask = rle_decode(mask_rle)\n        except:\n            mask = np.zeros((1400, 2100))\n        plt.imshow(mask, alpha=0.5, cmap='gray')\n        ax.set_title(f\"Image: {row['Image_Label'].split('_')[0]}. Label: {row['label']}\")", "processed": ["let look imag mask"]}, {"markdown": ["## Find optimal values\n\nFirst of all, my thanks to @samusram for finding a mistake in my validation\nhttps://www.kaggle.com/c/understanding_cloud_organization/discussion/107711#622412\n\nAnd now I find optimal values separately for each class."], "code": "# Reference: https://www.kaggle.com/code/artgor/segmentation-in-pytorch-using-convenient-tools\n\nclass_params = {}\nfor class_id in range(4):\n    print(class_id)\n    attempts = []\n    for t in range(0, 100, 5):\n        t /= 100\n        for ms in [0, 100, 1200, 5000, 10000]:\n            masks = []\n            for i in range(class_id, len(probabilities), 4):\n                probability = probabilities[i]\n                predict, num_predict = post_process(sigmoid(probability), t, ms)\n                masks.append(predict)\n\n            d = []\n            for i, j in zip(masks, valid_masks[class_id::4]):\n                if (i.sum() == 0) & (j.sum() == 0):\n                    d.append(1)\n                else:\n                    d.append(dice(i, j))\n\n            attempts.append((t, ms, np.mean(d)))\n\n    attempts_df = pd.DataFrame(attempts, columns=['threshold', 'size', 'dice'])\n\n\n    attempts_df = attempts_df.sort_values('dice', ascending=False)\n    print(attempts_df.head())\n    best_threshold = attempts_df['threshold'].values[0]\n    best_size = attempts_df['size'].values[0]\n    \n    class_params[class_id] = (best_threshold, best_size)\nprint(class_params)\nsns.lineplot(x='threshold', y='dice', hue='size', data=attempts_df);\nplt.title('Threshold and min size vs dice for one of the classes');", "processed": ["find optim valu first thank samusram find mistak valid http www kaggl com c understand cloud organ discus 107711 622412 find optim valu separ class"]}, {"markdown": ["## Flattening the features and doing basic EDA with seaborn", "Now, we flatten the 2D tensors associated with each segment into 1D arrays. Now, each data point (segment) is represented by a 1D array.\n\nHere are some flattened 1D arrays (with sparse selection) visualized with **matplotlib**."], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/lanl-earthquake-prediction-fresh-eda\n\nshape = X.shape\nnew_signals = X.reshape((shape[0], shape[1]*shape[2]))\n\nsparse_signals = []\nfor i in range(3):\n    sparse_signal = []\n    for j in range(len(new_signals[i])):\n        if j % 3 == 0:\n            sparse_signal.append(new_signals[i][j])\n    sparse_signals.append(sparse_signal)\n\nplt.plot(sparse_signals[0], 'purple')\nplt.show()\nplt.plot(sparse_signals[1], 'mediumvioletred')\nplt.show()\nplt.plot(sparse_signals[2], 'crimson')\nplt.show()", "processed": ["flatten featur basic eda seaborn", "flatten 2d tensor associ segment 1d array data point segment repres 1d array flatten 1d array spar select visual matplotlib"]}, {"markdown": ["#### Bivariate KDE distribution plot"], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/lanl-earthquake-prediction-fresh-eda\n\nplot = sns.jointplot(x=perm_entropies, y=targets, kind='kde', color='orangered')\nplot.set_axis_labels('perm_entropy', 'time_to_failure', fontsize=16)\nplt.show()", "processed": ["bivari kde distribut plot"]}, {"markdown": ["The KDE plot has highest density (darkness) along a line with positive slope.", "#### Bivariate hexplot"], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/lanl-earthquake-prediction-fresh-eda\n\nplot = sns.jointplot(x=perm_entropies, y=targets, kind='hex', color='orangered')\nplot.set_axis_labels('perm_entropy', 'time_to_failure', fontsize=16)\nplt.show()", "processed": ["kde plot highest densiti dark along line posit slope", "bivari hexplot"]}, {"markdown": ["The hexplot is also darkest around a positively-sloped line.", "#### Scatterplot with line of best fit"], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/lanl-earthquake-prediction-fresh-eda\n\nplot = sns.jointplot(x=perm_entropies, y=targets, kind='reg', color='orangered')\nplot.set_axis_labels('perm_entropy', 'time_to_failure', fontsize=16)\nplt.show()", "processed": ["hexplot also darkest around posit slope line", "scatterplot line best fit"]}, {"markdown": ["#### Bivariate KDE distribution plot"], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/lanl-earthquake-prediction-fresh-eda\n\nplot = sns.jointplot(x=app_entropies, y=targets, kind='kde', color='magenta')\nplot.set_axis_labels('app_entropy', 'time_to_failure', fontsize=16)\nplt.show()", "processed": ["bivari kde distribut plot"]}, {"markdown": ["The KDE plot has highest density (darkness) around a line with negative slope.", "#### Bivariate hexplot"], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/lanl-earthquake-prediction-fresh-eda\n\nplot = sns.jointplot(x=app_entropies, y=targets, kind='hex', color='magenta')\nplot.set_axis_labels('app_entropy', 'time_to_failure', fontsize=16)\nplt.show()", "processed": ["kde plot highest densiti dark around line neg slope", "bivari hexplot"]}, {"markdown": ["The hexplot is also darkest around a negatively-sloped line.", "#### Scatterplot with line of best fit"], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/lanl-earthquake-prediction-fresh-eda\n\nplot = sns.jointplot(x=app_entropies, y=targets, kind='reg', color='magenta')\nplot.set_axis_labels('app_entropy', 'time_to_failure', fontsize=16)\nplt.show()", "processed": ["hexplot also darkest around neg slope line", "scatterplot line best fit"]}, {"markdown": ["#### Bivariate KDE distribution plot"], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/lanl-earthquake-prediction-fresh-eda\n\nplot = sns.jointplot(x=higuchi_fds, y=targets, kind='kde', color='crimson')\nplot.set_axis_labels('higuchi_fd', 'time_to_failure', fontsize=16)\nplt.show()", "processed": ["bivari kde distribut plot"]}, {"markdown": ["The KDE plot has highest density (darkness) around a line with negative slope.", "#### Bivariate hexplot"], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/lanl-earthquake-prediction-fresh-eda\n\nplot = sns.jointplot(x=higuchi_fds, y=targets, kind='hex', color='crimson')\nplot.set_axis_labels('higuchi_fd', 'time_to_failure', fontsize=16)\nplt.show()", "processed": ["kde plot highest densiti dark around line neg slope", "bivari hexplot"]}, {"markdown": ["The hexplot is also darkest around a negatively-sloped line."], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/lanl-earthquake-prediction-fresh-eda\n\nplot = sns.jointplot(x=higuchi_fds, y=targets, kind='reg', color='crimson')\nplot.set_axis_labels('higuchi_fd', 'time_to_failure', fontsize=16)\nplt.show()", "processed": ["hexplot also darkest around neg slope line"]}, {"markdown": ["The line of best fit in the scatterplot has a clear negative slope.", "From the above three plots we can see a somewhat **negative correlation** between the Higuchi fractal dimension of the flattened feature array and the time left for the laboratory earthquake to occur.", "### Katz Fractal Dimension\nThe Katz fractal dimension is yet another way to calculate the fractal dimension of a two-dimensional curve."], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/lanl-earthquake-prediction-fresh-eda\n\ndef katz_fd(x):\n    \"\"\"Katz Fractal Dimension.\n    Parameters\n    ----------\n    x : list or np.array\n        One dimensional time series\n    Returns\n    -------\n    kfd : float\n        Katz fractal dimension\n    Notes\n    -----\n    The Katz Fractal dimension is defined by:\n    .. math:: FD_{Katz} = \\dfrac{log_{10}(n)}{log_{10}(d/L)+log_{10}(n)}\n    where :math:`L` is the total length of the time series and :math:`d`\n    is the Euclidean distance between the first point in the\n    series and the point that provides the furthest distance\n    with respect to the first point.\n    Original code from the mne-features package by Jean-Baptiste Schiratti\n    and Alexandre Gramfort.\n    References\n    ----------\n    .. [1] Esteller, R. et al. (2001). A comparison of waveform fractal\n           dimension algorithms. IEEE Transactions on Circuits and Systems I:\n           Fundamental Theory and Applications, 48(2), 177-183.\n    .. [2] Goh, Cindy, et al. \"Comparison of fractal dimension algorithms for\n           the computation of EEG biomarkers for dementia.\" 2nd International\n           Conference on Computational Intelligence in Medicine and Healthcare\n           (CIMED2005). 2005.\n    Examples\n    --------\n    Katz fractal dimension.\n        >>> import numpy as np\n        >>> from entropy import katz_fd\n        >>> np.random.seed(123)\n        >>> x = np.random.rand(100)\n        >>> print(katz_fd(x))\n            5.1214\n    \"\"\"\n    x = np.array(x)\n    dists = np.abs(np.ediff1d(x))\n    ll = dists.sum()\n    ln = np.log10(np.divide(ll, dists.mean()))\n    aux_d = x - x[0]\n    d = np.max(np.abs(aux_d[1:]))\n    return np.divide(ln, np.add(ln, np.log10(np.divide(d, ll))))\nkatz_fds = np.array([katz_fd(new_signal) for new_signal in new_signals])\nplot = sns.jointplot(x=katz_fds, y=targets, kind='kde', color='forestgreen')\nplot.set_axis_labels('katz_fd', 'time_to_failure', fontsize=16)\nplt.show()", "processed": ["line best fit scatterplot clear neg slope", "three plot see somewhat neg correl higuchi fractal dimens flatten featur array time left laboratori earthquak occur", "katz fractal dimens katz fractal dimens yet anoth way calcul fractal dimens two dimension curv"]}, {"markdown": ["The KDE plot has highest density (darkness) around an almost vertical line."], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/lanl-earthquake-prediction-fresh-eda\n\nplot = sns.jointplot(x=katz_fds, y=targets, kind='hex', color='forestgreen')\nplot.set_axis_labels('katz_fd', 'time_to_failure', fontsize=16)\nplt.show()", "processed": ["kde plot highest densiti dark around almost vertic line"]}, {"markdown": ["The hexplot also has highest density around an almost vertical line."], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/lanl-earthquake-prediction-fresh-eda\n\nplot = sns.jointplot(x=katz_fds, y=targets, kind='reg', color='forestgreen')\nplot.set_axis_labels('katz_fd', 'time_to_failure', fontsize=16)\nplt.show()", "processed": ["hexplot also highest densiti around almost vertic line"]}, {"markdown": ["# \"What is meant by probabilistic forecasting?\"... <a class=\"anchor\" id=\"prob_forecasting\"></a>\n\nWhen I read about this competition this was one of the first questions that came into my mind. I know a few probabilistic models and methods and I have done some time series analysis before but I haven't directly got in touch with probabilistic timeseries analysis so far. The M5 competition is a good way to close this gap and to learn something new. To start I like to follow a question driven approach...\n\n## What is a grouped time series? <a class=\"anchor\" id=\"grouped_ts\"></a>\n\n* Reading the competiton guideline we can find out that we have to deal with grouped time series of unit sales data. \n* They show a hierarchy of different aggregation levels that are weighted equally in the loss functions. \n* When working with grouped time series it's common to compute forecasts only for disaggregated time series and to add them up the same way the aggregation is performed for all remaining time series. \n* In Chapter 10 of [Forecasting - Principles and Practice](https://otexts.com/fpp2/hierarchical.html) we can find even more information about how to do this \"forecasting aggregation\".", "## How does the hierarchy look like? <a class=\"anchor\" id=\"hierarchy_ts\"></a>\n\nIn the competition guideline we can find that the hierarchy consits of 12 levels. Let's try to reconstruct some of them:\n\n1. The top is given by the unit sales of all products, aggregated for all stores/states. \n2. Unit sales of all products, aggregated for each state.\n3. Unit sales of all products, aggregated for each store.\n4. Unit sales of all products, aggregated for each category.\n5. Unit sales of all products, aggregated for each department.\n\n...\n\nOk, time for a vis: ;-)"], "code": "# Reference: https://www.kaggle.com/code/allunia/m5-sales-uncertainty-prediction\n\nseries_cols = train.columns[train.columns.str.contains(\"d_\")].values\nlevel_cols = train.columns[train.columns.str.contains(\"d_\")==False].values\ntrain.head(1)\nsns.set_palette(\"colorblind\")\n\nfig, ax = plt.subplots(5,1,figsize=(20,28))\ntrain[series_cols].sum().plot(ax=ax[0])\nax[0].set_title(\"Top-Level-1: Summed product sales of all stores and states\")\nax[0].set_ylabel(\"Unit sales of all products\");\ntrain.groupby(\"state_id\")[series_cols].sum().transpose().plot(ax=ax[1])\nax[1].set_title(\"Level-2: Summed product sales of all stores per state\");\nax[1].set_ylabel(\"Unit sales of all products\");\ntrain.groupby(\"store_id\")[series_cols].sum().transpose().plot(ax=ax[2])\nax[2].set_title(\"Level-3: Summed product sales per store\")\nax[2].set_ylabel(\"Unit sales of all products\");\ntrain.groupby(\"cat_id\")[series_cols].sum().transpose().plot(ax=ax[3])\nax[3].set_title(\"Level-4: Summed product sales per category\")\nax[3].set_ylabel(\"Unit sales of all products\");\ntrain.groupby(\"dept_id\")[series_cols].sum().transpose().plot(ax=ax[4])\nax[4].set_title(\"Level-4: Summed product sales per product department\")\nax[4].set_ylabel(\"Unit sales of all products\");", "processed": ["meant probabilist forecast class anchor id prob forecast read competit one first question came mind know probabilist model method done time seri analysi directli got touch probabilist timeseri analysi far m5 competit good way close gap learn someth new start like follow question driven approach group time seri class anchor id group t read competiton guidelin find deal group time seri unit sale data show hierarchi differ aggreg level weight equal loss function work group time seri common comput forecast disaggreg time seri add way aggreg perform remain time seri chapter 10 forecast principl practic http otext com fpp2 hierarch html find even inform forecast aggreg", "hierarchi look like class anchor id hierarchi t competit guidelin find hierarchi consit 12 level let tri reconstruct 1 top given unit sale product aggreg store state 2 unit sale product aggreg state 3 unit sale product aggreg store 4 unit sale product aggreg categori 5 unit sale product aggreg depart ok time vi"]}, {"markdown": ["Browsing through the submission ids, we can see that we are given values of $u_{i}$ and information about the aggregation type like:\n\n* the state id\n* the department id\n* the item id\n* the store id\n\nIt's a bit confusing that missing states are not represented by X. This makes splitting the id for EDA a bit more complicated. :-( Furthermore there is no clear separator. The $ \\_ $ sign is also present in the department id. **It seems that one asked aggregation always consists of 3 ids. In cases of counts smaller than 3, we can observe X as placeholder.**  ", "## Submission EDA <a class=\"anchor\" id=\"submission_eda\"></a> "], "code": "# Reference: https://www.kaggle.com/code/allunia/m5-sales-uncertainty-prediction\n\ndef find_quartil(l):\n    \n    if \"0.005\" in l:\n        return 0.005\n    elif \"0.025\" in l:\n        return 0.025\n    elif \"0.165\" in l:\n        return 0.165\n    elif \"0.25\" in l:\n        return 0.25\n    elif \"0.5\" in l:\n        return 0.5\n    elif \"0.75\" in l:\n        return 0.75\n    elif \"0.835\" in l:\n        return 0.835\n    elif \"0.975\" in l:\n        return 0.975\n    elif \"0.995\" in l:\n        return 0.995\n    else:\n        return 0\n    \ndef find_state(l):\n    if \"CA\" in l:\n        return \"CA\"\n    elif \"TX\" in l:\n        return \"TX\"\n    elif \"WI\" in l:\n        return \"WI\"\n    else:\n        return \"Unknown\"\n    \ndef find_category(l):\n    if \"FOODS\" in l:\n        return \"foods\"\n    elif \"HOBBIES\" in l:\n        return \"hobbies\"\n    elif \"HOUSEHOLD\" in l:\n        return \"household\"\n    else:\n        return \"Unknown\"\nsubmission_eda = pd.DataFrame(submission.id, columns=[\"id\"])\nsubmission_eda.loc[:, \"lb_type\"] = np.where(submission.id.str.contains(\"validation\"), \"validation\", \"evaluation\")\nsubmission_eda.loc[:, \"u\"] = submission.id.apply(lambda l: find_quartil(l))\nsubmission_eda.loc[:, \"state\"] = submission.id.apply(lambda l: find_state(l))\nsubmission_eda.loc[:, \"category\"] = submission.id.apply(lambda l: find_category(l))\nsns.set_palette(\"husl\")\n\nfig, ax = plt.subplots(3,3,figsize=(20,20))\nsns.countplot(submission_eda.u, ax=ax[0,0]);\nsns.countplot(submission_eda.lb_type, ax=ax[0,1]);\nsns.countplot(submission_eda.state, ax=ax[1,0]);\nsns.countplot(submission_eda.loc[submission_eda.lb_type==\"validation\"].state, ax=ax[1,1]);\nsns.countplot(submission_eda.loc[submission_eda.lb_type==\"evaluation\"].state, ax=ax[1,2]);\nsns.countplot(submission_eda.category, ax=ax[2,0]);\nsns.countplot(submission_eda.loc[submission_eda.lb_type==\"validation\"].category, ax=ax[2,1]);\nsns.countplot(submission_eda.loc[submission_eda.lb_type==\"evaluation\"].category, ax=ax[2,2]);\nfor n in range(1,3):\n    ax[n,2].set_title(\"in evaluation\")\n    ax[n,1].set_title(\"in validation\")", "processed": ["brow submiss id see given valu u inform aggreg type like state id depart id item id store id bit confus miss state repres x make split id eda bit complic furthermor clear separ sign also present depart id seem one ask aggreg alway consist 3 id case count smaller 3 observ x placehold", "submiss eda class anchor id submiss eda"]}, {"markdown": ["Let's take a look at the daily sales of this series:"], "code": "# Reference: https://www.kaggle.com/code/allunia/m5-sales-uncertainty-prediction\n\nplt.figure(figsize=(20,5))\nplt.plot(train[series_cols].iloc[idx].values, 'o')\nplt.title(\"Item 445 daily sales in shop CA_1\");\nplt.xlabel(\"observed days\")\nplt.ylabel(\"Unit sales\");", "processed": ["let take look daili sale seri"]}, {"markdown": ["## Residual analysis \n\nOk, let's start with the computation of residuals for our example time series. As we assumed that the last known value is valid for all future values, I would choose this one also as fitted value for all training data points in the past:"], "code": "# Reference: https://www.kaggle.com/code/allunia/m5-sales-uncertainty-prediction\n\nnaive_val\nresiduals = train_timeseries - naive_val\nfig, ax = plt.subplots(1,2,figsize=(20,5))\nsns.distplot(residuals, ax=ax[0], kde=False)\nax[0].set_xlabel(\"residuals\")\nax[0].set_ylabel(\"frequency\");\nax[0].set_title(\"Distribution of residuals\");\nnp.mean(residuals)", "processed": ["residu analysi ok let start comput residu exampl time seri assum last known valu valid futur valu would choos one also fit valu train data point past"]}, {"markdown": ["Ok, the rest now is simple. We have seen that we can compute PIs using our multipliers:\n\n$$y_{lower, h} = y - c \\cdot \\sigma_{h}$$\n\n$$y_{upper, h} = y + c \\cdot \\sigma_{h}$$\n\n* c = 2.58 for 99% PI\n* c = 1.96 for 95% PI\n* c ~ 0.95 for 67% PI\n* c = 0.67 for 50% PI", "I will only use one as an example: c=2.58 for 99% PI:"], "code": "# Reference: https://www.kaggle.com/code/allunia/m5-sales-uncertainty-prediction\n\ny_lower = np.ones(len(std_h))\ny_upper = np.ones(len(std_h))\nfor h in range(len(std_h)):\n    low_val = naive_Q[h] - 2.58 * std_h[h]\n    if low_val < 0:\n        y_lower[h] = 0\n    else:\n        y_lower[h] = low_val\n    y_upper[h] = naive_Q[h] + 2.58 * std_h[h]\nplt.figure(figsize=(20,5))\nplt.plot(y_lower, c=\"r\", label=\"0.005 boundary\")\nplt.plot(y_upper, c=\"g\", label=\"0.995 boundary\")\nplt.plot(naive_Q, 'o', c=\"b\", label=\"predicted value\")\nplt.title(\"Computing 99% PI for one timeseries of level 12\");\nplt.xlabel(\"time horizont h=28 days\")\nplt.ylabel(\"Unit sales\");\nplt.legend();", "processed": ["ok rest simpl seen comput pi use multipli lower h c cdot sigma h upper h c cdot sigma h c 2 58 99 pi c 1 96 95 pi c 0 95 67 pi c 0 67 50 pi", "use one exampl c 2 58 99 pi"]}, {"markdown": ["As we are asked to predict a time window of 28 days, the easiest way to go now is to use the last 28 days for validation: "], "code": "# Reference: https://www.kaggle.com/code/allunia/m5-sales-uncertainty-prediction\n\ntrain_timeseries = timeseries[0:-28]\neval_timeseries = timeseries[-28::]\nprint(len(train_timeseries), len(eval_timeseries))\ndays = np.arange(1, len(series_cols)+1)\nplt.figure(figsize=(20,5))\nplt.plot(days[0:-28], train_timeseries, label=\"train\")\nplt.plot(days[-28::], eval_timeseries, label=\"validation\")\nplt.title(\"Top-Level-1: Summed product sales of all stores and states\");\nplt.legend()\nplt.xlabel(\"Day\")\nplt.ylabel(\"Unit sales\");", "processed": ["ask predict time window 28 day easiest way go use last 28 day valid"]}, {"markdown": ["As far as I currently know Prophet likes to have the dates that we can find in our calendar dataframe:"], "code": "# Reference: https://www.kaggle.com/code/allunia/m5-sales-uncertainty-prediction\n\ndates = calendar.iloc[0:len(timeseries)].date.values\ndf = pd.DataFrame(dates, columns=[\"ds\"])\ndf.loc[:, \"y\"] = timeseries\ndf.head()\ntrain_df = df.iloc[0:-28]\ntrain_df.shape\neval_df = df.iloc[-28::]\neval_df.shape\nuncertainty_interval_width = 0.25\nm = Prophet(interval_width=uncertainty_interval_width)\nm.fit(train_df)\nfuture = m.make_future_dataframe(periods=28)\nforecast = m.predict(future)\nforecast.head()\ncol_int = ['ds', 'yhat', 'yhat_lower', 'yhat_upper']\nforecast[col_int].head()\nplt.plot(forecast.iloc[-28::].yhat.values, 'o', label=\"predicted yhat\")\nplt.plot(eval_df.y.values, 'o-', label=\"target\")\nplt.legend();\nfig = plot_plotly(m, forecast)  \npy.iplot(fig)", "processed": ["far current know prophet like date find calendar datafram"]}, {"markdown": ["## The top timeseries - preprocessing <a class=\"anchor\" id=\"preprocessing\"></a>", "Let's use the timeseries of total unit sales as an example again. For preprocessing we should remove the trend and scale the values."], "code": "# Reference: https://www.kaggle.com/code/allunia/m5-sales-uncertainty-prediction\n\ndiff_series = np.diff(timeseries)\ntrain_size = np.int(0.7 * len(diff_series))\ntrain_diff_series = diff_series[0:train_size]\neval_diff_series = diff_series[train_size::]\nscaler = MinMaxScaler(feature_range=(-1,1))\nscaled_train = scaler.fit_transform(train_diff_series.reshape(-1, 1))\nscaled_eval = scaler.transform(eval_diff_series.reshape(-1,1))\nfig, ax = plt.subplots(1,2,figsize=(20,5))\nax[0].plot(scaled_train, '-o', c=\"b\")\nax[1].plot(scaled_eval, '-o', c=\"g\")\nax[0].set_title(\"Single preprocessed top timeseries in train\")\nax[1].set_title(\"Single preprocessed top timeseries in eval\");\nax[0].set_xlabel(\"Days in dataset\")\nax[1].set_xlabel(\"Days in dataset\")\nax[0].set_ylabel(\"$\\Delta y$ scaled\")\nax[1].set_ylabel(\"$\\Delta y$ scaled\");", "processed": ["top timeseri preprocess class anchor id preprocess", "let use timeseri total unit sale exampl preprocess remov trend scale valu"]}, {"markdown": ["## Fitting the model to the top-level series <a class=\"anchor\" id=\"fitting_lstm\"></a>"], "code": "# Reference: https://www.kaggle.com/code/allunia/m5-sales-uncertainty-prediction\n\nseq_len = 400\ninput_dim = 1\nhidden_dim = 128\nnum_epochs = 600\nlr=0.0005\n\n\nx_train, y_train = create_sequences(scaled_train, seq_len)\nx_eval, y_eval = create_sequences(scaled_eval, seq_len)\n\nx_train = torch.from_numpy(x_train).float()\ny_train = torch.from_numpy(y_train).float()\nx_eval = torch.from_numpy(x_eval).float()\ny_eval = torch.from_numpy(y_eval).float()\n\ndata_dict = {\"train\": {\"input\": x_train, \"target\": y_train},\n             \"eval\": {\"input\": x_eval, \"target\": y_eval}}\nmodel = MyLSTM(input_dim=input_dim,\n               hidden_dim=hidden_dim,\n               batch_size=seq_len)\nmodel = model.to(device)\nrun_training = True\nif run_training:\n    losses_dict, predictions_dict = train_model(model, data_dict, num_epochs=num_epochs, lr=lr)\nif run_training:\n    \n    fig, ax = plt.subplots(3,1,figsize=(20,20))\n    ax[0].plot(losses_dict[\"train\"], '.-', label=\"train\", c=\"red\")\n    ax[0].set_xlabel(\"Epochs\")\n    ax[0].set_ylabel(\"MSE\")\n    ax[0].plot(losses_dict[\"eval\"], '.-', label=\"eval\", c=\"blue\");\n    ax[0].legend();\n\n    ax[1].plot(predictions_dict[\"train\"], '-o', c=\"red\")\n    ax[1].plot(y_train, '-o', c=\"green\")\n    ax[1].set_title(\"Fitted and true values of y in train\");\n    ax[1].set_ylabel(\"Unit sales y\");\n    ax[1].set_xlabel(\"Number of days in train\");\n\n    ax[2].plot(predictions_dict[\"eval\"], '-o', c=\"red\")\n    ax[2].plot(y_eval, '-o', c=\"green\")\n    ax[2].set_title(\"Predicted and true values of y in eval\");\n    ax[2].set_xlabel(\"Number of days in eval\");\n    ax[2].set_ylabel(\"Unit sales y\");", "processed": ["fit model top level seri class anchor id fit lstm"]}, {"markdown": ["## Check residuals for autocorrelation <a class=\"anchor\" id=\"residuals_checkup\"></a>"], "code": "# Reference: https://www.kaggle.com/code/allunia/m5-sales-uncertainty-prediction\n\nfrom statsmodels.graphics.tsaplots import plot_acf\n\nif run_training:\n    \n    train_residuals = y_train-predictions_dict[\"train\"]\n    eval_residuals = y_eval-predictions_dict[\"eval\"]\n    \n    fig, ax = plt.subplots(2,2,figsize=(20,10))\n    sns.distplot(train_residuals, ax=ax[0,0], color=\"red\")\n    sns.distplot(eval_residuals, ax=ax[0,1], color=\"green\")\n    ax[0,0].set_title(\"Train residuals\")\n    ax[0,1].set_title(\"Eval residuals\")\n    ax[0,0].set_xlabel(\"$y_{true} - y_{pred}$\")\n    ax[0,1].set_xlabel(\"$y_{true} - y_{pred}$\")\n    ax[0,0].set_ylabel(\"density\")\n    ax[0,1].set_ylabel(\"density\")\n    \n    plot_acf(train_residuals, ax=ax[1,0])\n    plot_acf(eval_residuals, ax=ax[1,1])", "processed": ["check residu autocorrel class anchor id residu checkup"]}, {"markdown": ["### Insights\n\n* The residuals in train only show a significant autocorrelation with their previous, 1-lag timepoint. \n* That's great as we are close to uncorrelated residuals that were assumed when using bootstrapped residuals.", "## Computing PIs using bootstrapped residuals <a class=\"anchor\" id=\"bootstrapped_PIs\"></a>\n\nThe idea of computing PIs using bootstrapped residuals is as follows:\n\n1. Fit the model to your data to obtain the fitted values $\\hat{y}_{i}$ and the forcasting errors $\\epsilon_{i} = y_{true, i} - \\hat{y}_{i}$.\n2. Randomly sample a residual $\\epsilon_{i}$ of the distribution of all $\\epsilon_{j}$ to generate a new response variable $y^{*}$ using the fitted value: $y^{*} = \\hat{y}_{i} + \\epsilon_{i}$. \n3. Doing this repeatively we obtain many different, synthetic values for future predictions that we can use to compute prediction intervals.\n\nLet's take a look at a single example first:"], "code": "# Reference: https://www.kaggle.com/code/allunia/m5-sales-uncertainty-prediction\n\nsampled_residuals = np.random.choice(train_residuals[:, 0], size=len(y_train), replace=True)\nsampled_residuals = sampled_residuals.reshape(-1,1)\nnew_response = predictions_dict[\"train\"] + sampled_residuals\nfig, ax = plt.subplots(2,2,figsize=(20,10))\nax[0,0].plot(predictions_dict[\"train\"][0:200], 'o-', color=\"purple\")\nax[0,0].set_title(\"Original fitted values $y_{pred}$ in \")\nax[0,0].set_xlabel(\"200 example days\")\nax[0,0].set_ylim(-0.4, 0.4)\nax[0,0].set_ylabel(\"$y_{fitted}$\")\n\nax[0,1].plot(new_response[0:200,0], 'o-', color=\"orange\")\nax[0,1].set_title(\"Response values $y^{*}$ using sampled residuals\");\nax[0,1].set_xlabel(\"200 example days\")\nax[0,1].set_ylabel(\"$y^{*}$\");\nax[0,1].set_ylim(-0.4, 0.4)\n\nax[1,0].plot(sampled_residuals[0:200], 'o-', color=\"cornflowerblue\")\nax[1,0].set_title(\"Sampled residuals\")\nax[1,0].set_xlabel(\"200 example days\")\nax[1,0].set_ylabel(\"$\\epsilon$\")\n\nax[1,1].plot(y_train[0:200], 'o-', color=\"firebrick\")\nax[1,1].set_title(\"True values $y_{train}$\")\nax[1,1].set_xlabel(\"200 example days\")\nax[1,1].set_ylabel(\"$y_{train}$\");\n", "processed": ["insight residu train show signific autocorrel previou 1 lag timepoint great close uncorrel residu assum use bootstrap residu", "comput pi use bootstrap residu class anchor id bootstrap pi idea comput pi use bootstrap residu follow 1 fit model data obtain fit valu hat forcast error epsilon true hat 2 randomli sampl residu epsilon distribut epsilon j gener new respons variabl use fit valu hat epsilon 3 repe obtain mani differ synthet valu futur predict use comput predict interv let take look singl exampl first"]}, {"markdown": ["We need to compute PIs for evaluation and validation data. In my case, I have splitted the original training timeseries into my own eval and train part. Consequently instead of computing response series for the training data, we need to do this for the evaluation data. While doing so we also need to reverse the preprocessing:"], "code": "# Reference: https://www.kaggle.com/code/allunia/m5-sales-uncertainty-prediction\n\nresponses = []\nfor n in range(100):\n    # sample residuals using the historical residuals found in train\n    sampled_residuals = np.random.choice(train_residuals[:, 0], size=len(y_eval), replace=True)\n    sampled_residuals = sampled_residuals.reshape(-1,1)\n    # create a synthetic future timeseries of eval by adding sampled residuals\n    new_response = predictions_dict[\"eval\"] + sampled_residuals\n    # reverse the scaling\n    new_response = scaler.inverse_transform(new_response)\n    # concat the first value of the evaluation series and the response series\n    new_response = np.hstack((timeseries[train_size], new_response[:,0]))\n    # reverse the differnciation (trend removal) using cumsum\n    new_response = np.cumsum(new_response)\n    # save the future timeseries\n    responses.append(new_response)\n    \nresponses = np.array(responses)\nresponses.shape\ny_eval.shape\nmedian_series = np.median(responses, axis=0)\neval_series = scaler.inverse_transform(y_eval)\neval_series = np.cumsum(np.hstack((timeseries[train_size-1], eval_series[:,0])))\nlow_q = 0.25\nup_q = 0.75\nplt.figure(figsize=(20,5))\nplt.plot(np.arange(0, len(median_series)), median_series, 'o-', label=\"median predicted series\")\nplt.plot(eval_series, '.-', color=\"cornflowerblue\", label=\"true eval series\")\nlower = np.quantile(responses, low_q, axis=0)\nupper = np.quantile(responses, up_q, axis=0)\nplt.fill_between(np.arange(0, len(median_series)), lower, upper, alpha=0.5)\nplt.title(\"Prediction interval {}% of eval timeseries\".format((up_q-low_q)*100));\nplt.xlabel(\"Days in eval\")\nplt.ylabel(\"Unit sales\");\nplt.legend();", "processed": ["need comput pi evalu valid data case split origin train timeseri eval train part consequ instead comput respons seri train data need evalu data also need revers preprocess"]}, {"markdown": ["We can see several things from this overview:\n\n* There is a variety of features: numerical, categorical, text and date;\n* Some columns have missing values: not all users define additional parameters of items or descriptions, sometimes they don't even provide descriptions. In some cases there are no photos of the wares;\n* As expected, there are a lot of unique users and most of them don't post a lot of ads, but there are outliers with 600+ ads;\n* There are 9 categories in parent_category_name and \"\u041b\u0438\u0447\u043d\u044b\u0435 \u0432\u0435\u0449\u0438\" (Private things) have 46,4% of values;\n* price will require a careful processing - the values are skewered and there are some outliers with huge values;\n* It is possible to use images in the analysis, but I'll simply use the fact whether there was image or not;", "## Feature analysis\nWe saw a lot of information about features, so let's now analyze each of them in more details.", "### activation_date\n\nAt first let's create new features based on activation_date: date, weekday and day of month."], "code": "# Reference: https://www.kaggle.com/code/artgor/eda-features-engineering-and-lightgbm\n\ntrain['activation_date'] = pd.to_datetime(train['activation_date'])\ntrain['date'] = train['activation_date'].dt.date\ntrain['weekday'] = train['activation_date'].dt.weekday\ntrain['day'] = train['activation_date'].dt.day\ncount_by_date_train = train.groupby('date')['deal_probability'].count()\nmean_by_date_train = train.groupby('date')['deal_probability'].mean()\n\ntest['activation_date'] = pd.to_datetime(test['activation_date'])\ntest['date'] = test['activation_date'].dt.date\ntest['weekday'] = test['activation_date'].dt.weekday\ntest['day'] = test['activation_date'].dt.day\ncount_by_date_test = test.groupby('date')['item_id'].count()\nfig, (ax1, ax3) = plt.subplots(figsize=(26, 8), ncols=2, sharey=True)\ncount_by_date_train.plot(ax=ax1, legend=False, label='Ads count')\nax1.set_ylabel('Ads count', color='b')\nax2 = ax1.twinx()\nmean_by_date_train.plot(ax=ax2, color='g', legend=False, label='Mean deal_probability')\nax2.set_ylabel('Mean deal_probability', color='g')\ncount_by_date_test.plot(ax=ax3, color='r', legend=False, label='Ads count test')\nplt.grid(False)\n\nax1.title.set_text('Trends of deal_probability and number of ads')\nax3.title.set_text('Trends of number of ads for test data')\nax1.legend(loc=(0.8, 0.35))\nax2.legend(loc=(0.8, 0.2))\nax3.legend(loc=\"upper right\")", "processed": ["see sever thing overview varieti featur numer categor text date column miss valu user defin addit paramet item descript sometim even provid descript case photo ware expect lot uniqu user post lot ad outlier 600 ad 9 categori parent categori name privat thing 46 4 valu price requir care process valu skewer outlier huge valu possibl use imag analysi simpli use fact whether imag", "featur analysi saw lot inform featur let analyz detail", "activ date first let creat new featur base activ date date weekday day month"]}, {"markdown": ["As we can see, we don't only several weeks of data in train and a little more than a week in test\n\n* For most of the data in train the number of ads is quite high (100 000 or more) and mean deal_probability is around 0.14, but after March 28 the number of ads drastically falls so deal_probability fluctuates. I wonder if decreased number of ads is intentional;\n* In test we have a reasonable number of ads up to April 18 and then number of ads become negligible - 64 and 1;\n* As a result I'd suggest not to use train data with too low number of ads (since 2017-03-29);"], "code": "# Reference: https://www.kaggle.com/code/artgor/eda-features-engineering-and-lightgbm\n\nfig, ax1 = plt.subplots(figsize=(16, 8))\nplt.title(\"Ads count and deal_probability by day of week.\")\nsns.countplot(x='weekday', data=train, ax=ax1)\nax1.set_ylabel('Ads count', color='b')\nplt.legend(['Projects count'])\nax2 = ax1.twinx()\nsns.pointplot(x=\"weekday\", y=\"deal_probability\", data=train, ci=99, ax=ax2, color='black')\nax2.set_ylabel('deal_probability', color='g')\nplt.legend(['deal_probability'], loc=(0.875, 0.9))\nplt.grid(False)", "processed": ["see sever week data train littl week test data train number ad quit high 100 000 mean deal probabl around 0 14 march 28 number ad drastic fall deal probabl fluctuat wonder decreas number ad intent test reason number ad april 18 number ad becom neglig 64 1 result suggest use train data low number ad sinc 2017 03 29"]}, {"markdown": ["Most of params belong to clothes or cars.", "## user_type \nThere are three main user_types. Let's see prices of their wares, where prices are below 100000."], "code": "# Reference: https://www.kaggle.com/code/artgor/eda-features-engineering-and-lightgbm\n\nsns.set(rc={'figure.figsize':(15, 8)})\ntrain_ = train[train.price.isnull() == False]\ntrain_ = train.loc[train.price < 100000.0]\nsns.boxplot(x=\"parent_category_name\", y=\"price\", hue=\"user_type\",  data=train_)\nplt.title(\"Price by parent category and user type\")\nplt.xticks(rotation='vertical')\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\nplt.show()", "processed": ["param belong cloth car", "user type three main user type let see price ware price 100000"]}, {"markdown": ["# Data"], "code": "# Reference: https://www.kaggle.com/code/iafoss/severstal-fast-ai-256x256-crops\n\nclass SegmentationLabelList(SegmentationLabelList):\n    def open(self, fn): return open_mask(fn, div=True)\n    \nclass SegmentationItemList(SegmentationItemList):\n    _label_cls = SegmentationLabelList\n\n# Setting transformations on masks to False on test set\ndef transform(self, tfms:Optional[Tuple[TfmList,TfmList]]=(None,None), **kwargs):\n    if not tfms: tfms=(None,None)\n    assert is_listy(tfms) and len(tfms) == 2\n    self.train.transform(tfms[0], **kwargs)\n    self.valid.transform(tfms[1], **kwargs)\n    kwargs['tfm_y'] = False # Test data has no labels\n    if self.test: self.test.transform(tfms[1], **kwargs)\n    return self\nfastai.data_block.ItemLists.transform = transform\n\ndef open_mask(fn:PathOrStr, div:bool=True, convert_mode:str='L', cls:type=ImageSegment,\n        after_open:Callable=None)->ImageSegment:\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\", UserWarning)\n        #generate empty mask if file doesn't exist\n        x = PIL.Image.open(fn).convert(convert_mode) \\\n          if Path(fn).exists() \\\n          else PIL.Image.fromarray(np.zeros((sz,sz)).astype(np.uint8))\n    if after_open: x = after_open(x)\n    x = pil2tensor(x,np.float32)\n    return cls(x)\ndf = pd.read_csv(HARD_NEGATIVE)\ndf['index'] = df.index\ndf.plot(x='index', y='pixels', kind = 'line');\nplt.yscale('log')\nstats = ([0.396,0.396,0.396], [0.179,0.179,0.179])\n#check https://www.kaggle.com/iafoss/256x256-images-with-defects for stats\n\n#the code below eliminates sharing patches of the same image across folds\nimg_p = set([p.stem[:-2] for p in Path(TRAIN).ls()])\n#select 12000 of the most difficult negative exaples\nneg = list(pd.read_csv(HARD_NEGATIVE).head(12000).fname)\nneg = [Path(TRAIN_N)/f for f in neg]\nimg_n = set([p.stem for p in neg])\nimg_set = img_p | img_n\nimg_p_list = sorted(img_p)\nimg_n_list = sorted(img_n)\nimg_list = img_p_list + img_n_list\nkf = KFold(n_splits=nfolds, shuffle=True, random_state=SEED)\n\ndef get_data(fold):\n    #split with making sure that crops of the same original image \n    #are not shared between folds, so additional training and validation \n    #could be done on full images later\n    valid_idx = list(kf.split(list(range(len(img_list)))))[fold][1]\n    valid = set([img_list[i] for i in valid_idx])\n    valid_idx = []\n    for i,p in enumerate(Path(TRAIN).ls() + neg):\n        if p.stem[:-2] in valid: valid_idx.append(i)\n            \n    # Create databunch\n    sl = SegmentationItemList.from_folder(TRAIN)\n    sl.items = np.array((list(sl.items) + neg))\n    data = (sl.split_by_idx(valid_idx)\n        .label_from_func(lambda x : str(x).replace('/images', '/masks'), classes=[0,1,2,3,4])\n        .transform(get_transforms(xtra_tfms=dihedral()), size=sz, tfm_y=True)\n        .databunch(path=Path('.'), bs=bs)\n        .normalize(stats))\n    return data\n\n# Display some images with masks\nget_data(0).show_batch()\n@dataclass\nclass CSVLogger(LearnerCallback):\n    def __init__(self, learn, filename= 'history'):\n        self.learn = learn\n        self.path = self.learn.path/f'{filename}.csv'\n        self.file = None\n\n    @property\n    def header(self):\n        return self.learn.recorder.names\n\n    def read_logged_file(self):\n        return pd.read_csv(self.path)\n\n    def on_train_begin(self, metrics_names: StrList, **kwargs: Any) -> None:\n        self.path.parent.mkdir(parents=True, exist_ok=True)\n        e = self.path.exists()\n        self.file = self.path.open('a')\n        if not e: self.file.write(','.join(self.header) + '\\n')\n\n    def on_epoch_end(self, epoch: int, smooth_loss: Tensor, last_metrics: MetricsList, **kwargs: Any) -> bool:\n        self.write_stats([epoch, smooth_loss] + last_metrics)\n\n    def on_train_end(self, **kwargs: Any) -> None:\n        self.file.flush()\n        self.file.close()\n\n    def write_stats(self, stats: TensorOrNumList) -> None:\n        stats = [str(stat) if isinstance(stat, int) else f'{stat:.6f}'\n                 for name, stat in zip(self.header, stats)]\n        str_stats = ','.join(stats)\n        self.file.write(str_stats + '\\n')", "processed": ["data"]}, {"markdown": ["### First Exploration", "Now, I will explore the data in this particular dataframe and see if I can derive any useful insights from it.", "### center_x and center_y", "**center_x** and **center_y** correspond to the *x* and *y* coordinates of the center of an object's location (bounding volume). These coordinates represent the location of an object on the *x-y* plane.", "### Distributions of *center_x* and *center_y*"], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/lyft-competition-understanding-the-data\n\nfig, ax = plt.subplots(figsize=(10, 10))\nsns.distplot(train_objects['center_x'], color='darkorange', ax=ax).set_title('center_x and center_y', fontsize=16)\nsns.distplot(train_objects['center_y'], color='purple', ax=ax).set_title('center_x and center_y', fontsize=16)\nplt.xlabel('center_x and center_y', fontsize=15)\nplt.show()", "processed": ["first explor", "explor data particular datafram see deriv use insight", "center x center", "center x center correspond x coordin center object locat bound volum coordin repres locat object x plane", "distribut center x center"]}, {"markdown": ["In the diagram above, the purple distribution is that of *center_y* and the orange distribution is that of *center_x*. From the diagram above, we can see that the distributions of both *center_x* and *center_y* have multiple peaks, and are therefore multimodal. Both distributions also have a clear rightward or positive skew. But, the distribution of *center_y* (purple) has a signficantly higher skew that the the distribution of *center_x* (orange). The *center_x* distribution is more evenly spread out. \n\nThis indicates that objects are spread out very evenly along the *x-axis*, but not likewise along the *y-axis*. This is probably because the car's camera can sense objects on either left or right easily (along the *x-axis*) due to the width of the road being small. But, since the length of the road is much greater than its width, and there is a higher chance of the camera's view being blocked from this angle, the camera can only find objects narrowly ahead or narrowly behind (and not further away).", "### Relationship between *center_x* and *center_y*", "### KDE Plot"], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/lyft-competition-understanding-the-data\n\nnew_train_objects = train_objects.query('class_name == \"car\"')\nplot = sns.jointplot(x=new_train_objects['center_x'][:1000], y=new_train_objects['center_y'][:1000], kind='kde', color='blueviolet')\nplot.set_axis_labels('center_x', 'center_y', fontsize=16)\nplt.show()", "processed": ["diagram purpl distribut center orang distribut center x diagram see distribut center x center multipl peak therefor multimod distribut also clear rightward posit skew distribut center purpl signficantli higher skew distribut center x orang center x distribut evenli spread indic object spread evenli along x axi likewis along axi probabl car camera sen object either left right easili along x axi due width road small sinc length road much greater width higher chanc camera view block angl camera find object narrowli ahead narrowli behind away", "relationship center x center", "kde plot"]}, {"markdown": ["In the KDE plot above, we can see that *center_x* and *center_y* seem to have a somewhat negative correlation. This is probably, once again, due to the limitations of the camera system. The camera can detect objects that are far ahead, but not too far to the side. And, it can also detect objects that are far to side, but not too far ahead. But, **the camera cannot detect objects that are both far ahead and far to the side**. Because of this, objects that are far ahead and far to the side are not detected at all, and only objects which satisfy one (or none) of those conditions are detected. This results in a negative relationship between *center_x* and *center_y*.", "### center_z", "**center_z** corresponds to the *xz* coordinate of the center of an object's location (bounding volume). This coordinate represents the height of the object above the *x-y* plane.", "### Distribution of *center_z*"], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/lyft-competition-understanding-the-data\n\nfig, ax = plt.subplots(figsize=(10, 10))\nsns.distplot(train_objects['center_z'], color='navy', ax=ax).set_title('center_z', fontsize=16)\nplt.xlabel('center_z', fontsize=15)\nplt.show()", "processed": ["kde plot see center x center seem somewhat neg correl probabl due limit camera system camera detect object far ahead far side also detect object far side far ahead camera detect object far ahead far side object far ahead far side detect object satisfi one none condit detect result neg relationship center x center", "center z", "center z correspond xz coordin center object locat bound volum coordin repres height object x plane", "distribut center z"]}, {"markdown": ["In the above diagram, we can see that the distribution of *center_z* has an extremely high positive (rightward) skew and is clustered around the -20 mark (which is approximates its mean value). The variation (spread) of *center_z* is significantly smaller than that of *center_x* and *center_y*. This is probably because most objects are very close to the flat plane of the road, and therefore, there is no great variation in the height of the objects above (or below) the camera. There is understandably much greater variation in the *x* and *y* coordiantes of the object.\n\nAlso, most *z* coordinates are negative because the camera is attached at the top of the car. So, most of the times, the camera has to \"look down\" to see the objects. Therefore, the height or *z*-coordinate of the objects relative to the camera are generally negative.", "### yaw", "**yaw** is the angle of the volume around the *z*-axis, making 'yaw' the direction the front of the vehicle / bounding box is pointing at while on the ground.", "### Distribution of *yaw*"], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/lyft-competition-understanding-the-data\n\nfig, ax = plt.subplots(figsize=(10, 10))\nsns.distplot(train_objects['yaw'], color='darkgreen', ax=ax).set_title('yaw', fontsize=16)\nplt.xlabel('yaw', fontsize=15)\nplt.show()", "processed": ["diagram see distribut center z extrem high posit rightward skew cluster around 20 mark approxim mean valu variat spread center z significantli smaller center x center probabl object close flat plane road therefor great variat height object camera understand much greater variat x coordiant object also z coordin neg camera attach top car time camera look see object therefor height z coordin object rel camera gener neg", "yaw", "yaw angl volum around z axi make yaw direct front vehicl bound box point ground", "distribut yaw"]}, {"markdown": ["In the diagram above, we can see that the distribution of *yaw* is roughly bimodal, *i.e.*, there are two mmajor peaks in the distribution. One of the peaks is around 0.5 and the other is around 2.5. One can estimate that the mean is between 1 and 2 (around 1.5). The distribution does not have any clear skew. The presence of the two peaks at symmetric positions reduces the skew in both directions (and they cancel out), making the distribution more balanced than the distributions of *center_x*, *center_y*, and *center_z*.", "### width", "**width** is simply the width of the bounding volume in which the object lies."], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/lyft-competition-understanding-the-data\n\nfig, ax = plt.subplots(figsize=(10, 10))\nsns.distplot(train_objects['width'], color='magenta', ax=ax).set_title('width', fontsize=16)\nplt.xlabel('width', fontsize=15)\nplt.show()", "processed": ["diagram see distribut yaw roughli bimod e two mmajor peak distribut one peak around 0 5 around 2 5 one estim mean 1 2 around 1 5 distribut clear skew presenc two peak symmetr posit reduc skew direct cancel make distribut balanc distribut center x center center z", "width", "width simpli width bound volum object lie"]}, {"markdown": ["In the diagram above, we can see that the *width* is approximately normally distirbuted with a mean of around 2, with some outliers on either side. The majority of the objects are cars (as we will see later), and these constitute a width of around 2 (at the peak). The outliers on the right represent larger objecs like trucks and vans, and the outliers on the left represent smaller objects like pedestrians and bicycles.", "### length", "**length** is simply the length of the bounding volume in which the object lies."], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/lyft-competition-understanding-the-data\n\nfig, ax = plt.subplots(figsize=(10, 10))\nsns.distplot(train_objects['length'], color='crimson', ax=ax).set_title('length', fontsize=16)\nplt.xlabel('length', fontsize=15)\nplt.show()", "processed": ["diagram see width approxim normal distirbut mean around 2 outlier either side major object car see later constitut width around 2 peak outlier right repres larger objec like truck van outlier left repres smaller object like pedestrian bicycl", "length", "length simpli length bound volum object lie"]}, {"markdown": ["In the diagram above, we can see that the *length* has a distribution with a strong positive (rightward skew) with a mean of around 5, with some outliers on either side. The majority of the objects are cars (as we will see later), and these constitute a length of around 5 (at the peak). The outliers on the right represent larger objecs like trucks and vans, and the outliers on the left represent smaller objects like pedestrians and bicycles.", "### height", "**height** is simply the height of the bounding volume in which the object lies."], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/lyft-competition-understanding-the-data\n\nfig, ax = plt.subplots(figsize=(10, 10))\nsns.distplot(train_objects['height'], color='indigo', ax=ax).set_title('height', fontsize=16)\nplt.xlabel('height', fontsize=15)\nplt.show()", "processed": ["diagram see length distribut strong posit rightward skew mean around 5 outlier either side major object car see later constitut length around 5 peak outlier right repres larger objec like truck van outlier left repres smaller object like pedestrian bicycl", "height", "height simpli height bound volum object lie"]}, {"markdown": ["In the diagram above, we can see that the *height* has a distribution with a strong positive (rightward skew) with a mean of around 2, with some outliers on either side. The majority of the objects are cars (as we will see later), and these constitute a length of around 2 (at the peak). The outliers on the right represent larger objecs like trucks and vans, and the outliers on the left represent smaller objects like pedestrians and bicycles.", "### Frequency of object classes"], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/lyft-competition-understanding-the-data\n\nfig, ax = plt.subplots(figsize=(10, 10))\nplot = sns.countplot(y=\"class_name\", data=train_objects.query('class_name != \"motorcycle\" and class_name != \"emergency_vehicle\" and class_name != \"animal\"'),\n                     palette=['navy', 'darkblue', 'blue', 'dodgerblue', 'skyblue', 'lightblue']).set_title('Object Frequencies', fontsize=16)\nplt.yticks(fontsize=14)\nplt.xlabel(\"Count\", fontsize=15)\nplt.ylabel(\"Class Name\", fontsize=15)\nplt.show(plot)", "processed": ["diagram see height distribut strong posit rightward skew mean around 2 outlier either side major object car see later constitut length around 2 peak outlier right repres larger objec like truck van outlier left repres smaller object like pedestrian bicycl", "frequenc object class"]}, {"markdown": ["From the above diagram, it can be seen that the most common object class in the dataset is \"car\". This is unsurprising because the images are taken from the streets of Palo Alto in Silicon Valley, California. And, the most common vehicle (or entity for that matter) visible on those roads are cars. All the other object classes are nowhere near cars in terms of frequency.", "### center_x *vs.* class_name", "In the plots below, I will explore how the distribution of **center_x** changes for different object **class_names**."], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/lyft-competition-understanding-the-data\n\nfig, ax = plt.subplots(figsize=(15, 15))\n\nplot = sns.violinplot(x=\"class_name\", y=\"center_x\",\n                      data=train_objects.query('class_name != \"motorcycle\" and class_name != \"emergency_vehicle\" and class_name != \"animal\"'),\n                      palette='YlGnBu',\n                      split=True, ax=ax).set_title('center_x (for different objects)', fontsize=16)\n\nplt.yticks(fontsize=14)\nplt.xticks(fontsize=14)\nplt.xlabel(\"Class Name\", fontsize=15)\nplt.ylabel(\"center_x\", fontsize=15)\nplt.show(plot)", "processed": ["diagram seen common object class dataset car unsurpris imag taken street palo alto silicon valley california common vehicl entiti matter visibl road car object class nowher near car term frequenc", "center x v class name", "plot explor distribut center x chang differ object class name"]}, {"markdown": ["In the violin plots above, we can see that the distributions of *center_x* for large vehicles including trucks, buses, and other vehicles are well spread. They barely have any skew and have greater means than the distributions for pedestrians and bicycles. This is probably because these large vehicles tend to keep greater distances from the other vehicles, and the smaller vehicles do not stay too close to these large vehicles in order to avoid accidents. Therefore, the mean *center_x* is clearly greater for larger vehicles like buses and trucks.\n\nContrastingly, the smaller objects like pedestrians and bicycles have *center_x* distributions with strong positive (rightward) skews. These distributions also have clearly lower means than the distributions for the larger vehicles. This is probably because pedestrians (road-crossers) and bicyclists do not need to maintain large distances with cars and trucks to avoid accidents. They usually cross the road during a red traffic signal, when the traffic halts."], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/lyft-competition-understanding-the-data\n\nfig, ax = plt.subplots(figsize=(15, 15))\n\nplot = sns.boxplot(x=\"class_name\", y=\"center_x\",\n                   data=train_objects.query('class_name != \"motorcycle\" and class_name != \"emergency_vehicle\" and class_name != \"animal\"'),\n                   palette='YlGnBu', ax=ax).set_title('center_x (for different objects)', fontsize=16)\n\nplt.yticks(fontsize=14)\nplt.xticks(fontsize=14)\nplt.xlabel(\"Class Name\", fontsize=15)\nplt.ylabel(\"center_x\", fontsize=15)\nplt.show(plot)", "processed": ["violin plot see distribut center x larg vehicl includ truck buse vehicl well spread bare skew greater mean distribut pedestrian bicycl probabl larg vehicl tend keep greater distanc vehicl smaller vehicl stay close larg vehicl order avoid accid therefor mean center x clearli greater larger vehicl like buse truck contrastingli smaller object like pedestrian bicycl center x distribut strong posit rightward skew distribut also clearli lower mean distribut larger vehicl probabl pedestrian road crosser bicyclist need maintain larg distanc car truck avoid accid usual cross road red traffic signal traffic halt"]}, {"markdown": ["In the box plots above, we can notice the same observation as in the violin plot above. The *center_x* distributions for smaller objects like pedestrians and bicycles have very low mean and quartile values as compared to larger objects like cars, trucks, and buses.", "### center_y *vs.* class_name", "In the plots below, I will explore how the distribution of **center_y** changes for different object **class_names**."], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/lyft-competition-understanding-the-data\n\nfig, ax = plt.subplots(figsize=(15, 15))\n\nplot = sns.violinplot(x=\"class_name\", y=\"center_y\",\n                      data=train_objects.query('class_name != \"motorcycle\" and class_name != \"emergency_vehicle\" and class_name != \"animal\"'),\n                      palette='YlOrRd',\n                      split=True, ax=ax).set_title('center_y (for different objects)', fontsize=16)\n\nplt.yticks(fontsize=14)\nplt.xticks(fontsize=14)\nplt.xlabel(\"Class Name\", fontsize=15)\nplt.ylabel(\"center_y\", fontsize=15)\nplt.show(plot)", "processed": ["box plot notic observ violin plot center x distribut smaller object like pedestrian bicycl low mean quartil valu compar larger object like car truck buse", "center v class name", "plot explor distribut center chang differ object class name"]}, {"markdown": ["In the violin plots above, we can see that the distributions of *center_y* for small objects including pedestrians and bicycles have a greater mean value than large objects like trucks and buses. The distributions for the small objects have much greater probability density concentrated at higher values of *center_y* as compared to large objects. This signifies that small objects, in general, have greater *center_y* values than large objects. \n\nThis is probably because the large vehicles tend to be within the field of view of the camera due to their large size. But, smaller objects like bicycles and pedestrians cannot remain in the field of view of the camera when they are too close. Therefore, most pedestrains and bicycles that are detected tend to be far away. This causes the *center_y* to be greater (on average) for small objects as compared to large objects."], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/lyft-competition-understanding-the-data\n\nfig, ax = plt.subplots(figsize=(15, 15))\n\nplot = sns.boxplot(x=\"class_name\", y=\"center_y\",\n                   data=train_objects.query('class_name != \"motorcycle\" and class_name != \"emergency_vehicle\" and class_name != \"animal\"'),\n                   palette='YlOrRd', ax=ax).set_title('center_y (for different objects)', fontsize=16)\n\nplt.yticks(fontsize=14)\nplt.xticks(fontsize=14)\nplt.xlabel(\"Class Name\", fontsize=15)\nplt.ylabel(\"center_y\", fontsize=15)\nplt.show(plot)", "processed": ["violin plot see distribut center small object includ pedestrian bicycl greater mean valu larg object like truck buse distribut small object much greater probabl densiti concentr higher valu center compar larg object signifi small object gener greater center valu larg object probabl larg vehicl tend within field view camera due larg size smaller object like bicycl pedestrian remain field view camera close therefor pedestrain bicycl detect tend far away caus center greater averag small object compar larg object"]}, {"markdown": ["In the box plots above, we can notice the same observation as in the violin plot above. The *center_y* distributions for smaller objects like pedestrians and bicycles have much larger mean and quartile values as compared to larger objects like cars, trucks, and buses.", "### center_z *vs.* class_name", "In the plots below, I will explore how the distribution of **center_z** changes for different object **class_names**."], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/lyft-competition-understanding-the-data\n\nfig, ax = plt.subplots(figsize=(15, 15))\n\nplot = sns.violinplot(x=\"class_name\", y=\"center_z\",\n                      data=train_objects.query('class_name != \"motorcycle\" and class_name != \"emergency_vehicle\" and class_name != \"animal\"').query('center_z <= -5'),\n                      palette='RdPu',\n                      split=True, ax=ax).set_title('center_z (for different objects)', fontsize=16)\n\nplt.yticks(fontsize=14)\nplt.xticks(fontsize=14)\nplt.xlabel(\"Class Name\", fontsize=15)\nplt.ylabel(\"center_z\", fontsize=15)\nplt.show(plot)", "processed": ["box plot notic observ violin plot center distribut smaller object like pedestrian bicycl much larger mean quartil valu compar larger object like car truck buse", "center z v class name", "plot explor distribut center z chang differ object class name"]}, {"markdown": ["In the violin plots above, we can see that the distributions of *center_z* for small objects including pedestrians and bicycles have a significantly smaller mean value than large objects like trucks and buses. The distributions for the small objects have much greater probability density concentrated at lower values of *center_z* as compared to large objects. This signifies that small objects, in general, have smaller *center_y* values than large objects. \n\nThis is probably because smaller objects like pedestrians and bicycles tend to have a lower height with repsect to the camera. And, on the other hand, larger objects like cars, trucks, and buses tend to have a greater height with respect to the camera."], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/lyft-competition-understanding-the-data\n\nfig, ax = plt.subplots(figsize=(15, 15))\n\nplot = sns.boxplot(x=\"class_name\", y=\"center_z\",\n                   data=train_objects.query('class_name != \"motorcycle\" and class_name != \"emergency_vehicle\" and class_name != \"animal\"').query('center_z <= -5'),\n                   palette='RdPu', ax=ax).set_title('center_z (for different objects)', fontsize=16)\n\nplt.yticks(fontsize=14)\nplt.xticks(fontsize=14)\nplt.xlabel(\"Class Name\", fontsize=15)\nplt.ylabel(\"center_z\", fontsize=15)\nplt.show(plot)", "processed": ["violin plot see distribut center z small object includ pedestrian bicycl significantli smaller mean valu larg object like truck buse distribut small object much greater probabl densiti concentr lower valu center z compar larg object signifi small object gener smaller center valu larg object probabl smaller object like pedestrian bicycl tend lower height repsect camera hand larger object like car truck buse tend greater height respect camera"]}, {"markdown": ["In the box plots above, we can notice the same observation as in the violin plot above. The *center_z* distributions for smaller objects like pedestrians and bicycles have much smaller mean and quartile values as compared to larger objects like cars, trucks, and buses.", "### width *vs.* class_name", "In the plots below, I will explore how the distribution of **width** changes for different object **class_names**."], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/lyft-competition-understanding-the-data\n\nfig, ax = plt.subplots(figsize=(15, 15))\n\nplot = sns.violinplot(x=\"class_name\", y=\"width\",\n                      data=train_objects.query('class_name != \"motorcycle\" and class_name != \"emergency_vehicle\" and class_name != \"animal\"'),\n                      palette='YlGn',\n                      split=True, ax=ax).set_title('width (for different objects)', fontsize=16)\n\nplt.yticks(fontsize=14)\nplt.xticks(fontsize=14)\nplt.xlabel(\"Class Name\", fontsize=15)\nplt.ylabel(\"width\", fontsize=15)\nplt.show(plot)", "processed": ["box plot notic observ violin plot center z distribut smaller object like pedestrian bicycl much smaller mean quartil valu compar larger object like car truck buse", "width v class name", "plot explor distribut width chang differ object class name"]}, {"markdown": ["In the violin plots, we can clearly see that the *width* distributions for large vehicles like cars, buses, and trucks have much larger means as compared to small objects like pedestrians and bicycles. This is not surprising because trucks, buses, and cars almost always have much greater width than pedestrians and bicycles."], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/lyft-competition-understanding-the-data\n\nfig, ax = plt.subplots(figsize=(15, 15))\n\nplot = sns.boxplot(x=\"class_name\", y=\"width\",\n                   data=train_objects.query('class_name != \"motorcycle\" and class_name != \"emergency_vehicle\" and class_name != \"animal\"'),\n                   palette='YlGn', ax=ax).set_title('width (for different objects)', fontsize=16)\n\nplt.yticks(fontsize=14)\nplt.xticks(fontsize=14)\nplt.xlabel(\"Class Name\", fontsize=15)\nplt.ylabel(\"width\", fontsize=15)\nplt.show(plot)", "processed": ["violin plot clearli see width distribut larg vehicl like car buse truck much larger mean compar small object like pedestrian bicycl surpris truck buse car almost alway much greater width pedestrian bicycl"]}, {"markdown": ["In the box plots above, we can notice the same observation as in the violin plot above. The *width* distributions for smaller objects like pedestrians and bicycles have much smaller mean and quartile values as compared to larger objects like cars, trucks, and buses.", "### length *vs.* class_name", "In the plots below, I will explore how the distribution of **length** changes for different object **class_names**."], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/lyft-competition-understanding-the-data\n\nfig, ax = plt.subplots(figsize=(15, 15))\n\nplot = sns.violinplot(x=\"class_name\", y=\"length\",\n                      data=train_objects.query('class_name != \"motorcycle\" and class_name != \"emergency_vehicle\" and class_name != \"animal\" and length < 15'),\n                      palette='Purples',\n                      split=True, ax=ax).set_title('length (for different objects)', fontsize=16)\n\nplt.yticks(fontsize=14)\nplt.xticks(fontsize=14)\nplt.xlabel(\"Class Name\", fontsize=15)\nplt.ylabel(\"length\", fontsize=15)\nplt.show(plot)", "processed": ["box plot notic observ violin plot width distribut smaller object like pedestrian bicycl much smaller mean quartil valu compar larger object like car truck buse", "length v class name", "plot explor distribut length chang differ object class name"]}, {"markdown": ["In the violin plots, we can clearly see that the *length* distributions for large vehicles like cars, buses, and trucks have much larger means as compared to small objects like pedestrians and bicycles. This is not surprising because trucks, buses, and cars almost always have much greater length than pedestrians and bicycles."], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/lyft-competition-understanding-the-data\n\nfig, ax = plt.subplots(figsize=(15, 15))\n\nplot = sns.boxplot(x=\"class_name\", y=\"length\",\n                   data=train_objects.query('class_name != \"motorcycle\" and class_name != \"emergency_vehicle\" and class_name != \"animal\" and length < 15'),\n                   palette='Purples', ax=ax).set_title('length (for different objects)', fontsize=16)\n\nplt.yticks(fontsize=14)\nplt.xticks(fontsize=14)\nplt.xlabel(\"Class Name\", fontsize=15)\nplt.ylabel(\"length\", fontsize=15)\nplt.show(plot)", "processed": ["violin plot clearli see length distribut larg vehicl like car buse truck much larger mean compar small object like pedestrian bicycl surpris truck buse car almost alway much greater length pedestrian bicycl"]}, {"markdown": ["In the box plots above, we can notice the same observation as in the violin plot above. The *length* distributions for smaller objects like pedestrians and bicycles have much smaller mean and quartile values as compared to larger objects like cars, trucks, and buses.", "### height *vs.* class_name", "In the plots below, I will explore how the distribution of **height** changes for different object **class_names**."], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/lyft-competition-understanding-the-data\n\nfig, ax = plt.subplots(figsize=(15, 15))\n\nplot = sns.violinplot(x=\"class_name\", y=\"height\",\n                      data=train_objects.query('class_name != \"motorcycle\" and class_name != \"emergency_vehicle\" and class_name != \"animal\" and height < 6'),\n                      palette='Reds',\n                      split=True, ax=ax).set_title('height (for different objects)', fontsize=16)\n\nplt.yticks(fontsize=14)\nplt.xticks(fontsize=14)\nplt.xlabel(\"Class Name\", fontsize=15)\nplt.ylabel(\"height\", fontsize=15)\nplt.show(plot)", "processed": ["box plot notic observ violin plot length distribut smaller object like pedestrian bicycl much smaller mean quartil valu compar larger object like car truck buse", "height v class name", "plot explor distribut height chang differ object class name"]}, {"markdown": ["In the violin plots, we can clearly see that the *length* distributions for large vehicles like buses and trucks have much larger means as compared to small objects like pedestrians and bicycles. This is not surprising because trucks and buses almost always have much greater length than pedestrians and bicycles.\n\nThe only exception to this trend are the cars. They tend to have a similar height to that of pedestrians."], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/lyft-competition-understanding-the-data\n\nfig, ax = plt.subplots(figsize=(15, 15))\n\nplot = sns.boxplot(x=\"class_name\", y=\"height\",\n                   data=train_objects.query('class_name != \"motorcycle\" and class_name != \"emergency_vehicle\" and class_name != \"animal\" and height < 6'),\n                   palette='Reds', ax=ax).set_title('height (for different objects)', fontsize=16)\n\nplt.yticks(fontsize=14)\nplt.xticks(fontsize=14)\nplt.xlabel(\"Class Name\", fontsize=15)\nplt.ylabel(\"height\", fontsize=15)\nplt.show(plot)", "processed": ["violin plot clearli see length distribut larg vehicl like buse truck much larger mean compar small object like pedestrian bicycl surpris truck buse almost alway much greater length pedestrian bicycl except trend car tend similar height pedestrian"]}, {"markdown": ["In the box plots above, we can notice the same observation as in the violin plot above. The *height* distributions for smaller objects like pedestrians and bicycles have much smaller mean and quartile values as compared to larger objects like cars, trucks, and buses.\n\nOnce again, the only exception to this trend are the cars. They tend to have a similar height to that of pedestrians.", "# Digging into the image and LiDAR data", "### Define some functions to help create the *LyftDataset* class\n#### (click CODE on the right side)"], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/lyft-competition-understanding-the-data\n\n# Lyft Dataset SDK dev-kit.\n# Code written by Oscar Beijbom, 2018.\n# Licensed under the Creative Commons [see licence.txt]\n# Modified by Vladimir Iglovikov 2019.\n\nclass PointCloud(ABC):\n    \"\"\"\n    Abstract class for manipulating and viewing point clouds.\n    Every point cloud (lidar and radar) consists of points where:\n    - Dimensions 0, 1, 2 represent x, y, z coordinates.\n        These are modified when the point cloud is rotated or translated.\n    - All other dimensions are optional. Hence these have to be manually modified if the reference frame changes.\n    \"\"\"\n\n    def __init__(self, points: np.ndarray):\n        \"\"\"\n        Initialize a point cloud and check it has the correct dimensions.\n        :param points: <np.float: d, n>. d-dimensional input point cloud matrix.\n        \"\"\"\n        assert points.shape[0] == self.nbr_dims(), (\n            \"Error: Pointcloud points must have format: %d x n\" % self.nbr_dims()\n        )\n        self.points = points\n\n    @staticmethod\n    @abstractmethod\n    def nbr_dims() -> int:\n        \"\"\"Returns the number of dimensions.\n        Returns: Number of dimensions.\n        \"\"\"\n        pass\n\n    @classmethod\n    @abstractmethod\n    def from_file(cls, file_name: str) -> \"PointCloud\":\n        \"\"\"Loads point cloud from disk.\n        Args:\n            file_name: Path of the pointcloud file on disk.\n        Returns: PointCloud instance.\n        \"\"\"\n        pass\n\n    @classmethod\n    def from_file_multisweep(\n        cls, lyftd, sample_rec: Dict, chan: str, ref_chan: str, num_sweeps: int = 26, min_distance: float = 1.0\n    ) -> Tuple[\"PointCloud\", np.ndarray]:\n        \"\"\"Return a point cloud that aggregates multiple sweeps.\n        As every sweep is in a different coordinate frame, we need to map the coordinates to a single reference frame.\n        As every sweep has a different timestamp, we need to account for that in the transformations and timestamps.\n        Args:\n            lyftd: A LyftDataset instance.\n            sample_rec: The current sample.\n            chan: The radar channel from which we track back n sweeps to aggregate the point cloud.\n            ref_chan: The reference channel of the current sample_rec that the point clouds are mapped to.\n            num_sweeps: Number of sweeps to aggregated.\n            min_distance: Distance below which points are discarded.\n        Returns: (all_pc, all_times). The aggregated point cloud and timestamps.\n        \"\"\"\n\n        # Init\n        points = np.zeros((cls.nbr_dims(), 0))\n        all_pc = cls(points)\n        all_times = np.zeros((1, 0))\n\n        # Get reference pose and timestamp\n        ref_sd_token = sample_rec[\"data\"][ref_chan]\n        ref_sd_rec = lyftd.get(\"sample_data\", ref_sd_token)\n        ref_pose_rec = lyftd.get(\"ego_pose\", ref_sd_rec[\"ego_pose_token\"])\n        ref_cs_rec = lyftd.get(\"calibrated_sensor\", ref_sd_rec[\"calibrated_sensor_token\"])\n        ref_time = 1e-6 * ref_sd_rec[\"timestamp\"]\n\n        # Homogeneous transform from ego car frame to reference frame\n        ref_from_car = transform_matrix(ref_cs_rec[\"translation\"], Quaternion(ref_cs_rec[\"rotation\"]), inverse=True)\n\n        # Homogeneous transformation matrix from global to _current_ ego car frame\n        car_from_global = transform_matrix(\n            ref_pose_rec[\"translation\"], Quaternion(ref_pose_rec[\"rotation\"]), inverse=True\n        )\n\n        # Aggregate current and previous sweeps.\n        sample_data_token = sample_rec[\"data\"][chan]\n        current_sd_rec = lyftd.get(\"sample_data\", sample_data_token)\n        for _ in range(num_sweeps):\n            # Load up the pointcloud.\n            current_pc = cls.from_file(lyftd.data_path / ('train_' + current_sd_rec[\"filename\"]))\n\n            # Get past pose.\n            current_pose_rec = lyftd.get(\"ego_pose\", current_sd_rec[\"ego_pose_token\"])\n            global_from_car = transform_matrix(\n                current_pose_rec[\"translation\"], Quaternion(current_pose_rec[\"rotation\"]), inverse=False\n            )\n\n            # Homogeneous transformation matrix from sensor coordinate frame to ego car frame.\n            current_cs_rec = lyftd.get(\"calibrated_sensor\", current_sd_rec[\"calibrated_sensor_token\"])\n            car_from_current = transform_matrix(\n                current_cs_rec[\"translation\"], Quaternion(current_cs_rec[\"rotation\"]), inverse=False\n            )\n\n            # Fuse four transformation matrices into one and perform transform.\n            trans_matrix = reduce(np.dot, [ref_from_car, car_from_global, global_from_car, car_from_current])\n            current_pc.transform(trans_matrix)\n\n            # Remove close points and add timevector.\n            current_pc.remove_close(min_distance)\n            time_lag = ref_time - 1e-6 * current_sd_rec[\"timestamp\"]  # positive difference\n            times = time_lag * np.ones((1, current_pc.nbr_points()))\n            all_times = np.hstack((all_times, times))\n\n            # Merge with key pc.\n            all_pc.points = np.hstack((all_pc.points, current_pc.points))\n\n            # Abort if there are no previous sweeps.\n            if current_sd_rec[\"prev\"] == \"\":\n                break\n            else:\n                current_sd_rec = lyftd.get(\"sample_data\", current_sd_rec[\"prev\"])\n\n        return all_pc, all_times\n\n    def nbr_points(self) -> int:\n        \"\"\"Returns the number of points.\"\"\"\n        return self.points.shape[1]\n\n    def subsample(self, ratio: float) -> None:\n        \"\"\"Sub-samples the pointcloud.\n        Args:\n            ratio: Fraction to keep.\n        \"\"\"\n        selected_ind = np.random.choice(np.arange(0, self.nbr_points()), size=int(self.nbr_points() * ratio))\n        self.points = self.points[:, selected_ind]\n\n    def remove_close(self, radius: float) -> None:\n        \"\"\"Removes point too close within a certain radius from origin.\n        Args:\n            radius: Radius below which points are removed.\n        Returns:\n        \"\"\"\n        x_filt = np.abs(self.points[0, :]) < radius\n        y_filt = np.abs(self.points[1, :]) < radius\n        not_close = np.logical_not(np.logical_and(x_filt, y_filt))\n        self.points = self.points[:, not_close]\n\n    def translate(self, x: np.ndarray) -> None:\n        \"\"\"Applies a translation to the point cloud.\n        Args:\n            x: <np.float: 3, 1>. Translation in x, y, z.\n        \"\"\"\n        for i in range(3):\n            self.points[i, :] = self.points[i, :] + x[i]\n\n    def rotate(self, rot_matrix: np.ndarray) -> None:\n        \"\"\"Applies a rotation.\n        Args:\n            rot_matrix: <np.float: 3, 3>. Rotation matrix.\n        Returns:\n        \"\"\"\n        self.points[:3, :] = np.dot(rot_matrix, self.points[:3, :])\n\n    def transform(self, transf_matrix: np.ndarray) -> None:\n        \"\"\"Applies a homogeneous transform.\n        Args:\n            transf_matrix: transf_matrix: <np.float: 4, 4>. Homogenous transformation matrix.\n        \"\"\"\n        self.points[:3, :] = transf_matrix.dot(np.vstack((self.points[:3, :], np.ones(self.nbr_points()))))[:3, :]\n\n    def render_height(\n        self,\n        ax: Axes,\n        view: np.ndarray = np.eye(4),\n        x_lim: Tuple = (-20, 20),\n        y_lim: Tuple = (-20, 20),\n        marker_size: float = 1,\n    ) -> None:\n        \"\"\"Simple method that applies a transformation and then scatter plots the points colored by height (z-value).\n        Args:\n            ax: Axes on which to render the points.\n            view: <np.float: n, n>. Defines an arbitrary projection (n <= 4).\n            x_lim: (min <float>, max <float>). x range for plotting.\n            y_lim: (min <float>, max <float>). y range for plotting.\n            marker_size: Marker size.\n        \"\"\"\n        self._render_helper(2, ax, view, x_lim, y_lim, marker_size)\n\n    def render_intensity(\n        self,\n        ax: Axes,\n        view: np.ndarray = np.eye(4),\n        x_lim: Tuple = (-20, 20),\n        y_lim: Tuple = (-20, 20),\n        marker_size: float = 1,\n    ) -> None:\n        \"\"\"Very simple method that applies a transformation and then scatter plots the points colored by intensity.\n        Args:\n            ax: Axes on which to render the points.\n            view: <np.float: n, n>. Defines an arbitrary projection (n <= 4).\n            x_lim: (min <float>, max <float>).\n            y_lim: (min <float>, max <float>).\n            marker_size: Marker size.\n        Returns:\n        \"\"\"\n        self._render_helper(3, ax, view, x_lim, y_lim, marker_size)\n\n    def _render_helper(\n        self, color_channel: int, ax: Axes, view: np.ndarray, x_lim: Tuple, y_lim: Tuple, marker_size: float\n    ) -> None:\n        \"\"\"Helper function for rendering.\n        Args:\n            color_channel: Point channel to use as color.\n            ax: Axes on which to render the points.\n            view: <np.float: n, n>. Defines an arbitrary projection (n <= 4).\n            x_lim: (min <float>, max <float>).\n            y_lim: (min <float>, max <float>).\n            marker_size: Marker size.\n        \"\"\"\n        points = view_points(self.points[:3, :], view, normalize=False)\n        ax.scatter(points[0, :], points[1, :], c=self.points[color_channel, :], s=marker_size)\n        ax.set_xlim(x_lim[0], x_lim[1])\n        ax.set_ylim(y_lim[0], y_lim[1])\n\n\nclass LidarPointCloud(PointCloud):\n    @staticmethod\n    def nbr_dims() -> int:\n        \"\"\"Returns the number of dimensions.\n        Returns: Number of dimensions.\n        \"\"\"\n        return 4\n\n    @classmethod\n    def from_file(cls, file_name: Path) -> \"LidarPointCloud\":\n        \"\"\"Loads LIDAR data from binary numpy format. Data is stored as (x, y, z, intensity, ring index).\n        Args:\n            file_name: Path of the pointcloud file on disk.\n        Returns: LidarPointCloud instance (x, y, z, intensity).\n        \"\"\"\n\n        assert file_name.suffix == \".bin\", \"Unsupported filetype {}\".format(file_name)\n\n        scan = np.fromfile(str(file_name), dtype=np.float32)\n        points = scan.reshape((-1, 5))[:, : cls.nbr_dims()]\n        return cls(points.T)\n\n\nclass RadarPointCloud(PointCloud):\n\n    # Class-level settings for radar pointclouds, see from_file().\n    invalid_states = [0]  # type: List[int]\n    dynprop_states = range(7)  # type: List[int] # Use [0, 2, 6] for moving objects only.\n    ambig_states = [3]  # type: List[int]\n\n    @staticmethod\n    def nbr_dims() -> int:\n        \"\"\"Returns the number of dimensions.\n        Returns: Number of dimensions.\n        \"\"\"\n        return 18\n\n    @classmethod\n    def from_file(\n        cls,\n        file_name: Path,\n        invalid_states: List[int] = None,\n        dynprop_states: List[int] = None,\n        ambig_states: List[int] = None,\n    ) -> \"RadarPointCloud\":\n        \"\"\"Loads RADAR data from a Point Cloud Data file. See details below.\n        Args:\n            file_name: The path of the pointcloud file.\n            invalid_states: Radar states to be kept. See details below.\n            dynprop_states: Radar states to be kept. Use [0, 2, 6] for moving objects only. See details below.\n            ambig_states: Radar states to be kept. See details below. To keep all radar returns,\n                set each state filter to range(18).\n        Returns: <np.float: d, n>. Point cloud matrix with d dimensions and n points.\n        Example of the header fields:\n        # .PCD v0.7 - Point Cloud Data file format\n        VERSION 0.7\n        FIELDS x y z dyn_prop id rcs vx vy vx_comp vy_comp is_quality_valid ambig_\n                                                            state x_rms y_rms invalid_state pdh0 vx_rms vy_rms\n        SIZE 4 4 4 1 2 4 4 4 4 4 1 1 1 1 1 1 1 1\n        TYPE F F F I I F F F F F I I I I I I I I\n        COUNT 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n        WIDTH 125\n        HEIGHT 1\n        VIEWPOINT 0 0 0 1 0 0 0\n        POINTS 125\n        DATA binary\n        Below some of the fields are explained in more detail:\n        x is front, y is left\n        vx, vy are the velocities in m/s.\n        vx_comp, vy_comp are the velocities in m/s compensated by the ego motion.\n        We recommend using the compensated velocities.\n        invalid_state: state of Cluster validity state.\n        (Invalid states)\n        0x01\tinvalid due to low RCS\n        0x02\tinvalid due to near-field artefact\n        0x03\tinvalid far range cluster because not confirmed in near range\n        0x05\treserved\n        0x06\tinvalid cluster due to high mirror probability\n        0x07\tInvalid cluster because outside sensor field of view\n        0x0d\treserved\n        0x0e\tinvalid cluster because it is a harmonics\n        (Valid states)\n        0x00\tvalid\n        0x04\tvalid cluster with low RCS\n        0x08\tvalid cluster with azimuth correction due to elevation\n        0x09\tvalid cluster with high child probability\n        0x0a\tvalid cluster with high probability of being a 50 deg artefact\n        0x0b\tvalid cluster but no local maximum\n        0x0c\tvalid cluster with high artefact probability\n        0x0f\tvalid cluster with above 95m in near range\n        0x10\tvalid cluster with high multi-target probability\n        0x11\tvalid cluster with suspicious angle\n        dynProp: Dynamic property of cluster to indicate if is moving or not.\n        0: moving\n        1: stationary\n        2: oncoming\n        3: stationary candidate\n        4: unknown\n        5: crossing stationary\n        6: crossing moving\n        7: stopped\n        ambig_state: State of Doppler (radial velocity) ambiguity solution.\n        0: invalid\n        1: ambiguous\n        2: staggered ramp\n        3: unambiguous\n        4: stationary candidates\n        pdh0: False alarm probability of cluster (i.e. probability of being an artefact caused\n                                                                                    by multipath or similar).\n        0: invalid\n        1: <25%\n        2: 50%\n        3: 75%\n        4: 90%\n        5: 99%\n        6: 99.9%\n        7: <=100%\n        \"\"\"\n\n        assert file_name.suffix == \".pcd\", \"Unsupported filetype {}\".format(file_name)\n\n        meta = []\n        with open(str(file_name), \"rb\") as f:\n            for line in f:\n                line = line.strip().decode(\"utf-8\")\n                meta.append(line)\n                if line.startswith(\"DATA\"):\n                    break\n\n            data_binary = f.read()\n\n        # Get the header rows and check if they appear as expected.\n        assert meta[0].startswith(\"#\"), \"First line must be comment\"\n        assert meta[1].startswith(\"VERSION\"), \"Second line must be VERSION\"\n        sizes = meta[3].split(\" \")[1:]\n        types = meta[4].split(\" \")[1:]\n        counts = meta[5].split(\" \")[1:]\n        width = int(meta[6].split(\" \")[1])\n        height = int(meta[7].split(\" \")[1])\n        data = meta[10].split(\" \")[1]\n        feature_count = len(types)\n        assert width > 0\n        assert len([c for c in counts if c != c]) == 0, \"Error: COUNT not supported!\"\n        assert height == 1, \"Error: height != 0 not supported!\"\n        assert data == \"binary\"\n\n        # Lookup table for how to decode the binaries.\n        unpacking_lut = {\n            \"F\": {2: \"e\", 4: \"f\", 8: \"d\"},\n            \"I\": {1: \"b\", 2: \"h\", 4: \"i\", 8: \"q\"},\n            \"U\": {1: \"B\", 2: \"H\", 4: \"I\", 8: \"Q\"},\n        }\n        types_str = \"\".join([unpacking_lut[t][int(s)] for t, s in zip(types, sizes)])\n\n        # Decode each point.\n        offset = 0\n        point_count = width\n        points = []\n        for i in range(point_count):\n            point = []\n            for p in range(feature_count):\n                start_p = offset\n                end_p = start_p + int(sizes[p])\n                assert end_p < len(data_binary)\n                point_p = struct.unpack(types_str[p], data_binary[start_p:end_p])[0]\n                point.append(point_p)\n                offset = end_p\n            points.append(point)\n\n        # A NaN in the first point indicates an empty pointcloud.\n        point = np.array(points[0])\n        if np.any(np.isnan(point)):\n            return cls(np.zeros((feature_count, 0)))\n\n        # Convert to numpy matrix.\n        points = np.array(points).transpose()\n\n        # If no parameters are provided, use default settings.\n        invalid_states = cls.invalid_states if invalid_states is None else invalid_states\n        dynprop_states = cls.dynprop_states if dynprop_states is None else dynprop_states\n        ambig_states = cls.ambig_states if ambig_states is None else ambig_states\n\n        # Filter points with an invalid state.\n        valid = [p in invalid_states for p in points[-4, :]]\n        points = points[:, valid]\n\n        # Filter by dynProp.\n        valid = [p in dynprop_states for p in points[3, :]]\n        points = points[:, valid]\n\n        # Filter by ambig_state.\n        valid = [p in ambig_states for p in points[11, :]]\n        points = points[:, valid]\n\n        return cls(points)\n\n\nclass Box:\n    \"\"\" Simple data class representing a 3d box including, label, score and velocity. \"\"\"\n\n    def __init__(\n        self,\n        center: List[float],\n        size: List[float],\n        orientation: Quaternion,\n        label: int = np.nan,\n        score: float = np.nan,\n        velocity: Tuple = (np.nan, np.nan, np.nan),\n        name: str = None,\n        token: str = None,\n    ):\n        \"\"\"\n        Args:\n            center: Center of box given as x, y, z.\n            size: Size of box in width, length, height.\n            orientation: Box orientation.\n            label: Integer label, optional.\n            score: Classification score, optional.\n            velocity: Box velocity in x, y, z direction.\n            name: Box name, optional. Can be used e.g. for denote category name.\n            token: Unique string identifier from DB.\n        \"\"\"\n        assert not np.any(np.isnan(center))\n        assert not np.any(np.isnan(size))\n        assert len(center) == 3\n        assert len(size) == 3\n        assert type(orientation) == Quaternion\n\n        self.center = np.array(center)\n        self.wlh = np.array(size)\n        self.orientation = orientation\n        self.label = int(label) if not np.isnan(label) else label\n        self.score = float(score) if not np.isnan(score) else score\n        self.velocity = np.array(velocity)\n        self.name = name\n        self.token = token\n\n    def __eq__(self, other):\n        center = np.allclose(self.center, other.center)\n        wlh = np.allclose(self.wlh, other.wlh)\n        orientation = np.allclose(self.orientation.elements, other.orientation.elements)\n        label = (self.label == other.label) or (np.isnan(self.label) and np.isnan(other.label))\n        score = (self.score == other.score) or (np.isnan(self.score) and np.isnan(other.score))\n        vel = np.allclose(self.velocity, other.velocity) or (\n            np.all(np.isnan(self.velocity)) and np.all(np.isnan(other.velocity))\n        )\n\n        return center and wlh and orientation and label and score and vel\n\n    def __repr__(self):\n        repr_str = (\n            \"label: {}, score: {:.2f}, xyz: [{:.2f}, {:.2f}, {:.2f}], wlh: [{:.2f}, {:.2f}, {:.2f}], \"\n            \"rot axis: [{:.2f}, {:.2f}, {:.2f}], ang(degrees): {:.2f}, ang(rad): {:.2f}, \"\n            \"vel: {:.2f}, {:.2f}, {:.2f}, name: {}, token: {}\"\n        )\n\n        return repr_str.format(\n            self.label,\n            self.score,\n            self.center[0],\n            self.center[1],\n            self.center[2],\n            self.wlh[0],\n            self.wlh[1],\n            self.wlh[2],\n            self.orientation.axis[0],\n            self.orientation.axis[1],\n            self.orientation.axis[2],\n            self.orientation.degrees,\n            self.orientation.radians,\n            self.velocity[0],\n            self.velocity[1],\n            self.velocity[2],\n            self.name,\n            self.token,\n        )\n\n    @property\n    def rotation_matrix(self) -> np.ndarray:\n        \"\"\"Return a rotation matrix.\n        Returns: <np.float: 3, 3>. The box's rotation matrix.\n        \"\"\"\n        return self.orientation.rotation_matrix\n\n    def translate(self, x: np.ndarray) -> None:\n        \"\"\"Applies a translation.\n        Args:\n            x: <np.float: 3, 1>. Translation in x, y, z direction.\n        \"\"\"\n        self.center += x\n\n    def rotate(self, quaternion: Quaternion) -> None:\n        \"\"\"Rotates box.\n        Args:\n            quaternion: Rotation to apply.\n        \"\"\"\n        self.center = np.dot(quaternion.rotation_matrix, self.center)\n        self.orientation = quaternion * self.orientation\n        self.velocity = np.dot(quaternion.rotation_matrix, self.velocity)\n\n    def corners(self, wlh_factor: float = 1.0) -> np.ndarray:\n        \"\"\"Returns the bounding box corners.\n        Args:\n            wlh_factor: Multiply width, length, height by a factor to scale the box.\n        Returns: First four corners are the ones facing forward.\n                The last four are the ones facing backwards.\n        \"\"\"\n\n        width, length, height = self.wlh * wlh_factor\n\n        # 3D bounding box corners. (Convention: x points forward, y to the left, z up.)\n        x_corners = length / 2 * np.array([1, 1, 1, 1, -1, -1, -1, -1])\n        y_corners = width / 2 * np.array([1, -1, -1, 1, 1, -1, -1, 1])\n        z_corners = height / 2 * np.array([1, 1, -1, -1, 1, 1, -1, -1])\n        corners = np.vstack((x_corners, y_corners, z_corners))\n\n        # Rotate\n        corners = np.dot(self.orientation.rotation_matrix, corners)\n\n        # Translate\n        x, y, z = self.center\n        corners[0, :] = corners[0, :] + x\n        corners[1, :] = corners[1, :] + y\n        corners[2, :] = corners[2, :] + z\n\n        return corners\n\n    def bottom_corners(self) -> np.ndarray:\n        \"\"\"Returns the four bottom corners.\n        Returns: <np.float: 3, 4>. Bottom corners. First two face forward, last two face backwards.\n        \"\"\"\n        return self.corners()[:, [2, 3, 7, 6]]\n\n    def render(\n        self,\n        axis: Axes,\n        view: np.ndarray = np.eye(3),\n        normalize: bool = False,\n        colors: Tuple = (\"b\", \"r\", \"k\"),\n        linewidth: float = 2,\n    ):\n        \"\"\"Renders the box in the provided Matplotlib axis.\n        Args:\n            axis: Axis onto which the box should be drawn.\n            view: <np.array: 3, 3>. Define a projection in needed (e.g. for drawing projection in an image).\n            normalize: Whether to normalize the remaining coordinate.\n            colors: (<Matplotlib.colors>: 3). Valid Matplotlib colors (<str> or normalized RGB tuple) for front,\n            back and sides.\n            linewidth: Width in pixel of the box sides.\n        \"\"\"\n        corners = view_points(self.corners(), view, normalize=normalize)[:2, :]\n\n        def draw_rect(selected_corners, color):\n            prev = selected_corners[-1]\n            for corner in selected_corners:\n                axis.plot([prev[0], corner[0]], [prev[1], corner[1]], color=color, linewidth=linewidth)\n                prev = corner\n\n        # Draw the sides\n        for i in range(4):\n            axis.plot(\n                [corners.T[i][0], corners.T[i + 4][0]],\n                [corners.T[i][1], corners.T[i + 4][1]],\n                color=colors[2],\n                linewidth=linewidth,\n            )\n\n        # Draw front (first 4 corners) and rear (last 4 corners) rectangles(3d)/lines(2d)\n        draw_rect(corners.T[:4], colors[0])\n        draw_rect(corners.T[4:], colors[1])\n\n        # Draw line indicating the front\n        center_bottom_forward = np.mean(corners.T[2:4], axis=0)\n        center_bottom = np.mean(corners.T[[2, 3, 7, 6]], axis=0)\n        axis.plot(\n            [center_bottom[0], center_bottom_forward[0]],\n            [center_bottom[1], center_bottom_forward[1]],\n            color=colors[0],\n            linewidth=linewidth,\n        )\n\n    def render_cv2(\n        self,\n        image: np.ndarray,\n        view: np.ndarray = np.eye(3),\n        normalize: bool = False,\n        colors: Tuple = ((0, 0, 255), (255, 0, 0), (155, 155, 155)),\n        linewidth: int = 2,\n    ) -> None:\n        \"\"\"Renders box using OpenCV2.\n        Args:\n            image: <np.array: width, height, 3>. Image array. Channels are in BGR order.\n            view: <np.array: 3, 3>. Define a projection if needed (e.g. for drawing projection in an image).\n            normalize: Whether to normalize the remaining coordinate.\n            colors: ((R, G, B), (R, G, B), (R, G, B)). Colors for front, side & rear.\n            linewidth: Linewidth for plot.\n        Returns:\n        \"\"\"\n        corners = view_points(self.corners(), view, normalize=normalize)[:2, :]\n\n        def draw_rect(selected_corners, color):\n            prev = selected_corners[-1]\n            for corner in selected_corners:\n                cv2.line(image, (int(prev[0]), int(prev[1])), (int(corner[0]), int(corner[1])), color, linewidth)\n                prev = corner\n\n        # Draw the sides\n        for i in range(4):\n            cv2.line(\n                image,\n                (int(corners.T[i][0]), int(corners.T[i][1])),\n                (int(corners.T[i + 4][0]), int(corners.T[i + 4][1])),\n                colors[2][::-1],\n                linewidth,\n            )\n\n        # Draw front (first 4 corners) and rear (last 4 corners) rectangles(3d)/lines(2d)\n        draw_rect(corners.T[:4], colors[0][::-1])\n        draw_rect(corners.T[4:], colors[1][::-1])\n\n        # Draw line indicating the front\n        center_bottom_forward = np.mean(corners.T[2:4], axis=0)\n        center_bottom = np.mean(corners.T[[2, 3, 7, 6]], axis=0)\n        cv2.line(\n            image,\n            (int(center_bottom[0]), int(center_bottom[1])),\n            (int(center_bottom_forward[0]), int(center_bottom_forward[1])),\n            colors[0][::-1],\n            linewidth,\n        )\n\n    def copy(self) -> \"Box\":\n        \"\"\"        Create a copy of self.\n        Returns: A copy.\n        \"\"\"\n        return copy.deepcopy(self)", "processed": ["box plot notic observ violin plot height distribut smaller object like pedestrian bicycl much smaller mean quartil valu compar larger object like car truck buse except trend car tend similar height pedestrian", "dig imag lidar data", "defin function help creat lyftdataset class click code right side"]}, {"markdown": ["### Create another class called *LyftDatasetExplorer* which will help us to visualize the data\n#### (click CODE on the right side)"], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/lyft-competition-understanding-the-data\n\nclass LyftDatasetExplorer:\n    \"\"\"Helper class to list and visualize Lyft Dataset data. These are meant to serve as tutorials and templates for\n    working with the data.\"\"\"\n\n    def __init__(self, lyftd: LyftDataset):\n        self.lyftd = lyftd\n\n    @staticmethod\n    def get_color(category_name: str) -> Tuple[int, int, int]:\n        \"\"\"Provides the default colors based on the category names.\n        This method works for the general Lyft Dataset categories, as well as the Lyft Dataset detection categories.\n        Args:\n            category_name:\n        Returns:\n        \"\"\"\n        if \"bicycle\" in category_name or \"motorcycle\" in category_name:\n            return 255, 61, 99  # Red\n        elif \"vehicle\" in category_name or category_name in [\"bus\", \"car\", \"construction_vehicle\", \"trailer\", \"truck\"]:\n            return 255, 158, 0  # Orange\n        elif \"pedestrian\" in category_name:\n            return 0, 0, 230  # Blue\n        elif \"cone\" in category_name or \"barrier\" in category_name:\n            return 0, 0, 0  # Black\n        else:\n            return 255, 0, 255  # Magenta\n\n    def list_categories(self) -> None:\n        \"\"\"Print categories, counts and stats.\"\"\"\n\n        print(\"Category stats\")\n\n        # Add all annotations\n        categories = dict()\n        for record in self.lyftd.sample_annotation:\n            if record[\"category_name\"] not in categories:\n                categories[record[\"category_name\"]] = []\n            categories[record[\"category_name\"]].append(record[\"size\"] + [record[\"size\"][1] / record[\"size\"][0]])\n\n        # Print stats\n        for name, stats in sorted(categories.items()):\n            stats = np.array(stats)\n            print(\n                \"{:27} n={:5}, width={:5.2f}\\u00B1{:.2f}, len={:5.2f}\\u00B1{:.2f}, height={:5.2f}\\u00B1{:.2f}, \"\n                \"lw_aspect={:5.2f}\\u00B1{:.2f}\".format(\n                    name[:27],\n                    stats.shape[0],\n                    np.mean(stats[:, 0]),\n                    np.std(stats[:, 0]),\n                    np.mean(stats[:, 1]),\n                    np.std(stats[:, 1]),\n                    np.mean(stats[:, 2]),\n                    np.std(stats[:, 2]),\n                    np.mean(stats[:, 3]),\n                    np.std(stats[:, 3]),\n                )\n            )\n\n    def list_attributes(self) -> None:\n        \"\"\"Prints attributes and counts.\"\"\"\n        attribute_counts = dict()\n        for record in self.lyftd.sample_annotation:\n            for attribute_token in record[\"attribute_tokens\"]:\n                att_name = self.lyftd.get(\"attribute\", attribute_token)[\"name\"]\n                if att_name not in attribute_counts:\n                    attribute_counts[att_name] = 0\n                attribute_counts[att_name] += 1\n\n        for name, count in sorted(attribute_counts.items()):\n            print(\"{}: {}\".format(name, count))\n\n    def list_scenes(self) -> None:\n        \"\"\" Lists all scenes with some meta data. \"\"\"\n\n        def ann_count(record):\n            count = 0\n            sample = self.lyftd.get(\"sample\", record[\"first_sample_token\"])\n            while not sample[\"next\"] == \"\":\n                count += len(sample[\"anns\"])\n                sample = self.lyftd.get(\"sample\", sample[\"next\"])\n            return count\n\n        recs = [\n            (self.lyftd.get(\"sample\", record[\"first_sample_token\"])[\"timestamp\"], record)\n            for record in self.lyftd.scene\n        ]\n\n        for start_time, record in sorted(recs):\n            start_time = self.lyftd.get(\"sample\", record[\"first_sample_token\"])[\"timestamp\"] / 1000000\n            length_time = self.lyftd.get(\"sample\", record[\"last_sample_token\"])[\"timestamp\"] / 1000000 - start_time\n            location = self.lyftd.get(\"log\", record[\"log_token\"])[\"location\"]\n            desc = record[\"name\"] + \", \" + record[\"description\"]\n            if len(desc) > 55:\n                desc = desc[:51] + \"...\"\n            if len(location) > 18:\n                location = location[:18]\n\n            print(\n                \"{:16} [{}] {:4.0f}s, {}, #anns:{}\".format(\n                    desc,\n                    datetime.utcfromtimestamp(start_time).strftime(\"%y-%m-%d %H:%M:%S\"),\n                    length_time,\n                    location,\n                    ann_count(record),\n                )\n            )\n\n    def list_sample(self, sample_token: str) -> None:\n        \"\"\"Prints sample_data tokens and sample_annotation tokens related to the sample_token.\"\"\"\n\n        sample_record = self.lyftd.get(\"sample\", sample_token)\n        print(\"Sample: {}\\n\".format(sample_record[\"token\"]))\n        for sd_token in sample_record[\"data\"].values():\n            sd_record = self.lyftd.get(\"sample_data\", sd_token)\n            print(\n                \"sample_data_token: {}, mod: {}, channel: {}\".format(\n                    sd_token, sd_record[\"sensor_modality\"], sd_record[\"channel\"]\n                )\n            )\n        print(\"\")\n        for ann_token in sample_record[\"anns\"]:\n            ann_record = self.lyftd.get(\"sample_annotation\", ann_token)\n            print(\"sample_annotation_token: {}, category: {}\".format(ann_record[\"token\"], ann_record[\"category_name\"]))\n\n    def map_pointcloud_to_image(self, pointsensor_token: str, camera_token: str) -> Tuple:\n        \"\"\"Given a point sensor (lidar/radar) token and camera sample_data token, load point-cloud and map it to\n        the image plane.\n        Args:\n            pointsensor_token: Lidar/radar sample_data token.\n            camera_token: Camera sample_data token.\n        Returns: (pointcloud <np.float: 2, n)>, coloring <np.float: n>, image <Image>).\n        \"\"\"\n\n        cam = self.lyftd.get(\"sample_data\", camera_token)\n        pointsensor = self.lyftd.get(\"sample_data\", pointsensor_token)\n        pcl_path = self.lyftd.data_path / ('train_' + pointsensor[\"filename\"])\n        if pointsensor[\"sensor_modality\"] == \"lidar\":\n            pc = LidarPointCloud.from_file(pcl_path)\n        else:\n            pc = RadarPointCloud.from_file(pcl_path)\n        im = Image.open(str(self.lyftd.data_path / ('train_' + cam[\"filename\"])))\n\n        # Points live in the point sensor frame. So they need to be transformed via global to the image plane.\n        # First step: transform the point-cloud to the ego vehicle frame for the timestamp of the sweep.\n        cs_record = self.lyftd.get(\"calibrated_sensor\", pointsensor[\"calibrated_sensor_token\"])\n        pc.rotate(Quaternion(cs_record[\"rotation\"]).rotation_matrix)\n        pc.translate(np.array(cs_record[\"translation\"]))\n\n        # Second step: transform to the global frame.\n        poserecord = self.lyftd.get(\"ego_pose\", pointsensor[\"ego_pose_token\"])\n        pc.rotate(Quaternion(poserecord[\"rotation\"]).rotation_matrix)\n        pc.translate(np.array(poserecord[\"translation\"]))\n\n        # Third step: transform into the ego vehicle frame for the timestamp of the image.\n        poserecord = self.lyftd.get(\"ego_pose\", cam[\"ego_pose_token\"])\n        pc.translate(-np.array(poserecord[\"translation\"]))\n        pc.rotate(Quaternion(poserecord[\"rotation\"]).rotation_matrix.T)\n\n        # Fourth step: transform into the camera.\n        cs_record = self.lyftd.get(\"calibrated_sensor\", cam[\"calibrated_sensor_token\"])\n        pc.translate(-np.array(cs_record[\"translation\"]))\n        pc.rotate(Quaternion(cs_record[\"rotation\"]).rotation_matrix.T)\n\n        # Fifth step: actually take a \"picture\" of the point cloud.\n        # Grab the depths (camera frame z axis points away from the camera).\n        depths = pc.points[2, :]\n\n        # Retrieve the color from the depth.\n        coloring = depths\n\n        # Take the actual picture (matrix multiplication with camera-matrix + renormalization).\n        points = view_points(pc.points[:3, :], np.array(cs_record[\"camera_intrinsic\"]), normalize=True)\n\n        # Remove points that are either outside or behind the camera. Leave a margin of 1 pixel for aesthetic reasons.\n        mask = np.ones(depths.shape[0], dtype=bool)\n        mask = np.logical_and(mask, depths > 0)\n        mask = np.logical_and(mask, points[0, :] > 1)\n        mask = np.logical_and(mask, points[0, :] < im.size[0] - 1)\n        mask = np.logical_and(mask, points[1, :] > 1)\n        mask = np.logical_and(mask, points[1, :] < im.size[1] - 1)\n        points = points[:, mask]\n        coloring = coloring[mask]\n\n        return points, coloring, im\n\n    def render_pointcloud_in_image(\n        self,\n        sample_token: str,\n        dot_size: int = 2,\n        pointsensor_channel: str = \"LIDAR_TOP\",\n        camera_channel: str = \"CAM_FRONT\",\n        out_path: str = None,\n    ) -> None:\n        \"\"\"Scatter-plots a point-cloud on top of image.\n        Args:\n            sample_token: Sample token.\n            dot_size: Scatter plot dot size.\n            pointsensor_channel: RADAR or LIDAR channel name, e.g. 'LIDAR_TOP'.\n            camera_channel: Camera channel name, e.g. 'CAM_FRONT'.\n            out_path: Optional path to save the rendered figure to disk.\n        Returns:\n        \"\"\"\n        sample_record = self.lyftd.get(\"sample\", sample_token)\n\n        # Here we just grab the front camera and the point sensor.\n        pointsensor_token = sample_record[\"data\"][pointsensor_channel]\n        camera_token = sample_record[\"data\"][camera_channel]\n\n        points, coloring, im = self.map_pointcloud_to_image(pointsensor_token, camera_token)\n        plt.figure(figsize=(9, 16))\n        plt.imshow(im)\n        plt.scatter(points[0, :], points[1, :], c=coloring, s=dot_size)\n        plt.axis(\"off\")\n\n        if out_path is not None:\n            plt.savefig(out_path)\n\n    def render_sample(\n        self, token: str, box_vis_level: BoxVisibility = BoxVisibility.ANY, nsweeps: int = 1, out_path: str = None\n    ) -> None:\n        \"\"\"Render all LIDAR and camera sample_data in sample along with annotations.\n        Args:\n            token: Sample token.\n            box_vis_level: If sample_data is an image, this sets required visibility for boxes.\n            nsweeps: Number of sweeps for lidar and radar.\n            out_path: Optional path to save the rendered figure to disk.\n        Returns:\n        \"\"\"\n        record = self.lyftd.get(\"sample\", token)\n\n        # Separate RADAR from LIDAR and vision.\n        radar_data = {}\n        nonradar_data = {}\n        for channel, token in record[\"data\"].items():\n            sd_record = self.lyftd.get(\"sample_data\", token)\n            sensor_modality = sd_record[\"sensor_modality\"]\n            if sensor_modality in [\"lidar\", \"camera\"]:\n                nonradar_data[channel] = token\n            else:\n                radar_data[channel] = token\n\n        num_radar_plots = 1 if len(radar_data) > 0 else 0\n\n        # Create plots.\n        n = num_radar_plots + len(nonradar_data)\n        cols = 2\n        fig, axes = plt.subplots(int(np.ceil(n / cols)), cols, figsize=(16, 24))\n\n        if len(radar_data) > 0:\n            # Plot radar into a single subplot.\n            ax = axes[0, 0]\n            for i, (_, sd_token) in enumerate(radar_data.items()):\n                self.render_sample_data(\n                    sd_token, with_anns=i == 0, box_vis_level=box_vis_level, ax=ax, num_sweeps=nsweeps\n                )\n            ax.set_title(\"Fused RADARs\")\n\n        # Plot camera and lidar in separate subplots.\n        for (_, sd_token), ax in zip(nonradar_data.items(), axes.flatten()[num_radar_plots:]):\n            self.render_sample_data(sd_token, box_vis_level=box_vis_level, ax=ax, num_sweeps=nsweeps)\n\n        axes.flatten()[-1].axis(\"off\")\n        plt.tight_layout()\n        fig.subplots_adjust(wspace=0, hspace=0)\n\n        if out_path is not None:\n            plt.savefig(out_path)\n\n    def render_ego_centric_map(self, sample_data_token: str, axes_limit: float = 40, ax: Axes = None) -> None:\n        \"\"\"Render map centered around the associated ego pose.\n        Args:\n            sample_data_token: Sample_data token.\n            axes_limit: Axes limit measured in meters.\n            ax: Axes onto which to render.\n        \"\"\"\n\n        def crop_image(image: np.array, x_px: int, y_px: int, axes_limit_px: int) -> np.array:\n            x_min = int(x_px - axes_limit_px)\n            x_max = int(x_px + axes_limit_px)\n            y_min = int(y_px - axes_limit_px)\n            y_max = int(y_px + axes_limit_px)\n\n            cropped_image = image[y_min:y_max, x_min:x_max]\n\n            return cropped_image\n\n        sd_record = self.lyftd.get(\"sample_data\", sample_data_token)\n\n        # Init axes.\n        if ax is None:\n            _, ax = plt.subplots(1, 1, figsize=(9, 9))\n\n        sample = self.lyftd.get(\"sample\", sd_record[\"sample_token\"])\n        scene = self.lyftd.get(\"scene\", sample[\"scene_token\"])\n        log = self.lyftd.get(\"log\", scene[\"log_token\"])\n        map = self.lyftd.get(\"map\", log[\"map_token\"])\n        map_mask = map[\"mask\"]\n\n        pose = self.lyftd.get(\"ego_pose\", sd_record[\"ego_pose_token\"])\n        pixel_coords = map_mask.to_pixel_coords(pose[\"translation\"][0], pose[\"translation\"][1])\n\n        scaled_limit_px = int(axes_limit * (1.0 / map_mask.resolution))\n        mask_raster = map_mask.mask()\n\n        cropped = crop_image(mask_raster, pixel_coords[0], pixel_coords[1], int(scaled_limit_px * math.sqrt(2)))\n\n        ypr_rad = Quaternion(pose[\"rotation\"]).yaw_pitch_roll\n        yaw_deg = -math.degrees(ypr_rad[0])\n\n        rotated_cropped = np.array(Image.fromarray(cropped).rotate(yaw_deg))\n        ego_centric_map = crop_image(\n            rotated_cropped, rotated_cropped.shape[1] / 2, rotated_cropped.shape[0] / 2, scaled_limit_px\n        )\n        ax.imshow(\n            ego_centric_map, extent=[-axes_limit, axes_limit, -axes_limit, axes_limit], cmap=\"gray\", vmin=0, vmax=150\n        )\n\n    def render_sample_data(\n        self,\n        sample_data_token: str,\n        with_anns: bool = True,\n        box_vis_level: BoxVisibility = BoxVisibility.ANY,\n        axes_limit: float = 40,\n        ax: Axes = None,\n        num_sweeps: int = 1,\n        out_path: str = None,\n        underlay_map: bool = False,\n    ):\n        \"\"\"Render sample data onto axis.\n        Args:\n            sample_data_token: Sample_data token.\n            with_anns: Whether to draw annotations.\n            box_vis_level: If sample_data is an image, this sets required visibility for boxes.\n            axes_limit: Axes limit for lidar and radar (measured in meters).\n            ax: Axes onto which to render.\n            num_sweeps: Number of sweeps for lidar and radar.\n            out_path: Optional path to save the rendered figure to disk.\n            underlay_map: When set to true, LIDAR data is plotted onto the map. This can be slow.\n        \"\"\"\n\n        # Get sensor modality.\n        sd_record = self.lyftd.get(\"sample_data\", sample_data_token)\n        sensor_modality = sd_record[\"sensor_modality\"]\n\n        if sensor_modality == \"lidar\":\n            # Get boxes in lidar frame.\n            _, boxes, _ = self.lyftd.get_sample_data(\n                sample_data_token, box_vis_level=box_vis_level, flat_vehicle_coordinates=True\n            )\n\n            # Get aggregated point cloud in lidar frame.\n            sample_rec = self.lyftd.get(\"sample\", sd_record[\"sample_token\"])\n            chan = sd_record[\"channel\"]\n            ref_chan = \"LIDAR_TOP\"\n            pc, times = LidarPointCloud.from_file_multisweep(\n                self.lyftd, sample_rec, chan, ref_chan, num_sweeps=num_sweeps\n            )\n\n            # Compute transformation matrices for lidar point cloud\n            cs_record = self.lyftd.get(\"calibrated_sensor\", sd_record[\"calibrated_sensor_token\"])\n            pose_record = self.lyftd.get(\"ego_pose\", sd_record[\"ego_pose_token\"])\n            vehicle_from_sensor = np.eye(4)\n            vehicle_from_sensor[:3, :3] = Quaternion(cs_record[\"rotation\"]).rotation_matrix\n            vehicle_from_sensor[:3, 3] = cs_record[\"translation\"]\n\n            ego_yaw = Quaternion(pose_record[\"rotation\"]).yaw_pitch_roll[0]\n            rot_vehicle_flat_from_vehicle = np.dot(\n                Quaternion(scalar=np.cos(ego_yaw / 2), vector=[0, 0, np.sin(ego_yaw / 2)]).rotation_matrix,\n                Quaternion(pose_record[\"rotation\"]).inverse.rotation_matrix,\n            )\n\n            vehicle_flat_from_vehicle = np.eye(4)\n            vehicle_flat_from_vehicle[:3, :3] = rot_vehicle_flat_from_vehicle\n\n            # Init axes.\n            if ax is None:\n                _, ax = plt.subplots(1, 1, figsize=(9, 9))\n\n            if underlay_map:\n                self.render_ego_centric_map(sample_data_token=sample_data_token, axes_limit=axes_limit, ax=ax)\n\n            # Show point cloud.\n            points = view_points(\n                pc.points[:3, :], np.dot(vehicle_flat_from_vehicle, vehicle_from_sensor), normalize=False\n            )\n            dists = np.sqrt(np.sum(pc.points[:2, :] ** 2, axis=0))\n            colors = np.minimum(1, dists / axes_limit / np.sqrt(2))\n            ax.scatter(points[0, :], points[1, :], c=colors, s=0.2)\n\n            # Show ego vehicle.\n            ax.plot(0, 0, \"x\", color=\"red\")\n\n            # Show boxes.\n            if with_anns:\n                for box in boxes:\n                    c = np.array(self.get_color(box.name)) / 255.0\n                    box.render(ax, view=np.eye(4), colors=(c, c, c))\n\n            # Limit visible range.\n            ax.set_xlim(-axes_limit, axes_limit)\n            ax.set_ylim(-axes_limit, axes_limit)\n\n        elif sensor_modality == \"radar\":\n            # Get boxes in lidar frame.\n            sample_rec = self.lyftd.get(\"sample\", sd_record[\"sample_token\"])\n            lidar_token = sample_rec[\"data\"][\"LIDAR_TOP\"]\n            _, boxes, _ = self.lyftd.get_sample_data(lidar_token, box_vis_level=box_vis_level)\n\n            # Get aggregated point cloud in lidar frame.\n            # The point cloud is transformed to the lidar frame for visualization purposes.\n            chan = sd_record[\"channel\"]\n            ref_chan = \"LIDAR_TOP\"\n            pc, times = RadarPointCloud.from_file_multisweep(\n                self.lyftd, sample_rec, chan, ref_chan, num_sweeps=num_sweeps\n            )\n\n            # Transform radar velocities (x is front, y is left), as these are not transformed when loading the point\n            # cloud.\n            radar_cs_record = self.lyftd.get(\"calibrated_sensor\", sd_record[\"calibrated_sensor_token\"])\n            lidar_sd_record = self.lyftd.get(\"sample_data\", lidar_token)\n            lidar_cs_record = self.lyftd.get(\"calibrated_sensor\", lidar_sd_record[\"calibrated_sensor_token\"])\n            velocities = pc.points[8:10, :]  # Compensated velocity\n            velocities = np.vstack((velocities, np.zeros(pc.points.shape[1])))\n            velocities = np.dot(Quaternion(radar_cs_record[\"rotation\"]).rotation_matrix, velocities)\n            velocities = np.dot(Quaternion(lidar_cs_record[\"rotation\"]).rotation_matrix.T, velocities)\n            velocities[2, :] = np.zeros(pc.points.shape[1])\n\n            # Init axes.\n            if ax is None:\n                _, ax = plt.subplots(1, 1, figsize=(9, 9))\n\n            # Show point cloud.\n            points = view_points(pc.points[:3, :], np.eye(4), normalize=False)\n            dists = np.sqrt(np.sum(pc.points[:2, :] ** 2, axis=0))\n            colors = np.minimum(1, dists / axes_limit / np.sqrt(2))\n            sc = ax.scatter(points[0, :], points[1, :], c=colors, s=3)\n\n            # Show velocities.\n            points_vel = view_points(pc.points[:3, :] + velocities, np.eye(4), normalize=False)\n            max_delta = 10\n            deltas_vel = points_vel - points\n            deltas_vel = 3 * deltas_vel  # Arbitrary scaling\n            deltas_vel = np.clip(deltas_vel, -max_delta, max_delta)  # Arbitrary clipping\n            colors_rgba = sc.to_rgba(colors)\n            for i in range(points.shape[1]):\n                ax.arrow(points[0, i], points[1, i], deltas_vel[0, i], deltas_vel[1, i], color=colors_rgba[i])\n\n            # Show ego vehicle.\n            ax.plot(0, 0, \"x\", color=\"black\")\n\n            # Show boxes.\n            if with_anns:\n                for box in boxes:\n                    c = np.array(self.get_color(box.name)) / 255.0\n                    box.render(ax, view=np.eye(4), colors=(c, c, c))\n\n            # Limit visible range.\n            ax.set_xlim(-axes_limit, axes_limit)\n            ax.set_ylim(-axes_limit, axes_limit)\n\n        elif sensor_modality == \"camera\":\n            # Load boxes and image.\n            data_path, boxes, camera_intrinsic = self.lyftd.get_sample_data(\n                sample_data_token, box_vis_level=box_vis_level\n            )\n\n            data = Image.open(str(data_path)[:len(str(data_path)) - 46] + 'train_images/' +\\\n                              str(data_path)[len(str(data_path)) - 39 : len(str(data_path))])\n\n            # Init axes.\n            if ax is None:\n                _, ax = plt.subplots(1, 1, figsize=(9, 16))\n\n            # Show image.\n            ax.imshow(data)\n\n            # Show boxes.\n            if with_anns:\n                for box in boxes:\n                    c = np.array(self.get_color(box.name)) / 255.0\n                    box.render(ax, view=camera_intrinsic, normalize=True, colors=(c, c, c))\n\n            # Limit visible range.\n            ax.set_xlim(0, data.size[0])\n            ax.set_ylim(data.size[1], 0)\n\n        else:\n            raise ValueError(\"Error: Unknown sensor modality!\")\n\n        ax.axis(\"off\")\n        ax.set_title(sd_record[\"channel\"])\n        ax.set_aspect(\"equal\")\n\n        if out_path is not None:\n            num = len([name for name in os.listdir(out_path)])\n            out_path = out_path + str(num).zfill(5) + \"_\" + sample_data_token + \".png\"\n            plt.savefig(out_path)\n            plt.close(\"all\")\n            return out_path\n\n    def render_annotation(\n        self,\n        ann_token: str,\n        margin: float = 10,\n        view: np.ndarray = np.eye(4),\n        box_vis_level: BoxVisibility = BoxVisibility.ANY,\n        out_path: str = None,\n    ) -> None:\n        \"\"\"Render selected annotation.\n        Args:\n            ann_token: Sample_annotation token.\n            margin: How many meters in each direction to include in LIDAR view.\n            view: LIDAR view point.\n            box_vis_level: If sample_data is an image, this sets required visibility for boxes.\n            out_path: Optional path to save the rendered figure to disk.\n        \"\"\"\n\n        ann_record = self.lyftd.get(\"sample_annotation\", ann_token)\n        sample_record = self.lyftd.get(\"sample\", ann_record[\"sample_token\"])\n        assert \"LIDAR_TOP\" in sample_record[\"data\"].keys(), \"No LIDAR_TOP in data, cant render\"\n\n        fig, axes = plt.subplots(1, 2, figsize=(18, 9))\n\n        # Figure out which camera the object is fully visible in (this may return nothing)\n        boxes, cam = [], []\n        cams = [key for key in sample_record[\"data\"].keys() if \"CAM\" in key]\n        for cam in cams:\n            _, boxes, _ = self.lyftd.get_sample_data(\n                sample_record[\"data\"][cam], box_vis_level=box_vis_level, selected_anntokens=[ann_token]\n            )\n            if len(boxes) > 0:\n                break  # We found an image that matches. Let's abort.\n        assert len(boxes) > 0, \"Could not find image where annotation is visible. Try using e.g. BoxVisibility.ANY.\"\n        assert len(boxes) < 2, \"Found multiple annotations. Something is wrong!\"\n\n        cam = sample_record[\"data\"][cam]\n\n        # Plot LIDAR view\n        lidar = sample_record[\"data\"][\"LIDAR_TOP\"]\n        data_path, boxes, camera_intrinsic = self.lyftd.get_sample_data(lidar, selected_anntokens=[ann_token])\n        LidarPointCloud.from_file(Path(str(data_path)[:len(str(data_path)) - 46] + 'train_lidar/' +\\\n                                       str(data_path)[len(str(data_path)) - 40 : len(str(data_path))])).render_height(axes[0], view=view)\n        for box in boxes:\n            c = np.array(self.get_color(box.name)) / 255.0\n            box.render(axes[0], view=view, colors=(c, c, c))\n            corners = view_points(boxes[0].corners(), view, False)[:2, :]\n            axes[0].set_xlim([np.min(corners[0, :]) - margin, np.max(corners[0, :]) + margin])\n            axes[0].set_ylim([np.min(corners[1, :]) - margin, np.max(corners[1, :]) + margin])\n            axes[0].axis(\"off\")\n            axes[0].set_aspect(\"equal\")\n\n        # Plot CAMERA view\n        data_path, boxes, camera_intrinsic = self.lyftd.get_sample_data(cam, selected_anntokens=[ann_token])\n        im = Image.open(Path(str(data_path)[:len(str(data_path)) - 46] + 'train_images/' +\\\n                             str(data_path)[len(str(data_path)) - 39 : len(str(data_path))]))\n        axes[1].imshow(im)\n        axes[1].set_title(self.lyftd.get(\"sample_data\", cam)[\"channel\"])\n        axes[1].axis(\"off\")\n        axes[1].set_aspect(\"equal\")\n        for box in boxes:\n            c = np.array(self.get_color(box.name)) / 255.0\n            box.render(axes[1], view=camera_intrinsic, normalize=True, colors=(c, c, c))\n\n        if out_path is not None:\n            plt.savefig(out_path)\n\n    def render_instance(self, instance_token: str, out_path: str = None) -> None:\n        \"\"\"Finds the annotation of the given instance that is closest to the vehicle, and then renders it.\n        Args:\n            instance_token: The instance token.\n            out_path: Optional path to save the rendered figure to disk.\n        Returns:\n        \"\"\"\n\n        ann_tokens = self.lyftd.field2token(\"sample_annotation\", \"instance_token\", instance_token)\n        closest = [np.inf, None]\n        for ann_token in ann_tokens:\n            ann_record = self.lyftd.get(\"sample_annotation\", ann_token)\n            sample_record = self.lyftd.get(\"sample\", ann_record[\"sample_token\"])\n            sample_data_record = self.lyftd.get(\"sample_data\", sample_record[\"data\"][\"LIDAR_TOP\"])\n            pose_record = self.lyftd.get(\"ego_pose\", sample_data_record[\"ego_pose_token\"])\n            dist = np.linalg.norm(np.array(pose_record[\"translation\"]) - np.array(ann_record[\"translation\"]))\n            if dist < closest[0]:\n                closest[0] = dist\n                closest[1] = ann_token\n        self.render_annotation(closest[1], out_path=out_path)\n\n    def render_scene(self, scene_token: str, freq: float = 10, image_width: int = 640, out_path: Path = None) -> None:\n        \"\"\"Renders a full scene with all surround view camera channels.\n        Args:\n            scene_token: Unique identifier of scene to render.\n            freq: Display frequency (Hz).\n            image_width: Width of image to render. Height is determined automatically to preserve aspect ratio.\n            out_path: Optional path to write a video file of the rendered frames.\n        \"\"\"\n\n        if out_path is not None:\n            assert out_path.suffix == \".avi\"\n\n        # Get records from DB.\n        scene_rec = self.lyftd.get(\"scene\", scene_token)\n        first_sample_rec = self.lyftd.get(\"sample\", scene_rec[\"first_sample_token\"])\n        last_sample_rec = self.lyftd.get(\"sample\", scene_rec[\"last_sample_token\"])\n\n        channels = [\"CAM_FRONT_LEFT\", \"CAM_FRONT\", \"CAM_FRONT_RIGHT\", \"CAM_BACK_LEFT\", \"CAM_BACK\", \"CAM_BACK_RIGHT\"]\n\n        horizontal_flip = [\"CAM_BACK_LEFT\", \"CAM_BACK\", \"CAM_BACK_RIGHT\"]  # Flip these for aesthetic reasons.\n\n        time_step = 1 / freq * 1e6  # Time-stamps are measured in micro-seconds.\n\n        window_name = \"{}\".format(scene_rec[\"name\"])\n        cv2.namedWindow(window_name)\n        cv2.moveWindow(window_name, 0, 0)\n\n        # Load first sample_data record for each channel\n        current_recs = {}  # Holds the current record to be displayed by channel.\n        prev_recs = {}  # Hold the previous displayed record by channel.\n        for channel in channels:\n            current_recs[channel] = self.lyftd.get(\"sample_data\", first_sample_rec[\"data\"][channel])\n            prev_recs[channel] = None\n\n        # We assume that the resolution is the same for all surround view cameras.\n        image_height = int(image_width * current_recs[channels[0]][\"height\"] / current_recs[channels[0]][\"width\"])\n        image_size = (image_width, image_height)\n\n        # Set some display parameters\n        layout = {\n            \"CAM_FRONT_LEFT\": (0, 0),\n            \"CAM_FRONT\": (image_size[0], 0),\n            \"CAM_FRONT_RIGHT\": (2 * image_size[0], 0),\n            \"CAM_BACK_LEFT\": (0, image_size[1]),\n            \"CAM_BACK\": (image_size[0], image_size[1]),\n            \"CAM_BACK_RIGHT\": (2 * image_size[0], image_size[1]),\n        }\n\n        canvas = np.ones((2 * image_size[1], 3 * image_size[0], 3), np.uint8)\n        if out_path is not None:\n            fourcc = cv2.VideoWriter_fourcc(*\"MJPG\")\n            out = cv2.VideoWriter(out_path, fourcc, freq, canvas.shape[1::-1])\n        else:\n            out = None\n\n        current_time = first_sample_rec[\"timestamp\"]\n\n        while current_time < last_sample_rec[\"timestamp\"]:\n\n            current_time += time_step\n\n            # For each channel, find first sample that has time > current_time.\n            for channel, sd_rec in current_recs.items():\n                while sd_rec[\"timestamp\"] < current_time and sd_rec[\"next\"] != \"\":\n                    sd_rec = self.lyftd.get(\"sample_data\", sd_rec[\"next\"])\n                    current_recs[channel] = sd_rec\n\n            # Now add to canvas\n            for channel, sd_rec in current_recs.items():\n\n                # Only update canvas if we have not already rendered this one.\n                if not sd_rec == prev_recs[channel]:\n\n                    # Get annotations and params from DB.\n                    image_path, boxes, camera_intrinsic = self.lyftd.get_sample_data(\n                        sd_rec[\"token\"], box_vis_level=BoxVisibility.ANY\n                    )\n\n                    # Load and render\n                    if not image_path.exists():\n                        raise Exception(\"Error: Missing image %s\" % image_path)\n                    im = cv2.imread(str(image_path))\n                    for box in boxes:\n                        c = self.get_color(box.name)\n                        box.render_cv2(im, view=camera_intrinsic, normalize=True, colors=(c, c, c))\n\n                    im = cv2.resize(im, image_size)\n                    if channel in horizontal_flip:\n                        im = im[:, ::-1, :]\n\n                    canvas[\n                        layout[channel][1] : layout[channel][1] + image_size[1],\n                        layout[channel][0] : layout[channel][0] + image_size[0],\n                        :,\n                    ] = im\n\n                    prev_recs[channel] = sd_rec  # Store here so we don't render the same image twice.\n\n            # Show updated canvas.\n            cv2.imshow(window_name, canvas)\n            if out_path is not None:\n                out.write(canvas)\n\n            key = cv2.waitKey(1)  # Wait a very short time (1 ms).\n\n            if key == 32:  # if space is pressed, pause.\n                key = cv2.waitKey()\n\n            if key == 27:  # if ESC is pressed, exit.\n                cv2.destroyAllWindows()\n                break\n\n        cv2.destroyAllWindows()\n        if out_path is not None:\n            out.release()\n\n    def render_scene_channel(\n        self,\n        scene_token: str,\n        channel: str = \"CAM_FRONT\",\n        freq: float = 10,\n        image_size: Tuple[float, float] = (640, 360),\n        out_path: Path = None,\n    ) -> None:\n        \"\"\"Renders a full scene for a particular camera channel.\n        Args:\n            scene_token: Unique identifier of scene to render.\n            channel: Channel to render.\n            freq: Display frequency (Hz).\n            image_size: Size of image to render. The larger the slower this will run.\n            out_path: Optional path to write a video file of the rendered frames.\n        \"\"\"\n\n        valid_channels = [\n            \"CAM_FRONT_LEFT\",\n            \"CAM_FRONT\",\n            \"CAM_FRONT_RIGHT\",\n            \"CAM_BACK_LEFT\",\n            \"CAM_BACK\",\n            \"CAM_BACK_RIGHT\",\n        ]\n\n        assert image_size[0] / image_size[1] == 16 / 9, \"Aspect ratio should be 16/9.\"\n        assert channel in valid_channels, \"Input channel {} not valid.\".format(channel)\n\n        if out_path is not None:\n            assert out_path.suffix == \".avi\"\n\n        # Get records from DB\n        scene_rec = self.lyftd.get(\"scene\", scene_token)\n        sample_rec = self.lyftd.get(\"sample\", scene_rec[\"first_sample_token\"])\n        sd_rec = self.lyftd.get(\"sample_data\", sample_rec[\"data\"][channel])\n\n        # Open CV init\n        name = \"{}: {} (Space to pause, ESC to exit)\".format(scene_rec[\"name\"], channel)\n        cv2.namedWindow(name)\n        cv2.moveWindow(name, 0, 0)\n\n        if out_path is not None:\n            fourcc = cv2.VideoWriter_fourcc(*\"MJPG\")\n            out = cv2.VideoWriter(out_path, fourcc, freq, image_size)\n        else:\n            out = None\n\n        has_more_frames = True\n        while has_more_frames:\n\n            # Get data from DB\n            image_path, boxes, camera_intrinsic = self.lyftd.get_sample_data(\n                sd_rec[\"token\"], box_vis_level=BoxVisibility.ANY\n            )\n\n            # Load and render\n            if not image_path.exists():\n                raise Exception(\"Error: Missing image %s\" % image_path)\n            image = cv2.imread(str(image_path))\n            for box in boxes:\n                c = self.get_color(box.name)\n                box.render_cv2(image, view=camera_intrinsic, normalize=True, colors=(c, c, c))\n\n            # Render\n            image = cv2.resize(image, image_size)\n            cv2.imshow(name, image)\n            if out_path is not None:\n                out.write(image)\n\n            key = cv2.waitKey(10)  # Images stored at approx 10 Hz, so wait 10 ms.\n            if key == 32:  # If space is pressed, pause.\n                key = cv2.waitKey()\n\n            if key == 27:  # if ESC is pressed, exit\n                cv2.destroyAllWindows()\n                break\n\n            if not sd_rec[\"next\"] == \"\":\n                sd_rec = self.lyftd.get(\"sample_data\", sd_rec[\"next\"])\n            else:\n                has_more_frames = False\n\n        cv2.destroyAllWindows()\n        if out_path is not None:\n            out.release()\n\n    def render_egoposes_on_map(\n        self,\n        log_location: str,\n        scene_tokens: List = None,\n        close_dist: float = 100,\n        color_fg: Tuple[int, int, int] = (167, 174, 186),\n        color_bg: Tuple[int, int, int] = (255, 255, 255),\n        out_path: Path = None,\n    ) -> None:\n        \"\"\"Renders ego poses a the map. These can be filtered by location or scene.\n        Args:\n            log_location: Name of the location, e.g. \"singapore-onenorth\", \"singapore-hollandvillage\",\n                             \"singapore-queenstown' and \"boston-seaport\".\n            scene_tokens: Optional list of scene tokens.\n            close_dist: Distance in meters for an ego pose to be considered within range of another ego pose.\n            color_fg: Color of the semantic prior in RGB format (ignored if map is RGB).\n            color_bg: Color of the non-semantic prior in RGB format (ignored if map is RGB).\n            out_path: Optional path to save the rendered figure to disk.\n        Returns:\n        \"\"\"\n\n        # Get logs by location\n        log_tokens = [l[\"token\"] for l in self.lyftd.log if l[\"location\"] == log_location]\n        assert len(log_tokens) > 0, \"Error: This split has 0 scenes for location %s!\" % log_location\n\n        # Filter scenes\n        scene_tokens_location = [e[\"token\"] for e in self.lyftd.scene if e[\"log_token\"] in log_tokens]\n        if scene_tokens is not None:\n            scene_tokens_location = [t for t in scene_tokens_location if t in scene_tokens]\n        if len(scene_tokens_location) == 0:\n            print(\"Warning: Found 0 valid scenes for location %s!\" % log_location)\n\n        map_poses = []\n        map_mask = None\n\n        print(\"Adding ego poses to map...\")\n        for scene_token in tqdm(scene_tokens_location):\n\n            # Get records from the database.\n            scene_record = self.lyftd.get(\"scene\", scene_token)\n            log_record = self.lyftd.get(\"log\", scene_record[\"log_token\"])\n            map_record = self.lyftd.get(\"map\", log_record[\"map_token\"])\n            map_mask = map_record[\"mask\"]\n\n            # For each sample in the scene, store the ego pose.\n            sample_tokens = self.lyftd.field2token(\"sample\", \"scene_token\", scene_token)\n            for sample_token in sample_tokens:\n                sample_record = self.lyftd.get(\"sample\", sample_token)\n\n                # Poses are associated with the sample_data. Here we use the lidar sample_data.\n                sample_data_record = self.lyftd.get(\"sample_data\", sample_record[\"data\"][\"LIDAR_TOP\"])\n                pose_record = self.lyftd.get(\"ego_pose\", sample_data_record[\"ego_pose_token\"])\n\n                # Calculate the pose on the map and append\n                map_poses.append(\n                    np.concatenate(\n                        map_mask.to_pixel_coords(pose_record[\"translation\"][0], pose_record[\"translation\"][1])\n                    )\n                )\n\n        # Compute number of close ego poses.\n        print(\"Creating plot...\")\n        map_poses = np.vstack(map_poses)\n        dists = sklearn.metrics.pairwise.euclidean_distances(map_poses * map_mask.resolution)\n        close_poses = np.sum(dists < close_dist, axis=0)\n\n        if len(np.array(map_mask.mask()).shape) == 3 and np.array(map_mask.mask()).shape[2] == 3:\n            # RGB Colour maps\n            mask = map_mask.mask()\n        else:\n            # Monochrome maps\n            # Set the colors for the mask.\n            mask = Image.fromarray(map_mask.mask())\n            mask = np.array(mask)\n\n            maskr = color_fg[0] * np.ones(np.shape(mask), dtype=np.uint8)\n            maskr[mask == 0] = color_bg[0]\n            maskg = color_fg[1] * np.ones(np.shape(mask), dtype=np.uint8)\n            maskg[mask == 0] = color_bg[1]\n            maskb = color_fg[2] * np.ones(np.shape(mask), dtype=np.uint8)\n            maskb[mask == 0] = color_bg[2]\n            mask = np.concatenate(\n                (np.expand_dims(maskr, axis=2), np.expand_dims(maskg, axis=2), np.expand_dims(maskb, axis=2)), axis=2\n            )\n\n        # Plot.\n        _, ax = plt.subplots(1, 1, figsize=(10, 10))\n        ax.imshow(mask)\n        title = \"Number of ego poses within {}m in {}\".format(close_dist, log_location)\n        ax.set_title(title, color=\"k\")\n        sc = ax.scatter(map_poses[:, 0], map_poses[:, 1], s=10, c=close_poses)\n        color_bar = plt.colorbar(sc, fraction=0.025, pad=0.04)\n        plt.rcParams[\"figure.facecolor\"] = \"black\"\n        color_bar_ticklabels = plt.getp(color_bar.ax.axes, \"yticklabels\")\n        plt.setp(color_bar_ticklabels, color=\"k\")\n        plt.rcParams[\"figure.facecolor\"] = \"white\"  # Reset for future plots\n\n        if out_path is not None:\n            plt.savefig(out_path)\n            plt.close(\"all\")", "processed": ["creat anoth class call lyftdatasetexplor help u visual data click code right side"]}, {"markdown": ["We can now convert the field to dtype 'float' and then get the counts"], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/just-another-visualisation\n\ntrain['age'] = train['age'].astype('float64')\n\nage_series = train.age.value_counts()\nplt.figure(figsize=(12,4))\nsns.barplot(age_series.index.astype('int'), age_series.values, alpha=0.8)\nplt.ylabel('Number of Occurrences of the customer', fontsize=12)\nplt.xlabel('Age', fontsize=12)\nplt.xticks(rotation='vertical')\nplt.show()", "processed": ["convert field dtype float get count"]}, {"markdown": ["We have 27734 missing values and the mean age is 40. We could probably do a mean imputation here. \n\nWe could look at test set age distribution to confirm both train and test have same distribution."], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/just-another-visualisation\n\ntest = pd.read_csv(test_file, usecols=['age'])\ntest['age'] = test['age'].replace(to_replace=[' NA'], value=np.nan)\ntest['age'] = test['age'].astype('float64')\n\nage_series = test.age.value_counts()\nplt.figure(figsize=(12,4))\nsns.barplot(age_series.index.astype('int'), age_series.values, alpha=0.8)\nplt.ylabel('Number of Occurrences of the customer', fontsize=12)\nplt.xlabel('Age', fontsize=12)\nplt.xticks(rotation='vertical')\nplt.show()", "processed": ["27734 miss valu mean age 40 could probabl mean imput could look test set age distribut confirm train test distribut"]}, {"markdown": ["We have 38 special values. If we use a tree based model, we could probably leave it as such or if we use a linear model, we need to map it to mean or some value in the range of 0 to 256.\n\nNow we can see the distribution plot of this variable."], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/just-another-visualisation\n\ncol_series = train.antiguedad.value_counts()\nplt.figure(figsize=(12,4))\nsns.barplot(col_series.index.astype('int'), col_series.values, alpha=0.8)\nplt.ylabel('Number of Occurrences of the customer', fontsize=12)\nplt.xlabel('Customer Seniority', fontsize=12)\nplt.xticks(rotation='vertical')\nplt.show()", "processed": ["38 special valu use tree base model could probabl leav use linear model need map mean valu rang 0 256 see distribut plot variabl"]}, {"markdown": ["There are few peaks and troughs in the plot but there are no visible gaps or anything as such which is alarming (atleast to me.!)\n\nSo we can also see whether test follows a similar pattern and if it does then we are good."], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/just-another-visualisation\n\ntest = pd.read_csv(test_file, usecols=['antiguedad'])\ntest['antiguedad'] = test['antiguedad'].replace(to_replace=[' NA'], value=np.nan)\ntest['antiguedad'] = test['antiguedad'].astype('float64')\n\ncol_series = test.antiguedad.value_counts()\nplt.figure(figsize=(12,4))\nsns.barplot(col_series.index.astype('int'), col_series.values, alpha=0.8)\nplt.ylabel('Number of Occurrences of the customer', fontsize=12)\nplt.xlabel('Customer Seniority', fontsize=12)\nplt.xticks(rotation='vertical')\nplt.show()", "processed": ["peak trough plot visibl gap anyth alarm atleast also see whether test follow similar pattern good"]}, {"markdown": ["There are quite a few number of missing values present in this field.! We can do some form of imputation for the same. One very good idea is given by Alan in this [script][1].\n\nWe can check the quantile distribution to see how the value changes in the last percentile.\n\n\n  [1]: https://www.kaggle.com/apryor6/santander-product-recommendation/detailed-cleaning-visualization-python"], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/just-another-visualisation\n\ntrain.fillna(101850., inplace=True) #filling NA as median for now\nquantile_series = train.renta.quantile(np.arange(0.99,1,0.001))\nplt.figure(figsize=(12,4))\nsns.barplot((quantile_series.index*100), quantile_series.values, alpha=0.8)\nplt.ylabel('Rent value', fontsize=12)\nplt.xlabel('Quantile value', fontsize=12)\nplt.xticks(rotation='vertical')\nplt.show()", "processed": ["quit number miss valu present field form imput one good idea given alan script 1 check quantil distribut see valu chang last percentil 1 http www kaggl com apryor6 santand product recommend detail clean visual python"]}, {"markdown": ["As we can see there is a sudden increase in the rent value from 99.9% to 100%. So let us max cap the rent values at 99.9% and then get a box plot."], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/just-another-visualisation\n\nrent_max_cap = train.renta.quantile(0.999)\ntrain['renta'][train['renta']>rent_max_cap] = 101850.0 # assigining median value \nsns.boxplot(train.renta.values)\nplt.show()", "processed": ["see sudden increas rent valu 99 9 100 let u max cap rent valu 99 9 get box plot"]}, {"markdown": ["*Please note that there is a new value '   NA' present in the test data set while it is not in train data.*\n\nThe distribution looks similar to train though."], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/just-another-visualisation\n\ntest.renta.mean()\ntest.fillna(101850., inplace=True) #filling NA as median for now\nquantile_series = test.renta.quantile(np.arange(0.99,1,0.001))\nplt.figure(figsize=(12,4))\nsns.barplot((quantile_series.index*100), quantile_series.values, alpha=0.8)\nplt.ylabel('Rent value', fontsize=12)\nplt.xlabel('Quantile value', fontsize=12)\nplt.xticks(rotation='vertical')\nplt.show()\ntest['renta'][test['renta']>rent_max_cap] = 101850.0 # assigining median value \nsns.boxplot(test.renta.values)\nplt.show()", "processed": ["plea note new valu na present test data set train data distribut look similar train though"]}, {"markdown": ["So box and quantile plots are similar to that of the train dataset for rent.!\n\n**Numerical variables Vs Target variables:**\n\nNow let us see how the targets are distributed based on the numerical variables present in the data. Let us subset the first 100K rows for the same. "], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/just-another-visualisation\n\ntrain = pd.read_csv(data_path+\"train_ver2.csv\", nrows=100000)\ntarget_cols = ['ind_cco_fin_ult1', 'ind_cder_fin_ult1',\n                             'ind_cno_fin_ult1', 'ind_ctju_fin_ult1',\n                             'ind_ctma_fin_ult1', 'ind_ctop_fin_ult1',\n                             'ind_ctpp_fin_ult1', 'ind_deco_fin_ult1',\n                             'ind_deme_fin_ult1', 'ind_dela_fin_ult1',\n                             'ind_ecue_fin_ult1', 'ind_fond_fin_ult1',\n                             'ind_hip_fin_ult1', 'ind_plan_fin_ult1',\n                             'ind_pres_fin_ult1', 'ind_reca_fin_ult1',\n                             'ind_tjcr_fin_ult1', 'ind_valo_fin_ult1',\n                             'ind_viv_fin_ult1', 'ind_nomina_ult1',\n                             'ind_nom_pens_ult1', 'ind_recibo_ult1']\ntrain[target_cols] = (train[target_cols].fillna(0))\ntrain[\"age\"] = train['age'].map(str.strip).replace(['NA'], value=0).astype('float')\ntrain[\"antiguedad\"] = train[\"antiguedad\"].map(str.strip)\ntrain[\"antiguedad\"] = train['antiguedad'].replace(['NA'], value=0).astype('float')\ntrain[\"antiguedad\"].ix[train[\"antiguedad\"]>65] = 65 # there is one very high skewing the graph\ntrain[\"renta\"].ix[train[\"renta\"]>1e6] = 1e6 # capping the higher values for better visualisation\ntrain.fillna(-1, inplace=True)\nfig = plt.figure(figsize=(16, 120))\nnumeric_cols = ['age', 'antiguedad', 'renta']\n#for ind1, numeric_col in enumerate(numeric_cols):\nplot_count = 0\nfor ind, target_col in enumerate(target_cols):\n    for numeric_col in numeric_cols:\n        plot_count += 1\n        plt.subplot(22, 3, plot_count)\n        sns.boxplot(x=target_col, y=numeric_col, data=train)\n        plt.title(numeric_col+\" Vs \"+target_col)\nplt.show()", "processed": ["box quantil plot similar train dataset rent numer variabl v target variabl let u see target distribut base numer variabl present data let u subset first 100k row"]}, {"markdown": ["So PCA doesn't seem to help much here.\n\nLet's take a look at clustering. We'll try to fit the KMeans clustering on the entire dataset, and try to see what the optimal number of clusters is."], "code": "# Reference: https://www.kaggle.com/code/tunguz/just-some-overfitting-eda\n\nSum_of_squared_distances = []\nK = range(1,15)\nfor k in K:\n    km = KMeans(n_clusters=k)\n    km = km.fit(train_test)\n    Sum_of_squared_distances.append(km.inertia_)\nplt.plot(K, Sum_of_squared_distances, 'bx-')\nplt.xlabel('k')\nplt.ylabel('Sum_of_squared_distances')\nplt.title('Elbow Method For Optimal k')\nplt.show()", "processed": ["pca seem help much let take look cluster tri fit kmean cluster entir dataset tri see optim number cluster"]}, {"markdown": ["**The training data is recordings in time. At each 10,000th of a second, the strength of the signal was recorded and the number of ion channels open was recorded. It is our task to build a model that predicts the number of open channels from signal at each time step. Furthermore we are told that the data was recorded in batches of 50 seconds. Therefore each 500,000 rows is one batch. The training data contains 10 batches and the test data contains 4 batches. Let's display the number of open channels and signal strength together for each training batch.**", "# Electrophysiology: The Art of the Heart's Rhythm\n\n**[What Happens During an Electrophysiology Study?](https://www.chestercountyhospital.org/news/health-eliving-blog/2019/february/electrophysiology-the-art-of-the-hearts-rhythm)**", "> An electrophysiology study (EPS) is a test that records your heart's electrical activity. It lets the electrophysiologist know if it's beating as it should or if you'll need treatment to get it back in rhythm.\n\n![](https://www.chestercountyhospital.org/-/media/images/chestercounty/news%20images/2019/arrhythmia.ashx?h=419&w=800&la=en)", "![](https://image.slidesharecdn.com/m7s5jm7zqxq4ntnu80nv-signature-5967dd0451739e2af475d635299752bfdaf370c5b5350b71f63bc5acce8dc46d-poli-150110161535-conversion-gate01/95/ionic-equilibria-and-membrane-potential-13-638.jpg?cb=1420906606)\n\n![](https://image.slidesharecdn.com/m7s5jm7zqxq4ntnu80nv-signature-5967dd0451739e2af475d635299752bfdaf370c5b5350b71f63bc5acce8dc46d-poli-150110161535-conversion-gate01/95/ionic-equilibria-and-membrane-potential-15-638.jpg?cb=1420906606)", "![](https://image.slidesharecdn.com/m7s5jm7zqxq4ntnu80nv-signature-5967dd0451739e2af475d635299752bfdaf370c5b5350b71f63bc5acce8dc46d-poli-150110161535-conversion-gate01/95/ionic-equilibria-and-membrane-potential-19-638.jpg?cb=1420906606)", "from the picture above we can see, in state 1 when Ionic Current (nA) is in 0th level means channel is closed then in state 2 we can see the graph moving toward -2 means the voltage gate is open and electric current are flowing through so the channel is open then in state 3 it is inactive as it reaches -2 level then in stage 4 it is inactivated again because it hasn't yet reach 0th level then in state 5 it gets closed as we reach at 0th level position\n\n**I'm not expert in this field,just trying to understand. so i can be wrong,correct me in the comment box if i am wrong**\n\ninfo source : [Ionic Equilibria and Membrane Potential ](https://www.slideshare.net/CsillaEgri/membrane-potential-43389721)", "# what does Membrane Protein do?", "**Membrane proteins are the gatekeepers to the cell and are essential to the function of all cells, controlling the flow of molecules and information across the cell membrane.**\nsource : [Membrane Protein](https://www.sciencedirect.com/topics/medicine-and-dentistry/membrane-protein)", "[**Ion Channels and Action Potential Generation**](https://www.sciencedirect.com/topics/neuroscience/voltage-clamp)", "The voltage-clamp technique is an experimental method that allows an experimenter to control (or \u201ccommand\u201d) the desired membrane voltage of the cell. The experimenter uses a set of electronic equipment (referred to here as a voltage-clamp device) to hold the membrane voltage at a desired level (the command voltage) while measuring the current that flows across the cell membrane at that voltage. The voltage-clamp device uses a negative feedback circuit to control the membrane voltage. To do this, the equipment measures the membrane voltage and compares it with the command voltage set by the experimenter. If the measured voltage is different from the command voltage, an error signal is generated and this tells the voltage-clamp device to pass current through an electrode in the neuron in order to correct the error and set the voltage to the command level. This can be accomplished using two microelectrodes inserted into the cell, one to measure voltage and another to pass current (see Figure Below), or using one large-diameter electrode that performs both functions.\n\n![](https://ars.els-cdn.com/content/image/3-s2.0-B9780128153208000041-f04-10-9780128153208.jpg?_)\n\n> Figure Description : The two-electrode voltage-clamp technique.\n> This diagram depicts the circuit that is used to clamp the voltage of a neuron and measure the current that flows at that membrane voltage.", "For more about Ion channels understanding please check this research paper : [Voltage Clamp](https://www.sciencedirect.com/topics/neuroscience/voltage-clamp) , this nature publication [Deep-Channel uses deep neural networks to detect single-molecule events from patch-clamp data](https://www.nature.com/articles/s42003-019-0729-3) and also this beautiful EDA kernel : [Ion Switching Competition : Signal EDA](https://www.kaggle.com/tarunpaparaju/ion-switching-competition-signal-eda)", "# About Data", "as from this kernel [One Feature Model](https://www.kaggle.com/cdeotte/one-feature-model-0-930) we get to know that The training data is recordings in time. At each 10,000th of a second, the strength of the signal was recorded and the number of ion channels open was recorded. It is our task to build a model that predicts the number of open channels from signal at each time step. Furthermore we are told that the data was recorded in batches of 50 seconds. Therefore each 500,000 rows is one batch. The training data contains 10 batches and the test data contains 4 batches. Let's display the number of open channels and signal strength together for each training batch."], "code": "# Reference: https://www.kaggle.com/code/mobassir/understanding-ion-switching-with-modeling\n\n#https://www.kaggle.com/cdeotte/one-feature-model-0-930\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(20,5)); res = 1000\nplt.plot(range(0,train.shape[0],res),train.signal[0::res])\nfor i in range(11): plt.plot([i*500000,i*500000],[-5,12.5],'r')\nfor j in range(10): plt.text(j*500000+200000,10,str(j+1),size=20)\nplt.xlabel('Row',size=16); plt.ylabel('Signal',size=16); \nplt.title('Training Data Signal - 10 batches',size=20)\nplt.show()\n\n\n#https://www.kaggle.com/cdeotte/one-feature-model-0-930\nplt.figure(figsize=(20,5)); res = 1000\nplt.plot(range(0,train.shape[0],res),train.open_channels[0::res])\nfor i in range(11): plt.plot([i*500000,i*500000],[-5,12.5],'r')\nfor j in range(10): plt.text(j*500000+200000,10,str(j+1),size=20)\nplt.xlabel('Row',size=16); plt.ylabel('Channels Open',size=16); \nplt.title('Training Data Open Channels - 10 batches',size=20)\nplt.show()\ntrain.head()\ntest.head()\nprint(len(train))\nprint(len(test))\ntrain.describe()", "processed": ["train data record time 10 000th second strength signal record number ion channel open record task build model predict number open channel signal time step furthermor told data record batch 50 second therefor 500 000 row one batch train data contain 10 batch test data contain 4 batch let display number open channel signal strength togeth train batch", "electrophysiolog art heart rhythm happen electrophysiolog studi http www chestercountyhospit org news health eliv blog 2019 februari electrophysiolog art heart rhythm", "electrophysiolog studi ep test record heart electr activ let electrophysiologist know beat need treatment get back rhythm http www chestercountyhospit org medium imag chestercounti news 20imag 2019 arrhythmia ashx h 419 w 800 la en", "http imag slidesharecdn com m7s5jm7zqxq4ntnu80nv signatur 5967dd0451739e2af475d635299752bfdaf370c5b5350b71f63bc5acce8dc46d poli 150110161535 convers gate01 95 ionic equilibrium membran potenti 13 638 jpg cb 1420906606 http imag slidesharecdn com m7s5jm7zqxq4ntnu80nv signatur 5967dd0451739e2af475d635299752bfdaf370c5b5350b71f63bc5acce8dc46d poli 150110161535 convers gate01 95 ionic equilibrium membran potenti 15 638 jpg cb 1420906606", "http imag slidesharecdn com m7s5jm7zqxq4ntnu80nv signatur 5967dd0451739e2af475d635299752bfdaf370c5b5350b71f63bc5acce8dc46d poli 150110161535 convers gate01 95 ionic equilibrium membran potenti 19 638 jpg cb 1420906606", "pictur see state 1 ionic current na 0th level mean channel close state 2 see graph move toward 2 mean voltag gate open electr current flow channel open state 3 inact reach 2 level stage 4 inactiv yet reach 0th level state 5 get close reach 0th level posit expert field tri understand wrong correct comment box wrong info sourc ionic equilibrium membran potenti http www slideshar net csillaegri membran potenti 43389721", "membran protein", "membran protein gatekeep cell essenti function cell control flow molecul inform across cell membran sourc membran protein http www sciencedirect com topic medicin dentistri membran protein", "ion channel action potenti gener http www sciencedirect com topic neurosci voltag clamp", "voltag clamp techniqu experiment method allow experiment control command desir membran voltag cell experiment use set electron equip refer voltag clamp devic hold membran voltag desir level command voltag measur current flow across cell membran voltag voltag clamp devic use neg feedback circuit control membran voltag equip measur membran voltag compar command voltag set experiment measur voltag differ command voltag error signal gener tell voltag clamp devic pas current electrod neuron order correct error set voltag command level accomplish use two microelectrod insert cell one measur voltag anoth pas current see figur use one larg diamet electrod perform function http ar el cdn com content imag 3 s2 0 b9780128153208000041 f04 10 9780128153208 jpg figur descript two electrod voltag clamp techniqu diagram depict circuit use clamp voltag neuron measur current flow membran voltag", "ion channel understand plea check research paper voltag clamp http www sciencedirect com topic neurosci voltag clamp natur public deep channel use deep neural network detect singl molecul event patch clamp data http www natur com articl s42003 019 0729 3 also beauti eda kernel ion switch competit signal eda http www kaggl com tarunpaparaju ion switch competit signal eda", "data", "kernel one featur model http www kaggl com cdeott one featur model 0 930 get know train data record time 10 000th second strength signal record number ion channel open record task build model predict number open channel signal time step furthermor told data record batch 50 second therefor 500 000 row one batch train data contain 10 batch test data contain 4 batch let display number open channel signal strength togeth train batch"]}, {"markdown": ["## Exploration"], "code": "# Reference: https://www.kaggle.com/code/xhlulu/exploration-and-preprocessing-for-keras-224x224\n\nlabel_df = pd.read_csv('../input/train.csv')\nsubmission_df = pd.read_csv('../input/sample_submission.csv')\nlabel_df.head()\nlabel_df['Id'].describe()\n# Display the most frequent ID (without counting new_whale)\nlabel_df['Id'].value_counts()[1:16].plot(kind='bar')\ndef display_samples(df, columns=4, rows=3):\n    fig=plt.figure(figsize=(5*columns, 3*rows))\n\n    for i in range(columns*rows):\n        image_path = df.loc[i,'Image']\n        image_id = df.loc[i,'Id']\n        img = cv2.imread(f'../input/train/{image_path}')\n        fig.add_subplot(rows, columns, i+1)\n        plt.title(image_id)\n        plt.imshow(img)\n\ndisplay_samples(label_df)", "processed": ["explor"]}, {"markdown": ["That's really interesting. Even with so few features we are gettign AUC of 0.75. That's farily significant for this kind of problem.\n\nLet's take a look at the feature imporances."], "code": "# Reference: https://www.kaggle.com/code/tunguz/adversarial-ions\n\ncolumns = ['time', 'signal', 'signal_scaled', 'signal_2']\nfeature_imp = pd.DataFrame(sorted(zip(clf.feature_importance(),columns)), columns=['Value','Feature'])\n\nplt.figure(figsize=(20, 10))\nsns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False).head(20))\nplt.title('LightGBM Features')\nplt.tight_layout()\nplt.show()\nplt.savefig('lgbm_importances-01.png')", "processed": ["realli interest even featur gettign auc 0 75 farili signific kind problem let take look featur impor"]}, {"markdown": ["We have around 182K rows in the dataset with 16 columns. Let us first look into the distribution of the target variable \"project_is_approved\" to understand more about the class imbalance."], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-eda-fe-notebook-donorschoose\n\ntemp_series = train_df[\"project_is_approved\"].value_counts()\n\nlabels = (np.array(temp_series.index))\nsizes = (np.array((temp_series / temp_series.sum())*100))\n\ntrace = go.Pie(labels=labels, values=sizes)\nlayout = go.Layout(\n    title='Project Proposal is Approved'\n)\ndata = [trace]\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename=\"ProjectApproval\")", "processed": ["around 182k row dataset 16 column let u first look distribut target variabl project approv understand class imbal"]}, {"markdown": ["Nice to see that ~85% of the project proposals are approved. So we do have a class imbalance with the majority class as positive. It is good that we have [area under the ROC curve](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) as evaluation metric, given this class imbalance. \n\nNow that we got an idea about the distribution of the classes, let us now look at the inidividual variables to understand more. \n\n**Project Grade Category:**"], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-eda-fe-notebook-donorschoose\n\n### Stacked Bar Chart ###\nx_values = train_df[\"project_grade_category\"].value_counts().index.tolist()\ny0_values = []\ny1_values = []\ny_values = []\nfor val in x_values:\n    y1_values.append(np.sum(train_df[\"project_is_approved\"][train_df[\"project_grade_category\"]==val] == 1))\n    y0_values.append(np.sum(train_df[\"project_is_approved\"][train_df[\"project_grade_category\"]==val] == 0))\n    y_values.append(np.mean(train_df[\"project_is_approved\"][train_df[\"project_grade_category\"]==val]))\n    \ntrace1 = go.Bar(\n    x = x_values,\n    y = y1_values,\n    name='Accepted Proposals'\n)\ntrace2 = go.Bar(\n    x = x_values,\n    y = y0_values, \n    name='Rejected Proposals'\n)\n\ndata = [trace1, trace2]\nlayout = go.Layout(\n    title = \"Project Grade Distribution\",\n    barmode='stack',\n    width = 1000\n)\n\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename='ProjectGradeCategory')\n\n### Bar chart ###\ntrace = go.Bar(\n    x = x_values,\n    y = y_values,\n    name='Accepted Proposals'\n)\ndata = [trace]\nlayout = go.Layout(\n    title = \"Project acceptance percentage by Project grade\",\n    width = 1000,\n    yaxis=dict(range=[0.7, 0.95])\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename='ProjectGradePerc')\n", "processed": ["nice see 85 project propos approv class imbal major class posit good area roc curv http en wikipedia org wiki receiv oper characterist evalu metric given class imbal got idea distribut class let u look inidividu variabl understand project grade categori"]}, {"markdown": ["Project proposal acceptance percentage is 83 to 85% for all the Grades of the class. So may be this variable is not really important for our prediction. Now let us look at the next variable \"project_subject_categories\".\n\n**Project Subject Category:**"], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-eda-fe-notebook-donorschoose\n\n### Stacked Bar Chart ###\nx_values = train_df[\"project_subject_categories\"].value_counts().index.tolist()\ny0_values = []\ny1_values = []\ny_values = []\nfor val in x_values:\n    y1_values.append(np.sum(train_df[\"project_is_approved\"][train_df[\"project_subject_categories\"]==val] == 1))\n    y0_values.append(np.sum(train_df[\"project_is_approved\"][train_df[\"project_subject_categories\"]==val] == 0))\n    y_values.append(np.mean(train_df[\"project_is_approved\"][train_df[\"project_subject_categories\"]==val]))\n    \ntrace1 = go.Bar(\n    x = x_values,\n    y = y1_values,\n    name='Accepted Proposals'\n)\ntrace2 = go.Bar(\n    x = x_values,\n    y = y0_values, \n    name='Rejected Proposals'\n)\n\ndata = [trace1, trace2]\nlayout = go.Layout(\n    title = \"Project Subject Category Distribution\",\n    barmode='stack',\n)\n\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename='ProjectGradeCategory')\n\n### Bar chart ###\ntrace = go.Bar(\n    x = x_values,\n    y = y_values,\n    name='Accepted Proposals'\n)\ndata = [trace]\nlayout = go.Layout(\n    title = \"Project acceptance percentage by Project Subject Category\",\n    yaxis=dict(range=[0.6, 0.95])\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename='ProjectGradePerc')", "processed": ["project propos accept percentag 83 85 grade class may variabl realli import predict let u look next variabl project subject categori project subject categori"]}, {"markdown": ["Project subject category has a very long tail. Among the top categories, the acceptance percentage lies from 80 to 86% and so slightly better than the previous feature. \n\n**Teacher Prefix:**"], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-eda-fe-notebook-donorschoose\n\n### Stacked Bar Chart ###\nx_values = train_df[\"teacher_prefix\"].value_counts().index.tolist()\ny0_values = []\ny1_values = []\ny_values = []\nfor val in x_values:\n    y1_values.append(np.sum(train_df[\"project_is_approved\"][train_df[\"teacher_prefix\"]==val] == 1))\n    y0_values.append(np.sum(train_df[\"project_is_approved\"][train_df[\"teacher_prefix\"]==val] == 0))\n    y_values.append(np.mean(train_df[\"project_is_approved\"][train_df[\"teacher_prefix\"]==val]))\n    \ntrace1 = go.Bar(\n    x = x_values,\n    y = y1_values,\n    name='Accepted Proposals'\n)\ntrace2 = go.Bar(\n    x = x_values,\n    y = y0_values, \n    name='Rejected Proposals'\n)\n\ndata = [trace1, trace2]\nlayout = go.Layout(\n    title = \"Teacher Prefix Distribution\",\n    barmode='stack',\n    width = 1000\n)\n\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename='ProjectGradeCategory')\n\n### Bar chart ###\ntrace = go.Bar(\n    x = x_values,\n    y = y_values,\n    name='Accepted Proposals'\n)\ndata = [trace]\nlayout = go.Layout(\n    title = \"Project acceptance rate by Teacher Prefix\",\n    width = 1000,\n    yaxis=dict(range=[0.7, 0.95])\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename='ProjectGradePerc')", "processed": ["project subject categori long tail among top categori accept percentag lie 80 86 slightli better previou featur teacher prefix"]}, {"markdown": ["Among the prefixes, \"Teacher\" has low (79%) acceptance rate compared to others. Now let us look at the states at which the schools are located.\n\n**School States:**"], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-eda-fe-notebook-donorschoose\n\n### Stacked Bar Chart ###\nx_values = train_df[\"school_state\"].value_counts().index.tolist()\ny0_values = []\ny1_values = []\ny_values = []\nfor val in x_values:\n    y1_values.append(np.sum(train_df[\"project_is_approved\"][train_df[\"school_state\"]==val] == 1))\n    y0_values.append(np.sum(train_df[\"project_is_approved\"][train_df[\"school_state\"]==val] == 0))\n    y_values.append(np.mean(train_df[\"project_is_approved\"][train_df[\"school_state\"]==val]))\n    \ntrace1 = go.Bar(\n    x = x_values,\n    y = y1_values,\n    name='Accepted Proposals'\n)\ntrace2 = go.Bar(\n    x = x_values,\n    y = y0_values, \n    name='Rejected Proposals'\n)\n\ndata = [trace1, trace2]\nlayout = go.Layout(\n    title = \"School State Distribution\",\n    barmode='stack',\n)\n\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename='ProjectGradeCategory')\n\n### Bar chart ###\ntrace = go.Bar(\n    x = x_values,\n    y = y_values,\n    name='Accepted Proposals'\n)\ndata = [trace]\nlayout = go.Layout(\n    title = \"Project acceptance rate by School state\",\n    yaxis=dict(range=[0.75, 0.9])\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename='ProjectGradePerc')", "processed": ["among prefix teacher low 79 accept rate compar other let u look state school locat school state"]}, {"markdown": ["Now let us look at the same statewise distribution in US map for better visual understanding. "], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-eda-fe-notebook-donorschoose\n\ncon_df = pd.DataFrame(train_df[\"school_state\"].value_counts()).reset_index()\ncon_df.columns = ['state_code', 'num_proposals']\n\nscl = [[0.0, 'rgb(242,240,247)'],[0.2, 'rgb(218,218,235)'],[0.4, 'rgb(188,189,220)'],\\\n            [0.6, 'rgb(158,154,200)'],[0.8, 'rgb(117,107,177)'],[1.0, 'rgb(84,39,143)']]\n\ndata = [ dict(\n        type='choropleth',\n        colorscale = scl,\n        autocolorscale = False,\n        locations = con_df['state_code'],\n        z = con_df['num_proposals'].astype(float),\n        locationmode = 'USA-states',\n        text = con_df['state_code'],\n        marker = dict(\n            line = dict (\n                color = 'rgb(255,255,255)',\n                width = 2\n            ) ),\n        colorbar = dict(\n            title = \"Num Project Proposals\")\n        ) ]\n\nlayout = dict(\n        title = 'Project Proposals by US States<br>(Hover for breakdown)',\n        geo = dict(\n            scope='usa',\n            projection=dict( type='albers usa' ),\n            showlakes = True,\n            lakecolor = 'rgb(255, 255, 255)'),\n             )\n    \nfig = dict( data=data, layout=layout )\npy.iplot( fig, filename='d3-cloropleth-map' )\n\n\n### mean acceptance rate ###\ncon_df = pd.DataFrame(train_df.groupby(\"school_state\")[\"project_is_approved\"].apply(np.mean)).reset_index()\ncon_df.columns = ['state_code', 'mean_proposals']\n\nscl = [[0.0, 'rgb(242,240,247)'],[0.2, 'rgb(218,218,235)'],[0.4, 'rgb(188,189,220)'],\\\n            [0.6, 'rgb(158,154,200)'],[0.8, 'rgb(117,107,177)'],[1.0, 'rgb(84,39,143)']]\n\ndata = [ dict(\n        type='choropleth',\n        colorscale = scl,\n        autocolorscale = False,\n        locations = con_df['state_code'],\n        z = con_df['mean_proposals'].astype(float),\n        locationmode = 'USA-states',\n        text = con_df['state_code'],\n        marker = dict(\n            line = dict (\n                color = 'rgb(255,255,255)',\n                width = 2\n            ) ),\n        colorbar = dict(\n            title = \"Project Proposals Acceptance Rate\")\n        ) ]\n\nlayout = dict(\n        title = 'Project Proposals Acceptance Rate by US States<br>(Hover for breakdown)',\n        geo = dict(\n            scope='usa',\n            projection=dict( type='albers usa' ),\n            showlakes = True,\n            lakecolor = 'rgb(255, 255, 255)'),\n             )\n    \nfig = dict( data=data, layout=layout )\npy.iplot( fig, filename='d3-cloropleth-map' )\n", "processed": ["let u look statewis distribut u map better visual understand"]}, {"markdown": ["Now let us look at the data from temporal point of view. \n\n**Project Submission Time:**\n\nThis is the time at which the application is submitted. "], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-eda-fe-notebook-donorschoose\n\ntrain_df[\"project_submitted_datetime\"] = pd.to_datetime(train_df[\"project_submitted_datetime\"])\ntrain_df[\"date_created\"] = train_df[\"project_submitted_datetime\"].dt.date\n\nx_values = train_df[\"date_created\"].value_counts().index.tolist()\ny0_values = []\ny1_values = []\ny_values = []\nfor val in x_values:\n    y1_values.append(np.sum(train_df[\"project_is_approved\"][train_df[\"date_created\"]==val] == 1))\n    y0_values.append(np.sum(train_df[\"project_is_approved\"][train_df[\"date_created\"]==val] == 0))\n    y_values.append(np.mean(train_df[\"project_is_approved\"][train_df[\"date_created\"]==val]))\n\ntrace1 = go.Bar(\n    x = x_values,\n    y = y1_values,\n    name='Accepted Proposals'\n)\ntrace2 = go.Bar(\n    x = x_values,\n    y = y0_values, \n    name='Rejected Proposals'\n)\n\ndata = [trace1, trace2]\nlayout = go.Layout(\n    title = \"Project Proposal Submission Date Distribution\",\n    barmode='stack',\n)\n\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename='ProjectGradeCategory')\n\n### Bar chart ###\ntrace = go.Bar(\n    x = x_values,\n    y = y_values,\n    name='Accepted Proposals'\n)\ndata = [trace]\nlayout = go.Layout(\n    title = \"Project acceptance rate by Proposal Submission date\",\n    yaxis=dict(range=[0.7, 0.95])\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename='ProjectGradePerc')\n", "processed": ["let u look data tempor point view project submiss time time applic submit"]}, {"markdown": ["Some of the inferences from the plots are:\n\n* Looks like we have approximately one years' worth of data (May 2016 to April 2017) given in the training set. \n* There is a sudden spike on a single day (Sep 1, 2016) with respect to the number of proposals (may be some specific reason?)\n* There is a spike in the number of proposals coming in during the initial part of the academic year (August till October)\n* There is no visible pattern as such in the acceptance rate based on the timeline. \n\nNow let us also look at the plots at different levels of temporal attributes like month, day of month, weekday etc."], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-eda-fe-notebook-donorschoose\n\ntrain_df[\"month_created\"] = train_df[\"project_submitted_datetime\"].dt.month\n\nx_values = train_df[\"month_created\"].value_counts().index.tolist()\ny0_values = []\ny1_values = []\ny_values = []\nfor val in x_values:\n    y1_values.append(np.sum(train_df[\"project_is_approved\"][train_df[\"month_created\"]==val] == 1))\n    y0_values.append(np.sum(train_df[\"project_is_approved\"][train_df[\"month_created\"]==val] == 0))\n    y_values.append(np.mean(train_df[\"project_is_approved\"][train_df[\"month_created\"]==val]))\n\ntrace1 = go.Bar(\n    x = x_values,\n    y = y1_values,\n    name='Accepted Proposals'\n)\ntrace2 = go.Bar(\n    x = x_values,\n    y = y0_values, \n    name='Rejected Proposals'\n)\n\ndata = [trace1, trace2]\nlayout = go.Layout(\n    title = \"Project Proposal Submission Month Distribution\",\n    barmode='stack',\n)\n\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename='ProjectGradeCategory')\n\n### Bar chart ###\ntrace = go.Bar(\n    x = x_values,\n    y = y_values,\n    name='Accepted Proposals'\n)\ndata = [trace]\nlayout = go.Layout(\n    title = \"Project acceptance rate by Proposal Submission Month\",\n    yaxis=dict(range=[0.7, 0.95])\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename='ProjectGradePerc')", "processed": ["infer plot look like approxim one year worth data may 2016 april 2017 given train set sudden spike singl day sep 1 2016 respect number propos may specif reason spike number propos come initi part academ year august till octob visibl pattern accept rate base timelin let u also look plot differ level tempor attribut like month day month weekday etc"]}, {"markdown": ["September month has the second highest number of proposals and the least acceptance rate of all the months. \n\n"], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-eda-fe-notebook-donorschoose\n\ntrain_df[\"weekday_created\"] = train_df[\"project_submitted_datetime\"].dt.weekday\n\nx_values = train_df[\"weekday_created\"].value_counts().index.tolist()\ny0_values = []\ny1_values = []\ny_values = []\nfor val in x_values:\n    y1_values.append(np.sum(train_df[\"project_is_approved\"][train_df[\"weekday_created\"]==val] == 1))\n    y0_values.append(np.sum(train_df[\"project_is_approved\"][train_df[\"weekday_created\"]==val] == 0))\n    y_values.append(np.mean(train_df[\"project_is_approved\"][train_df[\"weekday_created\"]==val]))\nx_values = [\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"]\n\ntrace1 = go.Bar(\n    x = x_values,\n    y = y1_values,\n    name='Accepted Proposals'\n)\ntrace2 = go.Bar(\n    x = x_values,\n    y = y0_values, \n    name='Rejected Proposals'\n)\n\ndata = [trace1, trace2]\nlayout = go.Layout(\n    title = \"Project Proposal Submission Weekday Distribution\",\n    barmode='stack',\n)\n\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename='ProjectGradeCategory')\n\n### Bar chart ###\ntrace = go.Bar(\n    x = x_values,\n    y = y_values,\n    name='Accepted Proposals'\n)\ndata = [trace]\nlayout = go.Layout(\n    title = \"Project acceptance rate by Proposal Submission Weekday\",\n    yaxis=dict(range=[0.7, 0.95])\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename='ProjectGradePerc')", "processed": ["septemb month second highest number propos least accept rate month"]}, {"markdown": ["The number of proposals decreases as we move towards the end of the week. Now let us look at the hour of submission pattern."], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-eda-fe-notebook-donorschoose\n\ntrain_df[\"hour_created\"] = train_df[\"project_submitted_datetime\"].dt.hour\n\nx_values = train_df[\"hour_created\"].value_counts().index.tolist()\ny0_values = []\ny1_values = []\ny_values = []\nfor val in x_values:\n    y1_values.append(np.sum(train_df[\"project_is_approved\"][train_df[\"hour_created\"]==val] == 1))\n    y0_values.append(np.sum(train_df[\"project_is_approved\"][train_df[\"hour_created\"]==val] == 0))\n    y_values.append(np.mean(train_df[\"project_is_approved\"][train_df[\"hour_created\"]==val]))\n\ntrace1 = go.Bar(\n    x = x_values,\n    y = y1_values,\n    name='Accepted Proposals'\n)\ntrace2 = go.Bar(\n    x = x_values,\n    y = y0_values, \n    name='Rejected Proposals'\n)\n\ndata = [trace1, trace2]\nlayout = go.Layout(\n    title = \"Project Proposal Submission Hour Distribution\",\n    barmode='stack',\n)\n\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename='ProjectGradeCategory')\n\n### Bar chart ###\ntrace = go.Bar(\n    x = x_values,\n    y = y_values,\n    name='Accepted Proposals'\n)\ndata = [trace]\nlayout = go.Layout(\n    title = \"Project acceptance rate by Proposal Submission Hour\",\n    yaxis=dict(range=[0.7, 0.95])\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename='ProjectGradePerc')", "processed": ["number propos decreas move toward end week let u look hour submiss pattern"]}, {"markdown": ["Let us get the histogram of the price to understand the price requested for project proposals."], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-eda-fe-notebook-donorschoose\n\ntrace = go.Histogram(\n    x = np.log1p(train_df[\"price\"]),\n    nbinsx = 50,\n    opacity=0.75\n)\ndata = [trace]\nlayout = go.Layout(\n    title = \"Log Histogram of the prices of project proposal\",\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename='ProjectGradePerc')\nx1_values = np.log1p(train_df[\"price\"][train_df[\"project_is_approved\"]==1])\nx0_values = np.log1p(train_df[\"price\"][train_df[\"project_is_approved\"]==0])\n\ntrace0 = go.Histogram(\n    x = x0_values,\n    nbinsx = 50,\n    opacity = 0.75,\n    name = \"Rejected Proposals\"\n)\ntrace1 = go.Histogram(\n    x = x1_values,\n    nbinsx = 50,\n    opacity = 0.75,\n    name = \"Approved Proposal\"\n)\ndata = [trace1, trace0]\nlayout = go.Layout(\n    title = \"Proposal Approval at different price levels\",\n    barmode = \"overlay\"\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename='ProjectGradePerc')\nx_values = np.sort(train_df[\"quantity\"].value_counts().index.tolist())[:20]\ny0_values = []\ny1_values = []\ny_values = []\nfor val in x_values:\n    y1_values.append(np.sum(train_df[\"project_is_approved\"][train_df[\"quantity\"]==val] == 1))\n    y0_values.append(np.sum(train_df[\"project_is_approved\"][train_df[\"quantity\"]==val] == 0))\n    y_values.append(np.mean(train_df[\"project_is_approved\"][train_df[\"quantity\"]==val]))\n\ntrace1 = go.Bar(\n    x = x_values,\n    y = y1_values,\n    name='Accepted Proposals'\n)\ntrace2 = go.Bar(\n    x = x_values,\n    y = y0_values, \n    name='Rejected Proposals'\n)\n\ndata = [trace1, trace2]\nlayout = go.Layout(\n    title = \"Project Proposal - Item Quantity Distribution (Till 20)\",\n    barmode='stack',\n)\n\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename='ProjectGradeCategory')", "processed": ["let u get histogram price understand price request project propos"]}, {"markdown": ["## Culture Analysis", "### Define functions for visualizing the images and their tags"], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/imet-competition-cultural-eda\n\ndef display_culture_examples(culture, fraction=1, max_count=5):\n    count = 0\n    for i in range(int(len(train_images)*fraction)):\n        if count == max_count:\n            break\n        if culture in train_targets[i]:\n            fig, ax = plt.subplots(figsize=(3, 3))\n            ax.imshow(cv2.cvtColor(train_images[i], cv2.COLOR_BGR2RGB))\n            plt.title(train_targets[i], fontsize=8)\n            plt.show()\n            matplotlib.rcParams.update(matplotlib.rcParamsDefault)\n            count = count + 1\n            \ndef show_common_tags(culture, palette):\n    tags = []\n    for i in range(len(train_images)):\n        if culture in train_targets[i]:\n            tags.extend(train_targets[i])\n    tag_counts = Counter(tags)\n    size = len(tag_counts.keys())\n    tags_df = pd.DataFrame(np.zeros((size, 2)))\n    tags_df.columns = ['tag', 'count']\n    tags_df['tag'] = list(tag_counts.keys())\n    tags_df['count'] = list(tag_counts.values())\n    tags_df = tags_df.sort_values(by=['count'], ascending=False)\n    tags_df = tags_df.reset_index()\n    del tags_df['index']\n    tag_counts = sorted(tag_counts.items(), key = lambda kv:(kv[1], kv[0]))\n    tags = reversed(list(tag_counts)[-6:-1])\n    \n    print(\"MOST COMMON \" + culture.upper() + \" TAGS\")\n    print(\"\")\n    for i, tag in enumerate(tags_df[1:6].tag):\n        print(str(i+1) + '). ' + tag)\n    \n    sns.set_context(rc={'xtick.labelsize': 8})\n    fig, ax = plt.subplots(figsize=(12, 5))\n    plot = sns.barplot(x='tag', y='count', data=tags_df.loc[1:6], palette=palette)\n    plt.title('Common ' + culture[0].upper() + culture[1:] + ' tags')\n    plt.show()\n    sns.reset_defaults()", "processed": ["cultur analysi", "defin function visual imag tag"]}, {"markdown": ["Let's group the images per patient to see what is the distribution of images per patient."], "code": "# Reference: https://www.kaggle.com/code/gpreda/siim-isic-melanoma-classification-eda\n\ntmp = train_df.groupby(['patient_id', 'target'])['image_name'].count()\ntr_df = pd.DataFrame(tmp).reset_index(); tr_df.columns = ['patient_id', 'target', 'images']\n\ntmp = test_df.groupby(['patient_id'])['image_name'].count()\nte_df = pd.DataFrame(tmp).reset_index(); te_df.columns = ['patient_id', 'images']\n\n\ntr_df.head()\nte_df.head()\nplt.figure()\nfig, ax = plt.subplots(1,3,figsize=(16,6))\ng_tr0 = sns.distplot(tr_df.loc[tr_df.target==0, 'images'],kde=False,bins=50, color=\"green\",label='target = 0', ax=ax[0])\ng_tr1 = sns.distplot(tr_df.loc[tr_df.target==1, 'images'],kde=False,bins=50, color=\"red\",label='target = 1', ax=ax[1])\ng_te = sns.distplot(te_df['images'],kde=False,bins=50, color=\"blue\", label='columns', ax=ax[2])\ng_tr0.set_title('Number of images / patient - Train set\\n target = 0 (benign)')\ng_tr1.set_title('Number of images / patient - Train set\\n target = 1 (malignant)')\ng_te.set_title('Number of images / patient - Test set')\n\nlocs, labels = plt.xticks()\nplt.tick_params(axis='both', which='major', labelsize=12)\nplt.show()", "processed": ["let group imag per patient see distribut imag per patient"]}, {"markdown": ["We can observe that the numberof images / patient in the case of malignant cases have a more compact distribution, maximum number of images / patient being 8, while in the case of patients with the result of exam benign the total number of images / patient might be up to 120. Of course, also in the case of patients with benign diagnosis the majority of cases will have just few images. In the same time, we can most probably conclude that the number of images / patient might be a good predictor for the value of target if the number of images is large. \n"], "code": "# Reference: https://www.kaggle.com/code/gpreda/siim-isic-melanoma-classification-eda\n\ndef plot_count(df, feature, title='', size=2, rotate_axis = False):\n    f, ax = plt.subplots(1,1, figsize=(3*size,2*size))\n    total = float(len(df))\n    sns.countplot(df[feature],order = df[feature].value_counts().index, palette='Set3')\n    plt.title(title)\n    if(rotate_axis):\n        ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n    for p in ax.patches:\n        height = p.get_height()\n        ax.text(p.get_x()+p.get_width()/2.,\n                height + 3,\n                '{:1.2f}%'.format(100*height/total),\n                ha=\"center\") \n    plt.show()\nplot_count(train_df, 'sex', 'Patient sex (train) - data count and percent')\nplot_count(test_df, 'sex', 'Patient sex (test) - data count and percent')\nplot_count(test_df, 'age_approx', 'Patient approximate age (train) - data count and percent', size=4)\nplot_count(test_df, 'age_approx', 'Patient approximate age (test) - data count and percent', size=4)\nplot_count(train_df, 'anatom_site_general_challenge', 'Location of imaged site/anatomy part (train) - data count and percent', size=3)\nplot_count(test_df, 'anatom_site_general_challenge', 'Location of imaged site/anatomy part (test) - data count and percent', size=3)\nplot_count(train_df, 'diagnosis', 'Detailed diagnosis (train) - data count and percent', size=4, rotate_axis=True)\nplot_count(train_df, 'benign_malignant', 'Indicator of malignancy of imaged lesion  (train) - data count and percent')\nplot_count(train_df, 'target', 'Target value - 0: bening, 1: malignant (train)\\n data count and percent')\nfig, ax = plt.subplots(nrows=1,figsize=(16,6)) \ntmp = train_df.groupby('diagnosis')['target'].value_counts() \ndf = pd.DataFrame(data={'Exams': tmp.values}, index=tmp.index).reset_index() \nsns.barplot(ax=ax,x = 'diagnosis', y='Exams',hue='target',data=df, palette='Set1') \nplt.title(\"Number of examinations grouped on Diagnosis and Target\") \nplt.show()\nfig, ax = plt.subplots(nrows=1,figsize=(16,6)) \ntmp = train_df.groupby('diagnosis')['benign_malignant'].value_counts() \ndf = pd.DataFrame(data={'Exams': tmp.values}, index=tmp.index).reset_index() \nsns.barplot(ax=ax,x = 'diagnosis', y='Exams',hue='benign_malignant',data=df, palette='Set1') \nplt.title(\"Number of examinations grouped on Diagnosis and Benign/Malignant\") \nplt.show()\nfig, ax = plt.subplots(nrows=1,figsize=(16,6)) \ntmp = train_df.groupby('diagnosis')['sex'].value_counts() \ndf = pd.DataFrame(data={'Exams': tmp.values}, index=tmp.index).reset_index() \nsns.barplot(ax=ax,x = 'diagnosis', y='Exams',hue='sex',data=df, palette='Set1') \nplt.title(\"Number of examinations grouped on Diagnosis and Sex\") \nplt.show()\nfig, ax = plt.subplots(nrows=1,figsize=(8,6)) \ntmp = train_df.groupby('sex')['target'].value_counts() \ndf = pd.DataFrame(data={'Exams': tmp.values}, index=tmp.index).reset_index() \nsns.barplot(ax=ax,x = 'sex', y='Exams',hue='target',data=df, palette='Set1') \nplt.title(\"Number of examinations grouped on Sex and Target\") \nplt.show()\ndef plot_distribution_grouped(feature, feature_group, hist_flag=True):\n    fig, ax = plt.subplots(nrows=1,figsize=(12,6)) \n    for f in train_df[feature_group].unique():\n        df = train_df.loc[train_df[feature_group] == f]\n        sns.distplot(df[feature], hist=hist_flag, label=f)\n    plt.title(f'Data/image {feature} distribution, grouped by {feature_group}')\n    plt.legend()\n    plt.show()\nplot_distribution_grouped('age_approx', 'sex')", "processed": ["observ numberof imag patient case malign case compact distribut maximum number imag patient 8 case patient result exam benign total number imag patient might 120 cours also case patient benign diagnosi major case imag time probabl conclud number imag patient might good predictor valu target number imag larg"]}, {"markdown": ["**Original features**"], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/plasticc-comprehensive-eda-with-gp\n\ncolumns = features.columns\n\nfor column in columns[:-10]:\n    sns.pairplot(x_vars=column, y_vars=column, hue='target', diag_kind='kde', data=features)", "processed": ["origin featur"]}, {"markdown": ["**Engineered features**"], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/plasticc-comprehensive-eda-with-gp\n\nfor column in columns[-10:]:\n    sns.pairplot(x_vars=column, y_vars=column, hue='target', diag_kind='kde', data=features)", "processed": ["engin featur"]}, {"markdown": ["## PreTrained Model : VGG16\n\n\n\nKeras library also provides the pre-trained model in which one can load the saved model weights, and use them for different purposes : transfer learning, image feature extraction, and object detection. We can load the model architecture given in the library, and then add all the weights to the respective layers. \n\nBefore using the pretrained models, lets write a few functions which will be used to make some predictions. First, load some images and preprocess them. "], "code": "# Reference: https://www.kaggle.com/code/shivamb/cnn-architectures-vgg-resnet-inception-tl\n\nfrom keras.applications.vgg16 import decode_predictions\nfrom keras.applications.vgg16 import preprocess_input\nfrom keras.preprocessing import image\nimport matplotlib.pyplot as plt \nfrom PIL import Image \nimport seaborn as sns\nimport pandas as pd \nimport numpy as np \nimport os \n\nimg1 = \"../input/dogs-vs-cats-redux-kernels-edition/train/cat.11679.jpg\"\nimg2 = \"../input/dogs-vs-cats-redux-kernels-edition/train/dog.2811.jpg\"\nimg3 = \"../input/flowers-recognition/flowers/flowers/sunflower/7791014076_07a897cb85_n.jpg\"\nimg4 = \"../input/fruits/fruits-360_dataset/fruits-360/Training/Banana/254_100.jpg\"\nimgs = [img1, img2, img3, img4]\n\ndef _load_image(img_path):\n    img = image.load_img(img_path, target_size=(224, 224))\n    img = image.img_to_array(img)\n    img = np.expand_dims(img, axis=0)\n    img = preprocess_input(img)\n    return img \n\ndef _get_predictions(_model):\n    f, ax = plt.subplots(1, 4)\n    f.set_size_inches(80, 40)\n    for i in range(4):\n        ax[i].imshow(Image.open(imgs[i]).resize((200, 200), Image.ANTIALIAS))\n    plt.show()\n    \n    f, axes = plt.subplots(1, 4)\n    f.set_size_inches(80, 20)\n    for i,img_path in enumerate(imgs):\n        img = _load_image(img_path)\n        preds  = decode_predictions(_model.predict(img), top=3)[0]\n        b = sns.barplot(y=[c[1] for c in preds], x=[c[2] for c in preds], color=\"gray\", ax=axes[i])\n        b.tick_params(labelsize=55)\n        f.tight_layout()", "processed": ["pretrain model vgg16 kera librari also provid pre train model one load save model weight use differ purpos transfer learn imag featur extract object detect load model architectur given librari add weight respect layer use pretrain model let write function use make predict first load imag preprocess"]}, {"markdown": ["Let's look at some plots and see if we can spot anything."], "code": "# Reference: https://www.kaggle.com/code/jpmiller/eda-to-break-through-rmse-68\n\nimport hvplot.pandas\nopts = {'invert_yaxis': False,\n        'yticks': list(range(0,100,20)),\n        'padding': 0.1,\n        'width':450,\n        'height': 300,\n           }\n\n# df = bos\n# aggfunc='mean'\ndef make_plot(df, aggfunc):\n    assert (aggfunc == 'mean') | (aggfunc == 'std')\n    paths = df.set_index(('', 'Path')).loc[:, aggfunc].reset_index()\n    paths.columns = [paths.columns[0][1]] + [c[-4:] for c in paths.columns[1:]]\n    plot = hvplot.parallel_coordinates(paths, 'Path', **opts)\n    if aggfunc == 'mean':\n        return plot.options(ylabel='Mean Wait Time')\n    else:\n        return plot.options(ylabel='STD of Wait Times', show_legend=False)\n\nland_cambridge = make_plot(bos, 'mean').options(title=\"Land & Cambridgeside\") +\\\n    make_plot(bos, 'std')\nfifth_cambria = make_plot(phi, 'mean').options(title=\"5th & Cambria\") +\\\n    make_plot(phi, 'std')\n\ndisplay(land_cambridge, fifth_cambria)\n", "processed": ["let look plot see spot anyth"]}, {"markdown": ["There appear to be some things that make sense and other things that might depend on interactions with features we don't yet have. Looking at the Land intersection we can see that the left turns take the most time on average. The right turn off Cambridgeside is the quickest and has the lowest variation. We have the direction of turn data which should help a model. Going straight through on Land or u-turning has the highest variation. We'll look at that in a minute. \n\nFor the 5th and Cambria intersection, It looks like going east on Cambria has the highest average times, whether one is going straight or turning left. Let's look at that also to see what's up.", "### P-80 Deep Dive\nHere's a quick look at the p80 data to get more insight into high mean wait times and variations for the through traffic headed NE on Land."], "code": "# Reference: https://www.kaggle.com/code/jpmiller/eda-to-break-through-rmse-68\n\nopts = {'cmap': 'Paired',\n        'yticks': list(range(0,300,50)),\n        'colorbar': False,\n       'grid': True,\n         }\n\nland_ne = tt[(tt.IntersectionId == 2) &\n             (tt.EntryStreetName == 'Land Boulevard') &\n             (tt.ExitHeading == 'NE')\n             ].join(target_df)\nlandplot = land_ne.hvplot.scatter('Hour', 'TimeFromFirstStop_p80', \n                                    c='Weekend', **opts)\n\ncambria_e = tt[(tt.IntersectionId == 1824) &\n             (tt.EntryStreetName == 'West Cambria Street') &\n             (tt.ExitHeading == 'E')\n             ].join(target_df)\ncambplot = cambria_e.hvplot.scatter('Hour', 'TimeFromFirstStop_p80', \n                                    c='Weekend', **opts)\n\ndisplay(landplot.options(title='Land_NE_thru'), cambplot.options(title='Cambria_E_thru'))\n# alternate plot with seaborn to trigger viz output on the Notebooks page\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ncambria_e.plot(kind='scatter', x='Hour', y='TimeFromFirstStop_p20')", "processed": ["appear thing make sen thing might depend interact featur yet look land intersect see left turn take time averag right turn cambridgesid quickest lowest variat direct turn data help model go straight land u turn highest variat look minut 5th cambria intersect look like go east cambria highest averag time whether one go straight turn left let look also see", "p 80 deep dive quick look p80 data get insight high mean wait time variat traffic head ne land"]}, {"markdown": ["# Analize the Train Data"], "code": "# Reference: https://www.kaggle.com/code/robikscube/lets-explore-the-zillow-data\n\n# Describe the data\njoined_train.describe()\nvar_counts = pd.DataFrame(joined_train.count(), columns=['Count'])\nvar_counts.loc[var_counts['Count'] > 80000].sort_values(by=['Count'], ascending = False)\nfrom sklearn import model_selection\nmain_features = ['roomcnt','bedroomcnt','bathroomcnt',\n                 'fips','landtaxvaluedollarcnt','taxvaluedollarcnt',\n                 'taxamount','regionidzip','yearbuilt','finishedsquarefeet12','lotsizesquarefeet']\noutput_vars = ['transactiondate','logerror']\ndf = joined_train.loc[:,main_features+output_vars]\ndf.describe()\nsns.pairplot(df.loc[:,['roomcnt','bedroomcnt','bathroomcnt','yearbuilt','logerror','fips']].dropna(),\n            hue='fips')", "processed": ["anal train data"]}, {"markdown": ["## <a id='32'>Train and test data</a>  \n\nLet's check the distribution of train and test features.\n\nBoth have the same features:\n* card_id;  \n* feature1, feature2, feature3;  \n* first_active_month;  \n\nTrain has also the target value, called **target**.   \n\nLet's define few auxiliary functions.\n"], "code": "# Reference: https://www.kaggle.com/code/gpreda/elo-eda-and-prediction\n\ndef get_categories(data, val):\n    tmp = data[val].value_counts()\n    return pd.DataFrame(data={'Number': tmp.values}, index=tmp.index).reset_index()\ndef get_target_categories(data, val):\n    tmp = data.groupby('target')[val].value_counts()\n    return pd.DataFrame(data={'Number': tmp.values}, index=tmp.index).reset_index()\ndef draw_trace_bar(data_df,color='Blue'):\n    trace = go.Bar(\n            x = data_df['index'],\n            y = data_df['Number'],\n            marker=dict(color=color),\n            text=data_df['index']\n        )\n    return trace\n\ndef draw_trace_histogram(data_df,target,color='Blue'):\n    trace = go.Histogram(\n            y = data_df[target],\n            marker=dict(color=color)\n        )\n    return trace\ndef plot_bar(data_df, title, xlab, ylab,color='Blue'):\n    trace = draw_trace_bar(data_df, color)\n    data = [trace]\n    layout = dict(title = title,\n              xaxis = dict(title = xlab, showticklabels=True, tickangle=0,\n                          tickfont=dict(\n                            size=10,\n                            color='black'),), \n              yaxis = dict(title = ylab),\n              hovermode = 'closest'\n             )\n    fig = dict(data = data, layout = layout)\n    iplot(fig, filename='draw_trace')\ndef plot_two_bar(data_df1, data_df2, title1, title2, xlab, ylab):\n    trace1 = draw_trace_bar(data_df1, color='Blue')\n    trace2 = draw_trace_bar(data_df2, color='Lightblue')\n    \n    fig = tools.make_subplots(rows=1,cols=2, subplot_titles=(title1,title2))\n    fig.append_trace(trace1,1,1)\n    fig.append_trace(trace2,1,2)\n    \n    fig['layout']['xaxis'].update(title = xlab)\n    fig['layout']['xaxis2'].update(title = xlab)\n    fig['layout']['yaxis'].update(title = ylab)\n    fig['layout']['yaxis2'].update(title = ylab)\n    fig['layout'].update(showlegend=False)\n    \n    iplot(fig, filename='draw_trace')\ndef plot_target_distribution(var):\n    hist_data = []\n    varall = list(train_df.groupby([var])[var].nunique().index)\n    for i, varcrt in enumerate(varall):\n        classcrt = train_df[train_df[var] == varcrt]['target']\n        hist_data.append(classcrt)\n    fig = ff.create_distplot(hist_data, varall, show_hist=False, show_rug=False)\n    fig['layout'].update(title='Target variable density plot group by {}'.format(var), xaxis=dict(title='Target'))\n    iplot(fig, filename='dist_only')", "processed": ["id 32 train test data let check distribut train test featur featur card id feature1 feature2 feature3 first activ month train also target valu call target let defin auxiliari function"]}, {"markdown": ["Let's show the purchase amount grouped by purchase time types.\n\nBefore this, let's extract the date."], "code": "# Reference: https://www.kaggle.com/code/gpreda/elo-eda-and-prediction\n\nhistorical_trans_df['purchase_date'] = pd.to_datetime(historical_trans_df['purchase_date'])\nhistorical_trans_df['month'] = historical_trans_df['purchase_date'].dt.month\nhistorical_trans_df['dayofweek'] = historical_trans_df['purchase_date'].dt.dayofweek\nhistorical_trans_df['weekofyear'] = historical_trans_df['purchase_date'].dt.weekofyear\ndef plot_scatter_data(data, xtitle, ytitle, title, color='blue'):\n    trace = go.Scatter(\n        x = data.index,\n        y = data.values,\n        name=ytitle,\n        marker=dict(\n            color=color,\n        ),\n        mode='lines+markers'\n    )\n    data = [trace]\n    layout = dict(title = title,\n              xaxis = dict(title = xtitle), yaxis = dict(title = ytitle),\n             )\n    fig = dict(data=data, layout=layout)\n    iplot(fig, filename='lines')", "processed": ["let show purchas amount group purchas time type let extract date"]}, {"markdown": ["Let's check the distribution of the purchase amount grouped by various features. We will represent  log(purchase_amount + 1)."], "code": "# Reference: https://www.kaggle.com/code/gpreda/elo-eda-and-prediction\n\ndef plot_purchase_amount_distribution(data_df, var):\n    hist_data = []\n    varall = list(data_df.groupby([var])[var].nunique().index)\n    for i, varcrt in enumerate(varall):\n        classcrt = np.log(data_df[data_df[var] == varcrt]['purchase_amount'] + 1)\n        hist_data.append(classcrt)\n    fig = ff.create_distplot(hist_data, varall, show_hist=False, show_rug=False)\n    fig['layout'].update(title='Purchase amount (log) variable density plot group by {}'.format(var), xaxis=dict(title='log(purchase_amount + 1)'))\n    iplot(fig, filename='dist_only')\nplot_purchase_amount_distribution(new_merchant_trans_df,'category_1')\nplot_purchase_amount_distribution(new_merchant_trans_df,'category_2')\nplot_purchase_amount_distribution(new_merchant_trans_df,'category_3')\nplot_purchase_amount_distribution(new_merchant_trans_df,'state_id')", "processed": ["let check distribut purchas amount group variou featur repres log purchas amount 1"]}, {"markdown": ["Let's plot distribution of **numerical_1**, **numerical_2**, **avg_sales_lag3**, **avg_sales_lag6**, **avg_sales_lag12**."], "code": "# Reference: https://www.kaggle.com/code/gpreda/elo-eda-and-prediction\n\ndef plot_distribution(df,feature,color):\n    fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12,6))\n    s = sns.boxplot(ax = ax1, data = df[feature].dropna(),color=color,showfliers=True)\n    s.set_title(\"Distribution of %s (with outliers)\" % feature)\n    s = sns.boxplot(ax = ax2, data = df[feature].dropna(),color=color,showfliers=False)\n    s.set_title(\"Distribution of %s (no outliers)\" % feature)\n    plt.show()   \nplot_distribution(merchant_df, \"numerical_1\", \"blue\")\nplot_distribution(merchant_df, \"numerical_2\", \"green\")\nplot_distribution(merchant_df, \"avg_sales_lag3\", \"blue\")\nplot_distribution(merchant_df, \"avg_sales_lag6\", \"green\")\nplot_distribution(merchant_df, \"avg_sales_lag12\", \"green\")", "processed": ["let plot distribut numer 1 numer 2 avg sale lag3 avg sale lag6 avg sale lag12"]}, {"markdown": ["## Feature importance"], "code": "# Reference: https://www.kaggle.com/code/gpreda/elo-eda-and-prediction\n\n##plot the feature importance\nlogger.info(\"Feature importance plot\")\ncols = (feature_importance_df[[\"Feature\", \"importance\"]]\n        .groupby(\"Feature\")\n        .mean()\n        .sort_values(by=\"importance\", ascending=False)[:1000].index)\nbest_features = feature_importance_df.loc[feature_importance_df.Feature.isin(cols)]\nplt.figure(figsize=(14,26))\nsns.barplot(x=\"importance\", y=\"Feature\", data=best_features.sort_values(by=\"importance\",ascending=False))\nplt.title('LightGBM Features (averaged over folds)')\nplt.tight_layout()\nplt.savefig('lgbm_importances.png')", "processed": ["featur import"]}, {"markdown": ["We can join our regular season results on the team names to more clearly identify the games."], "code": "# Reference: https://www.kaggle.com/code/robikscube/2020-march-madness-data-first-look-eda\n\n# Lets Add the winning and losing team names to the results\nMRegularSeasonCompactResults = \\\n    MRegularSeasonCompactResults \\\n    .merge(MTeams[['TeamName', 'TeamID']],\n           left_on='WTeamID',\n           right_on='TeamID',\n           validate='many_to_one') \\\n    .drop('TeamID', axis=1) \\\n    .rename(columns={'TeamName': 'WTeamName'}) \\\n    .merge(MTeams[['TeamName', 'TeamID']],\n           left_on='LTeamID',\n           right_on='TeamID') \\\n    .drop('TeamID', axis=1) \\\n    .rename(columns={'TeamName': 'LTeamName'})\n\nWRegularSeasonCompactResults = \\\n    WRegularSeasonCompactResults \\\n    .merge(WTeams[['TeamName', 'TeamID']],\n           left_on='WTeamID',\n           right_on='TeamID',\n           validate='many_to_one') \\\n    .drop('TeamID', axis=1) \\\n    .rename(columns={'TeamName': 'WTeamName'}) \\\n    .merge(WTeams[['TeamName', 'TeamID']],\n           left_on='LTeamID',\n           right_on='TeamID') \\\n    .drop('TeamID', axis=1) \\\n    .rename(columns={'TeamName': 'LTeamName'})\nWRegularSeasonCompactResults.head()\nWRegularSeasonCompactResults['Score_Diff'] = WRegularSeasonCompactResults['WScore'] - WRegularSeasonCompactResults['LScore']\nMRegularSeasonCompactResults['Score_Diff'] = MRegularSeasonCompactResults['WScore'] - MRegularSeasonCompactResults['LScore']\nplt.style.use('fivethirtyeight')\nMRegularSeasonCompactResults['Score_Diff'] \\\n    .plot(kind='hist',\n          bins=90,\n          figsize=(15, 5),\n          label='Mens',\n          alpha=0.5)\nWRegularSeasonCompactResults['Score_Diff'] \\\n    .plot(kind='hist',\n          bins=105,\n          figsize=(15, 5),\n          label='Womens',\n          alpha=0.5)\nplt.title('Score Differential')\nplt.xlim(0,60)\nplt.legend()\nplt.show()\nplt.style.use('fivethirtyeight')\nfig, axs = plt.subplots(1, 2, figsize=(15, 5))\nWRegularSeasonCompactResults['counter'] = 1\nWRegularSeasonCompactResults.groupby('WTeamName')['counter'] \\\n    .count() \\\n    .sort_values() \\\n    .tail(20) \\\n    .plot(kind='barh',\n          title='\u2b06\ufe0f Most Regular Season Wins (Womens)',\n          figsize=(15, 8),\n          xlim=(400, 680),\n          color=mypal[0],\n          ax=axs[0])\nWRegularSeasonCompactResults.groupby('WTeamName')['counter'] \\\n    .count() \\\n    .sort_values(ascending=False) \\\n    .tail(20) \\\n    .plot(kind='barh',\n          title='\u2b07\ufe0f Least Regular Season Wins (Womens)',\n          figsize=(15, 8),\n          xlim=(0, 150),\n          color=mypal[1],\n          ax=axs[1])\nplt.tight_layout()\nplt.show()\n\nplt.style.use('fivethirtyeight')\nfig, axs = plt.subplots(1, 2, figsize=(15, 5))\nMRegularSeasonCompactResults['counter'] = 1\nMRegularSeasonCompactResults.groupby('WTeamName')['counter'] \\\n    .count() \\\n    .sort_values() \\\n    .tail(20) \\\n    .plot(kind='barh',\n          title='\u2b06\ufe0f Most Regular Season Wins (Mens)',\n          figsize=(15, 8),\n          xlim=(600, 920),\n          color=mypal[2],\n         ax=axs[0])\nMRegularSeasonCompactResults.groupby('WTeamName')['counter'] \\\n    .count() \\\n    .sort_values(ascending=False) \\\n    .tail(20) \\\n    .plot(kind='barh',\n          title='\u2b07\ufe0f Least Regular Season Wins (Mens)',\n          figsize=(15, 8),\n          xlim=(0, 150),\n          color=mypal[3],\n          ax=axs[1])\naxs[1].set_ylabel('')\nplt.tight_layout()\nplt.show()\n", "processed": ["join regular season result team name clearli identifi game"]}, {"markdown": ["## Common Event Types"], "code": "# Reference: https://www.kaggle.com/code/robikscube/2020-march-madness-data-first-look-eda\n\n# Event Types\nplt.style.use('fivethirtyeight')\nMEvents['counter'] = 1\nMEvents.groupby('EventType')['counter'] \\\n    .sum() \\\n    .sort_values(ascending=False) \\\n    .plot(kind='bar',\n          figsize=(15, 5),\n         color=mypal[2],\n         title='Event Type Frequency (Mens)')\nplt.xticks(rotation=0)\nplt.show()\n# Event Types\nplt.style.use('fivethirtyeight')\nWEvents['counter'] = 1\nWEvents.groupby('EventType')['counter'] \\\n    .sum() \\\n    .sort_values(ascending=False) \\\n    .plot(kind='bar',\n          figsize=(15, 5),\n         color=mypal[3],\n         title='Event Type Frequency (Womens)')\nplt.xticks(rotation=0)\nplt.show()", "processed": ["common event type"]}, {"markdown": ["# Area of Event\nWe are told that the `Area` feature describes the 13 \"areas\" of the court, as follows: 1=under basket; 2=in the paint; 3=inside right wing; 4=inside right; 5=inside center; 6=inside left; 7=inside left wing; 8=outside right wing; 9=outside right; 10=outside center; 11=outside left; 12=outside left wing; 13=backcourt.\n\nWe can map these values to their names."], "code": "# Reference: https://www.kaggle.com/code/robikscube/2020-march-madness-data-first-look-eda\n\narea_mapping = {0: np.nan,\n                1: 'under basket',\n                2: 'in the paint',\n                3: 'inside right wing',\n                4: 'inside right',\n                5: 'inside center',\n                6: 'inside left',\n                7: 'inside left wing',\n                8: 'outside right wing',\n                9: 'outside right',\n                10: 'outside center',\n                11: 'outside left',\n                12: 'outside left wing',\n                13: 'backcourt'}\n\nMEvents['Area_Name'] = MEvents['Area'].map(area_mapping)\nMEvents.groupby('Area_Name')['counter'].sum() \\\n    .sort_values() \\\n    .plot(kind='barh',\n          figsize=(15, 8),\n          title='Frequency of Event Area')\nplt.show()\nfig, ax = plt.subplots(figsize=(15, 8))\nfor i, d in MEvents.loc[~MEvents['Area_Name'].isna()].groupby('Area_Name'):\n    d.plot(x='X', y='Y', style='.', label=i, ax=ax, title='Visualizing Event Areas')\n    ax.legend()\nplt.legend(bbox_to_anchor=(1.04,1), loc=\"upper left\")\nax.set_xticks([])\nax.set_yticks([])\nax.set_xlabel('')\nax.set_xlim(0, 100)\nax.set_ylim(0, 100)\nplt.show()", "processed": ["area event told area featur describ 13 area court follow 1 basket 2 paint 3 insid right wing 4 insid right 5 insid center 6 insid left 7 insid left wing 8 outsid right wing 9 outsid right 10 outsid center 11 outsid left 12 outsid left wing 13 backcourt map valu name"]}, {"markdown": ["# NCAA Court Plot Function\nCheck out my notebook here for an example and code for a half court plot:\n\nhttps://www.kaggle.com/robikscube/ncaa-basketball-court-plot-helper-functions"], "code": "# Reference: https://www.kaggle.com/code/robikscube/2020-march-madness-data-first-look-eda\n\ndef create_ncaa_full_court(ax=None, three_line='mens', court_color='#dfbb85',\n                           lw=3, lines_color='black', lines_alpha=0.5,\n                           paint_fill='blue', paint_alpha=0.4,\n                           inner_arc=False):\n    \"\"\"\n    Version 2020.2.19\n    Creates NCAA Basketball Court\n    Dimensions are in feet (Court is 97x50 ft)\n    Created by: Rob Mulla / https://github.com/RobMulla\n\n    * Note that this function uses \"feet\" as the unit of measure.\n    * NCAA Data is provided on a x range: 0, 100 and y-range 0 to 100\n    * To plot X/Y positions first convert to feet like this:\n    ```\n    Events['X_'] = (Events['X'] * (94/100))\n    Events['Y_'] = (Events['Y'] * (50/100))\n    ```\n    \n    ax: matplotlib axes if None gets current axes using `plt.gca`\n\n\n    three_line: 'mens', 'womens' or 'both' defines 3 point line plotted\n    court_color : (hex) Color of the court\n    lw : line width\n    lines_color : Color of the lines\n    lines_alpha : transparency of lines\n    paint_fill : Color inside the paint\n    paint_alpha : transparency of the \"paint\"\n    inner_arc : paint the dotted inner arc\n    \"\"\"\n    if ax is None:\n        ax = plt.gca()\n\n    # Create Pathes for Court Lines\n    center_circle = Circle((94/2, 50/2), 6,\n                           linewidth=lw, color=lines_color, lw=lw,\n                           fill=False, alpha=lines_alpha)\n    hoop_left = Circle((5.25, 50/2), 1.5 / 2,\n                       linewidth=lw, color=lines_color, lw=lw,\n                       fill=False, alpha=lines_alpha)\n    hoop_right = Circle((94-5.25, 50/2), 1.5 / 2,\n                        linewidth=lw, color=lines_color, lw=lw,\n                        fill=False, alpha=lines_alpha)\n\n    # Paint - 18 Feet 10 inches which converts to 18.833333 feet - gross!\n    left_paint = Rectangle((0, (50/2)-6), 18.833333, 12,\n                           fill=paint_fill, alpha=paint_alpha,\n                           lw=lw, edgecolor=None)\n    right_paint = Rectangle((94-18.83333, (50/2)-6), 18.833333,\n                            12, fill=paint_fill, alpha=paint_alpha,\n                            lw=lw, edgecolor=None)\n    \n    left_paint_boarder = Rectangle((0, (50/2)-6), 18.833333, 12,\n                           fill=False, alpha=lines_alpha,\n                           lw=lw, edgecolor=lines_color)\n    right_paint_boarder = Rectangle((94-18.83333, (50/2)-6), 18.833333,\n                            12, fill=False, alpha=lines_alpha,\n                            lw=lw, edgecolor=lines_color)\n\n    left_arc = Arc((18.833333, 50/2), 12, 12, theta1=-\n                   90, theta2=90, color=lines_color, lw=lw,\n                   alpha=lines_alpha)\n    right_arc = Arc((94-18.833333, 50/2), 12, 12, theta1=90,\n                    theta2=-90, color=lines_color, lw=lw,\n                    alpha=lines_alpha)\n    \n    leftblock1 = Rectangle((7, (50/2)-6-0.666), 1, 0.666,\n                           fill=True, alpha=lines_alpha,\n                           lw=0, edgecolor=lines_color,\n                           facecolor=lines_color)\n    leftblock2 = Rectangle((7, (50/2)+6), 1, 0.666,\n                           fill=True, alpha=lines_alpha,\n                           lw=0, edgecolor=lines_color,\n                           facecolor=lines_color)\n    ax.add_patch(leftblock1)\n    ax.add_patch(leftblock2)\n    \n    left_l1 = Rectangle((11, (50/2)-6-0.666), 0.166, 0.666,\n                           fill=True, alpha=lines_alpha,\n                           lw=0, edgecolor=lines_color,\n                           facecolor=lines_color)\n    left_l2 = Rectangle((14, (50/2)-6-0.666), 0.166, 0.666,\n                           fill=True, alpha=lines_alpha,\n                           lw=0, edgecolor=lines_color,\n                           facecolor=lines_color)\n    left_l3 = Rectangle((17, (50/2)-6-0.666), 0.166, 0.666,\n                           fill=True, alpha=lines_alpha,\n                           lw=0, edgecolor=lines_color,\n                           facecolor=lines_color)\n    ax.add_patch(left_l1)\n    ax.add_patch(left_l2)\n    ax.add_patch(left_l3)\n    left_l4 = Rectangle((11, (50/2)+6), 0.166, 0.666,\n                           fill=True, alpha=lines_alpha,\n                           lw=0, edgecolor=lines_color,\n                           facecolor=lines_color)\n    left_l5 = Rectangle((14, (50/2)+6), 0.166, 0.666,\n                           fill=True, alpha=lines_alpha,\n                           lw=0, edgecolor=lines_color,\n                           facecolor=lines_color)\n    left_l6 = Rectangle((17, (50/2)+6), 0.166, 0.666,\n                           fill=True, alpha=lines_alpha,\n                           lw=0, edgecolor=lines_color,\n                           facecolor=lines_color)\n    ax.add_patch(left_l4)\n    ax.add_patch(left_l5)\n    ax.add_patch(left_l6)\n    \n    rightblock1 = Rectangle((94-7-1, (50/2)-6-0.666), 1, 0.666,\n                           fill=True, alpha=lines_alpha,\n                           lw=0, edgecolor=lines_color,\n                           facecolor=lines_color)\n    rightblock2 = Rectangle((94-7-1, (50/2)+6), 1, 0.666,\n                           fill=True, alpha=lines_alpha,\n                           lw=0, edgecolor=lines_color,\n                           facecolor=lines_color)\n    ax.add_patch(rightblock1)\n    ax.add_patch(rightblock2)\n\n    right_l1 = Rectangle((94-11, (50/2)-6-0.666), 0.166, 0.666,\n                           fill=True, alpha=lines_alpha,\n                           lw=0, edgecolor=lines_color,\n                           facecolor=lines_color)\n    right_l2 = Rectangle((94-14, (50/2)-6-0.666), 0.166, 0.666,\n                           fill=True, alpha=lines_alpha,\n                           lw=0, edgecolor=lines_color,\n                           facecolor=lines_color)\n    right_l3 = Rectangle((94-17, (50/2)-6-0.666), 0.166, 0.666,\n                           fill=True, alpha=lines_alpha,\n                           lw=0, edgecolor=lines_color,\n                           facecolor=lines_color)\n    ax.add_patch(right_l1)\n    ax.add_patch(right_l2)\n    ax.add_patch(right_l3)\n    right_l4 = Rectangle((94-11, (50/2)+6), 0.166, 0.666,\n                           fill=True, alpha=lines_alpha,\n                           lw=0, edgecolor=lines_color,\n                           facecolor=lines_color)\n    right_l5 = Rectangle((94-14, (50/2)+6), 0.166, 0.666,\n                           fill=True, alpha=lines_alpha,\n                           lw=0, edgecolor=lines_color,\n                           facecolor=lines_color)\n    right_l6 = Rectangle((94-17, (50/2)+6), 0.166, 0.666,\n                           fill=True, alpha=lines_alpha,\n                           lw=0, edgecolor=lines_color,\n                           facecolor=lines_color)\n    ax.add_patch(right_l4)\n    ax.add_patch(right_l5)\n    ax.add_patch(right_l6)\n    \n    # 3 Point Line\n    if (three_line == 'mens') | (three_line == 'both'):\n        # 22' 1.75\" distance to center of hoop\n        three_pt_left = Arc((6.25, 50/2), 44.291, 44.291, theta1=-78,\n                            theta2=78, color=lines_color, lw=lw,\n                            alpha=lines_alpha)\n        three_pt_right = Arc((94-6.25, 50/2), 44.291, 44.291,\n                             theta1=180-78, theta2=180+78,\n                             color=lines_color, lw=lw, alpha=lines_alpha)\n\n        # 4.25 feet max to sideline for mens\n        ax.plot((0, 11.25), (3.34, 3.34),\n                color=lines_color, lw=lw, alpha=lines_alpha)\n        ax.plot((0, 11.25), (50-3.34, 50-3.34),\n                color=lines_color, lw=lw, alpha=lines_alpha)\n        ax.plot((94-11.25, 94), (3.34, 3.34),\n                color=lines_color, lw=lw, alpha=lines_alpha)\n        ax.plot((94-11.25, 94), (50-3.34, 50-3.34),\n                color=lines_color, lw=lw, alpha=lines_alpha)\n        ax.add_patch(three_pt_left)\n        ax.add_patch(three_pt_right)\n\n    if (three_line == 'womens') | (three_line == 'both'):\n        # womens 3\n        three_pt_left_w = Arc((6.25, 50/2), 20.75 * 2, 20.75 * 2, theta1=-85,\n                              theta2=85, color=lines_color, lw=lw, alpha=lines_alpha)\n        three_pt_right_w = Arc((94-6.25, 50/2), 20.75 * 2, 20.75 * 2,\n                               theta1=180-85, theta2=180+85,\n                               color=lines_color, lw=lw, alpha=lines_alpha)\n\n        # 4.25 inches max to sideline for mens\n        ax.plot((0, 8.3), (4.25, 4.25), color=lines_color,\n                lw=lw, alpha=lines_alpha)\n        ax.plot((0, 8.3), (50-4.25, 50-4.25),\n                color=lines_color, lw=lw, alpha=lines_alpha)\n        ax.plot((94-8.3, 94), (4.25, 4.25),\n                color=lines_color, lw=lw, alpha=lines_alpha)\n        ax.plot((94-8.3, 94), (50-4.25, 50-4.25),\n                color=lines_color, lw=lw, alpha=lines_alpha)\n\n        ax.add_patch(three_pt_left_w)\n        ax.add_patch(three_pt_right_w)\n\n    # Add Patches\n    ax.add_patch(left_paint)\n    ax.add_patch(left_paint_boarder)\n    ax.add_patch(right_paint)\n    ax.add_patch(right_paint_boarder)\n    ax.add_patch(center_circle)\n    ax.add_patch(hoop_left)\n    ax.add_patch(hoop_right)\n    ax.add_patch(left_arc)\n    ax.add_patch(right_arc)\n    \n    if inner_arc:\n        left_inner_arc = Arc((18.833333, 50/2), 12, 12, theta1=90,\n                             theta2=-90, color=lines_color, lw=lw,\n                       alpha=lines_alpha, ls='--')\n        right_inner_arc = Arc((94-18.833333, 50/2), 12, 12, theta1=-90,\n                        theta2=90, color=lines_color, lw=lw,\n                        alpha=lines_alpha, ls='--')\n        ax.add_patch(left_inner_arc)\n        ax.add_patch(right_inner_arc)\n\n    # Restricted Area Marker\n    restricted_left = Arc((6.25, 50/2), 8, 8, theta1=-90,\n                        theta2=90, color=lines_color, lw=lw,\n                        alpha=lines_alpha)\n    restricted_right = Arc((94-6.25, 50/2), 8, 8,\n                         theta1=180-90, theta2=180+90,\n                         color=lines_color, lw=lw, alpha=lines_alpha)\n    ax.add_patch(restricted_left)\n    ax.add_patch(restricted_right)\n    \n    # Backboards\n    ax.plot((4, 4), ((50/2) - 3, (50/2) + 3),\n            color=lines_color, lw=lw*1.5, alpha=lines_alpha)\n    ax.plot((94-4, 94-4), ((50/2) - 3, (50/2) + 3),\n            color=lines_color, lw=lw*1.5, alpha=lines_alpha)\n    ax.plot((4, 4.6), (50/2, 50/2), color=lines_color,\n            lw=lw, alpha=lines_alpha)\n    ax.plot((94-4, 94-4.6), (50/2, 50/2),\n            color=lines_color, lw=lw, alpha=lines_alpha)\n\n    # Half Court Line\n    ax.axvline(94/2, color=lines_color, lw=lw, alpha=lines_alpha)\n\n    # Boarder\n    boarder = Rectangle((0.3,0.3), 94-0.4, 50-0.4, fill=False, lw=3, color='black', alpha=lines_alpha)\n    ax.add_patch(boarder)\n    \n    # Plot Limit\n    ax.set_xlim(0, 94)\n    ax.set_ylim(0, 50)\n    ax.set_facecolor(court_color)\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.set_xlabel('')\n    return ax\n\n\nfig, ax = plt.subplots(figsize=(15, 8.5))\ncreate_ncaa_full_court(ax, three_line='both', paint_alpha=0.4)\nplt.show()\n\nfig, ax = plt.subplots(figsize=(15, 7.8))\nms = 10\nax = create_ncaa_full_court(ax, paint_alpha=0.1)\nMEvents.query('EventType == \"turnover\"') \\\n    .plot(x='X_', y='Y_', style='X',\n          title='Turnover Locations (Mens)',\n          c='red',\n          alpha=0.3,\n         figsize=(15, 9),\n         label='Steals',\n         ms=ms,\n         ax=ax)\nax.set_xlabel('')\nax.get_legend().remove()\nplt.show()\nCOURT_COLOR = '#dfbb85'\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 4))\n# Where are 3 pointers made from? (This is really cool)\nWEvents.query('EventType == \"made3\"') \\\n    .plot(x='X_', y='Y_', style='.',\n          color='blue',\n          title='3 Pointers Made (Womens)',\n          alpha=0.01, ax=ax1)\nax1 = create_ncaa_full_court(ax1, lw=0.5, three_line='womens', paint_alpha=0.1)\nax1.set_facecolor(COURT_COLOR)\nWEvents.query('EventType == \"miss3\"') \\\n    .plot(x='X_', y='Y_', style='.',\n          title='3 Pointers Missed (Womens)',\n          color='red',\n          alpha=0.01, ax=ax2)\nax2.set_facecolor(COURT_COLOR)\nax2 = create_ncaa_full_court(ax2, lw=0.5, three_line='womens', paint_alpha=0.1)\nax1.get_legend().remove()\nax2.get_legend().remove()\nax1.set_xticks([])\nax1.set_yticks([])\nax2.set_xticks([])\nax2.set_yticks([])\nax1.set_xlabel('')\nax2.set_xlabel('')\nplt.show()\nCOURT_COLOR = '#dfbb85'\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 4))\n# Where are 3 pointers made from? (This is really cool)\nWEvents.query('EventType == \"made2\"') \\\n    .plot(x='X_', y='Y_', style='.',\n          color='blue',\n          title='2 Pointers Made (Womens)',\n          alpha=0.01, ax=ax1)\nax1.set_facecolor(COURT_COLOR)\nax1 = create_ncaa_full_court(ax1, lw=0.5, three_line='womens', paint_alpha=0.1)\nWEvents.query('EventType == \"miss2\"') \\\n    .plot(x='X_', y='Y_', style='.',\n          title='2 Pointers Missed (Womens)',\n          color='red',\n          alpha=0.01, ax=ax2)\nax2.set_facecolor(COURT_COLOR)\nax2 = create_ncaa_full_court(ax2, lw=0.5, three_line='womens', paint_alpha=0.1)\nax1.get_legend().remove()\nax2.get_legend().remove()\nax1.set_xticks([])\nax1.set_yticks([])\nax2.set_xticks([])\nax2.set_yticks([])\nax1.set_xlabel('')\nax2.set_xlabel('')\nplt.show()", "processed": ["ncaa court plot function check notebook exampl code half court plot http www kaggl com robikscub ncaa basketbal court plot helper function"]}, {"markdown": ["# Common Events by Player"], "code": "# Reference: https://www.kaggle.com/code/robikscube/2020-march-madness-data-first-look-eda\n\nMEvents.loc[MEvents['PlayerID'] == 2825].groupby('EventType')['EventID'].count() \\\n    .sort_values() \\\n    .plot(kind='barh',\n          figsize=(15, 5),\n          title='Zion Williamson event type count',\n          color=mypal[1])\nplt.show()", "processed": ["common event player"]}, {"markdown": ["# Plotting Specific Players' Made/Missed Shots\nNow that we have player names in the event data, lets single out specific players. Starting with one of the most exciting players of the last decade.\n\n![](https://thenypost.files.wordpress.com/2018/11/zion-williamson-duke-freshman-scouting-comparables.jpg?quality=80&strip=all&w=618&h=410&crop=1)"], "code": "# Reference: https://www.kaggle.com/code/robikscube/2020-march-madness-data-first-look-eda\n\nms = 10 # Marker Size\nFirstName = 'Zion'\nLastName = 'Williamson'\nfig, ax = plt.subplots(figsize=(15, 8))\nax = create_ncaa_full_court(ax)\nMEvents.query('FirstName == @FirstName and LastName == @LastName and EventType == \"made2\"') \\\n    .plot(x='X_', y='Y_', style='o',\n          title='Shots (Zion Williamson)',\n          alpha=0.5,\n         figsize=(15, 8),\n         label='Made 2',\n         ms=ms,\n         ax=ax)\nplt.legend()\nMEvents.query('FirstName == @FirstName and LastName == @LastName and EventType == \"miss2\"') \\\n    .plot(x='X_', y='Y_', style='X',\n          alpha=0.5, ax=ax,\n         label='Missed 2',\n         ms=ms)\nplt.legend()\nMEvents.query('FirstName == @FirstName and LastName == @LastName and EventType == \"made3\"') \\\n    .plot(x='X_', y='Y_', style='o',\n          c='brown',\n          alpha=0.5,\n         figsize=(15, 8),\n         label='Made 3', ax=ax,\n         ms=ms)\nplt.legend()\nMEvents.query('FirstName == @FirstName and LastName == @LastName and EventType == \"miss3\"') \\\n    .plot(x='X_', y='Y_', style='X',\n          c='green',\n          alpha=0.5, ax=ax,\n         label='Missed 3',\n         ms=ms)\nax.set_xlabel('')\nplt.legend()\nplt.show()", "processed": ["plot specif player made miss shot player name event data let singl specif player start one excit player last decad http thenypost file wordpress com 2018 11 zion williamson duke freshman scout compar jpg qualiti 80 strip w 618 h 410 crop 1"]}, {"markdown": ["Next lets look at Katie Lou Samuelson. She is known to be a 3-point shooter. As such, we can see her shots mostly come from outside the 3-point line.\n\n![](https://imagesvc.timeincapp.com/v3/fan/image?url=https://highposthoops.com/wp-content/uploads/getty-images/2018/10/951142340.jpeg?&w=618&h=410&crop=1)"], "code": "# Reference: https://www.kaggle.com/code/robikscube/2020-march-madness-data-first-look-eda\n\nms = 10 # Marker Size\nFirstName = 'Katie Lou'\nLastName = 'Samuelson'\nfig, ax = plt.subplots(figsize=(15, 8))\nax = create_ncaa_full_court(ax, three_line='womens')\nWEvents.query('FirstName == @FirstName and LastName == @LastName and EventType == \"made2\"') \\\n    .plot(x='X_', y='Y_', style='o',\n          title='Shots (Katie Lou Samuelson)',\n          alpha=0.5,\n         figsize=(15, 8),\n         label='Made 2',\n         ms=ms,\n         ax=ax)\nplt.legend()\nWEvents.query('FirstName == @FirstName and LastName == @LastName and EventType == \"miss2\"') \\\n    .plot(x='X_', y='Y_', style='X',\n          alpha=0.5, ax=ax,\n         label='Missed 2',\n         ms=ms)\nplt.legend()\nWEvents.query('FirstName == @FirstName and LastName == @LastName and EventType == \"made3\"') \\\n    .plot(x='X_', y='Y_', style='o',\n          c='brown',\n          alpha=0.5,\n         figsize=(15, 8),\n         label='Made 3', ax=ax,\n         ms=ms)\nplt.legend()\nWEvents.query('FirstName == @FirstName and LastName == @LastName and EventType == \"miss3\"') \\\n    .plot(x='X_', y='Y_', style='X',\n          c='green',\n          alpha=0.5, ax=ax,\n         label='Missed 3',\n         ms=ms)\nax.set_xlabel('')\nplt.legend()\nplt.show()", "processed": ["next let look kati lou samuelson known 3 point shooter see shot mostli come outsid 3 point line http imagesvc timeincapp com v3 fan imag url http highposthoop com wp content upload getti imag 2018 10 951142340 jpeg w 618 h 410 crop 1"]}, {"markdown": ["# Shot Heatmap\nWe can plot a heatmap of where shots occur on the court. Interesting observation when comparing the mens to womens game is that many of the shots for mens come from directly under the hoop, while the hot spots for women shots come more frequently from the left and right of the hoop."], "code": "# Reference: https://www.kaggle.com/code/robikscube/2020-march-madness-data-first-look-eda\n\nN_bins = 100\nshot_events = MEvents.loc[MEvents['EventType'].isin(['miss3','made3','miss2','made2']) & (MEvents['X_'] != 0)]\nfig, ax = plt.subplots(figsize=(15, 7))\nax = create_ncaa_full_court(ax,\n                            paint_alpha=0.0,\n                            three_line='mens',\n                            court_color='black',\n                            lines_color='white')\n_ = plt.hist2d(shot_events['X_'].values + np.random.normal(0, 0.1, shot_events['X_'].shape), # Add Jitter to values for plotting\n           shot_events['Y_'].values + np.random.normal(0, 0.1, shot_events['Y_'].shape),\n           bins=N_bins, norm=mpl.colors.LogNorm(),\n               cmap='plasma')\n\n# Plot a colorbar with label.\ncb = plt.colorbar()\ncb.set_label('Number of shots')\n\nax.set_title('Shot Heatmap (Mens)')\nplt.show()\nN_bins = 100\nshot_events = WEvents.loc[WEvents['EventType'].isin(['miss3','made3','miss2','made2']) & (WEvents['X_'] != 0)]\nfig, ax = plt.subplots(figsize=(15, 7))\nax = create_ncaa_full_court(ax, three_line='womens', paint_alpha=0.0,\n                            court_color='black',\n                            lines_color='white')\n_ = plt.hist2d(shot_events['X_'].values + np.random.normal(0, 0.2, shot_events['X_'].shape),\n           shot_events['Y_'].values + np.random.normal(0, 0.2, shot_events['Y_'].shape),\n           bins=N_bins, norm=mpl.colors.LogNorm(),\n               cmap='plasma')\n\n# Plot a colorbar with label.\ncb = plt.colorbar()\ncb.set_label('Number of shots')\n\nax.set_title('Shot Heatmap (Womens)')\nplt.show()\nMEvents['PointsScored'] =  0\nMEvents.loc[MEvents['EventType'] == 'made2', 'PointsScored'] = 2\nMEvents.loc[MEvents['EventType'] == 'made3', 'PointsScored'] = 3\nMEvents.loc[MEvents['EventType'] == 'missed2', 'PointsScored'] = 0\nMEvents.loc[MEvents['EventType'] == 'missed3', 'PointsScored'] = 0\n# # Average Points Scored per xy coord\n# avg_pnt_xy = MEvents.loc[MEvents['EventType'].isin(['miss3','made3','miss2','made2']) & (MEvents['X_'] != 0)] \\\n#     .groupby(['X_','Y_'])['PointsScored'].mean().reset_index()\n\n# # .plot(x='X_',y='Y_', style='.')\n# fig, ax = plt.subplots(figsize=(15, 8))\n# ax = sns.scatterplot(data=avg_pnt_xy, x='X_', y='Y_', hue='PointsScored', cmap='coolwarm')\n# ax = create_ncaa_full_court(ax)\n# plt.show()\n\n# avg_made_xy.sort_values('Made')\n# avg_made_xy['Made'] / avg_made_xy['Missed']\n# MEvents['Made'] = False\n# MEvents['Made'] = False\n# MEvents.loc[MEvents['EventType'] == 'made2', 'Made'] = True\n# MEvents.loc[MEvents['EventType'] == 'made3', 'Made'] = True\n# MEvents.loc[MEvents['EventType'] == 'missed2', 'Made'] = False\n# MEvents.loc[MEvents['EventType'] == 'missed3', 'Made'] = False\n# MEvents.loc[MEvents['EventType'] == 'made2', 'Missed'] = False\n# MEvents.loc[MEvents['EventType'] == 'made3', 'Missed'] = False\n# MEvents.loc[MEvents['EventType'] == 'missed2', 'Missed'] = True\n# MEvents.loc[MEvents['EventType'] == 'missed3', 'Missed'] = True\n\n# # Average Pct Made per xy coord\n# avg_made_xy = MEvents.loc[MEvents['EventType'].isin(['miss3','made3','miss2','made2']) & (MEvents['X_'] != 0)] \\\n#     .groupby(['X_','Y_'])['Made','Missed'].sum().reset_index()\n\n# # .plot(x='X_',y='Y_', style='.')\n# fig, ax = plt.subplots(figsize=(15, 8))\n# cmap = sns.cubehelix_palette(as_cmap=True)\n# ax = sns.scatterplot(data=avg_made_xy, x='X_', y='Y_', size='Made', cmap='plasma')\n# ax = create_ncaa_full_court(ax, paint_alpha=0)\n# ax.set_title('Number of Shots Made')\n# plt.show()", "processed": ["shot heatmap plot heatmap shot occur court interest observ compar men woman game mani shot men come directli hoop hot spot woman shot come frequent left right hoop"]}, {"markdown": ["## Plot each item, scroll through and visually inspect for trends"], "code": "# Reference: https://www.kaggle.com/code/robikscube/fun-eda-with-fake-names-and-lots-of-plots\n\ngrouped = train.groupby(by=['item_name'])\nfor i, d in grouped:\n    myplot = d.set_index('date').groupby('store_name')['sales'] \\\n        .plot(figsize=(15,2), style='.', title=str(i), legend=False)\n    plt.show()", "processed": ["plot item scroll visual inspect trend"]}, {"markdown": ["# Plot Year over Year"], "code": "# Reference: https://www.kaggle.com/code/robikscube/fun-eda-with-fake-names-and-lots-of-plots\n\ndef plot_year_over_year(item, store):\n    sample = train.loc[(train['store'] == store) & (train['item'] == item)].set_index('date')\n    pv = pd.pivot_table(sample, index=sample.index.month, columns=sample.index.year,\n                        values='sales', aggfunc='sum')\n    ax = pv.plot(figsize=(15,3), title=fake_store_names[store] + ' - ' + fake_items[item])\n    ax.set_xlabel(\"Month\")\nplot_year_over_year(1, 1)\nplot_year_over_year(1, 2)\nplot_year_over_year(20, 5)\nplot_year_over_year(20, 6)", "processed": ["plot year year"]}, {"markdown": ["# Plot Day of Week"], "code": "# Reference: https://www.kaggle.com/code/robikscube/fun-eda-with-fake-names-and-lots-of-plots\n\ndef plot_year_over_year_dow(item, store):\n    sample = train.loc[(train['store'] == store) & (train['item'] == item)].set_index('date')\n    pv = pd.pivot_table(sample, index=sample.index.weekday, columns=sample.index.year,\n                        values='sales', aggfunc='sum')\n    ax = pv.plot(figsize=(15,3), title=fake_store_names[store] + ' - ' + fake_items[item])\n    ax.set_xlabel(\"Day of Week\")\nplot_year_over_year_dow(1, 1)\nplot_year_over_year_dow(1, 2)\nplot_year_over_year_dow(20, 5)\nplot_year_over_year_dow(20, 6)", "processed": ["plot day week"]}, {"markdown": ["# Time Series Clustering"], "code": "# Reference: https://www.kaggle.com/code/robikscube/fun-eda-with-fake-names-and-lots-of-plots\n\n# Data prep\ntrain['store_item'] = train['store_name'] + '-' + train['item_name']\ntrain['store_item_mean'] = train.groupby('store_item')['sales'].transform('mean')\ntrain['deviation_from_storeitem_mean'] = train['sales'] - train['store_item_mean']\ntrain['dev_rolling'] = train.groupby('store_item')['deviation_from_storeitem_mean'].rolling(30).mean().reset_index()['deviation_from_storeitem_mean']\ntrain_pivoted = train.pivot(index='store_item', columns='date', values='sales')\ntrain_pivoted.head()\ndeviation_pivot = train.pivot(index='store_item', columns='date', values='dev_rolling')\ndeviation_pivot = deviation_pivot.dropna(axis=1)\ndeviation_pivot.head()\n# Example from here:\n# https://stackoverflow.com/questions/34940808/hierarchical-clustering-of-time-series-in-python-scipy-numpy-pandas\n    \nimport scipy.cluster.hierarchy as hac\nfrom scipy import stats\n# Here we use spearman correlation\ndef my_metric(x, y):\n    r = stats.pearsonr(x, y)[0]\n    return 1 - r # correlation to distance: range 0 to 2\n\nZ = hac.linkage(deviation_pivot, method='single', metric=my_metric)\nfrom scipy.cluster.hierarchy import fcluster\n\ndef print_clusters(deviation_pivot, Z, k, plot=False):\n    # k Number of clusters I'd like to extract\n    results = fcluster(Z, k, criterion='maxclust')\n\n    # check the results\n    s = pd.Series(results)\n    clusters = s.unique()\n\n    for c in clusters:\n        cluster_indeces = s[s==c].index\n        print(\"Cluster %d number of entries %d\" % (c, len(cluster_indeces)))\n        if plot:\n            deviation_pivot.T.iloc[:,cluster_indeces].plot()\n            plt.show()\n\nprint_clusters(deviation_pivot, Z, 5, plot=False)", "processed": ["time seri cluster"]}, {"markdown": ["### Visualization"], "code": "# Reference: https://www.kaggle.com/code/iafoss/unet34-dice-0-87\n\ndef Show_images(x,yp,yt):\n    columns = 3\n    rows = min(bs,8)\n    fig=plt.figure(figsize=(columns*4, rows*4))\n    for i in range(rows):\n        fig.add_subplot(rows, columns, 3*i+1)\n        plt.axis('off')\n        plt.imshow(x[i])\n        fig.add_subplot(rows, columns, 3*i+2)\n        plt.axis('off')\n        plt.imshow(yp[i])\n        fig.add_subplot(rows, columns, 3*i+3)\n        plt.axis('off')\n        plt.imshow(yt[i])\n    plt.show()\nlearn.model.eval();\nx,y = next(iter(md.val_dl))\nyp = to_np(F.sigmoid(learn.model(V(x))))\nShow_images(np.asarray(md.val_ds.denorm(x)), yp, y)", "processed": ["visual"]}, {"markdown": ["As we can see, the missing values now become much more apparent and clear when we visualise it, where the empty white bands (data that is missing) superposed on the vertical dark red bands (non-missing data) reflect the nullity of the data in that particular column. In this instance, we can observe that there are 7 features out of the 59 total features (although as rightly pointed out by Justin Nafe in the comments section there are really a grand total of 13 columns with missing values) that actually contained null values. This is due to the fact that the missingno matrix plot can only comfortable fit in approximately 40 odd features to one plot after which some columns may be excluded, and hence the remaining 5 null columns have been excluded. To visualize all nulls, try changing the figsize argument as well as tweaking how we slice the dataframe.\n\nFor the 7 null columns that we are able to observe, they are hence listed here as follows:\n\n**ps_ind_05_cat | ps_reg_03 | ps_car_03_cat | ps_car_05_cat | ps_car_07_cat | ps_car_09_cat | ps_car_14**\n\nMost of the missing values occur in the columns suffixed with _cat. One should really take further note of the columns ps_reg_03, ps_car_03_cat and ps_car_05_cat. Evinced from the ratio of white to dark bands, it is very apparent that a big majority of values are missing from these 3 columns, and therefore a blanket replacement of -1 for the nulls might not be a very good strategy.", "**Target variable inspection**\n\nAnother standard check normally conducted on the data is with regards to our target variable, where in this case, the column is conveniently titled \"target\". The target value also comes by the moniker of class/label/correct answer and is used in supervised learning models along with the corresponding data that is given (in our case all our train data except the id column) to learn the function that best maps the data to our target in the hope that this learned function can generalize and predict well with new unseen data."], "code": "# Reference: https://www.kaggle.com/code/arthurtok/interactive-porto-insights-a-plot-ly-tutorial\n\ndata = [go.Bar(\n            x = train[\"target\"].value_counts().index.values,\n            y = train[\"target\"].value_counts().values,\n            text='Distribution of target variable'\n    )]\n\nlayout = go.Layout(\n    title='Target variable distribution'\n)\n\nfig = go.Figure(data=data, layout=layout)\n\npy.iplot(fig, filename='basic-bar')", "processed": ["see miss valu becom much appar clear visualis empti white band data miss superpos vertic dark red band non miss data reflect nulliti data particular column instanc observ 7 featur 59 total featur although rightli point justin nafe comment section realli grand total 13 column miss valu actual contain null valu due fact missingno matrix plot comfort fit approxim 40 odd featur one plot column may exclud henc remain 5 null column exclud visual null tri chang figsiz argument well tweak slice datafram 7 null column abl observ henc list follow p ind 05 cat p reg 03 p car 03 cat p car 05 cat p car 07 cat p car 09 cat p car 14 miss valu occur column suffix cat one realli take note column p reg 03 p car 03 cat p car 05 cat evinc ratio white dark band appar big major valu miss 3 column therefor blanket replac 1 null might good strategi", "target variabl inspect anoth standard check normal conduct data regard target variabl case column conveni titl target target valu also come monik class label correct answer use supervis learn model along correspond data given case train data except id column learn function best map data target hope learn function gener predict well new unseen data"]}, {"markdown": ["From the correlation plot, we can see that the majority of the features display zero or no correlation to one another. This is quite an interesting observation that will warrant our further investigation later down. For now, the paired features that display a positive linear correlation are listed as follows:\n\n**(ps_reg_01, ps_reg_03)**\n\n**(ps_reg_02, ps_reg_03)**\n\n**(ps_car_12, ps_car_13)**\n\n**(ps_car_13, ps_car_15)**", "**Correlation of integer features**\n\nFor the columns of interger datatype, I shall now switch to using the Plotly library to show how one can also generate a heatmap of correlation values interactively. Much like our earlier Plotly plot, we generate a heatmap object by simply invoking the \"go.Heatmap\". Here we have to provide values to three different axes, where x and y axes take in the column names while the correlation value is provided by the z-axis. The colorscale attribute takes in keywords that correspond to different color palettes that you will see in the heatmap where in this example, I have used the Greys colorscale (others include Portland and Viridis - try it for yourself). "], "code": "# Reference: https://www.kaggle.com/code/arthurtok/interactive-porto-insights-a-plot-ly-tutorial\n\n#train_int = train_int.drop([\"id\", \"target\"], axis=1)\n# colormap = plt.cm.bone\n# plt.figure(figsize=(21,16))\n# plt.title('Pearson correlation of categorical features', y=1.05, size=15)\n# sns.heatmap(train_cat.corr(),linewidths=0.1,vmax=1.0, square=True, cmap=colormap, linecolor='white', annot=False)\ndata = [\n    go.Heatmap(\n        z= train_int.corr().values,\n        x=train_int.columns.values,\n        y=train_int.columns.values,\n        colorscale='Viridis',\n        reversescale = False,\n        text = True ,\n        opacity = 1.0 )\n]\n\nlayout = go.Layout(\n    title='Pearson Correlation of Integer-type features',\n    xaxis = dict(ticks='', nticks=36),\n    yaxis = dict(ticks='' ),\n    width = 900, height = 700)\n\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename='labelled-heatmap')", "processed": ["correl plot see major featur display zero correl one anoth quit interest observ warrant investig later pair featur display posit linear correl list follow p reg 01 p reg 03 p reg 02 p reg 03 p car 12 p car 13 p car 13 p car 15", "correl integ featur column interg datatyp shall switch use plotli librari show one also gener heatmap correl valu interact much like earlier plotli plot gener heatmap object simpli invok go heatmap provid valu three differ axe x axe take column name correl valu provid z axi colorscal attribut take keyword correspond differ color palett see heatmap exampl use grey colorscal other includ portland viridi tri"]}, {"markdown": ["## Binary features inspection\n\nAnother aspect of the data that we may want to inspect would be the columns that only contain binary values, i.e where values take on only either of the two values 1 or 0. Proceeding, we store all columns that contain these binary values and then generate a vertical plotly barplot of these binary values as follows:"], "code": "# Reference: https://www.kaggle.com/code/arthurtok/interactive-porto-insights-a-plot-ly-tutorial\n\nbin_col = [col for col in train.columns if '_bin' in col]\nzero_list = []\none_list = []\nfor col in bin_col:\n    zero_list.append((train[col]==0).sum())\n    one_list.append((train[col]==1).sum())\ntrace1 = go.Bar(\n    x=bin_col,\n    y=zero_list ,\n    name='Zero count'\n)\ntrace2 = go.Bar(\n    x=bin_col,\n    y=one_list,\n    name='One count'\n)\n\ndata = [trace1, trace2]\nlayout = go.Layout(\n    barmode='stack',\n    title='Count of 1 and 0 in binary variables'\n)\n\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename='stacked-bar')", "processed": ["binari featur inspect anoth aspect data may want inspect would column contain binari valu e valu take either two valu 1 0 proceed store column contain binari valu gener vertic plotli barplot binari valu follow"]}, {"markdown": ["**Plot.ly Scatter Plot of feature importances**\n\nHaving trained the Random Forest, we can obtain the list of feature importances by invoking the attribute \"feature_importances_\" and plot our next Plotly plot, the Scatter plot.\n\nHere we invoke the command Scatter and as per the previous Plotly plots, we have to define our y and x-axes. However the one thing that we pay attention to in scatter plots is the marker attribute. It is the marker attribute where we define and hence control the size, color and scale of the scatter points embedded."], "code": "# Reference: https://www.kaggle.com/code/arthurtok/interactive-porto-insights-a-plot-ly-tutorial\n\n# Scatter plot \ntrace = go.Scatter(\n    y = rf.feature_importances_,\n    x = features,\n    mode='markers',\n    marker=dict(\n        sizemode = 'diameter',\n        sizeref = 1,\n        size = 13,\n        #size= rf.feature_importances_,\n        #color = np.random.randn(500), #set color equal to a variable\n        color = rf.feature_importances_,\n        colorscale='Portland',\n        showscale=True\n    ),\n    text = features\n)\ndata = [trace]\n\nlayout= go.Layout(\n    autosize= True,\n    title= 'Random Forest Feature Importance',\n    hovermode= 'closest',\n     xaxis= dict(\n         ticklen= 5,\n         showgrid=False,\n        zeroline=False,\n        showline=False\n     ),\n    yaxis=dict(\n        title= 'Feature Importance',\n        showgrid=False,\n        zeroline=False,\n        ticklen= 5,\n        gridwidth= 2\n    ),\n    showlegend= False\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig,filename='scatter2010')", "processed": ["plot ly scatter plot featur import train random forest obtain list featur import invok attribut featur import plot next plotli plot scatter plot invok command scatter per previou plotli plot defin x axe howev one thing pay attent scatter plot marker attribut marker attribut defin henc control size color scale scatter point embed"]}, {"markdown": ["Furthermore we could also display a sorted list of all the features ranked by order of their importance, from highest to lowest via the same plotly barplots as follows:"], "code": "# Reference: https://www.kaggle.com/code/arthurtok/interactive-porto-insights-a-plot-ly-tutorial\n\nx, y = (list(x) for x in zip(*sorted(zip(rf.feature_importances_, features), \n                                                            reverse = False)))\ntrace2 = go.Bar(\n    x=x ,\n    y=y,\n    marker=dict(\n        color=x,\n        colorscale = 'Viridis',\n        reversescale = True\n    ),\n    name='Random Forest Feature importance',\n    orientation='h',\n)\n\nlayout = dict(\n    title='Barplot of Feature importances',\n     width = 900, height = 2000,\n    yaxis=dict(\n        showgrid=False,\n        showline=False,\n        showticklabels=True,\n#         domain=[0, 0.85],\n    ))\n\nfig1 = go.Figure(data=[trace2])\nfig1['layout'].update(layout)\npy.iplot(fig1, filename='plots')", "processed": ["furthermor could also display sort list featur rank order import highest lowest via plotli barplot follow"]}, {"markdown": ["## Feature importance via Gradient Boosting model\n\nJust for curiosity, let us try another learning method in getting our feature importances. This time, we use a Gradient Boosting classifier to fit to the training data . Gradient Boosting proceeds in a forward stage-wise fashion, where at each stage regression tress are fitted on the gradient of the loss function (which defaults to the deviance in Sklearn implementation). "], "code": "# Reference: https://www.kaggle.com/code/arthurtok/interactive-porto-insights-a-plot-ly-tutorial\n\nfrom sklearn.ensemble import GradientBoostingClassifier\ngb = GradientBoostingClassifier(n_estimators=100, max_depth=3, min_samples_leaf=4, max_features=0.2, random_state=0)\ngb.fit(train.drop(['id', 'target'],axis=1), train.target)\nfeatures = train.drop(['id', 'target'],axis=1).columns.values\nprint(\"----- Training Done -----\")\n# Scatter plot \ntrace = go.Scatter(\n    y = gb.feature_importances_,\n    x = features,\n    mode='markers',\n    marker=dict(\n        sizemode = 'diameter',\n        sizeref = 1,\n        size = 13,\n        #size= rf.feature_importances_,\n        #color = np.random.randn(500), #set color equal to a variable\n        color = gb.feature_importances_,\n        colorscale='Portland',\n        showscale=True\n    ),\n    text = features\n)\ndata = [trace]\n\nlayout= go.Layout(\n    autosize= True,\n    title= 'Gradient Boosting Machine Feature Importance',\n    hovermode= 'closest',\n     xaxis= dict(\n         ticklen= 5,\n         showgrid=False,\n        zeroline=False,\n        showline=False\n     ),\n    yaxis=dict(\n        title= 'Feature Importance',\n        showgrid=False,\n        zeroline=False,\n        ticklen= 5,\n        gridwidth= 2\n    ),\n    showlegend= False\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig,filename='scatter2010')\nx, y = (list(x) for x in zip(*sorted(zip(gb.feature_importances_, features), \n                                                            reverse = False)))\ntrace2 = go.Bar(\n    x=x ,\n    y=y,\n    marker=dict(\n        color=x,\n        colorscale = 'Viridis',\n        reversescale = True\n    ),\n    name='Gradient Boosting Classifer Feature importance',\n    orientation='h',\n)\n\nlayout = dict(\n    title='Barplot of Feature importances',\n     width = 900, height = 2000,\n    yaxis=dict(\n        showgrid=False,\n        showline=False,\n        showticklabels=True,\n    ))\n\nfig1 = go.Figure(data=[trace2])\nfig1['layout'].update(layout)\npy.iplot(fig1, filename='plots')", "processed": ["featur import via gradient boost model curio let u tri anoth learn method get featur import time use gradient boost classifi fit train data gradient boost proce forward stage wise fashion stage regress tress fit gradient loss function default devianc sklearn implement"]}, {"markdown": ["# Define EDA Python Functions\nI wrote two EDA Python functions. To see the code, click the show code button. One function visualizes overall density and detection rate per category value. The other visualizes density and detection rate over time. Feel free to fork my kernel and use my functions to explore the data. I've also made my timestamp database public so you can import timestamps."], "code": "# Reference: https://www.kaggle.com/code/cdeotte/time-series-eda-malware-0-64\n\nimport calendar, math\n\n# PARAMETERS\n# data : pandas.DataFrame : your data to plot\n# col  : str : which column to plot histogram for left y-axis\n# target : str : which column for mean/rate on right y-axis\n# bars : int : how many histogram bars to show (or less if you set show or min)\n# show : float : stop displaying bars after 100*show% of data is showing\n# minn : float : don't display bars containing under 100*minn% of data\n# sortby : str : either 'frequency', 'category', or 'rate'\n# verbose : int : display text summary 1=yes, 0=no\n# top : int : give this many bars nice color (and matches a subsequent dynamicPlot)\n# title : str : title of plot\n# asc : boolean : sort ascending (for category and rate)\n# dropna : boolean : include missing data as a category or not\n\ndef staticPlot(data, col, target='HasDetections', bars=10, show=1.0, sortby='frequency'\n               , verbose=1, top=5, title='',asc=False, dropna=False, minn=0.0):\n    # calcuate density and detection rate\n    cv = data[col].value_counts(dropna=dropna)\n    cvd = cv.to_dict()\n    nm = cv.index.values; lnn = len(nm); lnn2 = lnn\n    th = show * len(data)\n    th2 = minn * len(data)\n    sum = 0; lnn2 = 0\n    for x in nm[0:bars]:\n        lnn2 += 1\n        try: sum += cvd[x]\n        except: sum += cv[x]\n        if sum>th:\n            break\n        try:\n            if cvd[x]<th2: break\n        except:\n            if cv[x]<th2: break\n    if lnn2<bars: bars = lnn2\n    pct = round(100.0*sum/len(data),2)\n    lnn = min(lnn,lnn2)\n    ratio = [0.0]*lnn; lnn3 = lnn\n    if sortby =='frequency': lnn3 = min(lnn3,bars)\n    elif sortby=='category': lnn3 = 0\n    for i in range(lnn3):\n        if target not in data:\n            ratio[i] = np.nan\n        elif nan_check(nm[i]):\n            ratio[i] = data[target][data[col].isna()].mean()\n        else:\n            ratio[i] = data[target][data[col]==nm[i]].mean()\n    try: all = pd.DataFrame( {'category':nm[0:lnn],'frequency':[cvd[x] for x in nm[0:lnn]],'rate':ratio} )\n    except: all = pd.DataFrame( {'category':nm[0:lnn],'frequency':[cv[x] for x in nm[0:lnn]],'rate':ratio} )\n    if sortby=='rate': \n        all = all.sort_values(sortby, ascending=asc)\n    elif sortby=='category':\n        try: \n            all['temp'] = all['category'].astype('float')\n            all = all.sort_values('temp', ascending=asc)\n        except:\n            all = all.sort_values('category', ascending=asc)\n    if bars<lnn: all = all[0:bars]\n    if verbose==1 and target in data:\n        print('TRAIN.CSV variable',col,'has',len(nm),'categories')\n        print('The',min(bars,lnn),'bars displayed here contain',pct,'% of data.')\n        mlnn = data[col].isna().sum()\n        print(\"The data has %.1f %% NA. The plot is sorted by \" % (100.0*mlnn/len(data)) + sortby )\n    \n    # plot density and detection rate\n    fig = plt.figure(1,figsize=(15,3))\n    ax1 = fig.add_subplot(1,1,1)\n    clrs = ['red', 'green', 'blue', 'yellow', 'magenta']\n    barss = ax1.bar([str(x) for x in all['category']],[x/float(len(data)) for x in all['frequency']],color=clrs)\n    for i in range(len(all)-top):\n        barss[top+i].set_color('cyan')\n    if target in data:\n        ax2 = ax1.twinx()\n        if sortby!='category': infected = all['rate'][0:lnn]\n        else:\n            infected=[]\n            for x in all['category']:\n                if nan_check(x): infected.append( data[ data[col].isna() ][target].mean() )\n                elif cvd[x]!=0: infected.append( data[ data[col]==x ][target].mean() )\n                else: infected.append(-1)\n        ax2.plot([str(x) for x in all['category']],infected[0:lnn],'k:o')\n        #ax2.set_ylim(a,b)\n        ax2.spines['left'].set_color('red')\n        ax2.set_ylabel('Detection Rate', color='k')\n    ax1.spines['left'].set_color('red')\n    ax1.yaxis.label.set_color('red')\n    ax1.tick_params(axis='y', colors='red')\n    ax1.set_ylabel('Category Proportion', color='r')\n    if title!='': plt.title(title)\n    plt.show()\n    if verbose==1 and target not in data:\n        print('TEST.CSV variable',col,'has',len(nm),'categories')\n        print('The',min(bars,lnn),'bars displayed here contain',pct,'% of the data.')\n        mlnn = data[col].isna().sum()\n        print(\"The data has %.1f %% NA. The plot is sorted by \" % (100.0*mlnn/len(data)) + sortby )\n\n# PARAMETERS\n# data : pandas.DataFrame : your data to plot\n# col  : str : which column for density on left y-axis\n# target : str : which column for mean/rate on right y-axis\n# start : datetime.datetime : x-axis minimum\n# end : datetime.datetime : x-axis maximum\n# inc_hr : int : resolution of time sampling = inc_hr + inc_dy*24 + inc_mn*720 hours\n# inc_dy : int : resolution of time sampling = inc_hr + inc_dy*24 + inc_mn*720 hours\n# inc_mn : int : resolution of time sampling = inc_hr + inc_dy*24 + inc_mn*720 hours\n# show : float : only show the most frequent category values that include 100*show% of data\n# top : int : plot this many solid lines\n# top2 : int : plot this many dotted lines\n# title : str : title of plot\n# legend : int : include legend or not. 1=yes, 0=no\n# dropna : boolean : include missing data as a category or not\n        \ndef dynamicPlot(data,col, target='HasDetections', start=datetime(2018,4,1), end=datetime(2018,12,1)\n                ,inc_hr=0,inc_dy=7,inc_mn=0,show=0.99,top=5,top2=4,title='',legend=1, dropna=False):\n    # check for timestamps\n    if 'Date' not in data:\n        print('Error dynamicPlot: DataFrame needs column Date of datetimes')\n        return\n    \n    # remove detection line if category density is too small\n    cv = data[col].value_counts(dropna=dropna)\n    cvd = cv.to_dict()\n    nm = cv.index.values\n    th = show * len(data)\n    sum = 0; lnn2 = 0\n    for x in nm:\n        lnn2 += 1\n        try: sum += cvd[x]\n        except: sum += cv[x]\n        if sum>th:\n            break\n    top = min(top,len(nm))\n    top2 = min(top2,len(nm),lnn2,top)\n\n    # calculate rate within each time interval\n    diff = (end-start).days*24*3600 + (end-start).seconds\n    size = diff//(3600*((inc_mn * 28 + inc_dy) * 24 + inc_hr)) + 5\n    data_counts = np.zeros([size,2*top+1],dtype=float)\n    idx=0; idx2 = {}\n    for i in range(top):\n        idx2[nm[i]] = i+1\n    low = start\n    high = add_time(start,inc_mn,inc_dy,inc_hr)\n    data_times = [low+(high-low)/2]\n    while low<end:\n        slice = data[ (data['Date']<high) & (data['Date']>=low) ]\n        data_counts[idx,0] = len(slice)\n        for key in idx2:\n            if nan_check(key): slice2 = slice[slice[col].isna()]\n            else: slice2 = slice[slice[col]==key]\n            data_counts[idx,idx2[key]] = len(slice2)\n            if target in data:\n                data_counts[idx,top+idx2[key]] = slice2['HasDetections'].mean()\n        low = high\n        high = add_time(high,inc_mn,inc_dy,inc_hr)\n        data_times.append(low+(high-low)/2)\n        idx += 1\n\n    # plot lines\n    fig = plt.figure(1,figsize=(15,3))\n    cl = ['r','g','b','y','m']\n    ax3 = fig.add_subplot(1,1,1)\n    lines = []; labels = []\n    for i in range(top):\n        tmp, = ax3.plot(data_times,data_counts[0:idx+1,i+1],cl[i%5])\n        lines.append(tmp)\n        labels.append(str(nm[i]))\n    ax3.spines['left'].set_color('red')\n    ax3.yaxis.label.set_color('red')\n    ax3.tick_params(axis='y', colors='red')\n    if col!='ones': ax3.set_ylabel('Category Density', color='r')\n    else: ax3.set_ylabel('Data Density', color='r')\n    ax3.set_yticklabels([])\n    if target in data:\n        ax4 = ax3.twinx()\n        for i in range(top2):\n            ax4.plot(data_times,data_counts[0:idx+1,i+1+top],cl[i%5]+\":\")\n        ax4.spines['left'].set_color('red')\n        ax4.set_ylabel('Detection Rate', color='k')\n    if title!='': plt.title(title)\n    if legend==1: plt.legend(lines,labels)\n    plt.show()\n        \n# INCREMENT A DATETIME\ndef add_time(sdate,months=0,days=0,hours=0):\n    month = sdate.month -1 + months\n    year = sdate.year + month // 12\n    month = month % 12 + 1\n    day = sdate.day + days\n    if day>calendar.monthrange(year,month)[1]:\n        day -= calendar.monthrange(year,month)[1]\n        month += 1\n        if month>12:\n            month = 1\n            year += 1\n    hour = sdate.hour + hours\n    if hour>23:\n        hour = 0\n        day += 1\n        if day>calendar.monthrange(year,month)[1]:\n            day -= calendar.monthrange(year,month)[1]\n            month += 1\n            if month>12:\n                month = 1\n                year += 1\n    return datetime(year,month,day,hour,sdate.minute)\n\n# CHECK FOR NAN\ndef nan_check(x):\n    if isinstance(x,float):\n        if math.isnan(x):\n            return True\n    return False", "processed": ["defin eda python function wrote two eda python function see code click show code button one function visual overal densiti detect rate per categori valu visual densiti detect rate time feel free fork kernel use function explor data also made timestamp databas public import timestamp"]}, {"markdown": ["# Training ROC Curve\nThis competiton's metric is AUC, area under ROC. Let's plot our training ROC and calculate our training AUC. (We should really calculate validation AUC, but since our model is linear, training AUC should be similar.)"], "code": "# Reference: https://www.kaggle.com/code/cdeotte/time-series-eda-malware-0-64\n\n#https://stackoverflow.com/questions/25009284/how-to-plot-roc-curve-in-python\nfrom sklearn import metrics\nfpr, tpr, threshold = metrics.roc_curve(df_train['HasDetections'].values, df_train['Prob'].values)\nroc_auc = metrics.auc(fpr, tpr)\n\n# method I: plt\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(8,8))\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.3f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()", "processed": ["train roc curv competiton metric auc area roc let plot train roc calcul train auc realli calcul valid auc sinc model linear train auc similar"]}, {"markdown": ["<a id=\"de\"></a>\n## Data exploration"], "code": "# Reference: https://www.kaggle.com/code/artgor/how-to-not-overfit\n\ntrain.head()\ntrain[train.columns[2:]].std().plot('hist');\nplt.title('Distribution of stds of all columns');\ntrain[train.columns[2:]].mean().plot('hist');\nplt.title('Distribution of means of all columns');\n# we have no missing values\ntrain.isnull().any().any()\nprint('Distributions of first 28 columns')\nplt.figure(figsize=(26, 24))\nfor i, col in enumerate(list(train.columns)[2:30]):\n    plt.subplot(7, 4, i + 1)\n    plt.hist(train[col])\n    plt.title(col)\ntrain['target'].value_counts()", "processed": ["id de data explor"]}, {"markdown": ["We can see that correlations between features are lower that 0.3 and the most correlated feature with target has correlation of 0.37. So we have no highly correlated features which we could drop, on the other hand we could drop some columns with have little correlation with the target.", "<a id=\"bm\"></a>\n## Basic modelling"], "code": "# Reference: https://www.kaggle.com/code/artgor/how-to-not-overfit\n\nX_train = train.drop(['id', 'target'], axis=1)\ny_train = train['target']\nX_test = test.drop(['id'], axis=1)\nn_fold = 20\nfolds = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=42)\nrepeated_folds = RepeatedStratifiedKFold(n_splits=20, n_repeats=20, random_state=42)\n\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\ndef train_model(X, X_test, y, params, folds=folds, model_type='lgb', plot_feature_importance=False, averaging='usual', model=None):\n    oof = np.zeros(len(X))\n    prediction = np.zeros(len(X_test))\n    scores = []\n    feature_importance = pd.DataFrame()\n    for fold_n, (train_index, valid_index) in enumerate(folds.split(X, y)):\n        # print('Fold', fold_n, 'started at', time.ctime())\n        X_train, X_valid = X[train_index], X[valid_index]\n        y_train, y_valid = y[train_index], y[valid_index]\n        \n        if model_type == 'lgb':\n            train_data = lgb.Dataset(X_train, label=y_train)\n            valid_data = lgb.Dataset(X_valid, label=y_valid)\n            \n            model = lgb.train(params,\n                    train_data,\n                    num_boost_round=2000,\n                    valid_sets = [train_data, valid_data],\n                    verbose_eval=500,\n                    early_stopping_rounds = 200)\n            \n            y_pred_valid = model.predict(X_valid)\n            y_pred = model.predict(X_test, num_iteration=model.best_iteration_)\n            \n        if model_type == 'xgb':\n            train_data = xgb.DMatrix(data=X_train, label=y_train, feature_names=X_tr.columns)\n            valid_data = xgb.DMatrix(data=X_valid, label=y_valid, feature_names=X_tr.columns)\n\n            watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]\n            model = xgb.train(dtrain=train_data, num_boost_round=20000, evals=watchlist, early_stopping_rounds=200, verbose_eval=500, params=params)\n            y_pred_valid = model.predict(xgb.DMatrix(X_valid, feature_names=X_tr.columns), ntree_limit=model.best_ntree_limit)\n            y_pred = model.predict(xgb.DMatrix(X_test, feature_names=X_tr.columns), ntree_limit=model.best_ntree_limit)\n        \n        if model_type == 'cat':\n            model = CatBoostClassifier(iterations=20000,  eval_metric='AUC', **params)\n            model.fit(X_train, y_train, eval_set=(X_valid, y_valid), cat_features=[], use_best_model=True, verbose=False)\n\n            y_pred_valid = model.predict(X_valid)\n            y_pred = model.predict(X_test)\n            \n            \n        if model_type == 'sklearn':\n            model = model\n            model.fit(X_train, y_train)\n            y_pred_valid = model.predict(X_valid).reshape(-1,)\n            score = roc_auc_score(y_valid, y_pred_valid)\n            # print(f'Fold {fold_n}. AUC: {score:.4f}.')\n            # print('')\n            \n            y_pred = model.predict_proba(X_test)[:, 1]\n            \n        if model_type == 'glm':\n            model = sm.GLM(y_train, X_train, family=sm.families.Binomial())\n            model_results = model.fit()\n            model_results.predict(X_test)\n            y_pred_valid = model_results.predict(X_valid).reshape(-1,)\n            score = roc_auc_score(y_valid, y_pred_valid)\n            \n            y_pred = model_results.predict(X_test)\n        \n        oof[valid_index] = y_pred_valid.reshape(-1,)\n        scores.append(roc_auc_score(y_valid, y_pred_valid))\n\n        if averaging == 'usual':\n            prediction += y_pred\n        elif averaging == 'rank':\n            prediction += pd.Series(y_pred).rank().values  \n        \n        if model_type == 'lgb':\n            # feature importance\n            fold_importance = pd.DataFrame()\n            fold_importance[\"feature\"] = X.columns\n            fold_importance[\"importance\"] = model.feature_importances_\n            fold_importance[\"fold\"] = fold_n + 1\n            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n\n    prediction /= n_fold\n    \n    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n    \n    if model_type == 'lgb':\n        feature_importance[\"importance\"] /= n_fold\n        if plot_feature_importance:\n            cols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n                by=\"importance\", ascending=False)[:50].index\n\n            best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n\n            plt.figure(figsize=(16, 12));\n            sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False));\n            plt.title('LGB Features (avg over folds)');\n        \n            return oof, prediction, feature_importance\n        return oof, prediction, scores\n    \n    else:\n        return oof, prediction, scores\n# A lot of people are using logreg currently, let's try\nmodel = linear_model.LogisticRegression(class_weight='balanced', penalty='l1', C=0.1, solver='liblinear')\noof_lr, prediction_lr, scores = train_model(X_train, X_test, y_train, params=None, model_type='sklearn', model=model)\n# A lot of people are using logreg currently, let's try\ncat_params = {'learning_rate': 0.02,\n              'depth': 5,\n              'l2_leaf_reg': 10,\n              'bootstrap_type': 'Bernoulli',\n              #'metric_period': 500,\n              'od_type': 'Iter',\n              'od_wait': 50,\n              'random_seed': 11,\n              'allow_writing_files': False}\noof_lr, prediction_lr, scores = train_model(X_train, X_test, y_train, params=cat_params, model_type='cat')\nmodel = CatBoostClassifier(iterations=400,  eval_metric='AUC', **cat_params)\nmodel.fit(X_train, y_train, cat_features=[], use_best_model=True, verbose=200)\nmodel = linear_model.LogisticRegression(class_weight='balanced', penalty='l1', C=0.1, solver='liblinear')\noof_lr, prediction_lr_repeated, scores = train_model(X_train, X_test, y_train, params=None, model_type='sklearn', model=model, folds=repeated_folds)", "processed": ["see correl featur lower 0 3 correl featur target correl 0 37 highli correl featur could drop hand could drop column littl correl target", "id bm basic model"]}, {"markdown": ["Wow, if we select columns by permutation importance, CV score drops significantly. It seems it doesn't work well in out case.", "<a id=\"shap\"></a>\n## SHAP\n\nAnother interesting tool is SHAP. It also provides explanations for a variety of models."], "code": "# Reference: https://www.kaggle.com/code/artgor/how-to-not-overfit\n\nX_train = train.drop(['id', 'target'], axis=1)\ny_train = train['target']\nX_test = test.drop(['id'], axis=1)\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\nmodel = linear_model.LogisticRegression(class_weight='balanced', penalty='l1', C=0.1, solver='liblinear')\noof_lr, prediction_lr, _ = train_model(X_train, X_test, y_train, params=None, model_type='sklearn', model=model)\nexplainer = shap.LinearExplainer(model, X_train)\nshap_values = explainer.shap_values(X_train)\n\nshap.summary_plot(shap_values, X_train)", "processed": ["wow select column permut import cv score drop significantli seem work well case", "id shap shap anoth interest tool shap also provid explan varieti model"]}, {"markdown": ["So, parameters for logreg are optimal, let's try other models"], "code": "# Reference: https://www.kaggle.com/code/artgor/how-to-not-overfit\n\nfrom sklearn.naive_bayes import GaussianNB\ngnb = GaussianNB()\noof_gnb, prediction_gnb, scores_gnb = train_model(X_train, X_test, y_train, params=None, model_type='sklearn', model=gnb)\nfrom sklearn.ensemble import AdaBoostClassifier\nabc = AdaBoostClassifier()\n\nparameter_grid = {'n_estimators': [5, 10, 20, 50, 100],\n                  'learning_rate': [0.001, 0.01, 0.1, 1.0, 10.0]\n                 }\n\ngrid_search = GridSearchCV(abc, param_grid=parameter_grid, cv=folds, scoring='roc_auc')\ngrid_search.fit(X_train, y_train)\nprint('Best score: {}'.format(grid_search.best_score_))\nprint('Best parameters: {}'.format(grid_search.best_params_))\nabc = AdaBoostClassifier(**grid_search.best_params_)\noof_abc, prediction_abc, scores_abc = train_model(X_train, X_test, y_train, params=None, model_type='sklearn', model=abc)\nfrom sklearn.ensemble import ExtraTreesClassifier\netc = ExtraTreesClassifier()\n\nparameter_grid = {'n_estimators': [10, 50, 100, 1000],\n                  'max_depth': [None, 3, 5, 15]\n                 }\n\ngrid_search = GridSearchCV(etc, param_grid=parameter_grid, cv=folds, scoring='roc_auc', n_jobs=-1)\ngrid_search.fit(X_train, y_train)\nprint('Best score: {}'.format(grid_search.best_score_))\nprint('Best parameters: {}'.format(grid_search.best_params_))\netc = ExtraTreesClassifier(**grid_search.best_params_)\noof_etc, prediction_etc, scores_etc = train_model(X_train, X_test, y_train, params=None, model_type='sklearn', model=etc)\nfrom sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier()\n\nparameter_grid = {'n_estimators': [10, 50, 100, 1000],\n                  'max_depth': [None, 3, 5, 15]\n                 }\n\ngrid_search = GridSearchCV(rfc, param_grid=parameter_grid, cv=folds, scoring='roc_auc', n_jobs=-1)\ngrid_search.fit(X_train, y_train)\nprint('Best score: {}'.format(grid_search.best_score_))\nprint('Best parameters: {}'.format(grid_search.best_params_))\nrfc = RandomForestClassifier(**grid_search.best_params_)\noof_rfc, prediction_rfc, scores_rfc = train_model(X_train, X_test, y_train, params=None, model_type='sklearn', model=rfc)\nfrom sklearn.gaussian_process import GaussianProcessClassifier\ngpc = GaussianProcessClassifier()\noof_gpc, prediction_gpc, scores_gpc = train_model(X_train, X_test, y_train, params=None, model_type='sklearn', model=gpc)\nfrom sklearn.svm import SVC\nsvc = SVC(probability=True, gamma='scale')\n\nparameter_grid = {'C': [0.001, 0.01, 0.1, 1.0, 10.0],\n                  'kernel': ['linear', 'poly', 'rbf'],\n                 }\n\ngrid_search = GridSearchCV(svc, param_grid=parameter_grid, cv=folds, scoring='roc_auc', n_jobs=-1)\ngrid_search.fit(X_train, y_train)\nprint('Best score: {}'.format(grid_search.best_score_))\nprint('Best parameters: {}'.format(grid_search.best_params_))\nsvc = SVC(probability=True, gamma='scale', **grid_search.best_params_)\noof_svc, prediction_svc, scores_svc = train_model(X_train, X_test, y_train, params=None, model_type='sklearn', model=svc)\nfrom sklearn.neighbors import KNeighborsClassifier\nknc = KNeighborsClassifier()\n\nparameter_grid = {'n_neighbors': [2, 3, 5, 10, 20],\n                  'weights': ['uniform', 'distance'],\n                  'leaf_size': [5, 10, 30]\n                 }\n\ngrid_search = GridSearchCV(knc, param_grid=parameter_grid, cv=folds, scoring='roc_auc', n_jobs=-1)\ngrid_search.fit(X_train, y_train)\nprint('Best score: {}'.format(grid_search.best_score_))\nprint('Best parameters: {}'.format(grid_search.best_params_))\nknc = KNeighborsClassifier(**grid_search.best_params_)\noof_knc, prediction_knc, scores_knc = train_model(X_train, X_test, y_train, params=None, model_type='sklearn', model=knc)\nfrom sklearn.naive_bayes import BernoulliNB\nbnb = BernoulliNB()\n\nparameter_grid = {'alpha': [0.0001, 1, 2, 10]\n                 }\n\ngrid_search = GridSearchCV(bnb, param_grid=parameter_grid, cv=folds, scoring='roc_auc', n_jobs=-1)\ngrid_search.fit(X_train, y_train)\nprint('Best score: {}'.format(grid_search.best_score_))\nprint('Best parameters: {}'.format(grid_search.best_params_))\nbnb = BernoulliNB(**grid_search.best_params_)\noof_bnb, prediction_bnb, scores_bnb = train_model(X_train, X_test, y_train, params=None, model_type='sklearn', model=bnb)\nsgd = linear_model.SGDClassifier(eta0=1, max_iter=1000, tol=0.0001)\n\nparameter_grid = {'loss': ['log', 'modified_huber'],\n                  'penalty': ['l1', 'l2', 'elasticnet'],\n                  'alpha': [0.001, 0.01],\n                  'l1_ratio': [0, 0.15, 0.5, 1.0],\n                  'learning_rate': ['optimal', 'invscaling', 'adaptive']\n                 }\n\ngrid_search = GridSearchCV(sgd, param_grid=parameter_grid, cv=folds, scoring='roc_auc', n_jobs=-1)\ngrid_search.fit(X_train, y_train)\nprint('Best score: {}'.format(grid_search.best_score_))\nprint('Best parameters: {}'.format(grid_search.best_params_))\nsgd = linear_model.SGDClassifier(eta0=1, tol=0.0001, **grid_search.best_params_)\noof_sgd, prediction_sgd, scores_sgd = train_model(X_train, X_test, y_train, params=None, model_type='sklearn', model=sgd)\nplt.figure(figsize=(12, 8));\nscores_df = pd.DataFrame({'LogisticRegression': scores})\nscores_df['GaussianNB'] = scores_gnb\nscores_df['AdaBoostClassifier'] = scores_abc\nscores_df['ExtraTreesClassifier'] = scores_etc\nscores_df['GaussianProcessClassifier'] = scores_gpc\nscores_df['SVC'] = scores_svc\nscores_df['KNeighborsClassifier'] = scores_knc\nscores_df['BernoulliNB'] = scores_bnb\nscores_df['SGDClassifier'] = scores_sgd\nscores_df['RandomForestClassifier'] = scores_rfc\n\nsns.boxplot(data=scores_df);\nplt.xticks(rotation=45);", "processed": ["paramet logreg optim let tri model"]}, {"markdown": ["The number of polynomial features is ~45k which is too much. We need some way to select some of them. Let's try use correlations with target."], "code": "# Reference: https://www.kaggle.com/code/artgor/how-to-not-overfit\n\ncor = pd.DataFrame(X_train_poly).corrwith(y_train)\nsc = []\nfor i in range(10, 510, 5):\n    top_corr_cols = list(cor.abs().sort_values().tail(i).reset_index()['index'].values)\n    X_train_poly1 = X_train_poly[:, top_corr_cols]\n    X_test_poly1 = X_test_poly[:, top_corr_cols]\n    oof_lr_poly, prediction_lr_poly, scores = train_model(X_train_poly1, X_test_poly1, y_train, params=None, model_type='sklearn', model=model)\n    sc.append(scores)\ndata = [go.Scatter(\n        x = list(range(10, 510, 5)),\n        y = [np.round(np.mean(i), 4) for i in sc],\n        name = 'CV scores'\n    )]\nlayout = go.Layout(dict(title = \"Top N poly features vs CV\",\n                  xaxis = dict(title = 'Top N features'),\n                  yaxis = dict(title = 'CV score'),\n                  ))\npy.iplot(dict(data=data, layout=layout), filename='basic-line')", "processed": ["number polynomi featur 45k much need way select let tri use correl target"]}, {"markdown": ["<a id=\"select\"></a>\n## Sklearn feature selection\n\nSklearn has several methods to do feature selection. Let's try some of them!"], "code": "# Reference: https://www.kaggle.com/code/artgor/how-to-not-overfit\n\n# baseline score\nX_train = train.drop(['id', 'target'], axis=1)\nX_test = test.drop(['id'], axis=1)\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\nmodel = linear_model.LogisticRegression(class_weight='balanced', penalty='l1', C=0.1, solver='liblinear')\noof_lr_1, prediction_lr_1, scores = train_model(X_train, X_test, y_train, params=None, model_type='sklearn', model=model)\nscores_dict = {'f_classif': [], 'mutual_info_classif': []}\nfor i in range(5, 100, 5):\n    s1 = SelectPercentile(f_classif, percentile=i)\n    X_train1 = s1.fit_transform(X_train, y_train.values.astype(int))\n    X_test1 = s1.transform(X_test)\n    oof_lr_1, prediction_lr_1, scores = train_model(X_train1, X_test1, y_train, params=None, model_type='sklearn', model=model)\n    scores_dict['f_classif'].append(np.mean(scores))\n    \n    s2 = SelectPercentile(mutual_info_classif, percentile=i)\n    X_train1 = s2.fit_transform(X_train, y_train.values.astype(int))\n    X_test1 = s2.transform(X_test)\n    oof_lr_1, prediction_lr_1, scores = train_model(X_train1, X_test1, y_train, params=None, model_type='sklearn', model=model)\n    scores_dict['mutual_info_classif'].append(np.mean(scores))\ndata = [go.Scatter(\n        x = list(range(5, 100, 5)),\n        y = scores_dict['f_classif'],\n        name = 'CV scores f_classif'\n    ), go.Scatter(\n        x = list(range(5, 100, 5)),\n        y = scores_dict['mutual_info_classif'],\n        name = 'CV scores mutual_info_classif')]\nlayout = go.Layout(dict(title = \"Top N features by percentile vs CV\",\n                  xaxis = dict(title = 'Top N features by percentile'),\n                  yaxis = dict(title = 'CV score'),\n                  ))\npy.iplot(dict(data=data, layout=layout), filename='basic-line')\nscores_dict = {'f_classif': [], 'mutual_info_classif': []}\nfor i in range(10, 301, 10):\n    s1 = SelectKBest(f_classif, k=i)\n    X_train1 = s1.fit_transform(X_train, y_train.values.astype(int))\n    X_test1 = s1.transform(X_test)\n    oof_lr_1, prediction_lr_1, scores = train_model(X_train1, X_test1, y_train, params=None, model_type='sklearn', model=model)\n    scores_dict['f_classif'].append(np.mean(scores))\n    \n    s2 = SelectKBest(mutual_info_classif, k=i)\n    X_train1 = s2.fit_transform(X_train, y_train.values.astype(int))\n    X_test1 = s2.transform(X_test)\n    oof_lr_1, prediction_lr_1, scores = train_model(X_train1, X_test1, y_train, params=None, model_type='sklearn', model=model)\n    scores_dict['mutual_info_classif'].append(np.mean(scores))\ndata = [go.Scatter(\n        x = list(range(10, 301, 10)),\n        y = scores_dict['f_classif'],\n        name = 'CV scores f_classif'\n    ), go.Scatter(\n        x = list(range(10, 301, 10)),\n        y = scores_dict['mutual_info_classif'],\n        name = 'CV scores mutual_info_classif')]\nlayout = go.Layout(dict(title = \"Top N features by SelectKBest vs CV\",\n                  xaxis = dict(title = 'Top N features by SelectKBest'),\n                  yaxis = dict(title = 'CV score'),\n                  ))\npy.iplot(dict(data=data, layout=layout), filename='basic-line')\nselector = SelectKBest(f_classif, k=60)\nX_trainK = selector.fit_transform(X_train, y_train.values.astype(int))\nX_testK = selector.transform(X_test)\noof_lr_1, prediction_lr_1, scores = train_model(X_trainK, X_testK, y_train, params=None, model_type='sklearn', model=model)\nsubmission = pd.read_csv('../input/sample_submission.csv')\nsubmission['target'] = prediction_lr_1\n# submission.to_csv('top_n_features.csv', index=False)\n\nsubmission.head()\nscores_list = []\nfor i in range(10, 301, 5):\n    s = RFE(model, i, step=1)\n    X_train1 = s.fit_transform(X_train, y_train.values.astype(int))\n    X_test1 = s.transform(X_test)\n    oof_lr_1, prediction_lr_1, scores = train_model(X_train1, X_test1, y_train, params=None, model_type='sklearn', model=model)\n    scores_list.append(np.mean(scores))\ndata = [go.Scatter(\n        x = list(range(10, 301, 5)),\n        y = scores_list,\n        name = 'CV scores RFE'\n    )]\nlayout = go.Layout(dict(title = \"Top N features by RFE vs CV\",\n                  xaxis = dict(title = 'Top N features by RFE'),\n                  yaxis = dict(title = 'CV score'),\n                  ))\npy.iplot(dict(data=data, layout=layout), filename='basic-line')\nselector = RFE(model, 20, step=1)\nX_trainK = selector.fit_transform(X_train, y_train.values.astype(int))\nX_testK = selector.transform(X_test)\noof_lr_1, prediction_lr_rfe_20, scores = train_model(X_trainK, X_testK, y_train, params=None, model_type='sklearn', model=model)\nsubmission = pd.read_csv('../input/sample_submission.csv')\nsubmission['target'] = prediction_lr_rfe_20\nsubmission.to_csv('rfe_20.csv', index=False)\n\nsubmission.head()", "processed": ["id select sklearn featur select sklearn sever method featur select let tri"]}, {"markdown": ["Here, we take an example with negative target.", "### Negative Target"], "code": "# Reference: https://www.kaggle.com/code/xhlulu/exploring-signal-processing-with-scipy\n\nplt.figure(figsize=(15, 10))\nwindow = 10\n\nfor phase in range(3):\n    sig = signals[0, phase, :]\n    \n    plt.plot(sig, label=f'Phase {phase} Raw')\n    convolved = apply_convolution(sig, window)\n    plt.plot(convolved, label=f'Phase {phase} Convolved')\n\nplt.legend()\nplt.title(f\"Applying convolutions - Window Size {window}\", size=15)\nplt.show()\nplt.figure(figsize=(15, 10))\nwindow = 100\n\nfor phase in range(3):\n    sig = signals[0, phase, :]\n    \n    plt.plot(sig, label=f'Phase {phase} Raw')\n    convolved = apply_convolution(sig, window)\n    plt.plot(convolved, label=f'Phase {phase} Convolved')\n\nplt.legend()\nplt.title(f\"Applying convolutions - Window Size {window}\", size=15)\nplt.show()\nplt.figure(figsize=(15, 10))\nwindow = 1000\n\nfor phase in range(3):\n    sig = signals[0, phase, :]\n    \n    plt.plot(sig, label=f'Phase {phase} Raw')\n    convolved = apply_convolution(sig, window)\n    plt.plot(convolved, label=f'Phase {phase} Convolved')\n\nplt.legend()\nplt.title(f\"Applying convolutions - Window Size {window}\", size=15)\nplt.show()", "processed": ["take exampl neg target", "neg target"]}, {"markdown": ["Now, let's see a positive target.", "### Positive Target"], "code": "# Reference: https://www.kaggle.com/code/xhlulu/exploring-signal-processing-with-scipy\n\nplt.figure(figsize=(15, 10))\nwindow = 10\n\nfor phase in range(3):\n    sig = signals[1, phase, :]\n    \n    plt.plot(sig, label=f'Phase {phase} Raw')\n    convolved = apply_convolution(sig, window)\n    plt.plot(convolved, label=f'Phase {phase} Convolved')\n\nplt.legend()\nplt.title(f\"Applying convolutions - Window Size {window}\", size=15)\nplt.show()\nplt.figure(figsize=(15, 10))\nwindow = 100\n\nfor phase in range(3):\n    sig = signals[1, phase, :]\n    \n    plt.plot(sig, label=f'Phase {phase} Raw')\n    convolved = apply_convolution(sig, window)\n    plt.plot(convolved, label=f'Phase {phase} Convolved')\n\nplt.legend()\nplt.title(f\"Applying convolutions - Window Size {window}\", size=15)\nplt.show()\nplt.figure(figsize=(15, 10))\nwindow = 1000\n\nfor phase in range(3):\n    sig = signals[1, phase, :]\n    \n    plt.plot(sig, label=f'Phase {phase} Raw')\n    convolved = apply_convolution(sig, window)\n    plt.plot(convolved, label=f'Phase {phase} Convolved')\n\nplt.legend()\nplt.title(f\"Applying convolutions - Window Size {window}\", size=15)\nplt.show()", "processed": ["let see posit target", "posit target"]}, {"markdown": ["## 2. C-Spline", "### Negative Target"], "code": "# Reference: https://www.kaggle.com/code/xhlulu/exploring-signal-processing-with-scipy\n\n%%time\nsmoothing = 0\nplt.figure(figsize=(15, 10))\n\nfor phase in range(3):\n    sig = signals[0, phase, :]\n    filtered = signal.cspline1d(sig, smoothing)\n    \n    plt.plot(sig, label=f'Phase {phase} Raw')\n    plt.plot(filtered, label=f'Phase {phase} Filtered')\n\nplt.legend()\nplt.title(f\"Applying Cubic Spline, Smoothing: {smoothing}\", size=15)\nplt.show()\nsmoothing = 1\nplt.figure(figsize=(15, 10))\n\nfor phase in range(3):\n    sig = signals[0, phase, :]\n    filtered = signal.cspline1d(sig, smoothing)\n    \n    plt.plot(sig, label=f'Phase {phase} Raw')\n    plt.plot(filtered, label=f'Phase {phase} Filtered')\n\nplt.legend()\nplt.title(f\"Applying Cubic Spline, Smoothing: {smoothing}\", size=15)\nplt.show()\nsmoothing = 10\nplt.figure(figsize=(15, 10))\n\nfor phase in range(3):\n    sig = signals[0, phase, :]\n    filtered = signal.cspline1d(sig, smoothing)\n    \n    plt.plot(sig, label=f'Phase {phase} Raw')\n    plt.plot(filtered, label=f'Phase {phase} Filtered')\n\nplt.legend()\nplt.title(f\"Applying Cubic Spline, Smoothing: {smoothing}\", size=15)\nplt.show()", "processed": ["2 c spline", "neg target"]}, {"markdown": ["### Positive Target"], "code": "# Reference: https://www.kaggle.com/code/xhlulu/exploring-signal-processing-with-scipy\n\nsmoothing = 0\nplt.figure(figsize=(15, 10))\n\nfor phase in range(3):\n    sig = signals[1, phase, :]\n    filtered = signal.cspline1d(sig, smoothing)\n    \n    plt.plot(sig, label=f'Phase {phase} Raw')\n    plt.plot(filtered, label=f'Phase {phase} Filtered')\n\nplt.legend()\nplt.title(f\"Applying Cubic Spline, Smoothing: {smoothing}\", size=15)\nplt.show()\nsmoothing = 1\nplt.figure(figsize=(15, 10))\n\nfor phase in range(3):\n    sig = signals[1, phase, :]\n    filtered = signal.cspline1d(sig, smoothing)\n    \n    plt.plot(sig, label=f'Phase {phase} Raw')\n    plt.plot(filtered, label=f'Phase {phase} Filtered')\n\nplt.legend()\nplt.title(f\"Applying Cubic Spline, Smoothing: {smoothing}\", size=15)\nplt.show()\nsmoothing = 10\nplt.figure(figsize=(15, 10))\n\nfor phase in range(3):\n    sig = signals[1, phase, :]\n    filtered = signal.cspline1d(sig, smoothing)\n    \n    plt.plot(sig, label=f'Phase {phase} Raw')\n    plt.plot(filtered, label=f'Phase {phase} Filtered')\n\nplt.legend()\nplt.title(f\"Applying Cubic Spline, Smoothing: {smoothing}\", size=15)\nplt.show()", "processed": ["posit target"]}, {"markdown": ["## 3. Q-Spline\n\nScipy does not support smoothing Q-Spline yet, so we set it to be 0. ", "### Negative Target"], "code": "# Reference: https://www.kaggle.com/code/xhlulu/exploring-signal-processing-with-scipy\n\n%%time\n# Start with negative target.\nsmoothing = 0\nplt.figure(figsize=(15, 10))\n\nfor phase in range(3):\n    sig = signals[1, phase, :]\n    filtered = signal.qspline1d(sig, smoothing)\n    \n    plt.plot(sig, label=f'Phase {phase} Raw')\n    plt.plot(filtered, label=f'Phase {phase} Filtered')\n\nplt.legend()\nplt.title(f\"Applying Quadratic Spline, Smoothing: {smoothing}\", size=15)\nplt.show()", "processed": ["3 q spline scipi support smooth q spline yet set 0", "neg target"]}, {"markdown": ["### Positive Target"], "code": "# Reference: https://www.kaggle.com/code/xhlulu/exploring-signal-processing-with-scipy\n\nsmoothing = 0\nplt.figure(figsize=(15, 10))\n\nfor phase in range(3):\n    sig = signals[1, phase, :]\n    filtered = signal.qspline1d(sig, smoothing)\n    \n    plt.plot(sig, label=f'Phase {phase} Raw')\n    plt.plot(filtered, label=f'Phase {phase} Filtered')\n\nplt.legend()\nplt.title(f\"Applying Quadratic Spline, Smoothing: {smoothing}\", size=15)\nplt.show()", "processed": ["posit target"]}, {"markdown": ["## 4. Median Filtering", "### Negative Example"], "code": "# Reference: https://www.kaggle.com/code/xhlulu/exploring-signal-processing-with-scipy\n\n%%time\nkernel_size = 1\nplt.figure(figsize=(15, 10))\n\nfor phase in range(3):\n    sig = signals[0, phase, :]\n    filtered = signal.medfilt(sig, kernel_size)\n    \n    plt.plot(sig, label=f'Phase {phase} Raw')\n    plt.plot(filtered, label=f'Phase {phase} Filtered')\n\nplt.legend()\nplt.title(f\"Applying Median Filters, Kernel Size: {kernel_size}\", size=15)\nplt.show()\nkernel_size = 11\nplt.figure(figsize=(15, 10))\n\nfor phase in range(3):\n    sig = signals[0, phase, :]\n    filtered = signal.medfilt(sig, kernel_size)\n    \n    plt.plot(sig, label=f'Phase {phase} Raw')\n    plt.plot(filtered, label=f'Phase {phase} Filtered')\n\nplt.legend()\nplt.title(f\"Applying Median Filters, Kernel Size: {kernel_size}\", size=15)\nplt.show()\nkernel_size = 51\nplt.figure(figsize=(15, 10))\n\nfor phase in range(3):\n    sig = signals[0, phase, :]\n    filtered = signal.medfilt(sig, kernel_size)\n    \n    plt.plot(sig, label=f'Phase {phase} Raw')\n    plt.plot(filtered, label=f'Phase {phase} Filtered')\n\nplt.legend()\nplt.title(f\"Applying Median Filters, Kernel Size: {kernel_size}\", size=15)\nplt.show()\nkernel_size = 101\nplt.figure(figsize=(15, 10))\n\nfor phase in range(3):\n    sig = signals[0, phase, :]\n    filtered = signal.medfilt(sig, kernel_size)\n    \n    plt.plot(sig, label=f'Phase {phase} Raw')\n    plt.plot(filtered, label=f'Phase {phase} Filtered')\n\nplt.legend()\nplt.title(f\"Applying Median Filters, Kernel Size: {kernel_size}\", size=15)\nplt.show()", "processed": ["4 median filter", "neg exampl"]}, {"markdown": ["### Positive Example"], "code": "# Reference: https://www.kaggle.com/code/xhlulu/exploring-signal-processing-with-scipy\n\nkernel_size = 1\nplt.figure(figsize=(15, 10))\n\nfor phase in range(3):\n    sig = signals[1, phase, :]\n    filtered = signal.medfilt(sig, kernel_size)\n    \n    plt.plot(sig, label=f'Phase {phase} Raw')\n    plt.plot(filtered, label=f'Phase {phase} Filtered')\n\nplt.legend()\nplt.title(f\"Applying Median Filters, Kernel Size: {kernel_size}\", size=15)\nplt.show()\nkernel_size = 11\nplt.figure(figsize=(15, 10))\n\nfor phase in range(3):\n    sig = signals[1, phase, :]\n    filtered = signal.medfilt(sig, kernel_size)\n    \n    plt.plot(sig, label=f'Phase {phase} Raw')\n    plt.plot(filtered, label=f'Phase {phase} Filtered')\n\nplt.legend()\nplt.title(f\"Applying Median Filters, Kernel Size: {kernel_size}\", size=15)\nplt.show()\nkernel_size = 51\nplt.figure(figsize=(15, 10))\n\nfor phase in range(3):\n    sig = signals[1, phase, :]\n    filtered = signal.medfilt(sig, kernel_size)\n    \n    plt.plot(sig, label=f'Phase {phase} Raw')\n    plt.plot(filtered, label=f'Phase {phase} Filtered')\n\nplt.legend()\nplt.title(f\"Applying Median Filters, Kernel Size: {kernel_size}\", size=15)\nplt.show()\nkernel_size = 101\nplt.figure(figsize=(15, 10))\n\nfor phase in range(3):\n    sig = signals[1, phase, :]\n    filtered = signal.medfilt(sig, kernel_size)\n    \n    plt.plot(sig, label=f'Phase {phase} Raw')\n    plt.plot(filtered, label=f'Phase {phase} Filtered')\n\nplt.legend()\nplt.title(f\"Applying Median Filters, Kernel Size: {kernel_size}\", size=15)\nplt.show()", "processed": ["posit exampl"]}, {"markdown": ["## 5. Digital Filters (IIR)\n\n#### Butter Filter Design\n\nThis is what we will be using:\n\n> Butterworth digital and analog filter design.\n> Design an Nth-order digital or analog Butterworth filter and return the filter coefficients.\n\nIn this case the Numerator (b) and denominator (a) polynomials of the IIR filter are returned. The following describes the Wn parameter:\n\n> A scalar or length-2 sequence giving the critical frequencies. For a Butterworth filter, this is the point at which the gain drops to 1/sqrt(2) that of the passband (the \u201c-3 dB point\u201d).\n> For digital filters, Wn are in the same units as fs. By default, fs is 2 half-cycles/sample, so these are normalized from 0 to 1, where 1 is the Nyquist frequency. (Wn is thus in half-cycles / sample.)\n> For analog filters, Wn is an angular frequency (e.g. rad/s).\n\nIs Butterworth good for Time Series? Check out [this paper](https://amstat.tandfonline.com/doi/abs/10.1198/073500101681019909). Here's the abstract:\n\n> Long-term trends and business cycles are usually estimated by applying the Hodrick and Prescott (HP) filter to X-11 seasonally adjusted data. A two-stage procedure is proposed in this article to improve this methodology. The improvement is based on (a) using Butterworth or band-pass filters specifically designed for the problem at hand as an alternative to the HP filter, (b) applying the selected filter to estimated trend cycles instead of to seasonally adjusted series, and (c) using autoregressive integrated moving average models to extend the input series with forecasts and backcasts.", "### Negative Example"], "code": "# Reference: https://www.kaggle.com/code/xhlulu/exploring-signal-processing-with-scipy\n\n%%time\nWn = 0.50\nplt.figure(figsize=(15, 10))\n\nfor phase in range(3):\n    sig = signals[0, phase, :]\n    \n    b, a = signal.butter(3, Wn)\n    filtered = signal.filtfilt(b, a, sig)\n    \n    plt.plot(sig, label=f'Phase {phase} Raw')\n    plt.plot(filtered, label=f'Phase {phase} Filtered')\n\nplt.legend()\nplt.title(f\"Applying IIR Filtering with Butterworth, Wn: {Wn}\", size=15)\nplt.show()\nWn = 0.05\nplt.figure(figsize=(15, 10))\n\nfor phase in range(3):\n    sig = signals[0, phase, :]\n    \n    b, a = signal.butter(3, Wn)\n    filtered = signal.filtfilt(b, a, sig)\n    \n    plt.plot(sig, label=f'Phase {phase} Raw')\n    plt.plot(filtered, label=f'Phase {phase} Filtered')\n\nplt.legend()\nplt.title(f\"Applying IIR Filtering with Butterworth, Wn: {Wn}\", size=15)\nplt.show()\nWn = 0.01\nplt.figure(figsize=(15, 10))\n\nfor phase in range(3):\n    sig = signals[0, phase, :]\n    \n    b, a = signal.butter(3, Wn)\n    filtered = signal.filtfilt(b, a, sig)\n    \n    plt.plot(sig, label=f'Phase {phase} Raw')\n    plt.plot(filtered, label=f'Phase {phase} Filtered')\n\nplt.legend()\nplt.title(f\"Applying IIR Filtering with Butterworth, Wn: {Wn}\", size=15)\nplt.show()\n", "processed": ["5 digit filter iir butter filter design use butterworth digit analog filter design design nth order digit analog butterworth filter return filter coeffici case numer b denomin polynomi iir filter return follow describ wn paramet scalar length 2 sequenc give critic frequenc butterworth filter point gain drop 1 sqrt 2 passband 3 db point digit filter wn unit f default f 2 half cycl sampl normal 0 1 1 nyquist frequenc wn thu half cycl sampl analog filter wn angular frequenc e g rad butterworth good time seri check paper http amstat tandfonlin com doi ab 10 1198 073500101681019909 abstract long term trend busi cycl usual estim appli hodrick prescott hp filter x 11 season adjust data two stage procedur propos articl improv methodolog improv base use butterworth band pas filter specif design problem hand altern hp filter b appli select filter estim trend cycl instead season adjust seri c use autoregress integr move averag model extend input seri forecast backcast", "neg exampl"]}, {"markdown": ["**Plot Labels freq**"], "code": "# Reference: https://www.kaggle.com/code/jesucristo/analysis-youtube8m-2019\n\nlabels_count_dict = dict(top_n)\nlabels_count_df = pd.DataFrame.from_dict(labels_count_dict, orient='index').reset_index()\nlabels_count_df.columns = ['label', 'count']\nlabels_count_df['label'] = labels_count_df['label'].map(label_mapping, na_action='ignore')\nTOP_labels = list(labels_count_df['label'])[:n]\nfig, ax = plt.subplots(figsize=(10,7))\nsns.barplot(y='label', x='count', data=labels_count_df)\nplt.title('Top {} labels with sample count'.format(n))", "processed": ["plot label freq"]}, {"markdown": ["<br>\n### Vertical 1"], "code": "# Reference: https://www.kaggle.com/code/jesucristo/analysis-youtube8m-2019\n\nplt.figure(figsize = (10,8))\nvocabulary.groupby('Vertical1').TrainVideoCount.sum().plot(kind=\"bar\")\nplt.title(\"Average TrainVideoCount per vertical1\")\nplt.show()\n\nplt.figure(figsize = (10,8))\nvocabulary.groupby('Vertical1').Index.count().plot(kind=\"bar\")\nplt.title(\"Average number video per vertical1\")\nplt.show()", "processed": ["br vertic 1"]}, {"markdown": ["### Vertical 2"], "code": "# Reference: https://www.kaggle.com/code/jesucristo/analysis-youtube8m-2019\n\nplt.figure(figsize = (10,8))\nvocabulary.groupby('Vertical2').TrainVideoCount.sum().plot(kind=\"bar\")\nplt.title(\"Average TrainVideoCount per vertical2\")\nplt.show()\n\nplt.figure(figsize = (10,8))\nvocabulary.groupby('Vertical2').TrainVideoCount.count().plot(kind=\"bar\")\nplt.title(\"Average video number per vertical2\")\nplt.show()", "processed": ["vertic 2"]}, {"markdown": ["### Vertical 3"], "code": "# Reference: https://www.kaggle.com/code/jesucristo/analysis-youtube8m-2019\n\nplt.figure(figsize = (10,8))\nvocabulary.groupby('Vertical3').TrainVideoCount.sum().plot(kind=\"bar\")\nplt.title(\"Average TrainVideoCount per vertical3\")\nplt.show()\n\nplt.figure(figsize = (10,8))\nvocabulary.groupby('Vertical3').TrainVideoCount.count().plot(kind=\"bar\")\nplt.title(\"Average video number per vertical3\")\nplt.show()", "processed": ["vertic 3"]}, {"markdown": ["### Plot data and regression model"], "code": "# Reference: https://www.kaggle.com/code/jesucristo/analysis-youtube8m-2019\n\nsns.lmplot(x='Index', y='TrainVideoCount', data=vocabulary , size=15)", "processed": ["plot data regress model"]}, {"markdown": ["### 5.9 LGBM <a class=\"anchor\" id=\"5.9\"></a>\n\n[Back to Table of Contents](#0.1)", "**Light GBM** is a fast, distributed, high-performance gradient boosting framework based on decision tree algorithms. It splits the tree leaf wise with the best fit whereas other boosting algorithms split the tree depth wise or level wise rather than leaf-wise. So when growing on the same leaf in Light GBM, the leaf-wise algorithm can reduce more loss than the level-wise algorithm and hence results in much better accuracy which can rarely be achieved by any of the existing boosting algorithms. Also, it is surprisingly very fast, hence the word \u2018Light\u2019. Reference [Analytics Vidhya](https://www.analyticsvidhya.com/blog/2017/06/which-algorithm-takes-the-crown-light-gbm-vs-xgboost/)."], "code": "# Reference: https://www.kaggle.com/code/vbmokin/bod-prediction-in-river-by-15-regression-models\n\n#%% split training set to validation set\nXtrain, Xval, Ztrain, Zval = train_test_split(trainb, targetb, test_size=0.2, random_state=0)\ntrain_set = lgb.Dataset(Xtrain, Ztrain, silent=False)\nvalid_set = lgb.Dataset(Xval, Zval, silent=False)\nparams = {\n        'boosting_type':'gbdt',\n        'objective': 'regression',\n        'num_leaves': 31,\n        'learning_rate': 0.01,\n        'max_depth': -1,\n        'subsample': 0.8,\n        'bagging_fraction' : 1,\n        'max_bin' : 5000 ,\n        'bagging_freq': 20,\n        'colsample_bytree': 0.6,\n        'metric': 'rmse',\n        'min_split_gain': 0.5,\n        'min_child_weight': 1,\n        'min_child_samples': 10,\n        'scale_pos_weight':1,\n        'zero_as_missing': False,\n        'seed':0,        \n    }\nmodelL = lgb.train(params, train_set = train_set, num_boost_round=10000,\n                   early_stopping_rounds=2000,verbose_eval=500, valid_sets=valid_set)\nacc_boosting_model(8,modelL,trainb,testb,modelL.best_iteration)\nfig =  plt.figure(figsize = (5,5))\naxes = fig.add_subplot(111)\nlgb.plot_importance(modelL,ax = axes,height = 0.5)\nplt.show();\nplt.close()", "processed": ["5 9 lgbm class anchor id 5 9 back tabl content 0 1", "light gbm fast distribut high perform gradient boost framework base decis tree algorithm split tree leaf wise best fit wherea boost algorithm split tree depth wise level wise rather leaf wise grow leaf light gbm leaf wise algorithm reduc loss level wise algorithm henc result much better accuraci rare achiev exist boost algorithm also surprisingli fast henc word light refer analyt vidhya http www analyticsvidhya com blog 2017 06 algorithm take crown light gbm v xgboost"]}, {"markdown": ["## 6. Models comparison <a class=\"anchor\" id=\"6\"></a>\n\n[Back to Table of Contents](#0.1)", "We can now compare our models and to choose the best one for our problem."], "code": "# Reference: https://www.kaggle.com/code/vbmokin/bod-prediction-in-river-by-15-regression-models\n\nmodels = pd.DataFrame({\n    'Model': ['Linear Regression', 'Support Vector Machines', 'Linear SVR', \n              'MLPRegressor', 'Stochastic Gradient Decent', \n              'Decision Tree Regressor', 'Random Forest',  'XGB', 'LGBM',\n              'GradientBoostingRegressor', 'RidgeRegressor', 'BaggingRegressor', 'ExtraTreesRegressor', \n              'AdaBoostRegressor', 'VotingRegressor'],\n    \n    'r2_train': acc_train_r2,\n    'r2_test': acc_test_r2,\n    'd_train': acc_train_d,\n    'd_test': acc_test_d,\n    'rmse_train': acc_train_rmse,\n    'rmse_test': acc_test_rmse\n                     })\npd.options.display.float_format = '{:,.2f}'.format\nprint('Prediction accuracy for models by R2 criterion - r2_test')\nmodels.sort_values(by=['r2_test', 'r2_train'], ascending=False)\nprint('Prediction accuracy for models by relative error - d_test')\nmodels.sort_values(by=['d_test', 'd_train'], ascending=True)\nprint('Prediction accuracy for models by RMSE - rmse_test')\nmodels.sort_values(by=['rmse_test', 'rmse_train'], ascending=True)\n# Plot\nplt.figure(figsize=[25,6])\nxx = models['Model']\nplt.tick_params(labelsize=14)\nplt.plot(xx, models['r2_train'], label = 'r2_train')\nplt.plot(xx, models['r2_test'], label = 'r2_test')\nplt.legend()\nplt.title('R2-criterion for 15 popular models for train and test datasets')\nplt.xlabel('Models')\nplt.ylabel('R2-criterion, %')\nplt.xticks(xx, rotation='vertical')\nplt.savefig('graph.png')\nplt.show()\n# Plot\nplt.figure(figsize=[25,6])\nxx = models['Model']\nplt.tick_params(labelsize=14)\nplt.plot(xx, models['d_train'], label = 'd_train')\nplt.plot(xx, models['d_test'], label = 'd_test')\nplt.legend()\nplt.title('Relative errors for 15 popular models for train and test datasets')\nplt.xlabel('Models')\nplt.ylabel('Relative error, %')\nplt.xticks(xx, rotation='vertical')\nplt.savefig('graph.png')\nplt.show()\n# Plot\nplt.figure(figsize=[25,6])\nxx = models['Model']\nplt.tick_params(labelsize=14)\nplt.plot(xx, models['rmse_train'], label = 'rmse_train')\nplt.plot(xx, models['rmse_test'], label = 'rmse_test')\nplt.legend()\nplt.title('RMSE for 15 popular models for train and test datasets')\nplt.xlabel('Models')\nplt.ylabel('RMSE, %')\nplt.xticks(xx, rotation='vertical')\nplt.savefig('graph.png')\nplt.show()", "processed": ["6 model comparison class anchor id 6 back tabl content 0 1", "compar model choos best one problem"]}, {"markdown": ["## Target varaible distribution"], "code": "# Reference: https://www.kaggle.com/code/tunguz/logistic-regression-tfidf\n\nfig,ax = plt.subplots(2,3,figsize=(16,10))\nax1,ax2,ax3,ax4,ax5,ax6 = ax.flatten()\nsns.countplot(train['toxic'],palette= 'magma',ax=ax1)\nsns.countplot(train['severe_toxic'], palette= 'viridis',ax=ax2)\nsns.countplot(train['obscene'], palette= 'Set1',ax=ax3)\nsns.countplot(train['threat'], palette= 'viridis',ax = ax4)\nsns.countplot(train['insult'], palette = 'magma',ax=ax5)\nsns.countplot(train['identity_hate'], palette = 'Set1', ax = ax6)", "processed": ["target varaibl distribut"]}, {"markdown": ["## Roc AUC curve"], "code": "# Reference: https://www.kaggle.com/code/tunguz/logistic-regression-tfidf\n\ncol = 'identity_hate'\nprint(\"Column:\",col)\npred_pro = lr.predict_proba(X)[:,1]\nfrp,trp,thres = roc_curve(y[col],pred_pro)\nauc_val =auc(frp,trp)\nplt.figure(figsize=(14,10))\nplt.plot([0,1],[0,1],color='b')\nplt.plot(frp,trp,color='r',label= 'AUC = %.2f'%auc_val)\nplt.legend(loc='lower right')\nplt.xlabel('True positive rate')\nplt.ylabel('False positive rate')\nplt.title('Reciever Operating Characteristic')", "processed": ["roc auc curv"]}, {"markdown": ["## PoS Count Features Analysis\n#### *Note : I have left out tags that don't provide useful insights*", "### Define function to visualize count features with KDE distribution plots", "#### *Note : Non-toxic in orange and Toxic in blue*"], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/jigsaw-competition-part-of-speech-tagging\n\ndef visualize_count_feature(tag):\n    df.pos_feature = [count_pos(counts[i], tag) for i in range(SAMPLE_SIZE)]\n    df.pos_feature = df.pos_feature.mask(df.pos_feature == 0, np.nan) # Ignore sample when tag not present in sentence\n\n    fig, ax = plt.subplots(figsize=(10, 10))\n    sns.distplot(ax=ax, a=[count for count in df.pos_feature.loc[df.target<0.5] if count==count], color='darkorange', label='non-toxic', hist=False)\n    sns.distplot(ax=ax, a=[count for count in df.pos_feature.loc[df.target>0.5] if count==count], color='navy', label='toxic', hist=False)\n    plt.title('\" ' + tag + ' \" ' + 'PoS tag count', fontsize=16, color='maroon')\n\n    plt.show()", "processed": ["po count featur analysi note left tag provid use insight", "defin function visual count featur kde distribut plot", "note non toxic orang toxic blue"]}, {"markdown": ["A part of the features with very high missing data percent in the **train** data have a lower percent of the missing data in the **test** set (~93%) and also there are some that are not appearing in the list with fields with missing values in the **test**.     \n\n\nWe can also see that there are fields that does appears only in the **train** and  set, for example  **trafficSource_campaignCode** . We will have to consider these aspects when we will decide what features to drop and what features to keep for the predictive model.\n\n\n<a href=\"#0\"><font size=\"1\">Go to top</font></a>", "## <a id=\"32\">Channel Grouping</a>\n\nLet's check the channelGrouping data distribution. We will only show the first 30 categories, where more than that."], "code": "# Reference: https://www.kaggle.com/code/gpreda/google-analytics-customer-revenue-extensive-eda\n\ndef get_categories(data, val):\n    tmp = data[val].value_counts()\n    return pd.DataFrame(data={'Number': tmp.values}, index=tmp.index).reset_index()\ndef draw_trace_bar(data, title, xlab, ylab,color='Blue'):\n    trace = go.Bar(\n            x = data.head(30)['index'],\n            y = data.head(30)['Number'],\n            marker=dict(color=color),\n            text=data.head(30)['index']\n        )\n    data = [trace]\n\n    layout = dict(title = title,\n              xaxis = dict(title = xlab, showticklabels=True, tickangle=15,\n                          tickfont=dict(\n                            size=9,\n                            color='black'),), \n              yaxis = dict(title = ylab),\n              hovermode = 'closest'\n             )\n    fig = dict(data = data, layout = layout)\n    iplot(fig, filename='draw_trace')\ndraw_trace_bar(get_categories(train_df,'channelGrouping'), \"Channel grouping\", \"Channel grouping\", \"Number\", \"Lightblue\")", "processed": ["part featur high miss data percent train data lower percent miss data test set 93 also appear list field miss valu test also see field appear train set exampl trafficsourc campaigncod consid aspect decid featur drop featur keep predict model href 0 font size 1 go top font", "id 32 channel group let check channelgroup data distribut show first 30 categori"]}, {"markdown": ["Not all the cities, network domains, metropolitan areas, continents are set.   \nMost numerous cities are not available in the dataset, as well as most metropolitan areas or network domains.  \nThe continent with the largest number of visits is America. The sub-continent with the largest number of visits is Northern America.", "Let's also show the geographical features on a plotly map. We will show the country feature distribution."], "code": "# Reference: https://www.kaggle.com/code/gpreda/google-analytics-customer-revenue-extensive-eda\n\ntmp = train_df['geoNetwork_country'].value_counts()\ncountry_visits = pd.DataFrame(data={'geoNetwork_country': tmp.values}, index=tmp.index).reset_index()\ncountry_visits.columns = ['Country', 'Visits']\ndef plot_country_map(data, location, z, legend, title, colormap='Rainbow'):\n    data = dict(type = 'choropleth', \n                colorscale = colormap,\n                autocolorscale = False,\n                reversescale = False,\n               locations = data[location],\n               locationmode = 'country names',\n               z = data[z], \n               text = data[z],\n               colorbar = {'title':legend})\n    layout = dict(title = title, \n                 geo = dict(showframe = False, \n                         projection = {'type': 'natural earth'}))\n    choromap = go.Figure(data = [data], layout=layout)\n    iplot(choromap)\nplot_country_map(country_visits, 'Country', 'Visits', 'Visits', 'Visits per country')", "processed": ["citi network domain metropolitan area contin set numer citi avail dataset well metropolitan area network domain contin largest number visit america sub contin largest number visit northern america", "let also show geograph featur plotli map show countri featur distribut"]}, {"markdown": ["For **totals_transactionRevenue**, let's also show the values distribution."], "code": "# Reference: https://www.kaggle.com/code/gpreda/google-analytics-customer-revenue-extensive-eda\n\ntrain_df['totals_transactionRevenue'] = pd.to_numeric(train_df['totals_transactionRevenue'])\ndf = train_df[train_df['totals_transactionRevenue'] > 0]['totals_transactionRevenue']\nf, ax = plt.subplots(1,1, figsize=(16,4))\nplt.title(\"Distribution of totals: transaction revenue\")\nsns.kdeplot(df, color=\"green\")\nplt.tick_params(axis='both', which='major', labelsize=12)\nplt.ylabel('Density plot', fontsize=12)\nplt.xlabel('Transaction revenue', fontsize=12)\nlocs, labels = plt.xticks()\nplt.show()", "processed": ["total transactionrevenu let also show valu distribut"]}, {"markdown": ["Let's check as well the log of the total transaction revenue."], "code": "# Reference: https://www.kaggle.com/code/gpreda/google-analytics-customer-revenue-extensive-eda\n\nplt.figure(figsize=(12,6))\nsns.distplot(np.log1p(df),color=\"darkgreen\",bins=50)\nplt.xlabel(\"Log(total transaction revenue)\");\nplt.title(\"Logarithmic distribution of total transaction revenue (non-zeros)\");", "processed": ["let check well log total transact revenu"]}, {"markdown": ["Let's plot the number of visits vs. date  and the amount of transaction revenues vs. date for the train set.\n\nFirst, let's show the number of visits finalized with a transaction per day."], "code": "# Reference: https://www.kaggle.com/code/gpreda/google-analytics-customer-revenue-extensive-eda\n\ndef plot_scatter_data(data, xtitle, ytitle, title, color='blue'):\n    trace = go.Scatter(\n        x = data.index,\n        y = data.values,\n        name=ytitle,\n        marker=dict(\n            color=color,\n        ),\n        mode='lines+markers'\n    )\n    data = [trace]\n    layout = dict(title = title,\n              xaxis = dict(title = xtitle), yaxis = dict(title = ytitle),\n             )\n    fig = dict(data=data, layout=layout)\n    iplot(fig, filename='lines')\ncount_all = train_df.groupby('date')['totals_transactionRevenue'].agg(['size'])\ncount_all.columns = [\"Total\"]\ncount_all = count_all.sort_index()\nplot_scatter_data(count_all['Total'],'Date', 'Total','Total count of visits (including zero transactions)','green')", "processed": ["let plot number visit v date amount transact revenu v date train set first let show number visit final transact per day"]}, {"markdown": ["Let's plot the total amount of non-zero transactions per day,  grouped by **channelGrouping**."], "code": "# Reference: https://www.kaggle.com/code/gpreda/google-analytics-customer-revenue-extensive-eda\n\nchannels = list(train_df['channelGrouping'].unique())\ndata = []\nfor channel in channels:\n    subset = train_df[train_df['channelGrouping']==channel]\n    subset = subset.groupby('date')['totals_transactionRevenue'].agg(['sum'])\n    subset.columns = [\"Total\"]\n    subset = subset.sort_index()\n    trace = go.Scatter(\n        x = subset['Total'].index,\n        y = subset['Total'].values,\n        name=channel,\n        mode='lines'\n    )\n    data.append(trace)\nlayout= go.Layout(\n    title= 'Total amount of non-zero transactions per day, grouped by channel',\n    xaxis = dict(title = 'Date'), yaxis = dict(title = 'Total'),\n    showlegend=True,\n)\nfig = dict(data=data, layout=layout)\niplot(fig, filename='lines')", "processed": ["let plot total amount non zero transact per day group channelgroup"]}, {"markdown": ["Let's plot the total amount of non-zero transactions per day,  grouped by **device_operatingSystem**."], "code": "# Reference: https://www.kaggle.com/code/gpreda/google-analytics-customer-revenue-extensive-eda\n\nopsys = list(train_df['device_operatingSystem'].unique())\ndata = []\nfor os in opsys:\n    subset = train_df[train_df['device_operatingSystem']==os]\n    subset = subset.groupby('date')['totals_transactionRevenue'].agg(['sum'])\n    subset.columns = [\"Total\"]\n    subset = subset.sort_index()\n    trace = go.Scatter(\n        x = subset['Total'].index,\n        y = subset['Total'].values,\n        name=os,\n        mode='lines'\n    )\n    data.append(trace)\nlayout= go.Layout(\n    title= 'Total amount of non-zero transactions per day, grouped by OS',\n    xaxis = dict(title = 'Date'), yaxis = dict(title = 'Total'),\n    showlegend=True,\n)\nfig = dict(data=data, layout=layout)\niplot(fig, filename='lines')", "processed": ["let plot total amount non zero transact per day group devic operatingsystem"]}, {"markdown": ["No obvious missing values in train and test. Perhaps there are some non-obvious once but let's move this topic to exploratory data analysis.", "## Basic exploratory analysis\n\nOk, now the colorful part starts. We have already found an interesting feature by peeking at the data. Perhaps we can find some more during basic EDA. Let's stay curious and critical! Do we know if test and train behave the same? No! For this reason, I don't like to combine train and test right now... :-)", "### Class balance"], "code": "# Reference: https://www.kaggle.com/code/allunia/instant-gratification-some-eda-to-go\n\nsns.countplot(train.target, palette=\"Set2\");", "processed": ["obviou miss valu train test perhap non obviou let move topic exploratori data analysi", "basic exploratori analysi ok color part start alreadi found interest featur peek data perhap find basic eda let stay curiou critic know test train behav reason like combin train test right", "class balanc"]}, {"markdown": ["Very balanced in train!", "### Feature correlation"], "code": "# Reference: https://www.kaggle.com/code/allunia/instant-gratification-some-eda-to-go\n\nmagic = \"wheezy-copper-turtle-magic\"\ntrain_corr = train.drop([\"target\", magic], axis=1).corr()\ntest_corr = test.drop(magic, axis=1).corr()\ntrain_corr_flat = train_corr.values.flatten()\ntrain_corr_flat = train_corr_flat[train_corr_flat != 1]\n\ntest_corr_flat = test_corr.values.flatten()\ntest_corr_flat = test_corr_flat[test_corr_flat != 1]\n\nfig, ax = plt.subplots(1,2,figsize=(20,5))\nsns.distplot(train_corr_flat, ax=ax[0], color=\"tomato\")\nsns.distplot(test_corr_flat, ax=ax[1], color=\"limegreen\");\nax[0].set_title(\"Off-diagonal train corr \\n distribution\")\nax[1].set_title(\"Off-diagonal test corr \\n distribution\");\nax[0].set_xlabel(\"feature correlation value\")\nax[1].set_xlabel(\"feature correlation value\");", "processed": ["balanc train", "featur correl"]}, {"markdown": [":-o What's that? Wheezy-copper-turtle-magic... again!"], "code": "# Reference: https://www.kaggle.com/code/allunia/instant-gratification-some-eda-to-go\n\nfig, ax = plt.subplots(2,2,figsize=(20,10))\nsns.distplot(train.loc[train.target==0, \"wheezy-myrtle-mandrill-entropy\"], color=\"Blue\", ax=ax[0,0])\nsns.distplot(train.loc[train.target==1, \"wheezy-myrtle-mandrill-entropy\"], color=\"Red\", ax=ax[0,0])\nsns.distplot(train.loc[train.target==0, \"wheezy-copper-turtle-magic\"], color=\"Blue\", ax=ax[0,1])\nsns.distplot(train.loc[train.target==1, \"wheezy-copper-turtle-magic\"], color=\"Red\", ax=ax[0,1])\nax[1,0].scatter(train[\"wheezy-myrtle-mandrill-entropy\"].values,\n                train[\"skanky-carmine-rabbit-contributor\"].values, c=train.target.values,\n                cmap=\"coolwarm\", s=1, alpha=0.5)\nax[1,0].set_xlabel(\"wheezy-myrtle-mandrill-entropy\")\nax[1,0].set_ylabel(\"skanky-carmine-rabbit-contributor\")\nax[1,1].scatter(train[\"wheezy-myrtle-mandrill-entropy\"].values,\n                train[\"wheezy-copper-turtle-magic\"].values, c=train.target.values,\n                cmap=\"coolwarm\", s=1, alpha=0.5)\nax[1,1].set_xlabel(\"wheezy-myrtle-mandrill-entropy\")\nax[1,1].set_ylabel(\"wheezy-copper-turtle-magic\");", "processed": ["wheezi copper turtl magic"]}, {"markdown": ["As Chris already pointed out, we won't get far by ignoring the pattern found with our magic turtle. ", "### The magic turtle again\n\nCan't get enough! :-) Let's look at a 3D-Scatterplot with magic turtle:"], "code": "# Reference: https://www.kaggle.com/code/allunia/instant-gratification-some-eda-to-go\n\nfeat1 = \"wheezy-myrtle-mandrill-entropy\"\nfeat2 = \"skanky-carmine-rabbit-contributor\"\nfeat3 = \"wheezy-copper-turtle-magic\"\nN = 10000\n\ntrace1 = go.Scatter3d(\n    x=train[feat1].values[0:N], \n    y=train[feat2].values[0:N],\n    z=train[feat3].values[0:N],\n    mode='markers',\n    marker=dict(\n        color=train.target.values[0:N],\n        colorscale = \"Jet\",\n        opacity=0.3,\n        size=2\n    )\n)\n\nfigure_data = [trace1]\nlayout = go.Layout(\n    title = 'The turtle place',\n    scene = dict(\n        xaxis = dict(title=feat1),\n        yaxis = dict(title=feat2),\n        zaxis = dict(title=feat3),\n    ),\n    margin=dict(\n        l=0,\n        r=0,\n        b=0,\n        t=0\n    ),\n    showlegend=True\n)\n\nfig = go.Figure(data=figure_data, layout=layout)\npy.iplot(fig, filename='simple-3d-scatter')\nfig, ax = plt.subplots(5,5,figsize=(20,25))\nfor turtle1 in range(5):\n    for turtle2 in range(5):\n        my_turtle=turtle2+turtle1*5\n        ax[turtle1, turtle2].scatter(train.loc[train[\"wheezy-copper-turtle-magic\"]==my_turtle, feat1].values,\n                                     train.loc[train[\"wheezy-copper-turtle-magic\"]==my_turtle, feat2].values,\n                                     c=train.loc[train[\"wheezy-copper-turtle-magic\"]==my_turtle, \"target\"].values, cmap=\"coolwarm\", s=5, alpha=0.5)\n        ax[turtle1, turtle2].set_xlim([-15,15])\n        ax[turtle1, turtle2].set_ylim([-15,15])", "processed": ["chri alreadi point get far ignor pattern found magic turtl", "magic turtl get enough let look 3d scatterplot magic turtl"]}, {"markdown": ["Let's follow the idea that these names indeed have some meaning... in this case: what are features to discard? Is there one more magic feature and what makes it magically?"], "code": "# Reference: https://www.kaggle.com/code/allunia/instant-gratification-some-eda-to-go\n\nfeature_names = pd.DataFrame(index=train.drop([\"target\", \"id\"], axis=1).columns.values, data=first_names, columns=[\"kind\"])\nfeature_names[\"color\"] = second_names\nfeature_names[\"animal\"] = third_names\nfeature_names[\"goal\"] = fourth_names\nfeature_names.head()\nplt.figure(figsize=(20,5))\nsns.countplot(x=\"kind\", data=feature_names, order=feature_names.kind.value_counts().index, palette=\"Greens_r\")\nplt.xticks(rotation=90);\nplt.figure(figsize=(20,5))\nsns.countplot(x=\"animal\", data=feature_names, order=feature_names.animal.value_counts().index, palette=\"Oranges_r\")\nplt.xticks(rotation=90);\nplt.figure(figsize=(20,5))\nsns.countplot(x=\"color\", data=feature_names, order=feature_names.color.value_counts().index, palette=\"Purples_r\")\nplt.xticks(rotation=90);\nplt.figure(figsize=(20,5))\nsns.countplot(x=\"goal\", data=feature_names, order=feature_names.goal.value_counts().index, palette=\"Reds_r\")\nplt.xticks(rotation=90);\nfeature_names[feature_names.goal==\"learn\"]", "processed": ["let follow idea name inde mean case featur discard one magic featur make magic"]}, {"markdown": ["## Diving into single datasets\n\nHow many single rows do we have given a single magic turtle value? Do all subsets in the data have the same amount of samples?"], "code": "# Reference: https://www.kaggle.com/code/allunia/instant-gratification-some-eda-to-go\n\nn_subsamples_test = test.groupby(magic).size() \nn_subsamples_train = train.groupby(magic).size() \n\nplt.figure(figsize=(20,5))\nplt.plot(n_subsamples_test.values, '.-', label=\"test\")\nplt.plot(n_subsamples_train.values, '.-', label=\"train\")\nplt.plot(n_subsamples_test.values + n_subsamples_train.values, '.-', label=\"total\")\nplt.legend();\nplt.xlabel(magic)\nplt.ylabel(\"sample count\");", "processed": ["dive singl dataset mani singl row given singl magic turtl valu subset data amount sampl"]}, {"markdown": ["## UPDATE: How Lucky Were We?\nHip hip hooray! `np.random.seed(42)` chose well. After the competition ended, I made the plot below showing this kernel's 30 public and private LB scores. The two randomly chosen final submissions are versions 6 and 19 colored green below. You can see that 19 has the best private LB score :-)\n\nThe highest **public** LB is 0.97481 achieved by version 25. The lowest public LB is 0.97439 by version 7. The highest **private** LB is 0.97588 by version 19. The lowest private LB is 0.97543 by version 23. The black dotted lines represent this kernel's public and private prediction averages. (For more info about this plot and the above plot, see Appendix 3)."], "code": "# Reference: https://www.kaggle.com/code/cdeotte/3-clusters-per-class-0-975\n\nimport matplotlib.pyplot as plt, numpy as np\npu = np.array([68,53,70,67,54,54,39,68,60,65,46,62,62,55,54,\n               60,59,55,43,52,63,68,51,75,81,56,68,55,60,48])\npr = np.array([58,54,68,54,48,61,59,49,60,57,70,54,53,69,72,\n               56,64,44,88,63,74,70,43,48,77,65,51,70,51,48])\nplt.scatter(0.974+pu/1e5,0.975+pr/1e5)\nplt.scatter(0.974+pu[5]/1e5,0.975+pr[5]/1e5,color='green',s=100)\nplt.scatter(0.974+pu[18]/1e5,0.975+pr[18]/1e5,color='green',s=100)\n\nmpu = 0.974 + np.mean(pu)/1e5\nmpr = 0.975 + np.mean(pr)/1e5\nplt.plot([mpu,mpu],[mpr-0.0005,mpr+0.0005],':k')\nplt.plot([mpu-0.0005,mpu+0.0005],[mpr,mpr],':k')\n\nplt.xlabel('Public LB'); plt.xlim((mpu-0.0005,mpu+0.0005))\nplt.ylabel('Private LB'); plt.ylim((mpr-0.0005,mpr+0.0005))\nplt.title(\"Public and private LB scores from 30 runs of this kernel.\\n \\\n    The green dots were the two randomly chosen submissions\")\nplt.show()", "processed": ["updat lucki hip hip hooray np random seed 42 chose well competit end made plot show kernel 30 public privat lb score two randomli chosen final submiss version 6 19 color green see 19 best privat lb score highest public lb 0 97481 achiev version 25 lowest public lb 0 97439 version 7 highest privat lb 0 97588 version 19 lowest privat lb 0 97543 version 23 black dot line repres kernel public privat predict averag info plot plot see appendix 3"]}, {"markdown": ["# First, let see the distribuition of transactions Revenues\n\nI will start exploring the quantile "], "code": "# Reference: https://www.kaggle.com/code/kabure/exploring-the-consumer-patterns-ml-pipeline\n\n# Printing some statistics of our data\nprint(\"Transaction Revenue Min Value: \", \n      df_train[df_train['totals.transactionRevenue'] > 0][\"totals.transactionRevenue\"].min()) # printing the min value\nprint(\"Transaction Revenue Mean Value: \", \n      df_train[df_train['totals.transactionRevenue'] > 0][\"totals.transactionRevenue\"].mean()) # mean value\nprint(\"Transaction Revenue Median Value: \", \n      df_train[df_train['totals.transactionRevenue'] > 0][\"totals.transactionRevenue\"].median()) # median value\nprint(\"Transaction Revenue Max Value: \", \n      df_train[df_train['totals.transactionRevenue'] > 0][\"totals.transactionRevenue\"].max()) # the max value\n\n# It I did to plot the quantiles but are not working\n#print(round(df_train['totals.transactionRevenue'].quantile([.025,.25,.5,.75,.975]),2))\n\n# seting the figure size of our plots\nplt.figure(figsize=(14,5))\n\n# Subplot allow us to plot more than one \n# in this case, will be create a subplot grid of 2 x 1\nplt.subplot(1,2,1)\n# seting the distribuition of our data and normalizing using np.log on values highest than 0 and + \n# also, we will set the number of bins and if we want or not kde on our histogram\nax = sns.distplot(np.log(df_train[df_train['totals.transactionRevenue'] > 0][\"totals.transactionRevenue\"] + 0.01), bins=40, kde=True)\nax.set_xlabel('Transaction RevenueLog', fontsize=15) #seting the xlabel and size of font\nax.set_ylabel('Distribuition', fontsize=15) #seting the ylabel and size of font\nax.set_title(\"Distribuition of Revenue Log\", fontsize=20) #seting the title and size of font\n\n# setting the second plot of our grid of graphs\nplt.subplot(1,2,2)\n# ordering the total of users and seting the values of transactions to understanding \nplt.scatter(range(df_train.shape[0]), np.sort(df_train['totals.transactionRevenue'].values))\nplt.xlabel('Index', fontsize=15) # xlabel and size of words\nplt.ylabel('Revenue value', fontsize=15) # ylabel and size of words\nplt.title(\"Revenue Value Distribution\", fontsize=20) # Setting Title and fontsize\n\nplt.show()", "processed": ["first let see distribuit transact revenu start explor quantil"]}, {"markdown": ["# Device Browsers"], "code": "# Reference: https://www.kaggle.com/code/kabure/exploring-the-consumer-patterns-ml-pipeline\n\n# the top 10 of browsers represent % of total\nprint(\"Percentual of Browser usage: \")\nprint(df_train['device.browser'].value_counts()[:7] ) # printing the top 7 percentage of browsers\n\n# seting the graph size\nplt.figure(figsize=(14,6))\n\n# Let explore the browser used by users\nsns.countplot(df_train[df_train['device.browser']\\\n                       .isin(df_train['device.browser']\\\n                             .value_counts()[:10].index.values)]['device.browser'], palette=\"hls\") # It's a module to count the category's\nplt.title(\"TOP 10 Most Frequent Browsers\", fontsize=20) # Adding Title and seting the size\nplt.xlabel(\"Browser Names\", fontsize=16) # Adding x label and seting the size\nplt.ylabel(\"Count\", fontsize=16) # Adding y label and seting the size\nplt.xticks(rotation=45) # Adjust the xticks, rotating the labels\n\nplt.show() #use plt.show to render the graph that we did above", "processed": ["devic browser"]}, {"markdown": ["### In our top 5 browsers we have more than 94% of total\n- TOP 1 - CHROME - 69,08%\n- TOP 2 - SAFARI - 20,04%\n- TOP 3 - FIREFOX - 3,77%\n\nNothing new under the sun... Chrome is the most used followed by Safari and firefox.\n", "## What if we cross the Revenue and Browser?\n"], "code": "# Reference: https://www.kaggle.com/code/kabure/exploring-the-consumer-patterns-ml-pipeline\n\nplt.figure(figsize=(13,6)) #figure size\n\n#It's another way to plot our data. using a variable that contains the plot parameters\ng1 = sns.boxenplot(x='device.browser', y='totals.transactionRevenue', \n                   data=df_train[(df_train['device.browser'].isin((df_train['device.browser'].value_counts()[:10].index.values))) &\n                                  df_train['totals.transactionRevenue'] > 0])\ng1.set_title('Browsers Name by Transactions Revenue', fontsize=20) # title and fontsize\ng1.set_xticklabels(g1.get_xticklabels(),rotation=45) # It's the way to rotate the xticks when we use variable to our graphs\ng1.set_xlabel('Device Names', fontsize=18) # Xlabel\ng1.set_ylabel('Trans Revenue(log) Dist', fontsize=18) #Ylabel\n\nplt.show()", "processed": ["top 5 browser 94 total top 1 chrome 69 08 top 2 safari 20 04 top 3 firefox 3 77 noth new sun chrome use follow safari firefox", "cross revenu browser"]}, {"markdown": ["I think that it's very insightful information.\n\nChrome have highest values in general but the highest value of transactions was did on Firefox.<br>\nWe can see a \"small\" but consistent sells in Safari. Also IE and Edge give some results to Google;", "## Let's see the Channel Grouping\n - The channel via which the user came to the Store."], "code": "# Reference: https://www.kaggle.com/code/kabure/exploring-the-consumer-patterns-ml-pipeline\n\n# the top 10 of browsers represent % of total\nprint(\"Percentual of Channel Grouping used: \")\nprint((df_train['channelGrouping'].value_counts()[:5])) # printing the top 7 percentage of browsers\n\n# seting the graph size\nplt.figure(figsize=(14,7))\n\n# let explore the browser used by users\nsns.countplot(df_train[\"channelGrouping\"], palette=\"hls\") # It's a module to count the category's\nplt.title(\"Channel Grouping Count\", fontsize=20) # seting the title size\nplt.xlabel(\"Channel Grouping Name\", fontsize=18) # seting the x label size\nplt.ylabel(\"Count\", fontsize=18) # seting the y label size\n\nplt.show() #use plt.show to render the graph that we did above", "processed": ["think insight inform chrome highest valu gener highest valu transact firefox br see small consist sell safari also ie edg give result googl", "let see channel group channel via user came store"]}, {"markdown": ["The TOP 5 Grouping Channels represents 97% of total values. \nRespectivelly: \n- TOP 1 => Organic Search - 42.99%\n- TOP 2 => Social - 24.39%\n- TOP 3 => Direct - 15.42%\n- TOP 4 => Referral - 11.89%\n- TOP 5 => Paid Search - 2.55%\n\nI have a new insight that I will explore furthuer. How wich channel are distributed by browsers? \n", "## Crossing Channel Grouping x Browsers"], "code": "# Reference: https://www.kaggle.com/code/kabure/exploring-the-consumer-patterns-ml-pipeline\n\n## I will use the crosstab to explore two categorical values\n\n# At index I will use set my variable that I want analyse and cross by another\ncrosstab_eda = pd.crosstab(index=df_train['channelGrouping'], normalize=True,\n                           # at this line, I am using the isin to select just the top 5 of browsers\n                           columns=df_train[df_train['device.browser'].isin(df_train['device.browser']\\\n                                                                            .value_counts()[:5].index.values)]['device.browser'])\n# Ploting the crosstab that we did above\ncrosstab_eda.plot(kind=\"bar\",    # select the bar to plot the count of categoricals\n                 figsize=(14,7), # adjusting the size of graphs\n                 stacked=True)   # code to unstack \nplt.title(\"Channel Grouping % for which Browser\", fontsize=20) # seting the title size\nplt.xlabel(\"The Channel Grouping Name\", fontsize=18) # seting the x label size\nplt.ylabel(\"Count\", fontsize=18) # seting the y label size\nplt.xticks(rotation=0)\nplt.show() # rendering", "processed": ["top 5 group channel repres 97 total valu respectivelli top 1 organ search 42 99 top 2 social 24 39 top 3 direct 15 42 top 4 referr 11 89 top 5 paid search 2 55 new insight explor furthuer wich channel distribut browser", "cross channel group x browser"]}, {"markdown": ["Very cool! Interesting patterns", "# Operational System "], "code": "# Reference: https://www.kaggle.com/code/kabure/exploring-the-consumer-patterns-ml-pipeline\n\n# the top 5 of browsers represent % of total\nprint(\"Percentual of Operational System: \")\nprint(df_train['device.operatingSystem'].value_counts()[:5]) # printing the top 7 percentage of browsers\n\n# seting the graph size\nplt.figure(figsize=(14,7))\n\n# let explore the browser used by users\nsns.countplot(df_train[\"device.operatingSystem\"], palette=\"hls\") # It's a module to count the category's\nplt.title(\"Operational System used Count\", fontsize=20) # seting the title size\nplt.xlabel(\"Operational System Name\", fontsize=16) # seting the x label size\nplt.ylabel(\"OS Count\", fontsize=16) # seting the y label size\nplt.xticks(rotation=45) # Adjust the xticks, rotating the labels\n\nplt.show() #use plt.show to render the graph that we did above", "processed": ["cool interest pattern", "oper system"]}, {"markdown": ["The TOP 5 of Operational System corresponds to 96%.\n\nTOP 1 => Windows - 38.75% <br>\nTOP 2 => Macintosh - 28.04% <br>\nTOP 3 => Android - 14.15% <br>\nTOP 4 => iOS - 11.75% <br>\nTOP 5 => Linux - 3.91% <br>\n\nIt's very interestign to me. In my country macbook isn't the most common SO. I will investigate further the SO by Country's", "## Now let's investigate the most used brower by Operational System"], "code": "# Reference: https://www.kaggle.com/code/kabure/exploring-the-consumer-patterns-ml-pipeline\n\n\n# At index I will use isin to substitute the loop and get just the values with more than 1%\ncrosstab_eda = pd.crosstab(index=df_train[df_train['device.operatingSystem']\\\n                                          .isin(df_train['device.operatingSystem']\\\n                                                .value_counts()[:6].index.values)]['device.operatingSystem'], \n                           \n                           # at this line, I am using the isin to select just the top 5 of browsers\n                           columns=df_train[df_train['device.browser'].isin(df_train['device.browser']\\\n                                                                            .value_counts()[:5].index.values)]['device.browser'])\n# Ploting the crosstab that we did above\ncrosstab_eda.plot(kind=\"bar\",    # select the bar to plot the count of categoricals\n                 figsize=(14,7), # adjusting the size of graphs\n                 stacked=True)   # code to unstack \nplt.title(\"Most frequent OS's by Browsers of users\", fontsize=22) # adjusting title and fontsize\nplt.xlabel(\"Operational System Name\", fontsize=19) # adjusting x label and fontsize\nplt.ylabel(\"Count OS\", fontsize=19) # adjusting y label and fontsize\nplt.xticks(rotation=0) # Adjust the xticks, rotating the labels\n\nplt.show() # rendering", "processed": ["top 5 oper system correspond 96 top 1 window 38 75 br top 2 macintosh 28 04 br top 3 android 14 15 br top 4 io 11 75 br top 5 linux 3 91 br interestign countri macbook common investig countri", "let investig use brower oper system"]}, {"markdown": ["Cool, we can have a better understanding of the distribution of Revenue by OS", "## Let's investigate the Device Category"], "code": "# Reference: https://www.kaggle.com/code/kabure/exploring-the-consumer-patterns-ml-pipeline\n\n# the top 5 of browsers represent % of total\nprint(\"Percentual of Operational System: \")\nprint(round(df_train['device.deviceCategory'].value_counts() / len(df_train['device.deviceCategory']) * 100, 2)) # printing the top 7 percentage of browsers\n\n# seting the graph size\nplt.figure(figsize=(14,5))\n\nplt.subplot(1,2,1)\n# let explore the browser used by users\nsns.countplot(df_train[\"device.deviceCategory\"], palette=\"hls\") # It's a module to count the category's\nplt.title(\"Device Category Count\", fontsize=20) # seting the title size\nplt.xlabel(\"Device Category\", fontsize=18) # seting the x label size\nplt.ylabel(\"Count\", fontsize=16) # seting the y label size\nplt.xticks(fontsize=18) # Adjust the xticks, rotating the labels\n\nplt.subplot(1,2,2)\nsns.boxenplot(x=\"device.deviceCategory\", y = 'totals.transactionRevenue', \n              data=df_train[df_train['totals.transactionRevenue'] > 0], palette=\"hls\") # It's a module to count the category's\nplt.title(\"Device Category Revenue Distribuition\", fontsize=20) # seting the title size\nplt.xlabel(\"Device Category\", fontsize=18) # seting the x label size\nplt.ylabel(\"Revenue(Log)\", fontsize=16) # seting the y label size\nplt.xticks(fontsize=18) # Adjust the xticks, rotating the labels\n\nplt.subplots_adjust(hspace = 0.9, wspace = 0.5)\n\nplt.show() #use plt.show to render the graph that we did above", "processed": ["cool better understand distribut revenu o", "let investig devic categori"]}, {"markdown": ["We have We can see the distribuition of ", "## Now, lets investigate the Device Category by Browsers"], "code": "# Reference: https://www.kaggle.com/code/kabure/exploring-the-consumer-patterns-ml-pipeline\n\n# At index I will use isin to substitute the loop and get just the values with more than 1%\ncrosstab_eda = pd.crosstab(index=df_train['device.deviceCategory'], # at this line, I am using the isin to select just the top 5 of browsers\n                           columns=df_train[df_train['device.operatingSystem']\\\n                                            .isin(df_train['device.operatingSystem']\\\n                                                  .value_counts()[:6].index.values)]['device.operatingSystem'])\n# Ploting the crosstab that we did above\ncrosstab_eda.plot(kind=\"bar\",    # select the bar to plot the count of categoricals\n                 figsize=(14,7), # adjusting the size of graphs\n                 stacked=True)   # code to unstack \nplt.title(\"Most frequent OS's by Device Categorys of users\", fontsize=22) # adjusting title and fontsize\nplt.xlabel(\"Device Name\", fontsize=19)                # adjusting x label and fontsize\nplt.ylabel(\"Count Device x OS\", fontsize=19)                               # adjusting y label and fontsize\nplt.xticks(rotation=0)                                            # Adjust the xticks, rotating the labels\n\n\nplt.show() # rendering", "processed": ["see distribuit", "let investig devic categori browser"]}, {"markdown": ["Very interesting values.", "# SubContinent "], "code": "# Reference: https://www.kaggle.com/code/kabure/exploring-the-consumer-patterns-ml-pipeline\n\n# the top 8 of browsers represent % of total\nprint(\"Description of SubContinent count: \")\nprint(df_train['geoNetwork.subContinent'].value_counts()[:8]) # printing the top 7 percentage of browsers\n\n# seting the graph size\nplt.figure(figsize=(16,7))\n\n# let explore the browser used by users\nsns.countplot(df_train[df_train['geoNetwork.subContinent']\\\n                       .isin(df_train['geoNetwork.subContinent']\\\n                             .value_counts()[:15].index.values)]['geoNetwork.subContinent'], palette=\"hls\") # It's a module to count the category's\nplt.title(\"TOP 15 most frequent SubContinents\", fontsize=20) # seting the title size\nplt.xlabel(\"subContinent Names\", fontsize=18) # seting the x label size\nplt.ylabel(\"SubContinent Count\", fontsize=18) # seting the y label size\nplt.xticks(rotation=45) # Adjust the xticks, rotating the labels\n\nplt.show() #use plt.show to render the graph that we did above", "processed": ["interest valu", "subcontin"]}, {"markdown": ["WoW, We have a very high number of users from North America. \n\nTOP 5 regions are equivalent of almost 70% +-  of total \n\nTOP 1 => Northern America - 44.18% <br>\nTOP 2 => Southeast Asia - 8.29% <br>\nTOP 3 => Northern Europe - 6.73% <br>\nTOP 4 => Southern Asia - 6.33% <br>\nTOP 5 => Western Europe - 6.23% <br>", "## Let's cross the SubContinent by Browser"], "code": "# Reference: https://www.kaggle.com/code/kabure/exploring-the-consumer-patterns-ml-pipeline\n\n## I will use the crosstab to explore two categorical values\n\n# At index I will use isin to substitute the loop and get just the values with more than 1%\ncrosstab_eda = pd.crosstab(index=df_train[df_train['geoNetwork.subContinent']\\\n                                          .isin(df_train['geoNetwork.subContinent']\\\n                                                .value_counts()[:10].index.values)]['geoNetwork.subContinent'], \n                           \n                           # at this line, I am using the isin to select just the top 5 of browsers\n                           columns=df_train[df_train['device.browser'].isin(df_train['device.browser']\\\n                                                                            .value_counts()[:5].index.values)]['device.browser'])\n# Ploting the crosstab that we did above\ncrosstab_eda.plot(kind=\"bar\",    # select the bar to plot the count of categoricals\n                 figsize=(16,7), # adjusting the size of graphs\n                 stacked=True)   # code to unstack \nplt.title(\"TOP 10 Most frequent Subcontinents by Browsers used\", fontsize=22) # adjusting title and fontsize\nplt.xlabel(\"Subcontinent Name\", fontsize=19) # adjusting x label and fontsize\nplt.ylabel(\"Count Subcontinent\", fontsize=19) # adjusting y label and fontsize\nplt.xticks(rotation=45) # Adjust the xticks, rotating the labels\nplt.legend(loc=1, prop={'size': 12}) # to \n\nplt.show() # rendering", "processed": ["wow high number user north america top 5 region equival almost 70 total top 1 northern america 44 18 br top 2 southeast asia 8 29 br top 3 northern europ 6 73 br top 4 southern asia 6 33 br top 5 western europ 6 23 br", "let cross subcontin browser"]}, {"markdown": ["## INTERACTIVE DATE FEATURES", "## First I will explore revenue and number of visits by day"], "code": "# Reference: https://www.kaggle.com/code/kabure/exploring-the-consumer-patterns-ml-pipeline\n\n# I saw and take a lot of inspiration to this interactive plots in kernel: \n# https://www.kaggle.com/jsaguiar/complete-exploratory-analysis-all-columns\n# I learned a lot in this kernel and I will implement and adapted some ideas\n\n#seting some static color options\ncolor_op = ['#5527A0', '#BB93D7', '#834CF7', '#6C941E', '#93EAEA', '#7425FF', '#F2098A', '#7E87AC', \n            '#EBE36F', '#7FD394', '#49C35D', '#3058EE', '#44FDCF', '#A38F85', '#C4CEE0', '#B63A05', \n            '#4856BF', '#F0DB1B', '#9FDBD9', '#B123AC']\n\n# Visits by time train\n\n# couting all entries by date to get number of visits by each date\ndates_temp = df_train['date'].value_counts().to_frame().reset_index().sort_values('index') \n# renaming the columns to apropriate names\ndates_temp = dates_temp.rename(columns = {\"date\" : \"visits\"}).rename(columns = {\"index\" : \"date\"})\n\n# creating the first trace with the necessary parameters\ntrace = go.Scatter(x=dates_temp.date.astype(str), y=dates_temp.visits,\n                    opacity = 0.8, line = dict(color = color_op[3]), name= 'Visits by day')\n\n# Below we will get the total values by Transaction Revenue Log by date\ndates_temp_sum = df_train.groupby('date')['totals.transactionRevenue'].sum().to_frame().reset_index()\n\n# using the new dates_temp_sum we will create the second trace\ntrace1 = go.Scatter(x=dates_temp_sum.date.astype(str), line = dict(color = color_op[1]), name=\"RevenueLog by day\",\n                        y=dates_temp_sum['totals.transactionRevenue'], opacity = 0.8)\n\n# Getting the total values by Transactions by each date\ndates_temp_count = df_train[df_train['totals.transactionRevenue'] > 0].groupby('date')['totals.transactionRevenue'].count().to_frame().reset_index()\n\n# using the new dates_temp_count we will create the third trace\ntrace2 = go.Scatter(x=dates_temp_count.date.astype(str), line = dict(color = color_op[5]), name=\"Sellings by day\",\n                        y=dates_temp_count['totals.transactionRevenue'], opacity = 0.8)\n\n#creating the layout the will allow us to give an title and \n# give us some interesting options to handle with the outputs of graphs\nlayout = dict(\n    title= \"Informations by Date\",\n    xaxis=dict(\n        rangeselector=dict(\n            buttons=list([\n                dict(count=1, label='1m', step='month', stepmode='backward'),\n                dict(count=3, label='3m', step='month', stepmode='backward'),\n                dict(count=6, label='6m', step='month', stepmode='backward'),\n                dict(step='all')\n            ])\n        ),\n        rangeslider=dict(visible = True),\n        type='date'\n    )\n)\n\n# creating figure with the both traces and layout\nfig = dict(data= [trace, trace1, trace2], layout=layout)\n\n#rendering the graphs\niplot(fig) #it's an equivalent to plt.show()", "processed": ["interact date featur", "first explor revenu number visit day"]}, {"markdown": ["### Creating an Sofistcated interactive graphics to better understanding of date features\n\nTo see the code click in \"code\". \n## SELECT THE OPTION: "], "code": "# Reference: https://www.kaggle.com/code/kabure/exploring-the-consumer-patterns-ml-pipeline\n\n# Setting the first trace\ntrace1 = go.Histogram(x=df_train[\"_year\"],\n                      name='Year Count')\n\n# Setting the second trace\ntrace2 = go.Histogram(x=df_train[\"_month\"],\n                name='Month Count')\n\n# Setting the third trace\ntrace3 = go.Bar(y=day.values,\n                x=day.index.values, \n                name='Day Count')\n\n# Setting the fourth trace\ntrace4 = go.Bar(y=weeday.values,\n                x=weeday.index.values,\n                name='Weekday Count')\n\n# puting all traces in the same \"array of graphics\" to we render it below\ndata = [trace1, trace2, trace4, trace3]\n\n#Creating the options to be posible we use in our \nupdatemenus = list([\n    dict(active=-1,\n         x=-0.15,\n         buttons=list([  \n             dict(\n                 label = 'Years Count',\n                 method = 'update',\n                 args = [{'visible': [True, False, False, False,False]}, \n                         {'title': 'Count of Year'}]),\n             dict(\n                 label = 'Months Count',\n                 method = 'update',\n                 args = [{'visible': [False, True, False, False,False]},\n                         {'title': 'Count of Months'}]),\n             dict(\n                 label = 'WeekDays Count',\n                 method = 'update',\n                 args = [{'visible': [False, False, True, False, False]},\n                         {'title': 'Count of WeekDays'}]),\n            dict(\n                label = 'Days Count ',\n                method = 'update',\n                args = [{'visible': [False, False, False, True,False]},\n                        {'title': 'Count of Day'}]) ])\n    )\n])\n\n\nlayout = dict(title='The percentual Distribuitions of Date Features (Select from Dropdown)',\n              showlegend=False,\n              updatemenus=updatemenus,\n#              xaxis = dict(\n#                  type=\"category\"\n#                      ),\n              barmode=\"group\"\n             )\nfig = dict(data=data, layout=layout)\nprint(\"SELECT BELOW: \")\niplot(fig)", "processed": ["creat sofistc interact graphic better understand date featur see code click code select option"]}, {"markdown": ["## Exploring Countrys"], "code": "# Reference: https://www.kaggle.com/code/kabure/exploring-the-consumer-patterns-ml-pipeline\n\ncountry_tree = df_train[\"geoNetwork.country\"].value_counts() #counting the values of Country\n\nprint(\"Description most frequent countrys: \")\nprint(country_tree[:15]) #printing the 15 top most \n\ncountry_tree = round((df_train[\"geoNetwork.country\"].value_counts()[:30] \\\n                       / len(df_train['geoNetwork.country']) * 100),2)\n\nplt.figure(figsize=(14,5))\ng = squarify.plot(sizes=country_tree.values, label=country_tree.index, \n                  value=country_tree.values,\n                  alpha=.4, color=color)\ng.set_title(\"'TOP 30 Countrys - % size of total\",fontsize=20)\ng.set_axis_off()\nplt.show()", "processed": ["explor countri"]}, {"markdown": ["USA have a very highest value than another countrys. \n\nBelow I will take a look on cities and find for the highest revenues from them\n", "## Now, I will look on City feature and see the principal cities in the dataset"], "code": "# Reference: https://www.kaggle.com/code/kabure/exploring-the-consumer-patterns-ml-pipeline\n\ndf_train.loc[df_train[\"geoNetwork.city\"] == \"not available in demo dataset\", 'geoNetwork.city'] = np.nan\ncity_tree = df_train[\"geoNetwork.city\"].value_counts() #counting \n\nprint(\"Description most frequent Citys: \" )\nprint(city_tree[:15])\n\ncity_tree = round((city_tree[:30] / len(df_train['geoNetwork.city']) * 100),2)\n\nplt.figure(figsize=(14,5))\ng = squarify.plot(sizes=city_tree.values, label=city_tree.index, \n                  value=city_tree.values,\n                  alpha=.4, color=color)\ng.set_title(\"'TOP 30 Citys - % size of total\",fontsize=20)\ng.set_axis_off()\nplt.show()", "processed": ["usa highest valu anoth countri take look citi find highest revenu", "look citi featur see princip citi dataset"]}, {"markdown": ["Nicelly distributed clients that accessed the store. \n(non set) have 3.81% of total, so I dont will consider in top five, but it was the top 2 most frequent. \n\nThe top 5 are:\n- Montain View\n- New York \n- San Francisco\n- Sunnyvale \n- London\n\nAnd in terms of money, how the Countrys and Cities are ? \n", "____________________", "### Creating a function with plotly to better investigate the dataset\n\n- Click in \"code\" to see the commented code"], "code": "# Reference: https://www.kaggle.com/code/kabure/exploring-the-consumer-patterns-ml-pipeline\n\ndef PieChart(df_colum, title, limit=15):\n    \"\"\"\n    This function helps to investigate the proportion of visits and total of transction revenue \n    by each category\n    \"\"\"\n\n    count_trace = df_train[df_colum].value_counts()[:limit].to_frame().reset_index()\n    rev_trace = df_train.groupby(df_colum)[\"totals.transactionRevenue\"].sum().nlargest(10).to_frame().reset_index()\n\n    trace1 = go.Pie(labels=count_trace['index'], values=count_trace[df_colum], name= \"% Acesses\", hole= .5, \n                    hoverinfo=\"label+percent+name\", showlegend=True,domain= {'x': [0, .48]}, \n                    marker=dict(colors=color))\n\n    trace2 = go.Pie(labels=rev_trace[df_colum], \n                    values=rev_trace['totals.transactionRevenue'], name=\"% Revenue\", hole= .5, \n                    hoverinfo=\"label+percent+name\", showlegend=False, domain= {'x': [.52, 1]})\n\n    layout = dict(title= title, height=450, font=dict(size=15),\n                  annotations = [\n                      dict(\n                          x=.25, y=.5,\n                          text='Visits', \n                          showarrow=False,\n                          font=dict(size=20)\n                      ),\n                      dict(\n                          x=.80, y=.5,\n                          text='Revenue', \n                          showarrow=False,\n                          font=dict(size=20)\n                      )\n        ])\n\n    fig = dict(data=[trace1, trace2], layout=layout)\n    iplot(fig)", "processed": ["nicelli distribut client access store non set 3 81 total dont consid top five top 2 frequent top 5 montain view new york san francisco sunnyval london term money countri citi", "", "creat function plotli better investig dataset click code see comment code"]}, {"markdown": ["##  Geolocation plot to visually understand the data"], "code": "# Reference: https://www.kaggle.com/code/kabure/exploring-the-consumer-patterns-ml-pipeline\n\n# Counting total visits by countrys\ncountMaps = pd.DataFrame(df_train['geoNetwork.country'].value_counts()).reset_index()\ncountMaps.columns=['country', 'counts'] #renaming columns\ncountMaps = countMaps.reset_index().drop('index', axis=1) #reseting index and droping the column\n\ndata = [ dict(\n        type = 'choropleth',\n        locations = countMaps['country'],\n        locationmode = 'country names',\n        z = countMaps['counts'],\n        text = countMaps['country'],\n        autocolorscale = False,\n        marker = dict(\n            line = dict (\n                color = 'rgb(180,180,180)',\n                width = 0.5\n            ) ),\n        colorbar = dict(\n            autotick = False,\n            tickprefix = '',\n            title = 'Number of Visits'),\n      ) ]\n\nlayout = dict(\n    title = 'Couting Visits Per Country',\n    geo = dict(\n        showframe = False,\n        showcoastlines = True,\n        projection = dict(\n            type = 'Mercator'\n        )\n    )\n)\n\nfigure = dict( data=data, layout=layout )\niplot(figure, validate=False, filename='map-countrys-count')", "processed": ["geoloc plot visual understand data"]}, {"markdown": ["## Total Revenues by Country "], "code": "# Reference: https://www.kaggle.com/code/kabure/exploring-the-consumer-patterns-ml-pipeline\n\n# I will crete a variable of Revenues by country sum\nsumRevMaps = df_train[df_train['totals.transactionRevenue'] > 0].groupby(\"geoNetwork.country\")[\"totals.transactionRevenue\"].count().to_frame().reset_index()\nsumRevMaps.columns = [\"country\", \"count_sales\"] # renaming columns\nsumRevMaps = sumRevMaps.reset_index().drop('index', axis=1) #reseting index and drop index column\n\ndata = [ dict(\n        type = 'choropleth',\n        locations = sumRevMaps['country'],\n        locationmode = 'country names',\n        z = sumRevMaps['count_sales'],\n        text = sumRevMaps['country'],\n        autocolorscale = False,\n        marker = dict(\n            line = dict (\n                color = 'rgb(180,180,180)',\n                width = 0.5\n            ) ),\n        colorbar = dict(\n            autotick = False,\n            tickprefix = '',\n            title = 'Count of Sales'),\n      ) ]\n\nlayout = dict(\n    title = 'Total Sales by Country',\n    geo = dict(\n        showframe = False,\n        showcoastlines = True,\n        projection = dict(\n            type = 'Mercator'\n        )\n    )\n)\n\nfigure = dict( data=data, layout=layout )\n\niplot(figure, validate=False, filename='map-countrys-total')", "processed": ["total revenu countri"]}, {"markdown": ["This looks very interesting. It seems like there is some order. What do you think?"], "code": "# Reference: https://www.kaggle.com/code/allunia/molecules-eda\n\nN = 100000\n\ntrace1 = go.Scatter3d(\n    x=structures.x.values[0:N], \n    y=structures.y.values[0:N],\n    z=structures.z.values[0:N],\n    mode='markers',\n    marker=dict(\n        color=structures.index.values[0:N],\n        colorscale = \"YlGnBu\",\n        opacity=0.3,\n        size=1\n    )\n)\n\nfigure_data = [trace1]\nlayout = go.Layout(\n    title = 'Coloring by index',\n    scene = dict(\n        xaxis = dict(title=\"x\"),\n        yaxis = dict(title=\"y\"),\n        zaxis = dict(title=\"z\"),\n    ),\n    margin=dict(\n        l=0,\n        r=0,\n        b=0,\n        t=0\n    ),\n    showlegend=True\n)\n\nfig = go.Figure(data=figure_data, layout=layout)\npy.iplot(fig, filename='simple-3d-scatter')", "processed": ["look interest seem like order think"]}, {"markdown": ["### Were larger molecules added later to the file?"], "code": "# Reference: https://www.kaggle.com/code/allunia/molecules-eda\n\nmolecules_size = structures.groupby(\"molecule_name\").atom.size()\nmean_size = molecules_size.rolling(window=500).mean()\nplt.figure(figsize=(20,5))\nplt.plot(molecules_size.values, '+')\nplt.plot(np.arange(250, len(molecules_size)-249), mean_size.dropna().values, '-')\nplt.xlabel(\"Molecule name number\")\nplt.ylabel(\"Number of atoms\");", "processed": ["larger molecul ad later file"]}, {"markdown": ["* Yes, the structure has a dependency on the molecule size, but it makes sense that this is not true for all points. Larger molecules have atoms close to the center 0 as well. Interestingly this is more true for C and H atoms.\n* If we color by the atom index of molecules this comes even more clear. Low index numbers are more centered whereas higher indices live more outside of the \"sphere\".\n* **Consequently the order we have found has definitely something to do with the size of the molecules**. The next question is now, why are molecules of similar or same size orientated in different angles? We can clearly see a symmetric order with respect to angles as well.", "### What about the mulliken charges?"], "code": "# Reference: https://www.kaggle.com/code/allunia/molecules-eda\n\nmulliken = pd.read_csv(\"../input/mulliken_charges.csv\")\ntrain_structures = structures.loc[structures.molecule_name.isin(train.molecule_name)].copy()\ntrain_structures = train_structures.set_index([\"molecule_name\", \"atom_index\"])\nmulliken = mulliken.set_index([\"molecule_name\", \"atom_index\"])\nmulliken[\"atom\"] = train_structures.atom\n\nplt.figure(figsize=(20,5))\nsns.violinplot(x=mulliken.atom, y=mulliken.mulliken_charge)\nplt.title(\"How are the mulliken charges distributed given the atom type?\");", "processed": ["ye structur depend molecul size make sen true point larger molecul atom close center 0 well interestingli true c h atom color atom index molecul come even clear low index number center wherea higher indic live outsid sphere consequ order found definit someth size molecul next question molecul similar size orient differ angl clearli see symmetr order respect angl well", "mulliken charg"]}, {"markdown": ["No!\n\nHence it's an information that we can use to create new features that we can somehow map on test. \n\n### How do the dipoles look like?\n\n"], "code": "# Reference: https://www.kaggle.com/code/allunia/molecules-eda\n\nplt.figure(figsize=(20,5))\nsns.distplot(dipoles.X, label=\"X\")\nsns.distplot(dipoles.Y, label=\"Y\")\nsns.distplot(dipoles.Z, label=\"Z\")\nplt.legend();\nplt.title(\"Coordinate specific dipole moment distribution\");", "processed": ["henc inform use creat new featur somehow map test dipol look like"]}, {"markdown": ["In contrast to X the coordinates Y and Z show a bimodality. Besides that we can see extreme outliers. Let's check the absolute sum of dipole moments:"], "code": "# Reference: https://www.kaggle.com/code/allunia/molecules-eda\n\ndipoles[\"total\"] = np.abs(dipoles.X) + np.abs(dipoles.Y) + np.abs(dipoles.Z)\nplt.figure(figsize=(20,5))\nsns.distplot(dipoles.total);\nplt.title(\"How is the absolute sum of X, Y, Z dipole moments distributed?\");", "processed": ["contrast x coordin z show bimod besid see extrem outlier let check absolut sum dipol moment"]}, {"markdown": ["# EDA", "## Transaction Amounts\n\nAs the name suggests, this is the amount of money transferred during the transaction, and this is clearly a continuous variable. I will visualize this feature in relation with the target, *isFraud*.\n\n", "### Distribution of transaction amounts for non-fraudelent and fraudulent cases", "### Violin Plot"], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/ieee-cis-competition-fresh-eda-and-modeling\n\nplt.rcParams[\"axes.labelsize\"] = 15\nplt.rcParams[\"xtick.labelsize\"] = 13\nplt.rcParams[\"ytick.labelsize\"] = 13\nfig, ax = plt.subplots(figsize=(10, 10))\nplot = sns.violinplot(x=\"isFraud\", y=\"TransactionAmt\", data=train_df.query(\"TransactionAmt < 1000\"), palette=[\"limegreen\", \"dodgerblue\"], linewidth=2\n                      , ax=ax).set_title('TransactionAmt', fontsize=16)\nplt.show()", "processed": ["eda", "transact amount name suggest amount money transfer transact clearli continu variabl visual featur relat target isfraud", "distribut transact amount non fraudel fraudul case", "violin plot"]}, {"markdown": ["In the violin plot above, the green violin is the distribution of transaction amounts for non-fraudulent samples and the red violin is that for fraudulent samples. It can be seen that both distributions have a strong positive (leftward) skew. But, the red violin has greater probability density towards the higher values of *TransactionAmt* as compared to the green distribution. \n\nThe green distribution has a very high probability denisty concentrated around the lower values of *TransactionAmt* and as a result, the probability density around the higher values of *TransactionAmt* is almost negligible. But, the red violin on the other hand, has lesser probability density concentrated around the lower values of *TransactionAmt* and thus, there is a considerable probability density around the higher values of *TransactionAmt*.\n\nThis happens because the green violin has multiple peaks (multimodal characteristic) around the lower values of *TransactionAmt*, whereas the red violin has only one clear peak in this region. This presence of only one peak in the red violin indicates that it has a much milder skew than the green violin. **Therefore, in general, the greater the transaction amount, the more likely it is for the transaction to be fraudulent.**\n\nThis makes intuitive sense, because very expensive transactions have a greater chance of being fraudulent than less expensive transactions.\n\n\n", "### Box Plot"], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/ieee-cis-competition-fresh-eda-and-modeling\n\nfig, ax = plt.subplots(figsize=(10, 10))\nplot = sns.boxplot(x=\"isFraud\", y=\"TransactionAmt\", data=train_df.query(\"TransactionAmt < 500\"), palette=[\"limegreen\", \"red\"], ax=ax).set_title('TransactionAmt', fontsize=16)\nplt.show()", "processed": ["violin plot green violin distribut transact amount non fraudul sampl red violin fraudul sampl seen distribut strong posit leftward skew red violin greater probabl densiti toward higher valu transactionamt compar green distribut green distribut high probabl denisti concentr around lower valu transactionamt result probabl densiti around higher valu transactionamt almost neglig red violin hand lesser probabl densiti concentr around lower valu transactionamt thu consider probabl densiti around higher valu transactionamt happen green violin multipl peak multimod characterist around lower valu transactionamt wherea red violin one clear peak region presenc one peak red violin indic much milder skew green violin therefor gener greater transact amount like transact fraudul make intuit sen expens transact greater chanc fraudul le expens transact", "box plot"]}, {"markdown": ["The box plot above also suggests that more expensive transactions are more likely to be fraudulent. This can be inferred from the fact that the mean value of the red box is greater than that of the green box. Although the first quartiles of the two distributions are very similar, the third quartile of the red box is significantly greater than that of the green box, providing further evidence that higher transaction amounts are more likely to be fraudulent than not, *i.e.* **the greater the transaction amount, the more likely it is for the transaction to be fraudulent.** ", "## Product CD", "I am not sure what Product CD exactly means, but it seems to be a categorical variable that provides some information regarding the category of a product. **Products in this dataset come under five broad categories: W, H, C, S, and R.** I will visualize this feature in relation with the target, *isFraud*.", "### Frequencies of the different product categories "], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/ieee-cis-competition-fresh-eda-and-modeling\n\nfig, ax = plt.subplots(figsize=(10, 10))\nplot = sns.countplot(y=\"ProductCD\", data=train_df, palette=reversed(['aquamarine', 'mediumaquamarine', 'mediumseagreen', 'seagreen', 'darkgreen'])).set_title('ProductCD', fontsize=16)\nplt.show(plot)", "processed": ["box plot also suggest expens transact like fraudul infer fact mean valu red box greater green box although first quartil two distribut similar third quartil red box significantli greater green box provid evid higher transact amount like fraudul e greater transact amount like transact fraudul", "product cd", "sure product cd exactli mean seem categor variabl provid inform regard categori product product dataset come five broad categori w h c r visual featur relat target isfraud", "frequenc differ product categori"]}, {"markdown": ["From the above plot, we can clearly see that the most common *ProductCD* value is W. The other four product categories, H, C, S, and R are very rare compared to W. "], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/ieee-cis-competition-fresh-eda-and-modeling\n\nfig, ax = plt.subplots(figsize=(10, 10))\n\nprops = train_df.query(\"TransactionAmt < 500\")\\\n                .groupby(\"ProductCD\")['isFraud'].value_counts(normalize=True).unstack()\n\nsns.set_palette(['lightgreen', 'green'])\nprops.plot(kind='bar', stacked='True', ax=ax).set_ylabel('Proportion')\nplt.show(plot)", "processed": ["plot clearli see common productcd valu w four product categori h c r rare compar w"]}, {"markdown": ["In the above proportion plot, the height of the dark green bar (at the top) represents the probability of a transaction involving a given product category being fraudulent. We can see that a product of category *C* is more likely to be involved in a fraudulent transaction as compared to any other product category. The next most fraudulence-prone product category is category *S*, and so on.\n\n", "### Distributions of transaction amounts for different *ProductCD* values (for non-fraudulent and fraudulent cases)"], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/ieee-cis-competition-fresh-eda-and-modeling\n\nfig, ax = plt.subplots(figsize=(15, 15))\n\nplot = sns.violinplot(x=\"ProductCD\", y=\"TransactionAmt\", hue=\"isFraud\",\n                      data=train_df.query('TransactionAmt < 500'), palette=['lightgreen', 'green'],\n                      split=True, ax=ax).set_title('ProductCD vs. TransactionAmt', fontsize=16)\n\nplt.show(plot)", "processed": ["proport plot height dark green bar top repres probabl transact involv given product categori fraudul see product categori c like involv fraudul transact compar product categori next fraudul prone product categori categori", "distribut transact amount differ productcd valu non fraudul fraudul case"]}, {"markdown": ["In the above violin plots, the light green sections represent the distribution for non-fraudulent cases and the dark green sections represent the distribution for fraudulent cases. We can see the same trend as before. Both the distributions types have strong positive (leftward) skews. But, the light green (non-fraudulent) distributions have a greater probability density concentrated around the lower values of transaction amounts, once again, because there are several peaks (unlike the dark green distributions).\n\nBut, there are a few exceptions to this trend in the above visualization. For example, the non-fraudulent and fraudulent distributions are very similar for the *C* product category. Both the distributions have only one peak and they look very similar in almost every way. Interestingly, the *C* product category also has the highest fraudulence rate, and this is probably the reason why the correlation between the transaction amount and the target is very low for this category.\n\nBut, for the rest of the product categories, the trend is roughly followed.\n\nThe distributions for the *S* product category seem to have the lowest means and the strongest skews. These distributions have a very high concentration of probability density around the lower values of *TransactionAmt*. Transactions of type *S* tend to have low transaction amounts. On the other side of the spectrum, the distributions for the *R* product category seem to have the highest means and the weakest skews. These distributions have a very even spread and almost no skew. Transactions of type *R* tend to have high transaction amounts.\n\nMaybe, one can create a model that changes the weightage of the *TransactionAmt* feature based on the product category. For example, the weightage of the *TransactionAmt* feature can be reduced for the *C* product category, because the fraudulent and non-fraudulent distributions are very similar for this product category. "], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/ieee-cis-competition-fresh-eda-and-modeling\n\nfig, ax = plt.subplots(figsize=(15, 15))\n\nplot = sns.boxplot(x=\"ProductCD\", y=\"TransactionAmt\", hue=\"isFraud\",\n                   data=train_df.query('TransactionAmt < 500'), palette=['lightgreen', 'green'],\n                   ax=ax).set_title('ProductCD vs. TransactionAmt', fontsize=16)\n\nplt.show(plot)", "processed": ["violin plot light green section repres distribut non fraudul case dark green section repres distribut fraudul case see trend distribut type strong posit leftward skew light green non fraudul distribut greater probabl densiti concentr around lower valu transact amount sever peak unlik dark green distribut except trend visual exampl non fraudul fraudul distribut similar c product categori distribut one peak look similar almost everi way interestingli c product categori also highest fraudul rate probabl reason correl transact amount target low categori rest product categori trend roughli follow distribut product categori seem lowest mean strongest skew distribut high concentr probabl densiti around lower valu transactionamt transact type tend low transact amount side spectrum distribut r product categori seem highest mean weakest skew distribut even spread almost skew transact type r tend high transact amount mayb one creat model chang weightag transactionamt featur base product categori exampl weightag transactionamt featur reduc c product categori fraudul non fraudul distribut similar product categori"]}, {"markdown": ["The same trends and patterns can be seen in the box plots above. Both the distributions types have strong positive (leftward) skews. But, the light green (non-fraudulent) distributions clearly have lower means as compared to the dark green (fraudulent) distributions.\n\nThe same exceptions to this trend can also be seen in the above visualization. For example, the non-fraudulent and fraudulent distributions are very similar for the *C* product category. Both the distributions have similar means, first quartiles, and third quartiles.", "## P_emaildomain", "The *P_emaildomain* represents the email domain through which the transaction was **payed**. The most common domains are *gmail.com, yahoo.com, hotmail.com, and anonymous.com*.", "### Frequencies of the different email domains (P)"], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/ieee-cis-competition-fresh-eda-and-modeling\n\nfig, ax = plt.subplots(figsize=(10, 10))\nplot = sns.countplot(y=\"P_emaildomain\", data=train_df.query(\"P_emaildomain in ['gmail.com', 'yahoo.com', 0.0, 'hotmail.com', 'anonymous.com']\").query(\"TransactionAmt < 500\"),\n                     palette=['navy', 'darkblue', 'blue', 'dodgerblue', 'skyblue']).set_title('P_emaildomain vs. TransactionAmt', fontsize=16)\nplt.show(plot)", "processed": ["trend pattern seen box plot distribut type strong posit leftward skew light green non fraudul distribut clearli lower mean compar dark green fraudul distribut except trend also seen visual exampl non fraudul fraudul distribut similar c product categori distribut similar mean first quartil third quartil", "p emaildomain", "p emaildomain repres email domain transact pay common domain gmail com yahoo com hotmail com anonym com", "frequenc differ email domain p"]}, {"markdown": ["From the above plot, we can clearly see that the most common *P_emaildomain* value is *gmail.com*. The next most common email domain (for payment) is *yahoo.com* and the rest are comparatively very rare. ", "### Fraudulence Proportion Plot"], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/ieee-cis-competition-fresh-eda-and-modeling\n\nfig, ax = plt.subplots(figsize=(10, 10))\n\nprops = train_df.query(\"P_emaildomain in ['gmail.com', 'yahoo.com', 0.0, 'hotmail.com', 'anonymous.com']\").query(\"TransactionAmt < 500\")\\\n                .groupby(\"P_emaildomain\")['isFraud'].value_counts(normalize=True).unstack()\n\nsns.set_palette(['lightblue', 'darkblue'])\nprops.plot(kind='bar', stacked='True', ax=ax).set_ylabel('Proportion')\nplt.show(plot)", "processed": ["plot clearli see common p emaildomain valu gmail com next common email domain payment yahoo com rest compar rare", "fraudul proport plot"]}, {"markdown": ["In the above proportion plot, the height of the dark blue bar (at the top) represents the probability of a transaction made through a given email domain being fraudulent. We can see that an email domain of *hotmail.com* is associated with the highest probability of a fraudulent transaction. The next most fraudulence-prone email domain is *gmail.com*. And, the least fraudulence-prone email domain is *yahoo.com*.  ", "### TransactionAmt vs. P_emaildomain Violin Plot"], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/ieee-cis-competition-fresh-eda-and-modeling\n\nfig, ax = plt.subplots(figsize=(15, 15))\n\nplot = sns.violinplot(x=\"P_emaildomain\", y=\"TransactionAmt\", hue=\"isFraud\",\n                      data=train_df.query(\"P_emaildomain in ['gmail.com', 'yahoo.com', 0.0, 'hotmail.com', 'anonymous.com']\").query(\"TransactionAmt < 500\"),\n                      palette=['lightblue', 'darkblue'], split=True, ax=ax).set_title('TransactionAmt vs. P_emaildomain', fontsize=16)\n\nplt.show(plot)", "processed": ["proport plot height dark blue bar top repres probabl transact made given email domain fraudul see email domain hotmail com associ highest probabl fraudul transact next fraudul prone email domain gmail com least fraudul prone email domain yahoo com", "transactionamt v p emaildomain violin plot"]}, {"markdown": ["In the above violin plots, the light blue sections represent the distribution for non-fraudulent cases and the dark blue sections represent the distribution for fraudulent cases. We can see the same trend as before. Both the distributions types have strong positive (leftward) skews. But, the light blue (non-fraudulent) distributions have a greater probability density concentrated around the lower values of transaction amounts, once again, because there are several peaks (unlike the dark blue distributions).\n\nThere are no real exceptions to this trend except for *hotmail.com*. The distributions for transaction amount (for fraudulent and non-fraudulent cases) are very similar at *hotmail.com*. The other domains, namely, *gmail.com, yahoo.com, and anonymous.com* follow the general trend.\n\nEach email domain's distributions are roughly concentrated around the same mean, but with differing variances. No single email domain has a much greater or lower mean than the other email domains.\n\n**Interestingly, once again, the most fraudulence-prone email domain, namely, *hotmail.com* is also the one that does not follow the trend. This is similar to the *C* product category, which was the most fraudulent category and was also the category that did not follow the trend.** \n \n\nMaybe, one can create a model that changes the weightage of the *TransactionAmt* feature based on the email domain. For example, the weightage of the *TransactionAmt* feature can be reduced for the *hotmail.com* email domain, because the fraudulent and non-fraudulent distributions are very similar for this email domain. ", "### TransactionAmt vs. P_emaildomain Box Plot "], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/ieee-cis-competition-fresh-eda-and-modeling\n\nfig, ax = plt.subplots(figsize=(15, 15))\n\nplot = sns.boxplot(x=\"P_emaildomain\", y=\"TransactionAmt\", hue=\"isFraud\",\n                   data=train_df.query(\"P_emaildomain in ['gmail.com', 'yahoo.com', 0.0, 'hotmail.com', 'anonymous.com']\").query(\"TransactionAmt < 500\"),\n                   palette=['lightblue', 'darkblue'], ax=ax).set_title('TransactionAmt vs. P_emaildomain', fontsize=16)\n\nplt.show(plot)", "processed": ["violin plot light blue section repres distribut non fraudul case dark blue section repres distribut fraudul case see trend distribut type strong posit leftward skew light blue non fraudul distribut greater probabl densiti concentr around lower valu transact amount sever peak unlik dark blue distribut real except trend except hotmail com distribut transact amount fraudul non fraudul case similar hotmail com domain name gmail com yahoo com anonym com follow gener trend email domain distribut roughli concentr around mean differ varianc singl email domain much greater lower mean email domain interestingli fraudul prone email domain name hotmail com also one follow trend similar c product categori fraudul categori also categori follow trend mayb one creat model chang weightag transactionamt featur base email domain exampl weightag transactionamt featur reduc hotmail com email domain fraudul non fraudul distribut similar email domain", "transactionamt v p emaildomain box plot"]}, {"markdown": ["The same trends and patterns can be seen in the box plots above. Both the distributions types have strong positive (leftward) skews. But, the light blue (non-fraudulent) distributions clearly have lower means as compared to the dark blue (fraudulent) distributions.\n\nThe same exceptions to this trend can also be seen in the above visualization. For example, the non-fraudulent and fraudulent distributions are very similar for the *hotmail.com* email domain. Both the distributions have similar means and first quartiles. Although the third quartiles for the *anonymous.com* distributions are very similar, their means are very different.", "## R_emaildomain", "The *R_emaildomain* represents the email domain through which the transaction was **received**. The most common domains are *gmail.com, yahoo.com, hotmail.com, and anonymous.com*. ", "### Frequencies of the different email domains (R)"], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/ieee-cis-competition-fresh-eda-and-modeling\n\nfig, ax = plt.subplots(figsize=(10, 10))\nplot = sns.countplot(y=\"R_emaildomain\", data=train_df.query(\"R_emaildomain in ['gmail.com', 'yahoo.com', 0.0, 'hotmail.com', 'anonymous.com']\").query(\"TransactionAmt < 500\"),\n                     palette=['red', 'crimson', 'mediumvioletred', 'darkmagenta', 'indigo']).set_title('R_emaildomain', fontsize=16)\nplt.show(plot)", "processed": ["trend pattern seen box plot distribut type strong posit leftward skew light blue non fraudul distribut clearli lower mean compar dark blue fraudul distribut except trend also seen visual exampl non fraudul fraudul distribut similar hotmail com email domain distribut similar mean first quartil although third quartil anonym com distribut similar mean differ", "r emaildomain", "r emaildomain repres email domain transact receiv common domain gmail com yahoo com hotmail com anonym com", "frequenc differ email domain r"]}, {"markdown": ["From the above plot, we can clearly see that the most common *R_emaildomain* value is *gmail.com*. The next most common email domain is *hotmail.com*. The least common receiver email domain is *yahoo.com*.", "### Fraudulence Proportion Plot"], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/ieee-cis-competition-fresh-eda-and-modeling\n\nfig, ax = plt.subplots(figsize=(10, 10))\n\nprops = train_df.query(\"R_emaildomain in ['gmail.com', 'yahoo.com', 0.0, 'hotmail.com', 'anonymous.com']\").query(\"TransactionAmt < 500\")\\\n                .groupby(\"R_emaildomain\")['isFraud'].value_counts(normalize=True).unstack()\n\nsns.set_palette(['pink', 'crimson'])\nprops.plot(kind='bar', stacked='True', ax=ax).set_ylabel('Proportion')\nplt.show(plot)", "processed": ["plot clearli see common r emaildomain valu gmail com next common email domain hotmail com least common receiv email domain yahoo com", "fraudul proport plot"]}, {"markdown": ["In the above proportion plot, the height of the dark red bar (at the top) represents the probability of a transaction received through a given email domain being fraudulent. We can see that an email domain of *gmail.com* is associated with the highest probability of a fraudulent transaction. The next most fraudulence-prone email domain is *hotmail.com*. And, the least fraudulence-prone email domain is *anonymous.com*.", "### TransactionAmt vs. R_emaildomain  Violin Plot"], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/ieee-cis-competition-fresh-eda-and-modeling\n\nfig, ax = plt.subplots(figsize=(15, 15))\n\nplot = sns.violinplot(x=\"R_emaildomain\", y=\"TransactionAmt\", hue=\"isFraud\",\n                      data=train_df.query(\"R_emaildomain in ['gmail.com', 'yahoo.com', 0.0, 'hotmail.com', 'anonymous.com']\").query(\"TransactionAmt < 500\"),\n                      palette=['pink', 'crimson'], split=True, ax=ax).set_title('TransactionAmt vs. R_emaildomain', fontsize=16)\n\nplt.show(plot)", "processed": ["proport plot height dark red bar top repres probabl transact receiv given email domain fraudul see email domain gmail com associ highest probabl fraudul transact next fraudul prone email domain hotmail com least fraudul prone email domain anonym com", "transactionamt v r emaildomain violin plot"]}, {"markdown": ["In the above violin plots, the pink sections represent the distribution for non-fraudulent cases and the red sections represent the distribution for fraudulent cases. We can see the same trend as before. Both the distributions types have strong positive (leftward) skews. But, the pink (non-fraudulent) distributions have a greater probability density concentrated around the lower values of transaction amounts, once again, because there are several peaks (unlike the red distributions).\n\nThe exception to this trend is *anonymous.com*. The distributions for transaction amount (at *anonymous.com*) is much more skewed in the fraudulent case as compared to the non-fraudulent case. The other domains, namely, *gmail.com, yahoo.com, and hotmail.com* follow the general trend.\n\nEach email domain's distributions are roughly concentrated around the same mean, but with differing variances. No single email domain has a much greater or lower mean than the other email domains.\n\nMaybe, one can create a model that changes the weightage of the *TransactionAmt* feature based on the email domain. For example, the weightage of the *TransactionAmt* feature can be reduced for the *hotmail.com* email domain, because the fraudulent and non-fraudulent distributions are very similar for this email domain. ", "### TransactionAmt vs. R_emaildomain  Box Plot"], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/ieee-cis-competition-fresh-eda-and-modeling\n\nfig, ax = plt.subplots(figsize=(15, 15))\n\nplot = sns.boxplot(x=\"R_emaildomain\", y=\"TransactionAmt\", hue=\"isFraud\",\n                   data=train_df.query(\"R_emaildomain in ['gmail.com', 'yahoo.com', 0.0, 'hotmail.com', 'anonymous.com']\").query(\"TransactionAmt < 500\"),\n                   palette=['pink', 'crimson'], ax=ax).set_title('TransactionAmt vs. R_emaildomain', fontsize=16)\n\nplt.show(plot)", "processed": ["violin plot pink section repres distribut non fraudul case red section repres distribut fraudul case see trend distribut type strong posit leftward skew pink non fraudul distribut greater probabl densiti concentr around lower valu transact amount sever peak unlik red distribut except trend anonym com distribut transact amount anonym com much skew fraudul case compar non fraudul case domain name gmail com yahoo com hotmail com follow gener trend email domain distribut roughli concentr around mean differ varianc singl email domain much greater lower mean email domain mayb one creat model chang weightag transactionamt featur base email domain exampl weightag transactionamt featur reduc hotmail com email domain fraudul non fraudul distribut similar email domain", "transactionamt v r emaildomain box plot"]}, {"markdown": ["The same trends and patterns can be seen in the box plots above. Both the distributions types have strong positive (leftward) skews. But, the light blue (non-fraudulent) distributions clearly have lower means as compared to the dark blue (fraudulent) distributions.\n\nThe same exceptions to this trend can also be seen in the above visualization. For example, at *anonymous.com*, the fraudulent distribution has a much lower mean than the non-fraudulent one (counterintuitively). Also, the non-fraudulent and fraudulent distributions are very similar for the *hotmail.com* email domain. Both the distributions have similar means, quartiles, minima, and maxima.", "## Card brand (card4)", "The card4 feature represents the brand of the card through which the transaction was made. The card brands in this dataset are *discover, mastercard, visa,* and *american express*.", "### Frequencies of the different card brands"], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/ieee-cis-competition-fresh-eda-and-modeling\n\nfig, ax = plt.subplots(figsize=(10, 10))\nplot = sns.countplot(y=\"card4\", data=train_df.query(\"TransactionAmt < 500\"),\n                     palette=reversed(['orangered', 'darkorange', 'orange', 'peachpuff', 'navajowhite'])).set_title('card4', fontsize=16)\nplt.show(plot)", "processed": ["trend pattern seen box plot distribut type strong posit leftward skew light blue non fraudul distribut clearli lower mean compar dark blue fraudul distribut except trend also seen visual exampl anonym com fraudul distribut much lower mean non fraudul one counterintuit also non fraudul fraudul distribut similar hotmail com email domain distribut similar mean quartil minimum maximum", "card brand card4", "card4 featur repres brand card transact made card brand dataset discov mastercard visa american express", "frequenc differ card brand"]}, {"markdown": ["From the above plot, it is clear that the most common card brand used for transactions is *visa*. The second most common card brand is *mastercard*. The remaining two card brands, namely, *discover* and *american express* are very rare (comparatively).", "### Fraudulence Proportion Plot"], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/ieee-cis-competition-fresh-eda-and-modeling\n\nfig, ax = plt.subplots(figsize=(10, 10))\n\nprops = train_df.query(\"TransactionAmt < 500\")\\\n                .groupby(\"card4\")['isFraud'].value_counts(normalize=True).unstack()\n\nsns.set_palette(['peachpuff', 'darkorange'])\nprops.plot(kind='bar', stacked='True', ax=ax).set_ylabel('Proportion')\nplt.show(plot)", "processed": ["plot clear common card brand use transact visa second common card brand mastercard remain two card brand name discov american express rare compar", "fraudul proport plot"]}, {"markdown": ["In the above proportion plot, the height of the dark orange bar (at the top) represents the probability of a transaction received through a given card brand being fraudulent. We can see that *discover* cards are associated with the highest probability of a fraudulent transaction. The remaining card brands have a very similar fraudelence proportion.", "### TransactionAmt vs. card4 Violin Plot "], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/ieee-cis-competition-fresh-eda-and-modeling\n\nfig, ax = plt.subplots(figsize=(15, 15))\n\nplot = sns.violinplot(x=\"card4\", y=\"TransactionAmt\", hue=\"isFraud\",\n                      data=train_df.query(\"TransactionAmt < 500\"),\n                      palette=['peachpuff', 'darkorange'], split=True, ax=ax).set_title('TransactionAmt vs. card4', fontsize=16)\n\nplt.show(plot)", "processed": ["proport plot height dark orang bar top repres probabl transact receiv given card brand fraudul see discov card associ highest probabl fraudul transact remain card brand similar fraudel proport", "transactionamt v card4 violin plot"]}, {"markdown": ["In the above violin plots, the light orange sections represent the distribution for non-fraudulent cases and the dark orange sections represent the distribution for fraudulent cases. We can see the same trend as before. Both the distributions types have strong positive (leftward) skews. But, the light orange (non-fraudulent) distributions have a greater probability density concentrated around the lower values of transaction amounts, once again, because they have several peaks (unlike the dark orange distributions).\n\nThere do not seem to be any exceptions to this trend. The distributions for *american express* and *discover* cards seem to have higher means as compared to the other two card types. Therefore, more expensive transactions tend to take place through *american express* and *discover* cards.", "### TransactionAmt vs. card4 Box Plot "], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/ieee-cis-competition-fresh-eda-and-modeling\n\nfig, ax = plt.subplots(figsize=(15, 15))\n\nplot = sns.boxplot(x=\"card4\", y=\"TransactionAmt\", hue=\"isFraud\",\n                   data=train_df.query(\"TransactionAmt < 500\"),\n                   palette=['peachpuff', 'darkorange'], ax=ax).set_title('TransactionAmt vs. card4', fontsize=16)\n\nplt.show(plot)", "processed": ["violin plot light orang section repres distribut non fraudul case dark orang section repres distribut fraudul case see trend distribut type strong posit leftward skew light orang non fraudul distribut greater probabl densiti concentr around lower valu transact amount sever peak unlik dark orang distribut seem except trend distribut american express discov card seem higher mean compar two card type therefor expens transact tend take place american express discov card", "transactionamt v card4 box plot"]}, {"markdown": ["In the box plot above, we can say that, for *mastercard* and *visa* cards, the fraudulent and non-fraudulent distributions are very similar. They have very similar means. It is also clear in this box plot, that *discover* and *american express* cards tend to be associated with higher transaction amounts. This can be inferred from the fact that they have much higher means as compared to the other two card brands.", "## Card type (card6)", "The card6 feature represents the btype of the card through which the transaction was made. The major card types in this dataset are *credit* and *debit*.", "### Frequencies of the different card types"], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/ieee-cis-competition-fresh-eda-and-modeling\n\nfig, ax = plt.subplots(figsize=(10, 10))\nplot = sns.countplot(y=\"card6\", data=train_df.query(\"TransactionAmt < 500\").query(\"card6 == 'credit' or card6 == 'debit'\"),\n                     palette=reversed(['red', 'crimson', 'mediumvioletred', 'darkmagenta', 'indigo'])).set_title('card6', fontsize=16)\nplt.show(plot)", "processed": ["box plot say mastercard visa card fraudul non fraudul distribut similar similar mean also clear box plot discov american express card tend associ higher transact amount infer fact much higher mean compar two card brand", "card type card6", "card6 featur repres btype card transact made major card type dataset credit debit", "frequenc differ card type"]}, {"markdown": ["The two most common card types in this dataset are credit and debit (the rest are comparatively negligible). Debit cards are more common than credit cards in this dataset.", "### Fraudulence Proportion Plot"], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/ieee-cis-competition-fresh-eda-and-modeling\n\nfig, ax = plt.subplots(figsize=(10, 10))\n\nprops = train_df.query(\"TransactionAmt < 500\").query(\"card6 == 'credit' or card6 == 'debit'\")\\\n                .groupby(\"card6\")['isFraud'].value_counts(normalize=True).unstack()\n\nsns.set_palette(['plum', 'purple'])\nprops.plot(kind='bar', stacked='True', ax=ax).set_ylabel('Proportion')\nplt.show(plot)", "processed": ["two common card type dataset credit debit rest compar neglig debit card common credit card dataset", "fraudul proport plot"]}, {"markdown": ["In the above proportion plot, the height of the dark purple bar (at the top) represents the probability of a transaction received through a given card type being fraudulent. We can see that a given *credit* card is more likely involved in a fraudulent transaction as compared to a given *debit card*.", "### TransactionAmt vs. card6 Violin Plot"], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/ieee-cis-competition-fresh-eda-and-modeling\n\nfig, ax = plt.subplots(figsize=(15, 15))\n\nplot = sns.violinplot(x=\"card6\", y=\"TransactionAmt\", hue=\"isFraud\",\n                      data=train_df.query(\"TransactionAmt < 500\"),\n                      palette=['plum', 'purple'], split=True, ax=ax).set_title('TransactionAmt vs. card6', fontsize=16)\n\nplt.show(plot)", "processed": ["proport plot height dark purpl bar top repres probabl transact receiv given card type fraudul see given credit card like involv fraudul transact compar given debit card", "transactionamt v card6 violin plot"]}, {"markdown": ["In the above violin plots, the light purple sections represent the distribution for non-fraudulent cases and the dark purple sections represent the distribution for fraudulent cases. We can see the same trend as before. Both the distributions types have strong positive (leftward) skews. But, the light purple (non-fraudulent) distributions have a greater probability density concentrated around the lower values of transaction amounts, once again, because they have several peaks (unlike the dark purple distributions).\n\nIn the above violin plot, it can be seen that debit cards cards are associated with lower transaction amounts as compared to credit cards because their distributions have stronger positive (leftward) skew. Also, the credit card distributions have higher means.", "### TransactionAmt vs. card6 Box Plot"], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/ieee-cis-competition-fresh-eda-and-modeling\n\nfig, ax = plt.subplots(figsize=(15, 15))\n\nplot = sns.boxplot(x=\"card6\", y=\"TransactionAmt\", hue=\"isFraud\",\n                   data=train_df.query(\"TransactionAmt < 500\"),\n                   palette=['plum', 'purple'], ax=ax).set_title('TransactionAmt vs. card6', fontsize=16)\n\nplt.show(plot)", "processed": ["violin plot light purpl section repres distribut non fraudul case dark purpl section repres distribut fraudul case see trend distribut type strong posit leftward skew light purpl non fraudul distribut greater probabl densiti concentr around lower valu transact amount sever peak unlik dark purpl distribut violin plot seen debit card card associ lower transact amount compar credit card distribut stronger posit leftward skew also credit card distribut higher mean", "transactionamt v card6 box plot"]}, {"markdown": ["In the box plot above, we can see that credit cards are associated with much higher mean transaction amounts as compared to debit cards. ", "## \"M\" Features", "I am not sure about the meanings of these features (M1, M2, and so on till M9), but the competition overview states that these are categorical features.", "### Fraudulence Proportion Plots of \"M\" features", "### M1"], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/ieee-cis-competition-fresh-eda-and-modeling\n\nfig, ax = plt.subplots(figsize=(7.5, 7.5))\n\nprops = train_df.query(\"TransactionAmt < 500\")\\\n                .groupby(\"M1\")['isFraud'].value_counts(normalize=True).unstack()\n\nsns.set_palette(['coral', 'orangered'])\nprops.plot(kind='bar', stacked='True', ax=ax).set_ylabel('Proportion')\nplt.show(plot)", "processed": ["box plot see credit card associ much higher mean transact amount compar debit card", "featur", "sure mean featur m1 m2 till m9 competit overview state categor featur", "fraudul proport plot featur", "m1"]}, {"markdown": ["From the above proportion plot, it can be seen that the most fraudulence prone M1 value is 0.0. This is because all the NaN values were converted to 0s in the beginning. Therefore, an undefined M1 value has the highest chance of being fraudulent. Besides this, a sample with an M1 value of T has a much higher chance of being fraudulent than one with an M1 value of F.", "### M2"], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/ieee-cis-competition-fresh-eda-and-modeling\n\nfig, ax = plt.subplots(figsize=(7.5, 7.5))\n\nprops = train_df.query(\"TransactionAmt < 500\").query(\"card6 == 'credit' or card6 == 'debit'\")\\\n                .groupby(\"M2\")['isFraud'].value_counts(normalize=True).unstack()\n\nsns.set_palette(['lightsalmon', 'orangered'])\nprops.plot(kind='bar', stacked='True', ax=ax).set_ylabel('Proportion')\nplt.show(plot)", "processed": ["proport plot seen fraudul prone m1 valu 0 0 nan valu convert 0 begin therefor undefin m1 valu highest chanc fraudul besid sampl m1 valu much higher chanc fraudul one m1 valu f", "m2"]}, {"markdown": ["From the above proportion plot, it can be seen that the most fraudulence prone M2 value is 0.0. This is because all the NaN values were converted to 0s in the beginning. Therefore, an undefined M2 value has the highest chance of being fraudulent. Besides this, a sample with an M2 value of F has a much higher chance of being fraudulent than one with an M2 value of T.", "### M3"], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/ieee-cis-competition-fresh-eda-and-modeling\n\nfig, ax = plt.subplots(figsize=(7.5, 7.5))\n\nprops = train_df.query(\"TransactionAmt < 500\").query(\"card6 == 'credit' or card6 == 'debit'\")\\\n                .groupby(\"M3\")['isFraud'].value_counts(normalize=True).unstack()\n\nsns.set_palette(['peachpuff', 'darkorange'])\nprops.plot(kind='bar', stacked='True', ax=ax).set_ylabel('Proportion')\nplt.show(plot)", "processed": ["proport plot seen fraudul prone m2 valu 0 0 nan valu convert 0 begin therefor undefin m2 valu highest chanc fraudul besid sampl m2 valu f much higher chanc fraudul one m2 valu", "m3"]}, {"markdown": ["From the above proportion plot, it can be seen that the most fraudulence prone M3 value is 0.0. This is because all the NaN values were converted to 0s in the beginning. Therefore, an undefined M3 value has the highest chance of being fraudulent. Besides this, a sample with an M3 value of F has a much higher chance of being fraudulent than one with an M3 value of T.", "### M4"], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/ieee-cis-competition-fresh-eda-and-modeling\n\nfig, ax = plt.subplots(figsize=(7.5, 7.5))\n\nprops = train_df.query(\"TransactionAmt < 500\").query(\"card6 == 'credit' or card6 == 'debit'\")\\\n                .groupby(\"M4\")['isFraud'].value_counts(normalize=True).unstack()\n\nsns.set_palette(['lemonchiffon', 'gold'])\nprops.plot(kind='bar', stacked='True', ax=ax).set_ylabel('Proportion')\nplt.show(plot)", "processed": ["proport plot seen fraudul prone m3 valu 0 0 nan valu convert 0 begin therefor undefin m3 valu highest chanc fraudul besid sampl m3 valu f much higher chanc fraudul one m3 valu", "m4"]}, {"markdown": ["From the above proportion plot, it can be seen that a that a sample with an M4 value of M2 has a much higher chance of being fraudulent than one with an M4 value of M0. Also, a sample with an M4 value of M0 has a higher chance of being fraudulent than one with an M4 value of M1. An M4 value of 0.0 (or NaN) has the lowest chance of being fraudulent.", "### M5"], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/ieee-cis-competition-fresh-eda-and-modeling\n\nfig, ax = plt.subplots(figsize=(7.5, 7.5))\n\nprops = train_df.query(\"TransactionAmt < 500\").query(\"card6 == 'credit' or card6 == 'debit'\")\\\n                .groupby(\"M5\")['isFraud'].value_counts(normalize=True).unstack()\n\nsns.set_palette(['lightgreen', 'green'])\nprops.plot(kind='bar', stacked='True', ax=ax).set_ylabel('Proportion')\nplt.show(plot)", "processed": ["proport plot seen sampl m4 valu m2 much higher chanc fraudul one m4 valu m0 also sampl m4 valu m0 higher chanc fraudul one m4 valu m1 m4 valu 0 0 nan lowest chanc fraudul", "m5"]}, {"markdown": ["From the above proportion plot, it can be seen that the most fraudulence prone M5 value is 0.0. This is because all the NaN values were converted to 0s in the beginning. Therefore, an undefined M5 value has the highest chance of being fraudulent. Besides this, a sample with an M5 value of T has a much higher chance of being fraudulent than one with an M5 value of F.", "### M6"], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/ieee-cis-competition-fresh-eda-and-modeling\n\nfig, ax = plt.subplots(figsize=(7.5, 7.5))\n\nprops = train_df.query(\"TransactionAmt < 500\").query(\"card6 == 'credit' or card6 == 'debit'\")\\\n                .groupby(\"M6\")['isFraud'].value_counts(normalize=True).unstack()\n\nsns.set_palette(['lightblue', 'darkblue'])\nprops.plot(kind='bar', stacked='True', ax=ax).set_ylabel('Proportion')\nplt.show(plot)", "processed": ["proport plot seen fraudul prone m5 valu 0 0 nan valu convert 0 begin therefor undefin m5 valu highest chanc fraudul besid sampl m5 valu much higher chanc fraudul one m5 valu f", "m6"]}, {"markdown": ["From the above proportion plot, it can be seen that the most fraudulence prone M6 value is 0.0. This is because all the NaN values were converted to 0s in the beginning. Therefore, an undefined M6 value has the highest chance of being fraudulent. Besides this, a sample with an M6 value of F has a much higher chance of being fraudulent than one with an M6 value of T.", "### M7"], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/ieee-cis-competition-fresh-eda-and-modeling\n\nfig, ax = plt.subplots(figsize=(7.5, 7.5))\n\nprops = train_df.query(\"TransactionAmt < 500\").query(\"card6 == 'credit' or card6 == 'debit'\")\\\n                .groupby(\"M7\")['isFraud'].value_counts(normalize=True).unstack()\n\nsns.set_palette(['mediumslateblue', 'indigo'])\nprops.plot(kind='bar', stacked='True', ax=ax).set_ylabel('Proportion')\nplt.show(plot)", "processed": ["proport plot seen fraudul prone m6 valu 0 0 nan valu convert 0 begin therefor undefin m6 valu highest chanc fraudul besid sampl m6 valu f much higher chanc fraudul one m6 valu", "m7"]}, {"markdown": ["From the above proportion plot, it can be seen that the most fraudulence prone M7 value is 0.0. This is because all the NaN values were converted to 0s in the beginning. Therefore, an undefined M7 value has the highest chance of being fraudulent. Besides this, a sample with an M7 value of F has a very similar chance of being fraudulent to one with an M7 value of T.", "### M8"], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/ieee-cis-competition-fresh-eda-and-modeling\n\nfig, ax = plt.subplots(figsize=(7.5, 7.5))\n\nprops = train_df.query(\"TransactionAmt < 500\").query(\"card6 == 'credit' or card6 == 'debit'\")\\\n                .groupby(\"M8\")['isFraud'].value_counts(normalize=True).unstack()\n\nsns.set_palette(['violet', 'darkviolet'])\nprops.plot(kind='bar', stacked='True', ax=ax).set_ylabel('Proportion')\nplt.show(plot)", "processed": ["proport plot seen fraudul prone m7 valu 0 0 nan valu convert 0 begin therefor undefin m7 valu highest chanc fraudul besid sampl m7 valu f similar chanc fraudul one m7 valu", "m8"]}, {"markdown": ["From the above proportion plot, it can be seen that the most fraudulence prone M8 value is 0.0. This is because all the NaN values were converted to 0s in the beginning. Therefore, an undefined M8 value has the highest chance of being fraudulent. Besides this, a sample with an M8 value of T has a very similar chance of being fraudulent to one with an M8 value of F.", "### M9"], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/ieee-cis-competition-fresh-eda-and-modeling\n\nfig, ax = plt.subplots(figsize=(7.5, 7.5))\n\nprops = train_df.query(\"TransactionAmt < 500\").query(\"card6 == 'credit' or card6 == 'debit'\")\\\n                .groupby(\"M9\")['isFraud'].value_counts(normalize=True).unstack()\n\nsns.set_palette(['mediumorchid', 'purple'])\nprops.plot(kind='bar', stacked='True', ax=ax).set_ylabel('Proportion')\nplt.show(plot)", "processed": ["proport plot seen fraudul prone m8 valu 0 0 nan valu convert 0 begin therefor undefin m8 valu highest chanc fraudul besid sampl m8 valu similar chanc fraudul one m8 valu f", "m9"]}, {"markdown": ["**This trained model can be used to make predictions on the training data using:**\n\n> model.predict(X_test)", "### Visualize change in accuracy"], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/ieee-cis-competition-fresh-eda-and-modeling\n\nfig, ax = plt.subplots(figsize=(7, 7))\nplt.plot(history.history['acc'], color='blue')\nplt.plot(history.history['val_acc'], color='orangered')\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='upper left')\nplt.show()", "processed": ["train model use make predict train data use model predict x test", "visual chang accuraci"]}, {"markdown": ["It can be clearly seen from the above plot that the training and validation accuracies are increasing as the training process proceeds.", "### Visualize change in loss"], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/ieee-cis-competition-fresh-eda-and-modeling\n\nfig, ax = plt.subplots(figsize=(7, 7))\nplt.plot(history.history['loss'], color='blue')\nplt.plot(history.history['val_loss'], color='orangered')\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='upper left')\nplt.show()", "processed": ["clearli seen plot train valid accuraci increas train process proce", "visual chang loss"]}, {"markdown": ["## <a id=\"43\">Check data unbalance</a>\n\nWe will check the **target** (**is_attributed**) data unbalance. "], "code": "# Reference: https://www.kaggle.com/code/gpreda/talkingdata-adtracking-eda\n\nplt.figure()\nfig, ax = plt.subplots(figsize=(6,6))\nx = trainset['is_attributed'].value_counts().index.values\ny = trainset[\"is_attributed\"].value_counts().values\n# Bar plot\n# Order the bars descending on target mean\nsns.barplot(ax=ax, x=x, y=y)\nplt.ylabel('Number of values', fontsize=12)\nplt.xlabel('is_attributed value', fontsize=12)\nplt.tick_params(axis='both', which='major', labelsize=12)\nplt.show();", "processed": ["id 43 check data unbal check target attribut data unbal"]}, {"markdown": ["We can observe that in the two sets, **ip** and **os** and **channel** have values in the same ranges for both **is_attributed** values (0 and 1). Maximum values for **app** is almost double and maximum value for **device** is four times larger for **is_attributed** = 0.\n\nLet's visualize the distribution of values of **app**, **device**, **os** and **channel**, grouped on **is_attributed**."], "code": "# Reference: https://www.kaggle.com/code/gpreda/talkingdata-adtracking-eda\n\nvar = ['app','device','os','channel']\n\ni = 0\nt1 = trainset.loc[trainset['is_attributed'] != 0]\nt0 = trainset.loc[trainset['is_attributed'] == 0]\n\nsns.set_style('whitegrid')\nplt.figure()\nfig, ax = plt.subplots(1,4,figsize=(16,4))\n\nfor feature in var:\n    i += 1\n    plt.subplot(1,4,i)\n    sns.kdeplot(t1[feature], bw=0.5,label=\"is_attributed = 1\")\n    sns.kdeplot(t0[feature], bw=0.5,label=\"is_attributed = 0\")\n    plt.ylabel('Density plot', fontsize=12)\n    plt.xlabel(feature, fontsize=12)\n    locs, labels = plt.xticks()\n    plt.tick_params(axis='both', which='major', labelsize=12)\nplt.show();", "processed": ["observ two set ip o channel valu rang attribut valu 0 1 maximum valu app almost doubl maximum valu devic four time larger attribut 0 let visual distribut valu app devic o channel group attribut"]}, {"markdown": ["# <a id=\"6\">Data engineering<a/>\n\n## <a id=\"61\">Extract date and time data</a>"], "code": "# Reference: https://www.kaggle.com/code/gpreda/talkingdata-adtracking-eda\n\ntrainset['year'] = pd.to_datetime(trainset.click_time).dt.year\ntrainset['month'] = pd.to_datetime(trainset.click_time).dt.month\ntrainset['day'] = pd.to_datetime(trainset.click_time).dt.day\ntrainset['hour'] = pd.to_datetime(trainset.click_time).dt.hour\ntrainset['min'] = pd.to_datetime(trainset.click_time).dt.minute\ntrainset['sec'] = pd.to_datetime(trainset.click_time).dt.second\ntrainset.head()\ntrainset.describe()\nvar = ['day','hour']\n\ni = 0\nt1 = trainset.loc[trainset['is_attributed'] != 0]\nt0 = trainset.loc[trainset['is_attributed'] == 0]\n\nsns.set_style('whitegrid')\nplt.figure()\nfig, ax = plt.subplots(1,2,figsize=(12,4))\n\nfor feature in var:\n    i += 1\n    plt.subplot(1,2,i)\n    sns.kdeplot(t1[feature], bw=0.5,label=\"is_attributed = 1\")\n    sns.kdeplot(t0[feature], bw=0.5,label=\"is_attributed = 0\")\n    plt.ylabel('Density plot', fontsize=12)\n    plt.xlabel(feature, fontsize=12)\n    locs, labels = plt.xticks()\n    plt.tick_params(axis='both', which='major', labelsize=12)\nplt.show();", "processed": ["id 6 data engin id 61 extract date time data"]}, {"markdown": ["One can observe that the distribution for **true** (**is_attributed = 1**) clicks is more diverse (hour, min, sec) compared with **false** (**is_attributed = 0**). This might be explained in two ways: one explanation can be that due to the reduced number of **true** cases, the distribution is less uniform. Another explanation might be (to be verified with larger number of cases) that due to programatic nature of *artificial* (**false**) clicks, their distribution is more uniform. One observation, related to the density plot for hours: for both **true** and **false** there is a certain hourly profile, with a plateau between 1 and 16, a saddle between 16 and 20 and a peak between 21 and 22. The plateau for the artificial (**false**) clicks shows an additional pattern, with oscilant profile.", "Let's represent the hour distribution with an alternative method, using barplots to show percent from all data of the **true** data."], "code": "# Reference: https://www.kaggle.com/code/gpreda/talkingdata-adtracking-eda\n\nvar = ['day','hour']\n\nfor feature in var:\n    fig, ax = plt.subplots(figsize=(16,6))\n    # Calculate the percentage of target=1 per category value\n    cat_perc = trainset[[feature, 'is_attributed']].groupby([feature],as_index=False).mean()\n    cat_perc.sort_values(by='is_attributed', ascending=False, inplace=True)\n    # Bar plot\n    #sns.barplot(ax=ax,x=feature, y='is_attributed', data=cat_perc, order=cat_perc[feature]) #for ordered bars\n    sns.barplot(ax=ax,x=feature, y='is_attributed', data=cat_perc)\n    plt.ylabel('Percent of `is_attributed` with value 1 [%]', fontsize=12)\n    plt.xlabel(feature, fontsize=12)\n    plt.tick_params(axis='both', which='major', labelsize=12)\n    plt.show();", "processed": ["one observ distribut true attribut 1 click diver hour min sec compar fals attribut 0 might explain two way one explan due reduc number true case distribut le uniform anoth explan might verifi larger number case due programat natur artifici fals click distribut uniform one observ relat densiti plot hour true fals certain hourli profil plateau 1 16 saddl 16 20 peak 21 22 plateau artifici fals click show addit pattern oscil profil", "let repres hour distribut altern method use barplot show percent data true data"]}, {"markdown": ["Now we call the routines for additional features calculation."], "code": "# Reference: https://www.kaggle.com/code/gpreda/talkingdata-adtracking-eda\n\ntrainset = perform_countuniq( trainset, ['ip'], 'channel', 'X0', 'uint8', show_max=True ); gc.collect()\ntrainset = perform_cumcount( trainset, ['ip', 'device', 'os'], 'app', 'X1', show_max=True ); gc.collect()\ntrainset = perform_countuniq( trainset, ['ip', 'day'], 'hour', 'X2', 'uint8', show_max=True ); gc.collect()\ntrainset = perform_countuniq( trainset, ['ip'], 'app', 'X3', 'uint8', show_max=True ); gc.collect()\ntrainset = perform_countuniq( trainset, ['ip', 'app'], 'os', 'X4', 'uint8', show_max=True ); gc.collect()\ntrainset = perform_countuniq( trainset, ['ip'], 'device', 'X5', 'uint16', show_max=True ); gc.collect()\ntrainset = perform_countuniq( trainset, ['app'], 'channel', 'X6', show_max=True ); gc.collect()\ntrainset = perform_cumcount( trainset, ['ip'], 'os', 'X7', show_max=True ); gc.collect()\ntrainset = perform_countuniq( trainset, ['ip', 'device', 'os'], 'app', 'X8', show_max=True ); gc.collect()\ntrainset = perform_count( trainset, ['ip', 'day', 'hour'], 'ip_tcount', show_max=True ); gc.collect()\ntrainset = perform_count( trainset, ['ip', 'app'], 'ip_app_count', show_max=True ); gc.collect()\ntrainset = perform_count( trainset, ['ip', 'app', 'os'], 'ip_app_os_count', 'uint16', show_max=True ); gc.collect()\ntrainset = perform_var( trainset, ['ip', 'day', 'channel'], 'hour', 'ip_tchan_count', show_max=True ); gc.collect()\ntrainset = perform_var( trainset, ['ip', 'app', 'os'], 'hour', 'ip_app_os_var', show_max=True ); gc.collect()\ntrainset = perform_var( trainset, ['ip', 'app', 'channel'], 'day', 'ip_app_channel_var_day', show_max=True ); gc.collect()\ntrainset = perform_mean( trainset, ['ip', 'app', 'channel'], 'hour', 'ip_app_channel_mean_hour', show_max=True ); gc.collect()\ntrainset.head(5)\nvar = ['X0','X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X7']\n\ni = 0\nt1 = trainset.loc[trainset['is_attributed'] != 0]\nt0 = trainset.loc[trainset['is_attributed'] == 0]\n\nsns.set_style('whitegrid')\nplt.figure()\nfig, ax = plt.subplots(2,4,figsize=(14,8))\n\nfor feature in var:\n    i += 1\n    plt.subplot(2,4,i)\n    sns.kdeplot(t1[feature], bw=0.5,label=\"is_attributed = 1\")\n    sns.kdeplot(t0[feature], bw=0.5,label=\"is_attributed = 0\")\n    plt.ylabel('Density plot', fontsize=12)\n    plt.xlabel(feature, fontsize=12)\n    locs, labels = plt.xticks()\n    plt.tick_params(axis='both', which='major', labelsize=12)\nplt.show();\nvar = ['X8', 'ip_tcount', 'ip_app_count','ip_app_os_count', \n        'ip_tchan_count','ip_app_os_var','ip_app_channel_var_day', 'ip_app_channel_mean_hour']\n\ni = 0\nt1 = trainset.loc[trainset['is_attributed'] != 0]\nt0 = trainset.loc[trainset['is_attributed'] == 0]\n\nsns.set_style('whitegrid')\nplt.figure()\nfig, ax = plt.subplots(2,4,figsize=(14,8))\n\nfor feature in var:\n    i += 1\n    plt.subplot(2,4,i)\n    sns.kdeplot(t1[feature], bw=0.5,label=\"is_attributed = 1\")\n    sns.kdeplot(t0[feature], bw=0.5,label=\"is_attributed = 0\")\n    plt.ylabel('Density plot', fontsize=12)\n    plt.xlabel(feature, fontsize=12)\n    locs, labels = plt.xticks()\n    plt.tick_params(axis='both', which='major', labelsize=12)\nplt.show();", "processed": ["call routin addit featur calcul"]}, {"markdown": ["## <a id=\"63\">Data unbalance between train and test data</a>\n\nLet's compare the distribution of the features in the train and test datasets.\n\n\nLet's start with the data about application, devide, operating system and channel."], "code": "# Reference: https://www.kaggle.com/code/gpreda/talkingdata-adtracking-eda\n\nvar = ['app','device','os','channel']\n\n# Bar plot\nsns.set_style('whitegrid')\n\nplt.figure()\nfig, ax = plt.subplots(1,4,figsize=(16,4))\ni = 0\nfor feature in var:\n    i = i + 1\n    plt.subplot(1,4,i)\n    sns.kdeplot(trainset[feature], bw=0.5, label=\"train\")\n    sns.kdeplot(testset[feature], bw=0.5, label=\"test\")\n    plt.ylabel('Distribution', fontsize=12)\n    plt.xlabel(feature, fontsize=12)\n    locs, labels = plt.xticks()\n    #plt.setp(labels, rotation=90)\n    plt.tick_params(axis='both', which='major', labelsize=12)\nplt.show();", "processed": ["id 63 data unbal train test data let compar distribut featur train test dataset let start data applic devid oper system channel"]}, {"markdown": ["Let's continue with the data on time. First we will have to do the date and time extraction operation on testset data."], "code": "# Reference: https://www.kaggle.com/code/gpreda/talkingdata-adtracking-eda\n\ntestset['year'] = pd.to_datetime(testset.click_time).dt.year\ntestset['month'] = pd.to_datetime(testset.click_time).dt.month\ntestset['day'] = pd.to_datetime(testset.click_time).dt.day\ntestset['hour'] = pd.to_datetime(testset.click_time).dt.hour\ntestset['min'] = pd.to_datetime(testset.click_time).dt.minute\ntestset['sec'] = pd.to_datetime(testset.click_time).dt.second\ntestset.head()\ntestset.describe()\nvar = ['day','hour']\n\nsns.set_style('whitegrid')\n\nplt.figure()\nfig, ax = plt.subplots(1,2,figsize=(12,4))\ni = 0\nfor feature in var:\n    i = i + 1\n    plt.subplot(1,2,i)\n    sns.kdeplot(trainset[feature], bw=0.5, label=\"train\")\n    sns.kdeplot(testset[feature], bw=0.5, label=\"test\")\n    plt.ylabel('Distribution', fontsize=12)\n    plt.xlabel(feature, fontsize=12)\n    locs, labels = plt.xticks()\n    #plt.setp(labels, rotation=90)\n    plt.tick_params(axis='both', which='major', labelsize=12)\nplt.show();", "processed": ["let continu data time first date time extract oper testset data"]}, {"markdown": ["We can see that the day for the train data is for 6-9 November 2017, the test data is only from 10 November 2017. As well, the hours distribution are very different between train and test data.\n", "Let's calculate the derived features as well for the test set."], "code": "# Reference: https://www.kaggle.com/code/gpreda/talkingdata-adtracking-eda\n\ntestset = perform_countuniq( testset, ['ip'], 'channel', 'X0', 'uint8', show_max=True ); gc.collect()\ntestset = perform_cumcount( testset, ['ip', 'device', 'os'], 'app', 'X1', show_max=True ); gc.collect()\ntestset = perform_countuniq( testset, ['ip', 'day'], 'hour', 'X2', 'uint8', show_max=True ); gc.collect()\ntestset = perform_countuniq( testset, ['ip'], 'app', 'X3', 'uint8', show_max=True ); gc.collect()\ntestset = perform_countuniq( testset, ['ip', 'app'], 'os', 'X4', 'uint8', show_max=True ); gc.collect()\ntestset = perform_countuniq( testset, ['ip'], 'device', 'X5', 'uint16', show_max=True ); gc.collect()\ntestset = perform_countuniq( testset, ['app'], 'channel', 'X6', show_max=True ); gc.collect()\ntestset = perform_cumcount( testset, ['ip'], 'os', 'X7', show_max=True ); gc.collect()\ntestset = perform_countuniq( testset, ['ip', 'device', 'os'], 'app', 'X8', show_max=True ); gc.collect()\ntestset = perform_count( testset, ['ip', 'day', 'hour'], 'ip_tcount', show_max=True ); gc.collect()\ntestset = perform_count( testset, ['ip', 'app'], 'ip_app_count', show_max=True ); gc.collect()\ntestset = perform_count( testset, ['ip', 'app', 'os'], 'ip_app_os_count', 'uint16', show_max=True ); gc.collect()\ntestset = perform_var( testset, ['ip', 'day', 'channel'], 'hour', 'ip_tchan_count', show_max=True ); gc.collect()\ntestset = perform_var( testset, ['ip', 'app', 'os'], 'hour', 'ip_app_os_var', show_max=True ); gc.collect()\ntestset = perform_var( testset, ['ip', 'app', 'channel'], 'day', 'ip_app_channel_var_day', show_max=True ); gc.collect()\ntestset = perform_mean( testset, ['ip', 'app', 'channel'], 'hour', 'ip_app_channel_mean_hour', show_max=True ); gc.collect()\nvar = ['X0','X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X7']\n\nsns.set_style('whitegrid')\n\nplt.figure()\nfig, ax = plt.subplots(2,4,figsize=(14,8))\ni = 0\nfor feature in var:\n    i = i + 1\n    plt.subplot(2,4,i)\n    sns.kdeplot(trainset[feature], bw=0.5, label=\"train\")\n    sns.kdeplot(testset[feature], bw=0.5, label=\"test\")\n    plt.ylabel('Distribution', fontsize=12)\n    plt.xlabel(feature, fontsize=12)\n    locs, labels = plt.xticks()\n    #plt.setp(labels, rotation=90)\n    plt.tick_params(axis='both', which='major', labelsize=12)\nplt.show();\nvar = ['X8', 'ip_tcount', 'ip_app_count','ip_app_os_count', \n       'ip_tchan_count', 'ip_app_os_var','ip_app_channel_var_day', 'ip_app_channel_mean_hour']\n\nsns.set_style('whitegrid')\n\nplt.figure()\nfig, ax = plt.subplots(2,4,figsize=(14,8))\ni = 0\nfor feature in var:\n    i = i + 1\n    plt.subplot(2,4,i)\n    sns.kdeplot(trainset[feature], bw=0.5, label=\"train\")\n    sns.kdeplot(testset[feature], bw=0.5, label=\"test\")\n    plt.ylabel('Distribution', fontsize=12)\n    plt.xlabel(feature, fontsize=12)\n    locs, labels = plt.xticks()\n    #plt.setp(labels, rotation=90)\n    plt.tick_params(axis='both', which='major', labelsize=12)\nplt.show();", "processed": ["see day train data 6 9 novemb 2017 test data 10 novemb 2017 well hour distribut differ train test data", "let calcul deriv featur well test set"]}, {"markdown": ["There are about 1 Million images provided in the train dataset and there are 228 distinct labels which are used to label these images. There are two other sources of data as well - test data and validation data but in thie notebook I have only used images from train dataset.\n\n## 1.2 Which are the top used Labels in the dataset ?"], "code": "# Reference: https://www.kaggle.com/code/shivamb/imaterialist-fashion-eda-object-detection-colors\n\ntrain_labels = Counter(train_annotations)\n\nxvalues = list(train_labels.keys())\nyvalues = list(train_labels.values())\n\ntrace1 = go.Bar(x=xvalues, y=yvalues, opacity=0.8, name=\"year count\", marker=dict(color='rgba(20, 20, 20, 1)'))\nlayout = dict(width=800, title='Distribution of different labels in the train dataset', legend=dict(orientation=\"h\"));\n\nfig = go.Figure(data=[trace1], layout=layout);\niplot(fig);\nvalid_labels = Counter(valid_annotations)\n\nxvalues = list(valid_labels.keys())\nyvalues = list(valid_labels.values())\n\ntrace1 = go.Bar(x=xvalues, y=yvalues, opacity=0.8, name=\"year count\", marker=dict(color='rgba(20, 20, 20, 1)'))\nlayout = dict(width=800, title='Distribution of different labels in the valid dataset', legend=dict(orientation=\"h\"));\n\nfig = go.Figure(data=[trace1], layout=layout);\niplot(fig);\ndef get_images_for_labels(labellist, data):\n    image_ids = []\n    for each in data['annotations']:\n        if all(x in each['labelId'] for x in labellist):\n            image_ids.append(each['imageId'])\n            if len(image_ids) == 2:\n                break\n    image_urls = []\n    for each in data['images']:\n        if each['imageId'] in image_ids:\n            image_urls.append(each['url'])\n    return image_urls\n# most common labels \n\ntemps = train_labels.most_common(10)\nlabels_tr = [\"Label-\"+str(x[0]) for x in temps]\nvalues = [x[1] for x in temps]\n\ntrace1 = go.Bar(x=labels_tr, y=values, opacity=0.7, name=\"year count\", marker=dict(color='rgba(120, 120, 120, 0.8)'))\nlayout = dict(height=400, title='Top 10 Labels in the train dataset', legend=dict(orientation=\"h\"));\n\nfig = go.Figure(data=[trace1], layout=layout);\niplot(fig);", "processed": ["1 million imag provid train dataset 228 distinct label use label imag two sourc data well test data valid data thie notebook use imag train dataset 1 2 top use label dataset"]}, {"markdown": ["Label 66 is the most used label with almost 750K images tagged with this label in the training dataset"], "code": "# Reference: https://www.kaggle.com/code/shivamb/imaterialist-fashion-eda-object-detection-colors\n\ntemps = valid_labels.most_common(10)\nlabels_vl = [\"Label-\"+str(x[0]) for x in temps]\nvalues = [x[1] for x in temps]\n\ntrace1 = go.Bar(x=labels_vl, y=values, opacity=0.7, name=\"year count\", marker=dict(color='rgba(120, 120, 120, 0.8)'))\nlayout = dict(height=400, title='Top 10 Labels in the valid dataset', legend=dict(orientation=\"h\"));\n\nfig = go.Figure(data=[trace1], layout=layout);\niplot(fig);", "processed": ["label 66 use label almost 750k imag tag label train dataset"]}, {"markdown": ["Again, in the validation dataset, Label 66 is the most used label but second most label used is label-17 not label-105 of training dataset", "## 1.3 What are the most Common Co-Occuring Labels in the dataset\n\nSince every image can be classified into multiple labels, it will be interesting to note which lables have co-occured together"], "code": "# Reference: https://www.kaggle.com/code/shivamb/imaterialist-fashion-eda-object-detection-colors\n\n# Most Commonly Occuring Labels \n\ndef cartesian_reduct(alist):\n    results = []\n    for x in alist:\n        for y in alist:\n            if x == y:\n                continue\n            srtd = sorted([int(x),int(y)])\n            srtd = \" AND \".join([str(x) for x in srtd])\n            results.append(srtd)\n    return results \n\nco_occurance = []\nfor i, each in enumerate(train_inp['annotations']):\n    prods = cartesian_reduct(each['labelId'])\n    co_occurance.extend(prods)\ncoocur = Counter(co_occurance).most_common(10)\nlabels = list(reversed([\"Label: \"+str(x[0]) for x in coocur]))\nvalues = list(reversed([x[1] for x in coocur]))\n\ntrace1 = go.Bar(x=values, y=labels, opacity=0.7, orientation=\"h\", name=\"year count\", marker=dict(color='rgba(130, 130, 230, 0.8)'))\nlayout = dict(height=400, title='Most Common Co-Occuring Labels in the dataset', legend=dict(orientation=\"h\"));\n\nfig = go.Figure(data=[trace1], layout=layout);\niplot(fig);", "processed": ["valid dataset label 66 use label second label use label 17 label 105 train dataset", "1 3 common co occur label dataset sinc everi imag classifi multipl label interest note labl co occur togeth"]}, {"markdown": ["## 1.6 Frequency Distribution of Images with respective Labels Counts in the dataset\n\nLets visualize how many images are there in each label count bucket. "], "code": "# Reference: https://www.kaggle.com/code/shivamb/imaterialist-fashion-eda-object-detection-colors\n\nlbldst = Counter([len(x['labelId']) for x in srtedlist])\n\nlabels = list(lbldst.keys())\nvalues = list(lbldst.values())\n\ntrace1 = go.Bar(x=labels, y=values, opacity=0.7, name=\"year count\", marker=dict(color='rgba(10, 80, 190, 0.8)'))\nlayout = dict(height=400, title='Frequency distribution of images with respective labels counts ', legend=dict(orientation=\"h\"));\n\nfig = go.Figure(data=[trace1], layout=layout);\niplot(fig);", "processed": ["1 6 frequenc distribut imag respect label count dataset let visual mani imag label count bucket"]}, {"markdown": ["As we can see on average questions in train and test datasets are similar, but there are quite long questions in train dataset."], "code": "# Reference: https://www.kaggle.com/code/artgor/eda-and-lstm-cnn\n\nmax_features = 90000\ntk = Tokenizer(lower = True, filters='', num_words=max_features)\nfull_text = list(train['question_text'].values) + list(test['question_text'].values)\ntk.fit_on_texts(full_text)\ntrain_tokenized = tk.texts_to_sequences(train['question_text'].fillna('missing'))\ntest_tokenized = tk.texts_to_sequences(test['question_text'].fillna('missing'))\ntrain['question_text'].apply(lambda x: len(x.split())).plot(kind='hist');\nplt.yscale('log');\nplt.title('Distribution of question text length in characters')", "processed": ["see averag question train test dataset similar quit long question train dataset"]}, {"markdown": ["Ok, not much to say about it.", "## What can we say about the target? <a class=\"anchor\" id=\"target\"></a>"], "code": "# Reference: https://www.kaggle.com/code/allunia/santander-customer-transaction-eda\n\nfig, ax = plt.subplots(1,2,figsize=(20,5))\nsns.countplot(train.target.values, ax=ax[0], palette=\"husl\")\nsns.violinplot(x=train.target.values, y=train.index.values, ax=ax[1], palette=\"husl\")\nsns.stripplot(x=train.target.values, y=train.index.values,\n              jitter=True, ax=ax[1], color=\"black\", size=0.5, alpha=0.5)\nax[1].set_xlabel(\"Target\")\nax[1].set_ylabel(\"Index\");\nax[0].set_xlabel(\"Target\")\nax[0].set_ylabel(\"Counts\");\ntrain.loc[train.target==1].shape[0] / train.loc[train.target==0].shape[0]", "processed": ["ok much say", "say target class anchor id target"]}, {"markdown": ["### Take Away\n\n* We have to solve an imbalanced class problem. The number of customers that will not make a transaction is much higher than those that will. \n* It seem that there is no relationship of the target with the index of the train dataframe. This is more empressend by the zero targets than for the ones. \n* Take a look at the jitter plots within the violinplots. We can see that the targets look uniformly distributed over the indexes. It seems that the competitors were careful during the process of ordering the data. Once more this indicates that the data is simulated.", "## Can we find relationships between features? <a class=\"anchor\" id=\"correlation\"></a>", "### Linear correlations\n\nI have already seen some correlation heatmaps in public kernels and it seems as if there is almost no correlation between features. Let's check this out by computing all correlation values and plotting the overall distribution:"], "code": "# Reference: https://www.kaggle.com/code/allunia/santander-customer-transaction-eda\n\ntrain_correlations = train.drop([\"target\"], axis=1).corr()\ntrain_correlations = train_correlations.values.flatten()\ntrain_correlations = train_correlations[train_correlations != 1]\n\ntest_correlations = test.corr()\ntest_correlations = test_correlations.values.flatten()\ntest_correlations = test_correlations[test_correlations != 1]\n\nplt.figure(figsize=(20,5))\nsns.distplot(train_correlations, color=\"Red\", label=\"train\")\nsns.distplot(test_correlations, color=\"Green\", label=\"test\")\nplt.xlabel(\"Correlation values found in train (except 1)\")\nplt.ylabel(\"Density\")\nplt.title(\"Are there correlations between features?\"); \nplt.legend();", "processed": ["take away solv imbalanc class problem number custom make transact much higher seem relationship target index train datafram empressend zero target one take look jitter plot within violinplot see target look uniformli distribut index seem competitor care process order data indic data simul", "find relationship featur class anchor id correl", "linear correl alreadi seen correl heatmap public kernel seem almost correl featur let check comput correl valu plot overal distribut"]}, {"markdown": ["Let's take a look at n_top features of your choice:"], "code": "# Reference: https://www.kaggle.com/code/allunia/santander-customer-transaction-eda\n\nn_top = 5 \nimportances = grid.best_estimator_.feature_importances_\nidx = np.argsort(importances)[::-1][0:n_top]\nfeature_names = train.drop(\"target\", axis=1).columns.values\n\nplt.figure(figsize=(20,5))\nsns.barplot(x=feature_names[idx], y=importances[idx]);\nplt.title(\"What are the top important features to start with?\");", "processed": ["let take look n top featur choic"]}, {"markdown": ["Ok, that's enough to start with the \"data-understanding-journey\".", "### Exploring top features\n\nFirst of all: How do the distributions of the variables look like with respect to the targets in train? Can we observe discrepancies between train and test features for selected top features?"], "code": "# Reference: https://www.kaggle.com/code/allunia/santander-customer-transaction-eda\n\nfig, ax = plt.subplots(n_top,2,figsize=(20,5*n_top))\n\nfor n in range(n_top):\n    sns.distplot(train.loc[train.target==0, feature_names[idx][n]], ax=ax[n,0], color=\"Orange\", norm_hist=True)\n    sns.distplot(train.loc[train.target==1, feature_names[idx][n]], ax=ax[n,0], color=\"Red\", norm_hist=True)\n    sns.distplot(test.loc[:, feature_names[idx][n]], ax=ax[n,1], color=\"Mediumseagreen\", norm_hist=True)\n    ax[n,0].set_title(\"Train {}\".format(feature_names[idx][n]))\n    ax[n,1].set_title(\"Test {}\".format(feature_names[idx][n]))\n    ax[n,0].set_xlabel(\"\")\n    ax[n,1].set_xlabel(\"\")", "processed": ["ok enough start data understand journey", "explor top featur first distribut variabl look like respect target train observ discrep train test featur select top featur"]}, {"markdown": ["### How do the scatter plots look like?"], "code": "# Reference: https://www.kaggle.com/code/allunia/santander-customer-transaction-eda\n\ntop = top.join(train.target)\nsns.pairplot(top, hue=\"target\")", "processed": ["scatter plot look like"]}, {"markdown": ["Crazy! Can you see the sharp limits of several variables where the samples with target 1 suddenly accumulate and seldomly pass over. Look at var 81 and 12 for example. You can see that there are limits close to 10 (var 81) and 13.5 (var 12). This finding could be a nice entry point for further feature engineering.", "## Baseline submissions \n\n### What score does the forest yield on public LB?"], "code": "# Reference: https://www.kaggle.com/code/allunia/santander-customer-transaction-eda\n\ny_proba = grid.predict_proba(test.values)\ny_proba_train = grid.predict_proba(train.drop(\"target\", axis=1).values)\nfig, ax = plt.subplots(2,1,figsize=(20,8))\nsns.distplot(y_proba_train[train.target==1,1], norm_hist=True, color=\"mediumseagreen\",\n             ax=ax[0], label=\"1\")\nsns.distplot(y_proba_train[train.target==0,1], norm_hist=True, color=\"coral\",\n             ax=ax[0], label=\"0\")\nsns.distplot(y_proba[:,1], norm_hist=True,\n             ax=ax[1], color=\"purple\")\nax[1].set_xlabel(\"Predicted probability for test data\");\nax[1].set_ylabel(\"Density\");\nax[0].set_xlabel(\"Predicted probability for train data\");\nax[0].set_ylabel(\"Density\");\nax[0].legend();\nsubmission[\"target\"] = y_proba\nsubmission.to_csv(\"submission_baseline_forest.csv\", index=False)", "processed": ["crazi see sharp limit sever variabl sampl target 1 suddenli accumul seldomli pas look var 81 12 exampl see limit close 10 var 81 13 5 var 12 find could nice entri point featur engin", "baselin submiss score forest yield public lb"]}, {"markdown": ["### New feature importances"], "code": "# Reference: https://www.kaggle.com/code/allunia/santander-customer-transaction-eda\n\ncv = StratifiedKFold(n_splits=3, random_state=0)\nforest = RandomForestClassifier(max_depth=15, n_estimators=15, min_samples_leaf=20,\n                                n_jobs=-1)\n\nscores = []\nX = train.drop(\"target\", axis=1).values\ny = train.target.values\n\nfor train_idx, test_idx in cv.split(X, y):\n    x_train = X[train_idx]\n    x_test = X[test_idx]\n    y_train = y[train_idx]\n    y_test = y[test_idx]\n    \n    forest.fit(x_train, y_train)\n    y_proba = forest.predict_proba(x_test)\n    y_pred = np.zeros(y_proba.shape[0])\n    y_pred[y_proba[:,1] >= 0.166] = 1\n    \n    score = roc_auc_score(y_test, y_pred)\n    print(score)\n    scores.append(score)\n\nprint(np.round(np.mean(scores),4))\nprint(np.round(np.std(scores), 4))\nimportances = forest.feature_importances_\nfeature_names = train.drop(\"target\", axis=1).columns.values\nidx = np.argsort(importances)[::-1][0:30]\n\nplt.figure(figsize=(20,5))\nsns.barplot(x=feature_names[idx], y=importances[idx]);\nplt.xticks(rotation=90);", "processed": ["new featur import"]}, {"markdown": ["## Gaussian Mixture Clustering  <a class=\"anchor\" id=\"clustering\"></a>\n\nThe majority of the data looks like a big gaussian distribution. Besides that there seems to be at least one or two more gaussians that could explain the second and third mode that we can find for important features. Let's motivate this even further by looking at scatter and kde-plots of some top-features:"], "code": "# Reference: https://www.kaggle.com/code/allunia/santander-customer-transaction-eda\n\ncol1 = \"var_81\"\ncol2 = \"var_12\"\nN=70000\nfig, ax = plt.subplots(1,1, figsize=(20,10))\nsns.kdeplot(train[col1].values[0:N], train[col2].values[0:N])\nax.scatter(train[col1].values[0:N], train[col2].values[0:N],\n           s=2, c=train.target.values[0:N], cmap=\"coolwarm\", alpha=0.5)\nax.set_xlabel(col1)\nax.set_xlabel(col2);", "processed": ["gaussian mixtur cluster class anchor id cluster major data look like big gaussian distribut besid seem least one two gaussian could explain second third mode find import featur let motiv even look scatter kde plot top featur"]}, {"markdown": ["* At least one big gaussians with one or two small, very thin but long gaussians.\n* It's very interesting that we can still find outliers beside sharp lines. \n\nLet's assume now that the data was generated using a mixture of gaussians and let's try to cluster them. Perhaps we can see that some clusters occupy more hot targets than others."], "code": "# Reference: https://www.kaggle.com/code/allunia/santander-customer-transaction-eda\n\ncombined = train.drop([\"target\", \"Id\"], axis=1).append(test.drop(\"Id\", axis=1))\ncombined.shape\nmax_components = 10\nstart_components = 3\nn_splits = 3\nK = train.shape[0]\n\nX = train.loc[:, original_features].values[0:K]\ny = train.target.values[0:K]\nseeds = np.random.RandomState(0).randint(0,100, size=(max_components-start_components))\nseeds\nscaler = RobustScaler()\nX_scaled = scaler.fit_transform(X)\nif fit_gaussians:\n    components = np.arange(start_components, max_components, 1)\n    kf = StratifiedKFold(random_state=0, n_splits=n_splits)\n    \n    scores = np.zeros(shape=(max_components-start_components, n_splits))\n\n    for m in components:\n        split=0\n        print(\"Components \" + str(m))\n        for train_index, test_index in kf.split(X_scaled, y):\n            print(\"Split \" + str(split))\n            x_train, x_test = X_scaled[train_index], X_scaled[test_index]\n            gm = GaussianMixture(n_components=m, random_state=seeds[m-start_components])\n            gm.fit(x_train)\n            score = gm.score(x_test)\n            scores[m-start_components,split] = score\n            split +=1\n    \n    print(np.round(np.mean(scores, axis=1), 2))\n    print(np.round(np.std(scores, axis=1), 2))\n    best_idx = np.argmax(np.mean(scores, axis=1))\n    best_component = components[best_idx]\n    best_seed = seeds[best_idx]\n    print(\"Best component found \" + str(best_component))\n    \nelse:\n    best_seed = seeds[0]\n    best_component = 3\nX = train.loc[:, original_features].values\n\ngm = GaussianMixture(n_components=best_component, random_state=best_seed)\nX_scaled = scaler.transform(X)\ngm.fit(X_scaled)\ntrain[\"cluster\"] = gm.predict(X_scaled)\ntrain[\"logL\"] = gm.score_samples(X_scaled)\ntest[\"cluster\"] = gm.predict(test.loc[:, original_features].values)\ntest[\"logL\"] = gm.score_samples(test.loc[:, original_features].values)\nfig, ax = plt.subplots(1,2,figsize=(20,5))\nsns.countplot(train.cluster, palette=\"Set2\", ax=ax[0])\nsns.distplot(train.logL, color=\"Dodgerblue\", ax=ax[1]);", "processed": ["least one big gaussian one two small thin long gaussian interest still find outlier besid sharp line let assum data gener use mixtur gaussian let tri cluster perhap see cluster occupi hot target other"]}, {"markdown": ["### Take-Away\n\n* By fitting the gaussian mixture model we are maximizing the log likelihood. The higher, the better the gaussians suite to our data. As it's difficult to choose the right number of components (gaussians) I decided to use a stratified k fold of the train data. This way we can fit gaussians to a train subset, and test how big the log likelihood is on the test subset. By doing so three times for each selected component, we gain some more information about the stability of our solution. **We can see that 3 gaussians seem to be sufficient as the log likelihood values decrease with more components**. \n* This need not be true as the **solution depends on the initialization of the gaussians (the seeds I used) and with more data, the result may be different**. \n* But we can say: There are **at least 3 gaussians**. This is what we have already found by visual exploration of the data. \n* The individual score per data spot can be understood as a measure of density. If it's low, the data spot lives in a region with other data points far away. If it's high, it should have a lot of neighbors. Consequently the individual logL-score can tell us something about outliers in the data."], "code": "# Reference: https://www.kaggle.com/code/allunia/santander-customer-transaction-eda\n\ncluster_occupation = train.groupby(\"cluster\").target.value_counts() / train.groupby(\"cluster\").size() * 100\ncluster_occupation = cluster_occupation.loc[:, 1]\n\ntarget_occupation = train.groupby(\"target\").cluster.value_counts() / train.groupby(\"target\").size() * 100\ntarget_occupation = target_occupation.loc[1, :]\ntarget_occupation.index = target_occupation.index.droplevel(\"target\")\n\nfig, ax = plt.subplots(1,2,figsize=(20,5))\nax[0].set_title(\"How many % of the data per cluster has hot targets?\")\nsns.barplot(cluster_occupation.index, cluster_occupation.values, ax=ax[0], color=\"cornflowerblue\")\nax[0].set_ylabel(\"% of cluster data\")\nax[0].set_ylim([0,100])\n\nax[1].set_title(\"How many % of total hot targets are in one cluster?\")\nsns.barplot(target_occupation.index, target_occupation.values, ax=ax[1], color=\"tomato\")\nax[1].set_ylabel(\"% of hot targets\")\nax[1].set_ylim([0,100]);", "processed": ["take away fit gaussian mixtur model maxim log likelihood higher better gaussian suit data difficult choos right number compon gaussian decid use stratifi k fold train data way fit gaussian train subset test big log likelihood test subset three time select compon gain inform stabil solut see 3 gaussian seem suffici log likelihood valu decreas compon need true solut depend initi gaussian seed use data result may differ say least 3 gaussian alreadi found visual explor data individu score per data spot understood measur densiti low data spot live region data point far away high lot neighbor consequ individu logl score tell u someth outlier data"]}, {"markdown": ["* As we have much more cold-targets (zero) that hot (ones), I'm not surprised that hot targets occupy only a small part of the data per cluster. Nonetheless we can see that cluster 1 has significantly more hot targets than the others.\n* The second plot shows that most hot targets are located in cluster 1 followed by cluster 2. This confirms our assumption that the big gaussian in the middle (cluster 0) has the smallest amount of hot targets and that the small, thin side distributions are more likely to have hot targets. "], "code": "# Reference: https://www.kaggle.com/code/allunia/santander-customer-transaction-eda\n\nplt.figure(figsize=(20,5))\nfor n in range(gm.means_.shape[0]):\n    plt.plot(gm.means_[n,:], 'o')\nplt.title(\"How do the gaussian means look like?\")\nplt.ylabel(\"Cluster mean value\")\nplt.xlabel(\"Feature\")", "processed": ["much cold target zero hot one surpris hot target occupi small part data per cluster nonetheless see cluster 1 significantli hot target other second plot show hot target locat cluster 1 follow cluster 2 confirm assumpt big gaussian middl cluster 0 smallest amount hot target small thin side distribut like hot target"]}, {"markdown": ["### Validation\n"], "code": "# Reference: https://www.kaggle.com/code/allunia/rsna-ih-detection-baseline\n\nsplit_seed = 1\nkfold = StratifiedKFold(n_splits=5, random_state=split_seed).split(np.arange(traindf.shape[0]), traindf[\"any\"].values)\n\ntrain_idx, dev_idx = next(kfold)\n\ntrain_data = traindf.iloc[train_idx]\ndev_data = traindf.iloc[dev_idx]\n\n#train_data, dev_data = train_test_split(traindf, test_size=0.1, stratify=traindf.values, random_state=split_seed)\nprint(train_data.shape)\nprint(dev_data.shape)\npos_perc_train = train_data.sum() / train_data.shape[0] * 100\npos_perc_dev = dev_data.sum() / dev_data.shape[0] * 100\n\nfig, ax = plt.subplots(2,1,figsize=(20,14))\nsns.barplot(x=pos_perc_train.index, y=pos_perc_train.values, palette=\"Set2\", ax=ax[0]);\nax[0].set_title(\"Target distribution used for training data\")\nsns.barplot(x=pos_perc_dev.index, y=pos_perc_dev.values, palette=\"Set2\", ax=ax[1]);\nax[1].set_title(\"Target distribution used for dev data\");", "processed": ["valid"]}, {"markdown": ["## Let it run! :-)"], "code": "# Reference: https://www.kaggle.com/code/allunia/rsna-ih-detection-baseline\n\nBACKBONE = \"resnet_50\"\nBATCH_SIZE = 16\nTEST_BATCH_SIZE = 5\nMIN_VALUE = 0\nMAX_VALUE = 90\nSTEPS = 50\nEPOCHS = 20\n\nLR = 0.0001\ntrain_preprocessor = Preprocessor(path=train_dir,\n                                  backbone=pretrained_models[BACKBONE],\n                                  hu_min_value=MIN_VALUE,\n                                  hu_max_value=MAX_VALUE,\n                                  augment=True)\n\ndev_preprocessor = Preprocessor(path=train_dir,\n                                backbone=pretrained_models[BACKBONE],\n                                hu_min_value=MIN_VALUE,\n                                hu_max_value=MAX_VALUE,\n                                augment=False)\n\ntest_preprocessor = Preprocessor(path=test_dir,\n                                backbone=pretrained_models[BACKBONE],\n                                hu_min_value=MIN_VALUE,\n                                hu_max_value=MAX_VALUE,\n                                augment=False)\nfig, ax = plt.subplots(1,4,figsize=(20,20))\n\n\nfor m in range(4):\n    example = train_data.index.values[m]\n    title = [col for col in train_data.loc[example,:].index if train_data.loc[example, col]==1]\n    if len(title) == 0:\n        title=\"Healthy\"\n    preprocess_example = train_preprocessor.preprocess(example)\n    ax[m].imshow(preprocess_example[:,:,2], cmap=\"Spectral\")\n    ax[m].grid(False)\n    ax[m].set_title(title);\nfig, ax = plt.subplots(2,1,figsize=(20,10))\nsns.distplot(preprocess_example[:,:,2].flatten(), kde=False, ax=ax[0])\nsns.distplot(train_preprocessor.normalize(preprocess_example)[:,:,2].flatten(), kde=False)\nplt.title(\"Image distribution after normalisation\");\ntrain_preprocessor.normalize(preprocess_example[:,:,2]).min()\ntrain_preprocessor.normalize(preprocess_example[:,:,2]).max()\ntrain_dataloader = DataLoader(train_data,\n                              train_preprocessor,\n                              BATCH_SIZE,\n                              shuffle=True,\n                              steps=STEPS)\n\ndev_dataloader = DataLoader(dev_data, \n                            dev_preprocessor,\n                            BATCH_SIZE,\n                            shuffle=True,\n                            steps=STEPS)\n\ntest_dataloader = DataLoader(testdf, \n                             test_preprocessor,\n                             TEST_BATCH_SIZE,\n                             shuffle=False)\ntrain_dataloader.__len__()\nlen(train_dataloader.data_ids)/BATCH_SIZE\ntrain_data.loc[train_dataloader.data_ids][\"any\"].value_counts()\ntest_dataloader.__len__()\ndev_dataloader.__len__()\nmy_class_weights = [0.5, 0.1, 0.1, 0.1, 0.1, 0.1]\ndef turn_pred_to_dataframe(data_df, pred):\n    df = pd.DataFrame(pred, columns=data_df.columns, index=data_df.index)\n    df = df.stack().reset_index()\n    df.loc[:, \"ID\"] = df.id.str.cat(df.subtype, sep=\"_\")\n    df = df.drop([\"id\", \"subtype\"], axis=1)\n    df = df.rename({0: \"Label\"}, axis=1)\n    return df\nif train_brute_force:\n    model = MyNetwork(model_fun=resnet_50,\n                      loss_fun=\"binary_crossentropy\", #multilabel_focal_loss(class_weights=my_class_weights, alpha=0.5, gamma=0),\n                      metrics_list=[multilabel_focal_loss(alpha=0.5, gamma=0)],\n                      train_generator=train_dataloader,\n                      dev_generator=dev_dataloader,\n                      epochs=EPOCHS,\n                      num_classes=6)\n    model.build_model()\n    model.compile_model()\n    history = model.learn()\n    \n    print(history.history.keys())\n    \n    fig, ax = plt.subplots(2,1,figsize=(20,5))\n    ax[0].plot(history.history[\"loss\"], 'o-')\n    ax[0].plot(history.history[\"val_loss\"], 'o-')\n    ax[1].plot(history.history[\"lr\"], 'o-')\n    \n    #test_pred = model.predict(test_dataloader)[0:testdf.shape[0]]\n    #dev_pred = model.predict(dev_dataloader)\n    \n    #test_pred_df = turn_pred_to_dataframe(testdf, test_pred)\n    #dev_pred_df = turn_pred_to_dataframe(dev_data, dev_pred)\n    \n    #test_pred_df.to_csv(\"brute_force_test_pred.csv\", index=False)\n    #dev_pred_df.to_csv(\"brute_force_dev_pred.csv\", index=False)\ndev_proba = model.predict(dev_dataloader)\ndev_proba.shape\nplt.figure(figsize=(20,5))\nfor n in range(6):\n    sns.distplot(dev_proba[:,n])", "processed": ["let run"]}, {"markdown": ["This is a better representation. About 40 percent of the tweets are neutral followed by positive and negative tweets. "], "code": "# Reference: https://www.kaggle.com/code/parulpandey/eda-and-preprocessing-for-bert\n\ntrain['sentiment'].value_counts(normalize=True).iplot(kind='bar',\n                                                      yTitle='Percentage', \n                                                      linecolor='black', \n                                                      opacity=0.7,\n                                                      color='red',\n                                                      theme='pearl',\n                                                      bargap=0.6,\n                                                      gridcolor='white',\n                                                     \n                                                      title='Distribution of Sentiment column in the training set')", "processed": ["better represent 40 percent tweet neutral follow posit neg tweet"]}, {"markdown": ["It would be interesting to see if the distribution is also same in the test set."], "code": "# Reference: https://www.kaggle.com/code/parulpandey/eda-and-preprocessing-for-bert\n\ntest['sentiment'].value_counts(normalize=True).iplot(kind='bar',\n                                                      yTitle='Percentage', \n                                                      linecolor='black', \n                                                      opacity=0.7,\n                                                      color='red',\n                                                      theme='pearl',\n                                                      bargap=0.6,\n                                                      gridcolor='white',\n                                                      title='Distribution  of Sentiment column in the test set')", "processed": ["would interest see distribut also test set"]}, {"markdown": ["## 5.1 Sentence length analysis "], "code": "# Reference: https://www.kaggle.com/code/parulpandey/eda-and-preprocessing-for-bert\n\npos['text_len'].iplot(\n    kind='hist',\n    bins=100,\n    xTitle='text length',\n    linecolor='black',\n    color='red',\n    yTitle='count',\n    title='Positive Text Length Distribution')\n\nneg['text_len'].iplot(\n    kind='hist',\n    bins=100,\n    xTitle='text length',\n    linecolor='black',\n    color='green',\n    yTitle='count',\n    title='Negative Text Length Distribution')\n\nneutral['text_len'].iplot(\n    kind='hist',\n    bins=100,\n    xTitle='text length',\n    linecolor='black',\n    yTitle='count',\n    title='Neutral Text Length Distribution')", "processed": ["5 1 sentenc length analysi"]}, {"markdown": ["The histogram shows that the length of the cleaned text ranges from around 2 to 140 characters and generally,it is almost same for all the polarities.", "\nLet's see a more consolidated comparison of the relationship of text lengths with sentiment of the text."], "code": "# Reference: https://www.kaggle.com/code/parulpandey/eda-and-preprocessing-for-bert\n\ntrace0 = go.Box(\n    y=pos['text_len'],\n    name = 'Positive Text',\n    marker = dict(\n        color = 'red',\n    )\n)\n\ntrace1 = go.Box(\n    y=neg['text_len'],\n    name = 'Negative Text',\n    marker = dict(\n        color = 'green',\n    )\n)\n\ntrace2 = go.Box(\n    y=neutral['text_len'],\n    name = 'Neutral Text',\n    marker = dict(\n        color = 'orange',\n    )\n)\ndata = [trace0, trace1, trace2]\nlayout = go.Layout(\n    title = \"Length of the text\"\n)\n\nfig = go.Figure(data=data,layout=layout)\niplot(fig, filename = \"Length of the text of different polarities\")", "processed": ["histogram show length clean text rang around 2 140 charact gener almost polar", "let see consolid comparison relationship text length sentiment text"]}, {"markdown": ["All the text appear to have more or less same length. Hence, length of the text isn't a powerful indicator of the polarity.", "## 5.2  Text word count analysis"], "code": "# Reference: https://www.kaggle.com/code/parulpandey/eda-and-preprocessing-for-bert\n\npos['text_word_count'].iplot(\n    kind='hist',\n    bins=50,\n    xTitle='text length',\n    linecolor='black',\n    color='red',\n    yTitle='count',\n    title='Positive Text word count')\n\nneg['text_word_count'].iplot(\n    kind='hist',\n    bins=50,\n    xTitle='text length',\n    linecolor='black',\n    color='green',\n    yTitle='count',\n    title='Negative Text word count')\nneutral['text_word_count'].iplot(\n    kind='hist',\n    bins=50,\n    xTitle='text length',\n    linecolor='black',\n    yTitle='count',\n    title='Neutral Text word count')\n", "processed": ["text appear le length henc length text power indic polar", "5 2 text word count analysi"]}, {"markdown": ["Again, more or less, word count is also similar across positive, negative and neutral texts.This will be more clear with the Box Plots below."], "code": "# Reference: https://www.kaggle.com/code/parulpandey/eda-and-preprocessing-for-bert\n\ntrace0 = go.Box(\n    y=pos['text_word_count'],\n    name = 'Positive Text',\n    marker = dict(\n        color = 'red',\n    )\n)\n\ntrace1 = go.Box(\n    y=neg['text_word_count'],\n    name = 'Negative Text',\n    marker = dict(\n        color = 'green',\n    )\n)\n\ntrace2 = go.Box(\n    y=neutral['text_word_count'],\n    name = 'Neutral Text',\n    marker = dict(\n        color = 'orange',\n    )\n)\ndata = [trace0, trace1, trace2]\nlayout = go.Layout(\n    title = \"word count of the text\"\n)\n\nfig = go.Figure(data=data,layout=layout)\niplot(fig, filename = \"word count of the text of different polarities\")", "processed": ["le word count also similar across posit neg neutral text clear box plot"]}, {"markdown": ["## Distribution of top unigrams "], "code": "# Reference: https://www.kaggle.com/code/parulpandey/eda-and-preprocessing-for-bert\n\npos_unigrams = get_top_n_words(pos['text_clean'],20)\nneg_unigrams = get_top_n_words(neg['text_clean'],20)\nneutral_unigrams = get_top_n_words(neutral['text_clean'],20)\n\n\n\n#for word, freq in top_unigrams:\n    #print(word, freq)\ndf1 = pd.DataFrame(pos_unigrams, columns = ['Text' , 'count'])\ndf1.groupby('Text').sum()['count'].sort_values(ascending=True).iplot(\n    kind='bar', yTitle='Count', linecolor='black',color='red', title='Top 20 Unigrams in positve text',orientation='h')\n\ndf2 = pd.DataFrame(neg_unigrams, columns = ['Text' , 'count'])\ndf2.groupby('Text').sum()['count'].sort_values(ascending=True).iplot(\n    kind='bar', yTitle='Count', linecolor='black', color='green',title='Top 20 Unigrams in negative text',orientation='h')\n\ndf3 = pd.DataFrame(neutral_unigrams, columns = ['Text' , 'count'])\ndf3.groupby('Text').sum()['count'].sort_values(ascending=True).iplot(\n    kind='bar', yTitle='Count', linecolor='black', title='Top 20 Unigrams in neutral text',orientation='h')", "processed": ["distribut top unigram"]}, {"markdown": ["## Distribution of top Bigrams"], "code": "# Reference: https://www.kaggle.com/code/parulpandey/eda-and-preprocessing-for-bert\n\ndef get_top_n_gram(corpus,ngram_range,n=None):\n    vec = CountVectorizer(ngram_range=ngram_range,stop_words = 'english').fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\n\npos_bigrams = get_top_n_gram(pos['text_clean'],(2,2),20)\nneg_bigrams = get_top_n_gram(neg['text_clean'],(2,2),20)\nneutral_bigrams = get_top_n_gram(neutral['text_clean'],(2,2),20)\n\n\n\n#for word, freq in top_bigrams:\n    #print(word, freq)\ndf1 = pd.DataFrame(pos_bigrams, columns = ['Text' , 'count'])\ndf1.groupby('Text').sum()['count'].sort_values(ascending=True).iplot(\n    kind='bar', yTitle='Count', linecolor='black',color='red', title='Top 20 Bigrams in positve text',orientation='h')\n\ndf2 = pd.DataFrame(neg_bigrams, columns = ['Text' , 'count'])\ndf2.groupby('Text').sum()['count'].sort_values(ascending=True).iplot(\n    kind='bar', yTitle='Count', linecolor='black', color='green',title='Top 20 Bigrams in negative text',orientation='h')\n\ndf3 = pd.DataFrame(neutral_bigrams, columns = ['Text' , 'count'])\ndf3.groupby('Text').sum()['count'].sort_values(ascending=True).iplot(\n    kind='bar', yTitle='Count', linecolor='black', title='Top 20 Bigrams in neutral text',orientation='h')", "processed": ["distribut top bigram"]}, {"markdown": ["## Distribution of top Trigrams"], "code": "# Reference: https://www.kaggle.com/code/parulpandey/eda-and-preprocessing-for-bert\n\npos_trigrams = get_top_n_gram(pos['text_clean'],(3,3),20)\nneg_trigrams = get_top_n_gram(neg['text_clean'],(3,3),20)\nneutral_trigrams = get_top_n_gram(neutral['text_clean'],(3,3),20)\n\ndf1 = pd.DataFrame(pos_trigrams, columns = ['Text' , 'count'])\ndf1.groupby('Text').sum()['count'].sort_values(ascending=True).iplot(\n    kind='bar', yTitle='Count', linecolor='black',color='red', title='Top 20 Trigrams in positve text',orientation='h')\n\ndf2 = pd.DataFrame(neg_trigrams, columns = ['Text' , 'count'])\ndf2.groupby('Text').sum()['count'].sort_values(ascending=True).iplot(\n    kind='bar', yTitle='Count', linecolor='black', color='green',title='Top 20 Trigrams in negative text',orientation='h')\n\ndf3 = pd.DataFrame(neutral_trigrams, columns = ['Text' , 'count'])\ndf3.groupby('Text').sum()['count'].sort_values(ascending=True).iplot(\n    kind='bar', yTitle='Count', linecolor='black', title='Top 20 Trigrams in neutral text',orientation='h')", "processed": ["distribut top trigram"]}, {"markdown": ["## One of these things is not like the others...\n- When we display summary statistics of each feature we notice that one feature `wheezy-copper-turtle-magic` stands out."], "code": "# Reference: https://www.kaggle.com/code/robikscube/eda-and-baseline-lgb-for-instant-gratification\n\ntrain_df['wheezy-copper-turtle-magic'].plot(kind='hist', bins=500, figsize=(15, 5), title='Distribution of Feature wheezy-copper-turtle-magic')\nplt.show()\n# This feature is more categorical than continious\ntrain_df['wheezy-copper-turtle-magic'] = train_df['wheezy-copper-turtle-magic'].astype('category')\ntest_df['wheezy-copper-turtle-magic'] = test_df['wheezy-copper-turtle-magic'].astype('category')\nX_train['wheezy-copper-turtle-magic'] = X_train['wheezy-copper-turtle-magic'].astype('category')\nX_test['wheezy-copper-turtle-magic'] = X_test['wheezy-copper-turtle-magic'].astype('category')", "processed": ["one thing like other display summari statist featur notic one featur wheezi copper turtl magic stand"]}, {"markdown": ["## 257 Features with some interesting names.\n- stealthy-beige-pinscher-golden?\n- nerdy-indigo-wolfhound-sorted?\n\nThis data looks simulated. And the names are funny but will just require a lot of useless typing..\n\nIf we want to we could rename the columns for those of us who worked on santander... :D"], "code": "# Reference: https://www.kaggle.com/code/robikscube/eda-and-baseline-lgb-for-instant-gratification\n\nX_train.columns = ['var_{}'.format(x) for x in range(0, 256)]\nX_test.columns = ['var_{}'.format(x) for x in range(0, 256)]\naverage_of_feat = train_df.groupby('target').agg(['mean']).T.reset_index().rename(columns={'level_0':'feature'}).drop('level_1', axis=1)\naverage_of_feat['pos_neg_diff'] = np.abs(average_of_feat[0] - average_of_feat[1])\naverage_of_feat.sort_values('pos_neg_diff', ascending=True) \\\n    .tail(20).set_index('feature')['pos_neg_diff'].plot(kind='barh',\n                                                        title='Top 20 feature with biggest difference in mean between positive and negative class',\n                                                       figsize=(15, 7),\n                                                       color='grey')\nplt.show()", "processed": ["257 featur interest name stealthi beig pinscher golden nerdi indigo wolfhound sort data look simul name funni requir lot useless type want could renam column u work santand"]}, {"markdown": ["## Plot positive vs negative feature distributions\n..pretty boring - or is it?"], "code": "# Reference: https://www.kaggle.com/code/robikscube/eda-and-baseline-lgb-for-instant-gratification\n\nfig, axes = plt.subplots(10, 2, figsize=(20, 30))\ntop20_diff = average_of_feat.sort_values('pos_neg_diff', ascending=True).tail(20)['feature'].values\nax_position = 0\nfor var in top20_diff:\n    if var not in ['target','id']:\n        for i, d in train_df.groupby('target'):\n            d[var].plot(kind='hist', bins=100, alpha=0.5, title=var, label='target={}'.format(i), ax=axes.flat[ax_position])\n        axes.flat[ax_position].legend()\n        ax_position += 1\nplt.show()", "processed": ["plot posit v neg featur distribut pretti bore"]}, {"markdown": ["**Plot Loss per EPOCH**\n> plot_loss()"], "code": "# Reference: https://www.kaggle.com/code/jesucristo/gan-introduction\n\ndef plot_loss (G_losses, D_losses, epoch):\n    plt.figure(figsize=(10,5))\n    plt.title(\"Generator and Discriminator Loss - EPOCH \"+ str(epoch))\n    plt.plot(G_losses,label=\"G\")\n    plt.plot(D_losses,label=\"D\")\n    plt.xlabel(\"iterations\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.show()", "processed": ["plot loss per epoch plot loss"]}, {"markdown": ["# Generation example\n\n**WARNING,THIS CONTAINS IMAGES THAT MAY HURT THE SENSITIVITY OF SOME PEOPLE**"], "code": "# Reference: https://www.kaggle.com/code/jesucristo/gan-introduction\n\nshow_generated_img(7)\nif not os.path.exists('../output_images'):\n    os.mkdir('../output_images')\n    \nim_batch_size = 50\nn_images=10000\n\nfor i_batch in tqdm(range(0, n_images, im_batch_size)):\n    gen_z = torch.randn(im_batch_size, nz, 1, 1, device=device)\n    gen_images = netG(gen_z)\n    images = gen_images.to(\"cpu\").clone().detach()\n    images = images.numpy().transpose(0, 2, 3, 1)\n    for i_image in range(gen_images.size(0)):\n        save_image(gen_images[i_image, :, :, :], os.path.join('../output_images', f'image_{i_batch+i_image:05d}.png'))\nfig = plt.figure(figsize=(25, 16))\n# display 10 images from each class\nfor i, j in enumerate(images[:32]):\n    ax = fig.add_subplot(4, 8, i + 1, xticks=[], yticks=[])\n    plt.imshow(j)", "processed": ["gener exampl warn contain imag may hurt sensit peopl"]}, {"markdown": ["Next we can represent the topic vectors in 2-d with t-SNE. Finally we plot the results with plotly."], "code": "# Reference: https://www.kaggle.com/code/jpmiller/seeing-toxicity-with-lda-t-sne\n\n####\n#train_lda = train_features.toarray()\n\ntsne = TSNE(n_components=2, perplexity=8, n_iter=1600, verbose=1, angle=0.5)\ntrain_tsne = tsne.fit_transform(train_lda)\nx_tsne = train_tsne[:, 0]\ny_tsne = train_tsne[:, 1]\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\nfrom plotly.graph_objs import Scatter, Figure, Layout\ninit_notebook_mode(connected=False)\n\n# create datafRAME WITH comments, target, tsnex,tsney\n#separate into 2 groups of x_nice, x_notnice, y_nice, y_notnice\nplotme = pd.DataFrame({'comment':train_text, 'class':train_tgt, 'xcoord': x_tsne, 'ycoord':y_tsne})\nnices = plotme[plotme['class'] == 0]\nnotnices = plotme[plotme['class'] > 0]\n\n\n\ntrace_nices = Scatter(\n    x = nices['xcoord'],\n    y = nices['ycoord'],\n    mode = 'markers',\n    marker = dict(\n      size=7,\n      color='lightgray',\n      symbol='circle',\n      line = dict(width = 0,\n        color='gray'),\n      opacity = 0.3\n     ),\n    text=nices['comment']\n)\n\ntrace_notnices = Scatter(\n    x = notnices['xcoord'],\n    y = notnices['ycoord'],\n    mode = 'markers',\n    marker = dict(\n      size=8,\n      color=notnices['class'],\n      symbol='triangle-up',\n      line = dict(width = 0,\n        color='Darkred'),\n      opacity = 0.6\n     ),\n    text=notnices['comment']\n)\n\ndata=[trace_nices, trace_notnices]\n\nlayout = Layout(\n    title = 'We See You...',\n    showlegend=False,\n    xaxis=dict(\n        autorange=True,\n        showgrid=True,\n        zeroline=False,\n        showline=False,\n        autotick=True,\n        ticks='',\n        showticklabels=False\n    ),\n    yaxis=dict(\n        autorange=True,\n        showgrid=True,\n        zeroline=False,\n        showline=False,\n        autotick=True,\n        ticks='',\n        showticklabels=False\n    )\n)\n# Plot and embed in ipython notebook!\nfig = Figure(data=data, layout=layout)\niplot(fig, filename='jupyter/scatter1')", "processed": ["next repres topic vector 2 sne final plot result plotli"]}, {"markdown": ["## 2. Different Levels of Poverty Household Groups "], "code": "# Reference: https://www.kaggle.com/code/shivamb/costa-rica-poverty-exploration-kernel\n\ntarget = train['Target'].value_counts().to_frame()\nlevels = [\"NonVulnerable\", \"Moderate Poverty\", \"Vulnerable\", \"Extereme Poverty\"]\ntrace = go.Bar(y=target.Target, x=levels, marker=dict(color='orange', opacity=0.6))\nlayout = dict(title=\"Household Poverty Levels\", margin=dict(l=200), width=800, height=400)\ndata = [trace]\nfig = go.Figure(data=data, layout=layout)\niplot(fig)", "processed": ["2 differ level poverti household group"]}, {"markdown": ["## 3. What do households own ?"], "code": "# Reference: https://www.kaggle.com/code/shivamb/costa-rica-poverty-exploration-kernel\n\ndef compare_plot(col, title):\n    tr1 = train[train['Target'] == 1][col].value_counts().to_dict()\n    tr2 = train[train['Target'] == 2][col].value_counts().to_dict()\n    tr3 = train[train['Target'] == 3][col].value_counts().to_dict()\n    tr4 = train[train['Target'] == 4][col].value_counts().to_dict()\n    \n    xx = ['Extereme', 'Moderate', 'Vulnerable', 'NonVulnerable']\n    trace1 = go.Bar(y=[tr1[0], tr2[0], tr3[0], tr4[0]], name=\"Not Present\", x=xx, marker=dict(color=\"orange\", opacity=0.6))\n    trace2 = go.Bar(y=[tr1[1], tr2[1], tr3[1], tr4[1]], name=\"Present\", x=xx, marker=dict(color=\"purple\", opacity=0.6))\n    \n    return trace1, trace2 \n    \ntr1, tr2 = compare_plot(\"v18q\", \"Tablet\")\ntr3, tr4 = compare_plot(\"refrig\", \"Refrigirator\")\ntr5, tr6 = compare_plot(\"computer\", \"Computer\")\ntr7, tr8 = compare_plot(\"television\", \"Television\")\ntr9, tr10 = compare_plot(\"mobilephone\", \"MobilePhone\")\ntitles = [\"Tablet\", \"Refrigirator\", \"Computer\", \"Television\", \"MobilePhone\"]\n\nfig = tools.make_subplots(rows=3, cols=2, print_grid=False, subplot_titles=titles)\nfig.append_trace(tr1, 1, 1)\nfig.append_trace(tr2, 1, 1)\nfig.append_trace(tr3, 1, 2)\nfig.append_trace(tr4, 1, 2)\nfig.append_trace(tr5, 2, 1)\nfig.append_trace(tr6, 2, 1)\nfig.append_trace(tr7, 2, 2)\nfig.append_trace(tr8, 2, 2)\nfig.append_trace(tr9, 3, 1)\nfig.append_trace(tr10, 3, 1)\n\nfig['layout'].update(height=1000, title=\"What do Households Own\", barmode=\"stack\", showlegend=False)\niplot(fig)", "processed": ["3 household"]}, {"markdown": ["## 4. Key Characteristics of the Households \n\n## 4.1 Household Materials and Methods"], "code": "# Reference: https://www.kaggle.com/code/shivamb/costa-rica-poverty-exploration-kernel\n\ndef find_prominent(row, mats):\n    for c in mats:\n        if row[c] == 1:\n            return c\n    return \n\ndef combine(starter, colname, title, replacemap):\n    mats = [c for c in train.columns if c.startswith(starter)]\n    train[colname] = train.apply(lambda row : find_prominent(row, mats), axis=1)\n    train[colname] = train[colname].apply(lambda x : replacemap[x] if x != None else x )\n\n    om1 = train[train['Target'] == 1][colname].value_counts().to_frame()\n    om2 = train[train['Target'] == 2][colname].value_counts().to_frame()\n    om3 = train[train['Target'] == 3][colname].value_counts().to_frame()\n    om4 = train[train['Target'] == 4][colname].value_counts().to_frame()\n\n    trace1 = go.Bar(y=om1[colname], x=om1.index, name=\"Extereme\", marker=dict(color='red', opacity=0.9))\n    trace2 = go.Bar(y=om2[colname], x=om2.index, name=\"Moderate\", marker=dict(color='red', opacity=0.5))\n    trace3 = go.Bar(y=om3[colname], x=om3.index, name=\"Vulnerable\", marker=dict(color='green', opacity=0.5))\n    trace4 = go.Bar(y=om4[colname], x=om4.index, name=\"NonVulnerable\", marker=dict(color='green', opacity=0.9))\n    return [trace1, trace2, trace3, trace4]\n\ntitles = [\"Outside Wall Material\", \"Floor Material\", \"Roof Material\", \"Sanitary Conditions\", \"Cooking Energy Sources\", \"Disposal Methods\"]\nfig = tools.make_subplots(rows=3, cols=2, print_grid=False, subplot_titles=titles)\n\n### outside material\nflr = {'paredblolad' : \"Block / Brick\", \"paredpreb\" : \"Cement\", \"paredmad\" : \"Wood\",\n      \"paredzocalo\" : \"Socket\", \"pareddes\" : \"Waste Material\", \"paredfibras\" : \"Fibres\",\n      \"paredother\" : \"Other\", \"paredzinc\": \"Zink\"}\nres = combine(\"pared\", \"outside_material\", \"Predominanat Material of the External Walls\", flr)      \nfor x in res:\n    fig.append_trace(x, 1, 1)\n\n### floor material \nflr = {'pisomoscer' : \"Mosaic / Ceramic\", \"pisocemento\" : \"Cement\", \"pisonatur\" : \"Natural Material\",\n      \"pisonotiene\" : \"No Floor\", \"pisomadera\" : \"Wood\", \"pisoother\" : \"Other\"}\nres = combine(\"piso\", \"floor_material\", \"Floor Material of the Households\", flr)\nfor x in res:\n    fig.append_trace(x, 1, 2)\n\n### Roof Material\nflr = {'techozinc' : \"Zinc\", \"techoentrepiso\" : \"Fibre / Cement\", \"techocane\" : \"Natural Fibre\", \"techootro\" : \"Other\"}\nres = combine(\"tech\", \"roof_material\", \"Roof Material of the Households\", flr)  \nfor x in res:\n    fig.append_trace(x, 2, 1)\n\n\n### Sanitary Conditions\nflr = {'sanitario1' : \"No Toilet\", \"sanitario2\" : \"Sewer / Cesspool\", \"sanitario3\" : \"Septic Tank\",\n       \"sanitario5\" : \"Black Hole\", \"sanitario6\" : \"Other System\"}\nres = combine(\"sanit\", \"sanitary\", \"Sanitary Conditions of the Households\", flr)\nfor x in res:\n    fig.append_trace(x, 2, 2)\n\n### Energy Source\nflr = {'energcocinar1' : \"No Kitchen\", \"energcocinar2\" : \"Electricity\", \"energcocinar3\" : \"Cooking Gas\",\n       \"energcocinar4\" : \"Wood Charcoal\"}\nres = combine(\"energ\", \"energy_source\", \"Main source of energy for cooking\", flr)  \nfor x in res:\n    fig.append_trace(x, 3, 1)\n\n### Disposal Methods\nflr = {\"elimbasu1\":\"Tanker truck\",\n\"elimbasu2\": \"Buried\",\n\"elimbasu3\": \"Burning\",\n\"elimbasu4\": \"Unoccupied space\",\n\"elimbasu5\": \"River\",\n\"elimbasu6\": \"Other\"}\nres = combine(\"elim\", \"waste_method\", \"Rubbish Disposals Method\", flr)  \nfor x in res:\n    fig.append_trace(x, 3, 2)\n\nfig['layout'].update(height=900, title=\"Key Characteristics of Households\", barmode=\"stack\", showlegend=False)\niplot(fig)", "processed": ["4 key characterist household 4 1 household materi method"]}, {"markdown": ["## 4.2 Quality of Walls, Roof, and Floor"], "code": "# Reference: https://www.kaggle.com/code/shivamb/costa-rica-poverty-exploration-kernel\n\ndef find_prominent2(row, mats):\n    for i,c in enumerate(mats):\n        if row[c] == 1 and c.endswith(\"1\"):\n            return \"Bad\"\n        elif row[c] == 1 and c.endswith(\"2\"):\n            return \"Regular\"\n        elif row[c] == 1 and c.endswith(\"3\"):\n            return \"Good\"\n    return \n\nbadwl = [c for c in train.columns if c.startswith(\"epar\")]\nbadrf = [c for c in train.columns if c.startswith(\"etec\")]\nbadfl = [c for c in train.columns if c.startswith(\"eviv\")]\ntrain[\"WallQuality\"] = train.apply(lambda row : find_prominent2(row, badwl), axis=1)\ntrain[\"RoofQuality\"] = train.apply(lambda row : find_prominent2(row, badrf), axis=1)\ntrain[\"FloorQuality\"] = train.apply(lambda row : find_prominent2(row, badfl), axis=1)\n\nwd1 = train[train['Target']==1]['WallQuality'].value_counts()\nwd2 = train[train['Target']==2]['WallQuality'].value_counts()\nwd3 = train[train['Target']==3]['WallQuality'].value_counts()\nwd4 = train[train['Target']==4]['WallQuality'].value_counts()\ntrace1=go.Bar(x=wd1.index, y=wd1.values, marker=dict(color=\"red\", opacity=0.99), name=\"Extereme\")\ntrace2=go.Bar(x=wd2.index, y=wd2.values, marker=dict(color=\"red\", opacity=0.69), name=\"Moderate\")\ntrace3=go.Bar(x=wd3.index, y=wd3.values, marker=dict(color=\"red\", opacity=0.49), name=\"Vulnerable\")\ntrace4=go.Bar(x=wd4.index, y=wd4.values, marker=dict(color=\"red\", opacity=0.29), name=\"NonVulnerable\")\n\nwd1 = train[train['Target']==1]['RoofQuality'].value_counts()\nwd2 = train[train['Target']==2]['RoofQuality'].value_counts()\nwd3 = train[train['Target']==3]['RoofQuality'].value_counts()\nwd4 = train[train['Target']==4]['RoofQuality'].value_counts()\ntrace5=go.Bar(x=wd1.index, y=wd1.values, marker=dict(color=\"green\", opacity=0.99), name=\"Extereme\")\ntrace6=go.Bar(x=wd2.index, y=wd2.values, marker=dict(color=\"green\", opacity=0.69), name=\"Moderate\")\ntrace7=go.Bar(x=wd3.index, y=wd3.values, marker=dict(color=\"green\", opacity=0.49), name=\"Vulnerable\")\ntrace8=go.Bar(x=wd4.index, y=wd4.values, marker=dict(color=\"green\", opacity=0.29), name=\"NonVulnerable\")\n\nwd1 = train[train['Target']==1]['FloorQuality'].value_counts()\nwd2 = train[train['Target']==2]['FloorQuality'].value_counts()\nwd3 = train[train['Target']==3]['FloorQuality'].value_counts()\nwd4 = train[train['Target']==4]['FloorQuality'].value_counts()\ntrace9=go.Bar(x=wd1.index, y=wd1.values, marker=dict(color=\"purple\", opacity=0.99), name=\"Extereme\")\ntrace10=go.Bar(x=wd2.index, y=wd2.values, marker=dict(color=\"purple\", opacity=0.69), name=\"Moderate\")\ntrace11=go.Bar(x=wd3.index, y=wd3.values, marker=dict(color=\"purple\", opacity=0.49), name=\"Vulnerable\")\ntrace12=go.Bar(x=wd4.index, y=wd4.values, marker=dict(color=\"purple\", opacity=0.29), name=\"NonVulnerable\")\n\nfig = tools.make_subplots(rows=1, cols=4, print_grid=False, subplot_titles=[\"Extereme Poverty\", \"Moderate Poverty\", \"Vulnerable\", \"NonVulnerable\"])\nfig.append_trace(trace1, 1, 1)\nfig.append_trace(trace2, 1, 2)\nfig.append_trace(trace3, 1, 3)\nfig.append_trace(trace4, 1, 4)\nfig['layout'].update(height=250, showlegend=False, title=\"Wall Quality of Households\")\niplot(fig)\n\nfig = tools.make_subplots(rows=1, cols=4, print_grid=False, subplot_titles=[\"Extereme Poverty\", \"Moderate Poverty\", \"Vulnerable\", \"NonVulnerable\"])\nfig.append_trace(trace5, 1, 1)\nfig.append_trace(trace6, 1, 2)\nfig.append_trace(trace7, 1, 3)\nfig.append_trace(trace8, 1, 4)\nfig['layout'].update(height=250, showlegend=False, title=\"Roof Quality of Households\")\niplot(fig)\n\nfig = tools.make_subplots(rows=1, cols=4, print_grid=False, subplot_titles=[\"Extereme Poverty\", \"Moderate Poverty\", \"Vulnerable\", \"NonVulnerable\"])\nfig.append_trace(trace9, 1, 1)\nfig.append_trace(trace10, 1, 2)\nfig.append_trace(trace11, 1, 3)\nfig.append_trace(trace12, 1, 4)\nfig['layout'].update(height=250, showlegend=False, title=\"Floor Quality of Households\")\niplot(fig)", "processed": ["4 2 qualiti wall roof floor"]}, {"markdown": ["## 5. Family Details of the households\n\n## 5.1 Education Details, Status, and Members"], "code": "# Reference: https://www.kaggle.com/code/shivamb/costa-rica-poverty-exploration-kernel\n\ndef combine2(starter, colname, title, replacemap, plotme = True):\n    mats = [c for c in train.columns if c.startswith(starter)]\n    train[colname] = train.apply(lambda row : find_prominent(row, mats), axis=1)\n    train[colname] = train[colname].apply(lambda x : replacemap[x] if x != None else x )\n\n    om1 = train[train['Target'] == 1][colname].value_counts().to_frame()\n    om2 = train[train['Target'] == 2][colname].value_counts().to_frame()\n    om3 = train[train['Target'] == 3][colname].value_counts().to_frame()\n    om4 = train[train['Target'] == 4][colname].value_counts().to_frame()\n\n    trace1 = go.Bar(y=om1[colname], x=om1.index, name=\"Extereme\", marker=dict(color='red', opacity=0.9))\n    trace2 = go.Bar(y=om2[colname], x=om2.index, name=\"Moderate\", marker=dict(color='red', opacity=0.5))\n    trace3 = go.Bar(y=om3[colname], x=om3.index, name=\"Vulnerable\", marker=dict(color='orange', opacity=0.9))\n    trace4 = go.Bar(y=om4[colname], x=om4.index, name=\"NonVulnerable\", marker=dict(color='orange', opacity=0.5))\n\n    data = [trace1, trace2, trace3, trace4]\n    layout = dict(title=title, legend=dict(y=1.1, orientation=\"h\"), barmode=\"stack\", margin=dict(l=50), height=400)\n    fig = go.Figure(data=data, layout=layout)\n    if plotme:\n        iplot(fig)\n\n\nflr = {\"instlevel1\": \"No Education\", \"instlevel2\": \"Incomplete Primary\", \"instlevel3\": \"Complete Primary\", \n       \"instlevel4\": \"Incomplete Sc.\", \"instlevel5\": \"Complete Sc.\", \"instlevel6\": \"Incomplete Tech Sc.\",\n       \"instlevel7\": \"Complete Tech Sc.\", \"instlevel8\": \"Undergraduation\", \"instlevel9\": \"Postgraduation\"}\ncombine2(\"instl\", \"education_details\", \"Education Details of Family Members\", flr)  \n\nflr = {\"estadocivil1\": \"< 10 years\", \"estadocivil2\": \"Free / Coupled union\", \"estadocivil3\": \"Married\", \n       \"estadocivil4\": \"Divorced\", \"estadocivil5\": \"Separated\", \"estadocivil6\": \"Widow\",\n       \"estadocivil7\": \"Single\"}\ncombine2(\"estado\", \"status_members\", \"Status of Family Members\", flr)  \n\nflr = {\"parentesco1\": \"Household Head\", \"parentesco2\": \"Spouse/Partner\", \"parentesco3\": \"Son/Daughter\", \n       \"parentesco4\": \"Stepson/Daughter\", \"parentesco5\" : \"Son/Daughter in Law\" , \"parentesco6\": \"Grandson/Daughter\", \n       \"parentesco7\": \"Mother/Father\", \"parentesco8\": \"Mother/Father in Law\", \"parentesco9\" : \"Brother/Sister\" , \n       \"parentesco10\" : \"Brother/Sister in law\", \"parentesco11\" : \"Other Family Member\", \"parentesco12\" : \"Other Non Family Member\"}\ncombine2(\"parentesc\", \"family_members\", \"Family Members in the Households\", flr)  \n\nflr = {\"lugar1\": \"Central\", \"lugar2\": \"Chorotega\", \"lugar3\": \"Pac\u00c3\u0192\u00c2\u00adfico central\", \n       \"lugar4\": \"Brunca\", \"lugar5\": \"Huetar Atl\u00c3\u0192\u00c2\u00a1ntica\", \"lugar6\": \"Huetar Norte\"}\ncombine2(\"lugar\", \"region\", \"Region of the Households\", flr, plotme=False)  ", "processed": ["5 famili detail household 5 1 educ detail statu member"]}, {"markdown": ["## 5.2 Gender and Age Distributions "], "code": "# Reference: https://www.kaggle.com/code/shivamb/costa-rica-poverty-exploration-kernel\n\ndef agbr(col):\n    temp1 = train[train['Target'] == 1][col].value_counts()\n    trace1 = go.Bar(x=temp1.index, y=temp1.values, marker=dict(color=\"red\", opacity=0.89), name=\"Extereme\")\n\n    temp2 = train[train['Target'] == 2][col].value_counts()\n    trace2 = go.Bar(x=temp2.index, y=temp2.values, marker=dict(color=\"orange\", opacity=0.79), name=\"Moderate\")\n\n    temp3 = train[train['Target'] == 3][col].value_counts()\n    trace3 = go.Bar(x=temp3.index, y=temp3.values, marker=dict(color=\"purple\", opacity=0.89), name=\"Vulnerable\")\n\n    temp4 = train[train['Target'] == 4][col].value_counts()\n    trace4 = go.Bar(x=temp4.index, y=temp4.values, marker=dict(color=\"green\", opacity=0.79), name=\"NonVulnerable\")\n    \n    return [trace1, trace2, trace3, trace4]\n    layout = dict(height=400)\n    fig = go.Figure(data=[trace1, trace2, trace3, trace4], layout=layout)\n    iplot(fig)\n\ntitles = [\"Total Persons\", \"< 12 Yrs\", \">= 12 Yrs\", \"Total Males\", \"Males < 12 Yrs\", \"Males >= 12 Yrs\", \n         \"Total Females\", \"Females < 12 Yrs\", \"Females >= 12 Yrs\"]\nfig = tools.make_subplots(rows=3, cols=3, print_grid=False, subplot_titles=titles)\n\nres = agbr('r4t1')\nfor x in res:\n    fig.append_trace(x, 1, 1)\nres = agbr('r4t2')\nfor x in res:\n    fig.append_trace(x, 1, 2)\nres = agbr('r4t3')\nfor x in res:\n    fig.append_trace(x, 1, 3)\n\nres = agbr('r4h1')\nfor x in res:\n    fig.append_trace(x, 2, 1)\nres = agbr('r4h2')\nfor x in res:\n    fig.append_trace(x, 2, 2)\nres = agbr('r4h3')\nfor x in res:\n    fig.append_trace(x, 2, 3)\n\nres = agbr('r4m1')\nfor x in res:\n    fig.append_trace(x, 3, 1)\nres = agbr('r4m2')\nfor x in res:\n    fig.append_trace(x, 3, 2)\nres = agbr('r4m3')\nfor x in res:\n    fig.append_trace(x, 3, 3)\n\n    \nfig['layout'].update(height=750, showlegend=False, title=\"Gender and Age Distributions\")\niplot(fig)", "processed": ["5 2 gender age distribut"]}, {"markdown": ["## 5.3  Age Groups among the households"], "code": "# Reference: https://www.kaggle.com/code/shivamb/costa-rica-poverty-exploration-kernel\n\ntitles = [\"Children\", \"Adults\", \"65+ Old\"]\nfig = tools.make_subplots(rows=1, cols=3, print_grid=False, subplot_titles=titles)\n\nres = agbr(\"hogar_nin\")\nfor x in res:\n    fig.append_trace(x, 1, 1)\nres = agbr(\"hogar_adul\")\nfor x in res:\n    fig.append_trace(x, 1, 2)\nres = agbr(\"hogar_mayor\")\nfor x in res:\n    fig.append_trace(x, 1, 3)\n\nfig['layout'].update(height=350, title=\"People Distribution in Households\", barmode=\"stack\", showlegend=False)\niplot(fig)", "processed": ["5 3 age group among household"]}, {"markdown": ["## 5.4 Household Size"], "code": "# Reference: https://www.kaggle.com/code/shivamb/costa-rica-poverty-exploration-kernel\n\ntm = agbr('tamhog')\nlayout = dict(title=\"Household People Size\", margin=dict(l=100), height=400, legend=dict(orientation=\"h\", y=1))\nfig = go.Figure(data=tm, layout=layout)\niplot(fig)", "processed": ["5 4 household size"]}, {"markdown": ["## 6. Multivariate Analysis \n\n## 6.1 Monthly Rent Comparisons "], "code": "# Reference: https://www.kaggle.com/code/shivamb/costa-rica-poverty-exploration-kernel\n\ndef compare_dists(col, title):\n    trace1 = go.Histogram(name=\"Extereme\", x=train[train['Target']==1][col])\n    trace2 = go.Histogram(name=\"Moderate\", x=train[train['Target']==2][col])\n    trace3 = go.Histogram(name=\"Vulnerable\", x=train[train['Target']==3][col])\n    trace4 = go.Histogram(name=\"NonVulnerable\", x=train[train['Target']==4][col])\n\n    fig = tools.make_subplots(rows=2, cols=2, print_grid=False)\n    fig.append_trace(trace1, 1, 1)\n    fig.append_trace(trace2, 1, 2)\n    fig.append_trace(trace3, 2, 1)\n    fig.append_trace(trace4, 2, 2)\n\n    fig['layout'].update(height=400, showlegend=False, title=title)\n    iplot(fig)\n\ncompare_dists('v2a1', \"Monthy Rent for four groups of houses\")", "processed": ["6 multivari analysi 6 1 monthli rent comparison"]}, {"markdown": ["## 6.2 Poverty Levels with respect to Monthly Rent and Age of the House\n\nSize of the bubbles repersents levels of poverty. Higher the size, higher is the poverty level."], "code": "# Reference: https://www.kaggle.com/code/shivamb/costa-rica-poverty-exploration-kernel\n\ntrace0 = go.Scatter(x=train['v2a1'], y=train['age'], name=\"Extereme\", \n                    mode='markers', marker=dict(color=train['Target'], opacity=1, size=16 - train['Target']**2))\nlayout = go.Layout(xaxis=dict(title=\"Monthly Rent of the house\", range=(0,400000)), yaxis=dict(title=\"Age of the House\"))\nfig = go.Figure(data =[trace0], layout=layout)\niplot(fig)", "processed": ["6 2 poverti level respect monthli rent age hous size bubbl repers level poverti higher size higher poverti level"]}, {"markdown": ["## 6.3 Poverty Levels with respect to Number of Rooms and People Living"], "code": "# Reference: https://www.kaggle.com/code/shivamb/costa-rica-poverty-exploration-kernel\n\ntrain1 = train[train['Target'] == 1]\ntrain2 = train[train['Target'] == 2]\ntrain3 = train[train['Target'] == 3]\ntrain4 = train[train['Target'] == 4]\n\ntrace0 = go.Scatter(x=train1['rooms'], y=train1['tamviv'], name=\"Extereme\", mode='markers', marker=dict(color=\"red\", opacity=0.4, size=10))\ntrace1 = go.Scatter(x=train2['rooms'], y=train2['tamviv'], name=\"Moderate\", mode='markers', marker=dict(color=\"orange\",opacity=0.4, size=10))\ntrace2 = go.Scatter(x=train3['rooms'], y=train3['tamviv'], name=\"Vulnerable\", mode='markers', marker=dict(color=\"blue\",opacity=0.4, size=10))\ntrace3 = go.Scatter(x=train4['rooms'], y=train4['tamviv'], name=\"NonVulnerable\", mode='markers', marker=dict(color=\"green\",opacity=0.4, size=10))\n\nfig = tools.make_subplots(rows=2, cols=2, print_grid=False, subplot_titles=[\"Extereme Poverty\", \"Moderate Poverty\", \"Vulnerable\", \"Non Vulnerable\"])\n\nfig.append_trace(trace0, 1, 1)\nfig.append_trace(trace1, 1, 2)\nfig.append_trace(trace2, 2, 1)\nfig.append_trace(trace3, 2, 2)\n\nfig['layout'].update(height=600, showlegend=False, title=\"Rooms (Yaxis) and Persons Living (Xaxis)\" )\niplot(fig)", "processed": ["6 3 poverti level respect number room peopl live"]}, {"markdown": ["## 6.4 Distributions (Poverty Levels wise) of some continuous variables "], "code": "# Reference: https://www.kaggle.com/code/shivamb/costa-rica-poverty-exploration-kernel\n\ndef boxs(col, title):\n    y1 = train[train['Target'] == 1][col]\n    y2 = train[train['Target'] == 2][col]\n    y3 = train[train['Target'] == 3][col]\n    y4 = train[train['Target'] == 4][col]\n\n    trace1 = go.Box(y=y1, name=\"Extereme\", marker=dict(color=\"red\", opacity=0.7))\n    trace2 = go.Box(y=y2, name=\"Moderate\", marker=dict(color=\"orange\", opacity=0.7))\n    trace3 = go.Box(y=y3, name=\"Vulnerable\", marker=dict(color=\"purple\", opacity=0.7))\n    trace4 = go.Box(y=y4, name=\"NonVulnerable\", marker=dict(color=\"green\", opacity=0.7))\n    data = [trace1, trace2, trace3, trace4]\n    return data \n    layout = dict(title=title, showlegend=False, height=400)\n    data = [trace1, trace2, trace3, trace4]\n    fig = go.Figure(data=data, layout=layout)\n    iplot(fig)\n\n\ntitles = [\"Number of Rooms\", \"Number of Bedrooms\", \"Mobile Phones Owned\", \"Tablets Owned\", \"Age of the House\", \"Overcrowding Per Persons\"]\nfig = tools.make_subplots(rows=3, cols=2, print_grid=False, subplot_titles=titles)\n\nres = boxs(\"rooms\", \"Number of \")\nfor x in res:\n    fig.append_trace(x, 1, 1)\nres = boxs(\"bedrooms\", \"Number of \")\nfor x in res:\n    fig.append_trace(x, 1, 2)\nres = boxs(\"qmobilephone\", \"Number of\")\nfor x in res:\n    fig.append_trace(x, 2, 1)\nres = boxs(\"v18q1\", \"Number of\")\nfor x in res:\n    fig.append_trace(x, 2, 2)\nres = boxs(\"age\", \"Number of\")\nfor x in res:\n    fig.append_trace(x, 3, 1)\nres = boxs(\"overcrowding\", \"Number of\")\nfor x in res:\n    fig.append_trace(x, 3, 2)\n\nfig['layout'].update(height=900, title=\"\", barmode=\"stack\", showlegend=False)\niplot(fig)", "processed": ["6 4 distribut poverti level wise continu variabl"]}, {"markdown": ["## 6.5 Effect of Number of Rooms and Outside Material on Poverty Levels "], "code": "# Reference: https://www.kaggle.com/code/shivamb/costa-rica-poverty-exploration-kernel\n\ntrgt = {1:'Extereme Poverty', 2:'Moderate Poverty', 3: 'Vulnerable Households', 4:'Non Vulnerable Households'}\ntrain['target'] = train['Target'].apply(lambda x : trgt[x])\nsns.set(rc={'figure.figsize':(15, 6)})\nsns.boxplot(x=\"outside_material\", y=\"rooms\", hue=\"target\",  palette=\"cool\", data=train)\nplt.title(\"Effect of Number of Rooms and Outside Material on Poverty Levels\", fontsize=14)\nplt.xticks(rotation='vertical')\nplt.xlabel('Outside Material',fontsize=14)\nplt.ylabel('Number of Rooms',fontsize=14)\nplt.show()", "processed": ["6 5 effect number room outsid materi poverti level"]}, {"markdown": ["## 6.6 Effect of Number of Rooms and Floor Material on Poverty Levels "], "code": "# Reference: https://www.kaggle.com/code/shivamb/costa-rica-poverty-exploration-kernel\n\nsns.set(rc={'figure.figsize':(15, 6)})\nsns.boxplot(x=\"floor_material\", y=\"rooms\", hue=\"target\",  palette=\"gist_stern\", data=train)\nplt.title(\"Effect of Number of Rooms and Floor Material wise Poverty Levels\", fontsize=14)\nplt.xticks(rotation='vertical')\nplt.xlabel('Floor Material',fontsize=14)\nplt.ylabel('Number of Rooms',fontsize=14)\nplt.show()", "processed": ["6 6 effect number room floor materi poverti level"]}, {"markdown": ["## 6.7 Effect of Education Details and Household Size on Poverty Levels "], "code": "# Reference: https://www.kaggle.com/code/shivamb/costa-rica-poverty-exploration-kernel\n\nsns.set(rc={'figure.figsize':(15, 6)})\nsns.boxplot(x=\"education_details\", y=\"hhsize\", hue=\"target\",  palette=\"Spectral\", data=train)\nplt.title(\"Effect of Education Details and Household Size on Poverty Levels\", fontsize=14)\nplt.xticks(rotation='vertical')\nplt.xlabel('Education Details',fontsize=14)\nplt.ylabel('Household Size',fontsize=14)\nplt.show()\n", "processed": ["6 7 effect educ detail household size poverti level"]}, {"markdown": ["## 6.8 Effect of Family Members Status and Household Size on Poverty Levels "], "code": "# Reference: https://www.kaggle.com/code/shivamb/costa-rica-poverty-exploration-kernel\n\nsns.set(rc={'figure.figsize':(15, 6)})\nsns.boxplot(x=\"status_members\", y=\"hhsize\", hue=\"target\",  palette=\"rocket\", data=train)\nplt.title(\"Effect of Family Members Status and and Household Size on Poverty Levels\", fontsize=14)\nplt.xticks(rotation='vertical')\nplt.xlabel('Family Members Status',fontsize=14)\nplt.ylabel('Household Size',fontsize=14)\nplt.show()", "processed": ["6 8 effect famili member statu household size poverti level"]}, {"markdown": ["## 6.9 Effect of Family Memebrs Type and Household Size on Poverty Levels "], "code": "# Reference: https://www.kaggle.com/code/shivamb/costa-rica-poverty-exploration-kernel\n\nsns.set(rc={'figure.figsize':(15, 6)})\nsns.boxplot(x=\"family_members\", y=\"hhsize\", hue=\"target\",  palette=\"cool\", data=train)\nplt.title(\"Effect of Family Members and Household Size on Poverty Levels\", fontsize=14)\nplt.xticks(rotation='vertical')\nplt.xlabel('Family Members',fontsize=14)\nplt.ylabel('Household Size',fontsize=14)\nplt.show()", "processed": ["6 9 effect famili memebr type household size poverti level"]}, {"markdown": ["Why 150 000? That is the size of one test example that we ought to predict. ANd why negative bvalues of time to failure? In order to \"approach\" the zero..."], "code": "# Reference: https://www.kaggle.com/code/zikazika/memory-problems\n\n%%time\nplt.figure(figsize=(20,5))\nplt.plot(-plot1.compute(), plot2.compute());\nplt.xlabel(\"- Quaketime\")\nplt.ylabel(\"Signal\")\nplt.title(\"PLOT 0\");\n%%time\nplt.figure(figsize=(20,5))\nplt.plot(-train.time_to_failure.values[0:150000], train.acoustic_data.values[0:150000]);\nplt.xlabel(\"- Quaketime\")\nplt.ylabel(\"Signal\")\nplt.title(\"PLOT 1\");", "processed": ["150 000 size one test exampl ought predict neg bvalu time failur order approach zero"]}, {"markdown": ["Let us proceede with further interesting **EDA**"], "code": "# Reference: https://www.kaggle.com/code/zikazika/memory-problems\n\nfig, ax1 = plt.subplots(figsize=(16, 8))\nplt.title(\"Trends of acoustic_data and time_to_failure. 150 000 rows\")\nplt.plot(train['acoustic_data'].values[:15000000], color='y')\nax1.set_ylabel('acoustic_data', color='y')\nplt.legend(['acoustic_data'])\nax2 = ax1.twinx()\nplt.plot(train['time_to_failure'].values[:15000000], color='r')\nax2.set_ylabel('time_to_failure', color='r')\nplt.legend(['time_to_failure'], loc=(0.875, 0.9))\nplt.grid(False)", "processed": ["let u proceed interest eda"]}, {"markdown": ["This time around we plotted the independently but on the same graph and the conclusion is the same. Right before the problem happens signal increases. Ok what wcan we now say regarding the univariate distribution of the columns themselves?"], "code": "# Reference: https://www.kaggle.com/code/zikazika/memory-problems\n\nfig, ax = plt.subplots(1,2, figsize=(20,5))\nsns.distplot(train['acoustic_data'].values[:15000000], ax=ax[0], color=\"Yellow\", bins=100)\nax[0].set_xlabel(\"acoustic_data\")\nax[0].set_ylabel(\"Density\")\nax[0].set_title(\"acoustic_data distribution\")\n\n\nsns.distplot(train['time_to_failure'].values[:15000000], ax=ax[1], color=\"Red\", bins=100)\nax[1].set_xlabel(\"time_to_failure\")\nax[1].set_ylabel(\"Density\")\nax[1].set_title(\"time_to_failure distribution\");", "processed": ["time around plot independ graph conclus right problem happen signal increas ok wcan say regard univari distribut column"]}, {"markdown": ["# Build Adversarial Classifier\nTo distinguish train images from test images, we will use pretrained Xception."], "code": "# Reference: https://www.kaggle.com/code/cdeotte/steel-adversarial-validation\n\nimport pandas as pd\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.models import Model\nfrom keras import layers\nfrom keras.callbacks import LearningRateScheduler\nimport matplotlib.pyplot as plt, time\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom keras import applications\nbase_model = applications.Xception(weights=None, input_shape=(256, 256, 3), include_top=False)\nbase_model.load_weights('../input/keras-pretrained-models/xception_weights_tf_dim_ordering_tf_kernels_notop.h5')\nbase_model.trainable = False\nx = base_model.output\nx = layers.Flatten()(x)\nx = layers.Dense(1024, activation=\"relu\")(x)\nx = layers.Dropout(0.5)(x)\npredictions = layers.Dense(1, activation=\"sigmoid\")(x)\nmodel = Model(input = base_model.input, output = predictions)\nmodel.compile(loss='binary_crossentropy', optimizer = \"adam\", metrics=['accuracy'])\nimg_dir = '../tmp/'\nimg_height = 256; img_width = 256\nbatch_size = 32; nb_epochs = 15\n\ntrain_datagen = ImageDataGenerator(rescale=1./255,\n    horizontal_flip=True,\n    validation_split=0.2) # set validation split\n\ntrain_generator = train_datagen.flow_from_directory(\n    img_dir,\n    target_size=(img_height, img_width),\n    batch_size=batch_size,\n    class_mode='binary',\n    subset='training') # set as training data\n\nvalidation_generator = train_datagen.flow_from_directory(\n    img_dir, # same directory as training data\n    target_size=(img_height, img_width),\n    batch_size=batch_size,\n    class_mode='binary',\n    subset='validation') # set as validation data\n\nannealer = LearningRateScheduler(lambda x: 0.0001 * 0.95 ** x)\nh = model.fit_generator(\n    train_generator,\n    steps_per_epoch = train_generator.samples // batch_size,\n    validation_data = validation_generator, \n    validation_steps = validation_generator.samples // batch_size,\n    epochs = nb_epochs,\n    callbacks = [annealer],\n    verbose=2)\nplt.figure(figsize=(15,5))\nplt.plot(h.history['acc'],label='Train ACC')\nplt.plot(h.history['val_acc'],label='Val ACC')\nplt.title('TRAIN COMPARED WITH TEST. Training History')\nplt.legend()\nplt.show()", "processed": ["build adversari classifi distinguish train imag test imag use pretrain xception"]}, {"markdown": ["Well, folks, doesn't get any better than the AUC of 1.0! Let's see which ones are the most responsibel columns."], "code": "# Reference: https://www.kaggle.com/code/tunguz/adversarial-nfl\n\nfeature_imp = pd.DataFrame(sorted(zip(clf.feature_importance(),features)), columns=['Value','Feature'])\n\nplt.figure(figsize=(20, 10))\nsns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False).head(20))\nplt.title('LightGBM Features')\nplt.tight_layout()\nplt.show()\nplt.savefig('lgbm_importances-01.png')", "processed": ["well folk get better auc 1 0 let see one responsibel column"]}, {"markdown": ["OK, AUC of 0.999261 is not too shabby eaither. Let's see what's going on here."], "code": "# Reference: https://www.kaggle.com/code/tunguz/adversarial-nfl\n\nfeature_imp = pd.DataFrame(sorted(zip(clf.feature_importance(),features)), columns=['Value','Feature'])\n\nplt.figure(figsize=(20, 10))\nsns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False).head(20))\nplt.title('LightGBM Features')\nplt.tight_layout()\nplt.show()\nplt.savefig('lgbm_importances-01.png')\n", "processed": ["ok auc 0 999261 shabbi eaither let see go"]}, {"markdown": ["# 3. Exploring different categories of leaf diseases"], "code": "# Reference: https://www.kaggle.com/code/parulpandey/eda-histograms-color-spaces-masking\n\n# Categories of Images\nclasses = ['healthy', 'multiple_diseases', 'rust', 'scab']\nprint(f\"The dataset images belong to the following categories - {classes} \")\nfor c in classes:\n    print(f\"The class {c} has {train_df[c].sum()} samples\")\nhealthy = train_df[train_df['healthy'] == 1]['image_id'].to_list()\nmultiple_diseases = train_df[train_df['multiple_diseases'] == 1]['image_id'].to_list()\nrust = train_df[train_df['rust'] == 1]['image_id'].to_list()\nscab = train_df[train_df['scab'] == 1]['image_id'].to_list()\n\ndiseases = [len(healthy), len(multiple_diseases), len(rust), len(scab)]\ndiseases\n\ntrace = go.Bar(\n                    x = classes,\n                    y = diseases ,\n                    orientation='v',\n                    marker = dict(color=COLORS,\n                                 line=dict(color='black',width=1)),\n                    )\ndata = [trace]\nlayout = go.Layout(barmode = \"group\",title='',width=800, height=500, \n                       xaxis= dict(title='Leaf Categories'),\n                       yaxis=dict(title=\"Count\"),\n                       showlegend=False)\nfig = go.Figure(data = data, layout = layout)\niplot(fig)", "processed": ["3 explor differ categori leaf diseas"]}, {"markdown": ["Hence, there are no duplicate images in the datasets", "# 5. Visualizing Images\n\n## 5.1 Visualizing a random selection of images\n\nLet's visualise some random set of images containing both diseases and non diseased leaves."], "code": "# Reference: https://www.kaggle.com/code/parulpandey/eda-histograms-color-spaces-masking\n\nimages = train_df['image_id'].values\n\n# Extract 9 random images from it\nrandom_images = [np.random.choice(images+'.jpg') for i in range(6)]\n\n# Location of the image dir\nimg_dir = IMAGE_PATH\n\nprint('Display Random Images')\n\n# Adjust the size of your images\nplt.figure(figsize=(15,10))\n\n# Iterate and plot random images\nfor i in range(6):\n    plt.subplot(2, 3, i + 1)\n    img = plt.imread(os.path.join(img_dir, random_images[i]))\n    plt.imshow(img, cmap='gray')\n    plt.axis('off')\n    \n# Adjust subplot parameters to give specified padding\nplt.tight_layout()   \ndef display_images(images, Image_dir, condition):\n    random_images = [np.random.choice(images+'.jpg') for i in range(6)]\n\n    print(f\"Display {condition} Images\")\n\n   # Adjust the size of your images\n    plt.figure(figsize=(15,10))\n\n  # Iterate and plot random images\n    for i in range(6):\n        plt.subplot(2, 3, i + 1)\n        img = plt.imread(os.path.join(img_dir, random_images[i]))\n        plt.imshow(img, cmap='gray')\n        plt.axis('off')\n    \n# Adjust subplot parameters to give specified padding\n    plt.tight_layout()   \n    \n    ", "processed": ["henc duplic imag dataset", "5 visual imag 5 1 visual random select imag let visualis random set imag contain diseas non diseas leav"]}, {"markdown": ["# 6. Histograms\n\nHistograms are a graphical representation showing how frequently various color values occur in the image i.e frequency of pixels intensity values. In a RGB color space, pixel values range from 0 to 255 where 0 stands for black and 255 stands for white. Analysis of a histogram can help us understand thee brightness, contrast and intensity distribution of an image. Now let's look at the histogram of a random selected sample from each category.\n\n## Healthy image"], "code": "# Reference: https://www.kaggle.com/code/parulpandey/eda-histograms-color-spaces-masking\n\nf = plt.figure(figsize=(16,8))\nf.add_subplot(1,2, 1)\n\nsample_img = healthy[0]+'.jpg'\nraw_image = plt.imread(os.path.join(img_dir, sample_img))\nplt.imshow(raw_image, cmap='gray')\nplt.colorbar()\nplt.title('Healthy Image')\nprint(f\"Image dimensions:  {raw_image.shape[0],raw_image.shape[1]}\")\nprint(f\"Maximum pixel value : {raw_image.max():.1f} ; Minimum pixel value:{raw_image.min():.1f}\")\nprint(f\"Mean value of the pixels : {raw_image.mean():.1f} ; Standard deviation : {raw_image.std():.1f}\")\n\nf.add_subplot(1,2, 2)\n\n#_ = plt.hist(raw_image.ravel(),bins = 256, color = 'orange',)\n_ = plt.hist(raw_image[:, :, 0].ravel(), bins = 256, color = 'red', alpha = 0.5)\n_ = plt.hist(raw_image[:, :, 1].ravel(), bins = 256, color = 'Green', alpha = 0.5)\n_ = plt.hist(raw_image[:, :, 2].ravel(), bins = 256, color = 'Blue', alpha = 0.5)\n_ = plt.xlabel('Intensity Value')\n_ = plt.ylabel('Count')\n_ = plt.legend(['Red_Channel', 'Green_Channel', 'Blue_Channel'])\nplt.show()\n", "processed": ["6 histogram histogram graphic represent show frequent variou color valu occur imag e frequenc pixel intens valu rgb color space pixel valu rang 0 255 0 stand black 255 stand white analysi histogram help u understand thee bright contrast intens distribut imag let look histogram random select sampl categori healthi imag"]}, {"markdown": ["## Rust infested image"], "code": "# Reference: https://www.kaggle.com/code/parulpandey/eda-histograms-color-spaces-masking\n\nf = plt.figure(figsize=(16,8))\nf.add_subplot(1,2, 1)\n\nrust_img = rust[0]+'.jpg'\nrust_image = plt.imread(os.path.join(img_dir, rust_img))\nplt.imshow(rust_image, cmap='gray')\nplt.colorbar()\nplt.title('Rust Image')\nprint(f\"Image dimensions:  {raw_image.shape[0],raw_image.shape[1]}\")\nprint(f\"Maximum pixel value : {raw_image.max():.1f} ; Minimum pixel value:{raw_image.min():.1f}\")\nprint(f\"Mean value of the pixels : {raw_image.mean():.1f} ; Standard deviation : {raw_image.std():.1f}\")\n\nf.add_subplot(1,2, 2)\n#_ = plt.hist(raw_image.ravel(),bins = 256, color = 'orange',)\n_ = plt.hist(rust_image[:, :, 0].ravel(), bins = 256, color = 'red', alpha = 0.5)\n_ = plt.hist(rust_image[:, :, 1].ravel(), bins = 256, color = 'Green', alpha = 0.5)\n_ = plt.hist(rust_image[:, :, 2].ravel(), bins = 256, color = 'Blue', alpha = 0.5)\n_ = plt.xlabel('Intensity Value')\n_ = plt.ylabel('Count')\n_ = plt.legend(['Red_Channel', 'Green_Channel', 'Blue_Channel'])\nplt.show()", "processed": ["rust infest imag"]}, {"markdown": ["## Scab infested image"], "code": "# Reference: https://www.kaggle.com/code/parulpandey/eda-histograms-color-spaces-masking\n\nf = plt.figure(figsize=(16,8))\nf.add_subplot(1,2, 1)\n\nscab_img = scab[0]+'.jpg'\nscab_image = plt.imread(os.path.join(img_dir, scab_img))\nplt.imshow(scab_image, cmap='gray')\nplt.colorbar()\nplt.title('Scab Image')\nprint(f\"Image dimensions:  {raw_image.shape[0],raw_image.shape[1]}\")\nprint(f\"Maximum pixel value : {raw_image.max():.1f} ; Minimum pixel value:{raw_image.min():.1f}\")\nprint(f\"Mean value of the pixels : {raw_image.mean():.1f} ; Standard deviation : {raw_image.std():.1f}\")\n\nf.add_subplot(1,2, 2)\n#source: https://towardsdatascience.com/histograms-in-image-processing-with-skimage-python-be5938962935\n#_ = plt.hist(raw_image.ravel(),bins = 256, color = 'orange',)\n_ = plt.hist(scab_image[:, :, 0].ravel(), bins = 256, color = 'red', alpha = 0.5)\n_ = plt.hist(scab_image[:, :, 1].ravel(), bins = 256, color = 'Green', alpha = 0.5)\n_ = plt.hist(scab_image[:, :, 2].ravel(), bins = 256, color = 'Blue', alpha = 0.5)\n_ = plt.xlabel('Intensity Value')\n_ = plt.ylabel('Count')\n_ = plt.legend(['Red_Channel', 'Green_Channel', 'Blue_Channel'])\nplt.show()\n", "processed": ["scab infest imag"]}, {"markdown": ["## Multiple Diseases "], "code": "# Reference: https://www.kaggle.com/code/parulpandey/eda-histograms-color-spaces-masking\n\nf = plt.figure(figsize=(16,8))\nf.add_subplot(1,2, 1)\n\nmultiple_diseases_img = multiple_diseases[0]+'.jpg'\nmultiple_diseases_image = plt.imread(os.path.join(img_dir, multiple_diseases_img))\nplt.imshow(multiple_diseases_image, cmap='gray')\nplt.colorbar()\nplt.title('Multiple Diseases Image')\nprint(f\"Image dimensions:  {raw_image.shape[0],raw_image.shape[1]}\")\nprint(f\"Maximum pixel value : {raw_image.max():.1f} ; Minimum pixel value:{raw_image.min():.1f}\")\nprint(f\"Mean value of the pixels : {raw_image.mean():.1f} ; Standard deviation : {raw_image.std():.1f}\")\n\nf.add_subplot(1,2, 2)\n#source: https://towardsdatascience.com/histograms-in-image-processing-with-skimage-python-be5938962935\n#_ = plt.hist(raw_image.ravel(),bins = 256, color = 'orange',)\n_ = plt.hist(multiple_diseases_image[:, :, 0].ravel(), bins = 256, color = 'red', alpha = 0.5)\n_ = plt.hist(multiple_diseases_image[:, :, 1].ravel(), bins = 256, color = 'Green', alpha = 0.5)\n_ = plt.hist(multiple_diseases_image[:, :, 2].ravel(), bins = 256, color = 'Blue', alpha = 0.5)\n_ = plt.xlabel('Intensity Value')\n_ = plt.ylabel('Count')\n_ = plt.legend(['Red_Channel', 'Green_Channel', 'Blue_Channel'])\nplt.show()\n", "processed": ["multipl diseas"]}, {"markdown": ["## 3D scatter plot for the image in RGB "], "code": "# Reference: https://www.kaggle.com/code/parulpandey/eda-histograms-color-spaces-masking\n\nr, g, b = cv2.split(img)\nfig = plt.figure()\naxis = fig.add_subplot(1, 1, 1, projection=\"3d\")\n\npixel_colors = img.reshape((np.shape(img)[0]*np.shape(img)[1], 3))\nnorm = colors.Normalize(vmin=-1.,vmax=1.)\nnorm.autoscale(pixel_colors)\npixel_colors = norm(pixel_colors).tolist()\n\naxis.scatter(r.flatten(), g.flatten(), b.flatten(), facecolors=pixel_colors, marker=\".\")\naxis.set_xlabel(\"Red\")\naxis.set_ylabel(\"Green\")\naxis.set_zlabel(\"Blue\")\nplt.show()\n\n\n", "processed": ["3d scatter plot imag rgb"]}, {"markdown": ["## 3D scatter plot for the image in HSV"], "code": "# Reference: https://www.kaggle.com/code/parulpandey/eda-histograms-color-spaces-masking\n\nhsv_image = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)\nh, s, v = cv2.split(hsv_image)\nfig = plt.figure()\naxis = fig.add_subplot(1, 1, 1, projection=\"3d\")\n\naxis.scatter(h.flatten(), s.flatten(), v.flatten(), facecolors=pixel_colors, marker=\".\")\naxis.set_xlabel(\"Hue\")\naxis.set_ylabel(\"Saturation\")\naxis.set_zlabel(\"Value\")\nplt.show()", "processed": ["3d scatter plot imag hsv"]}, {"markdown": ["It is quite evident that the colors in HSV colorspace are more concenterated in a particlular region and this property proves to be very useful for processes like Segmentation based on colors.", " # Image segmentation based on Green\n \n "], "code": "# Reference: https://www.kaggle.com/code/parulpandey/eda-histograms-color-spaces-masking\n\nboundaries = [([30,0,0],[70,255,255])]\nmask = cv2.inRange(hsv_image, (36, 0, 0), (70, 255,255))\nresult = cv2.bitwise_and(img, img, mask=mask)\n\nplt.figure(figsize=(16,8))\nplt.subplot(1, 2, 1)\nplt.imshow(mask, cmap=\"gray\")\nplt.subplot(1, 2, 2)\nplt.imshow(result)\nplt.show()\n", "processed": ["quit evid color hsv colorspac concenter particlular region properti prove use process like segment base color", "imag segment base green"]}, {"markdown": ["Let's check for another diseased leaf"], "code": "# Reference: https://www.kaggle.com/code/parulpandey/eda-histograms-color-spaces-masking\n\nimg_raw2 = cv2.imread('/kaggle/input/plant-pathology-2020-fgvc7/images/Train_3.jpg')\n\nimg2 = cv2.cvtColor(img_raw2, cv2.COLOR_BGR2RGB)\nhsv_image2 = cv2.cvtColor(img2, cv2.COLOR_RGB2HSV)\nplt.imshow(img2)\n\nboundaries = [([30,0,0],[70,255,255])]\nmask = cv2.inRange(hsv_image2, (36, 0, 0), (70, 255,255))\nresult = cv2.bitwise_and(img2, img2, mask=mask)\n\nplt.figure(figsize=(16,8))\nplt.subplot(1, 2, 1)\nplt.imshow(mask, cmap=\"gray\")\nplt.subplot(1, 2, 2)\nplt.imshow(result)\nplt.show()", "processed": ["let check anoth diseas leaf"]}, {"markdown": ["So with AUC of about 0.74, there seems to be a farily significant difference in distribution of features that we have used between the train and test sets. So let's take a look at what features are most responsible for this difference:"], "code": "# Reference: https://www.kaggle.com/code/tunguz/adversarial-geotab\n\nfeature_imp = pd.DataFrame(sorted(zip(clf.feature_importance(),FEAT_COLS)), columns=['Value','Feature'])\n\nplt.figure(figsize=(20, 10))\nsns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False).head(20))\nplt.title('LightGBM Features')\nplt.tight_layout()\nplt.show()\nplt.savefig('lgbm_importances-01.png')", "processed": ["auc 0 74 seem farili signific differ distribut featur use train test set let take look featur respons differ"]}, {"markdown": ["# Modelling"], "code": "# Reference: https://www.kaggle.com/code/xhlulu/severstal-predict-missing-masks\n\nBATCH_SIZE = 32\n\ndef create_datagen():\n    return ImageDataGenerator(\n        zoom_range=0.1,  # set range for random zoom\n        # set mode for filling points outside the input boundaries\n        fill_mode='constant',\n        cval=0.,\n        rotation_range=10,\n        height_shift_range=0.1,\n        width_shift_range=0.1,\n        horizontal_flip=True,\n        vertical_flip=True,\n        rescale=1/255.,\n        validation_split=0.15\n    )\n\ndef create_test_gen():\n    return ImageDataGenerator(rescale=1/255.).flow_from_dataframe(\n        test_nan_df,\n        directory='../input/severstal-steel-defect-detection/test_images/',\n        x_col='ImageId',\n        class_mode=None,\n        target_size=(256, 256),\n        batch_size=BATCH_SIZE,\n        shuffle=False\n    )\n\ndef create_flow(datagen, subset):\n    return datagen.flow_from_dataframe(\n        train_nan_df, \n        directory='../tmp/train',\n        x_col='ImageId', \n        y_col='allMissing', \n        class_mode='other',\n        target_size=(256, 256),\n        batch_size=BATCH_SIZE,\n        subset=subset\n    )\n\n# Using original generator\ndata_generator = create_datagen()\ntrain_gen = create_flow(data_generator, 'training')\nval_gen = create_flow(data_generator, 'validation')\ntest_gen = create_test_gen()\ndef build_model():\n    densenet = DenseNet121(\n        include_top=False,\n        input_shape=(256,256,3),\n        weights='../input/densenet-keras/DenseNet-BC-121-32-no-top.h5'\n    )\n    \n    model = Sequential()\n    model.add(densenet)\n    model.add(layers.GlobalAveragePooling2D())\n    model.add(layers.BatchNormalization())\n    model.add(layers.Dropout(0.5))\n    model.add(layers.Dense(512, activation='relu'))\n    model.add(layers.BatchNormalization())\n    model.add(layers.Dropout(0.5))\n    model.add(layers.Dense(1, activation='sigmoid'))\n    \n    model.compile(\n        loss='binary_crossentropy',\n        optimizer=Nadam(),\n        metrics=['accuracy']\n    )\n    \n    return model\nmodel = build_model()\nmodel.summary()\ntotal_steps = train_nan_df.shape[0] / BATCH_SIZE\n\ncheckpoint = ModelCheckpoint(\n    'model.h5', \n    monitor='val_acc', \n    verbose=1, \n    save_best_only=True, \n    save_weights_only=False,\n    mode='auto'\n)\n\nreduce_lr = ReduceLROnPlateau(\n    monitor='val_loss',\n    patience=5,\n    verbose=1,\n    min_lr=1e-6\n)\n\nhistory = model.fit_generator(\n    train_gen,\n    steps_per_epoch=total_steps * 0.85,\n    validation_data=val_gen,\n    validation_steps=total_steps * 0.15,\n    epochs=40,\n    callbacks=[checkpoint, reduce_lr]\n)\nhistory_df = pd.DataFrame(history.history)\nhistory_df[['loss', 'val_loss']].plot()\nhistory_df[['acc', 'val_acc']].plot()", "processed": ["model"]}, {"markdown": ["So this means that only 278 loans have some other type. Let's fo deeper."], "code": "# Reference: https://www.kaggle.com/code/artgor/eda-basic-fe-and-lgb\n\nnon_zero_good_price = application_train[application_train['AMT_GOODS_PRICE'].isnull() == False]\ncredit_to_good_price = non_zero_good_price['AMT_CREDIT'] / non_zero_good_price['AMT_GOODS_PRICE']\nplt.boxplot(credit_to_good_price);\nplt.title('Credit amount to goods price.');", "processed": ["mean 278 loan type let fo deeper"]}, {"markdown": ["We can see that most of the loans have the amount which is similar to the goods price, but there are some outliers.", "##### NAME_HOUSING_TYPE"], "code": "# Reference: https://www.kaggle.com/code/artgor/eda-basic-fe-and-lgb\n\nsns.countplot(application_train['NAME_HOUSING_TYPE']);\nplt.xticks(rotation=45);\nplt.title('Counts of housing type')", "processed": ["see loan amount similar good price outlier", "name hous type"]}, {"markdown": ["##### Contact information\nThere are 6 features showing that client provided some contact information, let's see how many ways of contact clients usually provide."], "code": "# Reference: https://www.kaggle.com/code/artgor/eda-basic-fe-and-lgb\n\napplication_train['contact_info'] = application_train['FLAG_MOBIL'] + application_train['FLAG_EMP_PHONE'] + application_train['FLAG_WORK_PHONE'] + application_train['FLAG_CONT_MOBILE'] + application_train['FLAG_PHONE'] + application_train['FLAG_EMAIL']\nsns.countplot(application_train['contact_info']);\nplt.title('Count of ways to contact client');", "processed": ["contact inform 6 featur show client provid contact inform let see mani way contact client usual provid"]}, {"markdown": ["Most clients provide 3 ways to contact them and usually minimus is 2, if we don't consider several people who left only 1.", "# deliquencies\n\nIt is very important to see how many times clients was late with payments or defaulted his loans. I suppose info about his social circle is also important. I'll divide values into 2 groups: 0, 1 and more than 1."], "code": "# Reference: https://www.kaggle.com/code/artgor/eda-basic-fe-and-lgb\n\napplication_train.loc[application_train['OBS_30_CNT_SOCIAL_CIRCLE'] > 1, 'OBS_30_CNT_SOCIAL_CIRCLE'] = '1+'\napplication_train.loc[application_train['DEF_30_CNT_SOCIAL_CIRCLE'] > 1, 'DEF_30_CNT_SOCIAL_CIRCLE'] = '1+'\napplication_train.loc[application_train['OBS_60_CNT_SOCIAL_CIRCLE'] > 1, 'OBS_60_CNT_SOCIAL_CIRCLE'] = '1+'\napplication_train.loc[application_train['DEF_60_CNT_SOCIAL_CIRCLE'] > 1, 'DEF_60_CNT_SOCIAL_CIRCLE'] = '1+'\n\nfig, ax = plt.subplots(figsize = (30, 8))\nplt.subplot(1, 4, 1)\nsns.countplot(application_train['OBS_30_CNT_SOCIAL_CIRCLE']);\nplt.subplot(1, 4, 2)\nsns.countplot(application_train['DEF_30_CNT_SOCIAL_CIRCLE']);\nplt.subplot(1, 4, 3)\nsns.countplot(application_train['OBS_60_CNT_SOCIAL_CIRCLE']);\nplt.subplot(1, 4, 4)\nsns.countplot(application_train['DEF_60_CNT_SOCIAL_CIRCLE']);", "processed": ["client provid 3 way contact usual minimu 2 consid sever peopl left 1", "deliqu import see mani time client late payment default loan suppos info social circl also import divid valu 2 group 0 1 1"]}, {"markdown": ["#### Continuous variables", "##### AMT_INCOME_TOTAL"], "code": "# Reference: https://www.kaggle.com/code/artgor/eda-basic-fe-and-lgb\n\nsns.boxplot(application_train['AMT_INCOME_TOTAL']);\nplt.title('AMT_INCOME_TOTAL boxplot');\nsns.boxplot(application_train[application_train['AMT_INCOME_TOTAL'] < np.percentile(application_train['AMT_INCOME_TOTAL'], 90)]['AMT_INCOME_TOTAL']);\nplt.title('AMT_INCOME_TOTAL boxplot on data within 90 percentile');\napplication_train.groupby('TARGET').agg({'AMT_INCOME_TOTAL': ['mean', 'median', 'count']})\nplt.hist(application_train['AMT_INCOME_TOTAL']);\nplt.title('AMT_INCOME_TOTAL histogram');\nplt.hist(application_train[application_train['AMT_INCOME_TOTAL'] < np.percentile(application_train['AMT_INCOME_TOTAL'], 90)]['AMT_INCOME_TOTAL']);\nplt.title('AMT_INCOME_TOTAL histogram on data within 90 percentile');\nplt.hist(np.log1p(application_train['AMT_INCOME_TOTAL']));\nplt.title('AMT_INCOME_TOTAL histogram on data with log1p transformation');", "processed": ["continu variabl", "amt incom total"]}, {"markdown": ["We can see following things from the information above:\n- income feature has some huge outliers. This could be due to rich individuals or due to errors in data;\n- average income is almost similar for those who repay the loans and those who don't;\n- if we leave only data within 90 percentile, it is almost normally distributed;\n- log transformation also helps;", "##### AMT_CREDIT"], "code": "# Reference: https://www.kaggle.com/code/artgor/eda-basic-fe-and-lgb\n\nsns.boxplot(application_train['AMT_CREDIT'], orient='v');\nplt.title('AMT_CREDIT boxplot');\nsns.boxplot(application_train[application_train['AMT_CREDIT'] < np.percentile(application_train['AMT_CREDIT'], 95)]['AMT_CREDIT'], orient='v');\nplt.title('AMT_CREDIT boxplot on data within 90 percentile');\napplication_train.groupby('TARGET').agg({'AMT_CREDIT': ['mean', 'median', 'count']})\nplt.hist(application_train['AMT_CREDIT']);\nplt.title('AMT_CREDIT histogram');\nplt.hist(application_train[application_train['AMT_CREDIT'] < np.percentile(application_train['AMT_CREDIT'], 90)]['AMT_CREDIT']);\nplt.title('AMT_INCOME_TOTAL histogram on data within 90 percentile');\nplt.hist(np.log1p(application_train['AMT_CREDIT']));\nplt.title('AMT_CREDIT histogram on data with log1p transformation');", "processed": ["see follow thing inform incom featur huge outlier could due rich individu due error data averag incom almost similar repay loan leav data within 90 percentil almost normal distribut log transform also help", "amt credit"]}, {"markdown": ["<a href=\"#0\"><font size=\"1\">Go to top</font></a>  \n\n\n##  <a id=\"53\">Target variable</a>\n\n\nLet's check the target variable distribution."], "code": "# Reference: https://www.kaggle.com/code/gpreda/santander-value-prediction-extensive-eda\n\n# Plot distribution of one feature\ndef plot_distribution(df,feature,color):\n    plt.figure(figsize=(10,6))\n    plt.title(\"Distribution of %s\" % feature)\n    sns.distplot(df[feature].dropna(),color=color, kde=True,bins=100)\n    plt.show()   \n    \nplot_distribution(train_df, \"target\", \"blue\")", "processed": ["href 0 font size 1 go top font id 53 target variabl let check target variabl distribut"]}, {"markdown": ["Let's check the distribution of log(target)."], "code": "# Reference: https://www.kaggle.com/code/gpreda/santander-value-prediction-extensive-eda\n\ndef plot_log_distribution(df,feature,color):\n    plt.figure(figsize=(10,6))\n    plt.title(\"Distribution of %s\" % feature)\n    sns.distplot(np.log1p(df[feature]).dropna(),color=color, kde=True,bins=100)\n    plt.title(\"Distribution of log(target)\")\n    plt.show()   \n\nplot_log_distribution(train_df, \"target\", \"green\")  ", "processed": ["let check distribut log target"]}, {"markdown": ["<a href=\"#0\"><font size=\"1\">Go to top</font></a>  \n\n\n##  <a id=\"54\">Distribution of non-zero features values per row</a>\n\nLet's check what is the distribution of non-zero features values per row in the train set."], "code": "# Reference: https://www.kaggle.com/code/gpreda/santander-value-prediction-extensive-eda\n\nnon_zeros = (train_df.ne(0).sum(axis=1))\n\nplt.figure(figsize=(10,6))\nplt.title(\"Distribution of log(number of non-zeros per row) - train set\")\nsns.distplot(np.log1p(non_zeros),color=\"red\", kde=True,bins=100)\nplt.show()\n", "processed": ["href 0 font size 1 go top font id 54 distribut non zero featur valu per row let check distribut non zero featur valu per row train set"]}, {"markdown": ["Let's check distribution of non-zero features values per row in the test set."], "code": "# Reference: https://www.kaggle.com/code/gpreda/santander-value-prediction-extensive-eda\n\nnon_zeros = (test_df.ne(0).sum(axis=1))\n\nplt.figure(figsize=(10,6))\nplt.title(\"Distribution of log(number of non-zeros per row) - test set\")\nsns.distplot(np.log1p(non_zeros),color=\"magenta\", kde=True,bins=100)\nplt.show()", "processed": ["let check distribut non zero featur valu per row test set"]}, {"markdown": ["Let's separate only the **real** values, excepting the **target**. And let's represent the distribution of non-zero features values only for these.\n\n### Distribution of non-zeros for float type features"], "code": "# Reference: https://www.kaggle.com/code/gpreda/santander-value-prediction-extensive-eda\n\nvar = metadata[(metadata.dtype == 'float64') & (metadata.use == 'input')].index\nnon_zeros = (train_df[var].ne(0).sum(axis=1))\n\nplt.figure(figsize=(10,6))\nplt.title(\"Distribution of log(number of non-zeros per row) - floats only - train set\")\nsns.distplot(np.log1p(non_zeros),color=\"green\", kde=True,bins=100)\nplt.show()\nnon_zeros = (test_df[var].ne(0).sum(axis=1))\n\nplt.figure(figsize=(10,6))\nplt.title(\"Distribution of log(number of non-zeros per row) - floats only - test set\")\nsns.distplot(np.log1p(non_zeros),color=\"blue\", kde=True,bins=100)\nplt.show()", "processed": ["let separ real valu except target let repres distribut non zero featur valu distribut non zero float type featur"]}, {"markdown": ["### Distribution of non-zeros for integer type features"], "code": "# Reference: https://www.kaggle.com/code/gpreda/santander-value-prediction-extensive-eda\n\nvar = metadata[(metadata.dtype == 'int64') & (metadata.use == 'input')].index\nnon_zeros = (train_df[var].ne(0).sum(axis=1))\n\nplt.figure(figsize=(10,6))\nplt.title(\"Distribution of log(number of non-zeros per row) - integers only -  train set\")\nsns.distplot(np.log1p(non_zeros),color=\"yellow\", kde=True,bins=100)\nplt.show()\nnon_zeros = (test_df[var].ne(0).sum(axis=1))\n\nplt.figure(figsize=(10,6))\nplt.title(\"Distribution of log(number of non-zeros per row) - integers only - train set\")\nsns.distplot(np.log1p(non_zeros),color=\"cyan\", kde=True,bins=100)\nplt.show()", "processed": ["distribut non zero integ type featur"]}, {"markdown": ["<a href=\"#0\"><font size=\"1\">Go to top</font></a>\n\n\n##  <a id=\"55\">Distribution of non-zero features values per column</a>\n\nLet's check what is the distribition of non-zero features values per column in the train set."], "code": "# Reference: https://www.kaggle.com/code/gpreda/santander-value-prediction-extensive-eda\n\nnon_zeros = (train_df.ne(0).sum(axis=0))\n\nplt.figure(figsize=(10,6))\nplt.title(\"Distribution of log(number of non-zeros per column) - train set\")\nsns.distplot(np.log1p(non_zeros),color=\"darkblue\", kde=True,bins=100)\nplt.show()", "processed": ["href 0 font size 1 go top font id 55 distribut non zero featur valu per column let check distribit non zero featur valu per column train set"]}, {"markdown": ["Let's check distribution of non-zero features values per row in the test set."], "code": "# Reference: https://www.kaggle.com/code/gpreda/santander-value-prediction-extensive-eda\n\nnon_zeros = (test_df.ne(0).sum(axis=0))\n\nplt.figure(figsize=(10,6))\nplt.title(\"Distribution of log(number of non-zeros per column) - test set\")\nsns.distplot(np.log1p(non_zeros),color=\"darkgreen\", kde=True,bins=100)\nplt.show()", "processed": ["let check distribut non zero featur valu per row test set"]}, {"markdown": ["<a href=\"#0\"><font size=\"1\">Go to top</font></a>  \n\n\n##  <a id=\"56\">Float features</a>\n\nLet's see now  the distribution of the sum of float features values per column."], "code": "# Reference: https://www.kaggle.com/code/gpreda/santander-value-prediction-extensive-eda\n\nvar = metadata[(metadata.dtype == 'float64') & (metadata.use == 'input')].index\nval = train_df[var].sum()\n\nfig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12,6))\nsns.boxplot(val, palette=\"Blues\",  showfliers=False,ax=ax1)\nsns.boxplot(val, palette=\"Greens\",  showfliers=True,ax=ax2)\nplt.show();", "processed": ["href 0 font size 1 go top font id 56 float featur let see distribut sum float featur valu per column"]}, {"markdown": ["<a href=\"#0\"><font size=\"1\">Go to top</font></a>  \n\n\n##  <a id=\"57\">Integer features</a>\n\nLet's see now  the distribution of the sum of integer features values per column."], "code": "# Reference: https://www.kaggle.com/code/gpreda/santander-value-prediction-extensive-eda\n\nvar = metadata[(metadata.dtype == 'int64') & (metadata.use == 'input')].index\nval = train_df[var].sum()\n\nfig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12,6))\nsns.boxplot(val, palette=\"Reds\",  showfliers=False,ax=ax1)\nsns.boxplot(val, palette=\"Blues\",  showfliers=True,ax=ax2)\nplt.show();", "processed": ["href 0 font size 1 go top font id 57 integ featur let see distribut sum integ featur valu per column"]}, {"markdown": ["Let's represent, for the highly correlated features, the distribution in the train and test set."], "code": "# Reference: https://www.kaggle.com/code/gpreda/santander-value-prediction-extensive-eda\n\ncorrmat\nvar = temp_df.columns.values\n\ni = 0\nsns.set_style('whitegrid')\nplt.figure()\nfig, ax = plt.subplots(5,4,figsize=(12,15))\n\nfor feature in var:\n    i += 1\n    plt.subplot(5,4,i)\n    sns.kdeplot(train_df[feature], bw=0.5,label=\"train\")\n    sns.kdeplot(test_df[feature], bw=0.5,label=\"test\")\n    plt.xlabel(feature, fontsize=12)\n    locs, labels = plt.xticks()\n    plt.tick_params(axis='both', which='major', labelsize=12)\nplt.show();", "processed": ["let repres highli correl featur distribut train test set"]}, {"markdown": ["Let's represent the relationship between two of the highest correlated features ('429687d5a';'e4159c59e') and ('6b119d8ce';'e8d9394a0')."], "code": "# Reference: https://www.kaggle.com/code/gpreda/santander-value-prediction-extensive-eda\n\nsns.set_style('whitegrid')\nplt.figure()\ns = sns.lmplot(x='429687d5a', y='e4159c59e',data=train_df, fit_reg=True,scatter_kws={'s':2})\ns = sns.lmplot(x='6b119d8ce', y='e8d9394a0',data=train_df, fit_reg=True,scatter_kws={'s':2})\ns = sns.lmplot(x='cbbc9c431', y='f296082ec',data=train_df, fit_reg=True,scatter_kws={'s':2})\ns = sns.lmplot(x='cbbc9c431', y='51707c671',data=train_df, fit_reg=True,scatter_kws={'s':2})\nplt.show()", "processed": ["let repres relationship two highest correl featur 429687d5a e4159c59 6b119d8ce e8d9394a0"]}, {"markdown": ["## Helper functions and classes"], "code": "# Reference: https://www.kaggle.com/code/artgor/oop-approach-to-fe-and-models\n\ndef add_datepart(df: pd.DataFrame, field_name: str,\n                 prefix: str = None, drop: bool = True, time: bool = True, date: bool = True):\n    \"\"\"\n    Helper function that adds columns relevant to a date in the column `field_name` of `df`.\n    from fastai: https://github.com/fastai/fastai/blob/master/fastai/tabular/transform.py#L55\n    \"\"\"\n    field = df[field_name]\n    prefix = ifnone(prefix, re.sub('[Dd]ate$', '', field_name))\n    attr = ['Year', 'Month', 'Week', 'Day', 'Dayofweek', 'Is_month_end', 'Is_month_start']\n    if date:\n        attr.append('Date')\n    if time:\n        attr = attr + ['Hour', 'Minute']\n    for n in attr:\n        df[prefix + n] = getattr(field.dt, n.lower())\n    if drop:\n        df.drop(field_name, axis=1, inplace=True)\n    return df\n\n\ndef ifnone(a: Any, b: Any) -> Any:\n    \"\"\"`a` if `a` is not None, otherwise `b`.\n    from fastai: https://github.com/fastai/fastai/blob/master/fastai/core.py#L92\"\"\"\n    return b if a is None else a\nfrom sklearn.base import BaseEstimator, TransformerMixin\n@jit\ndef qwk(a1, a2):\n    \"\"\"\n    Source: https://www.kaggle.com/c/data-science-bowl-2019/discussion/114133#latest-660168\n\n    :param a1:\n    :param a2:\n    :param max_rat:\n    :return:\n    \"\"\"\n    max_rat = 3\n    a1 = np.asarray(a1, dtype=int)\n    a2 = np.asarray(a2, dtype=int)\n\n    hist1 = np.zeros((max_rat + 1, ))\n    hist2 = np.zeros((max_rat + 1, ))\n\n    o = 0\n    for k in range(a1.shape[0]):\n        i, j = a1[k], a2[k]\n        hist1[i] += 1\n        hist2[j] += 1\n        o +=  (i - j) * (i - j)\n\n    e = 0\n    for i in range(max_rat + 1):\n        for j in range(max_rat + 1):\n            e += hist1[i] * hist2[j] * (i - j) * (i - j)\n\n    e = e / a1.shape[0]\n\n    return 1 - o / e\n\n\ndef eval_qwk_lgb(y_true, y_pred):\n    \"\"\"\n    Fast cappa eval function for lgb.\n    \"\"\"\n\n    y_pred = y_pred.reshape(len(np.unique(y_true)), -1).argmax(axis=0)\n    return 'cappa', qwk(y_true, y_pred), True\n\n\ndef eval_qwk_xgb(y_pred, y_true):\n    \"\"\"\n    Fast cappa eval function for xgb.\n    \"\"\"\n    # print('y_true', y_true)\n    # print('y_pred', y_pred)\n    y_true = y_true.get_label()\n    y_pred = y_pred.argmax(axis=1)\n    return 'cappa', -qwk(y_true, y_pred)\n\n\nclass LGBWrapper(object):\n    \"\"\"\n    A wrapper for lightgbm model so that we will have a single api for various models.\n    \"\"\"\n\n    def __init__(self):\n        self.model = lgb.LGBMClassifier()\n\n    def fit(self, X_train, y_train, X_valid=None, y_valid=None, X_holdout=None, y_holdout=None, params=None):\n\n        eval_set = [(X_train, y_train)]\n        eval_names = ['train']\n        self.model = self.model.set_params(**params)\n\n        if X_valid is not None:\n            eval_set.append((X_valid, y_valid))\n            eval_names.append('valid')\n\n        if X_holdout is not None:\n            eval_set.append((X_holdout, y_holdout))\n            eval_names.append('holdout')\n\n        if 'cat_cols' in params.keys():\n            cat_cols = [col for col in params['cat_cols'] if col in X_train.columns]\n            if len(cat_cols) > 0:\n                categorical_columns = params['cat_cols']\n            else:\n                categorical_columns = 'auto'\n        else:\n            categorical_columns = 'auto'\n\n        self.model.fit(X=X_train, y=y_train,\n                       eval_set=eval_set, eval_names=eval_names, eval_metric=eval_qwk_lgb,\n                       verbose=params['verbose'], early_stopping_rounds=params['early_stopping_rounds'],\n                       categorical_feature=categorical_columns)\n\n        self.best_score_ = self.model.best_score_\n        self.feature_importances_ = self.model.feature_importances_\n\n    def predict_proba(self, X_test):\n        if self.model.objective == 'binary':\n            return self.model.predict_proba(X_test, num_iteration=self.model.best_iteration_)[:, 1]\n        else:\n            return self.model.predict_proba(X_test, num_iteration=self.model.best_iteration_)\n\n\nclass CatWrapper(object):\n    \"\"\"\n    A wrapper for catboost model so that we will have a single api for various models.\n    \"\"\"\n\n    def __init__(self):\n        self.model = cat.CatBoostClassifier()\n\n    def fit(self, X_train, y_train, X_valid=None, y_valid=None, X_holdout=None, y_holdout=None, params=None):\n\n        eval_set = [(X_train, y_train)]\n        self.model = self.model.set_params(**{k: v for k, v in params.items() if k != 'cat_cols'})\n\n        if X_valid is not None:\n            eval_set.append((X_valid, y_valid))\n\n        if X_holdout is not None:\n            eval_set.append((X_holdout, y_holdout))\n\n        if 'cat_cols' in params.keys():\n            cat_cols = [col for col in params['cat_cols'] if col in X_train.columns]\n            if len(cat_cols) > 0:\n                categorical_columns = params['cat_cols']\n            else:\n                categorical_columns = None\n        else:\n            categorical_columns = None\n        \n        self.model.fit(X=X_train, y=y_train,\n                       eval_set=eval_set,\n                       verbose=params['verbose'], early_stopping_rounds=params['early_stopping_rounds'],\n                       cat_features=categorical_columns)\n\n        self.best_score_ = self.model.best_score_\n        self.feature_importances_ = self.model.feature_importances_\n\n    def predict_proba(self, X_test):\n        if 'MultiClass' not in self.model.get_param('loss_function'):\n            return self.model.predict_proba(X_test, ntree_end=self.model.best_iteration_)[:, 1]\n        else:\n            return self.model.predict_proba(X_test, ntree_end=self.model.best_iteration_)\n\n\nclass XGBWrapper(object):\n    \"\"\"\n    A wrapper for xgboost model so that we will have a single api for various models.\n    \"\"\"\n\n    def __init__(self):\n        self.model = xgb.XGBClassifier()\n\n    def fit(self, X_train, y_train, X_valid=None, y_valid=None, X_holdout=None, y_holdout=None, params=None):\n\n        eval_set = [(X_train, y_train)]\n        self.model = self.model.set_params(**params)\n\n        if X_valid is not None:\n            eval_set.append((X_valid, y_valid))\n\n        if X_holdout is not None:\n            eval_set.append((X_holdout, y_holdout))\n\n        self.model.fit(X=X_train, y=y_train,\n                       eval_set=eval_set, eval_metric=eval_qwk_xgb,\n                       verbose=params['verbose'], early_stopping_rounds=params['early_stopping_rounds'])\n\n        scores = self.model.evals_result()\n        self.best_score_ = {k: {m: m_v[-1] for m, m_v in v.items()} for k, v in scores.items()}\n        self.best_score_ = {k: {m: n if m != 'cappa' else -n for m, n in v.items()} for k, v in self.best_score_.items()}\n\n        self.feature_importances_ = self.model.feature_importances_\n\n    def predict_proba(self, X_test):\n        if self.model.objective == 'binary':\n            return self.model.predict_proba(X_test, ntree_limit=self.model.best_iteration)[:, 1]\n        else:\n            return self.model.predict_proba(X_test, ntree_limit=self.model.best_iteration)\n\n\n\n\nclass MainTransformer(BaseEstimator, TransformerMixin):\n\n    def __init__(self, convert_cyclical: bool = False, create_interactions: bool = False, n_interactions: int = 20):\n        \"\"\"\n        Main transformer for the data. Can be used for processing on the whole data.\n\n        :param convert_cyclical: convert cyclical features into continuous\n        :param create_interactions: create interactions between features\n        \"\"\"\n\n        self.convert_cyclical = convert_cyclical\n        self.create_interactions = create_interactions\n        self.feats_for_interaction = None\n        self.n_interactions = n_interactions\n\n    def fit(self, X, y=None):\n\n        if self.create_interactions:\n            self.feats_for_interaction = [col for col in X.columns if 'sum' in col\n                                          or 'mean' in col or 'max' in col or 'std' in col\n                                          or 'attempt' in col]\n            self.feats_for_interaction1 = np.random.choice(self.feats_for_interaction, self.n_interactions)\n            self.feats_for_interaction2 = np.random.choice(self.feats_for_interaction, self.n_interactions)\n\n        return self\n\n    def transform(self, X, y=None):\n        data = copy.deepcopy(X)\n        if self.create_interactions:\n            for col1 in self.feats_for_interaction1:\n                for col2 in self.feats_for_interaction2:\n                    data[f'{col1}_int_{col2}'] = data[col1] * data[col2]\n\n        if self.convert_cyclical:\n            data['timestampHour'] = np.sin(2 * np.pi * data['timestampHour'] / 23.0)\n            data['timestampMonth'] = np.sin(2 * np.pi * data['timestampMonth'] / 23.0)\n            data['timestampWeek'] = np.sin(2 * np.pi * data['timestampWeek'] / 23.0)\n            data['timestampMinute'] = np.sin(2 * np.pi * data['timestampMinute'] / 23.0)\n\n        data['installation_session_count'] = data.groupby(['installation_id'])['Clip'].transform('count')\n        data['installation_duration_mean'] = data.groupby(['installation_id'])['duration_mean'].transform('mean')\n        data['installation_title_nunique'] = data.groupby(['installation_id'])['session_title'].transform('nunique')\n\n        data['sum_event_code_count'] = data[['2000', '3010', '3110', '4070', '4090', '4030', '4035', '4021', '4020', '4010', '2080', '2083', '2040', '2020', '2030', '3021', '3121', '2050', '3020', '3120', '2060', '2070', '4031', '4025', '5000', '5010', '2081', '2025', '4022', '2035', '4040', '4100', '2010', '4110', '4045', '4095', '4220', '2075', '4230', '4235', '4080', '4050']].sum(axis=1)\n\n        # data['installation_event_code_count_mean'] = data.groupby(['installation_id'])['sum_event_code_count'].transform('mean')\n\n        return data\n\n    def fit_transform(self, X, y=None, **fit_params):\n        data = copy.deepcopy(X)\n        self.fit(data)\n        return self.transform(data)\n\n\nclass FeatureTransformer(BaseEstimator, TransformerMixin):\n\n    def __init__(self, main_cat_features: list = None, num_cols: list = None):\n        \"\"\"\n\n        :param main_cat_features:\n        :param num_cols:\n        \"\"\"\n        self.main_cat_features = main_cat_features\n        self.num_cols = num_cols\n\n    def fit(self, X, y=None):\n\n        self.num_cols = [col for col in X.columns if 'sum' in col or 'mean' in col or 'max' in col or 'std' in col\n                         or 'attempt' in col]\n\n        return self\n\n    def transform(self, X, y=None):\n        data = copy.deepcopy(X)\n#         for col in self.num_cols:\n#             data[f'{col}_to_mean'] = data[col] / data.groupby('installation_id')[col].transform('mean')\n#             data[f'{col}_to_std'] = data[col] / data.groupby('installation_id')[col].transform('std')\n\n        return data\n\n    def fit_transform(self, X, y=None, **fit_params):\n        data = copy.deepcopy(X)\n        self.fit(data)\n        return self.transform(data)\nclass ClassifierModel(object):\n    \"\"\"\n    A wrapper class for classification models.\n    It can be used for training and prediction.\n    Can plot feature importance and training progress (if relevant for model).\n\n    \"\"\"\n\n    def __init__(self, columns: list = None, model_wrapper=None):\n        \"\"\"\n\n        :param original_columns:\n        :param model_wrapper:\n        \"\"\"\n        self.columns = columns\n        self.model_wrapper = model_wrapper\n        self.result_dict = {}\n        self.train_one_fold = False\n        self.preprocesser = None\n\n    def fit(self, X: pd.DataFrame, y,\n            X_holdout: pd.DataFrame = None, y_holdout=None,\n            folds=None,\n            params: dict = None,\n            eval_metric='auc',\n            cols_to_drop: list = None,\n            preprocesser=None,\n            transformers: dict = None,\n            adversarial: bool = False,\n            plot: bool = True):\n        \"\"\"\n        Training the model.\n\n        :param X: training data\n        :param y: training target\n        :param X_holdout: holdout data\n        :param y_holdout: holdout target\n        :param folds: folds to split the data. If not defined, then model will be trained on the whole X\n        :param params: training parameters\n        :param eval_metric: metric for validataion\n        :param cols_to_drop: list of columns to drop (for example ID)\n        :param preprocesser: preprocesser class\n        :param transformers: transformer to use on folds\n        :param adversarial\n        :return:\n        \"\"\"\n\n        if folds is None:\n            folds = KFold(n_splits=3, random_state=42)\n            self.train_one_fold = True\n\n        self.columns = X.columns if self.columns is None else self.columns\n        self.feature_importances = pd.DataFrame(columns=['feature', 'importance'])\n        self.trained_transformers = {k: [] for k in transformers}\n        self.transformers = transformers\n        self.models = []\n        self.folds_dict = {}\n        self.eval_metric = eval_metric\n        n_target = 4# 1 if len(set(y.values)) == 2 else len(set(y.values))\n        self.oof = np.zeros((len(X), n_target))\n        self.n_target = n_target\n\n        X = X[self.columns]\n        if X_holdout is not None:\n            X_holdout = X_holdout[self.columns]\n\n        if preprocesser is not None:\n            self.preprocesser = preprocesser\n            self.preprocesser.fit(X, y)\n            X = self.preprocesser.transform(X, y)\n            self.columns = X.columns.tolist()\n            if X_holdout is not None:\n                X_holdout = self.preprocesser.transform(X_holdout)\n            # y = X['accuracy_group']\n\n        for fold_n, (train_index, valid_index) in enumerate(folds.split(X, y, X['installation_id'])):\n            if X_holdout is not None:\n                X_hold = X_holdout.copy()\n            else:\n                X_hold = None\n            self.folds_dict[fold_n] = {}\n            if params['verbose']:\n                print(f'Fold {fold_n + 1} started at {time.ctime()}')\n            self.folds_dict[fold_n] = {}\n\n            X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n            y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n            if self.train_one_fold:\n                X_train = X[self.original_columns]\n                y_train = y\n                X_valid = None\n                y_valid = None\n\n            datasets = {'X_train': X_train, 'X_valid': X_valid, 'X_holdout': X_hold, 'y_train': y_train}\n            X_train, X_valid, X_hold = self.transform_(datasets, cols_to_drop)\n\n            self.folds_dict[fold_n]['columns'] = X_train.columns.tolist()\n\n            model = copy.deepcopy(self.model_wrapper)\n\n            if adversarial:\n                X_new1 = X_train.copy()\n                if X_valid is not None:\n                    X_new2 = X_valid.copy()\n                elif X_holdout is not None:\n                    X_new2 = X_holdout.copy()\n                X_new = pd.concat([X_new1, X_new2], axis=0)\n                y_new = np.hstack((np.zeros((X_new1.shape[0])), np.ones((X_new2.shape[0]))))\n                X_train, X_valid, y_train, y_valid = train_test_split(X_new, y_new)\n\n            model.fit(X_train, y_train, X_valid, y_valid, X_hold, y_holdout, params=params)\n\n            self.folds_dict[fold_n]['scores'] = model.best_score_\n            if self.oof.shape[0] != len(X):\n                self.oof = np.zeros((X.shape[0], self.oof.shape[1]))\n            if not adversarial:\n                self.oof[valid_index] = model.predict_proba(X_valid).reshape(-1, n_target)\n\n            fold_importance = pd.DataFrame(list(zip(X_train.columns, model.feature_importances_)),\n                                           columns=['feature', 'importance'])\n            self.feature_importances = self.feature_importances.append(fold_importance)\n            self.models.append(model)\n\n        self.feature_importances['importance'] = self.feature_importances['importance'].astype(float)\n\n        # if params['verbose']:\n        self.calc_scores_()\n\n        if plot:\n            print(classification_report(y, self.oof.argmax(1)))\n            fig, ax = plt.subplots(figsize=(16, 12))\n            plt.subplot(2, 2, 1)\n            self.plot_feature_importance(top_n=25)\n            plt.subplot(2, 2, 2)\n            self.plot_metric()\n            plt.subplot(2, 2, 3)\n            g = sns.heatmap(confusion_matrix(y, self.oof.argmax(1)), annot=True, cmap=plt.cm.Blues,fmt=\"d\")\n            g.set(ylim=(-0.5, 4), xlim=(-0.5, 4), title='Confusion matrix')\n\n            plt.subplot(2, 2, 4)\n            plt.hist(self.oof.argmax(1))\n            plt.xticks(range(self.n_target), range(self.n_target))\n            plt.title('Distribution of oof predictions');\n\n    def transform_(self, datasets, cols_to_drop):\n        for name, transformer in self.transformers.items():\n            transformer.fit(datasets['X_train'], datasets['y_train'])\n            datasets['X_train'] = transformer.transform(datasets['X_train'])\n            if datasets['X_valid'] is not None:\n                datasets['X_valid'] = transformer.transform(datasets['X_valid'])\n            if datasets['X_holdout'] is not None:\n                datasets['X_holdout'] = transformer.transform(datasets['X_holdout'])\n            self.trained_transformers[name].append(transformer)\n        if cols_to_drop is not None:\n            cols_to_drop = [col for col in cols_to_drop if col in datasets['X_train'].columns]\n            self.cols_to_drop = cols_to_drop\n            datasets['X_train'] = datasets['X_train'].drop(cols_to_drop, axis=1)\n            if datasets['X_valid'] is not None:\n                datasets['X_valid'] = datasets['X_valid'].drop(cols_to_drop, axis=1)\n            if datasets['X_holdout'] is not None:\n                datasets['X_holdout'] = datasets['X_holdout'].drop(cols_to_drop, axis=1)\n\n        return datasets['X_train'], datasets['X_valid'], datasets['X_holdout']\n\n    def calc_scores_(self):\n        print()\n        datasets = [k for k, v in [v['scores'] for k, v in self.folds_dict.items()][0].items() if len(v) > 0]\n        self.scores = {}\n        for d in datasets:\n            scores = [v['scores'][d][self.eval_metric] for k, v in self.folds_dict.items()]\n            print(f\"CV mean score on {d}: {np.mean(scores):.4f} +/- {np.std(scores):.4f} std.\")\n            self.scores[d] = np.mean(scores)\n\n    def predict(self, X_test, averaging: str = 'usual'):\n        \"\"\"\n        Make prediction\n\n        :param X_test:\n        :param averaging: method of averaging\n        :return:\n        \"\"\"\n        full_prediction = np.zeros((X_test.shape[0], self.oof.shape[1]))\n        if self.preprocesser is not None:\n            X_test = self.preprocesser.transform(X_test)\n        for i in range(len(self.models)):\n            X_t = X_test.copy()\n            for name, transformers in self.trained_transformers.items():\n                X_t = transformers[i].transform(X_t)\n\n            cols_to_drop = [col for col in self.cols_to_drop if col in X_t.columns]\n            X_t = X_t.drop(cols_to_drop, axis=1)\n            y_pred = self.models[i].predict_proba(X_t[self.folds_dict[i]['columns']]).reshape(-1, full_prediction.shape[1])\n\n            # if case transformation changes the number of the rows\n            if full_prediction.shape[0] != len(y_pred):\n                full_prediction = np.zeros((y_pred.shape[0], self.oof.shape[1]))\n\n            if averaging == 'usual':\n                full_prediction += y_pred\n            elif averaging == 'rank':\n                full_prediction += pd.Series(y_pred).rank().values\n\n        return full_prediction / len(self.models)\n\n    def plot_feature_importance(self, drop_null_importance: bool = True, top_n: int = 10):\n        \"\"\"\n        Plot default feature importance.\n\n        :param drop_null_importance: drop columns with null feature importance\n        :param top_n: show top n columns\n        :return:\n        \"\"\"\n\n        top_feats = self.get_top_features(drop_null_importance, top_n)\n        feature_importances = self.feature_importances.loc[self.feature_importances['feature'].isin(top_feats)]\n        feature_importances['feature'] = feature_importances['feature'].astype(str)\n        top_feats = [str(i) for i in top_feats]\n        sns.barplot(data=feature_importances, x='importance', y='feature', orient='h', order=top_feats)\n        plt.title('Feature importances')\n\n    def get_top_features(self, drop_null_importance: bool = True, top_n: int = 10):\n        \"\"\"\n        Get top features by importance.\n\n        :param drop_null_importance:\n        :param top_n:\n        :return:\n        \"\"\"\n        grouped_feats = self.feature_importances.groupby(['feature'])['importance'].mean()\n        if drop_null_importance:\n            grouped_feats = grouped_feats[grouped_feats != 0]\n        return list(grouped_feats.sort_values(ascending=False).index)[:top_n]\n\n    def plot_metric(self):\n        \"\"\"\n        Plot training progress.\n        Inspired by `plot_metric` from https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/plotting.html\n\n        :return:\n        \"\"\"\n        full_evals_results = pd.DataFrame()\n        for model in self.models:\n            evals_result = pd.DataFrame()\n            for k in model.model.evals_result_.keys():\n                evals_result[k] = model.model.evals_result_[k][self.eval_metric]\n            evals_result = evals_result.reset_index().rename(columns={'index': 'iteration'})\n            full_evals_results = full_evals_results.append(evals_result)\n\n        full_evals_results = full_evals_results.melt(id_vars=['iteration']).rename(columns={'value': self.eval_metric,\n                                                                                            'variable': 'dataset'})\n        full_evals_results[self.eval_metric] = np.abs(full_evals_results[self.eval_metric])\n        sns.lineplot(data=full_evals_results, x='iteration', y=self.eval_metric, hue='dataset')\n        plt.title('Training progress')\nclass CategoricalTransformer(BaseEstimator, TransformerMixin):\n\n    def __init__(self, cat_cols=None, drop_original: bool = False, encoder=OrdinalEncoder()):\n        \"\"\"\n        Categorical transformer. This is a wrapper for categorical encoders.\n\n        :param cat_cols:\n        :param drop_original:\n        :param encoder:\n        \"\"\"\n        self.cat_cols = cat_cols\n        self.drop_original = drop_original\n        self.encoder = encoder\n        self.default_encoder = OrdinalEncoder()\n\n    def fit(self, X, y=None):\n\n        if self.cat_cols is None:\n            kinds = np.array([dt.kind for dt in X.dtypes])\n            is_cat = kinds == 'O'\n            self.cat_cols = list(X.columns[is_cat])\n        self.encoder.set_params(cols=self.cat_cols)\n        self.default_encoder.set_params(cols=self.cat_cols)\n\n        self.encoder.fit(X[self.cat_cols], y)\n        self.default_encoder.fit(X[self.cat_cols], y)\n\n        return self\n\n    def transform(self, X, y=None):\n        data = copy.deepcopy(X)\n        new_cat_names = [f'{col}_encoded' for col in self.cat_cols]\n        encoded_data = self.encoder.transform(data[self.cat_cols])\n        if encoded_data.shape[1] == len(self.cat_cols):\n            data[new_cat_names] = encoded_data\n        else:\n            pass\n\n        if self.drop_original:\n            data = data.drop(self.cat_cols, axis=1)\n        else:\n            data[self.cat_cols] = self.default_encoder.transform(data[self.cat_cols])\n\n        return data\n\n    def fit_transform(self, X, y=None, **fit_params):\n        data = copy.deepcopy(X)\n        self.fit(data)\n        return self.transform(data)", "processed": ["helper function class"]}, {"markdown": ["## 5. Tuning models and building the feature importance diagrams<a class=\"anchor\" id=\"5\"></a>\n\n[Back to Table of Contents](#0.1)", "### 5.1 LGBM <a class=\"anchor\" id=\"5.1\"></a>\n\n[Back to Table of Contents](#0.1)"], "code": "# Reference: https://www.kaggle.com/code/vbmokin/cfec-ii-feature-importan-xgb-lgb-logr-linr\n\nX = train\nz = target\n#%% split training set to validation set\nXtrain, Xval, Ztrain, Zval = train_test_split(X, z, test_size=0.2, random_state=0)\ntrain_set = lgbm.Dataset(Xtrain, Ztrain, silent=False)\nvalid_set = lgbm.Dataset(Xval, Zval, silent=False)\nparams = {\n        'boosting_type':'gbdt',\n        'objective': 'regression',\n        'num_leaves': 31,\n        'learning_rate': 0.05,\n        'max_depth': -1,\n        'subsample': 0.8,\n        'bagging_fraction' : 1,\n        'max_bin' : 5000 ,\n        'bagging_freq': 20,\n        'colsample_bytree': 0.6,\n        'metric': 'rmse',\n        'min_split_gain': 0.5,\n        'min_child_weight': 1,\n        'min_child_samples': 10,\n        'scale_pos_weight':1,\n        'zero_as_missing': True,\n        'seed':0,        \n    }\n\nmodelL = lgbm.train(params, train_set = train_set, num_boost_round=1000,\n                   early_stopping_rounds=50,verbose_eval=10, valid_sets=valid_set)\nfig =  plt.figure(figsize = (15,15))\naxes = fig.add_subplot(111)\nlgbm.plot_importance(modelL,ax = axes,height = 0.5)\nplt.show();plt.close()\nfeature_score = pd.DataFrame(train.columns, columns = ['feature']) \nfeature_score['score_lgb'] = modelL.feature_importance()", "processed": ["5 tune model build featur import diagram class anchor id 5 back tabl content 0 1", "5 1 lgbm class anchor id 5 1 back tabl content 0 1"]}, {"markdown": ["### 5.2 XGB<a class=\"anchor\" id=\"5.2\"></a>\n\n[Back to Table of Contents](#0.1)"], "code": "# Reference: https://www.kaggle.com/code/vbmokin/cfec-ii-feature-importan-xgb-lgb-logr-linr\n\n#%% split training set to validation set \ndata_tr  = xgb.DMatrix(Xtrain, label=Ztrain)\ndata_cv  = xgb.DMatrix(Xval   , label=Zval)\nevallist = [(data_tr, 'train'), (data_cv, 'valid')]\nparms = {'max_depth':8, #maximum depth of a tree\n         'objective':'reg:logistic',\n         'eta'      :0.3,\n         'subsample':0.8,#SGD will use this percentage of data\n         'lambda '  :4, #L2 regularization term,>1 more conservative \n         'colsample_bytree ':0.9,\n         'colsample_bylevel':1,\n         'min_child_weight': 10}\nmodelx = xgb.train(parms, data_tr, num_boost_round=200, evals = evallist,\n                  early_stopping_rounds=30, maximize=False, \n                  verbose_eval=10)\n\nprint('score = %1.5f, n_boost_round =%d.'%(modelx.best_score,modelx.best_iteration))\nfig =  plt.figure(figsize = (15,15))\naxes = fig.add_subplot(111)\nxgb.plot_importance(modelx,ax = axes,height = 0.5)\nplt.show();plt.close()\nfeature_score['score_xgb'] = feature_score['feature'].map(modelx.get_score(importance_type='weight'))\nfeature_score", "processed": ["5 2 xgb class anchor id 5 2 back tabl content 0 1"]}, {"markdown": ["Ok, currently we haven't made any predictions and except from Id all entries are filled with 0.", "# Exploratory data analysis <a class=\"anchor\" id=\"explore\"></a>", "## Which proteins occur most often in train images?"], "code": "# Reference: https://www.kaggle.com/code/allunia/protein-atlas-exploration-and-baseline\n\ntarget_counts = train_labels.drop([\"Id\", \"Target\"],axis=1).sum(axis=0).sort_values(ascending=False)\nplt.figure(figsize=(15,15))\nsns.barplot(y=target_counts.index.values, x=target_counts.values, order=target_counts.index)", "processed": ["ok current made predict except id entri fill 0", "exploratori data analysi class anchor id explor", "protein occur often train imag"]}, {"markdown": ["### Take-Away\n\n* We can see that most common protein structures belong to coarse grained cellular components like the plasma membrane, the cytosol and the nucleus. \n* In contrast small components like the lipid droplets, peroxisomes, endosomes, lysosomes, microtubule ends, rods and rings are very seldom in our train data. For these classes the prediction will be very difficult as we have only a few examples that may not cover all variabilities and as our model probably will be confused during ins learning process by the major classes. Due to this confusion we will make less accurate predictions on the minor classes.\n* Consequently accuracy is not the right score here to measure your performance and validation strategy should be very fine. ", "## How many targets are most common?"], "code": "# Reference: https://www.kaggle.com/code/allunia/protein-atlas-exploration-and-baseline\n\ntrain_labels[\"number_of_targets\"] = train_labels.drop([\"Id\", \"Target\"],axis=1).sum(axis=1)\ncount_perc = np.round(100 * train_labels[\"number_of_targets\"].value_counts() / train_labels.shape[0], 2)\nplt.figure(figsize=(20,5))\nsns.barplot(x=count_perc.index.values, y=count_perc.values, palette=\"Reds\")\nplt.xlabel(\"Number of targets per image\")\nplt.ylabel(\"% of train data\")", "processed": ["take away see common protein structur belong coars grain cellular compon like plasma membran cytosol nucleu contrast small compon like lipid droplet peroxisom endosom lysosom microtubul end rod ring seldom train data class predict difficult exampl may cover variabl model probabl confus in learn process major class due confus make le accur predict minor class consequ accuraci right score measur perform valid strategi fine", "mani target common"]}, {"markdown": ["### Take-away\n\n* We can see that many targets only have very slight correlations. \n* In contrast, endosomes and lysosomes often occur together and sometimes seem to be located at the endoplasmatic reticulum. \n* In addition we find that the mitotic spindle often comes together with the cytokinetic bridge. This makes sense as both are participants for cellular division. And in this process microtubules and thier ends are active and participate as well. Consequently we find a positive correlation between these targets.\n\nIf you like to dive deeper into target correlations you may like to take a look at my kernel notebook about [target clustering.](https://www.kaggle.com/allunia/in-depth-protein-correlations) :-)", "## How are special and seldom targets grouped?", "### Lysosomes and endosomes\n\nLet's start with these high correlated features!"], "code": "# Reference: https://www.kaggle.com/code/allunia/protein-atlas-exploration-and-baseline\n\ndef find_counts(special_target, labels):\n    counts = labels[labels[special_target] == 1].drop(\n        [\"Id\", \"Target\", \"number_of_targets\"],axis=1\n    ).sum(axis=0)\n    counts = counts[counts > 0]\n    counts = counts.sort_values()\n    return counts\nlyso_endo_counts = find_counts(\"Lysosomes\", train_labels)\n\nplt.figure(figsize=(10,3))\nsns.barplot(x=lyso_endo_counts.index.values, y=lyso_endo_counts.values, palette=\"Blues\")\nplt.ylabel(\"Counts in train data\")", "processed": ["take away see mani target slight correl contrast endosom lysosom often occur togeth sometim seem locat endoplasmat reticulum addit find mitot spindl often come togeth cytokinet bridg make sen particip cellular divis process microtubul thier end activ particip well consequ find posit correl target like dive deeper target correl may like take look kernel notebook target cluster http www kaggl com allunia depth protein correl", "special seldom target group", "lysosom endosom let start high correl featur"]}, {"markdown": ["### Rods and rings"], "code": "# Reference: https://www.kaggle.com/code/allunia/protein-atlas-exploration-and-baseline\n\nrod_rings_counts = find_counts(\"Rods & rings\", train_labels)\nplt.figure(figsize=(15,3))\nsns.barplot(x=rod_rings_counts.index.values, y=rod_rings_counts.values, palette=\"Greens\")\nplt.ylabel(\"Counts in train data\")", "processed": ["rod ring"]}, {"markdown": ["### Peroxisomes"], "code": "# Reference: https://www.kaggle.com/code/allunia/protein-atlas-exploration-and-baseline\n\nperoxi_counts = find_counts(\"Peroxisomes\", train_labels)\n\nplt.figure(figsize=(15,3))\nsns.barplot(x=peroxi_counts.index.values, y=peroxi_counts.values, palette=\"Reds\")\nplt.ylabel(\"Counts in train data\")", "processed": ["peroxisom"]}, {"markdown": ["### Microtubule ends"], "code": "# Reference: https://www.kaggle.com/code/allunia/protein-atlas-exploration-and-baseline\n\ntubeends_counts = find_counts(\"Microtubule ends\", train_labels)\n\nplt.figure(figsize=(15,3))\nsns.barplot(x=tubeends_counts.index.values, y=tubeends_counts.values, palette=\"Purples\")\nplt.ylabel(\"Counts in train data\")", "processed": ["microtubul end"]}, {"markdown": ["### Nuclear speckles"], "code": "# Reference: https://www.kaggle.com/code/allunia/protein-atlas-exploration-and-baseline\n\nnuclear_speckles_counts = find_counts(\"Nuclear speckles\", train_labels)\n\nplt.figure(figsize=(15,3))\nsns.barplot(x=nuclear_speckles_counts.index.values, y=nuclear_speckles_counts.values, palette=\"Oranges\")\nplt.xticks(rotation=\"70\")\nplt.ylabel(\"Counts in train data\");", "processed": ["nuclear speckl"]}, {"markdown": ["Even though this accuracy looks nice it's an illusion! We are far away from a good model. Let's try to understand why..."], "code": "# Reference: https://www.kaggle.com/code/allunia/protein-atlas-exploration-and-baseline\n\ny_pred[0]\ny_true[0]\nproba_predictions = baseline_proba_predictions.values\nhot_values = validation_labels.drop([\"Id\", \"Target\", \"number_of_targets\"], axis=1).values.flatten()\none_hot = (hot_values.sum()) / hot_values.shape[0] * 100\nzero_hot = (hot_values.shape[0] - hot_values.sum()) / hot_values.shape[0] * 100\n\nfig, ax = plt.subplots(1,2, figsize=(20,5))\nsns.distplot(proba_predictions.flatten() * 100, color=\"DodgerBlue\", ax=ax[0])\nax[0].set_xlabel(\"Probability in %\")\nax[0].set_ylabel(\"Density\")\nax[0].set_title(\"Predicted probabilities\")\nsns.barplot(x=[\"label = 0\", \"label = 1\"], y=[zero_hot, one_hot], ax=ax[1])\nax[1].set_ylim([0,100])\nax[1].set_title(\"True target label count\")\nax[1].set_ylabel(\"Percentage\");", "processed": ["even though accuraci look nice illus far away good model let tri understand"]}, {"markdown": ["### Take-Away\n\n* We can see that our model was always very uncertain to predict the presence of a target protein. All probabilities are close to zero and there are only a few with targets where our model predicted a protein structure with higher than 10 %.\n* If we take a look at the true target label count we can see that most of our targets are filled with zero. This corresponds to an absence of corresponding target proteins. This makes sense: For each image we have a high probability to contain either 1 or 2 target protein structures. Their label values are one whereas all others are zero. \n* Consequently our high accuracy belongs to the high correct prediction of the absence of target proteins. In contrast we weren't able to predict the presence of a target protein which is the most relevant part! \n* Now a bell should ring :-) Have you ever heard about imbalanced classes and model confusion? ", "### To which targets do the high and small predicted probabilities belong to?"], "code": "# Reference: https://www.kaggle.com/code/allunia/protein-atlas-exploration-and-baseline\n\nmean_predictions = np.mean(proba_predictions, axis=0)\nstd_predictions = np.std(proba_predictions, axis=0)\nmean_targets = validation_labels.drop([\"Id\", \"Target\", \"number_of_targets\"], axis=1).mean()\n\nlabels = validation_labels.drop([\"Id\", \"Target\", \"number_of_targets\"], axis=1).columns.values\n\nfig, ax = plt.subplots(1,2,figsize=(20,5))\nsns.barplot(x=labels,\n            y=mean_predictions,\n            ax=ax[0])\nax[0].set_xticklabels(labels=labels,\n                      rotation=90)\nax[0].set_ylabel(\"Mean predicted probability\")\nax[0].set_title(\"Mean predicted probability per class over all samples\")\nsns.barplot(x=labels,\n           y=std_predictions,\n           ax=ax[1])\nax[1].set_xticklabels(labels=labels,\n                      rotation=90)\nax[1].set_ylabel(\"Standard deviation\")\nax[1].set_title(\"Standard deviation of predicted probability per class over all samples\");\nfig, ax = plt.subplots(1,1,figsize=(20,5))\nsns.barplot(x=labels, y=mean_targets.values, ax=ax)\nax.set_xticklabels(labels=labels,\n                      rotation=90)\nax.set_ylabel(\"Percentage of hot (1)\")\nax.set_title(\"Percentage of hot counts (ones) per target class\")", "processed": ["take away see model alway uncertain predict presenc target protein probabl close zero target model predict protein structur higher 10 take look true target label count see target fill zero correspond absenc correspond target protein make sen imag high probabl contain either 1 2 target protein structur label valu one wherea other zero consequ high accuraci belong high correct predict absenc target protein contrast abl predict presenc target protein relev part bell ring ever heard imbalanc class model confus", "target high small predict probabl belong"]}, {"markdown": ["### Take-Away\n\n* Our baseline model seemed to learn something even if this something does not look very nice. \n* Taking a look at the standard deviation we can see that all samples have nearly the same predicted values. There is no deviation, no difference between them. This is of course very bad! :-(\n\nLet's go one step deeper and take a look at the Cytosol (choose another feature if you like ;-)). Here we can see a higher standard deviation than for all other samples and perhaps its corresponding distribution starts to diverge, trying to get bimodal. This would be great at it indicates that the model starts solving the problem of binary classification for this target:"], "code": "# Reference: https://www.kaggle.com/code/allunia/protein-atlas-exploration-and-baseline\n\nfeature = \"Cytosol\"\nplt.figure(figsize=(20,5))\nsns.distplot(baseline_proba_predictions[feature].values[0:-10], color=\"Purple\")\nplt.xlabel(\"Predicted probabilites of {}\".format(feature))\nplt.ylabel(\"Density\")\nplt.xlim([0,1])", "processed": ["take away baselin model seem learn someth even someth look nice take look standard deviat see sampl nearli predict valu deviat differ cours bad let go one step deeper take look cytosol choos anoth featur like see higher standard deviat sampl perhap correspond distribut start diverg tri get bimod would great indic model start solv problem binari classif target"]}, {"markdown": ["Ok, now let's increase the number of epochs and decrease the batch_size. This way we use more weight update steps and hopefully makes our model learn more than before: "], "code": "# Reference: https://www.kaggle.com/code/allunia/protein-atlas-exploration-and-baseline\n\nparameter = ModelParameter(train_path, num_classes=len(wishlist), n_epochs=5, batch_size=64)\npreprocessor = ImagePreprocessor(parameter)\nlabels = train_labels\ntraining_generator = ImprovedDataGenerator(partition['train'], labels,\n                                           parameter, preprocessor, wishlist)\nvalidation_generator = ImprovedDataGenerator(partition['validation'], labels,\n                                             parameter, preprocessor, wishlist)\npredict_generator = PredictGenerator(partition['validation'], preprocessor, train_path)\ntest_preprocessor = ImagePreprocessor(parameter)\nsubmission_predict_generator = PredictGenerator(test_names, test_preprocessor, test_path)\n# Run computation and store results as csv\nif kernelsettings.fit_improved_baseline == True:\n    model = ImprovedModel(parameter, use_dropout=use_dropout)\n    model.build_model()\n    model.compile_model()\n    model.set_generators(training_generator, validation_generator)\n    epoch_history = model.learn()\n    proba_predictions = model.predict(predict_generator)\n    #model.save(\"improved_model.h5\")\n    \n    improved_proba_predictions = pd.DataFrame(proba_predictions, columns=wishlist)\n    improved_proba_predictions.to_csv(\"improved_predictions.csv\")\n    improved_losses = pd.DataFrame(epoch_history.history[\"loss\"], columns=[\"train_loss\"])\n    improved_losses[\"val_loss\"] = epoch_history.history[\"val_loss\"]\n    improved_losses.to_csv(\"improved_losses.csv\")\n    improved_batch_losses = pd.DataFrame(model.history.losses, columns=[\"batch_losses\"])\n    improved_batch_losses.to_csv(\"improved_batch_losses.csv\")\n    \n    improved_submission_proba_predictions = model.predict(submission_predict_generator)\n    improved_test_labels = test_labels.copy()\n    improved_test_labels.loc[:, wishlist] = improved_submission_proba_predictions\n    improved_test_labels.to_csv(\"improved_submission_proba.csv\")\n# If you already have done a baseline fit once, \n# you can load predictions as csv and further fitting is not neccessary:\nelse:\n    improved_proba_predictions = pd.read_csv(\"../input/protein-atlas-eab-predictions/improved_predictions.csv\", index_col=0)\n    improved_losses= pd.read_csv(\"../input/protein-atlas-eab-predictions/improved_losses.csv\", index_col=0)\n    improved_batch_losses = pd.read_csv(\"../input/protein-atlas-eab-predictions/improved_batch_losses.csv\", index_col=0)\n    improved_test_labels = pd.read_csv(\"../input/protein-atlas-eab-predictions/improved_submission_proba.csv\",\n                                      index_col=0)\nfig, ax = plt.subplots(2,1,figsize=(20,13))\nax[0].plot(np.arange(1,6), improved_losses[\"train_loss\"].values, 'r--o', label=\"train_loss\")\nax[0].plot(np.arange(1,6), improved_losses[\"val_loss\"].values, 'g--o', label=\"validation_loss\")\nax[0].set_xlabel(\"Epoch\")\nax[0].set_ylabel(\"Loss\")\nax[0].set_title(\"Loss evolution per epoch\")\nax[0].legend()\nax[1].plot(improved_batch_losses.batch_losses.values, 'r-+', label=\"train_batch_losses\")\nax[1].set_xlabel(\"Number of update steps in total\")\nax[1].set_ylabel(\"Train loss\")\nax[1].set_title(\"Train loss evolution per batch\");", "processed": ["ok let increas number epoch decreas batch size way use weight updat step hope make model learn"]}, {"markdown": ["### What does the loss tell us?\n\nThe loss is very noisy! While decreasing the batch size we increased the number of learning steps. Hence our model learns faster. But... with smaller batch size there **are fewer samples to learn from, to compute gradients from**! The gradients we obtain may be very specific to the images and class labels that are covered by the batch of the current learning step. **There was a tradeoff we made**. We gained more learning speed but payed with a reduced gradient quality. Before increasing the batch size again and waiting too long for predictions we might improve by choosing another way:\n\n1. Weight regularization\n2. Gradient clipping\n\nThese two will be the next improvement steps. Nonetheless, one question remains: Has our model started learning? Can we see a separating force that tries to split zero and one predictions?", "### Does our model try to classify?\n\nIf this is the case and our model starts learning we should see more bimodal distributions of the predicted probability per target label:"], "code": "# Reference: https://www.kaggle.com/code/allunia/protein-atlas-exploration-and-baseline\n\nfig, ax = plt.subplots(3,1,figsize=(25,15))\nsns.distplot(improved_proba_predictions.values[:,0], color=\"Orange\", ax=ax[0])\nax[0].set_xlabel(\"Predicted probabilites of {}\".format(improved_proba_predictions.columns.values[0]))\nax[0].set_xlim([0,1])\nsns.distplot(improved_proba_predictions.values[:,1], color=\"Purple\", ax=ax[1])\nax[1].set_xlabel(\"Predicted probabilites of {}\".format(improved_proba_predictions.columns.values[1]))\nax[1].set_xlim([0,1])\nsns.distplot(improved_proba_predictions.values[:,2], color=\"Limegreen\", ax=ax[2])\nax[2].set_xlabel(\"Predicted probabilites of {}\".format(improved_proba_predictions.columns.values[2]))\nax[2].set_xlim([0,1]);", "processed": ["loss tell u loss noisi decreas batch size increas number learn step henc model learn faster smaller batch size fewer sampl learn comput gradient gradient obtain may specif imag class label cover batch current learn step tradeoff made gain learn speed pay reduc gradient qualiti increas batch size wait long predict might improv choos anoth way 1 weight regular 2 gradient clip two next improv step nonetheless one question remain model start learn see separ forc tri split zero one predict", "model tri classifi case model start learn see bimod distribut predict probabl per target label"]}, {"markdown": ["Ok, great. "], "code": "# Reference: https://www.kaggle.com/code/allunia/protein-atlas-exploration-and-baseline\n\nfig, ax = plt.subplots(3,1,figsize=(20,18))\nfor n in range(len(wishlist)):\n    sns.distplot(improved_proba_predictions.loc[validation_labels[wishlist[n]] == 1,\n                                                wishlist[n]], color=\"Green\", label=\"1-hot\", ax=ax[n])\n    sns.distplot(improved_proba_predictions.loc[validation_labels[wishlist[n]] == 0,\n                                                wishlist[n]], color=\"Red\", label=\"0-zero\", ax=ax[n])\n    ax[n].set_title(wishlist[n])\n    ax[n].legend()\nth_cytosol = 0.4\nth_plasma_membrane = 0.2\nth_nucleoplasm = 0.55\nimproved_submission = improved_test_labels.copy()\nimproved_submission.head(2)\nimproved_submission[\"Nucleoplasm\"] = np.where(improved_test_labels[\"Nucleoplasm\"] >= th_nucleoplasm, 1, 0)\nimproved_submission[\"Cytosol\"] = np.where(improved_test_labels[\"Cytosol\"] >= th_cytosol, 1, 0)\nimproved_submission[\"Plasma membrane\"] = np.where(\n    improved_test_labels[\"Plasma membrane\"] >= th_plasma_membrane, 1, 0)\n\nimproved_submission.head(5)", "processed": ["ok great"]}, {"markdown": ["## How can we tackle gradient jiggles?\n\nLet's try to dive deeper into the problem. We compute the gradients with respect to the weights after processing each batch this way:\n\n$$ \\partial_{w_{i,j}} E = \\sum_{m=1}^{M} \\partial_{w_{i,j}} E_{m} $$", "Looking at the sum we can see one disadvantage... it's mainly driven by high contributions. **An image in the batch that causes very high positive or negative gradients for the weight $w_{i,j}$ have more impact on the overall gradient than images with low absolute values**. This can be bad especially in the case of outlier images that are not representative to explain the pattern in the data. Consequently our model may try to learn from exotics. In addition we have to be very **careful with small batches as its target distribution might not reflect the overall pattern**. Imagine we would try to distinguish dogs from cats. With a batch size of 10 we are likely to fill up these places with imbalanced targets. For example it could be occupied with cats only. This would yield gradients that try to improve the detection of cats thereby changing the weights we might need to identify dogs. Hence beside image outliers the target distribution itself influences the learning as well. This can cause jiggles as well. One step we try to improve nucleoplasmn and the next perhaps cytosol but with a downgrade of the nucleoplasmn predictions and the next steps it could be the other way round. \n\nHow to solve this jiggle-wiggle problem?\n\nWell, first of all we might choose a higher batch_size again :-) Decreasing the batch size we made a tradeoff: We increased the learning speed but increased the risk of low quality gradients. Hence before playing with further strategies, we should make a step backwards again. With a batch_size of 128 and number of epochs 10, we obtain these losses:"], "code": "# Reference: https://www.kaggle.com/code/allunia/protein-atlas-exploration-and-baseline\n\nparameter = ModelParameter(train_path, num_classes=len(wishlist), n_epochs=10, batch_size=128)\npreprocessor = ImagePreprocessor(parameter)\nlabels = train_labels\n\ntraining_generator = ImprovedDataGenerator(partition['train'], labels,\n                                           parameter, preprocessor, wishlist)\nvalidation_generator = ImprovedDataGenerator(partition['validation'], labels,\n                                             parameter, preprocessor, wishlist)\npredict_generator = PredictGenerator(partition['validation'], preprocessor, train_path)\n# Run computation and store results as csv\nif kernelsettings.fit_improved_higher_batchsize == True:\n    model = ImprovedModel(parameter, use_dropout=True)\n    model.build_model()\n    model.compile_model()\n    model.set_generators(training_generator, validation_generator)\n    epoch_history = model.learn()\n    proba_predictions = model.predict(predict_generator)\n    #model.save(\"improved_model.h5\")\n    improved_proba_predictions = pd.DataFrame(proba_predictions, columns=wishlist)\n    improved_proba_predictions.to_csv(\"improved_hbatch_predictions.csv\")\n    improved_losses = pd.DataFrame(epoch_history.history[\"loss\"], columns=[\"train_loss\"])\n    improved_losses[\"val_loss\"] = epoch_history.history[\"val_loss\"]\n    improved_losses.to_csv(\"improved_hbatch_losses.csv\")\n    improved_batch_losses = pd.DataFrame(model.history.losses, columns=[\"batch_losses\"])\n    improved_batch_losses.to_csv(\"improved_hbatch_batch_losses.csv\")\n# If you already have done a baseline fit once, \n# you can load predictions as csv and further fitting is not neccessary:\nelse:\n    improved_proba_predictions = pd.read_csv(\n        \"../input/protein-atlas-eab-predictions/improved_hbatch_predictions.csv\", index_col=0)\n    improved_losses= pd.read_csv(\n        \"../input/protein-atlas-eab-predictions/improved_hbatch_losses.csv\", index_col=0)\n    improved_batch_losses = pd.read_csv(\"../input/protein-atlas-eab-predictions/improved_hbatch_batch_losses.csv\", index_col=0)\nfig, ax = plt.subplots(2,1,figsize=(20,13))\nax[0].plot(np.arange(1,11), improved_losses[\"train_loss\"].values, 'r--o', label=\"train_loss\")\nax[0].plot(np.arange(1,11), improved_losses[\"val_loss\"].values, 'g--o', label=\"validation_loss\")\nax[0].set_xlabel(\"Epoch\")\nax[0].set_ylabel(\"Loss\")\nax[0].set_title(\"Loss evolution per epoch\")\nax[0].legend()\nax[1].plot(improved_batch_losses.batch_losses.values, 'r-+', label=\"train_batch_losses\")\nax[1].set_xlabel(\"Number of update steps in total\")\nax[1].set_ylabel(\"Train loss\")\nax[1].set_title(\"Train loss evolution per batch\");\nfig, ax = plt.subplots(3,1,figsize=(25,15))\nsns.distplot(improved_proba_predictions.values[:,0], color=\"Orange\", ax=ax[0])\nax[0].set_xlabel(\"Predicted probabilites of {}\".format(improved_proba_predictions.columns.values[0]))\nax[0].set_xlim([0,1])\nsns.distplot(improved_proba_predictions.values[:,1], color=\"Purple\", ax=ax[1])\nax[1].set_xlabel(\"Predicted probabilites of {}\".format(improved_proba_predictions.columns.values[1]))\nax[1].set_xlim([0,1])\nsns.distplot(improved_proba_predictions.values[:,2], color=\"Limegreen\", ax=ax[2])\nax[2].set_xlabel(\"Predicted probabilites of {}\".format(improved_proba_predictions.columns.values[2]))\nax[2].set_xlim([0,1]);", "processed": ["tackl gradient jiggl let tri dive deeper problem comput gradient respect weight process batch way partial w j e sum 1 partial w j e", "look sum see one disadvantag mainli driven high contribut imag batch caus high posit neg gradient weight w j impact overal gradient imag low absolut valu bad especi case outlier imag repres explain pattern data consequ model may tri learn exot addit care small batch target distribut might reflect overal pattern imagin would tri distinguish dog cat batch size 10 like fill place imbalanc target exampl could occupi cat would yield gradient tri improv detect cat therebi chang weight might need identifi dog henc besid imag outlier target distribut influenc learn well caus jiggl well one step tri improv nucleoplasmn next perhap cytosol downgrad nucleoplasmn predict next step could way round solv jiggl wiggl problem well first might choos higher batch size decreas batch size made tradeoff increas learn speed increas risk low qualiti gradient henc play strategi make step backward batch size 128 number epoch 10 obtain loss"]}, {"markdown": ["### Dropout Layer\n\nI often read that dropout helps to avoid overfitting but for me it seems that there is one more useful advantage: Imagine the cat-problem above - Given a batch full of cats we only compute gradients for making the predictions of cats better. And this can be done to the detriment of the dogs as both classes share weights. Hence changing the weights with batches of imbalanced classes can lead to jiggles. \n\nNow let's consider a dropout-layer: It randomly selects a given percentage of input neurons and drops them during the current training step. Taking a look at [this paper](http://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf) (linked at keras documentation) you can see that the forward and backward propagation is done only with this reduced, thinned kind of network. In our case this means that each learning step some weights will be untouched and not used to compute gradients. This is great as this would mean that some weight that could be good for predicting dogs will not change after we process a batch with cats only. ;-)\n\nConsequently if you have a **problem with overfitting** (learning too much out of your training data and loosing generalization performance) **or you have very small, imbalanced batches you should consider dropout** as a strategy. But before turning happy and starting to use dropout frequently, we should think about its downside: Dropping neruons during training and learning with a thinned kind of networks means that **we freeze learning each step a bit**. And the randomness of frosty the dropout snowman can turn to a problem difficult to graps: Which neurons should be dropped, are there some for which dropping is good and some for which it is bad? After which layer in our network does it make sense to use it?Perhaps it would have been better to use some neurons that were dropped a random dropout session during one batch learning step... we don't know. Perhaps we have prevented the success of a learning step given one batch and improved learning given another batch. The information flow through the network is somehow a blackbox for us and this randomness of thinned network learning makes it more difficult to understand what's going on. \n\nYou can see that in my baseline model I'm already using dropout. This choice was somehow arbitrarily as I used an example network that can be found to classify mnist digits as a starting point. Hence let's improve again and **turn dropout to a plug-and-play feature**:\n\n```\nclass ImprovedModel(BaseLineModel):\n    \n    def __init__(self, modelparameter,\n                 my_metrics=[f1_mean, f1_std, f1_min, f1_max],\n                 use_dropout=True):\n        \n        super().__init__(modelparameter)\n        self.my_metrics = my_metrics\n        self.use_dropout = use_dropout\n\n    def build_model(self):\n        self.model = Sequential()\n        self.model.add(Conv2D(16, kernel_size=(3, 3), activation='relu', input_shape=self.input_shape))\n        self.model.add(Conv2D(32, (3, 3), activation='relu'))\n        self.model.add(MaxPooling2D(pool_size=(2, 2)))\n        if self.use_dropout:\n            self.model.add(Dropout(0.25))\n        self.model.add(Flatten())\n        self.model.add(Dense(64, activation='relu'))\n        if self.use_dropout:\n        self.model.add(Dropout(0.5))\n        self.model.add(Dense(self.num_classes, activation='sigmoid'))\n```", "Our last losses were obtained by using dropout with high percentage (25 % and 50 %) of dropped neurons. **What will happen if we do not use our dropout layers?**\n"], "code": "# Reference: https://www.kaggle.com/code/allunia/protein-atlas-exploration-and-baseline\n\n# Run computation and store results as csv\nif kernelsettings.fit_improved_without_dropout == True:\n    model = ImprovedModel(parameter, use_dropout=False)\n    model.build_model()\n    model.compile_model()\n    model.set_generators(training_generator, validation_generator)\n    epoch_history = model.learn()\n    proba_predictions = model.predict(predict_generator)\n    #model.save(\"improved_model.h5\")\n    improved_proba_predictions = pd.DataFrame(proba_predictions, columns=wishlist)\n    improved_proba_predictions.to_csv(\"improved_nodropout_predictions.csv\")\n    improved_losses = pd.DataFrame(epoch_history.history[\"loss\"], columns=[\"train_loss\"])\n    improved_losses[\"val_loss\"] = epoch_history.history[\"val_loss\"]\n    improved_losses.to_csv(\"improved_nodropout_losses.csv\")\n    improved_batch_losses = pd.DataFrame(model.history.losses, columns=[\"batch_losses\"])\n    improved_batch_losses.to_csv(\"improved_nodropout_batch_losses.csv\")\n# If you already have done a baseline fit once, \n# you can load predictions as csv and further fitting is not neccessary:\nelse:\n    improved_proba_predictions_no_dropout = pd.read_csv(\n        \"../input/protein-atlas-eab-predictions/improved_nodropout_predictions.csv\", index_col=0)\n    improved_losses_no_dropout= pd.read_csv(\n        \"../input/protein-atlas-eab-predictions/improved_nodropout_losses.csv\", index_col=0)\n    improved_batch_losses_no_dropout = pd.read_csv(\n        \"../input/protein-atlas-eab-predictions/improved_nodropout_batch_losses.csv\", index_col=0)\nfig, ax = plt.subplots(2,1,figsize=(20,13))\nax[0].plot(np.arange(1,11), improved_losses[\"train_loss\"].values, 'r--o', label=\"train_loss_dropout\")\nax[0].plot(np.arange(1,11), improved_losses_no_dropout[\"train_loss\"].values, 'r-o', label=\"train_loss_no_dropout\")\nax[0].plot(np.arange(1,11), improved_losses[\"val_loss\"].values, 'g--o', label=\"validation_loss\")\nax[0].plot(np.arange(1,11), improved_losses_no_dropout[\"val_loss\"].values, 'g-o', label=\"validation_loss_no_dropout\")\nax[0].set_xlabel(\"Epoch\")\nax[0].set_ylabel(\"Loss\")\nax[0].set_title(\"Loss evolution per epoch\")\nax[0].legend()\nax[1].plot(improved_batch_losses.batch_losses.values[-800::], 'r-+', label=\"train_batch_losses_dropout\")\nax[1].plot(improved_batch_losses_no_dropout.batch_losses.values[-800::], 'b-+',\n           label=\"train_batch_losses_no_dropout\")\nax[1].set_xlabel(\"Number of update steps in total\")\nax[1].set_ylabel(\"Train loss\")\nax[1].set_title(\"Train loss evolution per batch\");\nax[1].legend();", "processed": ["dropout layer often read dropout help avoid overfit seem one use advantag imagin cat problem given batch full cat comput gradient make predict cat better done detriment dog class share weight henc chang weight batch imbalanc class lead jiggl let consid dropout layer randomli select given percentag input neuron drop current train step take look paper http www jmlr org paper volume15 srivastava14a srivastava14a pdf link kera document see forward backward propag done reduc thin kind network case mean learn step weight untouch use comput gradient great would mean weight could good predict dog chang process batch cat consequ problem overfit learn much train data loo gener perform small imbalanc batch consid dropout strategi turn happi start use dropout frequent think downsid drop neruon train learn thin kind network mean freez learn step bit random frosti dropout snowman turn problem difficult grap neuron drop drop good bad layer network make sen use perhap would better use neuron drop random dropout session one batch learn step know perhap prevent success learn step given one batch improv learn given anoth batch inform flow network somehow blackbox u random thin network learn make difficult understand go see baselin model alreadi use dropout choic somehow arbitrarili use exampl network found classifi mnist digit start point henc let improv turn dropout plug play featur class improvedmodel baselinemodel def init self modelparamet metric f1 mean f1 std f1 min f1 max use dropout true super init modelparamet self metric metric self use dropout use dropout def build model self self model sequenti self model add conv2d 16 kernel size 3 3 activ relu input shape self input shape self model add conv2d 32 3 3 activ relu self model add maxpooling2d pool size 2 2 self use dropout self model add dropout 0 25 self model add flatten self model add den 64 activ relu self use dropout self model add dropout 0 5 self model add den self num class activ sigmoid", "last loss obtain use dropout high percentag 25 50 drop neuron happen use dropout layer"]}, {"markdown": ["# Phase 1: Train on all data"], "code": "# Reference: https://www.kaggle.com/code/xhlulu/recursion-2-headed-efficientnet-2-stage-training\n\nmodel = build_model(\n    input_shape=(300, 300, 3),\n    n_classes=train_target_df.shape[1]\n)\nmodel.summary()\ncheckpoint = ModelCheckpoint(\n    'model.h5', \n    monitor='val_loss', \n    verbose=0, \n    save_best_only=True, \n    save_weights_only=False,\n    mode='auto'\n)\n\nhistory = model.fit_generator(\n    train_generator,\n    validation_data=val_generator,\n    callbacks=[checkpoint],\n    use_multiprocessing=False,\n    workers=1,\n    verbose=2,\n    epochs=20\n)\nwith open('history.json', 'w') as f:\n    json.dump(history.history, f)\n\nhistory_df = pd.DataFrame(history.history)\nhistory_df[['loss', 'val_loss']].plot()\nhistory_df[['acc', 'val_acc']].plot()", "processed": ["phase 1 train data"]}, {"markdown": ["Let's look to the distribution of sentiments in the train and test data."], "code": "# Reference: https://www.kaggle.com/code/gpreda/tweet-sentiment-extraction-eda\n\ndef plot_sentiment_count(data_df, title):\n    plt.figure(figsize=(8,6))\n    sns.countplot(data_df['sentiment'])\n    plt.title(title)\n    plt.show()\nplot_sentiment_count(train_df, \"Sentiment distribution: train\")\nplot_sentiment_count(test_df, \"Sentiment distribution: test\")", "processed": ["let look distribut sentiment train test data"]}, {"markdown": ["So we do not have question ids for the test set. I hope the reason is as follows:\n\n*As an anti-cheating measure, Kaggle has supplemented the test set with computer-generated question pairs. Those rows do not come from Quora, and are not counted in the scoring. All of the questions in the training set are genuine examples from Quora.*\n\nSince some questions are not from Quora, question ids are not present I think.", "**Target Variable Exploration:**\n\nFirst let us look at the target variable distribution."], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-leaky-exploration-notebook-quora\n\nis_dup = train_df['is_duplicate'].value_counts()\n\nplt.figure(figsize=(8,4))\nsns.barplot(is_dup.index, is_dup.values, alpha=0.8, color=color[1])\nplt.ylabel('Number of Occurrences', fontsize=12)\nplt.xlabel('Is Duplicate', fontsize=12)\nplt.show()\nis_dup / is_dup.sum()", "processed": ["question id test set hope reason follow anti cheat measur kaggl supplement test set comput gener question pair row come quora count score question train set genuin exampl quora sinc question quora question id present think", "target variabl explor first let u look target variabl distribut"]}, {"markdown": ["So we have about 63% non-duplicate questions and 37% duplicate questions in the training data set.", "**Questions Exploration:**\n\nNow let us explore the question fields present in the train data. First let us check the number of words distribution in the questions."], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-leaky-exploration-notebook-quora\n\nall_ques_df = pd.DataFrame(pd.concat([train_df['question1'], train_df['question2']]))\nall_ques_df.columns = [\"questions\"]\n\nall_ques_df[\"num_of_words\"] = all_ques_df[\"questions\"].apply(lambda x : len(str(x).split()))\ncnt_srs = all_ques_df['num_of_words'].value_counts()\n\nplt.figure(figsize=(12,6))\nsns.barplot(cnt_srs.index, cnt_srs.values, alpha=0.8, color=color[0])\nplt.ylabel('Number of Occurrences', fontsize=12)\nplt.xlabel('Number of words in the question', fontsize=12)\nplt.xticks(rotation='vertical')\nplt.show()", "processed": ["63 non duplic question 37 duplic question train data set", "question explor let u explor question field present train data first let u check number word distribut question"]}, {"markdown": ["So the distribution is right skewed with upto 237 words in a question. There are also few questions with 1 or 2 words as well.\n\nNow let us explore the number of characters distribution as well."], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-leaky-exploration-notebook-quora\n\nall_ques_df[\"num_of_chars\"] = all_ques_df[\"questions\"].apply(lambda x : len(str(x)))\ncnt_srs = all_ques_df['num_of_chars'].value_counts()\n\nplt.figure(figsize=(50,8))\nsns.barplot(cnt_srs.index, cnt_srs.values, alpha=0.8, color=color[3])\nplt.ylabel('Number of Occurrences', fontsize=12)\nplt.xlabel('Number of characters in the question', fontsize=12)\nplt.xticks(rotation='vertical')\nplt.show()      \n\ndel all_ques_df", "processed": ["distribut right skew upto 237 word question also question 1 2 word well let u explor number charact distribut well"]}, {"markdown": ["Number of characters distribution as well is right skewed.\n\nOne interesting point is the sudden dip at the 150 character mark. Not sure why is that so.!\n\nNow let us look at the distribution of common unigrams between the given question pairs."], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-leaky-exploration-notebook-quora\n\ndef get_unigrams(que):\n    return [word for word in word_tokenize(que.lower()) if word not in eng_stopwords]\n\ndef get_common_unigrams(row):\n    return len( set(row[\"unigrams_ques1\"]).intersection(set(row[\"unigrams_ques2\"])) )\n\ndef get_common_unigram_ratio(row):\n    return float(row[\"unigrams_common_count\"]) / max(len( set(row[\"unigrams_ques1\"]).union(set(row[\"unigrams_ques2\"])) ),1)\n\ntrain_df[\"unigrams_ques1\"] = train_df['question1'].apply(lambda x: get_unigrams(str(x)))\ntrain_df[\"unigrams_ques2\"] = train_df['question2'].apply(lambda x: get_unigrams(str(x)))\ntrain_df[\"unigrams_common_count\"] = train_df.apply(lambda row: get_common_unigrams(row),axis=1)\ntrain_df[\"unigrams_common_ratio\"] = train_df.apply(lambda row: get_common_unigram_ratio(row), axis=1)\ncnt_srs = train_df['unigrams_common_count'].value_counts()\n\nplt.figure(figsize=(12,6))\nsns.barplot(cnt_srs.index, cnt_srs.values, alpha=0.8)\nplt.ylabel('Number of Occurrences', fontsize=12)\nplt.xlabel('Common unigrams count', fontsize=12)\nplt.show()", "processed": ["number charact distribut well right skew one interest point sudden dip 150 charact mark sure let u look distribut common unigram given question pair"]}, {"markdown": ["It is interesting to see that there are very few question pairs with no common words. "], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-leaky-exploration-notebook-quora\n\nplt.figure(figsize=(12,6))\nsns.boxplot(x=\"is_duplicate\", y=\"unigrams_common_count\", data=train_df)\nplt.xlabel('Is duplicate', fontsize=12)\nplt.ylabel('Common unigrams count', fontsize=12)\nplt.show()", "processed": ["interest see question pair common word"]}, {"markdown": ["There is some good difference between 0 and 1 class using the common unigram count variable. Let us look at the same graph using common unigrams ratio."], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-leaky-exploration-notebook-quora\n\nplt.figure(figsize=(12,6))\nsns.boxplot(x=\"is_duplicate\", y=\"unigrams_common_ratio\", data=train_df)\nplt.xlabel('Is duplicate', fontsize=12)\nplt.ylabel('Common unigrams ratio', fontsize=12)\nplt.show()", "processed": ["good differ 0 1 class use common unigram count variabl let u look graph use common unigram ratio"]}, {"markdown": ["**Q1-Q2 neighbor intersection count:**\n\nLet us first do simple count plots and see the distribution."], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-leaky-exploration-notebook-quora\n\ncnt_srs = train_df['q1_q2_intersect'].value_counts()\n\nplt.figure(figsize=(12,6))\nsns.barplot(cnt_srs.index, np.log1p(cnt_srs.values), alpha=0.8)\nplt.xlabel('Q1-Q2 neighbor intersection count', fontsize=12)\nplt.ylabel('Log of Number of Occurrences', fontsize=12)\nplt.xticks(rotation='vertical')\nplt.show()\ngrouped_df = train_df.groupby('q1_q2_intersect')['is_duplicate'].aggregate(np.mean).reset_index()\nplt.figure(figsize=(12,8))\nsns.pointplot(grouped_df[\"q1_q2_intersect\"].values, grouped_df[\"is_duplicate\"].values, alpha=0.8, color=color[2])\nplt.ylabel('Mean is_duplicate', fontsize=12)\nplt.xlabel('Q1-Q2 neighbor intersection count', fontsize=12)\nplt.xticks(rotation='vertical')\nplt.show()", "processed": ["q1 q2 neighbor intersect count let u first simpl count plot see distribut"]}, {"markdown": ["Wow. This explains why this variable is super predictive.!\n\n**Question1 Frequency:**"], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-leaky-exploration-notebook-quora\n\ncnt_srs = train_df['q1_freq'].value_counts()\n\nplt.figure(figsize=(12,8))\nsns.barplot(cnt_srs.index, cnt_srs.values, alpha=0.8)\nplt.xlabel('Q1 frequency', fontsize=12)\nplt.ylabel('Number of Occurrences', fontsize=12)\nplt.xticks(rotation='vertical')\nplt.show()", "processed": ["wow explain variabl super predict question1 frequenc"]}, {"markdown": ["We could see a long tail here as well. Now let us check the target variable distribution."], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-leaky-exploration-notebook-quora\n\nplt.figure(figsize=(12,8))\ngrouped_df = train_df.groupby('q1_freq')['is_duplicate'].aggregate(np.mean).reset_index()\nsns.barplot(grouped_df[\"q1_freq\"].values, grouped_df[\"is_duplicate\"].values, alpha=0.8, color=color[4])\nplt.ylabel('Mean is_duplicate', fontsize=12)\nplt.xlabel('Q1 frequency', fontsize=12)\nplt.xticks(rotation='vertical')\nplt.show()", "processed": ["could see long tail well let u check target variabl distribut"]}, {"markdown": ["# View OOF Examples\nBelow are examples of our predicted bounding boxes (masks). Yellow is the true mask and blue is our predicted bounding box. Note that we predict no bounding box when classification probability is less than 0.65. Below we show 9 examples for each cloud type where we predicted a bounded box (i.e. classification prob > 0.65)."], "code": "# Reference: https://www.kaggle.com/code/cdeotte/cloud-bounding-boxes-lb-0-61\n\ndef mask2contour(mask, width=5):\n    w = mask.shape[1]\n    h = mask.shape[0]\n    \n    mask2 = np.concatenate([mask[:,width:],np.zeros((h,width))],axis=1)\n    mask2 = np.logical_xor(mask,mask2)\n    mask3 = np.concatenate([mask[width:,:],np.zeros((width,w))],axis=0)\n    mask3 = np.logical_xor(mask,mask3)\n    \n    return np.logical_or(mask2,mask3) \nPATH = '../input/understanding_cloud_organization/train_images/'\nfor d in range(1,5):\n    print('#'*27); print('#'*5,type2[d-1],'CLOUDS','#'*7); print('#'*27)\n    plt.figure(figsize=(20,15)); k=0\n    for kk in range(9):\n        plt.subplot(3,3,kk+1)\n        while (train2.loc[train2.index[k],'o'+str(d)]<0.65): k += 1\n        f = train2.index[k]+'.jpg'\n        img = Image.open(PATH+f); img = img.resize((525,350)); img = np.array(img)\n        rle1 = train2.loc[train2.index[k],'e'+str(d)]; mask = rle2mask(rle1,shrink=4)\n        contour = mask2contour(mask,5); img[contour==1,:2] = 255\n        rle2 = train2.loc[train2.index[k],'bb'+str(d)]; mask = rle2mask(rle2,shape=(525,350))\n        contour = mask2contour(mask,5); img[contour==1,2] = 255\n        dice = np.round( dice_coef6(rle1,1,rle2,0),3 )\n        plt.title(f+'  Dice = '+str(dice)+'   Yellow true, Blue predicted')\n        plt.imshow(img); k += 1\n    plt.show()", "processed": ["view oof exampl exampl predict bound box mask yellow true mask blue predict bound box note predict bound box classif probabl le 0 65 show 9 exampl cloud type predict bound box e classif prob 0 65"]}, {"markdown": ["![](https://files.slack.com/files-pri/TJ8UK7F1Q-FK90XAA3G/img_20190602_173521.png)", "# Assumptions\n- Test set in that image are the same in this competition.\n- There are 9 Quakes in the test set with max TTF:\n    - 11 sec\n    - 11 sec\n    - 11 sec\n    - 5 sec\n    - 10 sec\n    - 15 sec\n    - 7 sec\n    - 15 sec to 10 sec"], "code": "# Reference: https://www.kaggle.com/code/robikscube/lanl-simulating-the-test-set\n\nsamp_per_second = 31\nttf_array = np.concatenate([np.linspace(11, 0, samp_per_second * 11),\n                np.linspace(11, 0, samp_per_second * 11),\n                np.linspace(11, 0, samp_per_second * 11),\n                np.linspace(5, 0, samp_per_second * 5),\n                np.linspace(10, 0, samp_per_second * 10),\n                np.linspace(15, 0, samp_per_second * 15),\n                np.linspace(7, 0, samp_per_second * 7),\n                np.linspace(15, 10, samp_per_second * 5)])\nss = pd.read_csv('../input/LANL-Earthquake-Prediction/sample_submission.csv')\nss_simulated = pd.DataFrame(ttf_array, columns=['time_to_failure'])\nss_simulated.plot(figsize=(15, 5))\nss_simulated.agg(['mean','median'])\nsub1352 = pd.read_csv('../input/lanl-submissions/LB1.352_submission_147.csv')\nsub1389 = pd.read_csv('../input/lanl-submissions/LB1.389_submissionverson_one.csv')\nsub1362 = pd.read_csv('../input/lanl-submissions/LB1.362_submission_147.csv')\nsub1371 = pd.read_csv('../input/lanl-submissions/LB1.371_submission_137.csv')\nsub1380 = pd.read_csv('../input/lanl-submissions/LB1.380_submission.csv')\nsub1381 = pd.read_csv('../input/lanl-submissions/LB1.381_submission_147.csv')\nsub1389 = pd.read_csv('../input/lanl-submissions/LB1.389_submissionverson_one.csv')\nsub_50feats = pd.read_csv('../input/lanl-submissions/submission_50feats_peaks 9000ks.csv')\n\nss['sub1.352'] = sub1352['time_to_failure']\nss['sub1.389'] = sub1389['time_to_failure']\nss['sub1.362'] = sub1362['time_to_failure']\nss['sub1.371'] = sub1371['time_to_failure']\nss['sub1.380'] = sub1380['time_to_failure']\nss['sub1.381'] = sub1381['time_to_failure']\nss['sub1.389'] = sub1389['time_to_failure']\nss['simulated'] = ss_simulated['time_to_failure']", "processed": ["http file slack com file pri tj8uk7f1q fk90xaa3g img 20190602 173521 png", "assumpt test set imag competit 9 quak test set max ttf 11 sec 11 sec 11 sec 5 sec 10 sec 15 sec 7 sec 15 sec 10 sec"]}, {"markdown": ["# Compare Distributions"], "code": "# Reference: https://www.kaggle.com/code/robikscube/lanl-simulating-the-test-set\n\nss_simulated.plot(kind='hist', bins= 100, figsize=(15, 5), alpha=0.5)\nsub1352['time_to_failure'].plot(kind='hist', bins= 100, figsize=(15, 5), label='1.352 SUB', alpha=0.5)\nsub1389['time_to_failure'].plot(kind='hist', bins= 100, figsize=(15, 5), label='1.389 SUB', alpha=0.5)\nsub1362['time_to_failure'].plot(kind='hist', bins= 100, figsize=(15, 5), label='1.362 SUB', alpha=0.5)\nplt.legend()\nplt.show()\nss.agg(['mean','median'])\nsub1389.agg(['mean','median'])\nss.sort_values('sub1.352').reset_index()['sub1.352'].plot(figsize=(15, 10), title='Compare TTF of simulated vs. Submissions')\nss.sort_values('sub1.389').reset_index()['sub1.389'].plot(figsize=(15, 10), title='Compare TTF of simulated vs. Submissions')\nss.sort_values('sub1.362').reset_index()['sub1.362'].plot(figsize=(15, 10), title='Compare TTF of simulated vs. Submissions')\nss.sort_values('sub1.371').reset_index()['sub1.371'].plot(figsize=(15, 10), title='Compare TTF of simulated vs. Submissions')\nss.sort_values('sub1.380').reset_index()['sub1.380'].plot(figsize=(15, 10), title='Compare TTF of simulated vs. Submissions')\nss.sort_values('sub1.381').reset_index()['sub1.381'].plot(figsize=(15, 10), title='Compare TTF of simulated vs. Submissions')\nss.sort_values('simulated').reset_index()['simulated'].plot(figsize=(15, 10), title='Compare TTF of simulated vs. Submissions')\n\nplt.xlabel('Sample Sorted')\nplt.ylabel('Time to Fault')\nplt.legend()\nplt.show()", "processed": ["compar distribut"]}, {"markdown": ["The target distribution is pretty interesting! Spikes near zero, and -20. There is also a good bit around 80. We will return to these files later."], "code": "# Reference: https://www.kaggle.com/code/robikscube/exploring-molecular-properties-data\n\n# Distribution of the target\ntrain_df['scalar_coupling_constant'].plot(kind='hist', figsize=(20, 5), bins=1000, title='Distribution of the target scalar coupling constant')\nplt.show()\n# Number of of atoms in molecule\nfig, ax = plt.subplots(1, 2)\ntrain_df.groupby('molecule_name').count().sort_values('id')['id'].plot(kind='hist',\n                                                                       bins=25,\n                                                                       color=color_pal[6],\n                                                                      figsize=(20, 5),\n                                                                      title='# of Atoms in Molecule (Train Set)',\n                                                                      ax=ax[0])\ntest_df.groupby('molecule_name').count().sort_values('id')['id'].plot(kind='hist',\n                                                                       bins=25,\n                                                                       color=color_pal[2],\n                                                                      figsize=(20, 5),\n                                                                      title='# of Atoms in Molecule (Test Set)',\n                                                                     ax=ax[1])\nplt.show()", "processed": ["target distribut pretti interest spike near zero 20 also good bit around 80 return file later"]}, {"markdown": ["## Structures.csv\nThis file contains the same information as the individual xyz structure files, but in a single file.\n\nAt first glance - this csv seems a lot more useable than the `xyz` files"], "code": "# Reference: https://www.kaggle.com/code/robikscube/exploring-molecular-properties-data\n\nstructures = pd.read_csv('../input/structures.csv')\nstructures.head()\n# 3D Plot!\nfrom mpl_toolkits.mplot3d import Axes3D\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\nexample = structures.loc[structures['molecule_name'] == 'dsgdb9nsd_000001']\nax.scatter(xs=example['x'], ys=example['y'], zs=example['z'], s=100)\nplt.suptitle('dsgdb9nsd_000001')\nplt.show()", "processed": ["structur csv file contain inform individu xyz structur file singl file first glanc csv seem lot useabl xyz file"]}, {"markdown": ["## magnetic_shielding_tensors.csv\n- contains the magnetic shielding tensors for all atoms in the molecules. The first column (molecule_name) contains the molecule name, the second column (atom_index) contains the index of the atom in the molecule, the third to eleventh columns contain the XX, YX, ZX, XY, YY, ZY, XZ, YZ and ZZ elements of the tensor/matrix respectively.\n\n## mulliken_charges.csv\n- contains the mulliken charges for all atoms in the molecules. The first column (molecule_name) contains the name of the molecule, the second column (atom_index) contains the index of the atom in the molecule, the third column (mulliken_charge) contains the mulliken charge of the atom.\n\n## potential_energy.csv\n- contains the potential energy of the molecules. The first column (molecule_name) contains the name of the molecule, the second column (potential_energy) contains the potential energy of the molecule."], "code": "# Reference: https://www.kaggle.com/code/robikscube/exploring-molecular-properties-data\n\nmst = pd.read_csv('../input/magnetic_shielding_tensors.csv')\nmul = pd.read_csv('../input/mulliken_charges.csv')\npote = pd.read_csv('../input/potential_energy.csv')\nscc = pd.read_csv('../input/scalar_coupling_contributions.csv')\nmst.head()\nmul.head()\n# Plot the distribution of mulliken_charges\nmul['mulliken_charge'].plot(kind='hist', figsize=(15, 5), bins=500, title='Distribution of Mulliken Charges')\nplt.show()\npote.head()\n# Plot the distribution of potential_energy\npote['potential_energy'].plot(kind='hist',\n                              figsize=(15, 5),\n                              bins=500,\n                              title='Distribution of Potential Energy',\n                              color='b')\nplt.show()", "processed": ["magnet shield tensor csv contain magnet shield tensor atom molecul first column molecul name contain molecul name second column atom index contain index atom molecul third eleventh column contain xx yx zx xy yy zy xz yz zz element tensor matrix respect mulliken charg csv contain mulliken charg atom molecul first column molecul name contain name molecul second column atom index contain index atom molecul third column mulliken charg contain mulliken charg atom potenti energi csv contain potenti energi molecul first column molecul name contain name molecul second column potenti energi contain potenti energi molecul"]}, {"markdown": ["## scalar_coupling_contributions.csv\n- The scalar coupling constants in train.csv (or corresponding files) are a sum of four terms. scalar_coupling_contributions.csv contain all these terms.\n    - The first column (molecule_name) are the **name of the molecule**,\n    - the second **(atom_index_0)** and\n    - third column **(atom_index_1)** are the atom indices of the atom-pair,\n    - the fourth column indicates the **type of coupling**,\n    - the fifth column (fc) is the **Fermi Contact contribution**,\n    - the sixth column (sd) is the **Spin-dipolar contribution**,\n    - the seventh column (pso) is the **Paramagnetic spin-orbit contribution** and\n    - the eighth column (dso) is the **Diamagnetic spin-orbit contribution**."], "code": "# Reference: https://www.kaggle.com/code/robikscube/exploring-molecular-properties-data\n\nscc.head()\nscc.groupby('type').count()['molecule_name'].sort_values().plot(kind='barh',\n                                                                color='grey',\n                                                               figsize=(15, 5),\n                                                               title='Count of Coupling Type in Train Set')\nplt.show()\nfig, ax = plt.subplots(2, 2, figsize=(20, 10))\nscc['fc'].plot(kind='hist', ax=ax.flat[0], bins=500, title='Fermi Contact contribution', color=color_pal[0])\nscc['sd'].plot(kind='hist', ax=ax.flat[1], bins=500, title='Spin-dipolar contribution', color=color_pal[1])\nscc['pso'].plot(kind='hist', ax=ax.flat[2], bins=500, title='Paramagnetic spin-orbit contribution', color=color_pal[2])\nscc['dso'].plot(kind='hist', ax=ax.flat[3], bins=500, title='Diamagnetic spin-orbit contribution', color=color_pal[3])\nplt.show()", "processed": ["scalar coupl contribut csv scalar coupl constant train csv correspond file sum four term scalar coupl contribut csv contain term first column molecul name name molecul second atom index 0 third column atom index 1 atom indic atom pair fourth column indic type coupl fifth column fc fermi contact contribut sixth column sd spin dipolar contribut seventh column pso paramagnet spin orbit contribut eighth column dso diamagnet spin orbit contribut"]}, {"markdown": ["These plots are beautiful. It's a shame we don't have this data for the test set."], "code": "# Reference: https://www.kaggle.com/code/robikscube/exploring-molecular-properties-data\n\n# Downsample to speed up plot time.\nsns.pairplot(data=scc.sample(5000), hue='type', vars=['fc','sd','pso','dso','scalar_coupling_constant'])\nplt.show()", "processed": ["plot beauti shame data test set"]}, {"markdown": ["When we look at the target `scalar_coupling_constant` in relation to the `atom_count` - there visually appears to be a relationship. We notice the gap in coupling constant values, between ~25 and ~75. It is rare to see a value within this range. Could this be a good case for a classification problem between the two clusters?"], "code": "# Reference: https://www.kaggle.com/code/robikscube/exploring-molecular-properties-data\n\ntrain_df.sample(1000).plot(x='atom_count',\n                           y='scalar_coupling_constant',\n                           kind='scatter',\n                           color=color_pal[0],\n                           figsize=(20, 5),\n                           alpha=0.5)\nplt.show()", "processed": ["look target scalar coupl constant relat atom count visual appear relationship notic gap coupl constant valu 25 75 rare see valu within rang could good case classif problem two cluster"]}, {"markdown": ["# Super Simple Baseline Model [1.239 Public LB]\nThe simplest thing we can do as a model is predict that the target is the **average** value that we observe for that **type** in the training set!"], "code": "# Reference: https://www.kaggle.com/code/robikscube/exploring-molecular-properties-data\n\ntrain_df.groupby('type')['scalar_coupling_constant'].mean().plot(kind='barh',\n                                                                 figsize=(15, 5),\n                                                                title='Average Scalar Coupling Constant by Type')\nplt.show()\n# THIS IS A MODEL!!! This is a model??\ntype_mean_dict = train_df.groupby('type')['scalar_coupling_constant'].mean().to_dict()\ntest_df['scalar_coupling_constant'] = test_df['type'].map(type_mean_dict)\ntest_df[['id','scalar_coupling_constant']].to_csv('super_simple_submission.csv', index=False)", "processed": ["super simpl baselin model 1 239 public lb simplest thing model predict target averag valu observ type train set"]}, {"markdown": ["# Feature Creation\nThis feature was found from `@inversion` 's kernel here: https://www.kaggle.com/inversion/atomic-distance-benchmark/output\nThe code was then made faster by `@seriousran` here: https://www.kaggle.com/seriousran/just-speed-up-calculate-distance-from-benchmark"], "code": "# Reference: https://www.kaggle.com/code/robikscube/exploring-molecular-properties-data\n\ndef map_atom_info(df, atom_idx):\n    df = pd.merge(df, structures, how = 'left',\n                  left_on  = ['molecule_name', f'atom_index_{atom_idx}'],\n                  right_on = ['molecule_name',  'atom_index'])\n    \n    df = df.drop('atom_index', axis=1)\n    df = df.rename(columns={'atom': f'atom_{atom_idx}',\n                            'x': f'x_{atom_idx}',\n                            'y': f'y_{atom_idx}',\n                            'z': f'z_{atom_idx}'})\n    return df\n\ntrain_df = map_atom_info(train_df, 0)\ntrain_df = map_atom_info(train_df, 1)\n\ntest_df = map_atom_info(test_df, 0)\ntest_df = map_atom_info(test_df, 1)\n# https://www.kaggle.com/seriousran/just-speed-up-calculate-distance-from-benchmark\ntrain_p_0 = train_df[['x_0', 'y_0', 'z_0']].values\ntrain_p_1 = train_df[['x_1', 'y_1', 'z_1']].values\ntest_p_0 = test_df[['x_0', 'y_0', 'z_0']].values\ntest_p_1 = test_df[['x_1', 'y_1', 'z_1']].values\n\ntrain_df['dist'] = np.linalg.norm(train_p_0 - train_p_1, axis=1)\ntest_df['dist'] = np.linalg.norm(test_p_0 - test_p_1, axis=1)\n# make categorical variables\natom_map = {'H': 0,\n            'C': 1,\n            'N': 2}\ntrain_df['atom_0_cat'] = train_df['atom_0'].map(atom_map).astype('int')\ntrain_df['atom_1_cat'] = train_df['atom_1'].map(atom_map).astype('int')\ntest_df['atom_0_cat'] = test_df['atom_0'].map(atom_map).astype('int')\ntest_df['atom_1_cat'] = test_df['atom_1'].map(atom_map).astype('int')\n# One Hot Encode the Type\ntrain_df = pd.concat([train_df, pd.get_dummies(train_df['type'])], axis=1)\ntest_df = pd.concat([test_df, pd.get_dummies(test_df['type'])], axis=1)\ncolor_index = 0\naxes_index = 0\nfig, axes = plt.subplots(8, 1, figsize=(20, 20), sharex=True)\nfor mtype, d in train_df.groupby('type'):\n    d['dist'].plot(kind='hist',\n                  bins=1000,\n                  title='Distribution of Distance Feature for {}'.format(mtype),\n                  color=color_pal[color_index],\n                  ax=axes[axes_index])\n    if color_index == 6:\n        color_index = 0\n    else:\n        color_index += 1\n    axes_index += 1\nplt.show()", "processed": ["featur creation featur found invers kernel http www kaggl com invers atom distanc benchmark output code made faster seriousran http www kaggl com seriousran speed calcul distanc benchmark"]}, {"markdown": ["## Save LGB Results, OOF, and Feature Importance\nIt's always a good idea to save your OOF, predictions and feature importances. You never know when they will come in handy in the future.\nI like to save the Number of folds and CV score in the filename."], "code": "# Reference: https://www.kaggle.com/code/robikscube/exploring-molecular-properties-data\n\nif RUN_LGB:\n    # Save Prediction and name appropriately\n    submission_csv_name = 'submission_lgb_{}folds_{}CV.csv'.format(n_fold, np.mean(scores))\n    oof_csv_name = 'oof_lgb_{}folds_{}CV.csv'.format(n_fold, np.mean(scores))\n    fi_csv_name = 'fi_lgb_{}folds_{}CV.csv'.format(n_fold, np.mean(scores))\n\n    print('Saving LGB Submission as:')\n    print(submission_csv_name)\n    ss = pd.read_csv('../input/sample_submission.csv')\n    ss['scalar_coupling_constant'] = prediction\n    ss.to_csv(submission_csv_name, index=False)\n    ss.head()\n    # OOF\n    oof_df = train_df[['id','molecule_name','scalar_coupling_constant']].copy()\n    oof_df['oof_pred'] = oof\n    oof_df.to_csv(oof_csv_name, index=False)\n    # Feature Importance\n    feature_importance.to_csv(fi_csv_name, index=False)\nif RUN_LGB:\n    # Plot feature importance as done in https://www.kaggle.com/artgor/artgor-utils\n    feature_importance[\"importance\"] /= folds.n_splits\n    cols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n        by=\"importance\", ascending=False)[:50].index\n\n    best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n\n    plt.figure(figsize=(15, 20));\n    ax = sns.barplot(x=\"importance\",\n                y=\"feature\",\n                hue='fold',\n                data=best_features.sort_values(by=\"importance\", ascending=False));\n    plt.title('LGB Features (avg over folds)');", "processed": ["save lgb result oof featur import alway good idea save oof predict featur import never know come handi futur like save number fold cv score filenam"]}, {"markdown": ["# Catboost"], "code": "# Reference: https://www.kaggle.com/code/robikscube/exploring-molecular-properties-data\n\nfrom catboost import Pool, cv\n\nRUN_CATBOOST_CV = False\n\nif RUN_CATBOOST_CV:\n    labels = train_df['scalar_coupling_constant'].values\n    cat_features = ['type','atom_count','atom_0','atom_1']\n    cv_data = train_df[['type','atom_count','atom_0','atom_1',\n                        'x_0','y_0','z_0','x_1','y_1','z_1','dist']]\n    cv_dataset = Pool(data=cv_data,\n                      label=labels,\n                      cat_features=cat_features)\n\n    ITERATIONS = 100000\n    params = {\"iterations\": ITERATIONS,\n              \"learning_rate\" : 0.02,\n              \"depth\": 7,\n              \"loss_function\": \"MAE\",\n              \"verbose\": False,\n              \"task_type\" : \"GPU\"}\n\n    scores = cv(cv_dataset,\n                params,\n                fold_count=5, \n                plot=\"True\")\n    \n    scores['iterations'] = scores['iterations'].astype('int')\n    scores.set_index('iterations')[['test-MAE-mean','train-MAE-mean']].plot(figsize=(15, 5), title='CV (MAE) Score by iteration (5 Folds)')\nfrom catboost import CatBoostRegressor, Pool\n\nITERATIONS = 200000\n\nFEATURES = [#'atom_index_0',\n            'atom_index_1',\n            'atom_0',\n            'x_0', 'y_0', 'z_0',\n            'atom_1', \n            'x_1', 'y_1', 'z_1',\n            'dist', 'dist_to_type_mean',\n            'atom_count',\n            'type']\nTARGET = 'scalar_coupling_constant'\nCAT_FEATS = ['atom_0','atom_1','type']\n\ntrain_dataset = Pool(data=train_df[FEATURES],\n                  label=train_df['scalar_coupling_constant'].values,\n                  cat_features=CAT_FEATS)\n\ncb_model = CatBoostRegressor(iterations=ITERATIONS,\n                             learning_rate=0.2,\n                             depth=7,\n                             eval_metric='MAE',\n                             random_seed = 529,\n                             task_type=\"GPU\")\n\n# Fit the model\ncb_model.fit(train_dataset, verbose=1000)\n\n# Predict\ntest_data = test_df[FEATURES]\n\ntest_dataset = Pool(data=test_data,\n                    cat_features=CAT_FEATS)\n\nss = pd.read_csv('../input/sample_submission.csv')\nss['scalar_coupling_constant'] = cb_model.predict(test_dataset)\nss.to_csv('basline_catboost_submission.csv', index=False)", "processed": ["catboost"]}, {"markdown": ["## 4. Evaluation"], "code": "# Reference: https://www.kaggle.com/code/xhlulu/cnn-baseline-iwildcam-2019\n\nhistory_df = pd.DataFrame(hist.history)\nhistory_df['val_f1'] = f1_metrics.val_f1s\nhistory_df['val_precision'] = f1_metrics.val_precisions\nhistory_df['val_recall'] = f1_metrics.val_recalls\n\nhistory_df[['loss', 'val_loss']].plot()\nhistory_df[['acc', 'val_acc']].plot()\nhistory_df[['val_f1', 'val_precision', 'val_recall']].plot()", "processed": ["4 evalu"]}, {"markdown": ["There are a total of 664996 entries in this file.  \n\n### 3.1 cells.value for single event\n\nLets observe the cells.value column for a single event. "], "code": "# Reference: https://www.kaggle.com/code/shivamb/trajectory-animation-eda\n\ndef dist(df, col, bins, color, title, kde=False):\n    plt.figure(figsize=(15,3))\n    sns.distplot(df[col].values, bins=bins, color=color, kde=kde)\n    plt.title(title, fontsize=14);\n    plt.show();\n\ndef mdist(df, col, bins, color, kde=False):\n    f, axes = plt.subplots(1, 2, figsize=(15,3))\n    sns.distplot(df[1][col].values, bins=bins, color=color, rug=False, ax=axes[0], kde=kde)\n    sns.distplot(df[2][col].values, bins=bins, color=color, rug=False, ax=axes[1], kde=kde)\n\n    f, axes = plt.subplots(1, 2, figsize=(15,3))\n    sns.distplot(df[3][col].values, bins=bins, color=color, rug=False, ax=axes[0], kde=kde)\n    sns.distplot(df[4][col].values, bins=bins, color=color, rug=False, ax=axes[1], kde=kde)\n\ndist(cells_df[0], 'value', 10, 'red', 'cells.value')", "processed": ["total 664996 entri file 3 1 cell valu singl event let observ cell valu column singl event"]}, {"markdown": ["### 3.4 mean of \"value\" for ch0 and ch1"], "code": "# Reference: https://www.kaggle.com/code/shivamb/trajectory-animation-eda\n\nch0df = cells_df[0].groupby('ch0').agg({'value' : 'mean'}).reset_index()\nch1df = cells_df[0].groupby('ch1').agg({'value' : 'mean'}).reset_index()\n\nf, axes = plt.subplots(2, 1, figsize=(15,10))\nsns.regplot(x='ch0', y='value', data=ch0df, fit_reg=False, color='#ff4c64', ax=axes[0])\nsns.regplot(x='ch1', y='value', data=ch1df, fit_reg=False, color='#89ea7c', ax=axes[1])\naxes[0].spines['right'].set_visible(False)\naxes[0].spines['top'].set_visible(False)\naxes[1].spines['right'].set_visible(False)\naxes[1].spines['top'].set_visible(False)", "processed": ["3 4 mean valu ch0 ch1"]}, {"markdown": ["\n### 4.1 values of x,y,z for hit in global coordinates"], "code": "# Reference: https://www.kaggle.com/code/shivamb/trajectory-animation-eda\n\n# f, axes = plt.subplots(1, 2, figsize=(15,5));\n# sns.distplot(hits_df[0].x.values, color='red', rug=False, ax=axes[0])\n# sns.distplot(hits_df[0].y.values, color='red', rug=False, ax=axes[1])\n# axes[0].set_title(\"distribution of x coordinate of particles\");\n# axes[1].set_title(\"distribution of y coordinate of particles\");\n\n# f, axes = plt.subplots(1, 2, figsize=(15,5));\n# sns.distplot(hits_df[0].z.values, color='red', rug=False, ax=axes[0])\n# sns.regplot(x=hits_df[0][:2000].x.values, y=hits_df[0][:2000].y.values, fit_reg=False, color='#ff4c64', ax=axes[1])\n# axes[0].set_title(\"distribution of z coordinate of particles\");\n# axes[1].set_title(\"plotting of x and y of particles\");\n# axes[1].set(xlabel='x', ylabel='y');\n\nhits_small = hits_df[0][['x','y','z']]\nsns.pairplot(hits_small, palette='husl', size=6)\nplt.show()", "processed": ["4 1 valu x z hit global coordin"]}, {"markdown": ["### 5.3 Plotting the initial x,y,z coordinates of particles and initial momentum values for the particles"], "code": "# Reference: https://www.kaggle.com/code/shivamb/trajectory-animation-eda\n\nf, axes = plt.subplots(1, 2, figsize=(15,3));\nsns.distplot(particles_df[0].vx.values, color='green', rug=False, ax=axes[0])\nsns.distplot(particles_df[0].px.values, color='green', rug=False, ax=axes[1])\naxes[0].set_title(\"x coordinate of particles\");\naxes[1].set_title(\"momentum of particle in x direction \");\n\nf, axes = plt.subplots(1, 2, figsize=(15,3));\nsns.distplot(particles_df[0].vy.values, color='green', rug=False, ax=axes[0])\nsns.distplot(particles_df[0].py.values, color='green', rug=False, ax=axes[1])\naxes[0].set_title(\"y coordinate of particles\");\naxes[1].set_title(\"momentum of particle in y direction \");\n\nf, axes = plt.subplots(1, 2, figsize=(15,3));\nsns.distplot(particles_df[0].vz.values, color='green', rug=False, ax=axes[0])\nsns.distplot(particles_df[0].pz.values, color='green', rug=False, ax=axes[1])\naxes[0].set_title(\"z coordinate of particles\");\naxes[1].set_title(\"momentum of particle in z direction \");\n\nf, axes = plt.subplots(1, 2, figsize=(15,5));\nsns.regplot(x=particles_df[0][:12000].vx.values, y=particles_df[0][:12000].vy.values, fit_reg=False, color='#ff4c64', ax=axes[0])\nsns.regplot(x=particles_df[0][:12000].vx.values, y=particles_df[0][:12000].vz.values, fit_reg=False, color='#ff4c64', ax=axes[1])\naxes[0].set_title(\"x and y position of particles\");\naxes[0].set(xlabel='x', ylabel='y');\naxes[1].set_title(\"x and z position of particles\");\naxes[1].set(xlabel='x', ylabel='z');", "processed": ["5 3 plot initi x z coordin particl initi momentum valu particl"]}, {"markdown": ["### 5.6 Pair plotting of initial positions of the particles"], "code": "# Reference: https://www.kaggle.com/code/shivamb/trajectory-animation-eda\n\npsmall = particles_df[0][['vx','vy','vz']]\nsns.pairplot(psmall, palette='husl', size=6)\nplt.show()", "processed": ["5 6 pair plot initi posit particl"]}, {"markdown": ["## Labels"], "code": "# Reference: https://www.kaggle.com/code/jesucristo/imet2020-visualization\n\nlabels_df.sample(10).head(10)\nlabels_df[\"attribute_type\"] = labels_df.attribute_name.apply(lambda x: x.split(\"::\")[0])\nprint(labels_df[\"attribute_type\"].value_counts())\nsns.countplot(labels_df.attribute_type)\nlabels_df.attribute_id.nunique()\n1920 + 768 + 681 + 100 + 5 # number of attributes = N_CLASSES\nlabels_df.attribute_type.unique()\nlabels_df[labels_df.attribute_type == \"tags\"]\n# https://www.kaggle.com/ttahara/eda-compare-number-of-culture-and-tag-attributes\ntrain_attr_ohot = np.zeros((len(train_df), len(labels_df)), dtype=int)\n\nfor idx, attr_arr in enumerate(train_df.attribute_ids.str.split(\" \").apply(lambda l: list(map(int, l))).values):\n    train_attr_ohot[idx, attr_arr] = 1\n    \nnames_arr = labels_df.attribute_name.values\ntrain_df[\"attribute_names\"] = [\", \".join(names_arr[arr == 1]) for arr in train_attr_ohot]\n\ntrain_df[\"attr_num\"] = train_attr_ohot.sum(axis=1)\ntrain_df[\"culture_attr_num\"] = train_attr_ohot[:, :398].sum(axis=1)\ntrain_df[\"tag_attr_num\"] = train_attr_ohot[:, 398:].sum(axis=1)\n# https://www.kaggle.com/ttahara/eda-compare-number-of-culture-and-tag-attributes\nfig = plt.figure(figsize=(5 * 5, 5 * 6))\nfig.subplots_adjust(wspace=0.5, hspace=0.5)\nfor i, (art_id, attr_names) in enumerate(train_df.sort_values(by=\"culture_attr_num\", ascending=False)[[\"id\", \"attribute_names\"]].values[:15]):\n    ax = fig.add_subplot(5, 3, i // 3 * 3 + i % 3 + 1)\n    im = Image.open(\"/kaggle/input/imet-2020-fgvc7/train/{}.png\".format(art_id))\n    ax.imshow(im)\n    im.close()\n    attr_split = attr_names.split(\", \")\n    attr_culture = list(map(lambda x: x.split(\"::\")[-1], filter(lambda x: x[:7] == \"culture\", attr_split)))\n    attr_tag = list(map(lambda x: x.split(\"::\")[-1], filter(lambda x: x[:3] == \"tag\", attr_split)))\n    ax.set_title(\"art id: {}\\nculture: {}\\ntag: {}\".format(art_id, attr_culture, attr_tag))\n# https://www.kaggle.com/ttahara/eda-compare-number-of-culture-and-tag-attributes\nfig = plt.figure(figsize=(5 * 6, 5 * 5))\nfig.subplots_adjust(wspace=0.6, hspace=0.6)\nfor i, (art_id, attr_names) in enumerate(train_df.sort_values(by=\"tag_attr_num\", ascending=False)[[\"id\", \"attribute_names\"]].values[:12]):\n    ax = fig.add_subplot(4, 3, i // 3 * 3 + i % 3 + 1)\n    im = Image.open(\"/kaggle/input/imet-2020-fgvc7/train/{}.png\".format(art_id))\n    ax.imshow(im)\n    im.close()\n    attr_split = attr_names.split(\", \")\n    attr_culture = list(map(lambda x: x.split(\"::\")[-1], filter(lambda x: x[:6] == \"culture\", attr_split)))\n    attr_tag = list(map(lambda x: x.split(\"::\")[-1], filter(lambda x: x[:3] == \"tag\", attr_split)))\n    ax.set_title(\"art id: {}\\nculture: {}\\ntag: {}\".format(art_id, attr_culture, attr_tag))\n# https://www.kaggle.com/ttahara/eda-compare-number-of-culture-and-tag-attributes\nfig = plt.figure(figsize=(5 * 8, 5 * 7))\nfig.subplots_adjust(wspace=0.6, hspace=0.6)\nfor i, (art_id, attr_names) in enumerate(train_df[train_df.tag_attr_num == 1][[\"id\", \"attribute_names\"]].values[:49]):\n    ax = fig.add_subplot(7, 7, i // 7 * 7 + i % 7 + 1)\n    im = Image.open(\"/kaggle/input/imet-2020-fgvc7/train/{}.png\".format(art_id))\n    ax.imshow(im)\n    im.close()\n    attr_split = attr_names.split(\", \")\n    attr_culture = list(map(lambda x: x.split(\"::\")[-1], filter(lambda x: x[:7] == \"culture\", attr_split)))\n    attr_tag = list(map(lambda x: x.split(\"::\")[-1], filter(lambda x: x[:3] == \"tag\", attr_split)))\n    ax.set_title(\"art id: {}\\nculture: {}\\ntag: {}\".format(art_id, attr_culture, attr_tag))\nfig = plt.figure(figsize=(5 * 8, 5 * 7))\nfig.subplots_adjust(wspace=0.6, hspace=0.6)\nfor i, (art_id, attr_names) in enumerate(train_df[train_df.tag_attr_num == 2][[\"id\", \"attribute_names\"]].values[:49]):\n    ax = fig.add_subplot(7, 7, i // 7 * 7 + i % 7 + 1)\n    im = Image.open(\"/kaggle/input/imet-2020-fgvc7/train/{}.png\".format(art_id))\n    ax.imshow(im)\n    im.close()\n    attr_split = attr_names.split(\", \")\n    attr_culture = list(map(lambda x: x.split(\"::\")[-1], filter(lambda x: x[:7] == \"culture\", attr_split)))\n    attr_tag = list(map(lambda x: x.split(\"::\")[-1], filter(lambda x: x[:3] == \"tag\", attr_split)))\n    ax.set_title(\"art id: {}\\nculture: {}\\ntag: {}\".format(art_id, attr_culture, attr_tag))\nfig = plt.figure(figsize=(5 * 8, 5 * 7))\nfig.subplots_adjust(wspace=0.6, hspace=0.6)\nfor i, (art_id, attr_names) in enumerate(train_df[train_df.tag_attr_num == 3][[\"id\", \"attribute_names\"]].values[:49]):\n    ax = fig.add_subplot(7, 7, i // 7 * 7 + i % 7 + 1)\n    im = Image.open(\"/kaggle/input/imet-2020-fgvc7/train/{}.png\".format(art_id))\n    ax.imshow(im)\n    im.close()\n    attr_split = attr_names.split(\", \")\n    attr_culture = list(map(lambda x: x.split(\"::\")[-1], filter(lambda x: x[:7] == \"culture\", attr_split)))\n    attr_tag = list(map(lambda x: x.split(\"::\")[-1], filter(lambda x: x[:3] == \"tag\", attr_split)))\n    ax.set_title(\"art id: {}\\nculture: {}\\ntag: {}\".format(art_id, attr_culture, attr_tag))", "processed": ["label"]}, {"markdown": ["# Description of Data\nThe training data is recordings in time. At each 10,000th of a second, the strength of the signal was recorded and the number of ion channels open was recorded. It is our task to build a model that predicts the number of open channels from signal at each time step. Furthermore we are told that the data was recorded in batches of 50 seconds. Therefore each 500,000 rows is one batch. The training data contains 10 batches and the test data contains 4 batches. Let's display the number of open channels and signal strength together for each training batch."], "code": "# Reference: https://www.kaggle.com/code/cdeotte/one-feature-model-0-930\n\nplt.figure(figsize=(20,5)); res = 1000\nplt.plot(range(0,train.shape[0],res),train.signal[0::res])\nfor i in range(11): plt.plot([i*500000,i*500000],[-5,12.5],'r')\nfor j in range(10): plt.text(j*500000+200000,10,str(j+1),size=20)\nplt.xlabel('Row',size=16); plt.ylabel('Signal',size=16); \nplt.title('Training Data Signal - 10 batches',size=20)\nplt.show()\nplt.figure(figsize=(20,5)); res = 1000\nplt.plot(range(0,train.shape[0],res),train.open_channels[0::res])\nfor i in range(11): plt.plot([i*500000,i*500000],[-5,12.5],'r')\nfor j in range(10): plt.text(j*500000+200000,10,str(j+1),size=20)\nplt.xlabel('Row',size=16); plt.ylabel('Channels Open',size=16); \nplt.title('Training Data Open Channels - 10 batches',size=20)\nplt.show()", "processed": ["descript data train data record time 10 000th second strength signal record number ion channel open record task build model predict number open channel signal time step furthermor told data record batch 50 second therefor 500 000 row one batch train data contain 10 batch test data contain 4 batch let display number open channel signal strength togeth train batch"]}, {"markdown": ["## Reflection\nFrom the plots above, it looks like they used 5 different synthetic models. One model produced maximum 1 channel open with low probability (batches 1 and 2). One model produced maximum 1 channel open with high probability (batches 3 and 7). One model produced maximum 3 channels open (batches 4 and 8). One model produced maximum 5 channels open (batches 6 and 9) and one model produced maximum 10 channels open (batches 7 and 10). Furthermore drift was added to batches 7, 8, 9, 10. And the beginning of batch 2.\n\nAccording to the paper [here][1], the data is synthesized. Also \"electrophysiological\" noise and drift were added. Drift is a signal bias causing the signal to no longer be a horizontal line like batches 2, 7, 8, 9, 10 above.\n\n> Data description and dataset construction. Ion channel dwell-times were\nsimulated using the method of Gillespie 43 from published single channel models.\nChannels are assumed to follow a stochastic Markovian process and transition\nfrom one state to the next simulated by randomly sampling from a lifetime\nprobability distribution calculated for each state. Authentic \u201celectrophysiological\u201d\nnoise was added to these events by passing the signal through a patch-clamp\namplifier and recording it back to file with CED\u2019s Signal software via an Axon\nelectronic \u201cmodel cell\u201d. In some datasets additional drift was applied to the final\ndata with Matlab. Two different stochastic gating models, (termed M1 and M2)\nwere used to generate semi-synthetic ion channel data. M1 is a low open probability model from ref. 41 (Fig. 3a, b), typically no more than one ion channel opens\nsimultaneously. Model M2 is from refs. 42,44 and has a much higher open probability (Fig. 3c, d), consequently up to five channels opened simultaneously and there are few instances of zero channels open.\n\n\n[1]: https://www.nature.com/articles/s42003-019-0729-3\n", "# Correlation Between Signal and Open Channels\nLet's look closely at random intervals of signal and open channels to observe how they relate. We notice that they are highly correlated and move up and down together. Therefore we can probabily predict open channels from the one feature signal. The only complication is the synthetic drift that was added. So we will remove it."], "code": "# Reference: https://www.kaggle.com/code/cdeotte/one-feature-model-0-930\n\nfor k in range(10):\n    a = int( np.random.uniform(0,train.shape[0]-50000) )\n    b=a+5000; res=10\n    print('#'*25)\n    print('### Random %i to %i'%(a,b))\n    print('#'*25)\n    plt.figure(figsize=(20,5))\n    plt.plot(range(a,b,res),train.signal[a:b][0::res])\n    plt.plot(range(a,b,res),train.open_channels[a:b][0::res])\n    plt.show()", "processed": ["reflect plot look like use 5 differ synthet model one model produc maximum 1 channel open low probabl batch 1 2 one model produc maximum 1 channel open high probabl batch 3 7 one model produc maximum 3 channel open batch 4 8 one model produc maximum 5 channel open batch 6 9 one model produc maximum 10 channel open batch 7 10 furthermor drift ad batch 7 8 9 10 begin batch 2 accord paper 1 data synthes also electrophysiolog nois drift ad drift signal bia caus signal longer horizont line like batch 2 7 8 9 10 data descript dataset construct ion channel dwell time simul use method gillespi 43 publish singl channel model channel assum follow stochast markovian process transit one state next simul randomli sampl lifetim probabl distribut calcul state authent electrophysiolog nois ad event pas signal patch clamp amplifi record back file ced signal softwar via axon electron model cell dataset addit drift appli final data matlab two differ stochast gate model term m1 m2 use gener semi synthet ion channel data m1 low open probabl model ref 41 fig 3a b typic one ion channel open simultan model m2 ref 42 44 much higher open probabl fig 3c consequ five channel open simultan instanc zero channel open 1 http www natur com articl s42003 019 0729 3", "correl signal open channel let look close random interv signal open channel observ relat notic highli correl move togeth therefor probabili predict open channel one featur signal complic synthet drift ad remov"]}, {"markdown": ["# Test Data\nLet's display the test data signal"], "code": "# Reference: https://www.kaggle.com/code/cdeotte/one-feature-model-0-930\n\nplt.figure(figsize=(20,5))\nres = 1000; let = ['A','B','C','D','E','F','G','H','I','J']\nplt.plot(range(0,test.shape[0],res),test.signal[0::res])\nfor i in range(5): plt.plot([i*500000,i*500000],[-5,12.5],'r')\nfor j in range(21): plt.plot([j*100000,j*100000],[-5,12.5],'r:')\nfor k in range(4): plt.text(k*500000+200000,10,str(k+1),size=20)\nfor k in range(10): plt.text(k*100000+40000,7,let[k],size=16)\nplt.xlabel('Row',size=16); plt.ylabel('Channels Open',size=16); \nplt.title('Test Data Signal - 4 batches - 10 subsamples',size=20)\nplt.show()", "processed": ["test data let display test data signal"]}, {"markdown": ["## Reflection\nFrom this plot we can locate the 5 models in action. And we can recognize the added drift. Batch 1 appears to be 5 subsamples where A, B, C, D, E were created by models 1s, 3, 5, 1s, 1f respectively. Model 1s is the model with maximum 1 channel open with low prob. Model 1f is the model with maximum 1 channel open with high prob. And models 3, 5, 10 are models with maximum 3, 5, 10 channels respectively. We observe slant drift in subsamples A, B, E, G, H, I. We observe parabola draft in batch 3. ", "# Remove Training Data Drift\nThis is a demonstration to show slant drift removal. We could also remove the parabolic drift in batches 7, 8, 9, 10 if we wanted. Below we will only train our models with batches 1, 3, 4, 5, 6. But after removing training drift, we can include the data from batches 2, 7, 8, 9, 10 in our training if we want."], "code": "# Reference: https://www.kaggle.com/code/cdeotte/one-feature-model-0-930\n\ntrain2 = train.copy()\n\na=500000; b=600000 # CLEAN TRAIN BATCH 2\ntrain2.loc[train.index[a:b],'signal'] = train2.signal[a:b].values - 3*(train2.time.values[a:b] - 50)/10.\nbatch=2; a=500000*(batch-1); b=500000*batch; res=50\nplt.figure(figsize=(20,5))\nplt.plot(range(a,b,res),train.signal[a:b][0::res])\nplt.title('Training Batch 2 with Slant Drift',size=16)\nplt.figure(figsize=(20,5))\nplt.plot(range(a,b,res),train2.signal[a:b][0::res])\nplt.title('Training Batch 2 without Slant Drift',size=16)\nplt.show()\ndef f(x,low,high,mid): return -((-low+high)/625)*(x-mid)**2+high -low\n\n# CLEAN TRAIN BATCH 7\nbatch = 7; a = 500000*(batch-1); b = 500000*batch\ntrain2.loc[train2.index[a:b],'signal'] = train.signal.values[a:b] - f(train.time[a:b].values,-1.817,3.186,325)\n# CLEAN TRAIN BATCH 8\nbatch = 8; a = 500000*(batch-1); b = 500000*batch\ntrain2.loc[train2.index[a:b],'signal'] = train.signal.values[a:b] - f(train.time[a:b].values,-0.094,4.936,375)\n# CLEAN TRAIN BATCH 9\nbatch = 9; a = 500000*(batch-1); b = 500000*batch\ntrain2.loc[train2.index[a:b],'signal'] = train.signal.values[a:b] - f(train.time[a:b].values,1.715,6.689,425)\n# CLEAN TRAIN BATCH 10\nbatch = 10; a = 500000*(batch-1); b = 500000*batch\ntrain2.loc[train2.index[a:b],'signal'] = train.signal.values[a:b] - f(train.time[a:b].values,3.361,8.45,475)\nplt.figure(figsize=(20,5))\nplt.plot(train.time[::1000],train.signal[::1000])\nplt.title('Training Batches 7-10 with Parabolic Drift',size=16)\nplt.figure(figsize=(20,5))\nplt.plot(train2.time[::1000],train2.signal[::1000])\nplt.title('Training Batches 7-10 without Parabolic Drift',size=16)\nplt.show()", "processed": ["reflect plot locat 5 model action recogn ad drift batch 1 appear 5 subsampl b c e creat model 1 3 5 1 1f respect model 1 model maximum 1 channel open low prob model 1f model maximum 1 channel open high prob model 3 5 10 model maximum 3 5 10 channel respect observ slant drift subsampl b e g h observ parabola draft batch 3", "remov train data drift demonstr show slant drift remov could also remov parabol drift batch 7 8 9 10 want train model batch 1 3 4 5 6 remov train drift includ data batch 2 7 8 9 10 train want"]}, {"markdown": ["# Analyze Test Data Drift\nLet's plot the drift in the training and test data", "## Training Data Drift\nWe observe drift whereever the following plot is not a horizontal line. We see drift in batches 2, 7, 8, 9, 10."], "code": "# Reference: https://www.kaggle.com/code/cdeotte/one-feature-model-0-930\n\n# ORIGINAL TRAIN DATA\nplt.figure(figsize=(20,5))\nr = train.signal.rolling(30000).mean()\nplt.plot(train.time.values,r)\nfor i in range(11): plt.plot([i*50,i*50],[-3,8],'r:')\nfor j in range(10): plt.text(j*50+20,6,str(j+1),size=20)\nplt.title('Training Signal Rolling Mean. Has Drift wherever plot is not horizontal line',size=16)\nplt.show()\n\n# TRAIN DATA WITHOUT DRIFT\nplt.figure(figsize=(20,5))\nr = train2.signal.rolling(30000).mean()\nplt.plot(train2.time.values,r)\nfor i in range(11): plt.plot([i*50,i*50],[-3,8],'r:')\nfor j in range(10): plt.text(j*50+20,6,str(j+1),size=20)\nplt.title('Training Signal Rolling Mean without Drift',size=16)\nplt.show()", "processed": ["analyz test data drift let plot drift train test data", "train data drift observ drift whereev follow plot horizont line see drift batch 2 7 8 9 10"]}, {"markdown": ["## Test Data Drift\nWe observe drift in test subsamples A, B, E, G, H, I and test batch 3.\n"], "code": "# Reference: https://www.kaggle.com/code/cdeotte/one-feature-model-0-930\n\nplt.figure(figsize=(20,5))\nlet = ['A','B','C','D','E','F','G','H','I','J']\nr = test.signal.rolling(30000).mean()\nplt.plot(test.time.values,r)\nfor i in range(21): plt.plot([500+i*10,500+i*10],[-3,6],'r:')\nfor i in range(5): plt.plot([500+i*50,500+i*50],[-3,6],'r')\nfor k in range(4): plt.text(525+k*50,5.5,str(k+1),size=20)\nfor k in range(10): plt.text(505+k*10,4,let[k],size=16)\nplt.title('Test Signal Rolling Mean. Has Drift wherever plot is not horizontal line',size=16)\nplt.show()", "processed": ["test data drift observ drift test subsampl b e g h test batch 3"]}, {"markdown": ["# Remove Test Data Drift"], "code": "# Reference: https://www.kaggle.com/code/cdeotte/one-feature-model-0-930\n\ntest2 = test.copy()\n# REMOVE BATCH 1 DRIFT\nstart=500\na = 0; b = 100000\ntest2.loc[test2.index[a:b],'signal'] = test2.signal.values[a:b] - 3*(test2.time.values[a:b]-start)/10.\nstart=510\na = 100000; b = 200000\ntest2.loc[test2.index[a:b],'signal'] = test2.signal.values[a:b] - 3*(test2.time.values[a:b]-start)/10.\nstart=540\na = 400000; b = 500000\ntest2.loc[test2.index[a:b],'signal'] = test2.signal.values[a:b] - 3*(test2.time.values[a:b]-start)/10.\n# REMOVE BATCH 2 DRIFT\nstart=560\na = 600000; b = 700000\ntest2.loc[test2.index[a:b],'signal'] = test2.signal.values[a:b] - 3*(test2.time.values[a:b]-start)/10.\nstart=570\na = 700000; b = 800000\ntest2.loc[test2.index[a:b],'signal'] = test2.signal.values[a:b] - 3*(test2.time.values[a:b]-start)/10.\nstart=580\na = 800000; b = 900000\ntest2.loc[test2.index[a:b],'signal'] = test2.signal.values[a:b] - 3*(test2.time.values[a:b]-start)/10.\n# REMOVE BATCH 3 DRIFT\ndef f(x):\n    return -(0.00788)*(x-625)**2+2.345 +2.58\na = 1000000; b = 1500000\ntest2.loc[test2.index[a:b],'signal'] = test2.signal.values[a:b] - f(test2.time[a:b].values)\nplt.figure(figsize=(20,5))\nres = 1000\nplt.plot(range(0,test2.shape[0],res),test2.signal[0::res])\nfor i in range(5): plt.plot([i*500000,i*500000],[-5,12.5],'r')\nfor i in range(21): plt.plot([i*100000,i*100000],[-5,12.5],'r:')\nfor k in range(4): plt.text(k*500000+250000,10,str(k+1),size=20)\nfor k in range(10): plt.text(k*100000+40000,7.5,let[k],size=16)\nplt.title('Test Signal without Drift',size=16)\nplt.show()\n\nplt.figure(figsize=(20,5))\nr = test2.signal.rolling(30000).mean()\nplt.plot(test2.time.values,r)\nfor i in range(21): plt.plot([500+i*10,500+i*10],[-2,6],'r:')\nfor i in range(5): plt.plot([500+i*50,500+i*50],[-2,6],'r')\nfor k in range(4): plt.text(525+k*50,5.5,str(k+1),size=20)\nfor k in range(10): plt.text(505+k*10,4,let[k],size=16)\nplt.title('Test Signal Rolling Mean without Drift',size=16)\nplt.show()", "processed": ["remov test data drift"]}, {"markdown": ["**Interactive Visualisations with Plotly**\n\nLet us take a look at the underlying data by plotting the daily oil prices in a time series plot via the interactive Python visualisation library Plot.ly as follows. Here we invoke the Plot.ly scatter plot function by calling \"Scatter\" and it is a simple matter of providing the date range in the x-axis and the corresponding daily oil prices in the y-axis ( as an aside I have also simultaneously dropped nulls by calling dropna( ) in the oil dataframe). I've hidden the Plot.ly code as it can get quite long so unhide it if you want to see the syntax."], "code": "# Reference: https://www.kaggle.com/code/arthurtok/comprehensive-python-and-d3-js-favorita-analytics\n\n# trace = go.Scatter(\n#     name='Oil prices',\n#     x=oil['date'],\n#     y=oil['dcoilwtico'].dropna(),\n#     mode='lines',\n#     line=dict(color='rgb(20, 15, 200, 0.8)'),\n#     #fillcolor='rgba(68, 68, 68, 0.3)',\n#     fillcolor='rgba(0, 0, 216, 0.3)',\n#     fill='tonexty' )\n\n# data = [trace]\n\n# layout = go.Layout(\n#     yaxis=dict(title='Daily Oil price'),\n#     title='Daily oil prices from Jan 2013 till July 2017',\n#     showlegend = False)\n# fig = go.Figure(data=data, layout=layout)\n# py.iplot(fig, filename='pandas-time-series-error-bars')", "processed": ["interact visualis plotli let u take look underli data plot daili oil price time seri plot via interact python visualis librari plot ly follow invok plot ly scatter plot function call scatter simpl matter provid date rang x axi correspond daili oil price axi asid also simultan drop null call dropna oil datafram hidden plot ly code get quit long unhid want see syntax"]}, {"markdown": ["**Treemap plots of store attributes**\n\nTreemap plots inspired from I, Coder's kernel: [Novice to Grandmaster](https://www.kaggle.com/ash316/novice-to-grandmaster)\n"], "code": "# Reference: https://www.kaggle.com/code/arthurtok/comprehensive-python-and-d3-js-favorita-analytics\n\nfig = plt.figure(figsize=(25, 21))\nmarrimeko=stores.city.value_counts().to_frame()\nax = fig.add_subplot(111, aspect=\"equal\")\nax = squarify.plot(sizes=marrimeko['city'].values,label=marrimeko.index,\n              color=sns.color_palette('cubehelix_r', 28), alpha=1)\nax.set_xticks([])\nax.set_yticks([])\nfig=plt.gcf()\nfig.set_size_inches(40,25)\nplt.title(\"Treemap of store counts across different cities\", fontsize=18)\nplt.show();\nfig = plt.figure(figsize=(25, 21))\nmarrimeko=stores.state.value_counts().to_frame()\nax = fig.add_subplot(111, aspect=\"equal\")\nax = squarify.plot(sizes=marrimeko['state'].values,label=marrimeko.index,\n              color=sns.color_palette('viridis_r', 28), alpha=1)\nax.set_xticks([])\nax.set_yticks([])\nfig=plt.gcf()\nfig.set_size_inches(40,25)\nplt.title(\"Treemap of store counts across different States\", fontsize=18)\nplt.show()\nstores.state.unique()", "processed": ["treemap plot store attribut treemap plot inspir coder kernel novic grandmast http www kaggl com ash316 novic grandmast"]}, {"markdown": ["**Inspecting the allocation of clusters to store numbers**\n\nWe can now generate will be that of our store numbers ordered against their respective store clusters so that we can observe if there are any apparent trends or relationships in the data. To do so, I will take our stores Python dataframe and group it based on the columns \"store_nbr\" and \"cluster\" via the **groupby** and pivot statement. After which, I will unstack the grouping which means that I will pivot on the level of store_nbr index labels, returning a DataFrame having a new level of columns which are the store clusters whose inner-most level relate to the pivoted store_nbr index labels. This technique is commonly used for producing stacked barplots in Python but since we only have unique store_nbr numbers, therefore we will simply get barplots of store numbers ordered by their relevant clusters."], "code": "# Reference: https://www.kaggle.com/code/arthurtok/comprehensive-python-and-d3-js-favorita-analytics\n\n# Unhide to see the sorted zip order\nneworder = [23, 24, 26, 36, 41, 15, 29, 31, 32, 34, 39, \n            53, 4, 37, 40, 43, 8, 10, 19, 20, 33, 38, 13, \n            21, 2, 6, 7, 3, 22, 25, 27, 28, 30, 35, 42, 44, \n            48, 51, 16, 0, 1, 5, 52, 45, 46, 47, 49, 9, 11, 12, 14, 18, 17, 50]\n# Finally plot the seaborn heatmap\nplt.style.use('dark_background')\nplt.figure(figsize=(15,12))\nstore_pivot = stores.dropna().pivot(\"store_nbr\",\"cluster\", \"store_nbr\")\nax = sns.heatmap(store_pivot, cmap='jet', annot=True, linewidths=0, linecolor='white')\nplt.title('Store numbers and the clusters they are assigned to')\n# plt.style.use('dark_background')\n# nbr_cluster = stores.groupby(['store_nbr','cluster']).size()\n# nbr_cluster.unstack().iloc[neworder].plot(kind='bar',stacked=True, colormap= 'tab20', figsize=(13,11),  grid=False)\n# plt.title('Store numbers and the clusters they are assigned to', fontsize=14)\n# plt.ylabel('')\n# plt.xlabel('Store number')\n# plt.show()", "processed": ["inspect alloc cluster store number gener store number order respect store cluster observ appar trend relationship data take store python datafram group base column store nbr cluster via groupbi pivot statement unstack group mean pivot level store nbr index label return datafram new level column store cluster whose inner level relat pivot store nbr index label techniqu commonli use produc stack barplot python sinc uniqu store nbr number therefor simpli get barplot store number order relev cluster"]}, {"markdown": ["**Takeaways from thse plot**\n\nFrom visualising the store numbers side-by-side based on the clustering, we can identify certain patterns. For example clusters 3, 6, 10 and 15 are the most common store clusters based off the fact that there are more store_nbrs attributed to them then the others while on the other end of the spectrum, we have clusters 5 and 17 which are only related to the stores 44 and 51 respectively.", "**Stacked Barplots of Types against clusters**\n\nHere it might be informative to look at the distribution of clusters based on the store type to see if we can identify any apparent relationship between types and the way the company has decided to cluster the particular store. Again we apply the groupby operation but this time on type and on cluster. This time when we pivot based off this grouped operation, we are able to get counts of each distinct cluster distributed and stacked on top of other clusters per store type as follows:"], "code": "# Reference: https://www.kaggle.com/code/arthurtok/comprehensive-python-and-d3-js-favorita-analytics\n\nplt.style.use('seaborn-white')\n#plt.style.use('dark_background')\ntype_cluster = stores.groupby(['type','cluster']).size()\ntype_cluster.unstack().plot(kind='bar',stacked=True, colormap= 'PuBu', figsize=(13,11),  grid=False)\nplt.title('Stacked Barplot of Store types and their cluster distribution', fontsize=18)\nplt.ylabel('Count of clusters in a particular store type', fontsize=16)\nplt.xlabel('Store type', fontsize=16)\nplt.show()", "processed": ["takeaway thse plot visualis store number side side base cluster identifi certain pattern exampl cluster 3 6 10 15 common store cluster base fact store nbr attribut other end spectrum cluster 5 17 relat store 44 51 respect", "stack barplot type cluster might inform look distribut cluster base store type see identifi appar relationship type way compani decid cluster particular store appli groupbi oper time type cluster time pivot base group oper abl get count distinct cluster distribut stack top cluster per store type follow"]}, {"markdown": ["**Takeaway from the plots**\n\nMost of the store types seem to contain a mix of the clusters, especially with regards to store type \"D\". Only type \"E\" stores seem to fall within the single cluster of cluster 10. However with regards to our initial plan of trying to suss out relationships between store types and clusters, it seems that there is nothing apparent that stands out. If we think of store types, one would normally think of categories such as convenience store types, huge general store types or bulk buy types.", "**Stacked barplot of types of stores across the different cities**\n\nAnother interesting distribution to observe would be the types of stores that Corporacion Favorita has decided to open for each city in Ecuador as well as the absolute number of stores for that city. All these three things can be achieved by turning to our usual groupby operation and pivoting again by unstacking and plotting as follows:"], "code": "# Reference: https://www.kaggle.com/code/arthurtok/comprehensive-python-and-d3-js-favorita-analytics\n\n# plt.style.use('dark_background')\nplt.style.use('seaborn-white')\ncity_cluster = stores.groupby(['city','type']).store_nbr.size()\ncity_cluster.unstack().plot(kind='bar',stacked=True, colormap= 'viridis', figsize=(13,11),  grid=False)\nplt.title('Stacked Barplot of Store types opened for each city')\nplt.ylabel('Count of stores for a particular city')\nplt.show()", "processed": ["takeaway plot store type seem contain mix cluster especi regard store type type e store seem fall within singl cluster cluster 10 howev regard initi plan tri sus relationship store type cluster seem noth appar stand think store type one would normal think categori conveni store type huge gener store type bulk buy type", "stack barplot type store across differ citi anoth interest distribut observ would type store corporacion favorita decid open citi ecuador well absolut number store citi three thing achiev turn usual groupbi oper pivot unstack plot follow"]}, {"markdown": ["**Takeaways from the plot**: \n\nAs observed from the stacked barplots, there are two cities that standout in terms of the variety of store types on offer - Guayaquil and Quito. These should come as no surprise as [Quito](https://en.wikipedia.org/wiki/Quito) is the capital city of Ecuador while [Guayaquil](https://en.wikipedia.org/wiki/Guayaquil) is the largest and most populous city in Ecuador. Therefore one would think it logical to expect Corporacion Favorita to target these major cities with the most diverse store types probably to capture different ends of the market (if we think store types as being high-end/premium/wholesale/discount etc) as well as opening up the highest number of stores evinced from the largest counts of store_nbrs attributed to those two cities.", "## 2c. Holiday Events data\n\nTrudging on, we can inspect the \"holiday_events.csv\" file which contains data on the national, regional and local level of Ecuador. According to the data dictionary, we should pay special attention to the \"transferred\" column. \n\n*A holiday that is transferred officially falls on that calendar day, but was moved to another date by the government. A transferred day is more like a normal day than a holiday. To find the day that it was actually celebrated, look for the corresponding row where type is Transfer. For example, the holiday Independencia de Guayaquil was transferred from 2012-10-09 to 2012-10-12, which means it was celebrated on 2012-10-12. Days that are type Bridge are extra days that are added to a holiday (e.g., to extend the break across a long weekend). These are frequently made up by the type Work Day which is a day not normally scheduled for work (e.g., Saturday) that is meant to payback the Bridge.*"], "code": "# Reference: https://www.kaggle.com/code/arthurtok/comprehensive-python-and-d3-js-favorita-analytics\n\nholiday_events.head(3)\nplt.style.use('seaborn-white')\n# plt.style.use('dark_background')\nholiday_local_type = holiday_events.groupby(['locale_name', 'type']).size()\nholiday_local_type.unstack().plot(kind='bar',stacked=True, colormap= 'magma_r', figsize=(12,10),  grid=False)\nplt.title('Stacked Barplot of locale name against event type')\nplt.ylabel('Count of entries')\nplt.show()", "processed": ["takeaway plot observ stack barplot two citi standout term varieti store type offer guayaquil quito come surpris quito http en wikipedia org wiki quito capit citi ecuador guayaquil http en wikipedia org wiki guayaquil largest popul citi ecuador therefor one would think logic expect corporacion favorita target major citi diver store type probabl captur differ end market think store type high end premium wholesal discount etc well open highest number store evinc largest count store nbr attribut two citi", "2c holiday event data trudg inspect holiday event csv file contain data nation region local level ecuador accord data dictionari pay special attent transfer column holiday transfer offici fall calendar day move anoth date govern transfer day like normal day holiday find day actual celebr look correspond row type transfer exampl holiday independencia de guayaquil transfer 2012 10 09 2012 10 12 mean celebr 2012 10 12 day type bridg extra day ad holiday e g extend break across long weekend frequent made type work day day normal schedul work e g saturday meant payback bridg"]}, {"markdown": ["## 2d. Transactions data\n", "\n**PERIODICITY IN TRANSACTION PATTERN**\n\nInspecting the transactions file, we must be aware that the transactional data is only included for the training timeframe according to the competition's data dictionary. Furthermore the columns in this file relate only to the count of sales transactions for "], "code": "# Reference: https://www.kaggle.com/code/arthurtok/comprehensive-python-and-d3-js-favorita-analytics\n\nprint(transactions.head(3))\nprint(\"=\"*60)\nprint(\"There are {0} rows and {1} columns in the transactions data\".\n      format(transactions.shape[0], transactions.shape[1]))\nplt.style.use('seaborn-white')\nplt.figure(figsize=(13,11))\nplt.plot(transactions.date.values, transactions.transactions.values, color='darkblue')\nplt.axvline(x='2015-12-23',color='red',alpha=0.3)\nplt.axvline(x='2016-12-23',color='red',alpha=0.3)\nplt.axvline(x='2014-12-23',color='red',alpha=0.3)\nplt.axvline(x='2013-12-23',color='red',alpha=0.3)\nplt.axvline(x='2013-05-12',color='green',alpha=0.2, linestyle= '--')\nplt.axvline(x='2015-05-10',color='green',alpha=0.2, linestyle= '--')\nplt.axvline(x='2016-05-08',color='green',alpha=0.2, linestyle= '--')\nplt.axvline(x='2014-05-11',color='green',alpha=0.2, linestyle= '--')\nplt.axvline(x='2017-05-14',color='green',alpha=0.2, linestyle= '--')\nplt.ylim(-50, 10000)\nplt.title(\"Distribution of transactions per day from 2013 till 2017\")\nplt.ylabel('transactions per day', fontsize= 16)\nplt.xlabel('Date', fontsize= 16)\nplt.show()", "processed": ["2d transact data", "period transact pattern inspect transact file must awar transact data includ train timefram accord competit data dictionari furthermor column file relat count sale transact"]}, {"markdown": ["**Takeaway from the plots**\n\nInterestingly when plotting transactions on a year-to-year basis, we can already pick out two different periodic spikes in transactions from the data (one in the solid Red line and the other the dotted Green line). The bigger yearly periodic spike in transactions seem to occur at the end of the year in December, specifically on the 23 December every year-end. Perhaps this is due to some sort of Christmas sale/discount that Corporacion Favorita holds every December thereby explaining the bump in transactions on this date. The weaker periodic spike given by the green dotted lines seems to occur around the middle of the months of May , specifically on the sunday in the second week of May. Perhaps another similar scheme is being implemented by Corporacion Favorita on those sundays.", "## 2e. Items data\n\nMoving onto the \"items.csv\" file, we can see that there are not many columns in the data. There is a unique item number identifier \"item_nbr\" which I presume relates to mappings to grocery items such as \"Housebrand tomatoes pack of 6\" or maybe \"4 blueberry muffins\" etc. The family column should be pretty self-explanatory in the sense that this relates to parent category that that particular item relates to."], "code": "# Reference: https://www.kaggle.com/code/arthurtok/comprehensive-python-and-d3-js-favorita-analytics\n\nitems.head()\nx, y = (list(x) for x in zip(*sorted(zip(items.family.value_counts().index, \n                                         items.family.value_counts().values), \n                                        reverse = False)))\ntrace2 = go.Bar(\n    y=items.family.value_counts().values,\n    x=items.family.value_counts().index,\n    marker=dict(\n        color=items.family.value_counts().values,\n        colorscale = 'Portland',\n        reversescale = False\n    ),\n    orientation='v',\n)\n\nlayout = dict(\n    title='Counts of items per family category',\n     width = 800, height = 800,\n    yaxis=dict(\n        showgrid=False,\n        showline=False,\n        showticklabels=True,\n#         domain=[0, 0.85],\n    ))\n\nfig1 = go.Figure(data=[trace2])\nfig1['layout'].update(layout)\npy.iplot(fig1, filename='plots')", "processed": ["takeaway plot interestingli plot transact year year basi alreadi pick two differ period spike transact data one solid red line dot green line bigger yearli period spike transact seem occur end year decemb specif 23 decemb everi year end perhap due sort christma sale discount corporacion favorita hold everi decemb therebi explain bump transact date weaker period spike given green dot line seem occur around middl month may specif sunday second week may perhap anoth similar scheme implement corporacion favorita sunday", "2e item data move onto item csv file see mani column data uniqu item number identifi item nbr presum relat map groceri item housebrand tomato pack 6 mayb 4 blueberri muffin etc famili column pretti self explanatori sen relat parent categori particular item relat"]}, {"markdown": ["**Takeaways from the plot**\n\nAs we can see from this barplot, the y-axis shows the counts of items while the x-axis displays the different family categories that the various retail items fall under, sorted from largest number of item counts to smallest number of items. As we can see from the plot, the top 3 family categories are the GROCERY I, BEVERAGES and CLEANING categories. "], "code": "# Reference: https://www.kaggle.com/code/arthurtok/comprehensive-python-and-d3-js-favorita-analytics\n\nx, y = (list(x) for x in zip(*sorted(zip(items['class'].value_counts().index, \n                                         items['class'].value_counts().values), \n                                        reverse = False)))\ntrace2 = go.Bar(\n    x=items['class'].value_counts().index,\n    y=items['class'].value_counts().values,\n    marker=dict(\n        color=items['class'].value_counts().values,\n        colorscale = 'Jet',\n        reversescale = True\n    ),\n    orientation='v',\n)\n\nlayout = dict(\n    title='Number of items attributed to a particular item class',\n     width = 800, height = 1400,\n    yaxis=dict(\n        showgrid=False,\n        showline=False,\n        showticklabels=True,\n#         domain=[0, 0.85],\n    ))\n\nfig1 = go.Figure(data=[trace2])\nfig1['layout'].update(layout)\npy.iplot(fig1, filename='plots')", "processed": ["takeaway plot see barplot axi show count item x axi display differ famili categori variou retail item fall sort largest number item count smallest number item see plot top 3 famili categori groceri beverag clean categori"]}, {"markdown": ["**Takeaways from the plot**\n\nPlotting the count of "], "code": "# Reference: https://www.kaggle.com/code/arthurtok/comprehensive-python-and-d3-js-favorita-analytics\n\nitems.head()\nplt.style.use('seaborn-white')\nfam_perishable = items.groupby(['family', 'perishable']).size()\nfam_perishable.unstack().plot(kind='bar',stacked=True, colormap= 'coolwarm', figsize=(12,10),  grid=False)\nplt.title('Stacked Barplot of locale name against event type')\nplt.ylabel('Count of entries')\nplt.show()", "processed": ["takeaway plot plot count"]}, {"markdown": ["**TOP-100**"], "code": "# Reference: https://www.kaggle.com/code/jesucristo/kuzushiji-recognition-complete-guide\n\nsns.set(style=\"whitegrid\")\nplt.figure(figsize=(22,20))\nax = sns.barplot(y=\"char\", x=\"count\", data=chars.sort_values(by=['count'], ascending=False).head(100))\nax.set_title(\"Character frequency in images (top 100)\")\nplt.show()", "processed": ["top 100"]}, {"markdown": ["So there are 8 folders present inside the train folder, one for each species.\n\nNow let us check the number of files present in each of these sub folders. "], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-notebook-v2-0\n\nsub_folders = check_output([\"ls\", \"../input/train/\"]).decode(\"utf8\").strip().split('\\n')\ncount_dict = {}\nfor sub_folder in sub_folders:\n    num_of_files = len(check_output([\"ls\", \"../input/train/\"+sub_folder]).decode(\"utf8\").strip().split('\\n'))\n    print(\"Number of files for the species\",sub_folder,\":\",num_of_files)\n    count_dict[sub_folder] = num_of_files\n    \nplt.figure(figsize=(12,4))\nsns.barplot(list(count_dict.keys()), list(count_dict.values()), alpha=0.8)\nplt.xlabel('Fish Species', fontsize=12)\nplt.ylabel('Number of Images', fontsize=12)\nplt.show()\n    \n    ", "processed": ["8 folder present insid train folder one speci let u check number file present sub folder"]}, {"markdown": ["**Image Size:**\n\nNow let us look at the image size of each of the files and see what different sizes are available."], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-notebook-v2-0\n\ntrain_path = \"../input/train/\"\nsub_folders = check_output([\"ls\", train_path]).decode(\"utf8\").strip().split('\\n')\ndifferent_file_sizes = {}\nfor sub_folder in sub_folders:\n    file_names = check_output([\"ls\", train_path+sub_folder]).decode(\"utf8\").strip().split('\\n')\n    for file_name in file_names:\n        im_array = imread(train_path+sub_folder+\"/\"+file_name)\n        size = \"_\".join(map(str,list(im_array.shape)))\n        different_file_sizes[size] = different_file_sizes.get(size,0) + 1\n\nplt.figure(figsize=(12,4))\nsns.barplot(list(different_file_sizes.keys()), list(different_file_sizes.values()), alpha=0.8)\nplt.xlabel('Image size', fontsize=12)\nplt.ylabel('Number of Images', fontsize=12)\nplt.title(\"Image size present in train dataset\")\nplt.xticks(rotation='vertical')\nplt.show()", "processed": ["imag size let u look imag size file see differ size avail"]}, {"markdown": ["So 720_1280_3 is the most common image size available in the train data and 10 different sizes are available. \n\n720_1244_3 is the smallest size of the available images in train set and 974_1732_3 is the largest one.\n\nNow let us look at the distribution in test dataset as well."], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-notebook-v2-0\n\ntest_path = \"../input/test_stg1/\"\nfile_names = check_output([\"ls\", test_path]).decode(\"utf8\").strip().split('\\n')\ndifferent_file_sizes = {}\nfor file_name in file_names:\n        size = \"_\".join(map(str,list(imread(test_path+file_name).shape)))\n        different_file_sizes[size] = different_file_sizes.get(size,0) + 1\n\nplt.figure(figsize=(12,4))\nsns.barplot(list(different_file_sizes.keys()), list(different_file_sizes.values()), alpha=0.8)\nplt.xlabel('File size', fontsize=12)\nplt.ylabel('Number of Images', fontsize=12)\nplt.xticks(rotation='vertical')\nplt.title(\"Image size present in test dataset\")\nplt.show()", "processed": ["720 1280 3 common imag size avail train data 10 differ size avail 720 1244 3 smallest size avail imag train set 974 1732 3 largest one let u look distribut test dataset well"]}, {"markdown": ["### Unique values"], "code": "# Reference: https://www.kaggle.com/code/jesucristo/quick-visualization-eda\n\nfor i in md.columns:\n    print (\">> \",i,\"\\t\", md[i].unique())\nfor col in ['cell_type', 'dataset', 'experiment', 'plate',  'site', 'well_type']:\n    print (col)\n    print (md[col].value_counts())\n    sns.countplot(y = col,\n              data = md,\n              order = md[col].value_counts().index)\n    plt.show()\n    ", "processed": ["uniqu valu"]}, {"markdown": ["### sirna distribution"], "code": "# Reference: https://www.kaggle.com/code/jesucristo/quick-visualization-eda\n\ntrain_df = md[md['dataset'] == 'train']\ntest_df = md[md['dataset'] == 'test']\n\ntrain_df.shape, test_df.shape\nplt.figure(figsize=(16,6))\nplt.title(\"Distribution of SIRNA in the train and test set\")\nsns.distplot(train_df.sirna,color=\"green\", kde=True,bins='auto', label='train')\nsns.distplot(test_df.sirna,color=\"blue\", kde=True, bins='auto', label='test')\nplt.legend()\nplt.show()", "processed": ["sirna distribut"]}, {"markdown": ["Remember, 0s were NaNs"], "code": "# Reference: https://www.kaggle.com/code/jesucristo/quick-visualization-eda\n\nfeat1 = 'sirna'\nfig = plt.subplots(figsize=(15, 5))\n\n# train\nplt.subplot(1, 2, 1)\nsns.kdeplot(train_df[feat1][train_df['site'] == 1], shade=False, color=\"b\", label = 'site 1')\nsns.kdeplot(train_df[feat1][train_df['site'] == 2], shade=False, color=\"r\", label = 'site 2')\nplt.title(feat1)\nplt.xlabel('Feature Values')\nplt.ylabel('Probability')\n\n# test\nplt.subplot(1, 2, 2)\nsns.kdeplot(test_df[feat1][test_df['site'] == 1], shade=False, color=\"b\", label = 'site 1')\nsns.kdeplot(test_df[feat1][test_df['site'] == 2], shade=False, color=\"r\", label = 'site 2')\nplt.title(feat1)\nplt.xlabel('Feature Values')\nplt.ylabel('Probability')\nplt.show()\n\n# Prevent: Output path '/rxrx1-utils/.git/logs/refs/remotes/origin/HEAD' contains too many nested subdirectories (max 6)\n!rm -r  rxrx1-utils\n!ls", "processed": ["rememb 0 nan"]}, {"markdown": ["# Data Preprocessing\nhttps://www.kaggle.com/osciiart/covid19-lightgbm"], "code": "# Reference: https://www.kaggle.com/code/robikscube/rob-s-covid19-week-2-submission-less-aggressive\n\n# Read in data\ntrain = pd.read_csv(\"../input/covid19-global-forecasting-week-2/train.csv\")\ntest = pd.read_csv(\"../input/covid19-global-forecasting-week-2/test.csv\")\n\ntt = pd.concat([train, test], sort=False)\ntt = train.merge(test, on=['Province_State','Country_Region','Date'], how='outer')\n\n# concat Country/Region and Province/State\ndef name_place(x):\n    try:\n        x_new = x['Country_Region'] + \"_\" + x['Province_State']\n    except:\n        x_new = x['Country_Region']\n    return x_new\ntt['Place'] = tt.apply(lambda x: name_place(x), axis=1)\n# tt = tt.drop(['Province_State','Country_Region'], axis=1)\ntt['Date'] = pd.to_datetime(tt['Date'])\ntt['doy'] = tt['Date'].dt.dayofyear\ntt['dow'] = tt['Date'].dt.dayofweek\ntt['hasProvidence'] = ~tt['Province_State'].isna()\n\n\ncountry_meta = pd.read_csv('../input/covid19-forecasting-metadata/region_metadata.csv')\ntt = tt.merge(country_meta, how='left')\n\ncountry_date_meta = pd.read_csv('../input/covid19-forecasting-metadata/region_date_metadata.csv')\n#tt = tt.merge(country_meta, how='left')\n\ntt['HasFatality'] = tt.groupby('Place')['Fatalities'].transform(lambda x: x.max() > 0)\ntt['HasCases'] = tt.groupby('Place')['ConfirmedCases'].transform(lambda x: x.max() > 0)\n\nfirst_case_date = tt.query('ConfirmedCases >= 1').groupby('Place')['Date'].min().to_dict()\nten_case_date = tt.query('ConfirmedCases >= 10').groupby('Place')['Date'].min().to_dict()\nhundred_case_date = tt.query('ConfirmedCases >= 100').groupby('Place')['Date'].min().to_dict()\nfirst_fatal_date = tt.query('Fatalities >= 1').groupby('Place')['Date'].min().to_dict()\nten_fatal_date = tt.query('Fatalities >= 10').groupby('Place')['Date'].min().to_dict()\nhundred_fatal_date = tt.query('Fatalities >= 100').groupby('Place')['Date'].min().to_dict()\n\ntt['First_Case_Date'] = tt['Place'].map(first_case_date)\ntt['Ten_Case_Date'] = tt['Place'].map(ten_case_date)\ntt['Hundred_Case_Date'] = tt['Place'].map(hundred_case_date)\ntt['First_Fatal_Date'] = tt['Place'].map(first_fatal_date)\ntt['Ten_Fatal_Date'] = tt['Place'].map(ten_fatal_date)\ntt['Hundred_Fatal_Date'] = tt['Place'].map(hundred_fatal_date)\n\ntt['Days_Since_First_Case'] = (tt['Date'] - tt['First_Case_Date']).dt.days\ntt['Days_Since_Ten_Cases'] = (tt['Date'] - tt['Ten_Case_Date']).dt.days\ntt['Days_Since_Hundred_Cases'] = (tt['Date'] - tt['Hundred_Case_Date']).dt.days\ntt['Days_Since_First_Fatal'] = (tt['Date'] - tt['First_Fatal_Date']).dt.days\ntt['Days_Since_Ten_Fatal'] = (tt['Date'] - tt['Ten_Fatal_Date']).dt.days\ntt['Days_Since_Hundred_Fatal'] = (tt['Date'] - tt['Hundred_Fatal_Date']).dt.days\n\n# Merge smoking data\nsmoking = pd.read_csv(\"../input/smokingstats/share-of-adults-who-smoke.csv\")\nsmoking = smoking.rename(columns={'Smoking prevalence, total (ages 15+) (% of adults)': 'Smoking_Rate'})\nsmoking_dict = smoking.groupby('Entity')['Year'].max().to_dict()\nsmoking['LastYear'] = smoking['Entity'].map(smoking_dict)\nsmoking = smoking.query('Year == LastYear').reset_index()\nsmoking['Entity'] = smoking['Entity'].str.replace('United States', 'US')\n\ntt = tt.merge(smoking[['Entity','Smoking_Rate']],\n         left_on='Country_Region',\n         right_on='Entity',\n         how='left',\n         validate='m:1') \\\n    .drop('Entity', axis=1)\n\n# Country data\ncountry_info = pd.read_csv('../input/countryinfo/covid19countryinfo.csv')\n\n\ntt = tt.merge(country_info, left_on=['Country_Region','Province_State'],\n              right_on=['country','region'],\n              how='left',\n              validate='m:1')\n\n# State info from wikipedia\nus_state_info = pd.read_html('https://simple.wikipedia.org/wiki/List_of_U.S._states_by_population')[0] \\\n    [['State','Population estimate, July 1, 2019[2]']] \\\n    .rename(columns={'Population estimate, July 1, 2019[2]' : 'Population'})\n#us_state_info['2019 population'] = pd.to_numeric(us_state_info['2019 population'].str.replace('[note 1]','').replace('[]',''))\n\ntt = tt.merge(us_state_info[['State','Population']],\n         left_on='Province_State',\n         right_on='State',\n         how='left')\n\ntt['pop'] = pd.to_numeric(tt['pop'].str.replace(',',''))\ntt['pop'] = tt['pop'].fillna(tt['Population'])\ntt['pop'] = pd.to_numeric(tt['pop'])\n\ntt['pop_diff'] = tt['pop'] - tt['Population']\ntt['Population_final'] = tt['Population']\ntt.loc[~tt['hasProvidence'], 'Population_final'] = tt.loc[~tt['hasProvidence']]['pop']\n\ntt['Confirmed_Cases_Diff'] = tt.groupby('Place')['ConfirmedCases'].diff()\ntt['Fatailities_Diff'] = tt.groupby('Place')['Fatalities'].diff()\nmax_date = tt.dropna(subset=['ConfirmedCases'])['Date'].max()\ntt['gdp2019'] = pd.to_numeric(tt['gdp2019'].str.replace(',',''))\n# Correcting population for missing countries\n# Googled their names and copied the numbers here\npop_dict = {'Angola': int(29.78 * 10**6),\n            'Australia_Australian Capital Territory': 423_800,\n            'Australia_New South Wales': int(7.544 * 10**6),\n            'Australia_Northern Territory': 244_300,\n            'Australia_Queensland' : int(5.071 * 10**6),\n            'Australia_South Australia' : int(1.677 * 10**6),\n            'Australia_Tasmania': 515_000,\n            'Australia_Victoria': int(6.359 * 10**6),\n            'Australia_Western Australia': int(2.589 * 10**6),\n            'Brazil': int(209.3 * 10**6),\n            'Canada_Alberta' : int(4.371 * 10**6),\n            'Canada_British Columbia' : int(5.071 * 10**6),\n            'Canada_Manitoba' : int(1.369 * 10**6),\n            'Canada_New Brunswick' : 776_827,\n            'Canada_Newfoundland and Labrador' : 521_542,\n            'Canada_Nova Scotia' : 971_395,\n            'Canada_Ontario' : int(14.57 * 10**6),\n            'Canada_Prince Edward Island' : 156_947,\n            'Canada_Quebec' : int(8.485 * 10**6),\n            'Canada_Saskatchewan': int(1.174 * 10**6),\n            'China_Anhui': int(62 * 10**6),\n            'China_Beijing': int(21.54 * 10**6),\n            'China_Chongqing': int(30.48 * 10**6),\n            'China_Fujian' :  int(38.56 * 10**6),\n            'China_Gansu' : int(25.58 * 10**6),\n            'China_Guangdong' : int(113.46 * 10**6),\n            'China_Guangxi' : int(48.38 * 10**6),\n            'China_Guizhou' : int(34.75 * 10**6),\n            'China_Hainan' : int(9.258 * 10**6),\n            'China_Hebei' : int(74.7 * 10**6),\n            'China_Heilongjiang' : int(38.31 * 10**6),\n            'China_Henan' : int(94 * 10**6),\n            'China_Hong Kong' : int(7.392 * 10**6),\n            'China_Hubei' : int(58.5 * 10**6),\n            'China_Hunan' : int(67.37 * 10**6),\n            'China_Inner Mongolia' :  int(24.71 * 10**6),\n            'China_Jiangsu' : int(80.4 * 10**6),\n            'China_Jiangxi' : int(45.2 * 10**6),\n            'China_Jilin' : int(27.3 * 10**6),\n            'China_Liaoning' : int(43.9 * 10**6),\n            'China_Macau' : 622_567,\n            'China_Ningxia' : int(6.301 * 10**6),\n            'China_Qinghai' : int(5.627 * 10**6),\n            'China_Shaanxi' : int(37.33 * 10**6),\n            'China_Shandong' : int(92.48 * 10**6),\n            'China_Shanghai' : int(24.28 * 10**6),\n            'China_Shanxi' : int(36.5 * 10**6),\n            'China_Sichuan' : int(81.1 * 10**6),\n            'China_Tianjin' : int(15 * 10**6),\n            'China_Tibet' : int(3.18 * 10**6),\n            'China_Xinjiang' : int(21.81 * 10**6),\n            'China_Yunnan' : int(45.97 * 10**6),\n            'China_Zhejiang' : int(57.37 * 10**6),\n            'Denmark_Faroe Islands' : 51_783,\n            'Denmark_Greenland' : 56_171,\n            'France_French Guiana' : 290_691,\n            'France_French Polynesia' : 283_007,\n            'France_Guadeloupe' : 395_700,\n            'France_Martinique' : 376_480,\n            'France_Mayotte' : 270_372,\n            'France_New Caledonia' : 99_926,\n            'France_Reunion' : 859_959,\n            'France_Saint Barthelemy' : 9_131,\n            'France_St Martin' : 32_125,\n            'Netherlands_Aruba' : 105_264,\n            'Netherlands_Curacao' : 161_014,\n            'Netherlands_Sint Maarten' : 41_109,\n            'Papua New Guinea' : int(8.251 * 10**6),\n            'US_Guam' : 164_229,\n            'US_Virgin Islands' : 107_268,\n            'United Kingdom_Bermuda' : 65_441,\n            'United Kingdom_Cayman Islands' : 61_559,\n            'United Kingdom_Channel Islands' : 170_499,\n            'United Kingdom_Gibraltar' : 34_571,\n            'United Kingdom_Isle of Man' : 84_287,\n            'United Kingdom_Montserrat' : 4_922\n           }\n\ntt['Population_final'] = tt['Population_final'].fillna(tt['Place'].map(pop_dict))\ntt.loc[tt['Place'] == 'Diamond Princess', 'Population final'] = 2_670\ntt['ConfirmedCases_Log'] = tt['ConfirmedCases'].apply(np.log1p)\ntt['Fatalities_Log'] = tt['Fatalities'].apply(np.log1p)\ntt['Population_final'] = tt['Population_final'].astype('int')\ntt['Cases_Per_100kPop'] = (tt['ConfirmedCases'] / tt['Population_final']) * 100000\ntt['Fatalities_Per_100kPop'] = (tt['Fatalities'] / tt['Population_final']) * 100000\n\ntt['Cases_Percent_Pop'] = ((tt['ConfirmedCases'] / tt['Population_final']) * 100)\ntt['Fatalities_Percent_Pop'] = ((tt['Fatalities'] / tt['Population_final']) * 100)\n\ntt['Cases_Log_Percent_Pop'] = ((tt['ConfirmedCases'] / tt['Population_final']) * 100).apply(np.log1p)\ntt['Fatalities_Log_Percent_Pop'] = ((tt['Fatalities'] / tt['Population_final']) * 100).apply(np.log1p)\n\n\ntt['Max_Confirmed_Cases'] = tt.groupby('Place')['ConfirmedCases'].transform(max)\ntt['Max_Fatalities'] = tt.groupby('Place')['Fatalities'].transform(max)\n\ntt['Max_Cases_Per_100kPop'] = tt.groupby('Place')['Cases_Per_100kPop'].transform(max)\ntt['Max_Fatalities_Per_100kPop'] = tt.groupby('Place')['Fatalities_Per_100kPop'].transform(max)\ntt.query('Date == @max_date') \\\n    .query('Place != \"Diamond Princess\"') \\\n    .query('Cases_Log_Percent_Pop > -10000') \\\n    ['Cases_Log_Percent_Pop'].plot(kind='hist', bins=500)\nplt.show()\ntt.query('Days_Since_Ten_Cases > 0') \\\n    .query('Place != \"Diamond Princess\"') \\\n    .dropna(subset=['Cases_Percent_Pop']) \\\n    .query('Days_Since_Ten_Cases < 40') \\\n    .plot(x='Days_Since_Ten_Cases', y='Cases_Log_Percent_Pop', style='.', figsize=(15, 5), alpha=0.2)\nplt.show()\nPLOT = False\nif PLOT:\n    for x in tt['Place'].unique():\n        try:\n            fig, ax = plt.subplots(1, 4, figsize=(15, 2))\n            tt.query('Place == @x') \\\n                .query('ConfirmedCases > 0') \\\n                .set_index('Date')['Cases_Log_Percent_Pop'] \\\n                .plot(title=f'{x} confirmed log pct pop', ax=ax[0])\n            tt.query('Place == @x') \\\n                .query('ConfirmedCases > 0') \\\n                .set_index('Date')['Cases_Percent_Pop'] \\\n                .plot(title=f'{x} confirmed cases', ax=ax[1])\n            tt.query('Place == @x') \\\n                .query('Fatalities > 0') \\\n                .set_index('Date')['Fatalities_Log_Percent_Pop'] \\\n                .plot(title=f'{x} confirmed log pct pop', ax=ax[2])\n            tt.query('Place == @x') \\\n                .query('Fatalities > 0') \\\n                .set_index('Date')['Fatalities_Percent_Pop'] \\\n                .plot(title=f'{x} confirmed cases', ax=ax[3])\n        except:\n            pass\n        plt.show()\ntt.query('Date == @max_date')[['Place','Max_Cases_Per_100kPop',\n                               'Max_Fatalities_Per_100kPop','Max_Confirmed_Cases',\n                               'Population_final',\n                              'Days_Since_First_Case',\n                              'Confirmed_Cases_Diff']] \\\n    .drop_duplicates() \\\n    .sort_values('Max_Cases_Per_100kPop', ascending=False)", "processed": ["data preprocess http www kaggl com osciiart covid19 lightgbm"]}, {"markdown": ["# Features about cases since first case/ fatality"], "code": "# Reference: https://www.kaggle.com/code/robikscube/rob-s-covid19-week-2-submission-less-aggressive\n\ntt['Past_7Days_ConfirmedCases_Std'] = tt['Place'].map(tt.dropna(subset=['ConfirmedCases']).query('Date >= 20200324').groupby('Place')['ConfirmedCases'].std().to_dict())\ntt['Past_7Days_Fatalities_Std'] = tt['Place'].map(tt.dropna(subset=['ConfirmedCases']).query('Date >= 20200324').groupby('Place')['Fatalities'].std().to_dict())\n\ntt['Past_7Days_ConfirmedCases_Min'] = tt['Place'].map(tt.dropna(subset=['ConfirmedCases']).query('Date >= 20200324').groupby('Place')['ConfirmedCases'].min().to_dict())\ntt['Past_7Days_Fatalities_Min'] = tt['Place'].map(tt.dropna(subset=['Fatalities']).query('Date >= 20200324').groupby('Place')['Fatalities'].min().to_dict())\n\ntt['Past_7Days_ConfirmedCases_Max'] = tt['Place'].map(tt.dropna(subset=['ConfirmedCases']).query('Date >= 20200324').groupby('Place')['ConfirmedCases'].max().to_dict())\ntt['Past_7Days_Fatalities_Max'] = tt['Place'].map(tt.dropna(subset=['Fatalities']).query('Date >= 20200324').groupby('Place')['Fatalities'].max().to_dict())\n\ntt['Past_7Days_Confirmed_Change_of_Total'] = (tt['Past_7Days_ConfirmedCases_Max'] - tt['Past_7Days_ConfirmedCases_Min']) / (tt['Past_7Days_ConfirmedCases_Max'])\ntt['Past_7Days_Fatalities_Change_of_Total'] = (tt['Past_7Days_Fatalities_Max'] - tt['Past_7Days_Fatalities_Min']) / (tt['Past_7Days_Fatalities_Max'])\ntt['Past_21Days_ConfirmedCases_Std'] = tt['Place'].map(tt.dropna(subset=['ConfirmedCases']).query('Date >= 20200310').groupby('Place')['ConfirmedCases'].std().to_dict())\ntt['Past_21Days_Fatalities_Std'] = tt['Place'].map(tt.dropna(subset=['ConfirmedCases']).query('Date >= 20200310').groupby('Place')['Fatalities'].std().to_dict())\n\ntt['Past_21Days_ConfirmedCases_Min'] = tt['Place'].map(tt.dropna(subset=['ConfirmedCases']).query('Date >= 20200310').groupby('Place')['ConfirmedCases'].min().to_dict())\ntt['Past_21Days_Fatalities_Min'] = tt['Place'].map(tt.dropna(subset=['Fatalities']).query('Date >= 20200324').groupby('Place')['Fatalities'].min().to_dict())\n\ntt['Past_21Days_ConfirmedCases_Max'] = tt['Place'].map(tt.dropna(subset=['ConfirmedCases']).query('Date >= 20200310').groupby('Place')['ConfirmedCases'].max().to_dict())\ntt['Past_21Days_Fatalities_Max'] = tt['Place'].map(tt.dropna(subset=['Fatalities']).query('Date >= 20200310').groupby('Place')['Fatalities'].max().to_dict())\n\ntt['Past_21Days_Confirmed_Change_of_Total'] = (tt['Past_21Days_ConfirmedCases_Max'] - tt['Past_21Days_ConfirmedCases_Min']) / (tt['Past_21Days_ConfirmedCases_Max'])\ntt['Past_21Days_Fatalities_Change_of_Total'] = (tt['Past_21Days_Fatalities_Max'] - tt['Past_21Days_Fatalities_Min']) / (tt['Past_21Days_Fatalities_Max'])\n\ntt['Past_7Days_Fatalities_Change_of_Total'] = tt['Past_7Days_Fatalities_Change_of_Total'].fillna(0)\ntt['Past_21Days_Fatalities_Change_of_Total'] = tt['Past_21Days_Fatalities_Change_of_Total'].fillna(0)\ntt['Date_7Days_Since_First_Case'] = tt['Place'].map(tt.loc[tt['Days_Since_First_Case'] == 7] \\\n    .set_index('Place')['Date'] \\\n    .to_dict())\ntt['Date_14Days_Since_First_Case'] = tt['Place'].map(tt.loc[tt['Days_Since_First_Case'] == 14] \\\n    .set_index('Place')['Date'] \\\n    .to_dict())\ntt['Date_21Days_Since_First_Case'] = tt['Place'].map(tt.loc[tt['Days_Since_First_Case'] == 21] \\\n    .set_index('Place')['Date'] \\\n    .to_dict())\ntt['Date_28Days_Since_First_Case'] = tt['Place'].map(tt.loc[tt['Days_Since_First_Case'] == 28] \\\n    .set_index('Place')['Date'] \\\n    .to_dict())\ntt['Date_35Days_Since_First_Case'] = tt['Place'].map(tt.loc[tt['Days_Since_First_Case'] == 35] \\\n    .set_index('Place')['Date'] \\\n    .to_dict())\ntt['Date_60Days_Since_First_Case'] = tt['Place'].map(tt.loc[tt['Days_Since_First_Case'] == 60] \\\n    .set_index('Place')['Date'] \\\n    .to_dict())\n\ntt['Date_7Days_Since_Ten_Cases'] = tt['Place'].map(tt.loc[tt['Days_Since_Ten_Cases'] == 7] \\\n    .set_index('Place')['Date'] \\\n    .to_dict())\ntt['Date_14Days_Since_Ten_Cases'] = tt['Place'].map(tt.loc[tt['Days_Since_Ten_Cases'] == 14] \\\n    .set_index('Place')['Date'] \\\n    .to_dict())\ntt['Date_21Days_Since_Ten_Cases'] = tt['Place'].map(tt.loc[tt['Days_Since_Ten_Cases'] == 21] \\\n    .set_index('Place')['Date'] \\\n    .to_dict())\ntt['Date_28Days_Since_Ten_Cases'] = tt['Place'].map(tt.loc[tt['Days_Since_Ten_Cases'] == 28] \\\n    .set_index('Place')['Date'] \\\n    .to_dict())\ntt['Date_35Days_Since_Ten_Cases'] = tt['Place'].map(tt.loc[tt['Days_Since_Ten_Cases'] == 35] \\\n    .set_index('Place')['Date'] \\\n    .to_dict())\ntt['Date_60Days_Since_Ten_Cases'] = tt['Place'].map(tt.loc[tt['Days_Since_Ten_Cases'] == 60] \\\n    .set_index('Place')['Date'] \\\n    .to_dict())\n\n\ntt['Date_7Days_Since_First_Fatal'] = tt['Place'].map(tt.loc[tt['Days_Since_First_Fatal'] == 7] \\\n    .set_index('Place')['Date'] \\\n    .to_dict())\ntt['Date_14Days_Since_First_Fatal'] = tt['Place'].map(tt.loc[tt['Days_Since_First_Fatal'] == 14] \\\n    .set_index('Place')['Date'] \\\n    .to_dict())\ntt['Date_21Days_Since_First_Fatal'] = tt['Place'].map(tt.loc[tt['Days_Since_First_Fatal'] == 21] \\\n    .set_index('Place')['Date'] \\\n    .to_dict())\ntt['Date_28Days_Since_First_Fatal'] = tt['Place'].map(tt.loc[tt['Days_Since_First_Fatal'] == 28] \\\n    .set_index('Place')['Date'] \\\n    .to_dict())\ntt['Date_35Days_Since_First_Fatal'] = tt['Place'].map(tt.loc[tt['Days_Since_First_Fatal'] == 35] \\\n    .set_index('Place')['Date'] \\\n    .to_dict())\ntt['Date_60Days_Since_First_Fatal'] = tt['Place'].map(tt.loc[tt['Days_Since_First_Fatal'] == 60] \\\n    .set_index('Place')['Date'] \\\n    .to_dict())\ntt['CC_7D_1stCase'] = tt.loc[tt['Date_7Days_Since_First_Case'] == tt['Date']]['ConfirmedCases']\ntt['CC_14D_1stCase'] = tt.loc[tt['Date_14Days_Since_First_Case'] == tt['Date']]['ConfirmedCases']\ntt['CC_21D_1stCase'] = tt.loc[tt['Date_21Days_Since_First_Case'] == tt['Date']]['ConfirmedCases']\ntt['CC_28D_1stCase'] = tt.loc[tt['Date_28Days_Since_First_Case'] == tt['Date']]['ConfirmedCases']\ntt['CC_35D_1stCase'] = tt.loc[tt['Date_35Days_Since_First_Case'] == tt['Date']]['ConfirmedCases']\ntt['CC_60D_1stCase'] = tt.loc[tt['Date_60Days_Since_First_Case'] == tt['Date']]['ConfirmedCases']\n\ntt['F_7D_1stCase'] = tt.loc[tt['Date_7Days_Since_First_Case'] == tt['Date']]['Fatalities']\ntt['F_14D_1stCase'] = tt.loc[tt['Date_14Days_Since_First_Case'] == tt['Date']]['Fatalities']\ntt['F_21D_1stCase'] = tt.loc[tt['Date_21Days_Since_First_Case'] == tt['Date']]['Fatalities']\ntt['F_28D_1stCase'] = tt.loc[tt['Date_28Days_Since_First_Case'] == tt['Date']]['Fatalities']\ntt['F_35D_1stCase'] = tt.loc[tt['Date_35Days_Since_First_Case'] == tt['Date']]['Fatalities']\ntt['F_60D_1stCase'] = tt.loc[tt['Date_60Days_Since_First_Case'] == tt['Date']]['Fatalities']\n\ntt['CC_7D_10Case'] = tt.loc[tt['Date_7Days_Since_Ten_Cases'] == tt['Date']]['ConfirmedCases']\ntt['CC_14D_10Case'] = tt.loc[tt['Date_14Days_Since_Ten_Cases'] == tt['Date']]['ConfirmedCases']\ntt['CC_21D_10Case'] = tt.loc[tt['Date_21Days_Since_Ten_Cases'] == tt['Date']]['ConfirmedCases']\ntt['CC_28D_10Case'] = tt.loc[tt['Date_28Days_Since_Ten_Cases'] == tt['Date']]['ConfirmedCases']\ntt['CC_35D_10Case'] = tt.loc[tt['Date_35Days_Since_Ten_Cases'] == tt['Date']]['ConfirmedCases']\ntt['CC_60D_10Case'] = tt.loc[tt['Date_60Days_Since_Ten_Cases'] == tt['Date']]['ConfirmedCases']\n\ntt['F_7D_10Case'] = tt.loc[tt['Date_7Days_Since_Ten_Cases'] == tt['Date']]['Fatalities']\ntt['F_14D_10Case'] = tt.loc[tt['Date_14Days_Since_Ten_Cases'] == tt['Date']]['Fatalities']\ntt['F_21D_10Case'] = tt.loc[tt['Date_21Days_Since_Ten_Cases'] == tt['Date']]['Fatalities']\ntt['F_28D_10Case'] = tt.loc[tt['Date_28Days_Since_Ten_Cases'] == tt['Date']]['Fatalities']\ntt['F_35D_10Case'] = tt.loc[tt['Date_35Days_Since_Ten_Cases'] == tt['Date']]['Fatalities']\ntt['F_60D_10Case'] = tt.loc[tt['Date_60Days_Since_Ten_Cases'] == tt['Date']]['Fatalities']\n\ntt['CC_7D_1Fatal'] = tt.loc[tt['Date_7Days_Since_First_Fatal'] == tt['Date']]['ConfirmedCases']\ntt['CC_14D_1Fatal'] = tt.loc[tt['Date_14Days_Since_First_Fatal'] == tt['Date']]['ConfirmedCases']\ntt['CC_21D_1Fatal'] = tt.loc[tt['Date_21Days_Since_First_Fatal'] == tt['Date']]['ConfirmedCases']\ntt['CC_28D_1Fatal'] = tt.loc[tt['Date_28Days_Since_First_Fatal'] == tt['Date']]['ConfirmedCases']\ntt['CC_35D_1Fatal'] = tt.loc[tt['Date_35Days_Since_First_Fatal'] == tt['Date']]['ConfirmedCases']\ntt['CC_60D_1Fatal'] = tt.loc[tt['Date_60Days_Since_First_Fatal'] == tt['Date']]['ConfirmedCases']\n\ntt['F_7D_1Fatal'] = tt.loc[tt['Date_7Days_Since_First_Fatal'] == tt['Date']]['Fatalities']\ntt['F_14D_1Fatal'] = tt.loc[tt['Date_14Days_Since_First_Fatal'] == tt['Date']]['Fatalities']\ntt['F_21D_1Fatal'] = tt.loc[tt['Date_21Days_Since_First_Fatal'] == tt['Date']]['Fatalities']\ntt['F_28D_1Fatal'] = tt.loc[tt['Date_28Days_Since_First_Fatal'] == tt['Date']]['Fatalities']\ntt['F_35D_1Fatal'] = tt.loc[tt['Date_35Days_Since_First_Fatal'] == tt['Date']]['Fatalities']\ntt['F_60D_1Fatal'] = tt.loc[tt['Date_60Days_Since_First_Fatal'] == tt['Date']]['Fatalities']\nfig, axs = plt.subplots(1, 2, figsize=(15, 5))\ntt[['Place','Past_7Days_Confirmed_Change_of_Total','Past_7Days_Fatalities_Change_of_Total',\n    'Past_7Days_ConfirmedCases_Max','Past_7Days_ConfirmedCases_Min',\n   'Past_7Days_Fatalities_Max','Past_7Days_Fatalities_Min']] \\\n    .drop_duplicates() \\\n    .sort_values('Past_7Days_Confirmed_Change_of_Total')['Past_7Days_Confirmed_Change_of_Total'] \\\n    .plot(kind='hist', bins=50, title='Distribution of Pct change confirmed past 7 days', ax=axs[0])\ntt[['Place','Past_21Days_Confirmed_Change_of_Total','Past_21Days_Fatalities_Change_of_Total',\n    'Past_21Days_ConfirmedCases_Max','Past_21Days_ConfirmedCases_Min',\n   'Past_21Days_Fatalities_Max','Past_21Days_Fatalities_Min']] \\\n    .drop_duplicates() \\\n    .sort_values('Past_21Days_Confirmed_Change_of_Total')['Past_21Days_Confirmed_Change_of_Total'] \\\n    .plot(kind='hist', bins=50, title='Distribution of Pct change confirmed past 21 days', ax=axs[1])\nplt.show()\n\nfig, axs = plt.subplots(1, 2, figsize=(15, 5))\ntt[['Place','Past_7Days_Fatalities_Change_of_Total']] \\\n    .drop_duplicates()['Past_7Days_Fatalities_Change_of_Total'] \\\n    .plot(kind='hist', bins=50, title='Distribution of Pct change confirmed past 7 days', ax=axs[0])\ntt[['Place', 'Past_21Days_Fatalities_Change_of_Total']] \\\n    .drop_duplicates()['Past_21Days_Fatalities_Change_of_Total'] \\\n    .plot(kind='hist', bins=50, title='Distribution of Pct change confirmed past 21 days', ax=axs[1])\nplt.show()\n# Example of flat prop\ntt.query(\"Place == 'China_Chongqing'\").set_index('Date')['ConfirmedCases'].dropna().plot(figsize=(15, 5))\nplt.show()", "processed": ["featur case sinc first case fatal"]}, {"markdown": ["# Other Data Prep"], "code": "# Reference: https://www.kaggle.com/code/robikscube/rob-s-covid19-week-2-submission-less-aggressive\n\n# Example of flat prop\ntt.query(\"Place == 'Italy'\").set_index('Date')[['ConfirmedCases']].dropna().plot(figsize=(15, 5))\nplt.show()\ntt.query(\"Place == 'Italy'\").set_index('Date')[['ConfirmedCases_Log']].dropna().plot(figsize=(15, 5))\nplt.show()\nlatest_summary_stats = tt.query('Date == @max_date') \\\n    [['Country_Region',\n      'Place',\n      'Max_Cases_Per_100kPop',\n      'Max_Fatalities_Per_100kPop',\n      'Max_Confirmed_Cases',\n      'Population_final',\n      'Days_Since_First_Case',\n      'Days_Since_Ten_Cases']] \\\n    .drop_duplicates()\nlatest_summary_stats.query('Place != \"Diamond Princess\"') \\\n    .query('Country_Region != \"China\"') \\\n    .plot(y='Max_Cases_Per_100kPop',\n          x='Days_Since_Ten_Cases',\n          style='.',\n          figsize=(15, 5))\ntt.query('Province_State == \"Maryland\"')[['ConfirmedCases','Confirmed_Cases_Diff']]", "processed": ["data prep"]}, {"markdown": ["# US States"], "code": "# Reference: https://www.kaggle.com/code/robikscube/rob-s-covid19-week-2-submission-less-aggressive\n\nfor myplace in us_states:\n    try:\n        # Confirmed Cases\n        fig, axs = plt.subplots(1, 2, figsize=(15, 3))\n        dat = tt.query('Place == @myplace and Days_Since_Ten_Cases >= 0')[['Days_Since_Ten_Cases','ConfirmedCases_Log']].dropna()\n        X = dat['Days_Since_Ten_Cases']\n        y = dat['ConfirmedCases_Log']\n        y = y.cummax()\n        dat_all = tt.query('Place == @myplace and Days_Since_Ten_Cases >= 0')[['Days_Since_Ten_Cases','ConfirmedCases_Log']]\n        X_pred = dat_all['Days_Since_Ten_Cases']\n        en = ElasticNet()\n        en.fit(X.values.reshape(-1, 1), y.values)\n        preds = en.predict(X_pred.values.reshape(-1, 1))\n        tt.loc[(tt['Place'] == myplace) & (tt['Days_Since_Ten_Cases'] >= 0), 'ConfirmedCases_Log_Pred1'] = preds\n        tt.loc[(tt['Place'] == myplace), 'ConfirmedCases_Pred1'] = tt['ConfirmedCases_Log_Pred1'].apply(np.expm1)\n        # Cap at 10 % Population\n        pop_myplace = tt.query('Place == @myplace')['Population_final'].values[0]\n        tt.loc[(tt['Place'] == myplace) & (tt['ConfirmedCases_Pred1'] > (0.05 * pop_myplace)), 'ConfirmedCases_Pred1'] = (0.05 * pop_myplace)\n        ax = tt.query('Place == @myplace').set_index('Date')[['ConfirmedCases','ConfirmedCases_Pred1']].plot(figsize=(15, 5), title=myplace, ax=axs[0])\n        # Fatalities\n        # If low count then do percent of confirmed:\n        dat = tt.query('Place == @myplace and Days_Since_Ten_Cases >= 0')[['Days_Since_Ten_Cases','Fatalities_Log']].dropna()\n        if len(dat) < 5:\n            tt.loc[(tt['Place'] == myplace), 'Fatalities_Pred1'] = tt.loc[(tt['Place'] == myplace)]['ConfirmedCases_Pred1'] * 0.0001\n        elif tt.query('Place == @myplace')['Fatalities'].max() < 5:\n            tt.loc[(tt['Place'] == myplace), 'Fatalities_Pred1'] = tt.loc[(tt['Place'] == myplace)]['ConfirmedCases_Pred1'] * 0.0001\n        else:\n            X = dat['Days_Since_Ten_Cases']\n            y = dat['Fatalities_Log']\n            y = y.cummax()\n            dat_all = tt.query('Place == @myplace and Days_Since_Ten_Cases >= 0')[['Days_Since_Ten_Cases','Fatalities_Log']]\n            X_pred = dat_all['Days_Since_Ten_Cases']\n            en = ElasticNet()\n            en.fit(X.values.reshape(-1, 1), y.values)\n            preds = en.predict(X_pred.values.reshape(-1, 1))\n            tt.loc[(tt['Place'] == myplace) & (tt['Days_Since_Ten_Cases'] >= 0), 'Fatalities_Log_Pred1'] = preds\n            tt.loc[(tt['Place'] == myplace), 'Fatalities_Pred1'] = tt['Fatalities_Log_Pred1'].apply(np.expm1)\n\n            # Cap at 0.0001 Population\n            pop_myplace = tt.query('Place == @myplace')['Population_final'].values[0]\n            tt.loc[(tt['Place'] == myplace) & (tt['Fatalities_Pred1'] > (0.0001 * pop_myplace)), 'Fatalities_Pred1'] = (0.0001 * pop_myplace)\n\n        ax = tt.query('Place == @myplace').set_index('Date')[['Fatalities','Fatalities_Pred1']].plot(figsize=(15, 5), title=myplace, ax=axs[1])\n        plt.show()\n    except:\n        print(f'============= FAILED FOR {myplace} =============')\n\nmyplace = 'US_Virgin Islands'\ndat = tt.query('Place == @myplace and Days_Since_Ten_Cases >= 0')[['Place','Days_Since_Ten_Cases','ConfirmedCases']].dropna()\ntt.loc[tt['Place'].isin(us_states)].groupby('Place')['Fatalities_Pred1'].max().sum()", "processed": ["u state"]}, {"markdown": ["# Deal with Flattened location"], "code": "# Reference: https://www.kaggle.com/code/robikscube/rob-s-covid19-week-2-submission-less-aggressive\n\nconstant_fatal_places\ntt.loc[tt['Place'].isin(constant_fatal_places), 'ConfirmedCases_Pred1'] = tt.loc[tt['Place'].isin(constant_fatal_places)]['Place'].map(tt.loc[tt['Place'].isin(constant_fatal_places)].groupby('Place')['ConfirmedCases'].max())\ntt.loc[tt['Place'].isin(constant_fatal_places), 'Fatalities_Pred1'] = tt.loc[tt['Place'].isin(constant_fatal_places)]['Place'].map(tt.loc[tt['Place'].isin(constant_fatal_places)].groupby('Place')['Fatalities'].max())\nfor myplace in constant_fatal_places:\n    fig, axs = plt.subplots(1, 2, figsize=(15, 3))\n    ax = tt.query('Place == @myplace').set_index('Date')[['ConfirmedCases','ConfirmedCases_Pred1']].plot(figsize=(15, 5), title=myplace, ax=axs[0])\n    ax = tt.query('Place == @myplace').set_index('Date')[['Fatalities','Fatalities_Pred1']].plot(figsize=(15, 5), title=myplace, ax=axs[1])\n    plt.show()", "processed": ["deal flatten locat"]}, {"markdown": ["# Remaining Locations\n## 217 left"], "code": "# Reference: https://www.kaggle.com/code/robikscube/rob-s-covid19-week-2-submission-less-aggressive\n\nremaining_places = pd.DataFrame(tt.groupby('Place')['ConfirmedCases_Pred1'].max().isna()).query('ConfirmedCases_Pred1').index.values\nprint(remaining_places)\nprint(len(remaining_places))\nfor myplace in remaining_places:\n    try:\n        # Confirmed Cases\n        fig, axs = plt.subplots(1, 2, figsize=(15, 3))\n        dat = tt.query('Place == @myplace and Days_Since_Ten_Cases >= 0')[['Days_Since_Ten_Cases','ConfirmedCases_Log']].dropna()\n        X = dat['Days_Since_Ten_Cases']\n        y = dat['ConfirmedCases_Log']\n        y = y.cummax()\n        dat_all = tt.query('Place == @myplace and Days_Since_Ten_Cases >= 0')[['Days_Since_Ten_Cases','ConfirmedCases_Log']]\n        X_pred = dat_all['Days_Since_Ten_Cases']\n        en = ElasticNet()\n        en.fit(X.values.reshape(-1, 1), y.values)\n        preds = en.predict(X_pred.values.reshape(-1, 1))\n        tt.loc[(tt['Place'] == myplace) & (tt['Days_Since_Ten_Cases'] >= 0), 'ConfirmedCases_Log_Pred1'] = preds\n        tt.loc[(tt['Place'] == myplace), 'ConfirmedCases_Pred1'] = tt['ConfirmedCases_Log_Pred1'].apply(np.expm1)\n        # Cap at 10 % Population\n        pop_myplace = tt.query('Place == @myplace')['Population_final'].values[0]\n        tt.loc[(tt['Place'] == myplace) & (tt['ConfirmedCases_Pred1'] > (0.05 * pop_myplace)), 'ConfirmedCases_Pred1'] = (0.05 * pop_myplace)\n        ax = tt.query('Place == @myplace').set_index('Date')[['ConfirmedCases','ConfirmedCases_Pred1']].plot(figsize=(15, 5), title=myplace, ax=axs[0])\n        # Fatalities\n        # If low count then do percent of confirmed:\n        dat = tt.query('Place == @myplace and Days_Since_Ten_Cases >= 0')[['Days_Since_Ten_Cases','Fatalities_Log']].dropna()\n        if len(dat) < 5:\n            tt.loc[(tt['Place'] == myplace), 'Fatalities_Pred1'] = tt.loc[(tt['Place'] == myplace)]['ConfirmedCases_Pred1'] * 0.0001\n        elif tt.query('Place == @myplace')['Fatalities'].max() < 5:\n            tt.loc[(tt['Place'] == myplace), 'Fatalities_Pred1'] = tt.loc[(tt['Place'] == myplace)]['ConfirmedCases_Pred1'] * 0.0001\n        else:\n            X = dat['Days_Since_Ten_Cases']\n            y = dat['Fatalities_Log']\n            y = y.cummax()\n            dat_all = tt.query('Place == @myplace and Days_Since_Ten_Cases >= 0')[['Days_Since_Ten_Cases','Fatalities_Log']]\n            X_pred = dat_all['Days_Since_Ten_Cases']\n            en = ElasticNet()\n            en.fit(X.values.reshape(-1, 1), y.values)\n            preds = en.predict(X_pred.values.reshape(-1, 1))\n            tt.loc[(tt['Place'] == myplace) & (tt['Days_Since_Ten_Cases'] >= 0), 'Fatalities_Log_Pred1'] = preds\n            tt.loc[(tt['Place'] == myplace), 'Fatalities_Pred1'] = tt['Fatalities_Log_Pred1'].apply(np.expm1)\n\n            # Cap at 0.0001 Population\n            pop_myplace = tt.query('Place == @myplace')['Population_final'].values[0]\n            tt.loc[(tt['Place'] == myplace) & (tt['Fatalities_Pred1'] > (0.0001 * pop_myplace)), 'Fatalities_Pred1'] = (0.0001 * pop_myplace)\n\n        ax = tt.query('Place == @myplace').set_index('Date')[['Fatalities','Fatalities_Pred1']].plot(figsize=(15, 5), title=myplace, ax=axs[1])\n        plt.show()\n    except:\n        print(f'============= FAILED FOR {myplace} =============')\n\n# Estimated total\ntt.groupby('Place')['Fatalities_Pred1'].max().sum()\n# Clean Up any time the actual is less than the real\ntt['ConfirmedCases_Pred1'] = tt[['ConfirmedCases','ConfirmedCases_Pred1']].max(axis=1)\ntt['Fatalities_Pred1'] = tt[['Fatalities','Fatalities_Pred1']].max(axis=1)\n\ntt['ConfirmedCases_Pred1'] = tt['ConfirmedCases_Pred1'].fillna(0)\ntt['Fatalities_Pred1'] = tt['Fatalities_Pred1'].fillna(0)\n\n# Fill pred with\ntt.loc[~tt['ConfirmedCases'].isna(), 'ConfirmedCases_Pred1'] = tt.loc[~tt['ConfirmedCases'].isna()]['ConfirmedCases']\ntt.loc[~tt['Fatalities'].isna(), 'Fatalities_Pred1'] = tt.loc[~tt['Fatalities'].isna()]['Fatalities']\n\ntt['ConfirmedCases_Pred1'] = tt.groupby('Place')['ConfirmedCases_Pred1'].transform('cummax')\ntt['Fatalities_Pred1'] = tt.groupby('Place')['Fatalities_Pred1'].transform('cummax')", "processed": ["remain locat 217 left"]}, {"markdown": ["# Plot them all!!"], "code": "# Reference: https://www.kaggle.com/code/robikscube/rob-s-covid19-week-2-submission-less-aggressive\n\nfor myplace in tt['Place'].unique():\n    try:\n        # Confirmed Cases\n        fig, axs = plt.subplots(1, 2, figsize=(15, 3))\n        ax = tt.query('Place == @myplace').set_index('Date')[['ConfirmedCases','ConfirmedCases_Pred1']].plot(title=myplace, ax=axs[0])\n        ax = tt.query('Place == @myplace').set_index('Date')[['Fatalities','Fatalities_Pred1']].plot(title=myplace, ax=axs[1])\n        plt.show()\n    except:\n        print(f'============= FAILED FOR {myplace} =============')\ntt.groupby('Place')['Fatalities_Pred1'].max().sort_values()\n# Questionable numbers\ntt.query('Place == \"Iran\"').set_index('Date')[['ConfirmedCases',\n                                               'ConfirmedCases_Pred1',]].plot(figsize=(15, 5))\n# Make Iran's Predictions Linear\n\ntt.query('Place == \"Iran\"').set_index('Date')[['Fatalities','Fatalities_Pred1']].plot(figsize=(15, 5))", "processed": ["plot"]}, {"markdown": ["# Make Iran's Predictions Linear"], "code": "# Reference: https://www.kaggle.com/code/robikscube/rob-s-covid19-week-2-submission-less-aggressive\n\ndat.iloc[-10:]\nfor myplace in ['Iran']:\n\n    # Confirmed Cases\n    fig, axs = plt.subplots(1, 2, figsize=(15, 3))\n    dat = tt.query('Place == @myplace and Days_Since_Ten_Cases >= 0')[['Days_Since_Ten_Cases','ConfirmedCases']].dropna()\n    dat = dat.iloc[-10:]\n    X = dat['Days_Since_Ten_Cases']\n    y = dat['ConfirmedCases']\n    y = y.cummax()\n    dat_all = tt.query('Place == @myplace and Days_Since_Ten_Cases >= 0')[['Days_Since_Ten_Cases','ConfirmedCases']]\n    X_pred = dat_all['Days_Since_Ten_Cases']\n    en = ElasticNet()\n    en.fit(X.values.reshape(-1, 1), y.values)\n    preds = en.predict(X_pred.values.reshape(-1, 1))\n    tt.loc[(tt['Place'] == myplace) & (tt['Days_Since_Ten_Cases'] >= 0), 'ConfirmedCases_Pred1'] = preds\n    # Cap at 10 % Population\n    pop_myplace = tt.query('Place == @myplace')['Population_final'].values[0]\n    tt.loc[(tt['Place'] == myplace) & (tt['ConfirmedCases_Pred1'] > (0.1 * pop_myplace)), 'ConfirmedCases_Pred1'] = (0.1 * pop_myplace)\n    ax = tt.query('Place == @myplace').set_index('Date')[['ConfirmedCases','ConfirmedCases_Pred1']].plot(figsize=(15, 5), title=myplace, ax=axs[0])\n    # Fatalities\n    # If low count then do percent of confirmed:\n    dat = tt.query('Place == @myplace and Days_Since_Ten_Cases >= 0')[['Days_Since_Ten_Cases','Fatalities']].dropna()\n    dat = dat.iloc[-10:]\n    if len(dat) < 5:\n        tt.loc[(tt['Place'] == myplace), 'Fatalities_Pred1'] = tt.loc[(tt['Place'] == myplace)]['ConfirmedCases_Pred1'] * 0.001\n    elif tt.query('Place == @myplace')['Fatalities'].max() < 5:\n        tt.loc[(tt['Place'] == myplace), 'Fatalities_Pred1'] = tt.loc[(tt['Place'] == myplace)]['ConfirmedCases_Pred1'] * 0.001\n    else:\n        X = dat['Days_Since_Ten_Cases']\n        y = dat['Fatalities']\n        y = y.cummax()\n        dat_all = tt.query('Place == @myplace and Days_Since_Ten_Cases >= 0')[['Days_Since_Ten_Cases','Fatalities']]\n        X_pred = dat_all['Days_Since_Ten_Cases']\n        en = ElasticNet()\n        en.fit(X.values.reshape(-1, 1), y.values)\n        preds = en.predict(X_pred.values.reshape(-1, 1))\n        tt.loc[(tt['Place'] == myplace) & (tt['Days_Since_Ten_Cases'] >= 0), 'Fatalities_Pred1'] = preds\n\n        # Cap at 0.0001 Population\n        pop_myplace = tt.query('Place == @myplace')['Population_final'].values[0]\n        tt.loc[(tt['Place'] == myplace) & (tt['Fatalities_Pred1'] > (0.0005 * pop_myplace)), 'Fatalities_Pred1'] = (0.0005 * pop_myplace)\n\n    ax = tt.query('Place == @myplace').set_index('Date')[['Fatalities','Fatalities_Pred1']].plot(figsize=(15, 5), title=myplace, ax=axs[1])\n    plt.show()\n# Clean Up any time the actual is less than the real\ntt['ConfirmedCases_Pred1'] = tt[['ConfirmedCases','ConfirmedCases_Pred1']].max(axis=1)\ntt['Fatalities_Pred1'] = tt[['Fatalities','Fatalities_Pred1']].max(axis=1)\n\ntt['ConfirmedCases_Pred1'] = tt['ConfirmedCases_Pred1'].fillna(0)\ntt['Fatalities_Pred1'] = tt['Fatalities_Pred1'].fillna(0)\n\n# Fill pred with\ntt.loc[~tt['ConfirmedCases'].isna(), 'ConfirmedCases_Pred1'] = tt.loc[~tt['ConfirmedCases'].isna()]['ConfirmedCases']\ntt.loc[~tt['Fatalities'].isna(), 'Fatalities_Pred1'] = tt.loc[~tt['Fatalities'].isna()]['Fatalities']\n\ntt['ConfirmedCases_Pred1'] = tt.groupby('Place')['ConfirmedCases_Pred1'].transform('cummax')\ntt['Fatalities_Pred1'] = tt.groupby('Place')['Fatalities_Pred1'].transform('cummax')\n# Questionable numbers\ntt.query('Place == \"Iran\"').set_index('Date')[['ConfirmedCases',\n                                               'ConfirmedCases_Pred1',]].plot(figsize=(15, 5))\n# Make Iran's Predictions Linear\n\ntt.query('Place == \"Iran\"').set_index('Date')[['Fatalities','Fatalities_Pred1']].plot(figsize=(15, 5))", "processed": ["make iran predict linear"]}, {"markdown": ["#### There are some \"interesting\" masks in the training set. What do you think, should we remove these images from the training set?"], "code": "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport imageio\n\nfrom scipy import ndimage\nfrom pathlib import Path\n\nim_dir = Path('../input/train/')\n\ndf_pred = pd.read_csv('../input/train.csv', index_col=[0])\ndf_pred.fillna('', inplace=True)\ndf_pred['suspicious'] = False\n\nfor index, row in df_pred.iterrows():\n    encoded_mask = row['rle_mask'].split(' ')\n    if len(encoded_mask) > 1 and len(encoded_mask) < 5 and int(encoded_mask[1]) % 101 == 0:\n        df_pred.loc[index,'suspicious'] = True\n\ndef show_plot(rows):\n    idx_images = 0\n    max_images = 60\n    grid_width = 15\n    grid_height = int(max_images / grid_width)\n\n    fig, axs = plt.subplots(grid_height, grid_width, figsize=(grid_width, grid_height))\n\n    for index, row in rows.iterrows():\n        im_path = im_dir / 'images' / '{}.png'.format(index)\n        img = imageio.imread(im_path.as_posix())\n\n        im_path = im_dir / 'masks' / '{}.png'.format(index)\n        mask = imageio.imread(im_path.as_posix())\n\n        ax = axs[int(idx_images / grid_width), idx_images % grid_width]\n        ax.imshow(img, cmap=\"Greys\")\n        ax.imshow(mask, alpha=0.45, cmap=\"Greens\")\n\n        ax.text(1, 1, np.count_nonzero(mask), color=\"black\", ha=\"left\", va=\"top\")\n        ax.set_yticklabels([])\n        ax.set_xticklabels([])\n\n        idx_images = idx_images + 1\n\n        if idx_images > 59:\n            break\n\n    plt.suptitle(\"Grey: training image, Green: training mask, Top-left: # of pixels\")", "processed": ["interest mask train set think remov imag train set"]}, {"markdown": ["## Correct mask examples"], "code": "# Reference: https://www.kaggle.com/code/pestipeti/fake-incorrect-training-masks\n\nshow_plot(df_pred[df_pred['suspicious'] == False])", "processed": ["correct mask exampl"]}, {"markdown": ["There are a number of clear patterns that allow us to hope that we can improve the solution.", "## 5.6. Clustering <a class=\"anchor\" id=\"5.6\"></a>\n\n[Back to Table of Contents](#0.1)", "Using my notebook https://www.kaggle.com/vbmokin/covid-19-week5-global-forecasting-eda-extratr"], "code": "# Reference: https://www.kaggle.com/code/vbmokin/tse2020-roberta-0-7175-prlb-outlier-analysis\n\ndata = train[['sentiment', 'start', 'end', 'start_pred', 'end_pred', 'len_text', 'len_selected_text', 'diff_num', 'share', 'metric', 'res']].dropna()\ndata\n# Thanks to https://www.kaggle.com/kashnitsky/topic-7-unsupervised-learning-pca-and-clustering\ninertia = []\npca = PCA(n_components=2)\n# fit X and apply the reduction to X \nx_3d = pca.fit_transform(data)\nfor k in range(1, 8):\n    kmeans = KMeans(n_clusters=k, random_state=1).fit(x_3d)\n    inertia.append(np.sqrt(kmeans.inertia_))\nplt.plot(range(1, 8), inertia, marker='s');\nplt.xlabel('$k$')\nplt.ylabel('$J(C_k)$');\n# Thanks to https://www.kaggle.com/arthurtok/a-cluster-of-colors-principal-component-analysis\n# Set a 3 KMeans clustering\nkmeans = KMeans(n_clusters=5, random_state=0)\n# Compute cluster centers and predict cluster indices\nX_clustered = kmeans.fit_predict(x_3d)\nLABEL_COLOR_MAP = {0 : 'r',\n                   1 : 'g',\n                   2 : 'b',\n                   3 : 'y',\n                   4 : 'c'}\n\nlabel_color = [LABEL_COLOR_MAP[l] for l in X_clustered]\nplt.figure(figsize = (7,7))\nplt.scatter(x_3d[:,0],x_3d[:,1], c= label_color, alpha=0.9)\nplt.show()", "processed": ["number clear pattern allow u hope improv solut", "5 6 cluster class anchor id 5 6 back tabl content 0 1", "use notebook http www kaggl com vbmokin covid 19 week5 global forecast eda extratr"]}, {"markdown": ["# Exploring", "## Understanding the Target Distribution"], "code": "# Reference: https://www.kaggle.com/code/kabure/simple-eda-lightgbm-autotuning-w-hyperopt\n\ndf_train['scalar_coupling_constant'].sample(200000).iplot(kind='hist', title='Scalar Coupling Constant Distribuition',\n                                                          xTitle='Scalar Coupling value', yTitle='Probability', histnorm='percent' )", "processed": ["explor", "understand target distribut"]}, {"markdown": ["We have a clear distribution", "## Looking the different Types"], "code": "# Reference: https://www.kaggle.com/code/kabure/simple-eda-lightgbm-autotuning-w-hyperopt\n\nplt.figure(figsize=(15,10))\n\ng = plt.subplot(211)\ng = sns.countplot(x='type', data=df_train, )\ng.set_title(\"Count of Different Molecule Types\", fontsize=22)\ng.set_xlabel(\"Molecular Type Name\", fontsize=18)\ng.set_ylabel(\"Count Molecules in each Type\", fontsize=18)\n\ng1 = plt.subplot(212)\ng1 = sns.boxplot(x='type', y='scalar_coupling_constant', data=df_train )\ng1.set_title(\"Count of Different Molecule Types\", fontsize=22)\ng1.set_xlabel(\"Molecular Type Name\", fontsize=18)\ng1.set_ylabel(\"Scalar Coupling distribution\", fontsize=18)\n\nplt.subplots_adjust(wspace = 0.5, hspace = 0.5,top = 0.9)\n\nplt.show()", "processed": ["clear distribut", "look differ type"]}, {"markdown": ["**Now, I will create a interactive button to set all chart in on chunk of s", "## Atom index 0 and Atom index 1 Counting distribution"], "code": "# Reference: https://www.kaggle.com/code/kabure/simple-eda-lightgbm-autotuning-w-hyperopt\n\nplt.figure(figsize=(15,10))\n\ng = plt.subplot(211)\ng = sns.countplot(x='atom_index_0', data=df_train, color='darkblue' )\ng.set_title(\"Count of Atom index 0\", fontsize=22)\ng.set_xlabel(\"index 0 Number\", fontsize=18)\ng.set_ylabel(\"Count\", fontsize=18)\n\ng1 = plt.subplot(212)\ng1 = sns.countplot(x='atom_index_1',data=df_train, color='darkblue' )\ng1.set_title(\"Count of Atom index 1\", fontsize=22)\ng1.set_xlabel(\"index 1 Number\", fontsize=18)\ng1.set_ylabel(\"Count\", fontsize=18)\n\nplt.subplots_adjust(wspace = 0.5, hspace = 0.5,top = 0.9)\n\nplt.show()", "processed": ["creat interact button set chart chunk", "atom index 0 atom index 1 count distribut"]}, {"markdown": ["", "## Scale Coupling Distribution by Atom Index 0 and Index 1"], "code": "# Reference: https://www.kaggle.com/code/kabure/simple-eda-lightgbm-autotuning-w-hyperopt\n\nplt.figure(figsize=(17,12))\n\ng = plt.subplot(211)\ng = sns.boxenplot(x='atom_index_0', y='scalar_coupling_constant', data=df_train, color='darkred' )\ng.set_title(\"Count of Atom index 0\", fontsize=22)\ng.set_xlabel(\"index 0 Number\", fontsize=18)\ng.set_ylabel(\"Count\", fontsize=18)\n\ng1 = plt.subplot(212)\ng1 = sns.boxenplot(x='atom_index_1', y='scalar_coupling_constant', data=df_train, color='darkblue' )\ng1.set_title(\"Count of Atom index 1\", fontsize=22)\ng1.set_xlabel(\"index 1 Number\", fontsize=18)\ng1.set_ylabel(\"Scalar Coupling distribution\", fontsize=18)\n\nplt.subplots_adjust(wspace = 0.5, hspace = 0.5,top = 0.9)\n\nplt.show()", "processed": ["", "scale coupl distribut atom index 0 index 1"]}, {"markdown": ["### Calculating the distance "], "code": "# Reference: https://www.kaggle.com/code/kabure/simple-eda-lightgbm-autotuning-w-hyperopt\n\n## This is a very performative way to compute the distances\ntrain_p_0 = df_train[['x_0', 'y_0', 'z_0']].values\ntrain_p_1 = df_train[['x_1', 'y_1', 'z_1']].values\ntest_p_0 = df_test[['x_0', 'y_0', 'z_0']].values\ntest_p_1 = df_test[['x_1', 'y_1', 'z_1']].values\n\n## linalg.norm, explanation:\n## This function is able to return one of eight different matrix norms, \n## or one of an infinite number of vector norms (described below),\n## depending on the value of the ord parameter.\ndf_train['dist'] = np.linalg.norm(train_p_0 - train_p_1, axis=1)\ndf_test['dist'] = np.linalg.norm(test_p_0 - test_p_1, axis=1)\n\ndf_train['dist_x'] = (df_train['x_0'] - df_train['x_1']) ** 2\ndf_test['dist_x'] = (df_test['x_0'] - df_test['x_1']) ** 2\ndf_train['dist_y'] = (df_train['y_0'] - df_train['y_1']) ** 2\ndf_test['dist_y'] = (df_test['y_0'] - df_test['y_1']) ** 2\ndf_train['dist_z'] = (df_train['z_0'] - df_train['z_1']) ** 2\ndf_test['dist_z'] = (df_test['z_0'] - df_test['z_1']) ** 2\ndf_train['dist'].sample(200000).iplot(kind='hist', title='Scalar Coupling Constant Distribuition',\n                                                          xTitle='Scalar Coupling value', yTitle='Probability', histnorm='percent' )", "processed": ["calcul distanc"]}, {"markdown": ["## Looking Boxplot of Index 0 and 1"], "code": "# Reference: https://www.kaggle.com/code/kabure/simple-eda-lightgbm-autotuning-w-hyperopt\n\nplt.figure(figsize=(14,12))\n\ng1 = plt.subplot(211)\ng1 = sns.boxenplot(x='atom_index_0', y='dist', data=df_train, color='blue' )\ng1.set_title(\"Distance Distribution by Atom index 0\", fontsize=22)\ng1.set_xlabel(\"Index 0 Number\", fontsize=18)\ng1.set_ylabel(\"Distance Distribution\", fontsize=18)\n\ng2 = plt.subplot(212)\ng2 = sns.boxenplot(x='atom_index_1', y='dist', data=df_train, color='green' )\ng2.set_title(\"Distance Distribution by Atom index 1\", fontsize=22)\ng2.set_xlabel(\"Index 1 Number\", fontsize=18)\ng2.set_ylabel(\"Distance Distribution\", fontsize=18)\n\nplt.subplots_adjust(wspace = 0.5, hspace = 0.3,top = 0.9)\n\nplt.show()", "processed": ["look boxplot index 0 1"]}, {"markdown": ["", "### Networks using the calculated distances\nWe have molecules, atom pairs, so this means data, which is interconnected. <br>\nNetwork graphs should be useful to visualize such data!<br>"], "code": "# Reference: https://www.kaggle.com/code/kabure/simple-eda-lightgbm-autotuning-w-hyperopt\n\n## graph from:\n### https://www.kaggle.com/artgor/molecular-properties-eda-and-models\n\nimport networkx as nx\n\nfig, ax = plt.subplots(figsize = (20, 12))\nfor i, t in enumerate(df_train['type'].unique()):\n    train_type = df_train.loc[df_train['type'] == t]\n    G = nx.from_pandas_edgelist(train_type, 'atom_index_0', 'atom_index_1', ['dist'])\n    plt.subplot(2, 4, i + 1);\n    nx.draw(G, with_labels=True);\n    plt.title(f'Graph for type {t}')", "processed": ["", "network use calcul distanc molecul atom pair mean data interconnect br network graph use visual data br"]}, {"markdown": ["But there is a little problem: as we saw earlier, there are atoms which are very rare, as a result graphs will be skewed due to them. Now I'll drop atoms for each type which are present in less then 1% of connections", "## Removing rare atoms for each type "], "code": "# Reference: https://www.kaggle.com/code/kabure/simple-eda-lightgbm-autotuning-w-hyperopt\n\nfig, ax = plt.subplots(figsize = (20, 12))\nfor i, t in enumerate(df_train['type'].unique()):\n    train_type = df_train.loc[df_train['type'] == t]\n    bad_atoms_0 = list(train_type['atom_index_0'].value_counts(normalize=True)[train_type['atom_index_0'].value_counts(normalize=True) < 0.01].index)\n    bad_atoms_1 = list(train_type['atom_index_1'].value_counts(normalize=True)[train_type['atom_index_1'].value_counts(normalize=True) < 0.01].index)\n    bad_atoms = list(set(bad_atoms_0 + bad_atoms_1))\n    train_type = train_type.loc[(train_type['atom_index_0'].isin(bad_atoms_0) == False) & (train_type['atom_index_1'].isin(bad_atoms_0) == False)]\n    G = nx.from_pandas_edgelist(train_type, 'atom_index_0', 'atom_index_1', ['scalar_coupling_constant'])\n    plt.subplot(2, 4, i + 1);\n    nx.draw(G, with_labels=True);\n    plt.title(f'Graph for type {t}')", "processed": ["littl problem saw earlier atom rare result graph skew due drop atom type present le 1 connect", "remov rare atom type"]}, {"markdown": ["It's a very interesting graph that show the distance", "### Now, I will plot some molecules in a 3D graph to we see the difference in their distances"], "code": "# Reference: https://www.kaggle.com/code/kabure/simple-eda-lightgbm-autotuning-w-hyperopt\n\nnumber_of_colors = df_structure.atom.value_counts().count() # total number of different collors that we will use\n\n# Here I will generate a bunch of hexadecimal colors \ncolor = [\"#\"+''.join([random.choice('0123456789ABCDEF') for j in range(6)])\n             for i in range(number_of_colors)]\ndf_structure['color'] = np.nan\n\nfor idx, col in enumerate(df_structure.atom.value_counts().index):\n    listcol = ['#C15477', '#7ECF7B', '#4BDBBD', '#338340', '#F9E951']\n    df_structure.loc[df_structure['atom'] == col, 'color'] = listcol[idx]\n## Building a \ntrace1 = go.Scatter3d(\n    x=df_structure[df_structure['molecule_name'] == 'dsgdb9nsd_000003'].x,\n    y=df_structure[df_structure['molecule_name'] == 'dsgdb9nsd_000003'].y,\n    z=df_structure[df_structure['molecule_name'] == 'dsgdb9nsd_000003'].z, \n    hovertext=df_structure[df_structure['molecule_name'] == 'dsgdb9nsd_000003'].atom,\n    mode='markers', name=\"3 Atoms Molecule\",visible=False, \n    marker=dict(\n        size=10,\n        color=df_structure[df_structure['molecule_name'] == 'dsgdb9nsd_000003'].color,                \n    )\n) \n\ntrace2 = go.Scatter3d(\n    x=df_structure[df_structure['molecule_name'] == 'dsgdb9nsd_002116'].x,\n    y=df_structure[df_structure['molecule_name'] == 'dsgdb9nsd_002116'].y,\n    z=df_structure[df_structure['molecule_name'] == 'dsgdb9nsd_002116'].z, \n    hovertext=df_structure[df_structure['molecule_name'] == 'dsgdb9nsd_002116'].atom,\n    mode='markers', name=\"8 Atoms Molecule\",visible=True, \n    marker=dict(symbol='circle',\n                size=6,\n                color=df_structure[df_structure['molecule_name'] == 'dsgdb9nsd_002116'].color,\n                colorscale='Viridis',\n                line=dict(color='rgb(50,50,50)', width=0.5)\n               ),\n    hoverinfo='text'\n)\n\ntrace3 = go.Scatter3d(\n    x=df_structure[df_structure['molecule_name'] == 'dsgdb9nsd_051136'].x,\n    y=df_structure[df_structure['molecule_name'] == 'dsgdb9nsd_051136'].y,\n    z=df_structure[df_structure['molecule_name'] == 'dsgdb9nsd_051136'].z, \n    hovertext=df_structure[df_structure['molecule_name'] == 'dsgdb9nsd_051136'].atom,\n    mode='markers', name=\"17 Atoms Molecule\",visible=False, \n    marker=dict(\n        size=10,\n        color=df_structure[df_structure['molecule_name'] == 'dsgdb9nsd_002116'].color,             \n    )\n) \n\ntrace4 = go.Scatter3d(\n    x=df_structure[df_structure['molecule_name'] == 'dsgdb9nsd_088951'].x,\n    y=df_structure[df_structure['molecule_name'] == 'dsgdb9nsd_088951'].y,\n    z=df_structure[df_structure['molecule_name'] == 'dsgdb9nsd_088951'].z, \n    hovertext=df_structure[df_structure['molecule_name'] == 'dsgdb9nsd_088951'].atom,\n    mode='markers', name=\"25 Atoms Molecule\",visible=False, \n    marker=dict(\n        size=10,\n        color=df_structure[df_structure['molecule_name'] == 'dsgdb9nsd_002116'].color,             \n    )\n) \ndata = [trace1, trace2, trace3, trace4]\n\nupdatemenus = list([\n    dict(active=-1,\n         showactive=True,\n         buttons=list([  \n            dict(\n                label = '3 Atoms',\n                 method = 'update',\n                 args = [{'visible': [True, False, False, False]}, \n                     {'title': 'Molecule with 3 Atoms'}]),\n             \n             dict(\n                  label = '8 Atoms',\n                 method = 'update',\n                 args = [{'visible': [False, True, False, False]},\n                     {'title': 'Molecule with 8 Atoms'}]),\n\n            dict(\n                 label = '17 Atoms',\n                 method = 'update',\n                 args = [{'visible': [False, False, True, False]},\n                     {'title': 'Molecule with 17 Atoms'}]),\n\n            dict(\n                 label = '25 Atoms',\n                 method = 'update',\n                 args = [{'visible': [False, False, False, True]},\n                     {'title': 'Molecule with 25 Atoms'}])\n        ]),\n    )\n])\n\n\nlayout = dict(title=\"The distance between atoms in some molecules <br>(Select from Dropdown)<br> Molecule of 8 Atoms\", \n              showlegend=False,\n              updatemenus=updatemenus)\n\nfig = dict(data=data, layout=layout)\n\niplot(fig)", "processed": ["interest graph show distanc", "plot molecul 3d graph see differ distanc"]}, {"markdown": ["### 2.3 Target variable distribution\n\nLets plot the distribution of target variable"], "code": "# Reference: https://www.kaggle.com/code/shivamb/bot-generated-baseline-kernel-id-18345\n\ntar_dist = dict(Counter(Y.values))\n\nxx = list(tar_dist.keys())\nyy = list(tar_dist.values())\n\nplt.figure(figsize=(5,3))\nsns.set(style=\"whitegrid\")\nax = sns.barplot(x=xx, y=yy, palette=\"rocket\")\nax.set_title('Distribution of Target')\nax.set_ylabel('count');\nax.set_xlabel(_target);", "processed": ["2 3 target variabl distribut let plot distribut target variabl"]}, {"markdown": ["### 2.4 Missing Value Counts \n\nLets check the count of missing values in the datasets"], "code": "# Reference: https://www.kaggle.com/code/shivamb/bot-generated-baseline-kernel-id-18345\n\nmcount = train_df.isna().sum() \nxx = mcount.index \nyy = mcount.values\n\nmissing_cols = 0\nfor each in yy:\n    if each > 0:\n        missing_cols += 1\nprint (\"there are \" + str(missing_cols) + \" columns in the dataset having missing values\")\n\nif missing_cols > 0:\n    plt.figure(figsize=(12,5))\n    sns.set(style=\"whitegrid\")\n    ax = sns.barplot(x=xx, y=yy, palette=\"gist_rainbow\")\n    ax.set_title('Number of Missing Values')\n    ax.set_ylabel('Number of Columns');", "processed": ["2 4 miss valu count let check count miss valu dataset"]}, {"markdown": ["### 3.3 Feature Engineering (only for text fields)\n\nIn this section, we will create relevant features which can be used in the modelling\n\n#### 3.3.1 Tf IDF features"], "code": "# Reference: https://www.kaggle.com/code/shivamb/bot-generated-baseline-kernel-id-18345\n\nif tag == \"doc\":\n    tfidf = TfidfVectorizer(min_df=3,  max_features=None, analyzer='word', \n                            token_pattern=r'\\w{1,}', stop_words = 'english')\n    tfidf.fit(list(train_df[textcol].values))\n    xtrain = tfidf.transform(train_df[textcol].values) \n    if textcol in test_df.columns:\n        xtest = tfidf.transform(test_df[textcol].values)\nelse:\n    xtrain = train_df\n    xtest = test_df\nif tag != \"doc\":\n    print (\"Lets plot the dataset distributions after preprocessing step ... \")\n    ## pair plots\n    sns.pairplot(train_df, palette=\"cool\")\n    \n    ## distributions\n    columns=train_df.columns\n    plt.subplots(figsize=(18,15))\n    length=len(columns)\n    for i,j in itertools.zip_longest(columns,range(length)):\n        plt.subplot((length/2),3,j+1)\n        plt.subplots_adjust(wspace=0.2,hspace=0.5)\n        train_df[i].hist(bins=20, edgecolor='white')\n        plt.title(i)\n    plt.show()", "processed": ["3 3 featur engin text field section creat relev featur use model 3 3 1 tf idf featur"]}, {"markdown": ["## Step 4 : Create baseline model\n\nNext step is the modelling step, lets start with the simple linear model \n\n### 4.1 : Logistic Regression\n\nTrain a binary classifier logistic regression"], "code": "# Reference: https://www.kaggle.com/code/shivamb/bot-generated-baseline-kernel-id-18345\n\nmodel1 = LogisticRegression()\nmodel1.fit(X_train, y_train)\nvalp = model1.predict(X_valid)\n\ndef generate_auc(y_valid, valp, model_name):\n    auc_scr = roc_auc_score(y_valid, valp)\n    print('The AUC for ' +model_name+ ' is :', auc_scr)\n\n    fpr, tpr, thresholds = roc_curve(y_valid, valp)\n    roc_auc = auc(fpr, tpr)\n\n    plt.figure(figsize=(6,5))\n    plt.title('Receiver Operating Characteristic')\n    plt.plot(fpr, tpr, 'purple', label = 'AUC = %0.2f' % roc_auc)\n    plt.legend(loc = 'upper left')\n    plt.plot([0, 1], [0, 1],'r--')\n    plt.xlim([0, 1])\n    plt.ylim([0, 1])\n\n    plt.ylabel('True Positive Rate')\n    plt.xlabel('False Positive Rate')\n    plt.show()\n\nif len(distinct_Y) == 2:\n    generate_auc(y_valid, valp, model_name=\"logistic regression\")\ndef plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n\n    plt.figure(figsize=(6,5));\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n    plt.grid(False)\n\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", \n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n        \n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\ncnf_matrix = confusion_matrix(y_valid, valp)\nnp.set_printoptions(precision=2)\n\nplt.figure(figsize=(8,8))\nplot_confusion_matrix(cnf_matrix, classes=distinct_Y, title='Confusion matrix Validation Set')\nplt.show()", "processed": ["step 4 creat baselin model next step model step let start simpl linear model 4 1 logist regress train binari classifi logist regress"]}, {"markdown": ["### Visualize the effect of high-pass filter and wavelet denoising on seismic signals"], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/lanl-earthquake-prediction-signal-denoising\n\nfig, ax = plt.subplots(nrows=3, ncols=4, figsize=(30, 15))\n\nax[0, 0].plot(signals[0], 'crimson') \nax[0, 0].set_title('Original Signal', fontsize=16)\nax[0, 1].plot(high_pass_filter(signals[0], low_cutoff=10000, SAMPLE_RATE=4000000), 'mediumvioletred') \nax[0, 1].set_title('After High-Pass Filter', fontsize=16)\nax[0, 2].plot(denoise_signal(signals[0]), 'darkmagenta')\nax[0, 2].set_title('After Wavelet Denoising', fontsize=16)\nax[0, 3].plot(denoise_signal(high_pass_filter(signals[0], low_cutoff=10000, SAMPLE_RATE=4000000), wavelet='haar', level=1), 'indigo')\nax[0, 3].set_title('After High-Pass Filter and Wavelet Denoising', fontsize=16)\n\nax[1, 0].plot(signals[1], 'crimson') \nax[1, 0].set_title('Original Signal', fontsize=16)\nax[1, 1].plot(high_pass_filter(signals[1], low_cutoff=10000, SAMPLE_RATE=4000000), 'mediumvioletred') \nax[1, 1].set_title('After High-Pass Filter', fontsize=16)\nax[1, 2].plot(denoise_signal(signals[1]), 'darkmagenta')\nax[1, 2].set_title('After Wavelet Denoising', fontsize=16)\nax[1, 3].plot(denoise_signal(high_pass_filter(signals[1], low_cutoff=10000, SAMPLE_RATE=4000000), wavelet='haar', level=1), 'indigo')\nax[1, 3].set_title('After High-Pass Filter and Wavelet Denoising', fontsize=16)\n\nax[2, 0].plot(signals[2], 'crimson') \nax[2, 0].set_title('Original Signal', fontsize=16)\nax[2, 1].plot(high_pass_filter(signals[2], low_cutoff=10000, SAMPLE_RATE=4000000), 'mediumvioletred') \nax[2, 1].set_title('After High-Pass Filter', fontsize=16)\nax[2, 2].plot(denoise_signal(signals[2]), 'darkmagenta')\nax[2, 2].set_title('After Wavelet Denoising', fontsize=16)\nax[2, 3].plot(denoise_signal(high_pass_filter(signals[2], low_cutoff=10000, SAMPLE_RATE=4000000), wavelet='haar', level=1), 'indigo')\nax[2, 3].set_title('After High-Pass Filter and Wavelet Denoising', fontsize=16)\n\nplt.show()", "processed": ["visual effect high pas filter wavelet denois seismic signal"]}, {"markdown": ["### Visualize the effect of average smoothing on seismic signals (with differing kernel sizes)"], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/lanl-earthquake-prediction-signal-denoising\n\nfig, ax = plt.subplots(nrows=3, ncols=4, figsize=(30, 15))\n\nax[0, 0].plot(signals[0], 'mediumaquamarine') \nax[0, 0].set_title('Original Signal', fontsize=14)\nax[0, 1].plot(average_smoothing(signals[0], kernel_size=5, stride=5), 'mediumseagreen')\nax[0, 1].set_title('After Average Smoothing (kernel_size=5 and stride=5)', fontsize=14)\nax[0, 2].plot(average_smoothing(signals[0], kernel_size=7, stride=5), 'seagreen')\nax[0, 2].set_title('After Average Smoothing (kernel_size=7 and stride=5)', fontsize=14)\nax[0, 3].plot(average_smoothing(signals[0], kernel_size=9, stride=5), 'darkgreen')\nax[0, 3].set_title('After Average Smoothing (kernel_size=9 and stride=5)', fontsize=14)\n\nax[1, 0].plot(signals[1], 'mediumaquamarine') \nax[1, 0].set_title('Original Signal', fontsize=14)\nax[1, 1].plot(average_smoothing(signals[1], kernel_size=5, stride=5), 'mediumseagreen') \nax[1, 1].set_title('After Average Smoothing (kernel_size=5 and stride=5)', fontsize=14)\nax[1, 2].plot(average_smoothing(signals[1], kernel_size=7, stride=5), 'seagreen')\nax[1, 2].set_title('After Average Smoothing (kernel_size=7 and stride=5)', fontsize=14)\nax[1, 3].plot(average_smoothing(signals[1], kernel_size=9, stride=5), 'darkgreen')\nax[1, 3].set_title('After Average Smoothing (kernel_size=9 and stride=5)', fontsize=14)\n\nax[2, 0].plot(signals[2], 'mediumaquamarine') \nax[2, 0].set_title('Original Signal', fontsize=14)\nax[2, 1].plot(average_smoothing(signals[2], kernel_size=5, stride=5), 'mediumseagreen') \nax[2, 1].set_title('After Average Smoothing (kernel_size=5 and stride=5)', fontsize=14)\nax[2, 2].plot(average_smoothing(signals[2], kernel_size=7, stride=5), 'seagreen')\nax[2, 2].set_title('After Average Smoothing (kernel_size=7 and stride=5)', fontsize=14)\nax[2, 3].plot(average_smoothing(signals[2], kernel_size=9, stride=5), 'darkgreen')\nax[2, 3].set_title('After Average Smoothing (kernel_size=9 and stride=5)', fontsize=14)\n\nplt.show()", "processed": ["visual effect averag smooth seismic signal differ kernel size"]}, {"markdown": ["All id's and url's are unique for the concatenated data. That means we do not have any id's or url's from train dataset leaked in the test data set as well.", "## Landmarks\n\nWe already know how many distincts landmarks there are in the train set. Let's inspect now how many occurences are for these landscapes in the train set."], "code": "# Reference: https://www.kaggle.com/code/gpreda/google-landmark-recogn-challenge-data-exploration\n\nplt.figure(figsize = (8, 8))\nplt.title('Landmark id density plot')\nsns.kdeplot(train_df['landmark_id'], color=\"tomato\", shade=True)\nplt.show()", "processed": ["id url uniqu concaten data mean id url train dataset leak test data set well", "landmark alreadi know mani distinct landmark train set let inspect mani occur landscap train set"]}, {"markdown": ["Let's represent the same data as a density plot"], "code": "# Reference: https://www.kaggle.com/code/gpreda/google-landmark-recogn-challenge-data-exploration\n\nplt.figure(figsize = (8, 8))\nplt.title('Landmark id distribuition and density plot')\nsns.distplot(train_df['landmark_id'],color='green', kde=True,bins=100)\nplt.show()", "processed": ["let repres data densiti plot"]}, {"markdown": ["Most frequent landmark has 50337 apparitions in train dataset."], "code": "# Reference: https://www.kaggle.com/code/gpreda/google-landmark-recogn-challenge-data-exploration\n\n# Plot the most frequent landmark occurences\nplt.figure(figsize = (6, 6))\nplt.title('Most frequent landmarks')\nsns.set_color_codes(\"pastel\")\nsns.barplot(x=\"landmark_id\", y=\"count\", data=th10,\n            label=\"Count\", color=\"darkgreen\")\nplt.show()\ntb10 = pd.DataFrame(train_df.landmark_id.value_counts().tail(10))\ntb10.reset_index(level=0, inplace=True)\ntb10.columns = ['landmark_id','count']\ntb10\n# Plot the least frequent landmark occurences\nplt.figure(figsize = (6,6))\nplt.title('Least frequent landmarks')\nsns.set_color_codes(\"pastel\")\nsns.barplot(x=\"landmark_id\", y=\"count\", data=tb10,\n            label=\"Count\", color=\"orange\")\nplt.show()", "processed": ["frequent landmark 50337 apparit train dataset"]}, {"markdown": ["The sites in train data are:"], "code": "# Reference: https://www.kaggle.com/code/gpreda/google-landmark-recogn-challenge-data-exploration\n\ntrain_site\n# Plot the site occurences in the train dataset\ntrsite = pd.DataFrame(list(train_site.index),train_site['site'])\ntrsite.reset_index(level=0, inplace=True)\ntrsite.columns = ['Count','Site']\nplt.figure(figsize = (6,6))\nplt.title('Sites storing images - train dataset')\nsns.set_color_codes(\"pastel\")\nsns.barplot(x = 'Site', y=\"Count\", data=trsite, color=\"blue\")\nlocs, labels = plt.xticks()\nplt.setp(labels, rotation=90)\nplt.show()", "processed": ["site train data"]}, {"markdown": ["We can observe that most of the images in the train dataset are stored on 4 sites, *lh3.googleusercontent.com*, *lh4.googleusercontent.com*, *lh5.googleusercontent.com* and *lh6.googleusercontent.com*.\n\nThe sites in test dataset are:"], "code": "# Reference: https://www.kaggle.com/code/gpreda/google-landmark-recogn-challenge-data-exploration\n\ntest_site\n# Plot the site occurences in the test dataset\ntesite = pd.DataFrame(list(test_site.index),test_site['site'])\ntesite.reset_index(level=0, inplace=True)\ntesite.columns = ['Count','Site']\nplt.figure(figsize = (6,6))\nplt.title('Sites storing images - test dataset')\nsns.set_color_codes(\"pastel\")\nsns.barplot(x = 'Site', y=\"Count\", data=tesite, color=\"magenta\")\nlocs, labels = plt.xticks()\nplt.setp(labels, rotation=90)\nplt.show()", "processed": ["observ imag train dataset store 4 site lh3 googleusercont com lh4 googleusercont com lh5 googleusercont com lh6 googleusercont com site test dataset"]}, {"markdown": ["# Plot Vlad's Predictions vs. QDA\n- Predictions are much more confident for target of 1\n- Less confident for target of 0"], "code": "# Reference: https://www.kaggle.com/code/robikscube/comparing-vlad-s-public-test-predictions-vs-qda\n\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 10))\ntest['qda_preds_target'] = preds\ntest['vlads_preds'] = vlad_preds[0:131073]\ntest[['qda_preds_target','vlads_preds']].sort_values('qda_preds_target').reset_index(drop=True) \\\n    .plot(style='.', alpha=0.1,\n          title='Vlad 0.973 Public Test predictions vs. QDA - Ordered by QDA',\n          ax=ax1)\ntest[['qda_preds_target','vlads_preds']].sort_values('vlads_preds').reset_index(drop=True) \\\n    .plot(style='.', alpha=0.1,\n          title='Vlad 0.973 Public Test predictions vs. QDA - Ordered by Vlads',\n          ax=ax2)\nplt.show()", "processed": ["plot vlad predict v qda predict much confid target 1 le confid target 0"]}, {"markdown": ["# Distribution of the difference between Vlad and QDA"], "code": "# Reference: https://www.kaggle.com/code/robikscube/comparing-vlad-s-public-test-predictions-vs-qda\n\ntest['diff'] = test['vlads_preds'] - test['qda_preds_target']\ntest['diff'].plot(kind='hist', figsize=(15, 5), bins=200, title='Distribution of difference between Vlad and Simple QDA preds')\nplt.show()\ntest['qda_preds_target'] = test['qda_preds_target'].round(5)\ntest[['vlads_preds','qda_preds_target','diff']].tail()\ntest[['vlads_preds','qda_preds_target','diff']] \\\n    .sort_values('diff') \\\n    .reset_index(drop=True) \\\n    .plot(style='.', figsize=(15, 5), title='Plot Predictions sorted by difference')\nplt.show()\ntest.plot(x='vlads_preds',\n          y='qda_preds_target',\n          kind='scatter',\n          figsize=(15, 15),\n          alpha=0.2,\n          title='Vlad Predictions vs QDA')\nplt.show()", "processed": ["distribut differ vlad qda"]}, {"markdown": ["### Visualize relationship between budget and revenue"], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/tmdb-box-office-comprehensive-eda\n\nsns.jointplot(x=budgets, y=revenues, dropna=True, color='blueviolet', kind='reg')\nplt.show()", "processed": ["visual relationship budget revenu"]}, {"markdown": ["There seems to be a positive correlation between budget and revenue. This implies that the revenue of a movie generally tends to increase when its budget increases. This is probably because the directors and producers can afford a better cast, a higher quality set, a more ambitious plot etc with a higher budget.", "## Popularity\n### Visualize the relationship between popularity and revenue"], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/tmdb-box-office-comprehensive-eda\n\nplot = sns.jointplot(x='popularity', y='revenue', data=train_df, dropna=True, color='orangered', kind='reg') ", "processed": ["seem posit correl budget revenu impli revenu movi gener tend increas budget increas probabl director produc afford better cast higher qualiti set ambiti plot etc higher budget", "popular visual relationship popular revenu"]}, {"markdown": ["The popularity and revenue do not seem to have any real correlation. There is only a very slight positive correlation. This is probably because a more popular movie generates more revenue :)", "## Language \n### Visualize the relationship between the original language and revenue of the *movie* ", "### Language two-letter codes\n\nLook [here](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes) for the two-letter language codes.\n", "## Most profitable movie languages"], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/tmdb-box-office-comprehensive-eda\n\nfig, ax = plt.subplots(figsize=(15, 15))\nax.tick_params(axis='both', labelsize=12)\nplt.title('Original Language and Revenue', fontsize=20)\nplt.xlabel('Revenue', fontsize=16)\nplt.ylabel('Original Language', fontsize=16)\nsns.boxplot(ax=ax, x='revenue', y='original_language', data=train_df, showfliers=False, orient='h')\nplt.show()", "processed": ["popular revenu seem real correl slight posit correl probabl popular movi gener revenu", "languag visual relationship origin languag revenu movi", "languag two letter code look http en wikipedia org wiki list iso 639 1 code two letter languag code", "profit movi languag"]}, {"markdown": ["The most common languages in the movie data seem to be English ('en'), French ('fr'), Russian ('ru'), Hindi ('hi') etc.", "## Genre\n### Visualize the relationship between the genre and revenue of the *movie* "], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/tmdb-box-office-comprehensive-eda\n\ngenres = []\nrepeated_revenues = []\nfor i in range(len(train_df)):\n  if train_df['genres'][i] == train_df['genres'][i]:\n      movie_genre = [genre['name'] for genre in eval(train_df['genres'][i])]\n      genres.extend(movie_genre)\n      repeated_revenues.extend([train_df['revenue'][i]]*len(movie_genre))\n  \ngenre_df = pd.DataFrame(np.zeros((len(genres), 2)))\ngenre_df.columns = ['genre', 'revenue']\ngenre_df['genre'] = genres\ngenre_df['revenue'] = repeated_revenues\nfig, ax = plt.subplots(figsize=(15, 15))\nax.tick_params(axis='both', labelsize=12)\nplt.title('Genres and Revenue', fontsize=20)\nplt.xlabel('revenue', fontsize=16)\nplt.ylabel('genre', fontsize=16)\nsns.boxplot(ax=ax, x=repeated_revenues, y=genres, showfliers=False, orient='h')\nplt.show()", "processed": ["common languag movi data seem english en french fr russian ru hindi hi etc", "genr visual relationship genr revenu movi"]}, {"markdown": ["## Negativity of overview", "### Visualize relationship between negativity of overview and revenue\n\n\n\n"], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/tmdb-box-office-comprehensive-eda\n\nsns.jointplot(x=negativities, y=revenues, dropna=True, color='mediumvioletred', kind='scatter')\nplt.show()", "processed": ["neg overview", "visual relationship neg overview revenu"]}, {"markdown": ["There seems to be a negative correlation between negativity of the overview and revenue. This is probably because the more negative the tagline, the less likely people are to watch the movie. This is because the negative tagline makes them not want to watch the movie. **This is probably why there is a massive peak in revenue at the lowest negativities close to 0.**\n\n*   List item\n*   List item\n\n", "## Neutrality of tagline", "### Visualize relationship between neutrality of tagline and revenue of movies"], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/tmdb-box-office-comprehensive-eda\n\nsns.jointplot(x=neutralities, y=revenues, dropna=True, color='mediumblue', kind='reg')\nplt.show()", "processed": ["seem neg correl neg overview revenu probabl neg taglin le like peopl watch movi neg taglin make want watch movi probabl massiv peak revenu lowest neg close 0 list item list item", "neutral taglin", "visual relationship neutral taglin revenu movi"]}, {"markdown": ["The neutrality of the tagline and revenue of the movie seem to be positively correlated. **This is probably because a movie with a more inclusive movie tagline which does not denounce any political or religious ideology is more likely to have a larger audience.** Thisis why the line of best fit has a positive slope. This is probably why there is large peak at the maximum neutrality point (1.0).", "## Compoundness of overview\n### Visualize relationship between compoundness of tagline and revenue of movies"], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/tmdb-box-office-comprehensive-eda\n\nsns.jointplot(x=compound, y=revenues, dropna=True, color='maroon', kind='reg')\nplt.show()", "processed": ["neutral taglin revenu movi seem posit correl probabl movi inclus movi taglin denounc polit religi ideolog like larger audienc thisi line best fit posit slope probabl larg peak maximum neutral point 1 0", "compound overview visual relationship compound taglin revenu movi"]}, {"markdown": ["There does not seem to be any apparent relationship between the compoundness (grammatical complexity) of the overview and its revenue.", "## Overview lengths"], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/tmdb-box-office-comprehensive-eda\n\nlengths = train_df['tagline'].apply(lambda x: len(str(x)))\nsns.jointplot(x=lengths, y=revenues, dropna=True, color='crimson')", "processed": ["seem appar relationship compound grammat complex overview revenu", "overview length"]}, {"markdown": ["# Helper Functions\n## Visualization"], "code": "# Reference: https://www.kaggle.com/code/xhlulu/flowers-tpu-concise-efficientnet-b7\n\ndef display_training_curves(training, validation, title, subplot):\n    if subplot%10==1: # set up the subplots on the first call\n        plt.subplots(figsize=(10,10), facecolor='#F0F0F0')\n        plt.tight_layout()\n    ax = plt.subplot(subplot)\n    ax.set_facecolor('#F8F8F8')\n    ax.plot(training)\n    ax.plot(validation)\n    ax.set_title('model '+ title)\n    ax.set_ylabel(title)\n    #ax.set_ylim(0.28,1.05)\n    ax.set_xlabel('epoch')\n    ax.legend(['train', 'valid.'])", "processed": ["helper function visual"]}, {"markdown": ["### damageDealt"], "code": "# Reference: https://www.kaggle.com/code/artgor/basic-eda\n\nprint('Max damage:', train['damageDealt'].max())\nprint('95% percentile:', np.percentile(train['damageDealt'], 95))\nprint('99% percentile:', np.percentile(train['damageDealt'], 99))\nprint('{0:.4f}% players dealt zero damage'.format((train['damageDealt'] == 0).sum()/ train.shape[0]))\nplt.figure(figsize=(12, 8))\nplt.hist(train.loc[train['damageDealt'] <= 800, 'damageDealt'], bins=40);\nplt.title('Distribution of damage dealt without outliers.');\ntrain.damageDealt.value_counts().head().iplot(kind='bar', title='Top 5 most common values of dealt damage')", "processed": ["damagedealt"]}, {"markdown": ["Well, most of players kill little enemies. "], "code": "# Reference: https://www.kaggle.com/code/artgor/basic-eda\n\ntrain.DBNOs.value_counts().head().iplot(kind='bar', title='Top 5 most common values of number of kills')", "processed": ["well player kill littl enemi"]}, {"markdown": ["It seems that distance travelled isn't really correlated with number of road kills.", "### Target"], "code": "# Reference: https://www.kaggle.com/code/artgor/basic-eda\n\ntrain['winPlacePerc'].plot(kind='hist');\nplt.title('Distibution of target.');", "processed": ["seem distanc travel realli correl number road kill", "target"]}, {"markdown": ["## Getting Prime Cities"], "code": "# Reference: https://www.kaggle.com/code/zikazika/greedy-algorithm-approach\n\ndef sieve_eratosthenes(n):\n    primes = [False, False] + [True for i in range(n-1)]\n    p = 2\n    while (p * p <= n):\n        if (primes[p] == True):\n            for i in range(p * 2, n + 1, p):\n                primes[i] = False\n        p += 1\n    return primes\nprimes = np.array(sieve_eratosthenes(nb_cities)).astype(int)\ndf['Prime'] = primes\npenalization = 0.1 * (1 - primes) + 1\ndf.head()\nplt.figure(figsize=(15, 10))\nsns.countplot(df.Prime)\nplt.title(\"Prime repartition : \" + str(Counter(df.Prime)))\nplt.show()", "processed": ["get prime citi"]}, {"markdown": ["**Target Distribution:**\n\nFirst let us look at the distribution of the target variable to understand more about the imbalance and so on."], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-notebook-qiqc\n\n## target count ##\ncnt_srs = train_df['target'].value_counts()\ntrace = go.Bar(\n    x=cnt_srs.index,\n    y=cnt_srs.values,\n    marker=dict(\n        color=cnt_srs.values,\n        colorscale = 'Picnic',\n        reversescale = True\n    ),\n)\n\nlayout = go.Layout(\n    title='Target Count',\n    font=dict(size=18)\n)\n\ndata = [trace]\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename=\"TargetCount\")\n\n## target distribution ##\nlabels = (np.array(cnt_srs.index))\nsizes = (np.array((cnt_srs / cnt_srs.sum())*100))\n\ntrace = go.Pie(labels=labels, values=sizes)\nlayout = go.Layout(\n    title='Target distribution',\n    font=dict(size=18),\n    width=600,\n    height=600,\n)\ndata = [trace]\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename=\"usertype\")", "processed": ["target distribut first let u look distribut target variabl understand imbal"]}, {"markdown": ["There seem to be a variety of words in there. May be it is a good idea to look at the most frequent words in each of the classes separately.\n\n**Word Frequency plot of sincere & insincere questions:****"], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-notebook-qiqc\n\nfrom collections import defaultdict\ntrain1_df = train_df[train_df[\"target\"]==1]\ntrain0_df = train_df[train_df[\"target\"]==0]\n\n## custom function for ngram generation ##\ndef generate_ngrams(text, n_gram=1):\n    token = [token for token in text.lower().split(\" \") if token != \"\" if token not in STOPWORDS]\n    ngrams = zip(*[token[i:] for i in range(n_gram)])\n    return [\" \".join(ngram) for ngram in ngrams]\n\n## custom function for horizontal bar chart ##\ndef horizontal_bar_chart(df, color):\n    trace = go.Bar(\n        y=df[\"word\"].values[::-1],\n        x=df[\"wordcount\"].values[::-1],\n        showlegend=False,\n        orientation = 'h',\n        marker=dict(\n            color=color,\n        ),\n    )\n    return trace\n\n## Get the bar chart from sincere questions ##\nfreq_dict = defaultdict(int)\nfor sent in train0_df[\"question_text\"]:\n    for word in generate_ngrams(sent):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace0 = horizontal_bar_chart(fd_sorted.head(50), 'blue')\n\n## Get the bar chart from insincere questions ##\nfreq_dict = defaultdict(int)\nfor sent in train1_df[\"question_text\"]:\n    for word in generate_ngrams(sent):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace1 = horizontal_bar_chart(fd_sorted.head(50), 'blue')\n\n# Creating two subplots\nfig = tools.make_subplots(rows=1, cols=2, vertical_spacing=0.04,\n                          subplot_titles=[\"Frequent words of sincere questions\", \n                                          \"Frequent words of insincere questions\"])\nfig.append_trace(trace0, 1, 1)\nfig.append_trace(trace1, 1, 2)\nfig['layout'].update(height=1200, width=900, paper_bgcolor='rgb(233,233,233)', title=\"Word Count Plots\")\npy.iplot(fig, filename='word-plots')\n\n#plt.figure(figsize=(10,16))\n#sns.barplot(x=\"ngram_count\", y=\"ngram\", data=fd_sorted.loc[:50,:], color=\"b\")\n#plt.title(\"Frequent words for Insincere Questions\", fontsize=16)\n#plt.show()\n", "processed": ["seem varieti word may good idea look frequent word class separ word frequenc plot sincer insincer question"]}, {"markdown": ["**Observations:**\n* Some of the top words are common across both the classes like 'people', 'will', 'think' etc\n* The other top words in sincere questions after excluding the common ones at the very top are 'best', 'good' etc\n* The other top words in insincere questions after excluding the common ones are 'trump', 'women', 'white' etc\n\nNow let us also create bigram frequency plots for both the classes separately to get more idea."], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-notebook-qiqc\n\nfreq_dict = defaultdict(int)\nfor sent in train0_df[\"question_text\"]:\n    for word in generate_ngrams(sent,2):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace0 = horizontal_bar_chart(fd_sorted.head(50), 'orange')\n\n\nfreq_dict = defaultdict(int)\nfor sent in train1_df[\"question_text\"]:\n    for word in generate_ngrams(sent,2):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace1 = horizontal_bar_chart(fd_sorted.head(50), 'orange')\n\n# Creating two subplots\nfig = tools.make_subplots(rows=1, cols=2, vertical_spacing=0.04,horizontal_spacing=0.15,\n                          subplot_titles=[\"Frequent bigrams of sincere questions\", \n                                          \"Frequent bigrams of insincere questions\"])\nfig.append_trace(trace0, 1, 1)\nfig.append_trace(trace1, 1, 2)\nfig['layout'].update(height=1200, width=900, paper_bgcolor='rgb(233,233,233)', title=\"Bigram Count Plots\")\npy.iplot(fig, filename='word-plots')\n", "processed": ["observ top word common across class like peopl think etc top word sincer question exclud common one top best good etc top word insincer question exclud common one trump woman white etc let u also creat bigram frequenc plot class separ get idea"]}, {"markdown": ["**Observations:**\n* The plot says it all. Please look at the plots and do the inference by yourselves ;)\n\nNow let usl look at the trigram plots as well."], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-notebook-qiqc\n\nfreq_dict = defaultdict(int)\nfor sent in train0_df[\"question_text\"]:\n    for word in generate_ngrams(sent,3):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace0 = horizontal_bar_chart(fd_sorted.head(50), 'green')\n\n\nfreq_dict = defaultdict(int)\nfor sent in train1_df[\"question_text\"]:\n    for word in generate_ngrams(sent,3):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace1 = horizontal_bar_chart(fd_sorted.head(50), 'green')\n\n# Creating two subplots\nfig = tools.make_subplots(rows=1, cols=2, vertical_spacing=0.04, horizontal_spacing=0.2,\n                          subplot_titles=[\"Frequent trigrams of sincere questions\", \n                                          \"Frequent trigrams of insincere questions\"])\nfig.append_trace(trace0, 1, 1)\nfig.append_trace(trace1, 1, 2)\nfig['layout'].update(height=1200, width=1200, paper_bgcolor='rgb(233,233,233)', title=\"Trigram Count Plots\")\npy.iplot(fig, filename='word-plots')", "processed": ["observ plot say plea look plot infer let usl look trigram plot well"]}, {"markdown": ["Now let us see how these meta features are distributed between both sincere and insincere questions."], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-notebook-qiqc\n\n## Truncate some extreme values for better visuals ##\ntrain_df['num_words'].loc[train_df['num_words']>60] = 60 #truncation for better visuals\ntrain_df['num_punctuations'].loc[train_df['num_punctuations']>10] = 10 #truncation for better visuals\ntrain_df['num_chars'].loc[train_df['num_chars']>350] = 350 #truncation for better visuals\n\nf, axes = plt.subplots(3, 1, figsize=(10,20))\nsns.boxplot(x='target', y='num_words', data=train_df, ax=axes[0])\naxes[0].set_xlabel('Target', fontsize=12)\naxes[0].set_title(\"Number of words in each class\", fontsize=15)\n\nsns.boxplot(x='target', y='num_chars', data=train_df, ax=axes[1])\naxes[1].set_xlabel('Target', fontsize=12)\naxes[1].set_title(\"Number of characters in each class\", fontsize=15)\n\nsns.boxplot(x='target', y='num_punctuations', data=train_df, ax=axes[2])\naxes[2].set_xlabel('Target', fontsize=12)\n#plt.ylabel('Number of punctuations in text', fontsize=12)\naxes[2].set_title(\"Number of punctuations in each class\", fontsize=15)\nplt.show()\n", "processed": ["let u see meta featur distribut sincer insincer question"]}, {"markdown": ["## 3. What is the distribution of labels in the Train Dataset"], "code": "# Reference: https://www.kaggle.com/code/shivamb/imaterialist-furniture-data-image-exploration\n\nplt.figure(figsize = (15, 8))\nplt.title('Distribuition of different labels in the train dataset')\nsns.distplot(train['label_id'], color=\"red\", kde=False);", "processed": ["3 distribut label train dataset"]}, {"markdown": ["## 4. Top 20 Labels of the dataset "], "code": "# Reference: https://www.kaggle.com/code/shivamb/imaterialist-furniture-data-image-exploration\n\nlabel_df = train.label_id.value_counts().reset_index()\nlabel_df['index'] = label_df['index'].astype(str)\nplt.figure(figsize=(15,8));\nsns.barplot(x=label_df['index'][:20], y=label_df['label_id'][:20], palette=\"Reds_d\");", "processed": ["4 top 20 label dataset"]}, {"markdown": ["## 6. Probable Objects predicted from object detection\n\nI used an ensemble of pre-trained ImageNet and TensorFlow Object Detection API models to detect the objects present in the images. More details are given in my [iMaterialist Fashion Kernel](https://www.kaggle.com/shivamb/imaterialist-fashion-eda-object-detection-colors)."], "code": "# Reference: https://www.kaggle.com/code/shivamb/imaterialist-furniture-data-image-exploration\n\nresults = {'12': {'url': u'https://img11.360buyimg.com/imgzone/jfs/t5119/73/1946159194/182429/9e2c2f9e/591674d9N548522ca.jpg', 'index': [u'table lamp', u'lamp', u'furniture', u'source of illumination', u'light brown color'], 'values': [0.909, 0.957, 0.957, 0.799, 0.846]}, '20': {'url': u'https://img14.360buyimg.com/imgzone/jfs/t2320/137/487468866/439466/bb0f64ce/561208eaNe5fce09d.jpg', 'index': [u'beverage', u'food', u'lager beer', u'beer', u'brew', u'alcoholic beverage', u'bottle green color'], 'values': [0.954, 0.954, 0.755, 0.776, 0.781, 0.783, 1]}, '21': {'url': u'http://wx4.sinaimg.cn/mw690/006boOKDgy1fjggzqwijlj30j60j640w.jpg', 'index': [u'sectional furniture', u'furniture', u'indoors', u'sage green color'], 'values': [0.888, 0.97, 0.783, 0.92]}, '38': {'url': u'https://www.uooyoo.com/img2017/9/26/2017092663144657.jpg', 'index': [u'fabric', u'sage green color'], 'values': [0.819, 0.902]}, '42': {'url': u'http://k.zol-img.com.cn/diybbs/6080/a6079920.jpg', 'index': [u'desk', u'table', u'furniture', u'computer', u'microscope', u'sage green color'], 'values': [0.914, 0.914, 0.914, 0.799, 0.789, 0.915]}, '3': {'url': u'https://img.alicdn.com/imgextra/TB2T9B3Xg1J.eBjy0FaXXaXeVXa_!!1945434197.jpg', 'index': [u'arm', u'support', u'armchair', u'chair', u'seat', u'furniture', u'coal black color'], 'values': [0.867, 0.921, 0.759, 0.847, 0.854, 0.854, 0.929]}, '122': {'url': u'https://static1.paizi.com/uploadfile/2017/1013/20171013053252466.jpg', 'index': [u'gray color'], 'values': [0.761]}, '89': {'url': u'https://img14.360buyimg.com/imgzone/jfs/t175/247/2029997415/239022/6e87243d/53c0fa8eN843d8932.jpg', 'index': [u'crown jewels', u'holding device', u'headdress', u'alabaster color', u'ivory color'], 'values': [0.812, 0.786, 0.801, 0.934, 0.814]}, '125': {'url': u'https://img13.360buyimg.com/imgzone/jfs/t3439/263/728869669/71540/eca6cade/5811bb8aN791655b1.jpg', 'index': [u'cup', u'drinking vessel', u'coal black color'], 'values': [0.864, 0.809, 0.953]}, '93': {'url': u'http://www.bvh.cc/images/200912/goods_img/547_P_1260577161314.jpg', 'index': [u'floor lamp', u'lamp', u'furniture'], 'values': [0.992, 0.994, 0.994]}, '92': {'url': u'https://img.alicdn.com/imgextra/TB2VGxwd3vD8KJjy0FlXXagBFXa_!!2529740865.jpg', 'index': [u'percale (fabric)', u'fabric', u'claret red color'], 'values': [0.8, 0.8, 0.942]}}\n\nl = label_df['index']\nx = l[0]\ny = results[x]\ndisplay(HTML(\"<div style='margin-left:100px'><h3>Label: \"+x+\"</h3><br><img src='\"+y['url']+\"' width=200 height=200></div>\"))\nsns.barplot(y=y['index'], x=y['values']);\nx = l[1]\ny = results[x]\ndisplay(HTML(\"<div style='margin-left:100px'><h3>Label: \"+x+\"</h3><br><img src='\"+y['url']+\"' width=200 height=200></div>\"))\nsns.barplot(y=y['index'], x=y['values']);\nx = l[3]\ny = results[x]\ndisplay(HTML(\"<div style='margin-left:100px'><h3>Label: \"+x+\"</h3><br><img src='\"+y['url']+\"' width=200 height=200></div>\"))\nsns.barplot(y=y['index'], x=y['values']);\nx = l[4]\ny = results[x]\ndisplay(HTML(\"<div style='margin-left:100px'><h3>Label: \"+x+\"</h3><br><img src='\"+y['url']+\"' width=200 height=200></div>\"))\nsns.barplot(y=y['index'], x=y['values']);\nx = l[5]\ny = results[x]\ndisplay(HTML(\"<div style='margin-left:100px'><h3>Label: \"+x+\"</h3><br><img src='\"+y['url']+\"' width=200 height=200></div>\"))\nsns.barplot(y=y['index'], x=y['values']);\nx = l[7]\ny = results[x]\ndisplay(HTML(\"<div style='margin-left:100px'><h3>Label: \"+x+\"</h3><br><img src='\"+y['url']+\"' width=200 height=200></div>\"))\nsns.barplot(y=y['index'], x=y['values']);\nx = l[9]\ny = results[x]\ndisplay(HTML(\"<div style='margin-left:100px'><h3>Label: \"+x+\"</h3><br><img src='\"+y['url']+\"' width=200 height=200></div>\"))\nsns.barplot(y=y['index'], x=y['values']);", "processed": ["6 probabl object predict object detect use ensembl pre train imagenet tensorflow object detect api model detect object present imag detail given imaterialist fashion kernel http www kaggl com shivamb imaterialist fashion eda object detect color"]}, {"markdown": ["The train dataset description is as follows:\n\n* item_id - Ad id.\n* user_id - User id.\n* region - Ad region.\n* city - Ad city.\n* parent_category_name - Top level ad category as classified by Avito's ad model.\n* category_name - Fine grain ad category as classified by Avito's ad model.\n* param_1 - Optional parameter from Avito's ad model.\n* param_2 - Optional parameter from Avito's ad model.\n* param_3 - Optional parameter from Avito's ad model.\n* title - Ad title.\n* description - Ad description.\n* price - Ad price.\n* item_seq_number - Ad sequential number for user.\n* activation_date- Date ad was placed.\n* user_type - User type.\n* image - Id code of image. Ties to a jpg file in train_jpg. Not every ad has an image.\n* image_top_1 - Avito's classification code for the image.\n* deal_probability - The target variable. This is the likelihood that an ad actually sold something. It's not possible to verify every transaction with certainty, so this column's value can be any float from zero to one.\n\nSo deal probability is our target variable and  is a float value between 0 and 1 as per the data page. Let us have a look at it. "], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-baseline-notebook-avito\n\nplt.figure(figsize=(12,8))\nsns.distplot(train_df[\"deal_probability\"].values, bins=100, kde=False)\nplt.xlabel('Deal Probility', fontsize=12)\nplt.title(\"Deal Probability Histogram\", fontsize=14)\nplt.show()\n\nplt.figure(figsize=(8,6))\nplt.scatter(range(train_df.shape[0]), np.sort(train_df['deal_probability'].values))\nplt.xlabel('index', fontsize=12)\nplt.ylabel('deal probability', fontsize=12)\nplt.title(\"Deal Probability Distribution\", fontsize=14)\nplt.show()", "processed": ["train dataset descript follow item id ad id user id user id region ad region citi ad citi parent categori name top level ad categori classifi avito ad model categori name fine grain ad categori classifi avito ad model param 1 option paramet avito ad model param 2 option paramet avito ad model param 3 option paramet avito ad model titl ad titl descript ad descript price ad price item seq number ad sequenti number user activ date date ad place user type user type imag id code imag tie jpg file train jpg everi ad imag imag top 1 avito classif code imag deal probabl target variabl likelihood ad actual sold someth possibl verifi everi transact certainti column valu float zero one deal probabl target variabl float valu 0 1 per data page let u look"]}, {"markdown": ["So almost 100K Ads has 0 probaility (which means it did not sell anything) and few ads have a probability of 1. Rest of the deal probabilities have values in between. \n\n**Region wise distribution of Ads:**\n\nLet us look at the region wise distribution of ads. "], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-baseline-notebook-avito\n\nfrom io import StringIO\n\ntemp_data = StringIO(\"\"\"\nregion,region_en\n\u0421\u0432\u0435\u0440\u0434\u043b\u043e\u0432\u0441\u043a\u0430\u044f \u043e\u0431\u043b\u0430\u0441\u0442\u044c, Sverdlovsk oblast\n\u0421\u0430\u043c\u0430\u0440\u0441\u043a\u0430\u044f \u043e\u0431\u043b\u0430\u0441\u0442\u044c, Samara oblast\n\u0420\u043e\u0441\u0442\u043e\u0432\u0441\u043a\u0430\u044f \u043e\u0431\u043b\u0430\u0441\u0442\u044c, Rostov oblast\n\u0422\u0430\u0442\u0430\u0440\u0441\u0442\u0430\u043d, Tatarstan\n\u0412\u043e\u043b\u0433\u043e\u0433\u0440\u0430\u0434\u0441\u043a\u0430\u044f \u043e\u0431\u043b\u0430\u0441\u0442\u044c, Volgograd oblast\n\u041d\u0438\u0436\u0435\u0433\u043e\u0440\u043e\u0434\u0441\u043a\u0430\u044f \u043e\u0431\u043b\u0430\u0441\u0442\u044c, Nizhny Novgorod oblast\n\u041f\u0435\u0440\u043c\u0441\u043a\u0438\u0439 \u043a\u0440\u0430\u0439, Perm Krai\n\u041e\u0440\u0435\u043d\u0431\u0443\u0440\u0433\u0441\u043a\u0430\u044f \u043e\u0431\u043b\u0430\u0441\u0442\u044c, Orenburg oblast\n\u0425\u0430\u043d\u0442\u044b-\u041c\u0430\u043d\u0441\u0438\u0439\u0441\u043a\u0438\u0439 \u0410\u041e, Khanty-Mansi Autonomous Okrug\n\u0422\u044e\u043c\u0435\u043d\u0441\u043a\u0430\u044f \u043e\u0431\u043b\u0430\u0441\u0442\u044c, Tyumen oblast\n\u0411\u0430\u0448\u043a\u043e\u0440\u0442\u043e\u0441\u0442\u0430\u043d, Bashkortostan\n\u041a\u0440\u0430\u0441\u043d\u043e\u0434\u0430\u0440\u0441\u043a\u0438\u0439 \u043a\u0440\u0430\u0439, Krasnodar Krai\n\u041d\u043e\u0432\u043e\u0441\u0438\u0431\u0438\u0440\u0441\u043a\u0430\u044f \u043e\u0431\u043b\u0430\u0441\u0442\u044c, Novosibirsk oblast\n\u041e\u043c\u0441\u043a\u0430\u044f \u043e\u0431\u043b\u0430\u0441\u0442\u044c, Omsk oblast\n\u0411\u0435\u043b\u0433\u043e\u0440\u043e\u0434\u0441\u043a\u0430\u044f \u043e\u0431\u043b\u0430\u0441\u0442\u044c, Belgorod oblast\n\u0427\u0435\u043b\u044f\u0431\u0438\u043d\u0441\u043a\u0430\u044f \u043e\u0431\u043b\u0430\u0441\u0442\u044c, Chelyabinsk oblast\n\u0412\u043e\u0440\u043e\u043d\u0435\u0436\u0441\u043a\u0430\u044f \u043e\u0431\u043b\u0430\u0441\u0442\u044c, Voronezh oblast\n\u041a\u0435\u043c\u0435\u0440\u043e\u0432\u0441\u043a\u0430\u044f \u043e\u0431\u043b\u0430\u0441\u0442\u044c, Kemerovo oblast\n\u0421\u0430\u0440\u0430\u0442\u043e\u0432\u0441\u043a\u0430\u044f \u043e\u0431\u043b\u0430\u0441\u0442\u044c, Saratov oblast\n\u0412\u043b\u0430\u0434\u0438\u043c\u0438\u0440\u0441\u043a\u0430\u044f \u043e\u0431\u043b\u0430\u0441\u0442\u044c, Vladimir oblast\n\u041a\u0430\u043b\u0438\u043d\u0438\u043d\u0433\u0440\u0430\u0434\u0441\u043a\u0430\u044f \u043e\u0431\u043b\u0430\u0441\u0442\u044c, Kaliningrad oblast\n\u041a\u0440\u0430\u0441\u043d\u043e\u044f\u0440\u0441\u043a\u0438\u0439 \u043a\u0440\u0430\u0439, Krasnoyarsk Krai\n\u042f\u0440\u043e\u0441\u043b\u0430\u0432\u0441\u043a\u0430\u044f \u043e\u0431\u043b\u0430\u0441\u0442\u044c, Yaroslavl oblast\n\u0423\u0434\u043c\u0443\u0440\u0442\u0438\u044f, Udmurtia\n\u0410\u043b\u0442\u0430\u0439\u0441\u043a\u0438\u0439 \u043a\u0440\u0430\u0439, Altai Krai\n\u0418\u0440\u043a\u0443\u0442\u0441\u043a\u0430\u044f \u043e\u0431\u043b\u0430\u0441\u0442\u044c, Irkutsk oblast\n\u0421\u0442\u0430\u0432\u0440\u043e\u043f\u043e\u043b\u044c\u0441\u043a\u0438\u0439 \u043a\u0440\u0430\u0439, Stavropol Krai\n\u0422\u0443\u043b\u044c\u0441\u043a\u0430\u044f \u043e\u0431\u043b\u0430\u0441\u0442\u044c, Tula oblast\n\"\"\")\n\nregion_df = pd.read_csv(temp_data)\ntrain_df = pd.merge(train_df, region_df, how=\"left\", on=\"region\")\ntemp_series = train_df['region_en'].value_counts()\nlabels = (np.array(temp_series.index))\nsizes = (np.array((temp_series / temp_series.sum())*100))\n\ntrace = go.Pie(labels=labels, values=sizes)\nlayout = go.Layout(\n    title='Region distribution',\n    width=900,\n    height=900,\n)\ndata = [trace]\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename=\"region\")", "processed": ["almost 100k ad 0 probail mean sell anyth ad probabl 1 rest deal probabl valu region wise distribut ad let u look region wise distribut ad"]}, {"markdown": ["The regions have percentage of ads between 1.71% to 9.41%. So the top regions are:\n1. Krasnodar region - 9.41%\n2. Sverdlovsk region - 6.28%\n3. Rostov region - 5.99%\n"], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-baseline-notebook-avito\n\nplt.figure(figsize=(12,8))\nsns.boxplot(y=\"region_en\", x=\"deal_probability\", data=train_df)\nplt.xlabel('Deal probability', fontsize=12)\nplt.ylabel('Region', fontsize=12)\nplt.title(\"Deal probability by region\")\nplt.xticks(rotation='vertical')\nplt.show()", "processed": ["region percentag ad 1 71 9 41 top region 1 krasnodar region 9 41 2 sverdlovsk region 6 28 3 rostov region 5 99"]}, {"markdown": ["**City wise distribution of Ads:**\n\nNow let us have a look at the top 20 cities present in the dataset."], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-baseline-notebook-avito\n\ncnt_srs = train_df['city'].value_counts().head(20)\ntrace = go.Bar(\n    y=cnt_srs.index[::-1],\n    x=cnt_srs.values[::-1],\n    orientation = 'h',\n    marker=dict(\n        color=cnt_srs.values[::-1],\n        colorscale = 'Blues',\n        reversescale = True\n    ),\n)\n\nlayout = dict(\n    title='City distribution of Ads',\n    )\ndata = [trace]\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename=\"CityAds\")", "processed": ["citi wise distribut ad let u look top 20 citi present dataset"]}, {"markdown": ["So the top cities where the ads are shown are\n1. Krasnodar\n2. Ekaterinburg\n3. Novosibirsk", "**Parent Category Name:**\n\nNow let us look at the distribution of parent cateory names."], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-baseline-notebook-avito\n\ntemp_data = StringIO(\"\"\"\nparent_category_name,parent_category_name_en\n\u041b\u0438\u0447\u043d\u044b\u0435 \u0432\u0435\u0449\u0438,Personal belongings\n\u0414\u043b\u044f \u0434\u043e\u043c\u0430 \u0438 \u0434\u0430\u0447\u0438,For the home and garden\n\u0411\u044b\u0442\u043e\u0432\u0430\u044f \u044d\u043b\u0435\u043a\u0442\u0440\u043e\u043d\u0438\u043a\u0430,Consumer electronics\n\u041d\u0435\u0434\u0432\u0438\u0436\u0438\u043c\u043e\u0441\u0442\u044c,Real estate\n\u0425\u043e\u0431\u0431\u0438 \u0438 \u043e\u0442\u0434\u044b\u0445,Hobbies & leisure\n\u0422\u0440\u0430\u043d\u0441\u043f\u043e\u0440\u0442,Transport\n\u0423\u0441\u043b\u0443\u0433\u0438,Services\n\u0416\u0438\u0432\u043e\u0442\u043d\u044b\u0435,Animals\n\u0414\u043b\u044f \u0431\u0438\u0437\u043d\u0435\u0441\u0430,For business\n\"\"\")\n\ntemp_df = pd.read_csv(temp_data)\ntrain_df = pd.merge(train_df, temp_df, on=\"parent_category_name\", how=\"left\")\ntemp_series = train_df['parent_category_name_en'].value_counts()\nlabels = (np.array(temp_series.index))\nsizes = (np.array((temp_series / temp_series.sum())*100))\n\ntrace = go.Pie(labels=labels, values=sizes)\nlayout = go.Layout(\n    title='Parent Category distribution',\n    width=900,\n    height=900,\n)\ndata = [trace]\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename=\"parentcategory\")", "processed": ["top citi ad shown 1 krasnodar 2 ekaterinburg 3 novosibirsk", "parent categori name let u look distribut parent cateori name"]}, {"markdown": ["1So 46.4% of the ads are for Personal belongings, 11.9% are for home and garden and 11.5% for consumer electronics."], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-baseline-notebook-avito\n\nplt.figure(figsize=(12,8))\nsns.boxplot(x=\"parent_category_name_en\", y=\"deal_probability\", data=train_df)\nplt.ylabel('Deal probability', fontsize=12)\nplt.xlabel('Parent Category', fontsize=12)\nplt.title(\"Deal probability by parent category\", fontsize=14)\nplt.xticks(rotation='vertical')\nplt.show()", "processed": ["1so 46 4 ad person belong 11 9 home garden 11 5 consum electron"]}, {"markdown": ["Services category seems to have slightly higher deal probability compared to others. ", "** Category of Ads:**\n\nNow let us look at the category of ads."], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-baseline-notebook-avito\n\ntemp_data = StringIO(\"\"\"\ncategory_name,category_name_en\n\"\u041e\u0434\u0435\u0436\u0434\u0430, \u043e\u0431\u0443\u0432\u044c, \u0430\u043a\u0441\u0435\u0441\u0441\u0443\u0430\u0440\u044b\",\"Clothing, shoes, accessories\"\n\u0414\u0435\u0442\u0441\u043a\u0430\u044f \u043e\u0434\u0435\u0436\u0434\u0430 \u0438 \u043e\u0431\u0443\u0432\u044c,Children's clothing and shoes\n\u0422\u043e\u0432\u0430\u0440\u044b \u0434\u043b\u044f \u0434\u0435\u0442\u0435\u0439 \u0438 \u0438\u0433\u0440\u0443\u0448\u043a\u0438,Children's products and toys\n\u041a\u0432\u0430\u0440\u0442\u0438\u0440\u044b,Apartments\n\u0422\u0435\u043b\u0435\u0444\u043e\u043d\u044b,Phones\n\u041c\u0435\u0431\u0435\u043b\u044c \u0438 \u0438\u043d\u0442\u0435\u0440\u044c\u0435\u0440,Furniture and interior\n\u041f\u0440\u0435\u0434\u043b\u043e\u0436\u0435\u043d\u0438\u0435 \u0443\u0441\u043b\u0443\u0433,Offer services\n\u0410\u0432\u0442\u043e\u043c\u043e\u0431\u0438\u043b\u0438,Cars\n\u0420\u0435\u043c\u043e\u043d\u0442 \u0438 \u0441\u0442\u0440\u043e\u0438\u0442\u0435\u043b\u044c\u0441\u0442\u0432\u043e,Repair and construction\n\u0411\u044b\u0442\u043e\u0432\u0430\u044f \u0442\u0435\u0445\u043d\u0438\u043a\u0430,Appliances\n\u0422\u043e\u0432\u0430\u0440\u044b \u0434\u043b\u044f \u043a\u043e\u043c\u043f\u044c\u044e\u0442\u0435\u0440\u0430,Products for computer\n\"\u0414\u043e\u043c\u0430, \u0434\u0430\u0447\u0438, \u043a\u043e\u0442\u0442\u0435\u0434\u0436\u0438\",\"Houses, villas, cottages\"\n\u041a\u0440\u0430\u0441\u043e\u0442\u0430 \u0438 \u0437\u0434\u043e\u0440\u043e\u0432\u044c\u0435,Health and beauty\n\u0410\u0443\u0434\u0438\u043e \u0438 \u0432\u0438\u0434\u0435\u043e,Audio and video\n\u0421\u043f\u043e\u0440\u0442 \u0438 \u043e\u0442\u0434\u044b\u0445,Sports and recreation\n\u041a\u043e\u043b\u043b\u0435\u043a\u0446\u0438\u043e\u043d\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435,Collecting\n\u041e\u0431\u043e\u0440\u0443\u0434\u043e\u0432\u0430\u043d\u0438\u0435 \u0434\u043b\u044f \u0431\u0438\u0437\u043d\u0435\u0441\u0430,Equipment for business\n\u0417\u0435\u043c\u0435\u043b\u044c\u043d\u044b\u0435 \u0443\u0447\u0430\u0441\u0442\u043a\u0438,Land\n\u0427\u0430\u0441\u044b \u0438 \u0443\u043a\u0440\u0430\u0448\u0435\u043d\u0438\u044f,Watches and jewelry\n\u041a\u043d\u0438\u0433\u0438 \u0438 \u0436\u0443\u0440\u043d\u0430\u043b\u044b,Books and magazines\n\u0421\u043e\u0431\u0430\u043a\u0438,Dogs\n\"\u0418\u0433\u0440\u044b, \u043f\u0440\u0438\u0441\u0442\u0430\u0432\u043a\u0438 \u0438 \u043f\u0440\u043e\u0433\u0440\u0430\u043c\u043c\u044b\",\"Games, consoles and software\"\n\u0414\u0440\u0443\u0433\u0438\u0435 \u0436\u0438\u0432\u043e\u0442\u043d\u044b\u0435,Other animals\n\u0412\u0435\u043b\u043e\u0441\u0438\u043f\u0435\u0434\u044b,Bikes\n\u041d\u043e\u0443\u0442\u0431\u0443\u043a\u0438,Laptops\n\u041a\u043e\u0448\u043a\u0438,Cats\n\u0413\u0440\u0443\u0437\u043e\u0432\u0438\u043a\u0438 \u0438 \u0441\u043f\u0435\u0446\u0442\u0435\u0445\u043d\u0438\u043a\u0430,Trucks and buses\n\u041f\u043e\u0441\u0443\u0434\u0430 \u0438 \u0442\u043e\u0432\u0430\u0440\u044b \u0434\u043b\u044f \u043a\u0443\u0445\u043d\u0438,Tableware and goods for kitchen\n\u0420\u0430\u0441\u0442\u0435\u043d\u0438\u044f,Plants\n\u041f\u043b\u0430\u043d\u0448\u0435\u0442\u044b \u0438 \u044d\u043b\u0435\u043a\u0442\u0440\u043e\u043d\u043d\u044b\u0435 \u043a\u043d\u0438\u0433\u0438,Tablets and e-books\n\u0422\u043e\u0432\u0430\u0440\u044b \u0434\u043b\u044f \u0436\u0438\u0432\u043e\u0442\u043d\u044b\u0445,Pet products\n\u041a\u043e\u043c\u043d\u0430\u0442\u044b,Room\n\u0424\u043e\u0442\u043e\u0442\u0435\u0445\u043d\u0438\u043a\u0430,Photo\n\u041a\u043e\u043c\u043c\u0435\u0440\u0447\u0435\u0441\u043a\u0430\u044f \u043d\u0435\u0434\u0432\u0438\u0436\u0438\u043c\u043e\u0441\u0442\u044c,Commercial property\n\u0413\u0430\u0440\u0430\u0436\u0438 \u0438 \u043c\u0430\u0448\u0438\u043d\u043e\u043c\u0435\u0441\u0442\u0430,Garages and Parking spaces\n\u041c\u0443\u0437\u044b\u043a\u0430\u043b\u044c\u043d\u044b\u0435 \u0438\u043d\u0441\u0442\u0440\u0443\u043c\u0435\u043d\u0442\u044b,Musical instruments\n\u041e\u0440\u0433\u0442\u0435\u0445\u043d\u0438\u043a\u0430 \u0438 \u0440\u0430\u0441\u0445\u043e\u0434\u043d\u0438\u043a\u0438,Office equipment and consumables\n\u041f\u0442\u0438\u0446\u044b,Birds\n\u041f\u0440\u043e\u0434\u0443\u043a\u0442\u044b \u043f\u0438\u0442\u0430\u043d\u0438\u044f,Food\n\u041c\u043e\u0442\u043e\u0446\u0438\u043a\u043b\u044b \u0438 \u043c\u043e\u0442\u043e\u0442\u0435\u0445\u043d\u0438\u043a\u0430,Motorcycles and bikes\n\u041d\u0430\u0441\u0442\u043e\u043b\u044c\u043d\u044b\u0435 \u043a\u043e\u043c\u043f\u044c\u044e\u0442\u0435\u0440\u044b,Desktop computers\n\u0410\u043a\u0432\u0430\u0440\u0438\u0443\u043c,Aquarium\n\u041e\u0445\u043e\u0442\u0430 \u0438 \u0440\u044b\u0431\u0430\u043b\u043a\u0430,Hunting and fishing\n\u0411\u0438\u043b\u0435\u0442\u044b \u0438 \u043f\u0443\u0442\u0435\u0448\u0435\u0441\u0442\u0432\u0438\u044f,Tickets and travel\n\u0412\u043e\u0434\u043d\u044b\u0439 \u0442\u0440\u0430\u043d\u0441\u043f\u043e\u0440\u0442,Water transport\n\u0413\u043e\u0442\u043e\u0432\u044b\u0439 \u0431\u0438\u0437\u043d\u0435\u0441,Ready business\n\u041d\u0435\u0434\u0432\u0438\u0436\u0438\u043c\u043e\u0441\u0442\u044c \u0437\u0430 \u0440\u0443\u0431\u0435\u0436\u043e\u043c,Property abroad\n\"\"\")\n\ntemp_df = pd.read_csv(temp_data)\ntrain_df = pd.merge(train_df, temp_df, on=\"category_name\", how=\"left\")\ncnt_srs = train_df['category_name_en'].value_counts()\ntrace = go.Bar(\n    y=cnt_srs.index[::-1],\n    x=cnt_srs.values[::-1],\n    orientation = 'h',\n    marker=dict(\n        color=cnt_srs.values[::-1],\n        colorscale = 'Blues',\n        reversescale = True\n    ),\n)\n\nlayout = dict(\n    title='Category Name of Ads - Count',\n    height=900\n    )\ndata = [trace]\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename=\"category name\")", "processed": ["servic categori seem slightli higher deal probabl compar other", "categori ad let u look categori ad"]}, {"markdown": ["So the top 3 categories are:\n1. Clothes, shoes, accessories\n2. Children's clothing and footwear\n3. Goods for children and toys\n", "** User Type:**\n\nNow let us look at the user type. "], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-baseline-notebook-avito\n\ntemp_series = train_df['user_type'].value_counts()\nlabels = (np.array(temp_series.index))\nsizes = (np.array((temp_series / temp_series.sum())*100))\n\ntrace = go.Pie(labels=labels, values=sizes)\nlayout = go.Layout(\n    title='User Type distribution',\n    width=600,\n    height=600,\n)\ndata = [trace]\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename=\"usertype\")", "processed": ["top 3 categori 1 cloth shoe accessori 2 child cloth footwear 3 good child toy", "user type let u look user type"]}, {"markdown": ["Private users constitute 72% of the data followed by company and shop.\n\n**Price:**\n\nThis is the price shown in the Ad. "], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-baseline-notebook-avito\n\ntrain_df[\"price_new\"] = train_df[\"price\"].values\ntrain_df[\"price_new\"].fillna(np.nanmean(train_df[\"price\"].values), inplace=True)\n\nplt.figure(figsize=(12,8))\nsns.distplot(np.log1p(train_df[\"price_new\"].values), bins=100, kde=False)\nplt.xlabel('Log of price', fontsize=12)\nplt.title(\"Log of Price Histogram\", fontsize=14)\nplt.show()", "processed": ["privat user constitut 72 data follow compani shop price price shown ad"]}, {"markdown": ["**Activation Date:**"], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-baseline-notebook-avito\n\ncnt_srs = train_df['activation_date'].value_counts()\n\ntrace = go.Bar(\n    x=cnt_srs.index,\n    y=cnt_srs.values,\n    marker=dict(\n        color=cnt_srs.values,\n        colorscale = 'Picnic',\n        reversescale = True\n    ),\n)\n\nlayout = go.Layout(\n    title='Activation Dates in Train'\n)\n\ndata = [trace]\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename=\"ActivationDate\")\n\n# Activation dates in test\ncnt_srs = test_df['activation_date'].value_counts()\n\ntrace = go.Bar(\n    x=cnt_srs.index,\n    y=cnt_srs.values,\n    marker=dict(\n        color=cnt_srs.values,\n        colorscale = 'Picnic',\n        reversescale = True\n    ),\n)\n\nlayout = go.Layout(\n    title='Activation Dates in Test'\n)\n\ndata = [trace]\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename=\"ActivationDate\")", "processed": ["activ date"]}, {"markdown": ["We have around 64K common titles between train and test set. Now let us look at the number of words present in the title column."], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-baseline-notebook-avito\n\ntrain_df[\"title_nwords\"] = train_df[\"title\"].apply(lambda x: len(x.split()))\ntest_df[\"title_nwords\"] = test_df[\"title\"].apply(lambda x: len(x.split()))\n\ncnt_srs = train_df['title_nwords'].value_counts()\n\ntrace = go.Bar(\n    x=cnt_srs.index,\n    y=cnt_srs.values,\n    marker=dict(\n        color=\"blue\",\n        #colorscale = 'Blues',\n        reversescale = True\n    ),\n)\n\nlayout = go.Layout(\n    title='Number of words in title column'\n)\n\ndata = [trace]\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename=\"title_nwords\")           ", "processed": ["around 64k common titl train test set let u look number word present titl column"]}, {"markdown": ["Majority of the tiles have 1, 2 or 3 words and has a long tail.\n\nNow we will do the following:\n1. Take the TF-IDF of the title column and this will be a sparse matrix with huge dimesnions.\n2. Get the top SVD components fof this TF-IDF \n3. Plot the distribution of SVD components with Deal probability to see if these variables help."], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-baseline-notebook-avito\n\n### TFIDF Vectorizer ###\ntfidf_vec = TfidfVectorizer(ngram_range=(1,1))\nfull_tfidf = tfidf_vec.fit_transform(train_df['title'].values.tolist() + test_df['title'].values.tolist())\ntrain_tfidf = tfidf_vec.transform(train_df['title'].values.tolist())\ntest_tfidf = tfidf_vec.transform(test_df['title'].values.tolist())\n\n### SVD Components ###\nn_comp = 3\nsvd_obj = TruncatedSVD(n_components=n_comp, algorithm='arpack')\nsvd_obj.fit(full_tfidf)\ntrain_svd = pd.DataFrame(svd_obj.transform(train_tfidf))\ntest_svd = pd.DataFrame(svd_obj.transform(test_tfidf))\ntrain_svd.columns = ['svd_title_'+str(i+1) for i in range(n_comp)]\ntest_svd.columns = ['svd_title_'+str(i+1) for i in range(n_comp)]\ntrain_df = pd.concat([train_df, train_svd], axis=1)\ntest_df = pd.concat([test_df, test_svd], axis=1)\ndel full_tfidf, train_tfidf, test_tfidf, train_svd, test_svd\n# 1st svd comp #\nplt.figure(figsize=(8,8))\nsns.jointplot(x=train_df[\"svd_title_1\"].values, y=train_df[\"deal_probability\"].values, size=10)\nplt.ylabel('Deal Probability', fontsize=12)\nplt.xlabel('First SVD component on Title', fontsize=12)\nplt.title(\"Deal Probability distribution for First SVD component on title\", fontsize=15)\nplt.show()\n\n# 2nd svd comp #\nplt.figure(figsize=(8,8))\nsns.jointplot(x=train_df[\"svd_title_2\"].values, y=train_df[\"deal_probability\"].values, size=10)\nplt.ylabel('Deal Probability', fontsize=12)\nplt.xlabel('Second SVD component on Title', fontsize=12)\nplt.title(\"Deal Probability distribution for Second SVD component on title\", fontsize=15)\nplt.show()\n\n# 3rd svd comp #\nplt.figure(figsize=(8,8))\nsns.jointplot(x=train_df[\"svd_title_3\"].values, y=train_df[\"deal_probability\"].values, size=10)\nplt.ylabel('Deal Probability', fontsize=12)\nplt.xlabel('Third SVD component on Title', fontsize=12)\nplt.title(\"Deal Probability distribution for Third SVD component on title\", fontsize=15)\nplt.show()", "processed": ["major tile 1 2 3 word long tail follow 1 take tf idf titl column spar matrix huge dimesnion 2 get top svd compon fof tf idf 3 plot distribut svd compon deal probabl see variabl help"]}, {"markdown": ["As we can see, the top SVD components capture quite an amount of variation in the data. So this might be helpful features in our modeling process.", "**Description:**\n\nLet us first check the number of words in the description column.\n"], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-baseline-notebook-avito\n\n## Filling missing values ##\ntrain_df[\"description\"].fillna(\"NA\", inplace=True)\ntest_df[\"description\"].fillna(\"NA\", inplace=True)\n\ntrain_df[\"desc_nwords\"] = train_df[\"description\"].apply(lambda x: len(x.split()))\ntest_df[\"desc_nwords\"] = test_df[\"description\"].apply(lambda x: len(x.split()))\n\ncnt_srs = train_df['desc_nwords'].value_counts().head(100)\n\ntrace = go.Bar(\n    x=cnt_srs.index,\n    y=cnt_srs.values,\n    marker=dict(\n        color=\"blue\",\n        #colorscale = 'Blues',\n        reversescale = True\n    ),\n)\n\nlayout = go.Layout(\n    title='Number of words in Description column'\n)\n\ndata = [trace]\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename=\"desc_nwords\")  ", "processed": ["see top svd compon captur quit amount variat data might help featur model process", "descript let u first check number word descript column"]}, {"markdown": ["Description column has a huge right tail till about 700 words. I have cut the same till top 100 for better visualization.\n\nNow let us create the first 3 SVD components for description column (just like title) and plot the same against deal probability."], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-baseline-notebook-avito\n\n### TFIDF Vectorizer ###\ntfidf_vec = TfidfVectorizer(ngram_range=(1,1), max_features=100000)\nfull_tfidf = tfidf_vec.fit_transform(train_df['description'].values.tolist() + test_df['description'].values.tolist())\ntrain_tfidf = tfidf_vec.transform(train_df['description'].values.tolist())\ntest_tfidf = tfidf_vec.transform(test_df['description'].values.tolist())\n\n### SVD Components ###\nn_comp = 3\nsvd_obj = TruncatedSVD(n_components=n_comp, algorithm='arpack')\nsvd_obj.fit(full_tfidf)\ntrain_svd = pd.DataFrame(svd_obj.transform(train_tfidf))\ntest_svd = pd.DataFrame(svd_obj.transform(test_tfidf))\ntrain_svd.columns = ['svd_desc_'+str(i+1) for i in range(n_comp)]\ntest_svd.columns = ['svd_desc_'+str(i+1) for i in range(n_comp)]\ntrain_df = pd.concat([train_df, train_svd], axis=1)\ntest_df = pd.concat([test_df, test_svd], axis=1)\ndel full_tfidf, train_tfidf, test_tfidf, train_svd, test_svd\n# 1st svd comp #\nplt.figure(figsize=(8,8))\nsns.jointplot(x=train_df[\"svd_desc_1\"].values, y=train_df[\"deal_probability\"].values, size=10)\nplt.ylabel('Deal Probability', fontsize=12)\nplt.xlabel('First SVD component on Description', fontsize=12)\nplt.title(\"Deal Probability distribution for First SVD component on Description\", fontsize=15)\nplt.show()\n\n# 2nd svd comp #\nplt.figure(figsize=(8,8))\nsns.jointplot(x=train_df[\"svd_desc_2\"].values, y=train_df[\"deal_probability\"].values, size=10)\nplt.ylabel('Deal Probability', fontsize=12)\nplt.xlabel('Second SVD component on Description', fontsize=12)\nplt.title(\"Deal Probability distribution for Second SVD component on title\", fontsize=15)\nplt.show()\n\n# 3rd svd comp #\nplt.figure(figsize=(8,8))\nsns.jointplot(x=train_df[\"svd_desc_3\"].values, y=train_df[\"deal_probability\"].values, size=10)\nplt.ylabel('Deal Probability', fontsize=12)\nplt.xlabel('Second SVD component on Description', fontsize=12)\nplt.title(\"Deal Probability distribution for Third SVD component on Description\", fontsize=15)\nplt.show()", "processed": ["descript column huge right tail till 700 word cut till top 100 better visual let u creat first 3 svd compon descript column like titl plot deal probabl"]}, {"markdown": ["Wow. This is huge.! Before we analyze more about the text, let us first check the class distribution.\n\n**Class Distribution:**"], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-notebook-personalized-medicine\n\nplt.figure(figsize=(12,8))\nsns.countplot(x=\"Class\", data=train_variants_df)\nplt.ylabel('Frequency', fontsize=12)\nplt.xlabel('Class Count', fontsize=12)\nplt.xticks(rotation='vertical')\nplt.title(\"Frequency of Classes\", fontsize=15)\nplt.show()", "processed": ["wow huge analyz text let u first check class distribut class distribut"]}, {"markdown": ["Let us look at the distribution of number of words in the text column."], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-notebook-personalized-medicine\n\nplt.figure(figsize=(12, 8))\nsns.distplot(train_text_df.Text_num_words.values, bins=50, kde=False, color='red')\nplt.xlabel('Number of words in text', fontsize=12)\nplt.ylabel('Count', fontsize=12)\nplt.title(\"Frequency of number of words\", fontsize=15)\nplt.show()", "processed": ["let u look distribut number word text column"]}, {"markdown": ["The peak is around 4000 words. Now let us look at character level."], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-notebook-personalized-medicine\n\nplt.figure(figsize=(12, 8))\nsns.distplot(train_text_df.Text_num_chars.values, bins=50, kde=False, color='brown')\nplt.xlabel('Number of characters in text', fontsize=12)\nplt.ylabel('log of Count', fontsize=12)\nplt.title(\"Frequency of Number of characters\", fontsize=15)\nplt.show()", "processed": ["peak around 4000 word let u look charact level"]}, {"markdown": ["The distribution is similar to the previous one. \n\nLet us now check if we could use the number of words in the text has predictive power."], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-notebook-personalized-medicine\n\ntrain_df = pd.merge(train_variants_df, train_text_df, on='ID')\n\nplt.figure(figsize=(12,8))\nsns.boxplot(x='Class', y='Text_num_words', data=train_df)\nplt.xlabel('Class', fontsize=12)\nplt.ylabel('Text - Number of words', fontsize=12)\nplt.show()", "processed": ["distribut similar previou one let u check could use number word text predict power"]}, {"markdown": ["Wow, many **useless** variables can predict our random synthetic target with greater than 0.600 AUC. That's crazy. Let's plot synthetic variables 133 and 162 together."], "code": "# Reference: https://www.kaggle.com/code/cdeotte/can-we-trust-cv-and-lb\n\nplt.figure(figsize=(5,5))\nplt.scatter(train[133],train[162],c=train['target'])\nplt.plot([-2,2],[-2,2],':k')\nplt.title('Among 300 simulated useless variables, we find these two!')\nplt.xlabel('synthetic variable 133')\nplt.ylabel('synthetic variable 162')\nplt.show()", "processed": ["wow mani useless variabl predict random synthet target greater 0 600 auc crazi let plot synthet variabl 133 162 togeth"]}, {"markdown": ["We can make few observations here:   \n\n* standard deviation is very close to 1 for all features, in both train and test set;  \n* all features are approximately centered to 0, with mean values close to 0;  \n* min and max absolute values for features in train data looks to be smaller than the ones for the test data;  \n* mean value for target variable is 0.64 which will imply that 64% of target values are 1.\n\n", "Let's check the distribution of **target** value in train dataset."], "code": "# Reference: https://www.kaggle.com/code/gpreda/overfitting-the-private-leaderboard\n\nsns.countplot(train_df['target'])\nprint(\"There are {}% target values with 1\".format(100 * train_df[\"target\"].value_counts()[1]/train_df.shape[0]))", "processed": ["make observ standard deviat close 1 featur train test set featur approxim center 0 mean valu close 0 min max absolut valu featur train data look smaller one test data mean valu target variabl 0 64 impli 64 target valu 1", "let check distribut target valu train dataset"]}, {"markdown": ["\n## <a id='32'>Density plots of features</a>  \n\nLet's show now the density plot of variables in train dataset. \n\nWe represent with different colors the distribution for values with **target** value **0** and **1**."], "code": "# Reference: https://www.kaggle.com/code/gpreda/overfitting-the-private-leaderboard\n\ndef plot_feature_distribution(df1, df2, label1, label2, features):\n    i = 0\n    sns.set_style('whitegrid')\n    plt.figure()\n    fig, ax = plt.subplots(10,10,figsize=(18,22))\n\n    for feature in features:\n        i += 1\n        plt.subplot(10,10,i)\n        sns.kdeplot(df1[feature], bw=0.5,label=label1)\n        sns.kdeplot(df2[feature], bw=0.5,label=label2)\n        plt.xlabel(feature, fontsize=9)\n        locs, labels = plt.xticks()\n        plt.tick_params(axis='x', which='major', labelsize=6, pad=-6)\n        plt.tick_params(axis='y', which='major', labelsize=6)\n    plt.show();", "processed": ["id 32 densiti plot featur let show densiti plot variabl train dataset repres differ color distribut valu target valu 0 1"]}, {"markdown": ["The train and test seems to be well ballanced with respect of  distribution of the numeric variables for most of the features.   \n\nThere are few features that shows some differences in distribution between train and test, for example: **2**, **8**, **12**, **16**, **37**, **72**, **84**, **100**, **103**, **104**, **123**, **144**, **155**, **181**, **202**, **203**, **204**, **229**, **241**, **264**, **288**.\n\n\n## <a id='33'>Distribution of mean and std</a>  \n\nLet's check the distribution of the mean values per row in the train and test set."], "code": "# Reference: https://www.kaggle.com/code/gpreda/overfitting-the-private-leaderboard\n\nplt.figure(figsize=(16,6))\nfeatures = train_df.columns.values[2:302]\nplt.title(\"Distribution of mean values per row in the train and test set\")\nsns.distplot(train_df[features].mean(axis=1),color=\"green\", kde=True,bins=120, label='train')\nsns.distplot(test_df[features].mean(axis=1),color=\"blue\", kde=True,bins=120, label='test')\nplt.legend()\nplt.show()", "processed": ["train test seem well ballanc respect distribut numer variabl featur featur show differ distribut train test exampl 2 8 12 16 37 72 84 100 103 104 123 144 155 181 202 203 204 229 241 264 288 id 33 distribut mean std let check distribut mean valu per row train test set"]}, {"markdown": ["Mean values per row for test data are close to a normal distribution while the mean values per row for train data shows multiple peak values. Most of the values are  between +/- 0.1.\n\nLet's check the distribution of the mean values per columns in the train and test set."], "code": "# Reference: https://www.kaggle.com/code/gpreda/overfitting-the-private-leaderboard\n\nplt.figure(figsize=(16,6))\nplt.title(\"Distribution of mean values per column in the train and test set\")\nsns.distplot(train_df[features].mean(axis=0),color=\"magenta\",kde=True,bins=120, label='train')\nsns.distplot(test_df[features].mean(axis=0),color=\"darkblue\", kde=True,bins=120, label='test')\nplt.legend()\nplt.show()", "processed": ["mean valu per row test data close normal distribut mean valu per row train data show multipl peak valu valu 0 1 let check distribut mean valu per column train test set"]}, {"markdown": ["These are the values that we already observed earlier that are mostly centered around 0. We can see that train is actually showing a larger spread of these values, while test values have a smaller deviation and a distribution closer to a normal one.\n\nLet's show the distribution of standard deviation of values per row for train and test datasets."], "code": "# Reference: https://www.kaggle.com/code/gpreda/overfitting-the-private-leaderboard\n\nplt.figure(figsize=(16,6))\nplt.title(\"Distribution of std values per row in the train and test set\")\nsns.distplot(train_df[features].std(axis=1),color=\"black\", kde=True,bins=120, label='train')\nsns.distplot(test_df[features].std(axis=1),color=\"red\", kde=True,bins=120, label='test')\nplt.legend();plt.show()", "processed": ["valu alreadi observ earlier mostli center around 0 see train actual show larger spread valu test valu smaller deviat distribut closer normal one let show distribut standard deviat valu per row train test dataset"]}, {"markdown": ["The average standard deviation per rows is 1 and most of values are between 1 +/- 0.1.\n\nLet's check the distribution of the standard deviation of values per columns in the train and test datasets."], "code": "# Reference: https://www.kaggle.com/code/gpreda/overfitting-the-private-leaderboard\n\nplt.figure(figsize=(16,6))\nplt.title(\"Distribution of std values per column in the train and test set\")\nsns.distplot(train_df[features].std(axis=0),color=\"blue\",kde=True,bins=120, label='train')\nsns.distplot(test_df[features].std(axis=0),color=\"green\", kde=True,bins=120, label='test')\nplt.legend(); plt.show()", "processed": ["averag standard deviat per row 1 valu 1 0 1 let check distribut standard deviat valu per column train test dataset"]}, {"markdown": ["Standard deviation values per columns in train dataset are between 0.9 and 1.1 while in test dataset are much smaller, confined between 0.99 and 1.01.\n\nLet's check now the distribution of the mean value per row in the train dataset, grouped by value of target."], "code": "# Reference: https://www.kaggle.com/code/gpreda/overfitting-the-private-leaderboard\n\nt0 = train_df.loc[train_df['target'] == 0]\nt1 = train_df.loc[train_df['target'] == 1]\nplt.figure(figsize=(16,6))\nplt.title(\"Distribution of mean values per row in the train set\")\nsns.distplot(t0[features].mean(axis=1),color=\"red\", kde=True,bins=120, label='target = 0')\nsns.distplot(t1[features].mean(axis=1),color=\"blue\", kde=True,bins=120, label='target = 1')\nplt.legend(); plt.show()", "processed": ["standard deviat valu per column train dataset 0 9 1 1 test dataset much smaller confin 0 99 1 01 let check distribut mean valu per row train dataset group valu target"]}, {"markdown": ["Let's check now the distribution of the mean value per column in the train dataset, grouped by value of target."], "code": "# Reference: https://www.kaggle.com/code/gpreda/overfitting-the-private-leaderboard\n\nplt.figure(figsize=(16,6))\nplt.title(\"Distribution of mean values per column in the train set\")\nsns.distplot(t0[features].mean(axis=0),color=\"green\", kde=True,bins=120, label='target = 0')\nsns.distplot(t1[features].mean(axis=0),color=\"darkblue\", kde=True,bins=120, label='target = 1')\nplt.legend(); plt.show()", "processed": ["let check distribut mean valu per column train dataset group valu target"]}, {"markdown": ["## <a id='34'>Distribution of min and max</a>  \n\nLet's check the distribution of min per row in the train and test set."], "code": "# Reference: https://www.kaggle.com/code/gpreda/overfitting-the-private-leaderboard\n\nplt.figure(figsize=(16,6))\nfeatures = train_df.columns.values[2:302]\nplt.title(\"Distribution of min values per row in the train and test set\")\nsns.distplot(train_df[features].min(axis=1),color=\"red\", kde=True,bins=120, label='train')\nsns.distplot(test_df[features].min(axis=1),color=\"orange\", kde=True,bins=120, label='test')\nplt.legend()\nplt.show()", "processed": ["id 34 distribut min max let check distribut min per row train test set"]}, {"markdown": ["\nA long queue to the lower values for both, extended as long as to -5.5 for test set, is observed.\n\nLet's now show the distribution of min per column in the train and test set."], "code": "# Reference: https://www.kaggle.com/code/gpreda/overfitting-the-private-leaderboard\n\nplt.figure(figsize=(16,6))\nfeatures = train_df.columns.values[2:302]\nplt.title(\"Distribution of min values per column in the train and test set\")\nsns.distplot(train_df[features].min(axis=0),color=\"magenta\", kde=True,bins=120, label='train')\nsns.distplot(test_df[features].min(axis=0),color=\"darkblue\", kde=True,bins=120, label='test')\nplt.legend()\nplt.show()", "processed": ["long queue lower valu extend long 5 5 test set observ let show distribut min per column train test set"]}, {"markdown": ["The distribution of min values per columns (i.e. per variables) is quite notably different for train and test set.\n\n\nLet's check now the distribution of max values per rows for train and test set."], "code": "# Reference: https://www.kaggle.com/code/gpreda/overfitting-the-private-leaderboard\n\nplt.figure(figsize=(16,6))\nfeatures = train_df.columns.values[2:302]\nplt.title(\"Distribution of max values per row in the train and test set\")\nsns.distplot(train_df[features].max(axis=1),color=\"brown\", kde=True,bins=120, label='train')\nsns.distplot(test_df[features].max(axis=1),color=\"lightblue\", kde=True,bins=120, label='test')\nplt.legend()\nplt.show()", "processed": ["distribut min valu per column e per variabl quit notabl differ train test set let check distribut max valu per row train test set"]}, {"markdown": ["Both distribution shows a long queue toward larger values, with test extended more, up to 5.5., while train has values as large as 4.5.   \n\nLet's show now the max distribution on columns for train and test set."], "code": "# Reference: https://www.kaggle.com/code/gpreda/overfitting-the-private-leaderboard\n\nplt.figure(figsize=(16,6))\nfeatures = train_df.columns.values[2:302]\nplt.title(\"Distribution of max values per column in the train and test set\")\nsns.distplot(train_df[features].max(axis=0),color=\"blue\", kde=True,bins=120, label='train')\nsns.distplot(test_df[features].max(axis=0),color=\"red\", kde=True,bins=120, label='test')\nplt.legend()\nplt.show()", "processed": ["distribut show long queue toward larger valu test extend 5 5 train valu larg 4 5 let show max distribut column train test set"]}, {"markdown": ["The two distributions are neatly separated.  \n\n\nLet's  show now the distributions of min values per row in train set, separated on the values of target (0 and 1)."], "code": "# Reference: https://www.kaggle.com/code/gpreda/overfitting-the-private-leaderboard\n\nt0 = train_df.loc[train_df['target'] == 0]\nt1 = train_df.loc[train_df['target'] == 1]\nplt.figure(figsize=(16,6))\nplt.title(\"Distribution of min values per row in the train set\")\nsns.distplot(t0[features].min(axis=1),color=\"red\", kde=True,bins=120, label='target = 0')\nsns.distplot(t1[features].min(axis=1),color=\"blue\", kde=True,bins=120, label='target = 1')\nplt.legend(); plt.show()", "processed": ["two distribut neatli separ let show distribut min valu per row train set separ valu target 0 1"]}, {"markdown": ["We show here the distribution of min values per columns in train set."], "code": "# Reference: https://www.kaggle.com/code/gpreda/overfitting-the-private-leaderboard\n\nplt.figure(figsize=(16,6))\nplt.title(\"Distribution of min values per column in the train set\")\nsns.distplot(t0[features].min(axis=0),color=\"red\", kde=True,bins=120, label='target = 0')\nsns.distplot(t1[features].min(axis=0),color=\"blue\", kde=True,bins=120, label='target = 1')\nplt.legend(); plt.show()", "processed": ["show distribut min valu per column train set"]}, {"markdown": ["We can observe a relative good separation between the two distributions, with the values for **target = 0** with lower peaks and with a longer queue toward larger values (up to close to -1), while the mins for **target = 1** are extended only until -1.5.\n\nLet's show now the distribution of max values per rown in the train set."], "code": "# Reference: https://www.kaggle.com/code/gpreda/overfitting-the-private-leaderboard\n\nplt.figure(figsize=(16,6))\nplt.title(\"Distribution of max values per row in the train set\")\nsns.distplot(t0[features].max(axis=1),color=\"red\", kde=True,bins=120, label='target = 0')\nsns.distplot(t1[features].max(axis=1),color=\"blue\", kde=True,bins=120, label='target = 1')\nplt.legend(); plt.show()", "processed": ["observ rel good separ two distribut valu target 0 lower peak longer queue toward larger valu close 1 min target 1 extend 1 5 let show distribut max valu per rown train set"]}, {"markdown": ["Let's show also the distribution of max values per columns in the train set."], "code": "# Reference: https://www.kaggle.com/code/gpreda/overfitting-the-private-leaderboard\n\nplt.figure(figsize=(16,6))\nplt.title(\"Distribution of max values per column in the train set\")\nsns.distplot(t0[features].max(axis=0),color=\"red\", kde=True,bins=120, label='target = 0')\nsns.distplot(t1[features].max(axis=0),color=\"blue\", kde=True,bins=120, label='target = 1')\nplt.legend(); plt.show()", "processed": ["let show also distribut max valu per column train set"]}, {"markdown": ["We can observe a relative good separation between the two distributions.\n", "## <a id='35'>Distribution of skew and kurtosis</a>  \n\nLet's see now what is the distribution of skew values per rows and columns.\n\nLet's see first the distribution of skewness calculated per rows in train and test sets.\n\n"], "code": "# Reference: https://www.kaggle.com/code/gpreda/overfitting-the-private-leaderboard\n\nplt.figure(figsize=(16,6))\nfeatures = train_df.columns.values[2:302]\nplt.title(\"Distribution of skew per row in the train and test set\")\nsns.distplot(train_df[features].skew(axis=1),color=\"red\", kde=True,bins=120, label='train')\nsns.distplot(test_df[features].skew(axis=1),color=\"orange\", kde=True,bins=120, label='test')\nplt.legend()\nplt.show()", "processed": ["observ rel good separ two distribut", "id 35 distribut skew kurtosi let see distribut skew valu per row column let see first distribut skew calcul per row train test set"]}, {"markdown": ["Let's see first the distribution of skewness calculated per columns in train and test set."], "code": "# Reference: https://www.kaggle.com/code/gpreda/overfitting-the-private-leaderboard\n\nplt.figure(figsize=(16,6))\nfeatures = train_df.columns.values[2:302]\nplt.title(\"Distribution of skew per column in the train and test set\")\nsns.distplot(train_df[features].skew(axis=0),color=\"magenta\", kde=True,bins=120, label='train')\nsns.distplot(test_df[features].skew(axis=0),color=\"darkblue\", kde=True,bins=120, label='test')\nplt.legend()\nplt.show()", "processed": ["let see first distribut skew calcul per column train test set"]}, {"markdown": ["Let's see now what is the distribution of kurtosis values per rows and columns.\n\nLet's see first the distribution of kurtosis calculated per rows in train and test sets."], "code": "# Reference: https://www.kaggle.com/code/gpreda/overfitting-the-private-leaderboard\n\nplt.figure(figsize=(16,6))\nfeatures = train_df.columns.values[2:302]\nplt.title(\"Distribution of kurtosis per row in the train and test set\")\nsns.distplot(train_df[features].kurtosis(axis=1),color=\"darkblue\", kde=True,bins=120, label='train')\nsns.distplot(test_df[features].kurtosis(axis=1),color=\"yellow\", kde=True,bins=120, label='test')\nplt.legend()\nplt.show()", "processed": ["let see distribut kurtosi valu per row column let see first distribut kurtosi calcul per row train test set"]}, {"markdown": ["\nLet's see first the distribution of kurtosis calculated per columns in train and test sets."], "code": "# Reference: https://www.kaggle.com/code/gpreda/overfitting-the-private-leaderboard\n\nplt.figure(figsize=(16,6))\nfeatures = train_df.columns.values[2:302]\nplt.title(\"Distribution of kurtosis per column in the train and test set\")\nsns.distplot(train_df[features].kurtosis(axis=0),color=\"magenta\", kde=True,bins=120, label='train')\nsns.distplot(test_df[features].kurtosis(axis=0),color=\"green\", kde=True,bins=120, label='test')\nplt.legend()\nplt.show()", "processed": ["let see first distribut kurtosi calcul per column train test set"]}, {"markdown": ["Let's see now the distribution of skewness on rows in train separated for values of target 0 and 1."], "code": "# Reference: https://www.kaggle.com/code/gpreda/overfitting-the-private-leaderboard\n\nt0 = train_df.loc[train_df['target'] == 0]\nt1 = train_df.loc[train_df['target'] == 1]\nplt.figure(figsize=(16,6))\nplt.title(\"Distribution of skew values per row in the train set\")\nsns.distplot(t0[features].skew(axis=1),color=\"red\", kde=True,bins=120, label='target = 0')\nsns.distplot(t1[features].skew(axis=1),color=\"blue\", kde=True,bins=120, label='target = 1')\nplt.legend(); plt.show()", "processed": ["let see distribut skew row train separ valu target 0 1"]}, {"markdown": ["Let's see now the distribution of skewness on columns in train separated for values of target 0 and 1."], "code": "# Reference: https://www.kaggle.com/code/gpreda/overfitting-the-private-leaderboard\n\nt0 = train_df.loc[train_df['target'] == 0]\nt1 = train_df.loc[train_df['target'] == 1]\nplt.figure(figsize=(16,6))\nplt.title(\"Distribution of skew values per column in the train set\")\nsns.distplot(t0[features].skew(axis=0),color=\"red\", kde=True,bins=120, label='target = 0')\nsns.distplot(t1[features].skew(axis=0),color=\"blue\", kde=True,bins=120, label='target = 1')\nplt.legend(); plt.show()", "processed": ["let see distribut skew column train separ valu target 0 1"]}, {"markdown": ["Let's see now the distribution of kurtosis on rows in train separated for values of target 0 and 1."], "code": "# Reference: https://www.kaggle.com/code/gpreda/overfitting-the-private-leaderboard\n\nt0 = train_df.loc[train_df['target'] == 0]\nt1 = train_df.loc[train_df['target'] == 1]\nplt.figure(figsize=(16,6))\nplt.title(\"Distribution of kurtosis values per row in the train set\")\nsns.distplot(t0[features].kurtosis(axis=1),color=\"red\", kde=True,bins=120, label='target = 0')\nsns.distplot(t1[features].kurtosis(axis=1),color=\"blue\", kde=True,bins=120, label='target = 1')\nplt.legend(); plt.show()", "processed": ["let see distribut kurtosi row train separ valu target 0 1"]}, {"markdown": ["Let's see now the distribution of kurtosis on columns in train separated for values of target 0 and 1."], "code": "# Reference: https://www.kaggle.com/code/gpreda/overfitting-the-private-leaderboard\n\nt0 = train_df.loc[train_df['target'] == 0]\nt1 = train_df.loc[train_df['target'] == 1]\nplt.figure(figsize=(16,6))\nplt.title(\"Distribution of kurtosis values per column in the train set\")\nsns.distplot(t0[features].kurtosis(axis=0),color=\"red\", kde=True,bins=120, label='target = 0')\nsns.distplot(t1[features].kurtosis(axis=0),color=\"blue\", kde=True,bins=120, label='target = 1')\nplt.legend(); plt.show()", "processed": ["let see distribut kurtosi column train separ valu target 0 1"]}, {"markdown": ["Let's check the new created features."], "code": "# Reference: https://www.kaggle.com/code/gpreda/overfitting-the-private-leaderboard\n\ntrain_df[train_df.columns[302:]].head()\ntest_df[test_df.columns[301:]].head()\ndef plot_new_feature_distribution(df1, df2, label1, label2, features):\n    i = 0\n    sns.set_style('whitegrid')\n    plt.figure()\n    fig, ax = plt.subplots(2,4,figsize=(18,8))\n\n    for feature in features:\n        i += 1\n        plt.subplot(2,4,i)\n        sns.kdeplot(df1[feature], bw=0.5,label=label1)\n        sns.kdeplot(df2[feature], bw=0.5,label=label2)\n        plt.xlabel(feature, fontsize=11)\n        locs, labels = plt.xticks()\n        plt.tick_params(axis='x', which='major', labelsize=8)\n        plt.tick_params(axis='y', which='major', labelsize=8)\n    plt.show();", "processed": ["let check new creat featur"]}, {"markdown": ["Let's check the feature importance."], "code": "# Reference: https://www.kaggle.com/code/gpreda/overfitting-the-private-leaderboard\n\ndef plot_feature_importance():\n    cols = (feature_importance_df[[\"Feature\", \"importance\"]]\n            .groupby(\"Feature\")\n            .mean()\n            .sort_values(by=\"importance\", ascending=False)[:50].index)\n    best_features = feature_importance_df.loc[feature_importance_df.Feature.isin(cols)]\n    plt.figure(figsize=(12,10))\n    sns.barplot(x=\"importance\", y=\"Feature\", data=best_features.sort_values(by=\"importance\",ascending=False))\n    plt.title('Features importance (averaged/folds)')\n    plt.tight_layout()\n    plt.savefig('FI.png')\nplot_feature_importance()", "processed": ["let check featur import"]}, {"markdown": ["# Data Generator\nThis data generator outputs random crops of size `352x512`. These crops are taken from the original `1400x2100` training images after they are resized to `700x1050` pixels. The masks are cropped to match the image crops. Also we have horizontal and vertical flip augmentation.\n\nBelow we display examples. The image on the left is the original image. The yellow rectangle is an original mask. The black rectangle is a random crop. The image on the right is the random crop outputted by the data generator. Notice how the original mask is cropped too."], "code": "# Reference: https://www.kaggle.com/code/cdeotte/train-with-crops-lb-0-63\n\nclass DataGenerator(keras.utils.Sequence):\n    # USES GLOBAL VARIABLE TRAIN2 COLUMNS E1, E2, E3, E4\n    'Generates data for Keras'\n    def __init__(self, list_IDs, batch_size=8, shuffle=False, width=512, height=352, scale=1/128., sub=1., mode='train_seg',\n                 path='../input/understanding_cloud_organization/train_images/', ext='.jpg', flips=False, shrink=2):\n        'Initialization'\n        self.list_IDs = list_IDs\n        self.shuffle = shuffle\n        self.batch_size = batch_size\n        self.path = path\n        self.scale = scale\n        self.sub = sub\n        self.path = path\n        self.ext = ext\n        self.width = width\n        self.height = height\n        self.mode = mode\n        self.flips = flips\n        self.shrink = shrink\n        self.on_epoch_end()\n        \n    def __len__(self):\n        'Denotes the number of batches per epoch'\n        ct = int(np.floor( len(self.list_IDs) / self.batch_size))\n        if len(self.list_IDs)>ct*self.batch_size: ct += 1\n        return int(ct)\n\n    def __getitem__(self, index):\n        'Generate one batch of data'\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n        X, y, msk, crp = self.__data_generation(indexes)\n        if (self.mode=='display'): return X, msk, crp\n        elif (self.mode=='train_seg')|(self.mode=='validate_seg'): return X, msk\n        elif (self.mode=='train')|(self.mode=='validate'): return X, y\n        else: return X\n\n    def on_epoch_end(self):\n        'Updates indexes after each epoch'\n        self.indexes = np.arange(int( len(self.list_IDs) ))\n        if self.shuffle: np.random.shuffle(self.indexes)\n\n    def __data_generation(self, indexes):\n        'Generates data containing batch_size samples' \n        # Initialization\n        lnn = len(indexes)\n        X = np.empty((lnn,self.height,self.width,3),dtype=np.float32)\n        msk = np.empty((lnn,self.height,self.width,4),dtype=np.int8)\n        crp = np.zeros((lnn,2),dtype=np.int16)\n        y = np.zeros((lnn,4),dtype=np.int8)\n        \n        # Generate data\n        for k in range(lnn):\n            img = cv2.imread(self.path + self.list_IDs[indexes[k]] + self.ext)\n            img = cv2.resize(img,(2100//self.shrink,1400//self.shrink),interpolation = cv2.INTER_AREA)\n            # AUGMENTATION FLIPS\n            hflip = False; vflip = False\n            if (self.flips):\n                if np.random.uniform(0,1)>0.5: hflip=True\n                if np.random.uniform(0,1)>0.5: vflip=True\n            if vflip: img = cv2.flip(img,0) # vertical\n            if hflip: img = cv2.flip(img,1) # horizontal\n            # RANDOM CROP\n            a = np.random.randint(0,2100//self.shrink-self.width+1)\n            b = np.random.randint(0,1400//self.shrink-self.height+1)\n            if (self.mode=='predict'):\n                a = (2100//self.shrink-self.width)//2\n                b = (1400//self.shrink-self.height)//2\n            img = img[b:self.height+b,a:self.width+a]\n            # NORMALIZE IMAGES\n            X[k,] = img*self.scale - self.sub      \n            # LABELS\n            if (self.mode!='predict'):\n                for j in range(1,5):\n                    rle = train2.loc[self.list_IDs[indexes[k]],'e'+str(j)]\n                    mask = rle2maskX(rle,shrink=self.shrink)\n                    if vflip: mask = np.flip(mask,axis=0)\n                    if hflip: mask = np.flip(mask,axis=1)\n                    msk[k,:,:,j-1] = mask[b:self.height+b,a:self.width+a]\n                    if (self.mode=='train')|(self.mode=='validate'):\n                        if np.sum( msk[k,:,:,j-1] )>0: y[k,j-1]=1\n            if (self.mode=='display'):\n                crp[k,0] = a; crp[k,1] = b\n\n        return X, y, msk, crp\ntypes = ['Fish','Flower','Gravel','Sugar']\ntrain_batch = DataGenerator(train2.index[:8], mode='display',batch_size=1,scale=1,sub=0)\nfor k,image in enumerate(train_batch):\n    plt.figure(figsize=(15,8))\n    \n    # RANDOMLY PICK CLOUD TYPE TO DISPLAY FROM NON-EMPTY MASKS\n    idx = np.argwhere( train2.loc[train2.index[k],['d1','d2','d3','d4']].values==1 ).flatten()\n    d = np.random.choice(idx)+1\n    \n    # DISPLAY ORIGINAL\n    img = Image.open(PATH+train2.index[k]+'.jpg'); img=np.array(img)\n    mask = rle2maskX( train2.loc[train2.index[k],'e'+str(d)] )\n    contour = mask2contour( mask,10 )\n    img[contour==1,:2]=255\n    diff = np.ones((1400,2100,3),dtype=np.int)*255-img.astype(int)\n    img=img.astype(int); img[mask==1,:] += diff[mask==1,:]//6\n    mask = np.zeros((1400,2100))\n    a = image[2][0,1]*2\n    b = image[2][0,0]*2\n    mask[a:a+2*352,b:b+2*512]=1\n    mask = mask2contour(mask,20)\n    img[mask==1,:]=0\n    plt.subplot(1,2,1); \n    plt.title('Original - '+train2.index[k]+'.jpg - '+types[d-1])\n    plt.imshow(img);\n    \n    # DISPLAY RANDOM CROP\n    img = image[0][0,]\n    mask = image[1][0,:,:,d-1]\n    contour = mask2contour( mask )\n    img[contour==1,:2]=255\n    diff = np.ones((352,512,3),dtype=np.int)*255-img.astype(int)\n    img=img.astype(int); img[mask==1,:] += diff[mask==1,:]//6\n    plt.subplot(1,2,2)\n    plt.title('Training Crop - '+train2.index[k]+'.jpg - '+types[d-1])\n    plt.imshow( img.astype(int) ); \n    plt.show()", "processed": ["data gener data gener output random crop size 352x512 crop taken origin 1400x2100 train imag resiz 700x1050 pixel mask crop match imag crop also horizont vertic flip augment display exampl imag left origin imag yellow rectangl origin mask black rectangl random crop imag right random crop output data gener notic origin mask crop"]}, {"markdown": ["# View OOF Examples\nBelow yellow outlines are true masks and blue outlines (with shaded insides) are predicted masks. The Dice score for each predicted mask is displayed above each image."], "code": "# Reference: https://www.kaggle.com/code/cdeotte/train-with-crops-lb-0-63\n\nfor d in range(1,5):\n    print('#'*27); print('#'*5,types[d-1].upper(),'CLOUDS','#'*7); print('#'*27)\n    plt.figure(figsize=(20,15)); k=0\n    for kk in range(9):\n        plt.subplot(3,3,kk+1)\n        while (train2.loc[train2.index[k],'e'+str(d)]==''): k += 1\n        f = train2.index[k]+'.jpg'\n        img = Image.open(PATH+f); img = img.resize((525,350)); img = np.array(img)\n        rle1 = train2.loc[train2.index[k],'e'+str(d)]; mask = rle2maskX(rle1,shrink=4)\n        contour = mask2contour(mask,5); img[contour==1,:2] = 255\n        rle2 = train2.loc[train2.index[k],'ee'+str(d)]; mask = rle2maskX(rle2,shape=(525,350))\n        contour = mask2contour(mask,5); img[contour==1,2] = 255\n        diff = np.ones((350,525,3),dtype=np.int)*255-img\n        img=img.astype(int); img[mask==1,:] += diff[mask==1,:]//4\n        dice = np.round( dice_coef6(rle1,1,rle2,0),3 )\n        plt.title(f+'  Dice = '+str(dice)+'   Yellow true, Blue predicted')\n        plt.imshow(img); k += 1\n    plt.show()", "processed": ["view oof exampl yellow outlin true mask blue outlin shade insid predict mask dice score predict mask display imag"]}, {"markdown": ["I begin with finding the optimal learning rate. The following function runs training with different lr and records the loss. Increase of the loss indicates onset of divergence of training. The optimal lr lies in the vicinity of the minimum of the curve but before the onset of divergence. Based on the following plot, for the current setup the divergence starts at ~0.01, and the recommended learning rate is ~0.002."], "code": "# Reference: https://www.kaggle.com/code/iafoss/fine-tuning-resnet34-on-ship-detection\n\nlearn.lr_find()\nlearn.sched.plot()", "processed": ["begin find optim learn rate follow function run train differ lr record loss increas loss indic onset diverg train optim lr lie vicin minimum curv onset diverg base follow plot current setup diverg start 0 01 recommend learn rate 0 002"]}, {"markdown": ["\n# CareerCon 2019 - Help Navigate Robots\n## Top Solutions Compilation\n\n![](https://www.lextronic.fr/imageslib/4D/0J7589.320.gif)\n\n---\n\nRobots are smart\u2026 by design. To fully understand and properly navigate a task, however, they need input about their environment.\n\nIn this competition, you\u2019ll help robots recognize the floor surface they\u2019re standing on using data collected from Inertial Measurement Units (IMU sensors).\n\n---\n\n> This is just a **compilation** of all the best solutions I have found, different types of analysis and posts (discussions).\n> **The purpose** of this is to have in a single kernel all the relevant information/solutions of this competition, so that if in the future there is a similar competition this material can be used as baseline, and maybe the authors delete the kernel or make it private, so we would lose all that amazing information.\n> **Please** comment bellow if you want me to add something, or if I missed something important... and please support the original kernels and their authors.\n\nMy best/original kernel in this competition is [#1 Smart Robots. Most Complete Notebook \ud83e\udd16](https://www.kaggle.com/jesucristo/1-smart-robots-most-complete-notebook)\n\n### References\n\n- [#16 Solution](https://www.kaggle.com/ilhamfp31/16-solution-0-76) by [@ilhamfp31](https://www.kaggle.com/ilhamfp31)\n- [Submission (Fourier, Neighbour Detection, SVM)](https://www.kaggle.com/trohwer64/submission-fourier-neighbour-detection-svm/code) by [Thomas Rohwer](https://www.kaggle.com/trohwer64)\n- [Starter Code for 3rd place Solution](https://www.kaggle.com/prith189/starter-code-for-3rd-place-solution)\n- https://www.kaggle.com/ilhamfp31/16-solution-0-76\n- https://www.kaggle.com/whoiskk/15-solution-private-0-77\n\n<br>", "# 1. Starter code: [#1 Smart Robots. Most Complete Notebook \ud83e\udd16](https://www.kaggle.com/jesucristo/1-smart-robots-most-complete-notebook)", "# 2. Basic Kernels", "## [Simple Fourier Analysis](https://www.kaggle.com/trohwer64/simple-fourier-analysis) by [Thomas Rohwer](https://www.kaggle.com/trohwer64)"], "code": "!ls ../input\nPATH = '../input/career-con-2019'\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport math\nimport gc\ngc.enable()\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\ntrain_x = pd.read_csv(PATH+'/X_train.csv')\ntrain_y = pd.read_csv(PATH+'/y_train.csv')\ndef prepare_data(t):\n    def f(d):\n        d=d.sort_values(by=['measurement_number'])\n        return pd.DataFrame({\n         'lx':[ d['linear_acceleration_X'].values ],\n         'ly':[ d['linear_acceleration_Y'].values ],\n         'lz':[ d['linear_acceleration_Z'].values ],\n         'ax':[ d['angular_velocity_X'].values ],\n         'ay':[ d['angular_velocity_Y'].values ],\n         'az':[ d['angular_velocity_Z'].values ],\n        })\n\n    t= t.groupby('series_id').apply(f)\n\n    def mfft(x):\n        return [ x/math.sqrt(128.0) for x in np.absolute(np.fft.fft(x)) ][1:65]\n\n    t['lx_f']=[ mfft(x) for x in t['lx'].values ]\n    t['ly_f']=[ mfft(x) for x in t['ly'].values ]\n    t['lz_f']=[ mfft(x) for x in t['lz'].values ]\n    t['ax_f']=[ mfft(x) for x in t['ax'].values ]\n    t['ay_f']=[ mfft(x) for x in t['ay'].values ]\n    t['az_f']=[ mfft(x) for x in t['az'].values ]\n    return t\n\nt=prepare_data(train_x)\n\nt=pd.merge(t,train_y[['series_id','surface','group_id']],on='series_id')\nt=t.rename(columns={\"surface\": \"y\"})\n\ndef aggf(d, feature):\n    va= np.array(d[feature].tolist())\n    mean= sum(va)/va.shape[0]\n    var= sum([ (va[i,:]-mean)**2 for i in range(va.shape[0]) ])/va.shape[0]\n    dev= [ math.sqrt(x) for x in var ]\n    return pd.DataFrame({\n        'mean': [ mean ],\n        'dev' : [ dev ],\n    })\n\ndisplay={\n'hard_tiles_large_space':'r-.',\n'concrete':'g-.',\n'tiled':'b-.',\n\n'fine_concrete':'r-',\n'wood':'g-',\n'carpet':'b-',\n'soft_pvc':'y-',\n\n'hard_tiles':'r--',\n'soft_tiles':'g--',\n}\n\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(14, 8*7))\n#plt.margins(x=0.0, y=0.0)\n#plt.tight_layout()\n# plt.figure()\n\nfeatures=['lx_f','ly_f','lz_f','ax_f','ay_f','az_f']\ncount=0\n\nfor feature in features:\n    stat= t.groupby('y').apply(aggf,feature)\n    stat.index= stat.index.droplevel(-1)\n    b=[*range(len(stat.at['carpet','mean']))]\n\n    count+=1\n    plt.subplot(len(features)+1,1,count)\n    for i,(k,v) in enumerate(display.items()):\n        plt.plot(b, stat.at[k,'mean'], v, label=k)\n        # plt.errorbar(b, stat.at[k,'mean'], yerr=stat.at[k,'dev'], fmt=v)\n   \n    leg = plt.legend(loc='best', ncol=3, mode=\"expand\", shadow=True, fancybox=True)\n    plt.title(\"sensor: \" + feature)\n    plt.xlabel(\"frequency component\")\n    plt.ylabel(\"amplitude\")\n\ncount+=1\nplt.subplot(len(features)+1,1,count)\nk='concrete'\nv=display[k]\nfeature='lz_f'\nstat= t.groupby('y').apply(aggf,feature)\nstat.index= stat.index.droplevel(-1)\nb=[*range(len(stat.at['carpet','mean']))]\n\nplt.errorbar(b, stat.at[k,'mean'], yerr=stat.at[k,'dev'], fmt=v)\nplt.title(\"sample for error bars (lz_f, surface concrete)\")\nplt.xlabel(\"frequency component\")\nplt.ylabel(\"amplitude\")\n\nplt.show()\ndel train_x, train_y \ngc.collect()", "processed": ["careercon 2019 help navig robot top solut compil http www lextron fr imageslib 4d 0j7589 320 gif robot smart design fulli understand properli navig task howev need input environ competit youll help robot recogn floor surfac theyr stand use data collect inerti measur unit imu sensor compil best solut found differ type analysi post discus purpos singl kernel relev inform solut competit futur similar competit materi use baselin mayb author delet kernel make privat would lose amaz inform plea comment bellow want add someth miss someth import plea support origin kernel author best origin kernel competit 1 smart robot complet notebook http www kaggl com jesucristo 1 smart robot complet notebook refer 16 solut http www kaggl com ilhamfp31 16 solut 0 76 ilhamfp31 http www kaggl com ilhamfp31 submiss fourier neighbour detect svm http www kaggl com trohwer64 submiss fourier neighbour detect svm code thoma rohwer http www kaggl com trohwer64 starter code 3rd place solut http www kaggl com prith189 starter code 3rd place solut http www kaggl com ilhamfp31 16 solut 0 76 http www kaggl com whoiskk 15 solut privat 0 77 br", "1 starter code 1 smart robot complet notebook http www kaggl com jesucristo 1 smart robot complet notebook", "2 basic kernel", "simpl fourier analysi http www kaggl com trohwer64 simpl fourier analysi thoma rohwer http www kaggl com trohwer64"]}, {"markdown": ["## [Deep Learning Starter](https://www.kaggle.com/theoviel/deep-learning-starter) by [Theo Viel](https://www.kaggle.com/theoviel)", "In this kernel, Theo directly feed the data into a **Recurrent Neural Network**. For fancyness, Theo added an **Attention Mechanism**.\n\n"], "code": "# Reference: https://www.kaggle.com/code/jesucristo/1-smart-robots-complete-compilation\n\nimport os\nimport numpy as np\nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport itertools\nimport math\n\nfrom sklearn.metrics import *\nfrom sklearn.model_selection import *\n\nimport keras\nfrom keras.layers import *\nfrom keras.callbacks import *\nfrom keras.models import Model\nfrom keras import backend as K\nfrom keras.engine.topology import Layer\nfrom keras import initializers, regularizers, constraints, optimizers, layers\nX_train = pd.read_csv(PATH+\"/X_train.csv\")\nX_test = pd.read_csv(PATH+\"/X_test.csv\")\ny_train = pd.read_csv(PATH+\"/y_train.csv\")\nsub = pd.read_csv(PATH+\"/sample_submission.csv\")\nprint (\"Train: measurements = %d , features = %d\" % X_train.shape)\nX_train.head()\nplt.figure(figsize=(15, 5))\nsns.countplot(y_train['surface'])\nplt.title('Target distribution', size=15)\nplt.show()", "processed": ["deep learn starter http www kaggl com theoviel deep learn starter theo viel http www kaggl com theoviel", "kernel theo directli feed data recurr neural network fancy theo ad attent mechan"]}, {"markdown": ["**Input: Prepare data for the Neural Network**"], "code": "# Reference: https://www.kaggle.com/code/jesucristo/1-smart-robots-complete-compilation\n\nX_train.drop(['row_id', \"series_id\", \"measurement_number\"], axis=1, inplace=True)\nX_train = X_train.values.reshape((3810, 128, 10))\n\nX_test.drop(['row_id', \"series_id\", \"measurement_number\"], axis=1, inplace=True)\nX_test = X_test.values.reshape((3816, 128, 10))\nfor j in range(2):\n    plt.figure(figsize=(15, 5))\n    plt.title(\"Target : \" + y_train['surface'][j], size=15)\n    for i in range(10):\n        plt.plot(X_train[j, :, i], label=i)\n    plt.legend()\n    plt.show()", "processed": ["input prepar data neural network"]}, {"markdown": ["## Visualization utilities\ndata -> pixels, nothing of much interest for the machine learning practitioner in this section."], "code": "# Reference: https://www.kaggle.com/code/gpreda/starter-100-flowers-on-tpu-fixed-strategy\n\n# numpy and matplotlib defaults\nnp.set_printoptions(threshold=15, linewidth=80)\n\ndef batch_to_numpy_images_and_labels(data):\n    images, labels = data\n    numpy_images = images.numpy()\n    numpy_labels = labels.numpy()\n    if numpy_labels.dtype == object: # binary string in this case, these are image ID strings\n        numpy_labels = [None for _ in enumerate(numpy_images)]\n    # If no labels, only image IDs, return None for labels (this is the case for test data)\n    return numpy_images, numpy_labels\n\ndef title_from_label_and_target(label, correct_label):\n    if correct_label is None:\n        return CLASSES[label], True\n    correct = (label == correct_label)\n    return \"{} [{}{}{}]\".format(CLASSES[label], 'OK' if correct else 'NO', u\"\\u2192\" if not correct else '',\n                                CLASSES[correct_label] if not correct else ''), correct\n\ndef display_one_flower(image, title, subplot, red=False, titlesize=16):\n    plt.subplot(*subplot)\n    plt.axis('off')\n    plt.imshow(image)\n    if len(title) > 0:\n        plt.title(title, fontsize=int(titlesize) if not red else int(titlesize/1.2), color='red' if red else 'black', fontdict={'verticalalignment':'center'}, pad=int(titlesize/1.5))\n    return (subplot[0], subplot[1], subplot[2]+1)\n    \ndef display_batch_of_images(databatch, predictions=None):\n    \"\"\"This will work with:\n    display_batch_of_images(images)\n    display_batch_of_images(images, predictions)\n    display_batch_of_images((images, labels))\n    display_batch_of_images((images, labels), predictions)\n    \"\"\"\n    # data\n    images, labels = batch_to_numpy_images_and_labels(databatch)\n    if labels is None:\n        labels = [None for _ in enumerate(images)]\n        \n    # auto-squaring: this will drop data that does not fit into square or square-ish rectangle\n    rows = int(math.sqrt(len(images)))\n    cols = len(images)//rows\n        \n    # size and spacing\n    FIGSIZE = 13.0\n    SPACING = 0.1\n    subplot=(rows,cols,1)\n    if rows < cols:\n        plt.figure(figsize=(FIGSIZE,FIGSIZE/cols*rows))\n    else:\n        plt.figure(figsize=(FIGSIZE/rows*cols,FIGSIZE))\n    \n    # display\n    for i, (image, label) in enumerate(zip(images[:rows*cols], labels[:rows*cols])):\n        title = '' if label is None else CLASSES[label]\n        correct = True\n        if predictions is not None:\n            title, correct = title_from_label_and_target(predictions[i], label)\n        dynamic_titlesize = FIGSIZE*SPACING/max(rows,cols)*40+3 # magic formula tested to work from 1x1 to 10x10 images\n        subplot = display_one_flower(image, title, subplot, not correct, titlesize=dynamic_titlesize)\n    \n    #layout\n    plt.tight_layout()\n    if label is None and predictions is None:\n        plt.subplots_adjust(wspace=0, hspace=0)\n    else:\n        plt.subplots_adjust(wspace=SPACING, hspace=SPACING)\n    plt.show()\n\ndef display_confusion_matrix(cmat, score, precision, recall):\n    plt.figure(figsize=(15,15))\n    ax = plt.gca()\n    ax.matshow(cmat, cmap='Reds')\n    ax.set_xticks(range(len(CLASSES)))\n    ax.set_xticklabels(CLASSES, fontdict={'fontsize': 7})\n    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"left\", rotation_mode=\"anchor\")\n    ax.set_yticks(range(len(CLASSES)))\n    ax.set_yticklabels(CLASSES, fontdict={'fontsize': 7})\n    plt.setp(ax.get_yticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n    titlestring = \"\"\n    if score is not None:\n        titlestring += 'f1 = {:.3f} '.format(score)\n    if precision is not None:\n        titlestring += '\\nprecision = {:.3f} '.format(precision)\n    if recall is not None:\n        titlestring += '\\nrecall = {:.3f} '.format(recall)\n    if len(titlestring) > 0:\n        ax.text(101, 1, titlestring, fontdict={'fontsize': 18, 'horizontalalignment':'right', 'verticalalignment':'top', 'color':'#804040'})\n    plt.show()\n    \ndef display_training_curves(training, validation, title, subplot):\n    if subplot%10==1: # set up the subplots on the first call\n        plt.subplots(figsize=(10,10), facecolor='#F0F0F0')\n        plt.tight_layout()\n    ax = plt.subplot(subplot)\n    ax.set_facecolor('#F8F8F8')\n    ax.plot(training)\n    ax.plot(validation)\n    ax.set_title('model '+ title)\n    ax.set_ylabel(title)\n    #ax.set_ylim(0.28,1.05)\n    ax.set_xlabel('epoch')\n    ax.legend(['train', 'valid.'])", "processed": ["visual util data pixel noth much interest machin learn practition section"]}, {"markdown": ["# EDA "], "code": "# Reference: https://www.kaggle.com/code/frtgnn/yam-potatoes-thanksgiving-2018\n\nplt.figure(figsize=(10, 6))\n\nsns.despine()\nsns.set_context(\"notebook\", font_scale=1.5, rc={\"lines.linewidth\": 2.5})\n\nsns.distplot(train['start_time_seconds_youtube_clip'],label='Start')\nsns.distplot(train['end_time_seconds_youtube_clip'],label='End')\nplt.title('Train Data Start & End Distribution')\nplt.legend(loc=\"upper right\")\nplt.xlabel('Start & End Time for the clips')\nplt.ylabel('Distribution')\nplt.figure(figsize=(10, 6))\n\nsns.despine()\nsns.set_context(\"notebook\", font_scale=1.5, rc={\"lines.linewidth\": 2.5})\n\nsns.distplot(train['start_time_seconds_youtube_clip'],label='Train')\nsns.distplot(test['start_time_seconds_youtube_clip'],label='Test')\nplt.title('Train & Test Data Start Distributions')\nplt.legend(loc=\"upper right\")\nplt.xlabel('Start Time for the clips')\nplt.ylabel('Distribution')\nplt.figure(figsize=(10, 6))\n\nsns.despine()\nsns.set_context(\"notebook\", font_scale=1.5, rc={\"lines.linewidth\": 2.5})\n\nsns.distplot(train['end_time_seconds_youtube_clip'],label='Train')\nsns.distplot(test['end_time_seconds_youtube_clip'],label='Test')\nplt.title('Train & Test Data End Distributions')\nplt.legend(loc=\"upper right\")\nplt.xlabel('End Time for the clips')\nplt.ylabel('Distribution')\nplt.figure(figsize=(8, 8))\ntrain['is_turkey'].value_counts().plot(kind='bar')\nplt.title('Train & Test Data End Distributions')\nplt.xlabel('Target Labels')\nplt.ylabel('Count')\n# got this two funcs from Tee Ming Yi, thanks!\n#https://www.kaggle.com/teemingyi/turkey-competition\ndef create_df(data, i):\n    df = pd.DataFrame([x for x in data.audio_embedding.iloc[i]])\n    df['vid_id'] = data.vid_id.iloc[i]\n    return df\ndef create_df_test(data, i):\n    df = pd.DataFrame([x for x in data.audio_embedding.iloc[i]])\n    df['vid_id'] = data.vid_id.iloc[i]\n    return df\nvid_train = []\nfor i in range(len(train.index)):\n    vid_train.append(create_df(train, i))\n    \nvid_train_flatten = pd.concat(vid_train)  \nvid_train_flatten.columns = ['feature_'+str(x) for x in vid_train_flatten.columns[:128]] + ['vid_id']\n\n#\n\nvid_test = []\nfor i in range(len(test.index)):\n    vid_test.append(create_df_test(test, i))\n    \nvid_test_flatten = pd.concat(vid_test)  \nvid_test_flatten.columns = ['feature_'+str(x) for x in vid_test_flatten.columns[:128]] + ['vid_id']\nvid_train_flatten.shape, vid_test_flatten.shape\nvid_train_flatten.info()\nprint('_' * 60, \"\\n\")\nvid_test_flatten.info()\ndf_train = pd.merge(train,vid_train_flatten, on = 'vid_id')\ndf_test  = pd.merge(test, vid_test_flatten , on = 'vid_id')\n\ndf_train = df_train.drop(['audio_embedding'],axis=1)\ndf_test  = df_test.drop(['audio_embedding'], axis=1)\ndf_train.shape, df_test.shape\nabs(df_train.corr())['is_turkey'].sort_values(ascending=False)[:10]", "processed": ["eda"]}, {"markdown": ["# 1 - Random Forest Classifier"], "code": "# Reference: https://www.kaggle.com/code/frtgnn/yam-potatoes-thanksgiving-2018\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nRFC = RandomForestClassifier(n_estimators=160, min_samples_split=3)\noof_train_rf, oof_test_rf = cross_validation(train_set=X, target=y, test_set=df_test_concat, cv_type=\"SKFold\", nfold=5, seed=2018, shuf=True, model=RFC)\nfpr, tpr, thresholds = roc_curve(y, oof_train_rf)\nroc_auc = auc(fpr, tpr)\n\nsns.set('talk', 'whitegrid', 'dark', font_scale=1.2,rc={\"lines.linewidth\": 2, 'grid.linestyle': '--'})\nlw = 2\nplt.figure()\nplt.plot(fpr, tpr, color='darkorange',lw=lw, label='ROC curve (AUC = %0.4f)' % roc_auc)\nplt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\nplt.xlim([-0.1, 1.1])\nplt.ylim([-0.1, 1.1])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Random Forest ROC')\nplt.legend(loc=\"lower right\")\nplt.show()\ny_pred_proba_RFC = oof_test_rf\ny_pred_proba_RFC = pd.DataFrame(y_pred_proba_RFC,columns=['is_turkey'])\ndf_test_concat.reset_index(inplace=True)\ndf_test_concat['is_turkey'] = y_pred_proba_RFC['is_turkey']\ndf_sub = df_test_concat[['vid_id','is_turkey']] \ndf_test_concat.drop('is_turkey',axis=1,inplace=True)\n\ndf_final_RFC = pd.merge(sample,df_sub,on='vid_id')\ndf_final_RFC.drop('is_turkey_x',axis=1,inplace=True)\ndf_final_RFC.columns = ['vid_id', 'is_turkey']\n\ndf_final_RFC.to_csv('submission_RFC.csv',index=False)\ndf_test_concat.set_index('vid_id',inplace=True)", "processed": ["1 random forest classifi"]}, {"markdown": ["# 2 - XG Boost Classifier"], "code": "# Reference: https://www.kaggle.com/code/frtgnn/yam-potatoes-thanksgiving-2018\n\nfrom xgboost import XGBClassifier\n\nXGB = XGBClassifier(max_depth=3, learning_rate=0.07, n_estimators=110, n_jobs=4)\noof_train_xgb, oof_test_xgb = cross_validation(train_set=X, target=y, test_set=df_test_concat, cv_type=\"SKFold\", nfold=5, seed=2018, shuf=True, model=XGB)\nfpr, tpr, thresholds = roc_curve(y, oof_train_xgb)\nroc_auc = auc(fpr, tpr)\n\nsns.set('talk', 'whitegrid', 'dark', font_scale=1.2,rc={\"lines.linewidth\": 2, 'grid.linestyle': '--'})\nlw = 2\nplt.figure()\nplt.plot(fpr, tpr, color='darkorange',lw=lw, label='ROC curve (AUC = %0.4f)' % roc_auc)\nplt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\nplt.xlim([-0.1, 1.1])\nplt.ylim([-0.1, 1.1])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('XGBOOST ROC')\nplt.legend(loc=\"lower right\")\nplt.show()\ny_pred_proba_XGB = oof_test_xgb \ny_pred_proba_XGB = pd.DataFrame(y_pred_proba_XGB,columns=['is_turkey'])\n\ndf_test_concat.reset_index(inplace=True)\ndf_test_concat['is_turkey'] = y_pred_proba_XGB['is_turkey']\ndf_sub = df_test_concat[['vid_id','is_turkey']] \ndf_test_concat.drop('is_turkey',axis=1,inplace=True)\n\ndf_final_XGB = pd.merge(sample,df_sub,on='vid_id')\ndf_final_XGB.drop('is_turkey_x',axis=1,inplace=True)\ndf_final_XGB.columns = ['vid_id', 'is_turkey']\n\ndf_final_XGB.to_csv('submission_XGB.csv',index=False)\ndf_test_concat.set_index('vid_id',inplace=True)", "processed": ["2 xg boost classifi"]}, {"markdown": ["# 3- Light GBM Classifier"], "code": "# Reference: https://www.kaggle.com/code/frtgnn/yam-potatoes-thanksgiving-2018\n\nfrom lightgbm import LGBMClassifier\n\nLGBC = LGBMClassifier(max_depth=-1, n_estimators=75, num_leaves=31)\noof_train_lgb, oof_test_lgb = cross_validation(train_set=X, target=y, test_set=df_test_concat, cv_type=\"SKFold\", nfold=5, seed=2018, shuf=True, model=LGBC)\nfpr, tpr, thresholds = roc_curve(y,oof_train_lgb)\nroc_auc = auc(fpr, tpr)\n\nsns.set('talk', 'whitegrid', 'dark', font_scale=1.2,rc={\"lines.linewidth\": 2, 'grid.linestyle': '--'})\nlw = 2\nplt.figure()\nplt.plot(fpr, tpr, color='darkorange',lw=lw, label='ROC curve (AUC = %0.4f)' % roc_auc)\nplt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\nplt.xlim([-0.1, 1.1])\nplt.ylim([-0.1, 1.1])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Light GBM ROC')\nplt.legend(loc=\"lower right\")\nplt.show()\ny_pred_proba_LGBC = oof_test_lgb\ny_pred_proba_LGBC = pd.DataFrame(y_pred_proba_LGBC,columns=['is_turkey'])\ndf_test_concat.reset_index(inplace=True)\ndf_test_concat['is_turkey'] = y_pred_proba_LGBC['is_turkey']\ndf_sub = df_test_concat[['vid_id','is_turkey']] \ndf_test_concat.drop('is_turkey',axis=1,inplace=True)\n\ndf_final_LGBC = pd.merge(sample,df_sub,on='vid_id')\ndf_final_LGBC.drop('is_turkey_x',axis=1,inplace=True)\ndf_final_LGBC.columns = ['vid_id', 'is_turkey']\n\ndf_final_LGBC.to_csv('submission_LGBM.csv',index=False)\ndf_test_concat.set_index('vid_id',inplace=True)", "processed": ["3 light gbm classifi"]}, {"markdown": ["# 4 - Logistic Regression "], "code": "# Reference: https://www.kaggle.com/code/frtgnn/yam-potatoes-thanksgiving-2018\n\nfrom sklearn.linear_model import LogisticRegression\nLR = LogisticRegression(C=0.00001,penalty='l2', solver=\"sag\", max_iter=100)\noof_train_lr, oof_test_lr = cross_validation(train_set=X, target=y, test_set=df_test_concat, cv_type=\"SKFold\", nfold=5, seed=2018, shuf=True, model=LR)\nfpr, tpr, thresholds = roc_curve(y,oof_train_lr)\nroc_auc = auc(fpr, tpr)\n\nsns.set('talk', 'whitegrid', 'dark', font_scale=1.2,rc={\"lines.linewidth\": 2, 'grid.linestyle': '--'})\nlw = 2\nplt.figure()\nplt.plot(fpr, tpr, color='darkorange',lw=lw, label='ROC curve (AUC = %0.4f)' % roc_auc)\nplt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\nplt.xlim([-0.1, 1.1])\nplt.ylim([-0.1, 1.1])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Logistic Regression ROC')\nplt.legend(loc=\"lower right\")\nplt.show()\ny_pred_proba_LR =  oof_test_lr\ny_pred_proba_LR = pd.DataFrame(y_pred_proba_LR,columns=['is_turkey'])\ndf_test_concat.reset_index(inplace=True)\ndf_test_concat['is_turkey'] = y_pred_proba_LR['is_turkey']\ndf_sub = df_test_concat[['vid_id','is_turkey']] \ndf_test_concat.drop('is_turkey',axis=1,inplace=True)\n\ndf_final_LR = pd.merge(sample,df_sub,on='vid_id')\ndf_final_LR.drop('is_turkey_x',axis=1,inplace=True)\ndf_final_LR.columns = ['vid_id', 'is_turkey']\n\ndf_final_LR.to_csv('submission_LR.csv',index=False)\ndf_test_concat.set_index('vid_id',inplace=True)", "processed": ["4 logist regress"]}, {"markdown": ["In contrast to the main kernel, here RAM is not the limiting factor. Therefore, the threshold is selected in the proper way: based on all oof predictions rather than average of thresholds for each fold. You can run the cell below for several values of noise_th to identify the best value."], "code": "# Reference: https://www.kaggle.com/code/iafoss/postprocessing-for-hypercolumns-kernel\n\nnoise_th = 75.0*255*(sz/128.0)**2 #threshold for the number of predicted pixels\n\ngc.collect();\ntorch.cuda.empty_cache()\npreds = preds0.clone()\npreds[preds.view(preds.shape[0],-1).float().sum(-1) < noise_th,...] = 0.0\n\ndices = []\n#here evrything is multiplied by 255, so 0.2 threshold corresponds to 51\nthrs = np.arange(40, 60, 1)\nfor th in progress_bar(thrs):\n    preds_m = (preds>th).byte()\n    dices.append(dice_overall(preds_m, ys).mean())\ndices = np.array(dices)    \n\nbest_dice = dices.max()\nbest_thr = thrs[dices.argmax()]\n\nplt.figure(figsize=(8,4))\nplt.plot(thrs, dices)\nplt.vlines(x=best_thr, ymin=dices.min(), ymax=dices.max())\nplt.text(best_thr, best_dice, f'DICE = {best_dice:.3f}', fontsize=14);\nplt.show()", "processed": ["contrast main kernel ram limit factor therefor threshold select proper way base oof predict rather averag threshold fold run cell sever valu nois th identifi best valu"]}, {"markdown": ["We have a date column, three anonymized categorical columns and target.", "### Features 1, 2, 3"], "code": "# Reference: https://www.kaggle.com/code/artgor/elo-eda-and-models\n\nfig, ax = plt.subplots(1, 3, figsize = (16, 6))\nplt.suptitle('Violineplots for features and target');\nsns.violinplot(x=\"feature_1\", y=\"target\", data=train, ax=ax[0], title='feature_1');\nsns.violinplot(x=\"feature_2\", y=\"target\", data=train, ax=ax[1], title='feature_2');\nsns.violinplot(x=\"feature_3\", y=\"target\", data=train, ax=ax[2], title='feature_3');\nfig, ax = plt.subplots(1, 3, figsize = (16, 6));\ntrain['feature_1'].value_counts().sort_index().plot(kind='bar', ax=ax[0], color='teal', title='feature_1');\ntrain['feature_2'].value_counts().sort_index().plot(kind='bar', ax=ax[1], color='brown', title='feature_2');\ntrain['feature_3'].value_counts().sort_index().plot(kind='bar', ax=ax[2], color='gold', title='feature_3');\nplt.suptitle('Counts of categiories for features');", "processed": ["date column three anonym categor column target", "featur 1 2 3"]}, {"markdown": ["### date"], "code": "# Reference: https://www.kaggle.com/code/artgor/elo-eda-and-models\n\nd1 = train['first_active_month'].value_counts().sort_index()\nd2 = test['first_active_month'].value_counts().sort_index()\ndata = [go.Scatter(x=d1.index, y=d1.values, name='train'), go.Scatter(x=d2.index, y=d2.values, name='test')]\nlayout = go.Layout(dict(title = \"Counts of first active\",\n                  xaxis = dict(title = 'Month'),\n                  yaxis = dict(title = 'Count'),\n                  ),legend=dict(\n                orientation=\"v\"))\npy.iplot(dict(data=data, layout=layout))", "processed": ["date"]}, {"markdown": ["## historical_transactions\nUp to 3 months' worth of historical transactions for each card_id"], "code": "# Reference: https://www.kaggle.com/code/artgor/elo-eda-and-models\n\nhistorical_transactions = pd.read_csv('../input/historical_transactions.csv')\ne = pd.read_excel('../input/Data_Dictionary.xlsx', sheet_name='history')\ne\nprint(f'{historical_transactions.shape[0]} samples in data')\nhistorical_transactions.head()\n# let's convert the authorized_flag to a binary value.\nhistorical_transactions['authorized_flag'] = historical_transactions['authorized_flag'].apply(lambda x: 1 if x == 'Y' else 0)\nprint(f\"At average {historical_transactions['authorized_flag'].mean() * 100:.4f}% transactions are authorized\")\nhistorical_transactions['authorized_flag'].value_counts().plot(kind='barh', title='authorized_flag value counts');", "processed": ["histor transact 3 month worth histor transact card id"]}, {"markdown": ["### purchase_amount\nSadly purchase_amount is normalized. Let's have a look at it nevertheless."], "code": "# Reference: https://www.kaggle.com/code/artgor/elo-eda-and-models\n\nplt.title('Purchase amount distribution.');\nhistorical_transactions['purchase_amount'].plot(kind='hist');\nfor i in [-1, 0]:\n    n = historical_transactions.loc[historical_transactions['purchase_amount'] < i].shape[0]\n    print(f\"There are {n} transactions with purchase_amount less than {i}.\")\nfor i in [0, 10, 100]:\n    n = historical_transactions.loc[historical_transactions['purchase_amount'] > i].shape[0]\n    print(f\"There are {n} transactions with purchase_amount more than {i}.\")\nplt.title('Purchase amount distribution for negative values.');\nhistorical_transactions.loc[historical_transactions['purchase_amount'] < 0, 'purchase_amount'].plot(kind='hist');", "processed": ["purchas amount sadli purchas amount normal let look nevertheless"]}, {"markdown": ["## new_merchant_transactions \nTwo months' worth of data for each card_id containing ALL purchases that card_id made at merchant_ids that were not visited in the historical data."], "code": "# Reference: https://www.kaggle.com/code/artgor/elo-eda-and-models\n\nnew_merchant_transactions = pd.read_csv('../input/new_merchant_transactions.csv')\ne = pd.read_excel('../input/Data_Dictionary.xlsx', sheet_name='new_merchant_period')\ne\nprint(f'{new_merchant_transactions.shape[0]} samples in data')\nnew_merchant_transactions.head()\n# let's convert the authorized_flag to a binary value.\nnew_merchant_transactions['authorized_flag'] = new_merchant_transactions['authorized_flag'].apply(lambda x: 1 if x == 'Y' else 0)\nprint(f\"At average {new_merchant_transactions['authorized_flag'].mean() * 100:.4f}% transactions are authorized\")\nnew_merchant_transactions['authorized_flag'].value_counts().plot(kind='barh', title='authorized_flag value counts');", "processed": ["new merchant transact two month worth data card id contain purchas card id made merchant id visit histor data"]}, {"markdown": ["### purchase_amount\nSadly purchase_amount is normalized. Let's have a look at it nevertheless."], "code": "# Reference: https://www.kaggle.com/code/artgor/elo-eda-and-models\n\nplt.title('Purchase amount distribution.');\nnew_merchant_transactions['purchase_amount'].plot(kind='hist');\nfor i in [-1, 0]:\n    n = new_merchant_transactions.loc[new_merchant_transactions['purchase_amount'] < i].shape[0]\n    print(f\"There are {n} transactions with purchase_amount less than {i}.\")\nfor i in [0, 10, 100]:\n    n = new_merchant_transactions.loc[new_merchant_transactions['purchase_amount'] > i].shape[0]\n    print(f\"There are {n} transactions with purchase_amount more than {i}.\")\nplt.title('Purchase amount distribution for negative values.');\nnew_merchant_transactions.loc[new_merchant_transactions['purchase_amount'] < 0, 'purchase_amount'].plot(kind='hist');", "processed": ["purchas amount sadli purchas amount normal let look nevertheless"]}, {"markdown": ["These two variables are very similar. In fact for 90% merchants they are the same.", "> most_recent_sales_range \tmost_recent_purchases_range \tavg_sales_lag3 \tavg_purchases_lag3 \tactive_months_lag3 \tavg_sales_lag6 \tavg_purchases_lag6 \tactive_months_lag6 \tavg_sales_lag12 \tavg_purchases_lag12 \tactive_months_lag12", "### most_recent_sales_range"], "code": "# Reference: https://www.kaggle.com/code/artgor/elo-eda-and-models\n\nmerchants['most_recent_sales_range'].value_counts().plot('bar');\nd = merchants['most_recent_sales_range'].value_counts().sort_index()\ne = merchants.loc[merchants['numerical_2'] < 0.1].groupby('most_recent_sales_range')['numerical_1'].mean()\ndata = [go.Bar(x=d.index, y=d.values, name='counts'), go.Scatter(x=e.index, y=e.values, name='mean numerical_1', yaxis='y2')]\nlayout = go.Layout(dict(title = \"Counts of values in categories of most_recent_sales_range\",\n                        xaxis = dict(title = 'most_recent_sales_range'),\n                        yaxis = dict(title = 'Counts'),\n                        yaxis2=dict(title='mean numerical_1', overlaying='y', side='right')),\n                   legend=dict(orientation=\"v\"))\npy.iplot(dict(data=data, layout=layout))", "processed": ["two variabl similar fact 90 merchant", "recent sale rang recent purchas rang avg sale lag3 avg purchas lag3 activ month lag3 avg sale lag6 avg purchas lag6 activ month lag6 avg sale lag12 avg purchas lag12 activ month lag12", "recent sale rang"]}, {"markdown": ["We can see that these ranges have different counts and different mean value of numerical_1 even after removing outliers.", "### most_recent_purchases_range"], "code": "# Reference: https://www.kaggle.com/code/artgor/elo-eda-and-models\n\nd = merchants['most_recent_purchases_range'].value_counts().sort_index()\ne = merchants.loc[merchants['numerical_2'] < 0.1].groupby('most_recent_purchases_range')['numerical_1'].mean()\ndata = [go.Bar(x=d.index, y=d.values, name='counts'), go.Scatter(x=e.index, y=e.values, name='mean numerical_1', yaxis='y2')]\nlayout = go.Layout(dict(title = \"Counts of values in categories of most_recent_purchases_range\",\n                        xaxis = dict(title = 'most_recent_purchases_range'),\n                        yaxis = dict(title = 'Counts'),\n                        yaxis2=dict(title='mean numerical_1', overlaying='y', side='right')),\n                   legend=dict(orientation=\"v\"))\npy.iplot(dict(data=data, layout=layout))", "processed": ["see rang differ count differ mean valu numer 1 even remov outlier", "recent purchas rang"]}, {"markdown": ["**In this kernel we will like to analyze images and  if possible (measure size of spots) and also some  data augmentation ideas that looks promising**", "![](https://nci-media.cancer.gov/pdq/media/images/578083-750.jpg)", "if you are a researcher, then probably you will like to spend some times analyzing melanoma mole sizes as i have shown LEVELS OF MELANOMA in my past kernel [In-Depth Melanoma with modeling](https://www.kaggle.com/mobassir/in-depth-melanoma-with-modeling) so we know that.... <br> <br> **The Clark Scale has 5 levels of melanoma:**\n\n* Cells are in the out layer of the skin (epidermis)\n\n* Cells are in the layer directly under the epidermis (pupillary dermis)\n\n* The cells are touching the next layer known as the deep dermis\n\n* Cells have spread to the reticular dermis\n\n* Cells have grown in the fat layer\n\n![](https://media.giphy.com/media/lSJElktZ5BKUvYSztq/giphy.gif)", "# References \n", "* [TensorFlow + Transfer Learning: Melanoma](https://www.kaggle.com/amyjang/tensorflow-transfer-learning-melanoma)\n* [Measuring size of objects in an image with OpenCV](https://www.pyimagesearch.com/2016/03/28/measuring-size-of-objects-in-an-image-with-opencv/)\n* [object-size](https://github.com/snsharma1311/object-size)\n* [Ensemble of Convolutional Neural Networks for Disease Classification of Skin Lesions](https://github.com/anindox8/Ensemble-of-Multi-Scale-CNN-for-Dermatoscopy-Classification)\n* [In-Depth Melanoma with modeling](https://www.kaggle.com/mobassir/in-depth-melanoma-with-modeling?scriptVersionId=39094350)\n* [GENERAL INFORMATION ABOUT MELANOMA](https://www.uhhospitals.org/services/cancer-services/skin-cancer/melanoma/about-melanoma)\n\n* [[Training CV] Melanoma Starter](https://www.kaggle.com/shonenkov/training-cv-melanoma-starter)\n\n* [Measuring Size of Objects with OpenCV](https://github.com/Practical-CV/Measuring-Size-of-Objects-with-OpenCV)\n* [Color Constancy](https://github.com/MinaSGorgi/Color-Constancy)\n* [Edge-Based Color Constancy](https://ieeexplore.ieee.org/document/4287009)\n* [Python | Thresholding techniques using OpenCV | Set-1 (Simple Thresholding)](https://www.geeksforgeeks.org/python-thresholding-techniques-using-opencv-set-1-simple-thresholding/)\n* [lesion-GAN](https://github.com/alxiang/lesion-GAN)\n* [Data-Augmentation-and-Segmentation-with-GANs-for-Medical-Images](https://github.com/apolanco3225/Data-Augmentation-and-Segmentation-with-GANs-for-Medical-Images)\n* [Towards Interpretable Skin Lesion Classification with Deep Learning Models](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7153112/pdf/3203149.pdf)", "# imports"], "code": "!pip install imutils\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\nfrom __future__ import unicode_literals\nfrom __future__ import print_function\nfrom __future__ import division\nfrom __future__ import absolute_import\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nfrom pathlib import Path\nimport pandas as pd\nfrom torch.utils.data import Dataset,DataLoader\n\nfrom scipy.spatial import distance as dist\nfrom imutils import perspective\nfrom imutils import contours\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport imutils\nimport cv2\n\nimport skimage.measure\nimport imageio\nfrom PIL import Image\nimport requests\nfrom io import BytesIO\nfrom torchvision import transforms as T\nimport torch.nn as nn\nimport torch\nimport torch.nn.functional as F\nfrom sklearn.model_selection import GroupKFold\nfrom kaggle_datasets import KaggleDatasets\n\nfrom scipy.spatial.distance import euclidean\nfrom imutils import perspective\nfrom imutils import contours\nimport numpy as np\nimport imutils\nimport cv2\nimport matplotlib.pyplot as plt\n\nfrom glob import glob\nimport pandas as pd\nfrom sklearn.model_selection import GroupKFold\nimport cv2\nfrom skimage import io\nimport albumentations as A\nimport torch\nimport os\nfrom datetime import datetime\nimport time\nimport random\nimport cv2\nimport pandas as pd\nimport numpy as np\nimport albumentations as A\nimport matplotlib.pyplot as plt\nfrom albumentations.pytorch.transforms import ToTensorV2\nfrom sklearn.model_selection import StratifiedKFold\nfrom torch.utils.data import Dataset,DataLoader\nfrom torch.utils.data.sampler import SequentialSampler, RandomSampler\nfrom torch.nn import functional as F\nfrom glob import glob\nimport sklearn\nfrom torch import nn\n\n\nimport keras\nimport numpy as np\nimport tensorflow as tf\nfrom keras.models import model_from_json, load_model\nimport json\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom functools import partial\n\nimport glob\nimport numpy as np\nimport cv2\nfrom skimage import filters as skifilters\nfrom scipy import ndimage\nfrom skimage import filters\nimport matplotlib.pyplot as plt\nimport tqdm\nfrom sklearn.utils import shuffle\nimport pandas as pd\n\nimport os\nimport h5py\nimport time\nimport json\nimport warnings\nfrom PIL import Image\n\nfrom fastprogress.fastprogress import master_bar, progress_bar\nfrom sklearn.metrics import accuracy_score, roc_auc_score\nfrom torchvision import models\nimport pdb\nimport albumentations as A\nfrom albumentations.pytorch.transforms import ToTensor\nimport matplotlib.pyplot as plt\n\nimport pickle \nimport os\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\ndef list_files(path:Path):\n    return [o for o in path.iterdir()]\npath = Path('../input/jpeg-melanoma-768x768/')\ndf_path = Path('../input/jpeg-melanoma-768x768/')\nim_sz = 256\nbs = 16\ntrain_fnames = list_files(path/'train')\ndf = pd.read_csv(df_path/'train.csv')\ndf.head()\n\n\ndf.target.value_counts(),df.shape\n\n\n\nGCS_PATH = KaggleDatasets().get_gcs_path('melanoma-768x768')\ndef decode_image(image):\n    image = tf.image.decode_jpeg(image, channels=3)\n    image = tf.cast(image, tf.float32) / 255.0\n    image = tf.reshape(image, [*IMAGE_SIZE, 3])\n    return image\ndef read_tfrecord(example, labeled):\n    tfrecord_format = {\n        \"image\": tf.io.FixedLenFeature([], tf.string),\n        \"target\": tf.io.FixedLenFeature([], tf.int64)\n    } if labeled else {\n        \"image\": tf.io.FixedLenFeature([], tf.string),\n        \"image_name\": tf.io.FixedLenFeature([], tf.string)\n    }\n    example = tf.io.parse_single_example(example, tfrecord_format)\n    image = decode_image(example['image'])\n    if labeled:\n        label = tf.cast(example['target'], tf.int32)\n        return image, label\n    idnum = example['image_name']\n    return image, idnum\ndef load_dataset(filenames, labeled=True, ordered=False):\n    ignore_order = tf.data.Options()\n    if not ordered:\n        ignore_order.experimental_deterministic = False # disable order, increase speed\n    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTOTUNE) # automatically interleaves reads from multiple files\n    dataset = dataset.with_options(ignore_order) # uses data as soon as it streams in, rather than in its original order\n    dataset = dataset.map(partial(read_tfrecord, labeled=labeled), num_parallel_calls=AUTOTUNE)\n    # returns a dataset of (image, label) pairs if labeled=True or (image, id) pairs if labeled=False\n    return dataset\n\nBATCH_SIZE = 8\nAUTOTUNE = tf.data.experimental.AUTOTUNE\nIMAGE_SIZE = [768, 768]\nTRAINING_FILENAMES, VALID_FILENAMES = train_test_split(\n    tf.io.gfile.glob(GCS_PATH + '/train*.tfrec'),\n    test_size=0.2, random_state=5\n)\nTEST_FILENAMES = tf.io.gfile.glob(GCS_PATH + '/test*.tfrec')\nprint('Train TFRecord Files:', len(TRAINING_FILENAMES))\nprint('Validation TFRecord Files:', len(VALID_FILENAMES))\nprint('Test TFRecord Files:', len(TEST_FILENAMES))\n\n\ndef get_training_dataset():\n    dataset = load_dataset(TRAINING_FILENAMES, labeled=True)\n    #dataset = dataset.map(augmentation_pipeline, num_parallel_calls=AUTOTUNE)\n    dataset = dataset.repeat()\n    dataset = dataset.shuffle(2048)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.prefetch(AUTOTUNE)\n    return dataset\n\n\ntrain_dataset = get_training_dataset()\ndef show_batch(image_batch, label_batch):\n    plt.figure(figsize=(15,15))\n    for n in range(8):\n        ax = plt.subplot(8,8,n+1)\n        plt.imshow(image_batch[n])\n        if label_batch[n]:\n            plt.title(\"MALIGNANT(1)\")\n        else:\n            plt.title(\"BENIGN(0)\")\n        plt.axis(\"off\")\n%%time\nfor i in range(0,10):\n    image_batch, label_batch = next(iter(train_dataset))\n    for j in range(0,8):\n        var = label_batch[j].numpy()\n        if(var!=0):\n            show_batch(image_batch.numpy(), label_batch.numpy())\n\n\nprint(\"Samples with Melanoma\")\nimgs = df[df.target==1]['image_name'].values\n_, axs = plt.subplots(2, 3, figsize=(20, 8))\naxs = axs.flatten()\nfor f_name,ax in zip(imgs[10:20],axs):\n    img = Image.open(path/f'train/{f_name}.jpg')\n    ax.imshow(img)\n    ax.axis('off')    \nplt.show()", "processed": ["kernel like analyz imag possibl measur size spot also data augment idea look promis", "http nci medium cancer gov pdq medium imag 578083 750 jpg", "research probabl like spend time analyz melanoma mole size shown level melanoma past kernel depth melanoma model http www kaggl com mobassir depth melanoma model know br br clark scale 5 level melanoma cell layer skin epidermi cell layer directli epidermi pupillari dermi cell touch next layer known deep dermi cell spread reticular dermi cell grown fat layer http medium giphi com medium lsjelktz5bkuvysztq giphi gif", "refer", "tensorflow transfer learn melanoma http www kaggl com amyjang tensorflow transfer learn melanoma measur size object imag opencv http www pyimagesearch com 2016 03 28 measur size object imag opencv object size http github com snsharma1311 object size ensembl convolut neural network diseas classif skin lesion http github com anindox8 ensembl multi scale cnn dermatoscopi classif depth melanoma model http www kaggl com mobassir depth melanoma model scriptversionid 39094350 gener inform melanoma http www uhhospit org servic cancer servic skin cancer melanoma melanoma train cv melanoma starter http www kaggl com shonenkov train cv melanoma starter measur size object opencv http github com practic cv measur size object opencv color constanc http github com minasgorgi color constanc edg base color constanc http ieeexplor ieee org document 4287009 python threshold techniqu use opencv set 1 simpl threshold http www geeksforgeek org python threshold techniqu use opencv set 1 simpl threshold lesion gan http github com alxiang lesion gan data augment segment gan medic imag http github com apolanco3225 data augment segment gan medic imag toward interpret skin lesion classif deep learn model http www ncbi nlm nih gov pmc articl pmc7153112 pdf 3203149 pdf", "import"]}, {"markdown": ["# EDA Submission"], "code": "# Reference: https://www.kaggle.com/code/cdeotte/top-solutions-ensemble-0-947\n\nsub.open_channels.value_counts()\nres=40\nplt.figure(figsize=(20,5))\nplt.plot(sub.time[::res],sub.open_channels[::res])\nplt.show()", "processed": ["eda submiss"]}, {"markdown": ["### Target"], "code": "# Reference: https://www.kaggle.com/code/artgor/which-bird-is-it\n\ntrain['ebird_code'].nunique()\nplt.figure(figsize=(12, 8))\ntrain['ebird_code'].value_counts().plot(kind='hist')", "processed": ["target"]}, {"markdown": ["We can see that most recordings were done in Canada and America"], "code": "# Reference: https://www.kaggle.com/code/artgor/which-bird-is-it\n\ntrain['location'].value_counts().plot(kind='hist')\ntrain['location'].nunique()", "processed": ["see record done canada america"]}, {"markdown": ["There are a lot of different locations! More than 6 thousands, with some locations having more than 100 recordings", "### Country"], "code": "# Reference: https://www.kaggle.com/code/artgor/which-bird-is-it\n\nplt.figure(figsize=(12, 8))\ntrain['country'].value_counts().head(20).plot(kind='barh');", "processed": ["lot differ locat 6 thousand locat 100 record", "countri"]}, {"markdown": ["## Date\n\nDate of recording"], "code": "# Reference: https://www.kaggle.com/code/artgor/which-bird-is-it\n\nplt.figure(figsize=(20, 8))\ntrain['date'].value_counts().sort_index().plot();", "processed": ["date date record"]}, {"markdown": ["Be aware that there are some missing values (aka 0000-00-00) and several strange old dates. Also there are some wrong values like `1992-12-00`. If we want to use the data, we will have to fix such values.", "### Rating"], "code": "# Reference: https://www.kaggle.com/code/artgor/which-bird-is-it\n\ntrain['rating'].value_counts().plot(kind='barh')\nplt.title('Counts of different ratings');\nfig, ax = plt.subplots(figsize=(24, 6))\nplt.subplot(1, 2, 1)\ntrain.groupby(['ebird_code']).agg({'rating': ['mean', 'std']}).reset_index().sort_values(('rating', 'mean'), ascending=False).set_index('ebird_code')['rating']['mean'].plot(kind='bar')\nplt.subplot(1, 2, 2)\ntrain.groupby(['ebird_code']).agg({'rating': ['mean', 'std']}).reset_index().sort_values(('rating', 'mean'), ascending=False).set_index('ebird_code')['rating']['mean'][:20].plot(kind='barh')", "processed": ["awar miss valu aka 0000 00 00 sever strang old date also wrong valu like 1992 12 00 want use data fix valu", "rate"]}, {"markdown": ["It is quite interesting to see that some birds have full 5 rating and some are less loved.", "### duration"], "code": "# Reference: https://www.kaggle.com/code/artgor/which-bird-is-it\n\ntrain['duration'].plot(kind='hist')\nplt.title('Distribution of durations');\nfor i in range(50, 100, 5):\n    perc = np.percentile(train['duration'], i)\n    print(f\"{i} percentile of duration is {perc}\")", "processed": ["quit interest see bird full 5 rate le love", "durat"]}, {"markdown": ["## Preparing data"], "code": "# Reference: https://www.kaggle.com/code/artgor/cancer-detection-with-kekas\n\nlabels = pd.read_csv('../input/train_labels.csv')\nfig = plt.figure(figsize=(25, 4))\n# display 20 images\ntrain_imgs = os.listdir(\"../input/train\")\nfor idx, img in enumerate(np.random.choice(train_imgs, 20)):\n    ax = fig.add_subplot(2, 20//2, idx+1, xticks=[], yticks=[])\n    im = Image.open(\"../input/train/\" + img)\n    plt.imshow(im)\n    lab = labels.loc[labels['id'] == img.split('.')[0], 'label'].values[0]\n    ax.set_title(f'Label: {lab}')", "processed": ["prepar data"]}, {"markdown": ["# Start by Looking at Historic Tournament Seeds"], "code": "# Reference: https://www.kaggle.com/code/robikscube/eda-of-women-s-ncaa-bracket-data-in-progress\n\n# Calculate the Average Team Seed\naverageseed = tourneyseeds.groupby(['TeamID']).agg(np.mean).sort_values('SeedNumber')\naverageseed = averageseed.merge(teams, left_index=True, right_on='TeamID') #Add Teamnname\naverageseed.head(20).plot(x='TeamName',\n                          y='SeedNumber',\n                          kind='bar',\n                          figsize=(15,5),\n                          title='Top 20 Average Tournament Seed',\n                          rot=45)\n# Pairplot of the Tourney Seed and Scores\nsns.pairplot(tourneycompactresults[['WScore',\n                                    'LScore',\n                                    'ScoreDiff',\n                                    'WSeed',\n                                    'LSeed',\n                                    'Season']], hue='Season')", "processed": ["start look histor tournament seed"]}, {"markdown": ["## Regular Season Games of Tourney Teams"], "code": "# Reference: https://www.kaggle.com/code/robikscube/eda-of-women-s-ncaa-bracket-data-in-progress\n\n# Pairplot of Regular Season Games\n# Only include teams who are both seeded in the tournament\nregseason_in_tourney = regseasoncompactresults.dropna(subset=['WSeed','LSeed'])\nsns.pairplot(data = regseason_in_tourney,\n             vars=['WScore','LScore','WSeed','LSeed'],\n             hue='WSeed')", "processed": ["regular season game tourney team"]}, {"markdown": ["We are getting a final R score of 0.01751 using this model. Now let us look at how the cumulative R value changes over time by plotting it along with zero-line."], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/am-i-over-fitting\n\nfig = plt.figure(figsize=(12, 6))\nplt.plot(ts_list, r1_overall_reward_list, c='blue')\nplt.plot(ts_list, [0]*len(ts_list), c='red')\nplt.title(\"Cumulative R value change for Univariate Ridge (technical_20)\")\nplt.ylim([-0.04,0.04])\nplt.xlim([850, 1850])\nplt.show()", "processed": ["get final r score 0 01751 use model let u look cumul r valu chang time plot along zero line"]}, {"markdown": ["The final R value of 0.019 is better than the previous one of 0.017\n\nWe can plot both the previous model and the current model to see how the cumulative R value changes for both the models."], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/am-i-over-fitting\n\nfig, ax = plt.subplots(figsize=(12, 6))\nax.plot(ts_list, r2_overall_reward_list, c='green', label='ridge-2')\nax.plot(ts_list, r1_overall_reward_list, c='blue', label='ridge-1')\nax.plot(ts_list, [0]*len(ts_list), c='red', label='zero line')\nax.legend(loc='lower right')\nax.set_ylim([-0.04,0.04])\nax.set_xlim([850, 1850])\nplt.title(\"Cumulative R value change for Bivariate Ridge\")\nplt.show()", "processed": ["final r valu 0 019 better previou one 0 017 plot previou model current model see cumul r valu chang model"]}, {"markdown": ["There is a very slight improvement in the validation R score from 0.01940 0.01946.\n\nNow let us look at the cumulative R score plots of both the models as well."], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/am-i-over-fitting\n\nfig, ax = plt.subplots(figsize=(12, 6))\nax.plot(ts_list, r3_overall_reward_list, c='brown', label='ridge-3')\nax.plot(ts_list, r2_overall_reward_list, c='green', label='ridge-2')\nax.plot(ts_list, [0]*len(ts_list), c='red', label='zero line')\nax.legend(loc='lower right')\nax.set_ylim([-0.04,0.04])\nax.set_xlim([850, 1850])\nplt.title(\"Cumulative R value change for Trivariate Ridge\")\nplt.show()", "processed": ["slight improv valid r score 0 01940 0 01946 let u look cumul r score plot model well"]}, {"markdown": ["Let us look at how the R value of the Extra trees model along with univariate ridge model (since univariate ridge is used in best public script)"], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/am-i-over-fitting\n\nfig, ax = plt.subplots(figsize=(12, 6))\nax.plot(ts_list, et_overall_reward_list, c='blue', label='Extra Trees')\nax.plot(ts_list, r1_overall_reward_list, c='green', label='Ridge-1')\nax.plot(ts_list, [0]*len(ts_list), c='red', label='zero line')\nax.legend(loc='lower right')\nax.set_ylim([-0.04,0.04])\nax.set_xlim([850, 1850])\nplt.title(\"Cumulative R value change for Extra Trees\")\n\nplt.show()", "processed": ["let u look r valu extra tree model along univari ridg model sinc univari ridg use best public script"]}, {"markdown": ["There are no missing values :)\n\n**Validation Strategy:**\n\nValidation strategy is very important because without a proper validation starategy, it will be very hard to evaluate the models against each other. \n\nSince dates are given as part of the dataset, it is essential to check whether the train and test datasets are from the same time period or different time period."], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/race-to-the-top-nycttd\n\ntrain_df['pickup_date'] = train_df['pickup_datetime'].dt.date\ntest_df['pickup_date'] = test_df['pickup_datetime'].dt.date\n\ncnt_srs = train_df['pickup_date'].value_counts()\nplt.figure(figsize=(12,4))\nax = plt.subplot(111)\nax.bar(cnt_srs.index, cnt_srs.values, alpha=0.8)\nax.xaxis_date()\nplt.xticks(rotation='vertical')\nplt.show()\ncnt_srs = test_df['pickup_date'].value_counts()\nplt.figure(figsize=(12,4))\nax = plt.subplot(111)\nax.bar(cnt_srs.index, cnt_srs.values, alpha=0.8)\nax.xaxis_date()\nplt.xticks(rotation='vertical')\nplt.show()", "processed": ["miss valu valid strategi valid strategi import without proper valid starategi hard evalu model sinc date given part dataset essenti check whether train test dataset time period differ time period"]}, {"markdown": ["# Data\n\n\nIn this competition you are predicting the probability that an online transaction is fraudulent, as denoted by the binary target ```isFraud```.\n\nThe data is broken into two files **identity** and **transaction**, which are joined by ```TransactionID```. \n\n> Note: Not all transactions have corresponding identity information.\n\n**Categorical Features - Transaction**\n\n- ProductCD\n- emaildomain\n- card1 - card6\n- addr1, addr2\n- P_emaildomain\n- R_emaildomain\n- M1 - M9\n\n**Categorical Features - Identity**\n\n- DeviceType\n- DeviceInfo\n- id_12 - id_38\n\n**The TransactionDT feature is a timedelta from a given reference datetime (not an actual timestamp).**\n\n**Files**\n\n- train_{transaction, identity}.csv - the training set\n- test_{transaction, identity}.csv - the test set (**you must predict the isFraud value for these observations**)\n- sample_submission.csv - a sample submission file in the correct format\n", "**Interactive Plots Utils**\n> from https://www.kaggle.com/kabure/baseline-fraud-detection-eda-interactive-views (more about Interactive plots there)"], "code": "# Reference: https://www.kaggle.com/code/jesucristo/fraud-complete-eda\n\n# functions from: https://www.kaggle.com/kabure/baseline-fraud-detection-eda-interactive-views\n\ndef resumetable(df):\n    print(f\"Dataset Shape: {df.shape}\")\n    summary = pd.DataFrame(df.dtypes,columns=['dtypes'])\n    summary = summary.reset_index()\n    summary['Name'] = summary['index']\n    summary = summary[['Name','dtypes']]\n    summary['Missing'] = df.isnull().sum().values    \n    summary['Uniques'] = df.nunique().values\n    summary['First Value'] = df.loc[0].values\n    summary['Second Value'] = df.loc[1].values\n    summary['Third Value'] = df.loc[2].values\n\n    for name in summary['Name'].value_counts().index:\n        summary.loc[summary['Name'] == name, 'Entropy'] = round(stats.entropy(df[name].value_counts(normalize=True), base=2),2) \n\n    return summary\n\ndef plot_distribution(df, var_select=None, title=None, bins=1.0): \n    # Calculate the correlation coefficient between the new variable and the target\n    tmp_fraud = df[df['isFraud'] == 1]\n    tmp_no_fraud = df[df['isFraud'] == 0]    \n    corr = df['isFraud'].corr(df[var_select])\n    corr = np.round(corr,3)\n    tmp1 = tmp_fraud[var_select].dropna()\n    tmp2 = tmp_no_fraud[var_select].dropna()\n    hist_data = [tmp1, tmp2]\n    \n    group_labels = ['Fraud', 'No Fraud']\n    colors = ['seagreen','indianred', ]\n\n    fig = ff.create_distplot(hist_data,\n                             group_labels,\n                             colors = colors, \n                             show_hist = True,\n                             curve_type='kde', \n                             bin_size = bins\n                            )\n    \n    fig['layout'].update(title = title+' '+'(corr target ='+ str(corr)+')')\n\n    iplot(fig, filename = 'Density plot')\n    \ndef plot_dist_churn(df, col, binary=None):\n    tmp_churn = df[df[binary] == 1]\n    tmp_no_churn = df[df[binary] == 0]\n    tmp_attr = round(tmp_churn[col].value_counts().sort_index() / df[col].value_counts().sort_index(),2)*100\n    print(f'Distribution of {col}: ')\n    trace1 = go.Bar(\n        x=tmp_churn[col].value_counts().sort_index().index,\n        y=tmp_churn[col].value_counts().sort_index().values, \n        name='Fraud',opacity = 0.8, marker=dict(\n            color='seagreen',\n            line=dict(color='#000000',width=1)))\n\n    trace2 = go.Bar(\n        x=tmp_no_churn[col].value_counts().sort_index().index,\n        y=tmp_no_churn[col].value_counts().sort_index().values,\n        name='No Fraud', opacity = 0.8, \n        marker=dict(\n            color='indianred',\n            line=dict(color='#000000',\n                      width=1)\n        )\n    )\n\n    trace3 =  go.Scatter(   \n        x=tmp_attr.sort_index().index,\n        y=tmp_attr.sort_index().values,\n        yaxis = 'y2', \n        name='% Fraud', opacity = 0.6, \n        marker=dict(\n            color='black',\n            line=dict(color='#000000',\n                      width=2 )\n        )\n    )\n    \n    layout = dict(title =  f'Distribution of {str(col)} feature by %Fraud',\n              xaxis=dict(type='category'), \n              yaxis=dict(title= 'Count'), \n              yaxis2=dict(range= [0, 15], \n                          overlaying= 'y', \n                          anchor= 'x', \n                          side= 'right',\n                          zeroline=False,\n                          showgrid= False, \n                          title= 'Percentual Fraud Transactions'\n                         ))\n\n    fig = go.Figure(data=[trace1, trace2, trace3], layout=layout)\n    iplot(fig)", "processed": ["data competit predict probabl onlin transact fraudul denot binari target isfraud data broken two file ident transact join transactionid note transact correspond ident inform categor featur transact productcd emaildomain card1 card6 addr1 addr2 p emaildomain r emaildomain m1 m9 categor featur ident devicetyp deviceinfo id 12 id 38 transactiondt featur timedelta given refer datetim actual timestamp file train transact ident csv train set test transact ident csv test set must predict isfraud valu observ sampl submiss csv sampl submiss file correct format", "interact plot util http www kaggl com kabur baselin fraud detect eda interact view interact plot"]}, {"markdown": ["### 2nd Problem ...\n\nNotice how **imbalanced** is our original dataset! Most of the transactions are non-fraud. If we use this dataframe as the base for our predictive models and analysis we might get a lot of errors and our algorithms will probably overfit since it will \"assume\" that most transactions are not fraud. But we don't want our model to assume, we want our model to detect patterns that give signs of fraud!\n\n**Imbalance** means that the number of data points available for different the classes is different\n\n<img src='https://www.datascience.com/hs-fs/hubfs/imbdata.png?t=1542328336307&width=487&name=imbdata.png'>"], "code": "# Reference: https://www.kaggle.com/code/jesucristo/fraud-complete-eda\n\nx = train_transaction['isFraud'].value_counts().index\ny = train_transaction['isFraud'].value_counts().values\n\ntrace2 = go.Bar(\n     x=x ,\n     y=y,\n     marker=dict(\n         color=y,\n         colorscale = 'Viridis',\n         reversescale = True\n     ),\n     name=\"Imbalance\",    \n )\nlayout = dict(\n     title=\"Data imbalance - isFraud\",\n     #width = 900, height = 500,\n     xaxis=go.layout.XAxis(\n     automargin=True),\n     yaxis=dict(\n         showgrid=False,\n         showline=False,\n         showticklabels=True,\n #         domain=[0, 0.85],\n     ), \n)\nfig1 = go.Figure(data=[trace2], layout=layout)\niplot(fig1)\n\ndel x,y\ngc.collect()", "processed": ["2nd problem notic imbalanc origin dataset transact non fraud use datafram base predict model analysi might get lot error algorithm probabl overfit sinc assum transact fraud want model assum want model detect pattern give sign fraud imbal mean number data point avail differ class differ img src http www datasci com h f hubf imbdata png 1542328336307 width 487 name imbdata png"]}, {"markdown": ["**TransactionDT** is not a timestamp, but somehow we use it to measure time."], "code": "# Reference: https://www.kaggle.com/code/jesucristo/fraud-complete-eda\n\ntrain_transaction['TransactionDT'].value_counts().head(10)\nfig, ax = plt.subplots(1, 2, figsize=(18,4))\n\ntime_val = train_transaction['TransactionDT'].values\n\nsns.distplot(time_val, ax=ax[0], color='r')\nax[0].set_title('Distribution of TransactionDT', fontsize=14)\nax[1].set_xlim([min(time_val), max(time_val)])\n\nsns.distplot(np.log(time_val), ax=ax[1], color='b')\nax[1].set_title('Distribution of LOG TransactionDT', fontsize=14)\nax[1].set_xlim([min(np.log(time_val)), max(np.log(time_val))])\n\nplt.show()\nfig, ax = plt.subplots(1, 2, figsize=(18,4))\n\ntime_val = train_transaction.loc[train_transaction['isFraud'] == 1]['TransactionDT'].values\n\nsns.distplot(np.log(time_val), ax=ax[0], color='r')\nax[0].set_title('Distribution of LOG TransactionDT, isFraud=1', fontsize=14)\nax[1].set_xlim([min(np.log(time_val)), max(np.log(time_val))])\n\ntime_val = train_transaction.loc[train_transaction['isFraud'] == 0]['TransactionDT'].values\n\nsns.distplot(np.log(time_val), ax=ax[1], color='b')\nax[1].set_title('Distribution of LOG TransactionDT, isFraud=0', fontsize=14)\nax[1].set_xlim([min(np.log(time_val)), max(np.log(time_val))])\n\n\nplt.show()\ntrain_transaction['TransactionDT'].plot(kind='hist',\n                                        figsize=(15, 5),\n                                        label='train',\n                                        bins=50,\n                                        title='Train vs Test TransactionDT distribution')\ntest_transaction['TransactionDT'].plot(kind='hist',\n                                       label='test',\n                                       bins=50)\nplt.legend()\nplt.show()", "processed": ["transactiondt timestamp somehow use measur time"]}, {"markdown": ["Also you should read this post by Rob [Plotting features over time shows something.... interesting\n](https://www.kaggle.com/c/ieee-fraud-detection/discussion/100167#latest-577688) he discovered a weird correlation between C and D features, and that's why I do the following plots :)", "### isFraud vs time"], "code": "# Reference: https://www.kaggle.com/code/jesucristo/fraud-complete-eda\n\ni = 'isFraud'\ncor = np.corrcoef(train_transaction['TransactionDT'], train_transaction[i])[0,1]\ntrain_transaction.loc[train_transaction['isFraud'] == 0].set_index('TransactionDT')[i].plot(style='.', title=i+\" corr= \"+str(round(cor,3)), figsize=(15, 3), label=\"isFraud=0\")\ntrain_transaction.loc[train_transaction['isFraud'] == 1].set_index('TransactionDT')[i].plot(style='.', title=i+\" corr= \"+str(round(cor,3)), figsize=(15, 3), label=\"isFraud=1\")\n#test_transaction.set_index('TransactionDT')[i].plot(style='.', title=i+\" corr= \"+str(round(cor,3)), figsize=(15, 3))\nplt.legend()\nplt.show()", "processed": ["also read post rob plot featur time show someth interest http www kaggl com c ieee fraud detect discus 100167 latest 577688 discov weird correl c featur follow plot", "isfraud v time"]}, {"markdown": ["### C features: C1, C2 ... C14"], "code": "# Reference: https://www.kaggle.com/code/jesucristo/fraud-complete-eda\n\nc_features = list(train_transaction.columns[16:30])\nfor i in c_features:\n    cor = np.corrcoef(train_transaction['TransactionDT'], train_transaction[i])[0,1]\n    train_transaction.set_index('TransactionDT')[i].plot(style='.', title=i+\" corr= \"+str(round(cor,3)), figsize=(15, 3))\n    test_transaction.set_index('TransactionDT')[i].plot(style='.', title=i+\" corr= \"+str(round(cor,3)), figsize=(15, 3))\n    plt.show()\nc_features = list(train_transaction.columns[16:30])\nfor i in c_features:\n    cor = np.corrcoef(train_transaction['TransactionDT'], train_transaction[i])[0,1]\n    train_transaction.set_index('TransactionDT')[i].plot(style='.', title=i+\" corr= \"+str(round(cor,3)), figsize=(15, 3))\n    test_transaction.set_index('TransactionDT')[i].plot(style='.', title=i+\" corr= \"+str(round(cor,3)), figsize=(15, 3))\n    plt.show()\ndel c_features\ngc.collect()", "processed": ["c featur c1 c2 c14"]}, {"markdown": ["### D features: D1 ... D15"], "code": "# Reference: https://www.kaggle.com/code/jesucristo/fraud-complete-eda\n\nd_features = list(train_transaction.columns[30:45])\n\nfor i in d_features:\n    cor = np.corrcoef(train_transaction['TransactionDT'], train_transaction[i])[0,1]\n    train_transaction.set_index('TransactionDT')[i].plot(style='.', title=i+\" corr= \"+str(round(cor,3)), figsize=(15, 3))\n    test_transaction.set_index('TransactionDT')[i].plot(style='.', title=i+\" corr= \"+str(round(cor,3)), figsize=(15, 3))\n    plt.show()", "processed": ["featur d1 d15"]}, {"markdown": ["If we consider D features, de 58.15% are missing values ... Let's plot without missing values"], "code": "# Reference: https://www.kaggle.com/code/jesucristo/fraud-complete-eda\n\nfor i in d_features:\n    cor_tr = np.corrcoef(train_transaction['TransactionDT'], train_transaction[i].fillna(-1))[0,1]\n    cor_te = np.corrcoef(test_transaction['TransactionDT'], test_transaction[i].fillna(-1))[0,1]\n    train_transaction.set_index('TransactionDT')[i].fillna(-1).plot(style='.', title=i+\" corr_tr= \"+str(round(cor_tr,3))+\" || corr_te= \"+str(round(cor_te,3)), figsize=(15, 3))\n    test_transaction.set_index('TransactionDT')[i].fillna(-1).plot(style='.', title=i+\" corr_tr= \"+str(round(cor_tr,3))+\"  || corr_te= \"+str(round(cor_te,3)), figsize=(15, 3))\n    plt.show()\ndel d_features, cor\ngc.collect()", "processed": ["consid featur de 58 15 miss valu let plot without miss valu"]}, {"markdown": ["## V150"], "code": "# Reference: https://www.kaggle.com/code/jesucristo/fraud-complete-eda\n\ni = \"V150\"\ncor_tr = np.corrcoef(train_transaction['TransactionDT'], train_transaction[i].fillna(-1))[0,1]\ncor_te = np.corrcoef(test_transaction['TransactionDT'], test_transaction[i].fillna(-1))[0,1]\ntrain_transaction.set_index('TransactionDT')[i].fillna(-1).plot(style='.', title=i+\" corr_tr= \"+str(round(cor_tr,3))+\" || corr_te= \"+str(round(cor_te,3)), figsize=(15, 3))\ntest_transaction.set_index('TransactionDT')[i].fillna(-1).plot(style='.', title=i+\" corr_tr= \"+str(round(cor_tr,3))+\"  || corr_te= \"+str(round(cor_te,3)), figsize=(15, 3))\nplt.show()", "processed": ["v150"]}, {"markdown": ["<br>\n# TransactionAmt"], "code": "# Reference: https://www.kaggle.com/code/jesucristo/fraud-complete-eda\n\nfig, ax = plt.subplots(1, 2, figsize=(18,4))\n\ntime_val = train_transaction['TransactionAmt'].values\n\nsns.distplot(time_val, ax=ax[0], color='r')\nax[0].set_title('Distribution of TransactionAmt', fontsize=14)\nax[1].set_xlim([min(time_val), max(time_val)])\n\nsns.distplot(np.log(time_val), ax=ax[1], color='b')\nax[1].set_title('Distribution of LOG TransactionAmt', fontsize=14)\nax[1].set_xlim([min(np.log(time_val)), max(np.log(time_val))])\n\nplt.show()\nfig, ax = plt.subplots(1, 2, figsize=(18,4))\n\ntime_val = train_transaction.loc[train_transaction['isFraud'] == 1]['TransactionAmt'].values\n\nsns.distplot(np.log(time_val), ax=ax[0], color='r')\nax[0].set_title('Distribution of LOG TransactionAmt, isFraud=1', fontsize=14)\nax[1].set_xlim([min(np.log(time_val)), max(np.log(time_val))])\n\ntime_val = train_transaction.loc[train_transaction['isFraud'] == 0]['TransactionAmt'].values\n\nsns.distplot(np.log(time_val), ax=ax[1], color='b')\nax[1].set_title('Distribution of LOG TransactionAmt, isFraud=0', fontsize=14)\nax[1].set_xlim([min(np.log(time_val)), max(np.log(time_val))])\n\n\nplt.show()\ndel time_val\ntmp = train_transaction[['TransactionAmt', 'isFraud']][0:100000]\nplot_distribution(tmp[(tmp['TransactionAmt'] <= 800)], 'TransactionAmt', 'Transaction Amount Distribution', bins=10.0,)\ndel tmp", "processed": ["br transactionamt"]}, {"markdown": ["# Unique Values", "### D Features"], "code": "# Reference: https://www.kaggle.com/code/jesucristo/fraud-complete-eda\n\nplt.figure(figsize=(10, 7))\nd_features = list(train_transaction.columns[30:45])\nuniques = [len(train_transaction[col].unique()) for col in d_features]\nsns.set(font_scale=1.2)\nax = sns.barplot(d_features, uniques, log=True)\nax.set(xlabel='Feature', ylabel='log(unique count)', title='Number of unique values per feature TRAIN')\nfor p, uniq in zip(ax.patches, uniques):\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()/2.,\n            height + 10,\n            uniq,\n            ha=\"center\") \nplt.figure(figsize=(10, 7))\nd_features = list(test_transaction.columns[30:45])\nuniques = [len(test_transaction[col].unique()) for col in d_features]\nsns.set(font_scale=1.2)\nax = sns.barplot(d_features, uniques, log=True)\nax.set(xlabel='Feature', ylabel='log(unique count)', title='Number of unique values per feature TEST')\nfor p, uniq in zip(ax.patches, uniques):\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()/2.,\n            height + 10,\n            uniq,\n            ha=\"center\") ", "processed": ["uniqu valu", "featur"]}, {"markdown": ["### C features"], "code": "# Reference: https://www.kaggle.com/code/jesucristo/fraud-complete-eda\n\nplt.figure(figsize=(10, 7))\nc_features = list(train_transaction.columns[16:30])\nuniques = [len(train_transaction[col].unique()) for col in c_features]\nsns.set(font_scale=1.2)\nax = sns.barplot(c_features, uniques, log=True)\nax.set(xlabel='Feature', ylabel='log(unique count)', title='Number of unique values per feature TRAIN')\nfor p, uniq in zip(ax.patches, uniques):\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()/2.,\n            height + 10,\n            uniq,\n            ha=\"center\") \nplt.figure(figsize=(10, 7))\nc_features = list(test_transaction.columns[16:30])\nuniques = [len(test_transaction[col].unique()) for col in c_features]\nsns.set(font_scale=1.2)\nax = sns.barplot(c_features, uniques, log=True)\nax.set(xlabel='Feature', ylabel='log(unique count)', title='Number of unique values per feature TEST')\nfor p, uniq in zip(ax.patches, uniques):\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()/2.,\n            height + 10,\n            uniq,\n            ha=\"center\") ", "processed": ["c featur"]}, {"markdown": ["### V features"], "code": "# Reference: https://www.kaggle.com/code/jesucristo/fraud-complete-eda\n\nplt.figure(figsize=(35, 8))\nv_features = list(train_transaction.columns[54:120])\nuniques = [len(train_transaction[col].unique()) for col in v_features]\nsns.set(font_scale=1.2)\nax = sns.barplot(v_features, uniques, log=True)\nax.set(xlabel='Feature', ylabel='log(unique count)', title='Number of unique values per feature')\nfor p, uniq in zip(ax.patches, uniques):\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()/2.,\n            height + 10,\n            uniq,\n            ha=\"center\") \nplt.figure(figsize=(35, 8))\nv_features = list(train_transaction.columns[120:170])\nuniques = [len(train_transaction[col].unique()) for col in v_features]\nsns.set(font_scale=1.2)\nax = sns.barplot(v_features, uniques, log=True)\nax.set(xlabel='Feature', ylabel='log(unique count)', title='Number of unique values per feature')\nfor p, uniq in zip(ax.patches, uniques):\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()/2.,\n            height + 10,\n            uniq,\n            ha=\"center\") \nplt.figure(figsize=(35, 8))\nv_features = list(train_transaction.columns[170:220])\nuniques = [len(train_transaction[col].unique()) for col in v_features]\nsns.set(font_scale=1.2)\nax = sns.barplot(v_features, uniques, log=True)\nax.set(xlabel='Feature', ylabel='log(unique count)', title='Number of unique values per feature')\nfor p, uniq in zip(ax.patches, uniques):\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()/2.,\n            height + 10,\n            uniq,\n            ha=\"center\") \nplt.figure(figsize=(35, 8))\nv_features = list(train_transaction.columns[220:270])\nuniques = [len(train_transaction[col].unique()) for col in v_features]\nsns.set(font_scale=1.2)\nax = sns.barplot(v_features, uniques, log=True)\nax.set(xlabel='Feature', ylabel='log(unique count)', title='Number of unique values per feature')\nfor p, uniq in zip(ax.patches, uniques):\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()/2.,\n            height + 10,\n            uniq,\n            ha=\"center\") \nplt.figure(figsize=(35, 8))\nv_features = list(train_transaction.columns[270:320])\nuniques = [len(train_transaction[col].unique()) for col in v_features]\nsns.set(font_scale=1.2)\nax = sns.barplot(v_features, uniques, log=True)\nax.set(xlabel='Feature', ylabel='log(unique count)', title='Number of unique values per feature')\nfor p, uniq in zip(ax.patches, uniques):\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()/2.,\n            height + 10,\n            uniq,\n            ha=\"center\") \nplt.figure(figsize=(38, 8))\nv_features = list(train_transaction.columns[320:390])\nuniques = [len(train_transaction[col].unique()) for col in v_features]\nsns.set(font_scale=1.2)\nax = sns.barplot(v_features, uniques, log=True)\nax.set(xlabel='Feature', ylabel='log(unique count)', title='Number of unique values per feature')\nfor p, uniq in zip(ax.patches, uniques):\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()/2.,\n            height + 10,\n            uniq,\n            ha=\"center\") ", "processed": ["v featur"]}, {"markdown": ["### id_code"], "code": "# Reference: https://www.kaggle.com/code/jesucristo/fraud-complete-eda\n\ntrain_identity.head(2)\nplt.figure(figsize=(35, 8))\nfeatures = list(train_identity.columns[0:38])\nuniques = [len(train_identity[col].unique()) for col in features]\nsns.set(font_scale=1.2)\nax = sns.barplot(features, uniques, log=True)\nax.set(xlabel='Feature', ylabel='log(unique count)', title='Number of unique values per feature TRAIN')\nfor p, uniq in zip(ax.patches, uniques):\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()/2.,\n            height + 10,\n            uniq,\n            ha=\"center\") \nplt.figure(figsize=(35, 8))\nfeatures = list(test_identity.columns[0:38])\nuniques = [len(test_identity[col].unique()) for col in features]\nsns.set(font_scale=1.2)\nax = sns.barplot(features, uniques, log=True)\nax.set(xlabel='Feature', ylabel='log(unique count)', title='Number of unique values per feature TEST')\nfor p, uniq in zip(ax.patches, uniques):\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()/2.,\n            height + 10,\n            uniq,\n            ha=\"center\") ", "processed": ["id code"]}, {"markdown": ["### ProductCD"], "code": "# Reference: https://www.kaggle.com/code/jesucristo/fraud-complete-eda\n\nfig, ax = plt.subplots(1, 2, figsize=(20,5))\n\nsns.countplot(x=\"ProductCD\", ax=ax[0], hue = \"isFraud\", data=train_transaction)\nax[0].set_title('ProductCD train', fontsize=14)\nsns.countplot(x=\"ProductCD\", ax=ax[1], data=test_transaction)\nax[1].set_title('ProductCD test', fontsize=14)\nplt.show()", "processed": ["productcd"]}, {"markdown": ["### Device Type & Device Info"], "code": "# Reference: https://www.kaggle.com/code/jesucristo/fraud-complete-eda\n\nax = sns.countplot(x=\"DeviceType\", data=train_identity)\nax.set_title('DeviceType', fontsize=14)\nplt.show()", "processed": ["devic type devic info"]}, {"markdown": ["### Card"], "code": "# Reference: https://www.kaggle.com/code/jesucristo/fraud-complete-eda\n\ncards = ['card1', 'card2', 'card3', 'card4', 'card5', 'card6']\nfor i in cards:\n    print (\"Unique \",i, \" = \",train_transaction[i].nunique())\nfig, ax = plt.subplots(1, 4, figsize=(25,5))\n\nsns.countplot(x=\"card4\", ax=ax[0], data=train_transaction.loc[train_transaction['isFraud'] == 0])\nax[0].set_title('card4 isFraud=0', fontsize=14)\nsns.countplot(x=\"card4\", ax=ax[1], data=train_transaction.loc[train_transaction['isFraud'] == 1])\nax[1].set_title('card4 isFraud=1', fontsize=14)\nsns.countplot(x=\"card6\", ax=ax[2], data=train_transaction.loc[train_transaction['isFraud'] == 0])\nax[2].set_title('card6 isFraud=0', fontsize=14)\nsns.countplot(x=\"card6\", ax=ax[3], data=train_transaction.loc[train_transaction['isFraud'] == 1])\nax[3].set_title('card6 isFraud=1', fontsize=14)\nplt.show()\ncards = train_transaction.iloc[:,4:7].columns\n\nplt.figure(figsize=(18,8*4))\ngs = gridspec.GridSpec(8, 4)\nfor i, cn in enumerate(cards):\n    ax = plt.subplot(gs[i])\n    sns.distplot(train_transaction.loc[train_transaction['isFraud'] == 1][cn], bins=50)\n    sns.distplot(train_transaction.loc[train_transaction['isFraud'] == 0][cn], bins=50)\n    ax.set_xlabel('')\n    ax.set_title('feature: ' + str(cn))\nplt.show()\n", "processed": ["card"]}, {"markdown": ["As you can see, ``` Card 1``` column is given as Categorical but it is behaving like Continuous Data. Having '13553' unique Values.\n\n> **From organizer: ** This is a encoded categorical variable. \nThe dataset contains many high-cardinality variables, and it's challenge to model such variable. Meanwhile, it's worthy to see how you talented people deal with them.\n\nCheck this post: https://www.kaggle.com/c/ieee-fraud-detection/discussion/100340#latest-578626", "### Email Domain"], "code": "# Reference: https://www.kaggle.com/code/jesucristo/fraud-complete-eda\n\n\"emaildomain\" in train_transaction.columns, \"emaildomain\" in train_identity.columns\nfig, ax = plt.subplots(1, 3, figsize=(32,10))\n\nsns.countplot(y=\"P_emaildomain\", ax=ax[0], data=train_transaction)\nax[0].set_title('P_emaildomain', fontsize=14)\nsns.countplot(y=\"P_emaildomain\", ax=ax[1], data=train_transaction.loc[train_transaction['isFraud'] == 1])\nax[1].set_title('P_emaildomain isFraud = 1', fontsize=14)\nsns.countplot(y=\"P_emaildomain\", ax=ax[2], data=train_transaction.loc[train_transaction['isFraud'] == 0])\nax[2].set_title('P_emaildomain isFraud = 0', fontsize=14)\nplt.show()\nfig, ax = plt.subplots(1, 3, figsize=(32,10))\n\nsns.countplot(y=\"R_emaildomain\", ax=ax[0], data=train_transaction)\nax[0].set_title('R_emaildomain', fontsize=14)\nsns.countplot(y=\"R_emaildomain\", ax=ax[1], data=train_transaction.loc[train_transaction['isFraud'] == 1])\nax[1].set_title('R_emaildomain isFraud = 1', fontsize=14)\nsns.countplot(y=\"R_emaildomain\", ax=ax[2], data=train_transaction.loc[train_transaction['isFraud'] == 0])\nax[2].set_title('R_emaildomain isFraud = 0', fontsize=14)\nplt.show()", "processed": ["see card 1 column given categor behav like continu data 13553 uniqu valu organ encod categor variabl dataset contain mani high cardin variabl challeng model variabl meanwhil worthi see talent peopl deal check post http www kaggl com c ieee fraud detect discus 100340 latest 578626", "email domain"]}, {"markdown": ["# PCA", "**PCA 2 components**"], "code": "# Reference: https://www.kaggle.com/code/jesucristo/fraud-complete-eda\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\ntrain_scaled = scaler.fit_transform(X_train)         \nPCA_train_x = PCA(2).fit_transform(train_scaled)\nplt.scatter(PCA_train_x[:, 0], PCA_train_x[:, 1], c=y_train, cmap=\"copper_r\")\nplt.axis('off')\nplt.colorbar()\nplt.show()\nfrom sklearn.decomposition import KernelPCA\n\nlin_pca = KernelPCA(n_components = 2, kernel=\"linear\", fit_inverse_transform=True)\nrbf_pca = KernelPCA(n_components = 2, kernel=\"rbf\", gamma=0.0433, fit_inverse_transform=True)\nsig_pca = KernelPCA(n_components = 2, kernel=\"sigmoid\", gamma=0.001, coef0=1, fit_inverse_transform=True)\n\n\nplt.figure(figsize=(11, 4))\nfor subplot, pca, title in ((131, lin_pca, \"Linear kernel\"), (132, rbf_pca, \"RBF kernel, $\\gamma=0.04$\"), \n                            (133, sig_pca, \"Sigmoid kernel, $\\gamma=10^{-3}, r=1$\")):\n       \n    PCA_train_x = PCA(2).fit_transform(train_scaled)\n    plt.subplot(subplot)\n    plt.title(title, fontsize=14)\n    plt.scatter(PCA_train_x[:, 0], PCA_train_x[:, 1], c=y_train, cmap=\"nipy_spectral_r\")\n    plt.xlabel(\"$z_1$\", fontsize=18)\n    if subplot == 131:\n        plt.ylabel(\"$z_2$\", fontsize=18, rotation=0)\n    plt.grid(True)\n\nplt.show()\ndel train_scaled,PCA_train_x,scaler, lin_pca,rbf_pca, sig_pca\ngc.collect()", "processed": ["pca", "pca 2 compon"]}, {"markdown": ["### Importance PLOT\n> last FOLD"], "code": "# Reference: https://www.kaggle.com/code/jesucristo/fraud-complete-eda\n\n# Get xgBoost importances\nimportance_dict = {}\nfor import_type in ['weight', 'gain', 'cover']:\n    importance_dict['xgBoost-'+import_type] = clf.get_booster().get_score(importance_type=import_type)\n    \n# MinMax scale all importances\nimportance_df = pd.DataFrame(importance_dict).fillna(0)\nimportance_df = pd.DataFrame(\n    preprocessing.MinMaxScaler().fit_transform(importance_df),\n    columns=importance_df.columns,\n    index=importance_df.index\n)\n\n# Create mean column\nimportance_df['mean'] = importance_df.mean(axis=1)\n\n# Plot the feature importances\nimportance_df.sort_values('mean').head(40).plot(kind='bar', figsize=(30, 7))\n\ndel clf, importance_df\ngc.collect()", "processed": ["import plot last fold"]}, {"markdown": ["### TTA\nDefine transformations for data augmentation and TTA function (default fast.ai functions do not transform a predicted mask back):"], "code": "# Reference: https://www.kaggle.com/code/iafoss/unet34-submission-tta-0-699-new-public-lb\n\ndef aug_unit(x,fwd=True,mask=False):\n    return x\n\ndef aug_flipV(x,fwd=True,mask=False):\n    return x.flip(2) if mask else x.flip(3)\n\ndef aug_flipH(x,fwd=True,mask=False):\n    return x.flip(1) if mask else x.flip(2)\n\ndef aug_T(x,fwd=True,mask=False):\n    return torch.transpose(x,1,2) if mask else torch.transpose(x,2,3)\n\ndef aug_rot_2(x,fwd=True,mask=False): #rotate pi/2\n    return aug_flipV(aug_flipH(x,fwd,mask),fwd,mask)\n\ndef aug_rot_4cr(x,fwd=True,mask=False): #rotate pi/4 counterclockwise\n    return aug_flipV(aug_T(x,fwd,mask),fwd,mask) if fwd else \\\n        aug_T(aug_flipV(x,fwd,mask),fwd,mask)\n\ndef aug_rot_4cw(x,fwd=True,mask=False): #rotate pi/4 clockwise\n    return aug_flipH(aug_T(x,fwd,mask),fwd,mask) if fwd else \\\n        aug_T(aug_flipH(x,fwd,mask),fwd,mask)\n\ndef aug_rot_2T(x,fwd=True,mask=False): #transpose and rotate pi/2\n    return aug_rot_2(aug_T(x,fwd,mask),fwd,mask)\n\ntrms_side_on = [aug_unit,aug_flipH]\ntrms_top_down = [aug_unit,aug_flipV]\ntrms_dihedral = [aug_unit,aug_flipH,aug_flipV,aug_T,aug_rot_2,aug_rot_2T,\n                 aug_rot_4cw,aug_rot_4cr]\ndef enc_img(img):\n    return torch.transpose(torch.tensor(img),0,2).unsqueeze(0)\n\ndef dec_img(img):\n    return to_np(torch.transpose(img.squeeze(0),0,2))\n\ndef display_augs(x,augs=aug_unit):\n    columns = 4\n    n = len(augs)\n    rows = n//4 + 1\n    fig=plt.figure(figsize=(columns*4, rows*4))\n    img = enc_img(x)\n    for i in range(rows):\n        for j in range(columns):\n            idx = j+i*columns\n            if idx >= n: break\n            fig.add_subplot(rows, columns, idx+1)\n            plt.axis('off')\n            plt.imshow(dec_img(augs[idx](img)))\n    plt.show()\n    \nimg = np.array(Image.open(os.path.join(TRAIN,'ce69faa4b.jpg')))\ndisplay_augs(img,trms_dihedral)", "processed": ["tta defin transform data augment tta function default fast ai function transform predict mask back"]}, {"markdown": ["Nice, this func give us a lot of cool and useful informations;\n- We have only two features with missing values. Entry and Exit StreetName", "# City's\n- I will start exploring the distribution of City's because it is a categorical with only a few categorys inside.\n"], "code": "# Reference: https://www.kaggle.com/code/kabure/insightful-eda-modeling-lgbm-hyperopt\n\nresumetable(df_train)\ntotal = len(df_train)\nplt.figure(figsize=(15,19))\n\nplt.subplot(311)\ng = sns.countplot(x=\"City\", data=df_train)\ng.set_title(\"City Count Distribution\", fontsize=20)\ng.set_ylabel(\"Count\",fontsize= 17)\ng.set_xlabel(\"City Names\", fontsize=17)\nsizes=[]\nfor p in g.patches:\n    height = p.get_height()\n    sizes.append(height)\n    g.text(p.get_x()+p.get_width()/2.,\n            height + 3,\n            '{:1.2f}%'.format(height/total*100),\n            ha=\"center\", fontsize=14) \ng.set_ylim(0, max(sizes) * 1.15)\n\nplt.show()", "processed": ["nice func give u lot cool use inform two featur miss valu entri exit streetnam", "citi start explor distribut citi categor categori insid"]}, {"markdown": ["We can note that:\n- The most common value is Philadelphia and it have 45.29% of the total entries.\n- The other categories don't have a so discrepant difference between them. \n?\nLet's \n", "# Date Features\n- Hour Distribution\n- Month Distribution\n"], "code": "# Reference: https://www.kaggle.com/code/kabure/insightful-eda-modeling-lgbm-hyperopt\n\ntmp_hour = df_train.groupby(['City', 'Hour'])['RowId'].nunique().reset_index()\nplt.figure(figsize=(15,12))\n\nplt.subplot(211)\ng = sns.countplot(x=\"Hour\", data=df_train, hue='City', dodge=True)\ng.set_title(\"Hour Count Distribution by Week and Weekend Days\", fontsize=20)\ng.set_ylabel(\"Count\",fontsize= 17)\ng.set_xlabel(\"Hours of Day\", fontsize=17)\nsizes=[]\nfor p in g.patches:\n    height = p.get_height()\n    sizes.append(height)\n\ng.set_ylim(0, max(sizes) * 1.15)\n\nplt.subplot(212)\ng1 = sns.countplot(x=\"Month\", data=df_train, hue='City', dodge=True)\ng1.set_title(\"Hour Count Distribution by Week and Weekend Days\", fontsize=20)\ng1.set_ylabel(\"Count\",fontsize= 17)\ng1.set_xlabel(\"Hours of Day\", fontsize=17)\nsizes=[]\nfor p in g1.patches:\n    height = p.get_height()\n    sizes.append(height)\n\ng1.set_ylim(0, max(sizes) * 1.15)\n\nplt.subplots_adjust(hspace = 0.3)\n\nplt.show()", "processed": ["note common valu philadelphia 45 29 total entri categori discrep differ let", "date featur hour distribut month distribut"]}, {"markdown": ["Cool. <br>\n\nIn the hours chart:\n- We can see that cities can have different hours patterns.\n- Philadelphia is by far the most common in all hours. Only on 5 a.m that is almost lose to Boston in total entries.\n- Atlanta is the city with less entries in all day, but after 17 p.m to 4a.m it's the second city with more rides \n\nIn the month chart:\n- We can note that the data is about only 6 months (with few values in January and May)\n- Also, the pattern of the Boston City improved througout the time and the others seem very unchanged. \n\nNow, let's explore the Entry and Exit features.\n", "# EntryHeading and Exit Heading"], "code": "# Reference: https://www.kaggle.com/code/kabure/insightful-eda-modeling-lgbm-hyperopt\n\nplt.figure(figsize=(15,12))\n\ntmp = round(((df_train.groupby(['EntryHeading'])['RowId'].nunique() / total) * 100)).reset_index()\n\nplt.subplot(211)\ng = sns.countplot(x=\"EntryHeading\",\n                  data=df_train,\n                  order=list(tmp['EntryHeading'].values),\n                  hue='ExitHeading', dodge=True)\ng.set_title(\"Entry Heading by Exit Heading\", fontsize=20)\ng.set_ylabel(\"Count\",fontsize= 17)\ng.set_xlabel(\"Entry Heading Region\", fontsize=17)\ngt = g.twinx()\ngt = sns.pointplot(x='EntryHeading', y='RowId', \n                   data=tmp, order=list(tmp['EntryHeading'].values),\n                   color='black', legend=False)\ngt.set_ylim(0, tmp['RowId'].max()*1.1)\ngt.set_ylabel(\"% of Total(Black Line)\", fontsize=16)\nsizes=[]\nfor p in g.patches:\n    height = p.get_height()\n    sizes.append(height)\n\ng.set_ylim(0, max(sizes) * 1.15)\n\nplt.subplot(212)\ng1 = sns.countplot(x=\"EntryHeading\", order=list(tmp['EntryHeading'].values), \n                   data=df_train, hue='City')\ng1.set_title(\"Entry Heading Distribution By Cities\", fontsize=20)\ng1.set_ylabel(\"Count\",fontsize= 17)\ng1.set_xlabel(\"Entry Heading Region\", fontsize=17)\nsizes=[]\nfor p in g1.patches:\n    height = p.get_height()\n    sizes.append(height)\n\ng1.set_ylim(0, max(sizes) * 1.15)\n\nplt.subplots_adjust(hspace = 0.3)\n\nplt.show()", "processed": ["cool br hour chart see citi differ hour pattern philadelphia far common hour 5 almost lose boston total entri atlanta citi le entri day 17 p 4a second citi ride month chart note data 6 month valu januari may also pattern boston citi improv througout time other seem unchang let explor entri exit featur", "entryhead exit head"]}, {"markdown": ["Nice. <br>\nIn Entry and Exit Heading chart:\n- We can note that in general the Entry and Exit Region is exactly the same. \n\nIn Entry by Cities chart:\n- We can note the difference patterns on the cities. It's a very interesting and could give us many interesting insights. ", "## IntersectionID "], "code": "# Reference: https://www.kaggle.com/code/kabure/insightful-eda-modeling-lgbm-hyperopt\n\nplt.figure(figsize=(15,6))\ndf_train.IntersectionId.value_counts()[:45].plot(kind='bar')\nplt.xlabel(\"Intersection Number\", fontsize=18)\nplt.ylabel(\"Count\", fontsize=18)\nplt.title(\"TOP 45 most commmon IntersectionID's \", fontsize=22)\n\nplt.show()\ndf_train.groupby(['IntersectionId', 'EntryHeading', 'ExitHeading'])['RowId'].count().reset_index().head()", "processed": ["nice br entri exit head chart note gener entri exit region exactli entri citi chart note differ pattern citi interest could give u mani interest insight", "intersectionid"]}, {"markdown": ["Cool. We can see differet patterns by the Cities and their weekend patterns. ", "# KMeans Clusterization\n- First, I will apply the elbow method to find the correct number of cluster we have in our data\n- After it, we will implement the kmeans with the best quantity"], "code": "# Reference: https://www.kaggle.com/code/kabure/insightful-eda-modeling-lgbm-hyperopt\n\n#sum of squared distances\nssd = []\n\nK = range(1,10)\nfor k in K:\n    km = KMeans(n_clusters=k, random_state=4)\n    km = km.fit(df_train[min_max_cols])\n    ssd.append(km.inertia_)\n    \nplt.plot(K, ssd, 'bx-')\nplt.xlabel('k')\nplt.ylabel('Sum of squared distances')\nplt.title('Elbow Method For Optimal k')\n\nplt.show()", "processed": ["cool see differet pattern citi weekend pattern", "kmean cluster first appli elbow method find correct number cluster data implement kmean best quantiti"]}, {"markdown": ["## Ploting Clusters\n- Understanding the cluster distribution\n- Exploring by Cities"], "code": "# Reference: https://www.kaggle.com/code/kabure/insightful-eda-modeling-lgbm-hyperopt\n\ntmp = pd.crosstab(df_train['City'], df_train['clusters_T'], \n                  normalize='columns').unstack('City').reset_index().rename(columns={0:\"perc\"})\n\ntotal = len(df_train)\nplt.figure(figsize=(15,16))\n\nplt.subplot(311)\ng = sns.countplot(x=\"clusters_T\", data=df_train)\ng.set_title(\"Cluster Target Count Distribution\", fontsize=20)\ng.set_ylabel(\"Count\",fontsize= 17)\ng.set_xlabel(\"Target Cluster Distributions\", fontsize=17)\nsizes=[]\nfor p in g.patches:\n    height = p.get_height()\n    sizes.append(height)\n    g.text(p.get_x()+p.get_width()/2.,\n            height + 3,\n            '{:1.2f}%'.format(height/total*100),\n            ha=\"center\", fontsize=14) \ng.set_ylim(0, max(sizes) * 1.15)\n\nplt.subplot(312)\ng1 = sns.countplot(x=\"clusters_T\", data=df_train, hue='City')\ng1.set_title(\"CITIES - Cluster Target Distribution\", fontsize=20)\ng1.set_ylabel(\"Count\",fontsize= 17)\ng1.set_xlabel(\"Target Cluster Distributions\", fontsize=17)\nsizes=[]\nfor p in g1.patches:\n    height = p.get_height()\n    sizes.append(height)\n    g1.text(p.get_x()+p.get_width()/2.,\n            height + 3,\n            '{:1.2f}%'.format(height/total*100),\n            ha=\"center\", fontsize=10)\ng1.set_ylim(0, max(sizes) * 1.15)\n\nplt.subplot(313)\ng1 = sns.boxplot(x=\"clusters_T\", y='Target_PCA0', \n                 data=df_train, hue='City')\ng1.set_title(\"PCA Feature - Distribution of PCA by Clusters and Cities\", \n             fontsize=20)\ng1.set_ylabel(\"PCA 0 Values\",fontsize= 17)\ng1.set_xlabel(\"Target Cluster Distributions\", fontsize=17)\n\nplt.subplots_adjust(hspace = 0.5)\n\nplt.show()", "processed": ["plote cluster understand cluster distribut explor citi"]}, {"markdown": ["Nice. <br>\n### In the first chart:\n- We can note that the most common cluster is the 1 that have 73% of all data.\n\n### Second chart: \n- Philadelphia is the most common in the first 3 clusters. \n- Boston is the second most common in 0,1 and the most common on Cluster 3;\n- In the second cluster, Atlanta is the second most common city.\n\n### Third Chart:\n- Is clear to understand how the algorithmn divided the data in PCA values\n\n## NOTE: EVERY TIME I RUN IT, THE VALUES CHANGES, SO SORRY BY THE WRONG \n", "# PCA values by CLUSTERS \n- Let's see in another way how the algorithmn have decided by the clusterization"], "code": "# Reference: https://www.kaggle.com/code/kabure/insightful-eda-modeling-lgbm-hyperopt\n\nplt.figure(figsize=(15,6))\n\nsns.scatterplot(x='Target_PCA0', y='Target_PCA1',\n                hue='clusters_T', data=df_train,\n                palette='Set1')\nplt.title(\"PCA 0 and PCA 1 by Clusters\", fontsize=22)\nplt.ylabel(\"Target PCA 1 values\", fontsize=18)\nplt.xlabel(\"Target PCA 0 values\", fontsize=18)\n\nplt.show()", "processed": ["nice br first chart note common cluster 1 73 data second chart philadelphia common first 3 cluster boston second common 0 1 common cluster 3 second cluster atlanta second common citi third chart clear understand algorithmn divid data pca valu note everi time run valu chang sorri wrong", "pca valu cluster let see anoth way algorithmn decid cluster"]}, {"markdown": ["# Data Preprocessing\nhttps://www.kaggle.com/osciiart/covid19-lightgbm"], "code": "# Reference: https://www.kaggle.com/code/robikscube/rob-s-covid19-week-2-submission\n\n# Read in data\ntrain = pd.read_csv(\"../input/covid19-global-forecasting-week-2/train.csv\")\ntest = pd.read_csv(\"../input/covid19-global-forecasting-week-2/test.csv\")\n\ntt = pd.concat([train, test], sort=False)\ntt = train.merge(test, on=['Province_State','Country_Region','Date'], how='outer')\n\n# concat Country/Region and Province/State\ndef name_place(x):\n    try:\n        x_new = x['Country_Region'] + \"_\" + x['Province_State']\n    except:\n        x_new = x['Country_Region']\n    return x_new\ntt['Place'] = tt.apply(lambda x: name_place(x), axis=1)\n# tt = tt.drop(['Province_State','Country_Region'], axis=1)\ntt['Date'] = pd.to_datetime(tt['Date'])\ntt['doy'] = tt['Date'].dt.dayofyear\ntt['dow'] = tt['Date'].dt.dayofweek\ntt['hasProvidence'] = ~tt['Province_State'].isna()\n\n\ncountry_meta = pd.read_csv('../input/covid19-forecasting-metadata/region_metadata.csv')\ntt = tt.merge(country_meta, how='left')\n\ncountry_date_meta = pd.read_csv('../input/covid19-forecasting-metadata/region_date_metadata.csv')\n#tt = tt.merge(country_meta, how='left')\n\ntt['HasFatality'] = tt.groupby('Place')['Fatalities'].transform(lambda x: x.max() > 0)\ntt['HasCases'] = tt.groupby('Place')['ConfirmedCases'].transform(lambda x: x.max() > 0)\n\nfirst_case_date = tt.query('ConfirmedCases >= 1').groupby('Place')['Date'].min().to_dict()\nten_case_date = tt.query('ConfirmedCases >= 10').groupby('Place')['Date'].min().to_dict()\nhundred_case_date = tt.query('ConfirmedCases >= 100').groupby('Place')['Date'].min().to_dict()\nfirst_fatal_date = tt.query('Fatalities >= 1').groupby('Place')['Date'].min().to_dict()\nten_fatal_date = tt.query('Fatalities >= 10').groupby('Place')['Date'].min().to_dict()\nhundred_fatal_date = tt.query('Fatalities >= 100').groupby('Place')['Date'].min().to_dict()\n\ntt['First_Case_Date'] = tt['Place'].map(first_case_date)\ntt['Ten_Case_Date'] = tt['Place'].map(ten_case_date)\ntt['Hundred_Case_Date'] = tt['Place'].map(hundred_case_date)\ntt['First_Fatal_Date'] = tt['Place'].map(first_fatal_date)\ntt['Ten_Fatal_Date'] = tt['Place'].map(ten_fatal_date)\ntt['Hundred_Fatal_Date'] = tt['Place'].map(hundred_fatal_date)\n\ntt['Days_Since_First_Case'] = (tt['Date'] - tt['First_Case_Date']).dt.days\ntt['Days_Since_Ten_Cases'] = (tt['Date'] - tt['Ten_Case_Date']).dt.days\ntt['Days_Since_Hundred_Cases'] = (tt['Date'] - tt['Hundred_Case_Date']).dt.days\ntt['Days_Since_First_Fatal'] = (tt['Date'] - tt['First_Fatal_Date']).dt.days\ntt['Days_Since_Ten_Fatal'] = (tt['Date'] - tt['Ten_Fatal_Date']).dt.days\ntt['Days_Since_Hundred_Fatal'] = (tt['Date'] - tt['Hundred_Fatal_Date']).dt.days\n\n# Merge smoking data\nsmoking = pd.read_csv(\"../input/smokingstats/share-of-adults-who-smoke.csv\")\nsmoking = smoking.rename(columns={'Smoking prevalence, total (ages 15+) (% of adults)': 'Smoking_Rate'})\nsmoking_dict = smoking.groupby('Entity')['Year'].max().to_dict()\nsmoking['LastYear'] = smoking['Entity'].map(smoking_dict)\nsmoking = smoking.query('Year == LastYear').reset_index()\nsmoking['Entity'] = smoking['Entity'].str.replace('United States', 'US')\n\ntt = tt.merge(smoking[['Entity','Smoking_Rate']],\n         left_on='Country_Region',\n         right_on='Entity',\n         how='left',\n         validate='m:1') \\\n    .drop('Entity', axis=1)\n\n# Country data\ncountry_info = pd.read_csv('../input/countryinfo/covid19countryinfo.csv')\n\n\ntt = tt.merge(country_info, left_on=['Country_Region','Province_State'],\n              right_on=['country','region'],\n              how='left',\n              validate='m:1')\n\n# State info from wikipedia\nus_state_info = pd.read_html('https://simple.wikipedia.org/wiki/List_of_U.S._states_by_population')[0] \\\n    [['State','Population estimate, July 1, 2019[2]']] \\\n    .rename(columns={'Population estimate, July 1, 2019[2]' : 'Population'})\n#us_state_info['2019 population'] = pd.to_numeric(us_state_info['2019 population'].str.replace('[note 1]','').replace('[]',''))\n\ntt = tt.merge(us_state_info[['State','Population']],\n         left_on='Province_State',\n         right_on='State',\n         how='left')\n\ntt['pop'] = pd.to_numeric(tt['pop'].str.replace(',',''))\ntt['pop'] = tt['pop'].fillna(tt['Population'])\ntt['pop'] = pd.to_numeric(tt['pop'])\n\ntt['pop_diff'] = tt['pop'] - tt['Population']\ntt['Population_final'] = tt['Population']\ntt.loc[~tt['hasProvidence'], 'Population_final'] = tt.loc[~tt['hasProvidence']]['pop']\n\ntt['Confirmed_Cases_Diff'] = tt.groupby('Place')['ConfirmedCases'].diff()\ntt['Fatailities_Diff'] = tt.groupby('Place')['Fatalities'].diff()\nmax_date = tt.dropna(subset=['ConfirmedCases'])['Date'].max()\ntt['gdp2019'] = pd.to_numeric(tt['gdp2019'].str.replace(',',''))\n# Correcting population for missing countries\n# Googled their names and copied the numbers here\npop_dict = {'Angola': int(29.78 * 10**6),\n            'Australia_Australian Capital Territory': 423_800,\n            'Australia_New South Wales': int(7.544 * 10**6),\n            'Australia_Northern Territory': 244_300,\n            'Australia_Queensland' : int(5.071 * 10**6),\n            'Australia_South Australia' : int(1.677 * 10**6),\n            'Australia_Tasmania': 515_000,\n            'Australia_Victoria': int(6.359 * 10**6),\n            'Australia_Western Australia': int(2.589 * 10**6),\n            'Brazil': int(209.3 * 10**6),\n            'Canada_Alberta' : int(4.371 * 10**6),\n            'Canada_British Columbia' : int(5.071 * 10**6),\n            'Canada_Manitoba' : int(1.369 * 10**6),\n            'Canada_New Brunswick' : 776_827,\n            'Canada_Newfoundland and Labrador' : 521_542,\n            'Canada_Nova Scotia' : 971_395,\n            'Canada_Ontario' : int(14.57 * 10**6),\n            'Canada_Prince Edward Island' : 156_947,\n            'Canada_Quebec' : int(8.485 * 10**6),\n            'Canada_Saskatchewan': int(1.174 * 10**6),\n            'China_Anhui': int(62 * 10**6),\n            'China_Beijing': int(21.54 * 10**6),\n            'China_Chongqing': int(30.48 * 10**6),\n            'China_Fujian' :  int(38.56 * 10**6),\n            'China_Gansu' : int(25.58 * 10**6),\n            'China_Guangdong' : int(113.46 * 10**6),\n            'China_Guangxi' : int(48.38 * 10**6),\n            'China_Guizhou' : int(34.75 * 10**6),\n            'China_Hainan' : int(9.258 * 10**6),\n            'China_Hebei' : int(74.7 * 10**6),\n            'China_Heilongjiang' : int(38.31 * 10**6),\n            'China_Henan' : int(94 * 10**6),\n            'China_Hong Kong' : int(7.392 * 10**6),\n            'China_Hubei' : int(58.5 * 10**6),\n            'China_Hunan' : int(67.37 * 10**6),\n            'China_Inner Mongolia' :  int(24.71 * 10**6),\n            'China_Jiangsu' : int(80.4 * 10**6),\n            'China_Jiangxi' : int(45.2 * 10**6),\n            'China_Jilin' : int(27.3 * 10**6),\n            'China_Liaoning' : int(43.9 * 10**6),\n            'China_Macau' : 622_567,\n            'China_Ningxia' : int(6.301 * 10**6),\n            'China_Qinghai' : int(5.627 * 10**6),\n            'China_Shaanxi' : int(37.33 * 10**6),\n            'China_Shandong' : int(92.48 * 10**6),\n            'China_Shanghai' : int(24.28 * 10**6),\n            'China_Shanxi' : int(36.5 * 10**6),\n            'China_Sichuan' : int(81.1 * 10**6),\n            'China_Tianjin' : int(15 * 10**6),\n            'China_Tibet' : int(3.18 * 10**6),\n            'China_Xinjiang' : int(21.81 * 10**6),\n            'China_Yunnan' : int(45.97 * 10**6),\n            'China_Zhejiang' : int(57.37 * 10**6),\n            'Denmark_Faroe Islands' : 51_783,\n            'Denmark_Greenland' : 56_171,\n            'France_French Guiana' : 290_691,\n            'France_French Polynesia' : 283_007,\n            'France_Guadeloupe' : 395_700,\n            'France_Martinique' : 376_480,\n            'France_Mayotte' : 270_372,\n            'France_New Caledonia' : 99_926,\n            'France_Reunion' : 859_959,\n            'France_Saint Barthelemy' : 9_131,\n            'France_St Martin' : 32_125,\n            'Netherlands_Aruba' : 105_264,\n            'Netherlands_Curacao' : 161_014,\n            'Netherlands_Sint Maarten' : 41_109,\n            'Papua New Guinea' : int(8.251 * 10**6),\n            'US_Guam' : 164_229,\n            'US_Virgin Islands' : 107_268,\n            'United Kingdom_Bermuda' : 65_441,\n            'United Kingdom_Cayman Islands' : 61_559,\n            'United Kingdom_Channel Islands' : 170_499,\n            'United Kingdom_Gibraltar' : 34_571,\n            'United Kingdom_Isle of Man' : 84_287,\n            'United Kingdom_Montserrat' : 4_922\n           }\n\ntt['Population_final'] = tt['Population_final'].fillna(tt['Place'].map(pop_dict))\ntt.loc[tt['Place'] == 'Diamond Princess', 'Population final'] = 2_670\ntt['ConfirmedCases_Log'] = tt['ConfirmedCases'].apply(np.log1p)\ntt['Fatalities_Log'] = tt['Fatalities'].apply(np.log1p)\ntt['Population_final'] = tt['Population_final'].astype('int')\ntt['Cases_Per_100kPop'] = (tt['ConfirmedCases'] / tt['Population_final']) * 100000\ntt['Fatalities_Per_100kPop'] = (tt['Fatalities'] / tt['Population_final']) * 100000\n\ntt['Cases_Percent_Pop'] = ((tt['ConfirmedCases'] / tt['Population_final']) * 100)\ntt['Fatalities_Percent_Pop'] = ((tt['Fatalities'] / tt['Population_final']) * 100)\n\ntt['Cases_Log_Percent_Pop'] = ((tt['ConfirmedCases'] / tt['Population_final']) * 100).apply(np.log1p)\ntt['Fatalities_Log_Percent_Pop'] = ((tt['Fatalities'] / tt['Population_final']) * 100).apply(np.log1p)\n\n\ntt['Max_Confirmed_Cases'] = tt.groupby('Place')['ConfirmedCases'].transform(max)\ntt['Max_Fatalities'] = tt.groupby('Place')['Fatalities'].transform(max)\n\ntt['Max_Cases_Per_100kPop'] = tt.groupby('Place')['Cases_Per_100kPop'].transform(max)\ntt['Max_Fatalities_Per_100kPop'] = tt.groupby('Place')['Fatalities_Per_100kPop'].transform(max)\ntt.query('Date == @max_date') \\\n    .query('Place != \"Diamond Princess\"') \\\n    .query('Cases_Log_Percent_Pop > -10000') \\\n    ['Cases_Log_Percent_Pop'].plot(kind='hist', bins=500)\nplt.show()\ntt.query('Days_Since_Ten_Cases > 0') \\\n    .query('Place != \"Diamond Princess\"') \\\n    .dropna(subset=['Cases_Percent_Pop']) \\\n    .query('Days_Since_Ten_Cases < 40') \\\n    .plot(x='Days_Since_Ten_Cases', y='Cases_Log_Percent_Pop', style='.', figsize=(15, 5), alpha=0.2)\nplt.show()\nPLOT = False\nif PLOT:\n    for x in tt['Place'].unique():\n        try:\n            fig, ax = plt.subplots(1, 4, figsize=(15, 2))\n            tt.query('Place == @x') \\\n                .query('ConfirmedCases > 0') \\\n                .set_index('Date')['Cases_Log_Percent_Pop'] \\\n                .plot(title=f'{x} confirmed log pct pop', ax=ax[0])\n            tt.query('Place == @x') \\\n                .query('ConfirmedCases > 0') \\\n                .set_index('Date')['Cases_Percent_Pop'] \\\n                .plot(title=f'{x} confirmed cases', ax=ax[1])\n            tt.query('Place == @x') \\\n                .query('Fatalities > 0') \\\n                .set_index('Date')['Fatalities_Log_Percent_Pop'] \\\n                .plot(title=f'{x} confirmed log pct pop', ax=ax[2])\n            tt.query('Place == @x') \\\n                .query('Fatalities > 0') \\\n                .set_index('Date')['Fatalities_Percent_Pop'] \\\n                .plot(title=f'{x} confirmed cases', ax=ax[3])\n        except:\n            pass\n        plt.show()\ntt.query('Date == @max_date')[['Place','Max_Cases_Per_100kPop',\n                               'Max_Fatalities_Per_100kPop','Max_Confirmed_Cases',\n                               'Population_final',\n                              'Days_Since_First_Case',\n                              'Confirmed_Cases_Diff']] \\\n    .drop_duplicates() \\\n    .sort_values('Max_Cases_Per_100kPop', ascending=False)", "processed": ["data preprocess http www kaggl com osciiart covid19 lightgbm"]}, {"markdown": ["# Features about cases since first case/ fatality"], "code": "# Reference: https://www.kaggle.com/code/robikscube/rob-s-covid19-week-2-submission\n\ntt['Past_7Days_ConfirmedCases_Std'] = tt['Place'].map(tt.dropna(subset=['ConfirmedCases']).query('Date >= 20200324').groupby('Place')['ConfirmedCases'].std().to_dict())\ntt['Past_7Days_Fatalities_Std'] = tt['Place'].map(tt.dropna(subset=['ConfirmedCases']).query('Date >= 20200324').groupby('Place')['Fatalities'].std().to_dict())\n\ntt['Past_7Days_ConfirmedCases_Min'] = tt['Place'].map(tt.dropna(subset=['ConfirmedCases']).query('Date >= 20200324').groupby('Place')['ConfirmedCases'].min().to_dict())\ntt['Past_7Days_Fatalities_Min'] = tt['Place'].map(tt.dropna(subset=['Fatalities']).query('Date >= 20200324').groupby('Place')['Fatalities'].min().to_dict())\n\ntt['Past_7Days_ConfirmedCases_Max'] = tt['Place'].map(tt.dropna(subset=['ConfirmedCases']).query('Date >= 20200324').groupby('Place')['ConfirmedCases'].max().to_dict())\ntt['Past_7Days_Fatalities_Max'] = tt['Place'].map(tt.dropna(subset=['Fatalities']).query('Date >= 20200324').groupby('Place')['Fatalities'].max().to_dict())\n\ntt['Past_7Days_Confirmed_Change_of_Total'] = (tt['Past_7Days_ConfirmedCases_Max'] - tt['Past_7Days_ConfirmedCases_Min']) / (tt['Past_7Days_ConfirmedCases_Max'])\ntt['Past_7Days_Fatalities_Change_of_Total'] = (tt['Past_7Days_Fatalities_Max'] - tt['Past_7Days_Fatalities_Min']) / (tt['Past_7Days_Fatalities_Max'])\ntt['Past_21Days_ConfirmedCases_Std'] = tt['Place'].map(tt.dropna(subset=['ConfirmedCases']).query('Date >= 20200310').groupby('Place')['ConfirmedCases'].std().to_dict())\ntt['Past_21Days_Fatalities_Std'] = tt['Place'].map(tt.dropna(subset=['ConfirmedCases']).query('Date >= 20200310').groupby('Place')['Fatalities'].std().to_dict())\n\ntt['Past_21Days_ConfirmedCases_Min'] = tt['Place'].map(tt.dropna(subset=['ConfirmedCases']).query('Date >= 20200310').groupby('Place')['ConfirmedCases'].min().to_dict())\ntt['Past_21Days_Fatalities_Min'] = tt['Place'].map(tt.dropna(subset=['Fatalities']).query('Date >= 20200324').groupby('Place')['Fatalities'].min().to_dict())\n\ntt['Past_21Days_ConfirmedCases_Max'] = tt['Place'].map(tt.dropna(subset=['ConfirmedCases']).query('Date >= 20200310').groupby('Place')['ConfirmedCases'].max().to_dict())\ntt['Past_21Days_Fatalities_Max'] = tt['Place'].map(tt.dropna(subset=['Fatalities']).query('Date >= 20200310').groupby('Place')['Fatalities'].max().to_dict())\n\ntt['Past_21Days_Confirmed_Change_of_Total'] = (tt['Past_21Days_ConfirmedCases_Max'] - tt['Past_21Days_ConfirmedCases_Min']) / (tt['Past_21Days_ConfirmedCases_Max'])\ntt['Past_21Days_Fatalities_Change_of_Total'] = (tt['Past_21Days_Fatalities_Max'] - tt['Past_21Days_Fatalities_Min']) / (tt['Past_21Days_Fatalities_Max'])\n\ntt['Past_7Days_Fatalities_Change_of_Total'] = tt['Past_7Days_Fatalities_Change_of_Total'].fillna(0)\ntt['Past_21Days_Fatalities_Change_of_Total'] = tt['Past_21Days_Fatalities_Change_of_Total'].fillna(0)\ntt['Date_7Days_Since_First_Case'] = tt['Place'].map(tt.loc[tt['Days_Since_First_Case'] == 7] \\\n    .set_index('Place')['Date'] \\\n    .to_dict())\ntt['Date_14Days_Since_First_Case'] = tt['Place'].map(tt.loc[tt['Days_Since_First_Case'] == 14] \\\n    .set_index('Place')['Date'] \\\n    .to_dict())\ntt['Date_21Days_Since_First_Case'] = tt['Place'].map(tt.loc[tt['Days_Since_First_Case'] == 21] \\\n    .set_index('Place')['Date'] \\\n    .to_dict())\ntt['Date_28Days_Since_First_Case'] = tt['Place'].map(tt.loc[tt['Days_Since_First_Case'] == 28] \\\n    .set_index('Place')['Date'] \\\n    .to_dict())\ntt['Date_35Days_Since_First_Case'] = tt['Place'].map(tt.loc[tt['Days_Since_First_Case'] == 35] \\\n    .set_index('Place')['Date'] \\\n    .to_dict())\ntt['Date_60Days_Since_First_Case'] = tt['Place'].map(tt.loc[tt['Days_Since_First_Case'] == 60] \\\n    .set_index('Place')['Date'] \\\n    .to_dict())\n\ntt['Date_7Days_Since_Ten_Cases'] = tt['Place'].map(tt.loc[tt['Days_Since_Ten_Cases'] == 7] \\\n    .set_index('Place')['Date'] \\\n    .to_dict())\ntt['Date_14Days_Since_Ten_Cases'] = tt['Place'].map(tt.loc[tt['Days_Since_Ten_Cases'] == 14] \\\n    .set_index('Place')['Date'] \\\n    .to_dict())\ntt['Date_21Days_Since_Ten_Cases'] = tt['Place'].map(tt.loc[tt['Days_Since_Ten_Cases'] == 21] \\\n    .set_index('Place')['Date'] \\\n    .to_dict())\ntt['Date_28Days_Since_Ten_Cases'] = tt['Place'].map(tt.loc[tt['Days_Since_Ten_Cases'] == 28] \\\n    .set_index('Place')['Date'] \\\n    .to_dict())\ntt['Date_35Days_Since_Ten_Cases'] = tt['Place'].map(tt.loc[tt['Days_Since_Ten_Cases'] == 35] \\\n    .set_index('Place')['Date'] \\\n    .to_dict())\ntt['Date_60Days_Since_Ten_Cases'] = tt['Place'].map(tt.loc[tt['Days_Since_Ten_Cases'] == 60] \\\n    .set_index('Place')['Date'] \\\n    .to_dict())\n\n\ntt['Date_7Days_Since_First_Fatal'] = tt['Place'].map(tt.loc[tt['Days_Since_First_Fatal'] == 7] \\\n    .set_index('Place')['Date'] \\\n    .to_dict())\ntt['Date_14Days_Since_First_Fatal'] = tt['Place'].map(tt.loc[tt['Days_Since_First_Fatal'] == 14] \\\n    .set_index('Place')['Date'] \\\n    .to_dict())\ntt['Date_21Days_Since_First_Fatal'] = tt['Place'].map(tt.loc[tt['Days_Since_First_Fatal'] == 21] \\\n    .set_index('Place')['Date'] \\\n    .to_dict())\ntt['Date_28Days_Since_First_Fatal'] = tt['Place'].map(tt.loc[tt['Days_Since_First_Fatal'] == 28] \\\n    .set_index('Place')['Date'] \\\n    .to_dict())\ntt['Date_35Days_Since_First_Fatal'] = tt['Place'].map(tt.loc[tt['Days_Since_First_Fatal'] == 35] \\\n    .set_index('Place')['Date'] \\\n    .to_dict())\ntt['Date_60Days_Since_First_Fatal'] = tt['Place'].map(tt.loc[tt['Days_Since_First_Fatal'] == 60] \\\n    .set_index('Place')['Date'] \\\n    .to_dict())\ntt['CC_7D_1stCase'] = tt.loc[tt['Date_7Days_Since_First_Case'] == tt['Date']]['ConfirmedCases']\ntt['CC_14D_1stCase'] = tt.loc[tt['Date_14Days_Since_First_Case'] == tt['Date']]['ConfirmedCases']\ntt['CC_21D_1stCase'] = tt.loc[tt['Date_21Days_Since_First_Case'] == tt['Date']]['ConfirmedCases']\ntt['CC_28D_1stCase'] = tt.loc[tt['Date_28Days_Since_First_Case'] == tt['Date']]['ConfirmedCases']\ntt['CC_35D_1stCase'] = tt.loc[tt['Date_35Days_Since_First_Case'] == tt['Date']]['ConfirmedCases']\ntt['CC_60D_1stCase'] = tt.loc[tt['Date_60Days_Since_First_Case'] == tt['Date']]['ConfirmedCases']\n\ntt['F_7D_1stCase'] = tt.loc[tt['Date_7Days_Since_First_Case'] == tt['Date']]['Fatalities']\ntt['F_14D_1stCase'] = tt.loc[tt['Date_14Days_Since_First_Case'] == tt['Date']]['Fatalities']\ntt['F_21D_1stCase'] = tt.loc[tt['Date_21Days_Since_First_Case'] == tt['Date']]['Fatalities']\ntt['F_28D_1stCase'] = tt.loc[tt['Date_28Days_Since_First_Case'] == tt['Date']]['Fatalities']\ntt['F_35D_1stCase'] = tt.loc[tt['Date_35Days_Since_First_Case'] == tt['Date']]['Fatalities']\ntt['F_60D_1stCase'] = tt.loc[tt['Date_60Days_Since_First_Case'] == tt['Date']]['Fatalities']\n\ntt['CC_7D_10Case'] = tt.loc[tt['Date_7Days_Since_Ten_Cases'] == tt['Date']]['ConfirmedCases']\ntt['CC_14D_10Case'] = tt.loc[tt['Date_14Days_Since_Ten_Cases'] == tt['Date']]['ConfirmedCases']\ntt['CC_21D_10Case'] = tt.loc[tt['Date_21Days_Since_Ten_Cases'] == tt['Date']]['ConfirmedCases']\ntt['CC_28D_10Case'] = tt.loc[tt['Date_28Days_Since_Ten_Cases'] == tt['Date']]['ConfirmedCases']\ntt['CC_35D_10Case'] = tt.loc[tt['Date_35Days_Since_Ten_Cases'] == tt['Date']]['ConfirmedCases']\ntt['CC_60D_10Case'] = tt.loc[tt['Date_60Days_Since_Ten_Cases'] == tt['Date']]['ConfirmedCases']\n\ntt['F_7D_10Case'] = tt.loc[tt['Date_7Days_Since_Ten_Cases'] == tt['Date']]['Fatalities']\ntt['F_14D_10Case'] = tt.loc[tt['Date_14Days_Since_Ten_Cases'] == tt['Date']]['Fatalities']\ntt['F_21D_10Case'] = tt.loc[tt['Date_21Days_Since_Ten_Cases'] == tt['Date']]['Fatalities']\ntt['F_28D_10Case'] = tt.loc[tt['Date_28Days_Since_Ten_Cases'] == tt['Date']]['Fatalities']\ntt['F_35D_10Case'] = tt.loc[tt['Date_35Days_Since_Ten_Cases'] == tt['Date']]['Fatalities']\ntt['F_60D_10Case'] = tt.loc[tt['Date_60Days_Since_Ten_Cases'] == tt['Date']]['Fatalities']\n\ntt['CC_7D_1Fatal'] = tt.loc[tt['Date_7Days_Since_First_Fatal'] == tt['Date']]['ConfirmedCases']\ntt['CC_14D_1Fatal'] = tt.loc[tt['Date_14Days_Since_First_Fatal'] == tt['Date']]['ConfirmedCases']\ntt['CC_21D_1Fatal'] = tt.loc[tt['Date_21Days_Since_First_Fatal'] == tt['Date']]['ConfirmedCases']\ntt['CC_28D_1Fatal'] = tt.loc[tt['Date_28Days_Since_First_Fatal'] == tt['Date']]['ConfirmedCases']\ntt['CC_35D_1Fatal'] = tt.loc[tt['Date_35Days_Since_First_Fatal'] == tt['Date']]['ConfirmedCases']\ntt['CC_60D_1Fatal'] = tt.loc[tt['Date_60Days_Since_First_Fatal'] == tt['Date']]['ConfirmedCases']\n\ntt['F_7D_1Fatal'] = tt.loc[tt['Date_7Days_Since_First_Fatal'] == tt['Date']]['Fatalities']\ntt['F_14D_1Fatal'] = tt.loc[tt['Date_14Days_Since_First_Fatal'] == tt['Date']]['Fatalities']\ntt['F_21D_1Fatal'] = tt.loc[tt['Date_21Days_Since_First_Fatal'] == tt['Date']]['Fatalities']\ntt['F_28D_1Fatal'] = tt.loc[tt['Date_28Days_Since_First_Fatal'] == tt['Date']]['Fatalities']\ntt['F_35D_1Fatal'] = tt.loc[tt['Date_35Days_Since_First_Fatal'] == tt['Date']]['Fatalities']\ntt['F_60D_1Fatal'] = tt.loc[tt['Date_60Days_Since_First_Fatal'] == tt['Date']]['Fatalities']\nfig, axs = plt.subplots(1, 2, figsize=(15, 5))\ntt[['Place','Past_7Days_Confirmed_Change_of_Total','Past_7Days_Fatalities_Change_of_Total',\n    'Past_7Days_ConfirmedCases_Max','Past_7Days_ConfirmedCases_Min',\n   'Past_7Days_Fatalities_Max','Past_7Days_Fatalities_Min']] \\\n    .drop_duplicates() \\\n    .sort_values('Past_7Days_Confirmed_Change_of_Total')['Past_7Days_Confirmed_Change_of_Total'] \\\n    .plot(kind='hist', bins=50, title='Distribution of Pct change confirmed past 7 days', ax=axs[0])\ntt[['Place','Past_21Days_Confirmed_Change_of_Total','Past_21Days_Fatalities_Change_of_Total',\n    'Past_21Days_ConfirmedCases_Max','Past_21Days_ConfirmedCases_Min',\n   'Past_21Days_Fatalities_Max','Past_21Days_Fatalities_Min']] \\\n    .drop_duplicates() \\\n    .sort_values('Past_21Days_Confirmed_Change_of_Total')['Past_21Days_Confirmed_Change_of_Total'] \\\n    .plot(kind='hist', bins=50, title='Distribution of Pct change confirmed past 21 days', ax=axs[1])\nplt.show()\n\nfig, axs = plt.subplots(1, 2, figsize=(15, 5))\ntt[['Place','Past_7Days_Fatalities_Change_of_Total']] \\\n    .drop_duplicates()['Past_7Days_Fatalities_Change_of_Total'] \\\n    .plot(kind='hist', bins=50, title='Distribution of Pct change confirmed past 7 days', ax=axs[0])\ntt[['Place', 'Past_21Days_Fatalities_Change_of_Total']] \\\n    .drop_duplicates()['Past_21Days_Fatalities_Change_of_Total'] \\\n    .plot(kind='hist', bins=50, title='Distribution of Pct change confirmed past 21 days', ax=axs[1])\nplt.show()\n# Example of flat prop\ntt.query(\"Place == 'China_Chongqing'\").set_index('Date')['ConfirmedCases'].dropna().plot(figsize=(15, 5))\nplt.show()", "processed": ["featur case sinc first case fatal"]}, {"markdown": ["# Other Data Prep"], "code": "# Reference: https://www.kaggle.com/code/robikscube/rob-s-covid19-week-2-submission\n\n# Example of flat prop\ntt.query(\"Place == 'Italy'\").set_index('Date')[['ConfirmedCases']].dropna().plot(figsize=(15, 5))\nplt.show()\ntt.query(\"Place == 'Italy'\").set_index('Date')[['ConfirmedCases_Log']].dropna().plot(figsize=(15, 5))\nplt.show()\nlatest_summary_stats = tt.query('Date == @max_date') \\\n    [['Country_Region',\n      'Place',\n      'Max_Cases_Per_100kPop',\n      'Max_Fatalities_Per_100kPop',\n      'Max_Confirmed_Cases',\n      'Population_final',\n      'Days_Since_First_Case',\n      'Days_Since_Ten_Cases']] \\\n    .drop_duplicates()\nlatest_summary_stats.query('Place != \"Diamond Princess\"') \\\n    .query('Country_Region != \"China\"') \\\n    .plot(y='Max_Cases_Per_100kPop',\n          x='Days_Since_Ten_Cases',\n          style='.',\n          figsize=(15, 5))\ntt.query('Province_State == \"Maryland\"')[['ConfirmedCases','Confirmed_Cases_Diff']]", "processed": ["data prep"]}, {"markdown": ["# Results dont look good"], "code": "# Reference: https://www.kaggle.com/code/robikscube/rob-s-covid19-week-2-submission\n\ntest.set_index('Date')[['Place','ConfirmedCases_pred','ConfirmedCases']].query('Place == \"US_Maryland\"').plot(figsize=(15, 5))\nplt.show()\n\ntest.set_index('Date')[['Place','ConfirmedCases_pred','ConfirmedCases']].query('Place == \"Bhutan\"').plot(figsize=(15, 5))\nplt.show()", "processed": ["result dont look good"]}, {"markdown": ["# US States"], "code": "# Reference: https://www.kaggle.com/code/robikscube/rob-s-covid19-week-2-submission\n\nmyplace = \"US_Florida\"\ntt.query('Place == @myplace and Days_Since_First_Fatal >= 0')[['Days_Since_First_Fatal','Fatalities_Log']]\nfor myplace in us_states:\n    try:\n        # Confirmed Cases\n        fig, axs = plt.subplots(1, 2, figsize=(15, 3))\n        dat = tt.query('Place == @myplace and Days_Since_Ten_Cases >= 0')[['Days_Since_Ten_Cases','ConfirmedCases_Log']].dropna()\n        X = dat['Days_Since_Ten_Cases']\n        y = dat['ConfirmedCases_Log']\n        y = y.cummax()\n        dat_all = tt.query('Place == @myplace and Days_Since_Ten_Cases >= 0')[['Days_Since_Ten_Cases','ConfirmedCases_Log']]\n        X_pred = dat_all['Days_Since_Ten_Cases']\n        en = ElasticNet()\n        en.fit(X.values.reshape(-1, 1), y.values)\n        preds = en.predict(X_pred.values.reshape(-1, 1))\n        tt.loc[(tt['Place'] == myplace) & (tt['Days_Since_Ten_Cases'] >= 0), 'ConfirmedCases_Log_Pred1'] = preds\n        tt.loc[(tt['Place'] == myplace), 'ConfirmedCases_Pred1'] = tt['ConfirmedCases_Log_Pred1'].apply(np.expm1)\n        # Cap at 10 % Population\n        pop_myplace = tt.query('Place == @myplace')['Population_final'].values[0]\n        tt.loc[(tt['Place'] == myplace) & (tt['ConfirmedCases_Pred1'] > (0.1 * pop_myplace)), 'ConfirmedCases_Pred1'] = (0.1 * pop_myplace)\n        ax = tt.query('Place == @myplace').set_index('Date')[['ConfirmedCases','ConfirmedCases_Pred1']].plot(figsize=(15, 5), title=myplace, ax=axs[0])\n        # Fatalities\n        # If low count then do percent of confirmed:\n        dat = tt.query('Place == @myplace and Days_Since_Ten_Cases >= 0')[['Days_Since_Ten_Cases','Fatalities_Log']].dropna()\n        if len(dat) < 5:\n            tt.loc[(tt['Place'] == myplace), 'Fatalities_Pred1'] = tt.loc[(tt['Place'] == myplace)]['ConfirmedCases_Pred1'] * 0.001\n        elif tt.query('Place == @myplace')['Fatalities'].max() < 5:\n            tt.loc[(tt['Place'] == myplace), 'Fatalities_Pred1'] = tt.loc[(tt['Place'] == myplace)]['ConfirmedCases_Pred1'] * 0.001\n        else:\n            X = dat['Days_Since_Ten_Cases']\n            y = dat['Fatalities_Log']\n            y = y.cummax()\n            dat_all = tt.query('Place == @myplace and Days_Since_Ten_Cases >= 0')[['Days_Since_Ten_Cases','Fatalities_Log']]\n            X_pred = dat_all['Days_Since_Ten_Cases']\n            en = ElasticNet()\n            en.fit(X.values.reshape(-1, 1), y.values)\n            preds = en.predict(X_pred.values.reshape(-1, 1))\n            tt.loc[(tt['Place'] == myplace) & (tt['Days_Since_Ten_Cases'] >= 0), 'Fatalities_Log_Pred1'] = preds\n            tt.loc[(tt['Place'] == myplace), 'Fatalities_Pred1'] = tt['Fatalities_Log_Pred1'].apply(np.expm1)\n\n            # Cap at 0.0001 Population\n            pop_myplace = tt.query('Place == @myplace')['Population_final'].values[0]\n            tt.loc[(tt['Place'] == myplace) & (tt['Fatalities_Pred1'] > (0.0005 * pop_myplace)), 'Fatalities_Pred1'] = (0.0005 * pop_myplace)\n\n        ax = tt.query('Place == @myplace').set_index('Date')[['Fatalities','Fatalities_Pred1']].plot(figsize=(15, 5), title=myplace, ax=axs[1])\n        plt.show()\n    except:\n        print(f'============= FAILED FOR {myplace} =============')\n\nmyplace = 'US_Virgin Islands'\ndat = tt.query('Place == @myplace and Days_Since_Ten_Cases >= 0')[['Place','Days_Since_Ten_Cases','ConfirmedCases']].dropna()\ntt.loc[tt['Place'].isin(us_states)].groupby('Place')['Fatalities_Pred1'].max().sum()", "processed": ["u state"]}, {"markdown": ["# Deal with Flattened location"], "code": "# Reference: https://www.kaggle.com/code/robikscube/rob-s-covid19-week-2-submission\n\nconstant_fatal_places\ntt.loc[tt['Place'].isin(constant_fatal_places), 'ConfirmedCases_Pred1'] = tt.loc[tt['Place'].isin(constant_fatal_places)]['Place'].map(tt.loc[tt['Place'].isin(constant_fatal_places)].groupby('Place')['ConfirmedCases'].max())\ntt.loc[tt['Place'].isin(constant_fatal_places), 'Fatalities_Pred1'] = tt.loc[tt['Place'].isin(constant_fatal_places)]['Place'].map(tt.loc[tt['Place'].isin(constant_fatal_places)].groupby('Place')['Fatalities'].max())\nfor myplace in constant_fatal_places:\n    fig, axs = plt.subplots(1, 2, figsize=(15, 3))\n    ax = tt.query('Place == @myplace').set_index('Date')[['ConfirmedCases','ConfirmedCases_Pred1']].plot(figsize=(15, 5), title=myplace, ax=axs[0])\n    ax = tt.query('Place == @myplace').set_index('Date')[['Fatalities','Fatalities_Pred1']].plot(figsize=(15, 5), title=myplace, ax=axs[1])\n    plt.show()", "processed": ["deal flatten locat"]}, {"markdown": ["# Remaining Locations\n## 217 left"], "code": "# Reference: https://www.kaggle.com/code/robikscube/rob-s-covid19-week-2-submission\n\nremaining_places = pd.DataFrame(tt.groupby('Place')['ConfirmedCases_Pred1'].max().isna()).query('ConfirmedCases_Pred1').index.values\nprint(remaining_places)\nprint(len(remaining_places))\nfor myplace in remaining_places:\n    try:\n        # Confirmed Cases\n        fig, axs = plt.subplots(1, 2, figsize=(15, 3))\n        dat = tt.query('Place == @myplace and Days_Since_Ten_Cases >= 0')[['Days_Since_Ten_Cases','ConfirmedCases_Log']].dropna()\n        X = dat['Days_Since_Ten_Cases']\n        y = dat['ConfirmedCases_Log']\n        y = y.cummax()\n        dat_all = tt.query('Place == @myplace and Days_Since_Ten_Cases >= 0')[['Days_Since_Ten_Cases','ConfirmedCases_Log']]\n        X_pred = dat_all['Days_Since_Ten_Cases']\n        en = ElasticNet()\n        en.fit(X.values.reshape(-1, 1), y.values)\n        preds = en.predict(X_pred.values.reshape(-1, 1))\n        tt.loc[(tt['Place'] == myplace) & (tt['Days_Since_Ten_Cases'] >= 0), 'ConfirmedCases_Log_Pred1'] = preds\n        tt.loc[(tt['Place'] == myplace), 'ConfirmedCases_Pred1'] = tt['ConfirmedCases_Log_Pred1'].apply(np.expm1)\n        # Cap at 10 % Population\n        pop_myplace = tt.query('Place == @myplace')['Population_final'].values[0]\n        tt.loc[(tt['Place'] == myplace) & (tt['ConfirmedCases_Pred1'] > (0.1 * pop_myplace)), 'ConfirmedCases_Pred1'] = (0.1 * pop_myplace)\n        ax = tt.query('Place == @myplace').set_index('Date')[['ConfirmedCases','ConfirmedCases_Pred1']].plot(figsize=(15, 5), title=myplace, ax=axs[0])\n        # Fatalities\n        # If low count then do percent of confirmed:\n        dat = tt.query('Place == @myplace and Days_Since_Ten_Cases >= 0')[['Days_Since_Ten_Cases','Fatalities_Log']].dropna()\n        if len(dat) < 5:\n            tt.loc[(tt['Place'] == myplace), 'Fatalities_Pred1'] = tt.loc[(tt['Place'] == myplace)]['ConfirmedCases_Pred1'] * 0.001\n        elif tt.query('Place == @myplace')['Fatalities'].max() < 5:\n            tt.loc[(tt['Place'] == myplace), 'Fatalities_Pred1'] = tt.loc[(tt['Place'] == myplace)]['ConfirmedCases_Pred1'] * 0.001\n        else:\n            X = dat['Days_Since_Ten_Cases']\n            y = dat['Fatalities_Log']\n            y = y.cummax()\n            dat_all = tt.query('Place == @myplace and Days_Since_Ten_Cases >= 0')[['Days_Since_Ten_Cases','Fatalities_Log']]\n            X_pred = dat_all['Days_Since_Ten_Cases']\n            en = ElasticNet()\n            en.fit(X.values.reshape(-1, 1), y.values)\n            preds = en.predict(X_pred.values.reshape(-1, 1))\n            tt.loc[(tt['Place'] == myplace) & (tt['Days_Since_Ten_Cases'] >= 0), 'Fatalities_Log_Pred1'] = preds\n            tt.loc[(tt['Place'] == myplace), 'Fatalities_Pred1'] = tt['Fatalities_Log_Pred1'].apply(np.expm1)\n\n            # Cap at 0.0001 Population\n            pop_myplace = tt.query('Place == @myplace')['Population_final'].values[0]\n            tt.loc[(tt['Place'] == myplace) & (tt['Fatalities_Pred1'] > (0.0005 * pop_myplace)), 'Fatalities_Pred1'] = (0.0005 * pop_myplace)\n\n        ax = tt.query('Place == @myplace').set_index('Date')[['Fatalities','Fatalities_Pred1']].plot(figsize=(15, 5), title=myplace, ax=axs[1])\n        plt.show()\n    except:\n        print(f'============= FAILED FOR {myplace} =============')\n        ax = tt.query('Place == @myplace').set_index('Date')[['ConfirmedCases','ConfirmedCases_Pred1']].plot(figsize=(15, 5), title=myplace, ax=axs[0])\n        ax = tt.query('Place == @myplace').set_index('Date')[['Fatalities','Fatalities_Pred1']].plot(figsize=(15, 5), title=myplace, ax=axs[1])\n# Estimated total\ntt.groupby('Place')['Fatalities_Pred1'].max().sum()\n# Clean Up any time the actual is less than the real\ntt['ConfirmedCases_Pred1'] = tt[['ConfirmedCases','ConfirmedCases_Pred1']].max(axis=1)\ntt['Fatalities_Pred1'] = tt[['Fatalities','Fatalities_Pred1']].max(axis=1)\n\ntt['ConfirmedCases_Pred1'] = tt['ConfirmedCases_Pred1'].fillna(0)\ntt['Fatalities_Pred1'] = tt['Fatalities_Pred1'].fillna(0)\n\n# Fill pred with\ntt.loc[~tt['ConfirmedCases'].isna(), 'ConfirmedCases_Pred1'] = tt.loc[~tt['ConfirmedCases'].isna()]['ConfirmedCases']\ntt.loc[~tt['Fatalities'].isna(), 'Fatalities_Pred1'] = tt.loc[~tt['Fatalities'].isna()]['Fatalities']\n\ntt['ConfirmedCases_Pred1'] = tt.groupby('Place')['ConfirmedCases_Pred1'].transform('cummax')\ntt['Fatalities_Pred1'] = tt.groupby('Place')['Fatalities_Pred1'].transform('cummax')", "processed": ["remain locat 217 left"]}, {"markdown": ["# Plot them all!!"], "code": "# Reference: https://www.kaggle.com/code/robikscube/rob-s-covid19-week-2-submission\n\nfor myplace in tt['Place'].unique():\n    try:\n        # Confirmed Cases\n        fig, axs = plt.subplots(1, 2, figsize=(15, 3))\n        ax = tt.query('Place == @myplace').set_index('Date')[['ConfirmedCases','ConfirmedCases_Pred1']].plot(title=myplace, ax=axs[0])\n        ax = tt.query('Place == @myplace').set_index('Date')[['Fatalities','Fatalities_Pred1']].plot(title=myplace, ax=axs[1])\n        plt.show()\n    except:\n        print(f'============= FAILED FOR {myplace} =============')\ntt.groupby('Place')['Fatalities_Pred1'].max().sort_values()\n# Questionable numbers\ntt.query('Place == \"Iran\"').set_index('Date')[['ConfirmedCases',\n                                               'ConfirmedCases_Pred1',]].plot(figsize=(15, 5))\n# Make Iran's Predictions Linear\n\ntt.query('Place == \"Iran\"').set_index('Date')[['Fatalities','Fatalities_Pred1']].plot(figsize=(15, 5))", "processed": ["plot"]}, {"markdown": ["# Make Iran's Predictions Linear"], "code": "# Reference: https://www.kaggle.com/code/robikscube/rob-s-covid19-week-2-submission\n\ndat.iloc[-10:]\nfor myplace in ['Iran']:\n\n    # Confirmed Cases\n    fig, axs = plt.subplots(1, 2, figsize=(15, 3))\n    dat = tt.query('Place == @myplace and Days_Since_Ten_Cases >= 0')[['Days_Since_Ten_Cases','ConfirmedCases']].dropna()\n    dat = dat.iloc[-10:]\n    X = dat['Days_Since_Ten_Cases']\n    y = dat['ConfirmedCases']\n    y = y.cummax()\n    dat_all = tt.query('Place == @myplace and Days_Since_Ten_Cases >= 0')[['Days_Since_Ten_Cases','ConfirmedCases']]\n    X_pred = dat_all['Days_Since_Ten_Cases']\n    en = ElasticNet()\n    en.fit(X.values.reshape(-1, 1), y.values)\n    preds = en.predict(X_pred.values.reshape(-1, 1))\n    tt.loc[(tt['Place'] == myplace) & (tt['Days_Since_Ten_Cases'] >= 0), 'ConfirmedCases_Pred1'] = preds\n    # Cap at 10 % Population\n    pop_myplace = tt.query('Place == @myplace')['Population_final'].values[0]\n    tt.loc[(tt['Place'] == myplace) & (tt['ConfirmedCases_Pred1'] > (0.1 * pop_myplace)), 'ConfirmedCases_Pred1'] = (0.1 * pop_myplace)\n    ax = tt.query('Place == @myplace').set_index('Date')[['ConfirmedCases','ConfirmedCases_Pred1']].plot(figsize=(15, 5), title=myplace, ax=axs[0])\n    # Fatalities\n    # If low count then do percent of confirmed:\n    dat = tt.query('Place == @myplace and Days_Since_Ten_Cases >= 0')[['Days_Since_Ten_Cases','Fatalities']].dropna()\n    dat = dat.iloc[-10:]\n    if len(dat) < 5:\n        tt.loc[(tt['Place'] == myplace), 'Fatalities_Pred1'] = tt.loc[(tt['Place'] == myplace)]['ConfirmedCases_Pred1'] * 0.001\n    elif tt.query('Place == @myplace')['Fatalities'].max() < 5:\n        tt.loc[(tt['Place'] == myplace), 'Fatalities_Pred1'] = tt.loc[(tt['Place'] == myplace)]['ConfirmedCases_Pred1'] * 0.001\n    else:\n        X = dat['Days_Since_Ten_Cases']\n        y = dat['Fatalities']\n        y = y.cummax()\n        dat_all = tt.query('Place == @myplace and Days_Since_Ten_Cases >= 0')[['Days_Since_Ten_Cases','Fatalities']]\n        X_pred = dat_all['Days_Since_Ten_Cases']\n        en = ElasticNet()\n        en.fit(X.values.reshape(-1, 1), y.values)\n        preds = en.predict(X_pred.values.reshape(-1, 1))\n        tt.loc[(tt['Place'] == myplace) & (tt['Days_Since_Ten_Cases'] >= 0), 'Fatalities_Pred1'] = preds\n\n        # Cap at 0.0001 Population\n        pop_myplace = tt.query('Place == @myplace')['Population_final'].values[0]\n        tt.loc[(tt['Place'] == myplace) & (tt['Fatalities_Pred1'] > (0.0005 * pop_myplace)), 'Fatalities_Pred1'] = (0.0005 * pop_myplace)\n\n    ax = tt.query('Place == @myplace').set_index('Date')[['Fatalities','Fatalities_Pred1']].plot(figsize=(15, 5), title=myplace, ax=axs[1])\n    plt.show()\n# Clean Up any time the actual is less than the real\ntt['ConfirmedCases_Pred1'] = tt[['ConfirmedCases','ConfirmedCases_Pred1']].max(axis=1)\ntt['Fatalities_Pred1'] = tt[['Fatalities','Fatalities_Pred1']].max(axis=1)\n\ntt['ConfirmedCases_Pred1'] = tt['ConfirmedCases_Pred1'].fillna(0)\ntt['Fatalities_Pred1'] = tt['Fatalities_Pred1'].fillna(0)\n\n# Fill pred with\ntt.loc[~tt['ConfirmedCases'].isna(), 'ConfirmedCases_Pred1'] = tt.loc[~tt['ConfirmedCases'].isna()]['ConfirmedCases']\ntt.loc[~tt['Fatalities'].isna(), 'Fatalities_Pred1'] = tt.loc[~tt['Fatalities'].isna()]['Fatalities']\n\ntt['ConfirmedCases_Pred1'] = tt.groupby('Place')['ConfirmedCases_Pred1'].transform('cummax')\ntt['Fatalities_Pred1'] = tt.groupby('Place')['Fatalities_Pred1'].transform('cummax')\n# Questionable numbers\ntt.query('Place == \"Iran\"').set_index('Date')[['ConfirmedCases',\n                                               'ConfirmedCases_Pred1',]].plot(figsize=(15, 5))\n# Make Iran's Predictions Linear\n\ntt.query('Place == \"Iran\"').set_index('Date')[['Fatalities','Fatalities_Pred1']].plot(figsize=(15, 5))", "processed": ["make iran predict linear"]}, {"markdown": ["# 1. Exploratory data analysis (EDA) <a id=\"section1\"></a>"], "code": "# Reference: https://www.kaggle.com/code/vbmokin/covid19-week-2-sir-model-ridge-regr\n\nsubmission_example = pd.read_csv(\"../input/covid19-global-forecasting-week-2/submission.csv\")\ntest = pd.read_csv(\"../input/covid19-global-forecasting-week-2/test.csv\")\ntrain = pd.read_csv(\"../input/covid19-global-forecasting-week-2/train.csv\")\ndisplay(train.head(5))\ndisplay(train.describe())\nprint(\"Number of Country_Region: \", train['Country_Region'].nunique())\nprint(\"Dates go from day\", max(train['Date']), \"to day\", min(train['Date']), \", a total of\", train['Date'].nunique(), \"days\")\nprint(\"Countries with Province/State informed: \", train[train['Province_State'].isna()==False]['Country_Region'].unique())\n#confirmed_country = train.groupby(['Country/Region', 'Province/State']).agg({'ConfirmedCases':['sum']})\n#fatalities_country = train.groupby(['Country/Region', 'Province/State']).agg({'Fatalities':['sum']})\nconfirmed_total_date = train.groupby(['Date']).agg({'ConfirmedCases':['sum']})\nfatalities_total_date = train.groupby(['Date']).agg({'Fatalities':['sum']})\ntotal_date = confirmed_total_date.join(fatalities_total_date)\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(17,7))\ntotal_date.plot(ax=ax1)\nax1.set_title(\"Global confirmed cases\", size=13)\nax1.set_ylabel(\"Number of cases\", size=13)\nax1.set_xlabel(\"Date\", size=13)\nfatalities_total_date.plot(ax=ax2, color='orange')\nax2.set_title(\"Global deceased cases\", size=13)\nax2.set_ylabel(\"Number of cases\", size=13)\nax2.set_xlabel(\"Date\", size=13)", "processed": ["1 exploratori data analysi eda id section1"]}, {"markdown": ["## 1.1. COVID-19 global tendency excluding China <a id=\"section11\"></a>"], "code": "# Reference: https://www.kaggle.com/code/vbmokin/covid19-week-2-sir-model-ridge-regr\n\n#confirmed_country_noChina = train[train['Country_Region']!='China'].groupby(['Country_Region', 'Province_State']).agg({'ConfirmedCases':['sum']})\n#fatalities_country_noChina = train[train['Country_Region']!='China'].groupby(['Country_Region', 'Province_State']).agg({'Fatalities':['sum']})\nconfirmed_total_date_noChina = train[train['Country_Region']!='China'].groupby(['Date']).agg({'ConfirmedCases':['sum']})\nfatalities_total_date_noChina = train[train['Country_Region']!='China'].groupby(['Date']).agg({'Fatalities':['sum']})\ntotal_date_noChina = confirmed_total_date_noChina.join(fatalities_total_date_noChina)\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,5))\ntotal_date_noChina.plot(ax=ax1)\nax1.set_title(\"Global confirmed cases excluding China\", size=13)\nax1.set_ylabel(\"Number of cases\", size=13)\nax1.set_xlabel(\"Date\", size=13)\nfatalities_total_date_noChina.plot(ax=ax2, color='orange')\nax2.set_title(\"Global deceased cases excluding China\", size=13)\nax2.set_ylabel(\"Number of cases\", size=13)\nax2.set_xlabel(\"Date\", size=13)", "processed": ["1 1 covid 19 global tendenc exclud china id section11"]}, {"markdown": ["## 1.2. COVID-19 tendency in China <a id=\"section12\"></a>"], "code": "# Reference: https://www.kaggle.com/code/vbmokin/covid19-week-2-sir-model-ridge-regr\n\n#confirmed_country_China = train[train['Country_Region']=='China'].groupby(['Country_Region', 'Province_State']).agg({'ConfirmedCases':['sum']})\n#fatalities_country_China = train[train['Country_Region']=='China'].groupby(['Country_Region', 'Province_State']).agg({'Fatalities':['sum']})\nconfirmed_total_date_China = train[train['Country_Region']=='China'].groupby(['Date']).agg({'ConfirmedCases':['sum']})\nfatalities_total_date_China = train[train['Country_Region']=='China'].groupby(['Date']).agg({'Fatalities':['sum']})\ntotal_date_China = confirmed_total_date_China.join(fatalities_total_date_China)\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,5))\ntotal_date_China.plot(ax=ax1)\nax1.set_title(\"China confirmed cases\", size=13)\nax1.set_ylabel(\"Number of cases\", size=13)\nax1.set_xlabel(\"Date\", size=13)\nfatalities_total_date_China.plot(ax=ax2, color='orange')\nax2.set_title(\"China deceased cases\", size=13)\nax2.set_ylabel(\"Number of cases\", size=13)\nax2.set_xlabel(\"Date\", size=13)", "processed": ["1 2 covid 19 tendenc china id section12"]}, {"markdown": ["## 1.3. Italy, Spain, UK and Singapore <a id=\"section13\"></a>"], "code": "# Reference: https://www.kaggle.com/code/vbmokin/covid19-week-2-sir-model-ridge-regr\n\n#confirmed_country_Italy = train[train['Country_Region']=='Italy'].groupby(['Country_Region', 'Province_State']).agg({'ConfirmedCases':['sum']})\n#fatalities_country_Italy = train[train['Country_Region']=='Italy'].groupby(['Country_Region', 'Province_State']).agg({'Fatalities':['sum']})\nconfirmed_total_date_Italy = train[train['Country_Region']=='Italy'].groupby(['Date']).agg({'ConfirmedCases':['sum']})\nfatalities_total_date_Italy = train[train['Country_Region']=='Italy'].groupby(['Date']).agg({'Fatalities':['sum']})\ntotal_date_Italy = confirmed_total_date_Italy.join(fatalities_total_date_Italy)\n\n#confirmed_country_Spain = train[train['Country_Region']=='Spain'].groupby(['Country_Region', 'Province_State']).agg({'ConfirmedCases':['sum']})\n#fatalities_country_Spain = train[train['Country_Region']=='Spain'].groupby(['Country_Region', 'Province_State']).agg({'Fatalities':['sum']})\nconfirmed_total_date_Spain = train[train['Country_Region']=='Spain'].groupby(['Date']).agg({'ConfirmedCases':['sum']})\nfatalities_total_date_Spain = train[train['Country_Region']=='Spain'].groupby(['Date']).agg({'Fatalities':['sum']})\ntotal_date_Spain = confirmed_total_date_Spain.join(fatalities_total_date_Spain)\n\n#confirmed_country_UK = train[train['Country_Region']=='United Kingdom'].groupby(['Country_Region', 'Province_State']).agg({'ConfirmedCases':['sum']})\n#fatalities_country_UK = train[train['Country_Region']=='United Kingdom'].groupby(['Country_Region', 'Province_State']).agg({'Fatalities':['sum']})\nconfirmed_total_date_UK = train[train['Country_Region']=='United Kingdom'].groupby(['Date']).agg({'ConfirmedCases':['sum']})\nfatalities_total_date_UK = train[train['Country_Region']=='United Kingdom'].groupby(['Date']).agg({'Fatalities':['sum']})\ntotal_date_UK = confirmed_total_date_UK.join(fatalities_total_date_UK)\n\n#confirmed_country_Australia = train[train['Country_Region']=='Australia'].groupby(['Country_Region', 'Province_State']).agg({'ConfirmedCases':['sum']})\n#fatalities_country_Australia = train[train['Country_Region']=='Australia'].groupby(['Country_Region', 'Province_State']).agg({'Fatalities':['sum']})\nconfirmed_total_date_Australia = train[train['Country_Region']=='Australia'].groupby(['Date']).agg({'ConfirmedCases':['sum']})\nfatalities_total_date_Australia = train[train['Country_Region']=='Australia'].groupby(['Date']).agg({'Fatalities':['sum']})\ntotal_date_Australia = confirmed_total_date_Australia.join(fatalities_total_date_Australia)\n\n#confirmed_country_Singapore = train[train['Country_Region']=='Singapore'].groupby(['Country_Region', 'Province_State']).agg({'ConfirmedCases':['sum']})\n#fatalities_country_Singapore = train[train['Country_Region']=='Singapore'].groupby(['Country_Region', 'Province_State']).agg({'Fatalities':['sum']})\nconfirmed_total_date_Singapore = train[train['Country_Region']=='Singapore'].groupby(['Date']).agg({'ConfirmedCases':['sum']})\nfatalities_total_date_Singapore = train[train['Country_Region']=='Singapore'].groupby(['Date']).agg({'Fatalities':['sum']})\ntotal_date_Singapore = confirmed_total_date_Singapore.join(fatalities_total_date_Singapore)\n\nplt.figure(figsize=(15,10))\nplt.subplot(2, 2, 1)\ntotal_date_Italy.plot(ax=plt.gca(), title='Italy')\nplt.ylabel(\"Confirmed infection cases\", size=13)\n\nplt.subplot(2, 2, 2)\ntotal_date_Spain.plot(ax=plt.gca(), title='Spain')\n\nplt.subplot(2, 2, 3)\ntotal_date_UK.plot(ax=plt.gca(), title='United Kingdom')\nplt.ylabel(\"Confirmed infection cases\", size=13)\n\nplt.subplot(2, 2, 4)\ntotal_date_Singapore.plot(ax=plt.gca(), title='Singapore')\npop_italy = 60486683.\npop_spain = 46749696.\npop_UK = 67784927.\npop_singapore = 5837230.\n\ntotal_date_Italy.ConfirmedCases = total_date_Italy.ConfirmedCases/pop_italy*100.\ntotal_date_Italy.Fatalities = total_date_Italy.ConfirmedCases/pop_italy*100.\ntotal_date_Spain.ConfirmedCases = total_date_Spain.ConfirmedCases/pop_spain*100.\ntotal_date_Spain.Fatalities = total_date_Spain.ConfirmedCases/pop_spain*100.\ntotal_date_UK.ConfirmedCases = total_date_UK.ConfirmedCases/pop_UK*100.\ntotal_date_UK.Fatalities = total_date_UK.ConfirmedCases/pop_UK*100.\ntotal_date_Singapore.ConfirmedCases = total_date_Singapore.ConfirmedCases/pop_singapore*100.\ntotal_date_Singapore.Fatalities = total_date_Singapore.ConfirmedCases/pop_singapore*100.\n\nplt.figure(figsize=(15,10))\nplt.subplot(2, 2, 1)\ntotal_date_Italy.ConfirmedCases.plot(ax=plt.gca(), title='Italy')\nplt.ylabel(\"Fraction of population infected\")\nplt.ylim(0, 0.06)\n\nplt.subplot(2, 2, 2)\ntotal_date_Spain.ConfirmedCases.plot(ax=plt.gca(), title='Spain')\nplt.ylim(0, 0.06)\n\nplt.subplot(2, 2, 3)\ntotal_date_UK.ConfirmedCases.plot(ax=plt.gca(), title='United Kingdom')\nplt.ylabel(\"Fraction of population infected\")\nplt.ylim(0, 0.005)\n\nplt.subplot(2, 2, 4)\ntotal_date_Singapore.ConfirmedCases.plot(ax=plt.gca(), title='Singapore')\nplt.ylim(0, 0.005)\n#confirmed_country_Italy = train[(train['Country_Region']=='Italy') & train['ConfirmedCases']!=0].groupby(['Country_Region', 'Province_State']).agg({'ConfirmedCases':['sum']})\n#fatalities_country_Italy = train[(train['Country_Region']=='Italy') & train['ConfirmedCases']!=0].groupby(['Country_Region', 'Province_State']).agg({'Fatalities':['sum']})\nconfirmed_total_date_Italy = train[(train['Country_Region']=='Italy') & train['ConfirmedCases']!=0].groupby(['Date']).agg({'ConfirmedCases':['sum']})\nfatalities_total_date_Italy = train[(train['Country_Region']=='Italy') & train['ConfirmedCases']!=0].groupby(['Date']).agg({'Fatalities':['sum']})\ntotal_date_Italy = confirmed_total_date_Italy.join(fatalities_total_date_Italy)\n\n#confirmed_country_Spain = train[(train['Country_Region']=='Spain') & (train['ConfirmedCases']!=0)].groupby(['Country_Region', 'Province_State']).agg({'ConfirmedCases':['sum']})\n#fatalities_country_Spain = train[(train['Country_Region']=='Spain') & (train['ConfirmedCases']!=0)].groupby(['Country_Region', 'Province_State']).agg({'Fatalities':['sum']})\nconfirmed_total_date_Spain = train[(train['Country_Region']=='Spain') & (train['ConfirmedCases']!=0)].groupby(['Date']).agg({'ConfirmedCases':['sum']})\nfatalities_total_date_Spain = train[(train['Country_Region']=='Spain') & (train['ConfirmedCases']!=0)].groupby(['Date']).agg({'Fatalities':['sum']})\ntotal_date_Spain = confirmed_total_date_Spain.join(fatalities_total_date_Spain)\n\n#confirmed_country_UK = train[(train['Country_Region']=='United Kingdom') & (train['ConfirmedCases']!=0)].groupby(['Country_Region', 'Province_State']).agg({'ConfirmedCases':['sum']})\n#fatalities_country_UK = train[(train['Country_Region']=='United Kingdom') & (train['ConfirmedCases']!=0)].groupby(['Country_Region', 'Province_State']).agg({'Fatalities':['sum']})\nconfirmed_total_date_UK = train[(train['Country_Region']=='United Kingdom') & (train['ConfirmedCases']!=0)].groupby(['Date']).agg({'ConfirmedCases':['sum']})\nfatalities_total_date_UK = train[(train['Country_Region']=='United Kingdom') & (train['ConfirmedCases']!=0)].groupby(['Date']).agg({'Fatalities':['sum']})\ntotal_date_UK = confirmed_total_date_UK.join(fatalities_total_date_UK)\n\n#confirmed_country_Australia = train[(train['Country_Region']=='Australia') & (train['ConfirmedCases']!=0)].groupby(['Country_Region', 'Province_State']).agg({'ConfirmedCases':['sum']})\n#fatalities_country_Australia = train[(train['Country_Region']=='Australia') & (train['ConfirmedCases']!=0)].groupby(['Country_Region', 'Province_State']).agg({'Fatalities':['sum']})\nconfirmed_total_date_Australia = train[(train['Country_Region']=='Australia') & (train['ConfirmedCases']!=0)].groupby(['Date']).agg({'ConfirmedCases':['sum']})\nfatalities_total_date_Australia = train[(train['Country_Region']=='Australia') & (train['ConfirmedCases']!=0)].groupby(['Date']).agg({'Fatalities':['sum']})\ntotal_date_Australia = confirmed_total_date_Australia.join(fatalities_total_date_Australia)\n\n#confirmed_country_Singapore = train[(train['Country_Region']=='Singapore') & (train['ConfirmedCases']!=0)].groupby(['Country_Region', 'Province_State']).agg({'ConfirmedCases':['sum']})\n#fatalities_country_Singapore = train[(train['Country_Region']=='Singapore') & (train['ConfirmedCases']!=0)].groupby(['Country_Region', 'Province_State']).agg({'Fatalities':['sum']})\nconfirmed_total_date_Singapore = train[(train['Country_Region']=='Singapore') & (train['ConfirmedCases']!=0)].groupby(['Date']).agg({'ConfirmedCases':['sum']})\nfatalities_total_date_Singapore = train[(train['Country_Region']=='Singapore') & (train['ConfirmedCases']!=0)].groupby(['Date']).agg({'Fatalities':['sum']})\ntotal_date_Singapore = confirmed_total_date_Singapore.join(fatalities_total_date_Singapore)\n\nitaly = [i for i in total_date_Italy.ConfirmedCases['sum'].values]\nitaly_30 = italy[0:50] \nspain = [i for i in total_date_Spain.ConfirmedCases['sum'].values]\nspain_30 = spain[0:50] \nUK = [i for i in total_date_UK.ConfirmedCases['sum'].values]\nUK_30 = UK[0:50] \nsingapore = [i for i in total_date_Singapore.ConfirmedCases['sum'].values]\nsingapore_30 = singapore[0:50] \n\n\n# Plots\nplt.figure(figsize=(12,6))\nplt.plot(italy_30)\nplt.plot(spain_30)\nplt.plot(UK_30)\nplt.plot(singapore_30)\nplt.legend([\"Italy\", \"Spain\", \"UK\", \"Singapore\"], loc='upper left')\nplt.title(\"COVID-19 infections from the first confirmed case\", size=15)\nplt.xlabel(\"Days\", size=13)\nplt.ylabel(\"Infected cases\", size=13)\nplt.ylim(0, 60000)\nplt.show()", "processed": ["1 3 itali spain uk singapor id section13"]}, {"markdown": ["# 2. SIR model <a id=\"section2\"></a>", "## 2.1. Implementing the SIR model <a id=\"section21\"></a>\n"], "code": "# Reference: https://www.kaggle.com/code/vbmokin/covid19-week-2-sir-model-ridge-regr\n\n# Susceptible equation\ndef fa(N, a, b, beta):\n    fa = -beta*a*b\n    return fa\n\n# Infected equation\ndef fb(N, a, b, beta, gamma):\n    fb = beta*a*b - gamma*b\n    return fb\n\n# Recovered/deceased equation\ndef fc(N, b, gamma):\n    fc = gamma*b\n    return fc\n# Runge-Kutta method of 4rth order for 3 dimensions (susceptible a, infected b and recovered r)\ndef rK4(N, a, b, c, fa, fb, fc, beta, gamma, hs):\n    a1 = fa(N, a, b, beta)*hs\n    b1 = fb(N, a, b, beta, gamma)*hs\n    c1 = fc(N, b, gamma)*hs\n    ak = a + a1*0.5\n    bk = b + b1*0.5\n    ck = c + c1*0.5\n    a2 = fa(N, ak, bk, beta)*hs\n    b2 = fb(N, ak, bk, beta, gamma)*hs\n    c2 = fc(N, bk, gamma)*hs\n    ak = a + a2*0.5\n    bk = b + b2*0.5\n    ck = c + c2*0.5\n    a3 = fa(N, ak, bk, beta)*hs\n    b3 = fb(N, ak, bk, beta, gamma)*hs\n    c3 = fc(N, bk, gamma)*hs\n    ak = a + a3\n    bk = b + b3\n    ck = c + c3\n    a4 = fa(N, ak, bk, beta)*hs\n    b4 = fb(N, ak, bk, beta, gamma)*hs\n    c4 = fc(N, bk, gamma)*hs\n    a = a + (a1 + 2*(a2 + a3) + a4)/6\n    b = b + (b1 + 2*(b2 + b3) + b4)/6\n    c = c + (c1 + 2*(c2 + c3) + c4)/6\n    return a, b, c\ndef SIR(N, b0, beta, gamma, hs):\n    \n    \"\"\"\n    N = total number of population\n    beta = transition rate S->I\n    gamma = transition rate I->R\n    k =  denotes the constant degree distribution of the network (average value for networks in which \n    the probability of finding a node with a different connectivity decays exponentially fast\n    hs = jump step of the numerical integration\n    \"\"\"\n    \n    # Initial condition\n    a = float(N-1)/N -b0\n    b = float(1)/N +b0\n    c = 0.\n\n    sus, inf, rec= [],[],[]\n    for i in range(10000): # Run for a certain number of time-steps\n        sus.append(a)\n        inf.append(b)\n        rec.append(c)\n        a,b,c = rK4(N, a, b, c, fa, fb, fc, beta, gamma, hs)\n\n    return sus, inf, rec\n# Parameters of the model\nN = 7800*(10**6)\nb0 = 0\nbeta = 0.7\ngamma = 0.2\nhs = 0.1\n\nsus, inf, rec = SIR(N, b0, beta, gamma, hs)\n\nf = plt.figure(figsize=(8,5)) \nplt.plot(sus, 'b.', label='susceptible');\nplt.plot(inf, 'r.', label='infected');\nplt.plot(rec, 'c.', label='recovered/deceased');\nplt.title(\"SIR model\")\nplt.xlabel(\"time\", fontsize=10);\nplt.ylabel(\"Fraction of population\", fontsize=10);\nplt.legend(loc='best')\nplt.xlim(0,1000)\nplt.savefig('SIR_example.png')\nplt.show()", "processed": ["2 sir model id section2", "2 1 implement sir model id section21"]}, {"markdown": ["## 2.2. Fit SIR parameters to real data <a id=\"section22\"></a>\n"], "code": "# Reference: https://www.kaggle.com/code/vbmokin/covid19-week-2-sir-model-ridge-regr\n\npopulation = float(46750238)\ncountry_df = pd.DataFrame()\ncountry_df['ConfirmedCases'] = train.loc[train['Country_Region']=='Spain'].ConfirmedCases.diff().fillna(0)\ncountry_df = country_df[10:]\ncountry_df['day_count'] = list(range(1,len(country_df)+1))\n\nydata = [i for i in country_df.ConfirmedCases]\nxdata = country_df.day_count\nydata = np.array(ydata, dtype=float)\nxdata = np.array(xdata, dtype=float)\n\nN = population\ninf0 = ydata[0]\nsus0 = N - inf0\nrec0 = 0.0\n\ndef sir_model(y, x, beta, gamma):\n    sus = -beta * y[0] * y[1] / N\n    rec = gamma * y[1]\n    inf = -(sus + rec)\n    return sus, inf, rec\n\ndef fit_odeint(x, beta, gamma):\n    return integrate.odeint(sir_model, (sus0, inf0, rec0), x, args=(beta, gamma))[:,1]\n\npopt, pcov = optimize.curve_fit(fit_odeint, xdata, ydata)\nfitted = fit_odeint(xdata, *popt)\n\nplt.plot(xdata, ydata, 'o')\nplt.plot(xdata, fitted)\nplt.title(\"Fit of SIR model for Spain infected cases\")\nplt.ylabel(\"Population infected\")\nplt.xlabel(\"Days\")\nplt.show()\nprint(\"Optimal parameters: beta =\", popt[0], \" and gamma = \", popt[1])", "processed": ["2 2 fit sir paramet real data id section22"]}, {"markdown": ["# 4. Predictions<a id=\"section4\"></a>\n\nOur obective in this section consists on  predicting the evolution of the expansion from a data-centric perspective, like any other regression problem. To do so, remember that the challenge specifies that submissions on the public LB shouldn only contain data previous to 2020-03-12.\n\nModels to apply:\n1. Ridge Regression for one country\n2. Ridge Regression for all countries (method 1)\n3. Ridge Regression for all countries (method 2)", "## 4.1. Ridge Regression for one country <a id=\"section41\"></a>"], "code": "# Reference: https://www.kaggle.com/code/vbmokin/covid19-week-2-sir-model-ridge-regr\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,6))\n\n# Day_num = 38 is March 1st\ny1 = all_data[(all_data['Country_Region']==country_dict['Spain']) & (all_data['Day_num']>39) & (all_data['Day_num']<=49)][['ConfirmedCases']]\nx1 = range(0, len(y1))\nax1.plot(x1, y1, 'bo--')\nax1.set_title(\"Spain ConfirmedCases between days 39 and 49\")\nax1.set_xlabel(\"Days\")\nax1.set_ylabel(\"ConfirmedCases\")\n\ny2 = all_data[(all_data['Country_Region']==country_dict['Spain']) & (all_data['Day_num']>39) & (all_data['Day_num']<=49)][['ConfirmedCases']].apply(lambda x: np.log(x))\nx2 = range(0, len(y2))\nax2.plot(x2, y2, 'bo--')\nax2.set_title(\"Spain Log ConfirmedCases between days 39 and 49\")\nax2.set_xlabel(\"Days\")\nax2.set_ylabel(\"Log ConfirmedCases\")\n# Filter selected features\ndata = all_data.copy()\nfeatures = ['Id', 'ForecastId', 'Country_Region', 'Province_State', 'ConfirmedCases', 'Fatalities', \n       'Day_num']\ndata = data[features]\n\n# Apply log transformation to all ConfirmedCases and Fatalities columns, except for trends\ndata[['ConfirmedCases', 'Fatalities']] = data[['ConfirmedCases', 'Fatalities']].astype('float64')\ndata[['ConfirmedCases', 'Fatalities']] = data[['ConfirmedCases', 'Fatalities']].apply(lambda x: np.log1p(x))\n\n# Replace infinites\ndata.replace([np.inf, -np.inf], 0, inplace=True)\n\n\n# Split data into train/test\ndef split_data(data):\n    \n    # Train set\n    x_train = data[data.ForecastId == -1].drop(['ConfirmedCases', 'Fatalities'], axis=1)\n    y_train_1 = data[data.ForecastId == -1]['ConfirmedCases']\n    y_train_2 = data[data.ForecastId == -1]['Fatalities']\n\n    # Test set\n    x_test = data[data.ForecastId != -1].drop(['ConfirmedCases', 'Fatalities'], axis=1)\n\n    # Clean Id columns and keep ForecastId as index\n    x_train.drop('Id', inplace=True, errors='ignore', axis=1)\n    x_train.drop('ForecastId', inplace=True, errors='ignore', axis=1)\n    x_test.drop('Id', inplace=True, errors='ignore', axis=1)\n    x_test.drop('ForecastId', inplace=True, errors='ignore', axis=1)\n    \n    return x_train, y_train_1, y_train_2, x_test\n\n\n# Ridge replace of the Linear regression model\ndef ridge_reg(X_train, Y_train, X_test):\n    # Create Ridge regression object\n    #regr = Ridge()        # commit 2\n    #regr = RidgeCV(cv=5)  # commit 4\n    #regr = Ridge(alpha=10) # commit 5\n    regr = Ridge(alpha=10) # now\n\n    # Train the model using the training sets\n    regr.fit(X_train, Y_train)\n\n    # Make predictions using the testing set\n    y_pred = regr.predict(X_test)\n    \n    return regr, y_pred\n\n\n# Submission function\ndef get_submission(s, df, target1, target2):\n    \n    prediction_1 = df[target1]\n    prediction_2 = df[target2]\n\n    # Submit predictions\n    prediction_1 = [int(item) for item in list(map(round, prediction_1))]\n    prediction_2 = [int(item) for item in list(map(round, prediction_2))]\n    \n    submission = pd.DataFrame({\n        \"ForecastId\": df['ForecastId'].astype('int32'), \n        \"ConfirmedCases\": prediction_1, \n        \"Fatalities\": prediction_2\n    })\n    submission.to_csv(s + '.csv', index=False)", "processed": ["4 predict id section4 obect section consist predict evolut expans data centric perspect like regress problem rememb challeng specifi submiss public lb contain data previou 2020 03 12 model appli 1 ridg regress one countri 2 ridg regress countri method 1 3 ridg regress countri method 2", "4 1 ridg regress one countri id section41"]}, {"markdown": ["Let's try to see results when training with a single country:\n\n* **Spain**"], "code": "# Reference: https://www.kaggle.com/code/vbmokin/covid19-week-2-sir-model-ridge-regr\n\n# Select train (real) data from March 1 to March 22nd\ndates_list = ['2020-03-01', '2020-03-02', '2020-03-03', '2020-03-04', '2020-03-05', '2020-03-06', '2020-03-07', '2020-03-08', '2020-03-09', \n                 '2020-03-10', '2020-03-11','2020-03-12','2020-03-13','2020-03-14','2020-03-15','2020-03-16','2020-03-17','2020-03-18',\n                 '2020-03-19','2020-03-20','2020-03-21','2020-03-22','2020-03-23', '2020-03-24', '2020-03-25', '2020-03-26', '2020-03-27', \n                 '2020-03-28', '2020-03-29', '2020-03-30', '2020-03-31']\nall_data.loc[all_data['Country_Region']==country_dict['Spain']][40:65]\ndef plot_rreg_basic_country(data, country_name, dates_list, day_start, shift):\n    \n    data_country = data[data['Country_Region']==country_dict[country_name]]\n    data_country = data_country.loc[data_country['Day_num']>=day_start]\n    X_train, Y_train_1, Y_train_2, X_test = split_data(data_country)\n    model, pred = ridge_reg(X_train, Y_train_1, X_test)\n\n    # Create a df with both real cases and predictions (predictions starting on March 12th)\n    X_train_check = X_train.copy()\n    X_train_check['Target'] = Y_train_1\n\n    X_test_check = X_test.copy()\n    X_test_check['Target'] = pred\n\n    X_final_check = pd.concat([X_train_check, X_test_check])\n\n    # Select predictions from March 1st to March 25th\n    predicted_data = X_final_check.loc[(X_final_check['Day_num'].isin(list(range(day_start, day_start+len(dates_list)))))].Target\n    real_data = train.loc[(train['Country_Region']==country_name) & (train['Date'].isin(dates_list))]['ConfirmedCases']\n    dates_list_num = list(range(0,len(dates_list)))\n\n    # Plot results\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,6))\n\n    ax1.plot(dates_list_num, np.expm1(predicted_data))\n    ax1.plot(dates_list_num, real_data)\n    ax1.axvline(17-shift, linewidth=2, ls = ':', color='grey', alpha=0.5)\n    ax1.legend(['Predicted cases', 'Actual cases', 'Train-test split'], loc='upper left')\n    ax1.set_xlabel(\"Day count (from March \" + str(1+shift) + \" to March 25th)\")\n    ax1.set_ylabel(\"Confirmed Cases\")\n\n    ax2.plot(dates_list_num, predicted_data)\n    ax2.plot(dates_list_num, np.log1p(real_data))\n    ax2.axvline(17-shift, linewidth=2, ls = ':', color='grey', alpha=0.5)\n    ax2.legend(['Predicted cases', 'Actual cases', 'Train-test split'], loc='upper left')\n    ax2.set_xlabel(\"Day count (from March \" + str(1+shift) + \" to March 30th)\")\n    ax2.set_ylabel(\"Log Confirmed Cases\")\n\n    plt.suptitle((\"ConfirmedCases predictions based on Log-Lineal Regression for \"+country_name))\n    \n    \n    \n# Filter Spain, run the Linear Regression workflow\ncountry_name = \"Spain\"\nmarch_day = 0\nday_start = 39+march_day\ndates_list2 = dates_list[march_day:]\nplot_rreg_basic_country(data, country_name, dates_list2, day_start, march_day)\n# Filter Spain, run the Linear Regression workflow\ncountry_name = \"Spain\"\nmarch_day = 15\nday_start = 39+march_day\ndates_list2 = dates_list[march_day:]\nplot_rreg_basic_country(data, country_name, dates_list2, day_start, march_day)", "processed": ["let tri see result train singl countri spain"]}, {"markdown": ["## 4.4. Ridge regression with lags <a id=\"section44\"></a>"], "code": "# Reference: https://www.kaggle.com/code/vbmokin/covid19-week-2-sir-model-ridge-regr\n\n# # New split function, for one forecast day\n# def split_data_one_day(data, d):\n    \n#     #Train\n#     x_train = data[data.Day_num<d]\n#     y_train_1 = x_train.ConfirmedCases\n#     y_train_2 = x_train.Fatalities\n#     x_train.drop(['ConfirmedCases', 'Fatalities'], axis=1, inplace=True)\n    \n#     #Test\n#     x_test = data[data.Day_num==d]\n#     x_test.drop(['ConfirmedCases', 'Fatalities'], axis=1, inplace=True)\n    \n#     # Clean Id columns and keep ForecastId as index\n#     x_train.drop('Id', inplace=True, errors='ignore', axis=1)\n#     x_train.drop('ForecastId', inplace=True, errors='ignore', axis=1)\n#     x_test.drop('Id', inplace=True, errors='ignore', axis=1)\n#     x_test.drop('ForecastId', inplace=True, errors='ignore', axis=1)\n    \n#     return x_train, y_train_1, y_train_2, x_test\n\n\n# def plot_real_vs_prediction_country(data, train, country_name, day_start, dates_list, march_day):\n\n#     # Select predictions from March 1st to March 25th\n#     predicted_data = data.loc[(data['Day_num'].isin(list(range(day_start, day_start+len(dates_list)))))].ConfirmedCases\n#     real_data = train.loc[(train['Country_Region']==country_name) & (train['Date'].isin(dates_list))]['ConfirmedCases']\n#     dates_list_num = list(range(0,len(dates_list)))\n\n#     # Plot results\n#     fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,6))\n\n#     ax1.plot(dates_list_num, np.expm1(predicted_data))\n#     ax1.plot(dates_list_num, real_data)\n#     ax1.axvline(17-march_day, linewidth=2, ls = ':', color='grey', alpha=0.5)\n#     ax1.legend(['Predicted cases', 'Actual cases', 'Train-test split'], loc='upper left')\n#     ax1.set_xlabel(\"Day count (starting on March \" + str(march_day) + \"))\")\n#     ax1.set_ylabel(\"Confirmed Cases\")\n\n#     ax2.plot(dates_list_num, predicted_data)\n#     ax2.plot(dates_list_num, np.log1p(real_data))\n#     ax2.axvline(17-march_day, linewidth=2, ls = ':', color='grey', alpha=0.5)\n#     ax2.legend(['Predicted cases', 'Actual cases', 'Train-test split'], loc='upper left')\n#     ax2.set_xlabel(\"Day count (starting on March \" + str(march_day) + \")\")\n#     ax2.set_ylabel(\"Log Confirmed Cases\")\n\n#     plt.suptitle((\"ConfirmedCases predictions based on Log-Lineal Regression for \"+country_name))\n    \n    \n# def plot_real_vs_prediction_country_fatalities(data, train, country_name, day_start, dates_list, march_day):\n\n#     # Select predictions from March 1st to March 25th\n#     predicted_data = data.loc[(data['Day_num'].isin(list(range(day_start, day_start+len(dates_list)))))].Fatalities\n#     real_data = train.loc[(train['Country_Region']==country_name) & (train['Date'].isin(dates_list))]['Fatalities']\n#     dates_list_num = list(range(0,len(dates_list)))\n\n#     # Plot results\n#     fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,6))\n\n#     ax1.plot(dates_list_num, np.expm1(predicted_data))\n#     ax1.plot(dates_list_num, real_data)\n#     ax1.axvline(17-march_day, linewidth=2, ls = ':', color='grey', alpha=0.5)\n#     ax1.legend(['Predicted cases', 'Actual cases', 'Train-test split'], loc='upper left')\n#     ax1.set_xlabel(\"Day count (starting on March \" + str(march_day) + \")\")\n#     ax1.set_ylabel(\"Fatalities Cases\")\n\n#     ax2.plot(dates_list_num, predicted_data)\n#     ax2.plot(dates_list_num, np.log1p(real_data))\n#     ax2.axvline(17-march_day, linewidth=2, ls = ':', color='grey', alpha=0.5)\n#     ax2.legend(['Predicted cases', 'Actual cases', 'Train-test split'], loc='upper left')\n#     ax2.set_xlabel(\"Day count (starting on March \" + str(march_day) + \")\")\n#     ax2.set_ylabel(\"Log Fatalities Cases\")\n\n#     plt.suptitle((\"Fatalities predictions based on Log-Lineal Regression for \"+country_name))", "processed": ["4 4 ridg regress lag id section44"]}, {"markdown": ["# Fun Latent Walk\nBelow are latent walks from dogs to cats. Big GAN recognizes 1000 different category classes. There is a complete list of classes [here][1]. For example, `Labrador retriever` is class 208 and `Tiger` is class 292. By changing the `start class` and `end class` for `class_vector` below you can control what you would like to see morph into what. \n\nThe smooth transistions of the latent walks below demonstrate that Big GAN 128 is not a memorizer GAN.\n\n[1]: https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a"], "code": "# Reference: https://www.kaggle.com/code/cdeotte/big-gan-128-lb-12\n\nCOLS = 10\nROWS = 10\ntruncation = 0.5\nclass_vector = np.zeros((ROWS*COLS,1000),dtype=np.float32)\nnoise_vector = np.zeros((ROWS*COLS,128),dtype=np.float32)\n\nfor j in range(ROWS):\n    \n    # CHANGE THE FOLLOWING TWO LINES TO CONTROL WHAT CLASS MORPHS TO WHAT CLASS\n    #################\n    # DOGS ARE CLASSES 151 THRU 281, CATS ARE 281 THRU 294\n    # CATEGORY LIST HERE: https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a\n    class_vector[j*COLS,np.random.randint(151,281)]=1 # START CLASS\n    class_vector[(j+1)*COLS-1,np.random.randint(281,294)]=1 # END CLASS   \n    #################\n    \n    step = class_vector[(j+1)*COLS-1,] - class_vector[j*COLS,]\n    for k in range(1,COLS-1):\n        class_vector[j*COLS+k,] = class_vector[j*COLS,] + (k/(COLS-1))*step\n    noise_vector[j*COLS,:] = truncated_noise_sample(truncation=truncation, batch_size=1)\n    noise_vector[(j+1)*COLS-1,:] = truncated_noise_sample(truncation=truncation, batch_size=1)\n    step = noise_vector[(j+1)*COLS-1,]-noise_vector[j*COLS,]\n    for k in range(1,COLS-1):\n        noise_vector[j*COLS+k,] = noise_vector[j*COLS,] + (k/(COLS-1))*step\n\n# All in tensors\nnoise_vector = torch.from_numpy(noise_vector)\nclass_vector = torch.from_numpy(class_vector)\n\n# If you have a GPU, put everything on cuda\nnoise_vector = noise_vector.to('cuda')\nclass_vector = class_vector.to('cuda')\nmodel.to('cuda')\n\n# Generate an image\nwith torch.no_grad():\n    output = model(noise_vector, class_vector, truncation)\n\n# If you have a GPU put back on CPU\noutput = output.to('cpu')\noutput = output.numpy().transpose(0, 2, 3, 1)\n# DISPLAY LATENT WALK\nplt.figure(figsize=(int(2*COLS),int(2*ROWS)))\nplt.subplots_adjust(hspace=0,wspace=0)\nfor k in range(ROWS*COLS):\n    plt.subplot(ROWS,COLS,k+1)\n    img = Image.fromarray( ((output[k,]+1.)/2.*255).astype('uint8') )\n    plt.axis('off')\n    plt.imshow(img)\nplt.show(img)", "processed": ["fun latent walk latent walk dog cat big gan recogn 1000 differ categori class complet list class 1 exampl labrador retriev class 208 tiger class 292 chang start class end class class vector control would like see morph smooth transist latent walk demonstr big gan 128 memor gan 1 http gist github com yrevar 942d3a0ac09ec9e5eb3a"]}, {"markdown": ["# Display Examples"], "code": "# Reference: https://www.kaggle.com/code/cdeotte/big-gan-128-lb-12\n\n# DISPLAY GENERATED DOGS\nplt.figure(figsize=(15,15))\nplt.subplots_adjust(hspace=0,wspace=0)\nfor k in range(100):\n    plt.subplot(10,10,k+1)\n    img = Image.fromarray( ((output[k,]+1.)/2.*255).astype('uint8') )\n    plt.axis('off')\n    plt.imshow(img)\nplt.show(img)", "processed": ["display exampl"]}, {"markdown": ["## Categorical fields distribution"], "code": "# Reference: https://www.kaggle.com/code/gpreda/ieee-cis-fraud-detection-eda-model\n\ndef plot_count(feature, title, df, size=1):\n    f, ax = plt.subplots(1,1, figsize=(4*size,4))\n    total = float(len(df))\n    g = sns.countplot(df[feature], order = df[feature].value_counts().index[:30], palette='Set3')\n    g.set_title(\"Number and percentage of {}\".format(title))\n    if(size > 2):\n        plt.xticks(rotation=90, size=8)\n    for p in ax.patches:\n        height = p.get_height()\n        ax.text(p.get_x()+p.get_width()/2.,\n                height + 3,\n                '{:1.2f}%'.format(100*height/total),\n                ha=\"center\") \n    plt.show()   \nplot_count('id_30', 'train: id_30', df=train_identity_df, size=4)\nplot_count('id_30', 'test: id_30', df=test_identity_df, size=4)\nplot_count('id_31', 'train: id_31', df=train_identity_df, size=4)\nplot_count('id_31', 'test: id_31', df=test_identity_df, size=4)\nplot_count('id_33', 'train: id_33', df=train_identity_df, size=4)\nplot_count('id_33', 'test: id_33', df=test_identity_df, size=4)\nplot_count('DeviceInfo', 'train: DeviceInfo', df=train_identity_df, size=4)\nplot_count('DeviceInfo', 'test: DeviceInfo', df=test_identity_df, size=4)\nplot_count('P_emaildomain', 'train: P_emaildomain', df=train_transaction_df, size=4)\nplot_count('P_emaildomain', 'test: P_emaildomain', df=test_transaction_df, size=4)", "processed": ["categor field distribut"]}, {"markdown": ["Cool!! As we can note we don't have missing values and it's nice to us. <br>\nI will start exploring the Target and trying to find some patterns that could explain it;", "# Target Feature\n- Let's see the distribution and if we can identify what is the nature of this feature"], "code": "# Reference: https://www.kaggle.com/code/kabure/eda-feat-engineering-encode-conquer\n\ntotal = len(df_train)\nplt.figure(figsize=(12,6))\n\ng = sns.countplot(x='target', data=df_train, color='green')\ng.set_title(\"TARGET DISTRIBUTION\", fontsize = 20)\ng.set_xlabel(\"Target Vaues\", fontsize = 15)\ng.set_ylabel(\"Count\", fontsize = 15)\nsizes=[] # Get highest values in y\nfor p in g.patches:\n    height = p.get_height()\n    sizes.append(height)\n    g.text(p.get_x()+p.get_width()/2.,\n            height + 3,\n            '{:1.2f}%'.format(height/total*100),\n            ha=\"center\", fontsize=14) \ng.set_ylim(0, max(sizes) * 1.15) # set y limit based on highest heights\n\nplt.show()", "processed": ["cool note miss valu nice u br start explor target tri find pattern could explain", "target featur let see distribut identifi natur featur"]}, {"markdown": ["Cool;  \nWe can see that our target is a binary feature and as it is 0 or 1 we can't know what is about.<br>\nAnother interesting thing to note is that isn't so imbalanced:<br>\n- Category 0 with 79.4% <br>\n- Category 1 with 30.6\n\nNow, as we have much of them, let's explore the patterns of other binary features", "# Binary Features"], "code": "# Reference: https://www.kaggle.com/code/kabure/eda-feat-engineering-encode-conquer\n\nbin_cols = ['bin_0', 'bin_1', 'bin_2', 'bin_3', 'bin_4']\n#Looking the V's features\nimport matplotlib.gridspec as gridspec # to do the grid of plots\ngrid = gridspec.GridSpec(3, 2) # The grid of chart\nplt.figure(figsize=(16,20)) # size of figure\n\n# loop to get column and the count of plots\nfor n, col in enumerate(df_train[bin_cols]): \n    ax = plt.subplot(grid[n]) # feeding the figure of grid\n    sns.countplot(x=col, data=df_train, hue='target', palette='hls') \n    ax.set_ylabel('Count', fontsize=15) # y axis label\n    ax.set_title(f'{col} Distribution by Target', fontsize=18) # title label\n    ax.set_xlabel(f'{col} values', fontsize=15) # x axis label\n    sizes=[] # Get highest values in y\n    for p in ax.patches: # loop to all objects\n        height = p.get_height()\n        sizes.append(height)\n        ax.text(p.get_x()+p.get_width()/2.,\n                height + 3,\n                '{:1.2f}%'.format(height/total*100),\n                ha=\"center\", fontsize=14) \n    ax.set_ylim(0, max(sizes) * 1.15) # set y limit based on highest heights\n    \nplt.show()", "processed": ["cool see target binari featur 0 1 know br anoth interest thing note imbalanc br categori 0 79 4 br categori 1 30 6 much let explor pattern binari featur", "binari featur"]}, {"markdown": ["Cool! Now, it's ok to model this feature into a machine learning algorithmn. <br>\nLet's work in the other features", "# Nominal Features (with more than 2 and less than 15 values)\n- Let's see the distribution of the feature and target Ratio for each value in nominal features"], "code": "# Reference: https://www.kaggle.com/code/kabure/eda-feat-engineering-encode-conquer\n\nnom_cols = ['nom_0', 'nom_1', 'nom_2', 'nom_3', 'nom_4']\ndef ploting_cat_fet(df, cols, vis_row=5, vis_col=2):\n    \n    grid = gridspec.GridSpec(vis_row,vis_col) # The grid of chart\n    plt.figure(figsize=(17, 35)) # size of figure\n\n    # loop to get column and the count of plots\n    for n, col in enumerate(df_train[cols]): \n        tmp = pd.crosstab(df_train[col], df_train['target'], normalize='index') * 100\n        tmp = tmp.reset_index()\n        tmp.rename(columns={0:'No',1:'Yes'}, inplace=True)\n\n        ax = plt.subplot(grid[n]) # feeding the figure of grid\n        sns.countplot(x=col, data=df_train, order=list(tmp[col].values) , color='green') \n        ax.set_ylabel('Count', fontsize=15) # y axis label\n        ax.set_title(f'{col} Distribution by Target', fontsize=18) # title label\n        ax.set_xlabel(f'{col} values', fontsize=15) # x axis label\n\n        # twinX - to build a second yaxis\n        gt = ax.twinx()\n        gt = sns.pointplot(x=col, y='Yes', data=tmp,\n                           order=list(tmp[col].values),\n                           color='black', legend=False)\n        gt.set_ylim(tmp['Yes'].min()-5,tmp['Yes'].max()*1.1)\n        gt.set_ylabel(\"Target %True(1)\", fontsize=16)\n        sizes=[] # Get highest values in y\n        for p in ax.patches: # loop to all objects\n            height = p.get_height()\n            sizes.append(height)\n            ax.text(p.get_x()+p.get_width()/2.,\n                    height + 3,\n                    '{:1.2f}%'.format(height/total*100),\n                    ha=\"center\", fontsize=14) \n        ax.set_ylim(0, max(sizes) * 1.15) # set y limit based on highest heights\n\n\n    plt.subplots_adjust(hspace = 0.5, wspace=.3)\n    plt.show()\nploting_cat_fet(df_train, nom_cols, vis_row=5, vis_col=2)", "processed": ["cool ok model featur machin learn algorithmn br let work featur", "nomin featur 2 le 15 valu let see distribut featur target ratio valu nomin featur"]}, {"markdown": ["Cool! In Ordinal features, the rule of nominal isn't real. <br>\nWe can see that only on ord_0 the highest ratio in target in the less common category;\n\nAs the \"ord_4\" and \"ord_5\" have highest cardinality I will plot it separated\n\n## Ord_4 and ord_5"], "code": "# Reference: https://www.kaggle.com/code/kabure/eda-feat-engineering-encode-conquer\n\ndf_train['ord_5_ot'] = 'Others'\ndf_train.loc[df_train['ord_5'].isin(df_train['ord_5'].value_counts()[:25].sort_index().index), 'ord_5_ot'] = df_train['ord_5']\ntmp = pd.crosstab(df_train['ord_4'], df_train['target'], normalize='index') * 100\ntmp = tmp.reset_index()\ntmp.rename(columns={0:'No',1:'Yes'}, inplace=True)\nplt.figure(figsize=(15,12))\n\nplt.subplot(211)\nax = sns.countplot(x='ord_4', data=df_train, order=list(tmp['ord_4'].values) , color='green') \nax.set_ylabel('Count', fontsize=17) # y axis label\nax.set_title('ord_4 Distribution with Target %ratio', fontsize=20) # title label\nax.set_xlabel('ord_4 values', fontsize=17) # x axis label\n# twinX - to build a second yaxis\ngt = ax.twinx()\ngt = sns.pointplot(x='ord_4', y='Yes', data=tmp,\n                   order=list(tmp['ord_4'].values),\n                   color='black', legend=False)\ngt.set_ylim(tmp['Yes'].min()-5,tmp['Yes'].max()*1.1)\ngt.set_ylabel(\"Target %True(1)\", fontsize=16)\n\ntmp = pd.crosstab(df_train['ord_5_ot'], df_train['target'], normalize='index') * 100\ntmp = tmp.reset_index()\ntmp.rename(columns={0:'No',1:'Yes'}, inplace=True)\n\nplt.subplot(212)\nax1 = sns.countplot(x='ord_5_ot', data=df_train,\n                   order=list(df_train['ord_5_ot'].value_counts().sort_index().index) ,\n                   color='green') \nax1.set_ylabel('Count', fontsize=17) # y axis label\nax1.set_title('TOP 25 ord_5 and \"others\" Distribution with Target %ratio', fontsize=20) # title label\nax1.set_xlabel('ord_5 values', fontsize=17) # x axis label\n# twinX - to build a second yaxis\ngt = ax1.twinx()\ngt = sns.pointplot(x='ord_5_ot', y='Yes', data=tmp,\n                   order=list(df_train['ord_5_ot'].value_counts().sort_index().index),\n                   color='black', legend=False)\ngt.set_ylim(tmp['Yes'].min()-5,tmp['Yes'].max()*1.1)\ngt.set_ylabel(\"Target %True(1)\", fontsize=16)\n\nplt.subplots_adjust(hspace = 0.4, wspace=.3)\n\nplt.show()", "processed": ["cool ordin featur rule nomin real br see ord 0 highest ratio target le common categori ord 4 ord 5 highest cardin plot separ ord 4 ord 5"]}, {"markdown": ["## Distribution of ord_5 features"], "code": "# Reference: https://www.kaggle.com/code/kabure/eda-feat-engineering-encode-conquer\n\nplt.figure(figsize=(12,6))\n\ng = sns.distplot(ord_5_count, bins= 50)\ng.set_title(\"Frequency of ord_5 category values\", fontsize=22)\ng.set_xlabel(\"Total of entries in ord_5 category's\", fontsize=18)\ng.set_ylabel(\"Density\", fontsize=18)\n\nplt.show()", "processed": ["distribut ord 5 featur"]}, {"markdown": ["# Creating pipeline to evaluate different models"], "code": "# Reference: https://www.kaggle.com/code/kabure/eda-feat-engineering-encode-conquer\n\nclfs = []\nseed = 42\n\nclfs.append((\"LogReg\", \n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"LogReg\", LogisticRegression())])))\n\nclfs.append((\"XGBClassifier\", XGBClassifier()))\n\n# clfs.append((\"KNN\", \n#              Pipeline([(\"Scaler\", StandardScaler()),\n#                        (\"KNN\", KNeighborsClassifier(n_neighbors=5))])))\n\nclfs.append((\"DecisionTreeClassifier\", DecisionTreeClassifier()))\n\nclfs.append((\"RandomForestClassifier\", RandomForestClassifier(n_estimators=100)))\n\nclfs.append((\"GradientBoostingClassifier\", GradientBoostingClassifier(n_estimators=100)))\n\nclfs.append((\"RidgeClassifier\", \n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"RidgeClassifier\", RidgeClassifier())])))\n\nclfs.append((\"BaggingClassifier\",\n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"BaggingClassifier\", BaggingClassifier())])))\n\nclfs.append((\"ExtraTreesClassifier\",ExtraTreesClassifier()))\n\n#'neg_mean_absolute_error', 'neg_mean_squared_error','r2'\nscoring = 'roc_auc'\nn_folds = 7\n\nresults, names  = [], [] \n\nfor name, model  in clfs:\n    kfold = KFold(n_splits=n_folds, shuffle=False, random_state=seed)\n    \n    cv_results = cross_val_score(model, \n                                 X_train.values, y_train, \n                                 cv= kfold, scoring=scoring,\n                                 n_jobs=-1)    \n    names.append(name)\n    results.append(cv_results)    \n    msg = \"%s: %f (+/- %f)\" % (name, cv_results.mean(),  \n                               cv_results.std())\n    print(msg)\n    \n# boxplot algorithm comparison\nfig = plt.figure(figsize=(15,6))\nfig.suptitle('Classifier Algorithm Comparison', fontsize=22)\nax = fig.add_subplot(111)\nsns.boxplot(x=names, y=results)\nax.set_xticklabels(names)\nax.set_xlabel(\"Algorithmn\", fontsize=20) \nax.set_ylabel(\"Accuracy of Models\", fontsize=18)\nax.set_xticklabels(ax.get_xticklabels(),rotation=45)\n\nplt.show()", "processed": ["creat pipelin evalu differ model"]}, {"markdown": ["These are just a few of the options available (for more, see the [documentation](https://keras.io/preprocessing/image/)). Let's see what do these transformation mean- Source: [Keras Blog](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html)\n\n* **rotation_range** is a value in degrees (0-180), a range within which to randomly rotate pictures\n* shear_range is for randomly applying [shearing transformations](https://en.wikipedia.org/wiki/Shear_mapping)\n* zoom_range is for randomly zooming inside pictures\n* horizontal_flip is for randomly flipping half of the images horizontally --relevant when there are no assumptions of horizontal assymetry (e.g. real-world pictures).\n* fill_mode is the strategy used for filling in newly created pixels, which can appear after a rotation or a width/height shift.\n\nLet's look at the images which have been created in our directory"], "code": "# Reference: https://www.kaggle.com/code/parulpandey/overview-of-popular-image-augmentation-packages\n\nimages = os.listdir(\"../output/keras_augmentations/\")\nimages\n# Let's look at the augmented images\naug_images = []\nfor img_path in glob.glob(\"../output/keras_augmentations/*.jpeg\"):\n    aug_images.append(mpimg.imread(img_path))\n\nplt.figure(figsize=(20,10))\ncolumns = 5\nfor i, image in enumerate(aug_images):\n    plt.subplot(len(aug_images) / columns + 1, columns, i + 1)\n    plt.imshow(image)\n\n\n# selecting a sample image\nimage5 = train_images[25]\nimshow(image5)\nprint(image5.shape)\nplt.axis('off')", "processed": ["option avail see document http kera io preprocess imag let see transform mean sourc kera blog http blog kera io build power imag classif model use littl data html rotat rang valu degre 0 180 rang within randomli rotat pictur shear rang randomli appli shear transform http en wikipedia org wiki shear map zoom rang randomli zoom insid pictur horizont flip randomli flip half imag horizont relev assumpt horizont assymetri e g real world pictur fill mode strategi use fill newli creat pixel appear rotat width height shift let look imag creat directori"]}, {"markdown": ["# 7. SOLT : Streaming over lightweight data transformations \n\n![](https://github.com/MIPT-Oulu/solt/raw/master/doc/source/_static/logo.png)\n[SOLT](https://github.com/MIPT-Oulu/solt) is a fast data augmentation library, supporting arbitrary amount of images, segmentation masks, keypoints and data labels. It has OpenCV in its back-end, thus it works very fast."], "code": "# Reference: https://www.kaggle.com/code/parulpandey/overview-of-popular-image-augmentation-packages\n\nh,w,c = image5.shape\nimg = image5[:w]\nstream = solt.Stream([\n    slt.Rotate(angle_range=(-90, 90), p=1, padding='r'),\n    slt.Flip(axis=1, p=0.5),\n    slt.Flip(axis=0, p=0.5),\n    slt.Shear(range_x=0.3, range_y=0.8, p=0.5, padding='r'),\n    slt.Scale(range_x=(0.8, 1.3), padding='r', range_y=(0.8, 1.3), same=False, p=0.5),\n    slt.Pad((w, h), 'r'),\n    slt.Crop((w, w), 'r'),\n    slt.CvtColor('rgb2gs', keep_dim=True, p=0.2),\n    slt.HSV((0, 10), (0, 10), (0, 10)),\n    slt.Blur(k_size=7, blur_type='m'),\n    solt.SelectiveStream([\n        slt.CutOut(40, p=1),\n        slt.CutOut(50, p=1),\n        slt.CutOut(10, p=1),\n        solt.Stream(),\n        solt.Stream(),\n    ], n=3),\n], ignore_fast_mode=True)\nfig = plt.figure(figsize=(16,16))\nn_augs = 6\n\n\nrandom.seed(42)\nfor i in range(n_augs):\n    img_aug = stream({'image': img}, return_torch=False, ).data[0].squeeze()\n\n    ax = fig.add_subplot(1,n_augs,i+1)\n    if i == 0:\n        ax.imshow(img)\n    else:\n        ax.imshow(img_aug)\n    ax.set_xticks([])\n    ax.set_yticks([])\n\nplt.show()", "processed": ["7 solt stream lightweight data transform http github com mipt oulu solt raw master doc sourc static logo png solt http github com mipt oulu solt fast data augment librari support arbitrari amount imag segment mask keypoint data label opencv back end thu work fast"]}, {"markdown": ["Training function:"], "code": "# Reference: https://www.kaggle.com/code/artgor/ig-eda-and-models\n\n@jit\ndef fast_auc(y_true, y_prob):\n    \"\"\"\n    fast roc_auc computation: https://www.kaggle.com/c/microsoft-malware-prediction/discussion/76013\n    \"\"\"\n    y_true = np.asarray(y_true)\n    y_true = y_true[np.argsort(y_prob)]\n    nfalse = 0\n    auc = 0\n    n = len(y_true)\n    for i in range(n):\n        y_i = y_true[i]\n        nfalse += (1 - y_i)\n        auc += y_i * nfalse\n    auc /= (nfalse * (n - nfalse))\n    return auc\n\n\ndef eval_auc(y_true, y_pred):\n    \"\"\"\n    Fast auc eval function for lgb.\n    \"\"\"\n    return 'auc', fast_auc(y_true, y_pred), True\n\n\ndef train_model_classification(X, X_test, y, params, folds, model_type='lgb', eval_metric='auc', columns=None, plot_feature_importance=False, model=None,\n                               verbose=10000, early_stopping_rounds=200, n_estimators=50000):\n    \"\"\"\n    A function to train a variety of regression models.\n    Returns dictionary with oof predictions, test predictions, scores and, if necessary, feature importances.\n    \n    :params: X - training data, can be pd.DataFrame or np.ndarray (after normalizing)\n    :params: X_test - test data, can be pd.DataFrame or np.ndarray (after normalizing)\n    :params: y - target\n    :params: folds - folds to split data\n    :params: model_type - type of model to use\n    :params: eval_metric - metric to use\n    :params: columns - columns to use. If None - use all columns\n    :params: plot_feature_importance - whether to plot feature importance of LGB\n    :params: model - sklearn model, works only for \"sklearn\" model type\n    \n    \"\"\"\n    columns = X.columns if columns == None else columns\n    X_test = X_test[columns]\n    \n    # to set up scoring parameters\n    metrics_dict = {'auc': {'lgb_metric_name': eval_auc,\n                        'catboost_metric_name': 'AUC',\n                        'sklearn_scoring_function': metrics.roc_auc_score},\n                    }\n    \n    result_dict = {}\n    \n    # out-of-fold predictions on train data\n    oof = np.zeros((len(X), len(set(y.values))))\n    \n    # averaged predictions on train data\n    prediction = np.zeros((len(X_test), oof.shape[1]))\n    \n    # list of scores on folds\n    scores = []\n    feature_importance = pd.DataFrame()\n    \n    # split and train on folds\n    for fold_n, (train_index, valid_index) in enumerate(folds.split(X, y)):\n        print(f'Fold {fold_n + 1} started at {time.ctime()}')\n        if type(X) == np.ndarray:\n            X_train, X_valid = X[columns][train_index], X[columns][valid_index]\n            y_train, y_valid = y[train_index], y[valid_index]\n        else:\n            X_train, X_valid = X[columns].iloc[train_index], X[columns].iloc[valid_index]\n            y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n            \n        if model_type == 'lgb':\n            model = lgb.LGBMClassifier(**params, n_estimators=n_estimators, n_jobs = -1)\n            model.fit(X_train, y_train, \n                    eval_set=[(X_train, y_train), (X_valid, y_valid)],\n                    verbose=verbose, early_stopping_rounds=early_stopping_rounds)\n            \n            y_pred_valid = model.predict_proba(X_valid)\n            y_pred = model.predict_proba(X_test, num_iteration=model.best_iteration_)\n            \n        if model_type == 'xgb':\n            train_data = xgb.DMatrix(data=X_train, label=y_train, feature_names=X.columns)\n            valid_data = xgb.DMatrix(data=X_valid, label=y_valid, feature_names=X.columns)\n\n            watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]\n            model = xgb.train(dtrain=train_data, num_boost_round=n_estimators, evals=watchlist, early_stopping_rounds=early_stopping_rounds, verbose_eval=verbose, params=params)\n            y_pred_valid = model.predict(xgb.DMatrix(X_valid, feature_names=X.columns), ntree_limit=model.best_ntree_limit)\n            y_pred = model.predict(xgb.DMatrix(X_test, feature_names=X.columns), ntree_limit=model.best_ntree_limit)\n        \n        if model_type == 'sklearn':\n            model = model\n            model.fit(X_train, y_train)\n            \n            y_pred_valid = model.predict(X_valid).reshape(-1,)\n            score = metrics_dict[eval_metric]['sklearn_scoring_function'](y_valid, y_pred_valid)\n            print(f'Fold {fold_n}. {eval_metric}: {score:.4f}.')\n            print('')\n            \n            y_pred = model.predict_proba(X_test)\n        \n        if model_type == 'cat':\n            model = CatBoostClassifier(iterations=n_estimators, eval_metric=metrics_dict[eval_metric]['catboost_metric_name'], **params,\n                                      loss_function=metrics_dict[eval_metric]['catboost_metric_name'])\n            model.fit(X_train, y_train, eval_set=(X_valid, y_valid), cat_features=[], use_best_model=True, verbose=False)\n\n            y_pred_valid = model.predict(X_valid)\n            y_pred = model.predict(X_test)\n        \n        oof[valid_index] = y_pred_valid\n        scores.append(metrics_dict[eval_metric]['sklearn_scoring_function'](y_valid, y_pred_valid[:, 1]))\n\n        prediction += y_pred    \n        \n        if model_type == 'lgb' and plot_feature_importance:\n            # feature importance\n            fold_importance = pd.DataFrame()\n            fold_importance[\"feature\"] = columns\n            fold_importance[\"importance\"] = model.feature_importances_\n            fold_importance[\"fold\"] = fold_n + 1\n            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n\n    prediction /= folds.n_splits\n    \n    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n    \n    result_dict['oof'] = oof\n    result_dict['prediction'] = prediction\n    result_dict['scores'] = scores\n    \n    if model_type == 'lgb':\n        if plot_feature_importance:\n            feature_importance[\"importance\"] /= folds.n_splits\n            cols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n                by=\"importance\", ascending=False)[:50].index\n\n            best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n\n            plt.figure(figsize=(16, 12));\n            sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False));\n            plt.title('LGB Features (avg over folds)');\n            \n            result_dict['feature_importance'] = feature_importance\n        \n    return result_dict\n    \nparams = {'num_leaves': 128,\n          'min_child_samples': 79,\n          'objective': 'binary',\n          'max_depth': -1,\n          'learning_rate': 0.1,\n          \"boosting_type\": \"gbdt\",\n          \"subsample_freq\": 5,\n          \"subsample\": 0.9,\n          \"bagging_seed\": 11,\n          \"metric\": 'auc',\n          \"verbosity\": -1,\n          'reg_alpha': 0.1302650970728192,\n          'reg_lambda': 0.3603427518866501,\n          'colsample_bytree': 0.8\n         }\nresult_dict_lgb = train_model_classification(X=X, X_test=X_test, y=y, params=params, folds=folds, model_type='lgb',\n                                                                                  eval_metric='auc', plot_feature_importance=True, verbose=50, n_estimators=200)\nsub = pd.read_csv(\"../input/sample_submission.csv\")\nsub['target'] = result_dict_lgb['prediction'][:, 1]\nsub.to_csv(\"submission.csv\", index=False)\nsub.head()", "processed": ["train function"]}, {"markdown": ["# The \"Magic\" of Santander\nIn this kernel, we will display pictures of the Santander magic! Previously [here][1], in \"Modified Naive Bayes\", we saw that we can model each variable separately and then combine the 200 models to score LB 0.899. We will do the same here after adding a \"magic\" feature to each variable. We will then ensemble the 200 models with logistic regression and score LB 0.920  \n  \n![image](http://playagricola.com/Kaggle/magic41019.jpg)\n  \n[1]: https://www.kaggle.com/cdeotte/modified-naive-bayes-santander-0-899", "# The \"Magic\" Feature\nWhen LGBM \"looks\" at the histogram for `Var_198`, it \"sees\" that when `var_198<13` the probability of having `target=1` is high. And when `var_198>13` the probability is low. This can be displayed by showing the predictions made by LGBM (when building a model from only the variable `var_198`). LGBM basically predicts `target=0.18` for `var_198<13` and `target=0.10` otherwise. \n  \n![image](http://playagricola.com/Kaggle/198without.png)  \n  \nLGBM \"divides\" the histogram with **vertical lines** because LGBM does not see **horizontal** differences. A histogram places multiple values into a single bin and produces a smooth picture. If you place every value in its own bin, you will have a jagged picture, where bars change heights from value to value. Some values are unique, some values occur dozens of times (and in the case of `var_108`, some values occur over 300 times!!) Below is a histogram with one value per bin and we zoom in on `11.0000<x<11.1000`. We see that value `11.0712` occurs 5 times and its close neighbor `11.0720` occurs only once.  \n    \n![image](http://playagricola.com/Kaggle/198zoom3.png)   \n  \nThese counts are the \"magic\" feature. For each variable, we make a new feature (column) whose value is the number of counts of the corresponding variable. An example of this new column is displayed above next to the histogram. When LGBM has this new feature, it can now \"divide\" the histogram with **horizontal lines** in addition to vertical.  \n  \n![image](http://playagricola.com/Kaggle/198with.png)  \n  \nNotice now that LGBM predicts `target=0.1` when `var_198<13` AND `count=1`. When  `var_198<13` AND `count>1`, it predicts `target=0.36`. This improvement (using the magic) causes validation AUC to become 0.551 as opposed to 0.547 when using `var_198` alone.", "# Why is the Magic difficult to find?\nThe \"magic\" is difficult to find because the new feature `Var_198_FE` interacts with `Var_198`. Therefore if you add the new feature to an LGBM with `feature_fraction=0.05`, you will not increase your CV or LB. You must set `feature_fraction=1.0`. Then you will gain the benefit from the new feature but you also have the determental effect of modeling spurious original variable interactions. None-the-less, adding the new feature and using `feature_fraction=1.0` achieves CV 0.910. To reach 0.920, we must remove the spurious effects from the original variable interactions.  \n  \nUPDATE: I just discovered another reason why magic is hidden. When calculating frequency counts for the test data, you must remove the **fake** test data before counting. (Fake test described [here][1]) If you don't, then your test predictions will score LB 0.900 instead of LB 0.910 and you may disregard frequency counts as useless.\n\n# Maximizing Magic Feature\nTo maximize the gain of the \"magic\" feature (and climb from LB 0.910 to 0.920), we must allow the new feature to interact with the original variables while preventing the original variables from interacting with each other. Here are 3 ways to do that:\n* Use Data Augmentation (as shown in Jiwei's awesome kernel [here][2]). You must keep original and new feature in same row.\n* Use 200 separate models as shown in this kernel below.\n* Merge new feature and original feature into one feature. In original data, simply add 200 to each unique value. (And don't add new columns)\n\n# Let's Begin\nWhen counting the occurence of each value, we will merge the training data and **real** test data first, and count everything together. In YaG320's brilliant kernel [here][1], we learned that half the test data is fake.  \n  \n[1]: https://www.kaggle.com/yag320/list-of-fake-samples-and-public-private-lb-split\n[2]: https://www.kaggle.com/jiweiliu/lgb-2-leaves-augment"], "code": "# LOAD LIBRARIES\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd, numpy as np, gc\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom sklearn.metrics import roc_auc_score\nimport lightgbm as lgb\nimport statsmodels.api as sm\n\n# GET INDICIES OF REAL TEST DATA FOR FE\n#######################\n# TAKE FROM YAG320'S KERNEL\n# https://www.kaggle.com/yag320/list-of-fake-samples-and-public-private-lb-split\n\ntest_path = '../input/test.csv'\n\ndf_test = pd.read_csv(test_path)\ndf_test.drop(['ID_code'], axis=1, inplace=True)\ndf_test = df_test.values\n\nunique_samples = []\nunique_count = np.zeros_like(df_test)\nfor feature in range(df_test.shape[1]):\n    _, index_, count_ = np.unique(df_test[:, feature], return_counts=True, return_index=True)\n    unique_count[index_[count_ == 1], feature] += 1\n\n# Samples which have unique values are real the others are fake\nreal_samples_indexes = np.argwhere(np.sum(unique_count, axis=1) > 0)[:, 0]\nsynthetic_samples_indexes = np.argwhere(np.sum(unique_count, axis=1) == 0)[:, 0]\n\nprint('Found',len(real_samples_indexes),'real test')\nprint('Found',len(synthetic_samples_indexes),'fake test')\n\n###################\n\nd = {}\nfor i in range(200): d['var_'+str(i)] = 'float32'\nd['target'] = 'uint8'\nd['ID_code'] = 'object'\n\ntrain = pd.read_csv('../input/train.csv', dtype=d)\ntest = pd.read_csv('../input/test.csv', dtype=d)\n\nprint('Loaded',len(train),'rows of train')\nprint('Loaded',len(test),'rows of test')\n# FREQUENCY ENCODE\ndef encode_FE(df,col,test):\n    cv = df[col].value_counts()\n    nm = col+'_FE'\n    df[nm] = df[col].map(cv)\n    test[nm] = test[col].map(cv)\n    test[nm].fillna(0,inplace=True)\n    if cv.max()<=255:\n        df[nm] = df[nm].astype('uint8')\n        test[nm] = test[nm].astype('uint8')\n    else:\n        df[nm] = df[nm].astype('uint16')\n        test[nm] = test[nm].astype('uint16')        \n    return\n\ntest['target'] = -1\ncomb = pd.concat([train,test.loc[real_samples_indexes]],axis=0,sort=True)\nfor i in range(200): encode_FE(comb,'var_'+str(i),test)\ntrain = comb[:len(train)]; del comb\nprint('Added 200 new magic features!')\n# LGBM PARAMETERS\nparam = {\n    'learning_rate': 0.04,\n    'num_leaves': 3,\n    'metric':'auc',\n    'boost_from_average':'false',\n    'feature_fraction': 1.0,\n    'max_depth': -1,\n    'objective': 'binary',\n    'verbosity': -10}\ntrain2 = train.sample(frac=1,random_state=42)\nevals_result = {}\nnum_vars = 200\n\n# SAVE OUT-OF-FOLD PREDICTIONS\nall_oof = np.zeros((len(train2),num_vars+1))\nall_oof[:,0] = np.ones(len(train2))\nall_oofB = np.zeros((len(train2),num_vars+1))\nall_oofB[:,0] = np.ones(len(train2))\n\n# SAVE TEST PREDICTIONS\nall_preds = np.zeros((len(test),num_vars+1))\nall_preds[:,0] = np.ones(len(test))\nall_predsB = np.zeros((len(test),num_vars+1))\nall_predsB[:,0] = np.ones(len(test))\n\nfor j in range(num_vars):\n    \n    # MODEL WITH MAGIC\n    features = ['var_'+str(j),'var_'+str(j)+'_FE']\n    oof = np.zeros(len(train2))\n    preds = np.zeros(len(test))\n    \n    # PLOT DENSITIES    \n    plt.figure(figsize=(16,5))\n    plt.subplot(1,2,2)\n    sns.distplot(train2[train2['target']==0]['var_'+str(j)], label = 't=0')\n    sns.distplot(train2[train2['target']==1]['var_'+str(j)], label = 't=1')\n    plt.legend()\n    plt.yticks([])\n    plt.xlabel('Var_'+str(j))\n\n    # MAKE A GRID OF POINTS FOR LGBM TO PREDICT    \n    mn,mx = plt.xlim()\n    mnFE = train2['var_'+str(j)+'_FE'].min()\n    mxFE = train2['var_'+str(j)+'_FE'].max()\n    step = 50\n    stepB = train2['var_'+str(j)+'_FE'].nunique()\n    w = (mx-mn)/step\n    x = w * (np.arange(0,step)+0.5) + mn\n    x2 = np.array([])\n    for i in range(stepB):\n        x2 = np.concatenate([x,x2])\n    df = pd.DataFrame({'var_'+str(j):x2})\n    df['var_'+str(j)+'_FE'] = mnFE + (mxFE-mnFE)/(stepB-1) * (df.index//step)\n    df['pred'] = 0\n    \n    # 5-FOLD WITH MAGIC\n    for k in range(5):\n            valid = train2.iloc[k*40000:(k+1)*40000]\n            train = train2[ ~train2.index.isin(valid.index) ]    \n            trn_data  = lgb.Dataset(train[features], label=train['target'])\n            val_data = lgb.Dataset(valid[features], label=valid['target'])     \n            model = lgb.train(param, trn_data, 750, valid_sets = [trn_data, val_data], \n                    verbose_eval=False, evals_result=evals_result)      \n            x = evals_result['valid_1']['auc']\n            best = x.index(max(x))\n            #print('i=',i,'k=',k,'best=',best)\n            oof[k*40000:(k+1)*40000] = model.predict(valid[features], num_iteration=best)\n            preds += model.predict(test[features], num_iteration=best)/5.0\n            df['pred'] += model.predict(df[features], num_iteration=best)/5.0\n            \n    val_auc = roc_auc_score(train2['target'],oof)\n    print('VAR_'+str(j)+' with magic val_auc =',round(val_auc,5))\n    all_oof[:,j+1] = oof\n    all_preds[:,j+1] = preds\n    x = df['pred'].values\n    x = np.reshape(x,(stepB,step))\n    x = np.flip(x,axis=0)\n    \n    # PLOT LGBM PREDICTIONS USING MAGIC    \n    plt.subplot(1,2,1)\n    sns.heatmap(x, cmap='RdBu_r', center=0.0) \n    plt.title('VAR_'+str(j)+' Predictions with Magic',fontsize=16)    \n    plt.xticks(np.linspace(0,49,5),np.round(np.linspace(mn,mx,5),1))\n    plt.xlabel('Var_'+str(j))\n    s = min(mxFE-mnFE+1,20)\n    plt.yticks(np.linspace(mnFE,mxFE,s)-0.5,np.linspace(mxFE,mnFE,s).astype('int'))\n    plt.ylabel('Count')\n    plt.show()\n    \n    # MODEL WITHOUT MAGIC\n    features = ['var_'+str(j)]\n    oof = np.zeros(len(train2))\n    preds = np.zeros(len(test))\n    \n    # PLOT DENSITIES\n    plt.figure(figsize=(16,5))\n    plt.subplot(1,2,2)\n    sns.distplot(train2[train2['target']==0]['var_'+str(j)], label = 't=0')\n    sns.distplot(train2[train2['target']==1]['var_'+str(j)], label = 't=1')\n    plt.legend()\n    plt.yticks([])\n    plt.xlabel('Var_'+str(j))\n    \n    # MAKE A GRID OF POINTS FOR LGBM TO PREDICT\n    mn,mx = plt.xlim()\n    mnFE = train2['var_'+str(j)+'_FE'].min()\n    mxFE = train2['var_'+str(j)+'_FE'].max()\n    step = 50\n    stepB = train2['var_'+str(j)+'_FE'].nunique()\n    w = (mx-mn)/step\n    x = w * (np.arange(0,step)+0.5) + mn\n    x2 = np.array([])\n    for i in range(stepB):\n        x2 = np.concatenate([x,x2])\n    df = pd.DataFrame({'var_'+str(j):x2})\n    df['var_'+str(j)+'_FE'] = mnFE + (mxFE-mnFE)/(stepB-1) * (df.index//step)\n    df['pred'] = 0\n    \n    # 5-FOLD WITHOUT MAGIC\n    for k in range(5):\n            valid = train2.iloc[k*40000:(k+1)*40000]\n            train = train2[ ~train2.index.isin(valid.index) ]\n            trn_data  = lgb.Dataset(train[features], label=train['target'])\n            val_data = lgb.Dataset(valid[features], label=valid['target'])     \n            model = lgb.train(param, trn_data, 750, valid_sets = [trn_data, val_data], \n                    verbose_eval=False, evals_result=evals_result)      \n            x = evals_result['valid_1']['auc']\n            best = x.index(max(x))\n            #print('i=',i,'k=',k,'best=',best)\n            oof[k*40000:(k+1)*40000] = model.predict(valid[features], num_iteration=best)\n            preds += model.predict(test[features], num_iteration=best)/5.0\n            df['pred'] += model.predict(df[features], num_iteration=best)/5.0\n            \n    val_auc = roc_auc_score(train2['target'],oof)\n    print('VAR_'+str(j)+' without magic val_auc =',round(val_auc,5))\n    all_oofB[:,j+1] = oof\n    all_predsB[:,j+1] = preds\n    x = df['pred'].values\n    x = np.reshape(x,(stepB,step))\n    x = np.flip(x,axis=0)\n    \n    # PLOT LGBM PREDICTIONS WITHOUT USING MAGIC\n    plt.subplot(1,2,1)\n    sns.heatmap(x, cmap='RdBu_r', center=0.0) \n    plt.title('VAR_'+str(j)+' Predictions without Magic',fontsize=16)\n    plt.xticks(np.linspace(0,49,5),np.round(np.linspace(mn,mx,5),1))\n    plt.xlabel('Var_'+str(j))\n    plt.yticks([])\n    plt.ylabel('')\n    plt.show()", "processed": ["magic santand kernel display pictur santand magic previous 1 modifi naiv bay saw model variabl separ combin 200 model score lb 0 899 ad magic featur variabl ensembl 200 model logist regress score lb 0 920 imag http playagricola com kaggl magic41019 jpg 1 http www kaggl com cdeott modifi naiv bay santand 0 899", "magic featur lgbm look histogram var 198 see var 198 13 probabl target 1 high var 198 13 probabl low display show predict made lgbm build model variabl var 198 lgbm basic predict target 0 18 var 198 13 target 0 10 otherwis imag http playagricola com kaggl 198without png lgbm divid histogram vertic line lgbm see horizont differ histogram place multipl valu singl bin produc smooth pictur place everi valu bin jag pictur bar chang height valu valu valu uniqu valu occur dozen time case var 108 valu occur 300 time histogram one valu per bin zoom 11 0000 x 11 1000 see valu 11 0712 occur 5 time close neighbor 11 0720 occur imag http playagricola com kaggl 198zoom3 png count magic featur variabl make new featur column whose valu number count correspond variabl exampl new column display next histogram lgbm new featur divid histogram horizont line addit vertic imag http playagricola com kaggl 198with png notic lgbm predict target 0 1 var 198 13 count 1 var 198 13 count 1 predict target 0 36 improv use magic caus valid auc becom 0 551 oppos 0 547 use var 198 alon", "magic difficult find magic difficult find new featur var 198 fe interact var 198 therefor add new featur lgbm featur fraction 0 05 increas cv lb must set featur fraction 1 0 gain benefit new featur also determent effect model spuriou origin variabl interact none le ad new featur use featur fraction 1 0 achiev cv 0 910 reach 0 920 must remov spuriou effect origin variabl interact updat discov anoth reason magic hidden calcul frequenc count test data must remov fake test data count fake test describ 1 test predict score lb 0 900 instead lb 0 910 may disregard frequenc count useless maxim magic featur maxim gain magic featur climb lb 0 910 0 920 must allow new featur interact origin variabl prevent origin variabl interact 3 way use data augment shown jiwei awesom kernel 2 must keep origin new featur row use 200 separ model shown kernel merg new featur origin featur one featur origin data simpli add 200 uniqu valu add new column let begin count occur valu merg train data real test data first count everyth togeth yag320 brilliant kernel 1 learn half test data fake 1 http www kaggl com yag320 list fake sampl public privat lb split 2 http www kaggl com jiweiliu lgb 2 leav augment"]}, {"markdown": ["Plot several examples of input images."], "code": "# Reference: https://www.kaggle.com/code/iafoss/pretrained-resnet34-with-rgby-0-460-public-lb\n\ndef display_imgs(x):\n    columns = 4\n    bs = x.shape[0]\n    rows = min((bs+3)//4,4)\n    fig=plt.figure(figsize=(columns*4, rows*4))\n    for i in range(rows):\n        for j in range(columns):\n            idx = i+j*columns\n            fig.add_subplot(rows, columns, idx+1)\n            plt.axis('off')\n            plt.imshow((x[idx,:,:,:3]*255).astype(np.int))\n    plt.show()\n    \ndisplay_imgs(np.asarray(md.trn_ds.denorm(x)))", "processed": ["plot sever exampl input imag"]}, {"markdown": ["I begin with finding the optimal learning rate. The following function runs training with different lr and records the loss. Increase of the loss indicates onset of divergence of training. The optimal lr lies in the vicinity of the minimum of the curve but before the onset of divergence. Based on the following plot, for the current setup the divergence starts at ~0.05, and the recommended learning rate is ~0.005."], "code": "# Reference: https://www.kaggle.com/code/iafoss/pretrained-resnet34-with-rgby-0-460-public-lb\n\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    learner.lr_find()\nlearner.sched.plot()", "processed": ["begin find optim learn rate follow function run train differ lr record loss increas loss indic onset diverg train optim lr lie vicin minimum curv onset diverg base follow plot current setup diverg start 0 05 recommend learn rate 0 005"]}, {"markdown": ["# Custom LR scheduler\nFrom starter [kernel][1]\n\n[1]: https://www.kaggle.com/mgornergoogle/getting-started-with-100-flowers-on-tpu"], "code": "# Reference: https://www.kaggle.com/code/cdeotte/cutmix-and-mixup-on-gpu-tpu\n\n# Learning rate schedule for TPU, GPU and CPU.\n# Using an LR ramp up because fine-tuning a pre-trained model.\n# Starting with a high LR would break the pre-trained weights.\n\nLR_START = 0.00001\nLR_MAX = 0.00005 * strategy.num_replicas_in_sync\nLR_MIN = 0.00001\nLR_RAMPUP_EPOCHS = 5\nLR_SUSTAIN_EPOCHS = 0\nLR_EXP_DECAY = .8\n\ndef lrfn(epoch):\n    if epoch < LR_RAMPUP_EPOCHS:\n        lr = (LR_MAX - LR_START) / LR_RAMPUP_EPOCHS * epoch + LR_START\n    elif epoch < LR_RAMPUP_EPOCHS + LR_SUSTAIN_EPOCHS:\n        lr = LR_MAX\n    else:\n        lr = (LR_MAX - LR_MIN) * LR_EXP_DECAY**(epoch - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS) + LR_MIN\n    return lr\n    \nlr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose = True)\n\nrng = [i for i in range(25 if EPOCHS<25 else EPOCHS)]\ny = [lrfn(x) for x in rng]\nplt.plot(rng, y)\nprint(\"Learning rate schedule: {:.3g} to {:.3g} to {:.3g}\".format(y[0], max(y), y[-1]))", "processed": ["custom lr schedul starter kernel 1 1 http www kaggl com mgornergoogl get start 100 flower tpu"]}, {"markdown": ["# Display CutMix Augmentation"], "code": "# Reference: https://www.kaggle.com/code/cdeotte/cutmix-and-mixup-on-gpu-tpu\n\nrow = 6; col = 4;\nrow = min(row,AUG_BATCH//col)\nall_elements = get_training_dataset(load_dataset(TRAINING_FILENAMES),do_aug=False).unbatch()\naugmented_element = all_elements.repeat().batch(AUG_BATCH).map(cutmix)\n\nfor (img,label) in augmented_element:\n    plt.figure(figsize=(15,int(15*row/col)))\n    for j in range(row*col):\n        plt.subplot(row,col,j+1)\n        plt.axis('off')\n        plt.imshow(img[j,])\n    plt.show()\n    break", "processed": ["display cutmix augment"]}, {"markdown": ["# Display MixUp Augmentation"], "code": "# Reference: https://www.kaggle.com/code/cdeotte/cutmix-and-mixup-on-gpu-tpu\n\nrow = 6; col = 4;\nrow = min(row,AUG_BATCH//col)\nall_elements = get_training_dataset(load_dataset(TRAINING_FILENAMES),do_aug=False).unbatch()\naugmented_element = all_elements.repeat().batch(AUG_BATCH).map(mixup)\n\nfor (img,label) in augmented_element:\n    plt.figure(figsize=(15,int(15*row/col)))\n    for j in range(row*col):\n        plt.subplot(row,col,j+1)\n        plt.axis('off')\n        plt.imshow(img[j,])\n    plt.show()\n    break", "processed": ["display mixup augment"]}, {"markdown": ["# Display 33%/33%/33% CutMix/MixUp/None\nChange the variables `SWITCH`, `CUTMIX_PROB` and `MIXUP_PROB` in function `transform()` to control the amount of augmentation during training. CutMix will occur `SWITCH * CUTMIX_PROB` often and MixUp will occur `(1-SWITCH) * MIXUP_PROB` often. Currently `SWITCH=0.5`, `CUTMIX_PROB=0.666` and `MIXUP_PROB=0.666` resulting in 33% cutmix, 33% mixup, and 33% none during training."], "code": "# Reference: https://www.kaggle.com/code/cdeotte/cutmix-and-mixup-on-gpu-tpu\n\nrow = 6; col = 4;\nrow = min(row,AUG_BATCH//col)\nall_elements = get_training_dataset(load_dataset(TRAINING_FILENAMES),do_aug=False).unbatch()\naugmented_element = all_elements.repeat().batch(AUG_BATCH).map(transform)\n\nfor (img,label) in augmented_element:\n    plt.figure(figsize=(15,int(15*row/col)))\n    for j in range(row*col):\n        plt.subplot(row,col,j+1)\n        plt.axis('off')\n        plt.imshow(img[j,])\n    plt.show()\n    break", "processed": ["display 33 33 33 cutmix mixup none chang variabl switch cutmix prob mixup prob function transform control amount augment train cutmix occur switch cutmix prob often mixup occur 1 switch mixup prob often current switch 0 5 cutmix prob 0 666 mixup prob 0 666 result 33 cutmix 33 mixup 33 none train"]}, {"markdown": ["The chart below shows the survival rates. For each group you can see the proportion of players who drop out over time, where time is defined as the number of weeks since the patient's baseline CT scan. The size of the group with above average lung capacity decreases from left to right as the disease progresses. \n\nThe solid lines represent the observed survival rate for each group. When one curve is below the other it represents a lower survival rate; i.e., a higher rate of patients whose lung capacity drops below the mean.\n"], "code": "# Reference: https://www.kaggle.com/code/jpmiller/smoking-good-for-models-bad-for-health\n\nCONFIDENCE = 0.90\n\nidx1 =  (train.SmokingStatus == 'Never smoked')\nidx2 = (train.SmokingStatus.isin(['Ex-smoker', 'Currently smokes']))\n\ndurations1 = train.loc[idx1, 'Weeks']\ndurations2 = train.loc[idx2, 'Weeks']\n\nevents1 = train.loc[idx1, 'LowFVC']\nevents2 = train.loc[idx2, 'LowFVC']\n\nkmf1 = KaplanMeierFitter()\nkmf1.fit(durations1, events1, alpha=(1-CONFIDENCE), label='Never Smoked')\n\nkmf2 = KaplanMeierFitter()\nkmf2.fit(durations2, events2, alpha=(1-CONFIDENCE), label='Smoked')\n\nplt.clf()\n%matplotlib inline\n\nplt.figure(figsize=(12,8))\nplt.style.use('seaborn-whitegrid')\nSMALL_SIZE = 16\nMEDIUM_SIZE = 18\nBIGGER_SIZE = 24\nplt.rc('font', size=SMALL_SIZE)\nplt.rc('axes', titlesize=MEDIUM_SIZE)\nplt.rc('axes', labelsize=MEDIUM_SIZE)\nplt.rc('xtick', labelsize=MEDIUM_SIZE)\nplt.rc('ytick', labelsize=MEDIUM_SIZE)\nplt.rc('legend', fontsize=SMALL_SIZE)\nplt.rc('figure', titlesize=BIGGER_SIZE)\n\n\np1 = kmf1.plot()\np2 = kmf2.plot(ax=p1)\n\n\nplt.xlim(-5, 118)\nplt.title(\"Maintaining Lung Capacity\")\nplt.xlabel(\"Weeks since baseline CT\")\nplt.ylabel(\"Fraction of Group with above average FVC\")\nplt.show()\n", "processed": ["chart show surviv rate group see proport player drop time time defin number week sinc patient baselin ct scan size group averag lung capac decreas left right diseas progress solid line repres observ surviv rate group one curv repres lower surviv rate e higher rate patient whose lung capac drop mean"]}, {"markdown": ["Each of the 300 variables has a Gaussian distribution with mean 0 and standard deviation 1. We can see this by plotting histograms of the train and test data combined. Therefore the 20000 data points reside within a 300 dimensional hypersphere of approximate radius 3 centered at the origin (zero)."], "code": "# Reference: https://www.kaggle.com/code/cdeotte/fun-data-animation\n\nplt.figure(figsize=(15,15))\nfor i in range(5):\n    for j in range(5):\n        plt.subplot(5,5,5*i+j+1)\n        plt.hist(test[str(5*i+j)],bins=100)\n        plt.title('Variable '+str(5*i+j))\nplt.show()", "processed": ["300 variabl gaussian distribut mean 0 standard deviat 1 see plot histogram train test data combin therefor 20000 data point resid within 300 dimension hyperspher approxim radiu 3 center origin zero"]}, {"markdown": ["If we plot only variable 33 and 65 from the 250 training data points and color target=1 yellow and target=0 blue, we see that the data may be separable. We can confirm this with logistic regression on all 300 variables below."], "code": "# Reference: https://www.kaggle.com/code/cdeotte/fun-data-animation\n\nplt.figure(figsize=(10,10))\nplt.scatter(train['33'],train['65'],c=train['target'])\nplt.plot([-1.6,1.4],[3,-3],':k')\nplt.xlabel('variable 33')\nplt.ylabel('variable 65')\nplt.title('Training data')\nplt.show()", "processed": ["plot variabl 33 65 250 train data point color target 1 yellow target 0 blue see data may separ confirm logist regress 300 variabl"]}, {"markdown": ["What are the most frequent assets traded (remember not all are present at all times, bankruptcy, IPO etc)"], "code": "# Reference: https://www.kaggle.com/code/zikazika/predicting-stock-movement\n\nvolumeByAssets = market_train_df.groupby(market_train_df['assetCode'])['volume'].sum()\nhighestVolumes = volumeByAssets.sort_values(ascending=False)[0:10]\n\ntrace1 = go.Pie(\n    labels = highestVolumes.index,\n    values = highestVolumes.values\n)\n\nlayout = dict(title = \"Highest trading volumes\")\ndata = [trace1]\n\npy.iplot(dict(data=data, layout=layout), filename='basic-line')", "processed": ["frequent asset trade rememb present time bankruptci ipo etc"]}, {"markdown": ["Now let us take 5 random assets and plot them. Note that not all assets start measurament from 2007"], "code": "# Reference: https://www.kaggle.com/code/zikazika/predicting-stock-movement\n\ndata = []\nfor asset in np.random.choice(market_train_df['assetName'].unique(), 5):\n    asset_df = market_train_df[(market_train_df['assetName'] == asset)]\n\n    data.append(go.Scatter(\n        x = asset_df['time'].dt.strftime(date_format='%Y-%m-%d').values,\n        y = asset_df['close'].values,\n        name = asset\n    ))\nlayout = go.Layout(dict(title = \"Closing prices of 5 random assets\",\n                  xaxis = dict(title = 'Month'),\n                  yaxis = dict(title = 'Price (USD)'),\n                  ),legend=dict(\n                orientation=\"h\"))\npy.iplot(dict(data=data, layout=layout), filename='basic-line')", "processed": ["let u take 5 random asset plot note asset start measura 2007"]}, {"markdown": ["Those were just a 5 random companies, let us have a look now at the general trend of the whole stock market, i.e. the whole data set along with some significant dates (crashes)"], "code": "# Reference: https://www.kaggle.com/code/zikazika/predicting-stock-movement\n\ndata = []\nfor i in [0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95]:\n    price_df = market_train_df.groupby('time')['close'].quantile(i).reset_index()\n\n    data.append(go.Scatter(\n        x = price_df['time'].dt.strftime(date_format='%Y-%m-%d').values,\n        y = price_df['close'].values,\n        name = f'{i} quantile'\n    ))\nlayout = go.Layout(dict(title = \"Trends of closing prices by quantiles\",\n                  xaxis = dict(title = 'Month'),\n                  yaxis = dict(title = 'Price (USD)'),\n                  ),legend=dict(\n                orientation=\"h\"),\n    annotations=[\n        dict(\n            x='2008-09-01 22:00:00+0000',\n            y=82,\n            xref='x',\n            yref='y',\n            text='Collapse of Lehman Brothers',\n            showarrow=True,\n            font=dict(\n                family='Courier New, monospace',\n                size=16,\n                color='#ffffff'\n            ),\n            align='center',\n            arrowhead=2,\n            arrowsize=1,\n            arrowwidth=2,\n            arrowcolor='#636363',\n            ax=20,\n            ay=-30,\n            bordercolor='#c7c7c7',\n            borderwidth=2,\n            borderpad=4,\n            bgcolor='#ff7f0e',\n            opacity=0.8\n        ),\n        dict(\n            x='2011-08-01 22:00:00+0000',\n            y=85,\n            xref='x',\n            yref='y',\n            text='Black Monday',\n            showarrow=True,\n            font=dict(\n                family='Courier New, monospace',\n                size=16,\n                color='#ffffff'\n            ),\n            align='center',\n            arrowhead=2,\n            arrowsize=1,\n            arrowwidth=2,\n            arrowcolor='#636363',\n            ax=20,\n            ay=-30,\n            bordercolor='#c7c7c7',\n            borderwidth=2,\n            borderpad=4,\n            bgcolor='#ff7f0e',\n            opacity=0.8\n        ),\n        dict(\n            x='2014-10-01 22:00:00+0000',\n            y=120,\n            xref='x',\n            yref='y',\n            text='Another crisis',\n            showarrow=True,\n            font=dict(\n                family='Courier New, monospace',\n                size=16,\n                color='#ffffff'\n            ),\n            align='center',\n            arrowhead=2,\n            arrowsize=1,\n            arrowwidth=2,\n            arrowcolor='#636363',\n            ax=-20,\n            ay=-30,\n            bordercolor='#c7c7c7',\n            borderwidth=2,\n            borderpad=4,\n            bgcolor='#ff7f0e',\n            opacity=0.8\n        ),\n        dict(\n            x='2016-01-01 22:00:00+0000',\n            y=120,\n            xref='x',\n            yref='y',\n            text='Oil prices crash',\n            showarrow=True,\n            font=dict(\n                family='Courier New, monospace',\n                size=16,\n                color='#ffffff'\n            ),\n            align='center',\n            arrowhead=2,\n            arrowsize=1,\n            arrowwidth=2,\n            arrowcolor='#636363',\n            ax=20,\n            ay=-30,\n            bordercolor='#c7c7c7',\n            borderwidth=2,\n            borderpad=4,\n            bgcolor='#ff7f0e',\n            opacity=0.8\n        )\n    ])\npy.iplot(dict(data=data, layout=layout), filename='basic-line')", "processed": ["5 random compani let u look gener trend whole stock market e whole data set along signific date crash"]}, {"markdown": ["Now let us see which time span(month) has the most unusual behavior. And does it coincide with financial crisis."], "code": "# Reference: https://www.kaggle.com/code/zikazika/predicting-stock-movement\n\ng = grouped.sort_values(('price_diff', 'std'), ascending=False)[:10]\ng['min_text'] = 'Maximum price drop: ' + (-1 * g['price_diff']['min']).astype(str)\ntrace = go.Scatter(\n    x = g['time'].dt.strftime(date_format='%Y-%m-%d').values,\n    y = g['price_diff']['std'].values,\n    mode='markers',\n    marker=dict(\n        size = g['price_diff']['std'].values,\n        color = g['price_diff']['std'].values,\n        colorscale='Portland',\n        showscale=True\n    ),\n    text = g['min_text'].values\n    #text = f\"Maximum price drop: {g['price_diff']['min'].values}\"\n    #g['time'].dt.strftime(date_format='%Y-%m-%d').values\n)\ndata = [trace]\n\nlayout= go.Layout(\n    autosize= True,\n    title= 'Top 10 months by standard deviation of price change within a day',\n    hovermode= 'closest',\n    yaxis=dict(\n        title= 'price_diff',\n        ticklen= 5,\n        gridwidth= 2,\n    ),\n    showlegend= False\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig,filename='scatter2010')", "processed": ["let u see time span month unusu behavior coincid financi crisi"]}, {"markdown": ["After we impute it, we can observe standard fluctuation:"], "code": "# Reference: https://www.kaggle.com/code/zikazika/predicting-stock-movement\n\nmarket_train_df['price_diff'] = market_train_df['close'] - market_train_df['open']\ngrouped = market_train_df.groupby(['time']).agg({'price_diff': ['std', 'min']}).reset_index()\ng = grouped.sort_values(('price_diff', 'std'), ascending=False)[:10]\ng['min_text'] = 'Maximum price drop: ' + (-1 * np.round(g['price_diff']['min'], 2)).astype(str)\ntrace = go.Scatter(\n    x = g['time'].dt.strftime(date_format='%Y-%m-%d').values,\n    y = g['price_diff']['std'].values,\n    mode='markers',\n    marker=dict(\n        size = g['price_diff']['std'].values * 5,\n        color = g['price_diff']['std'].values,\n        colorscale='Portland',\n        showscale=True\n    ),\n    text = g['min_text'].values\n    #text = f\"Maximum price drop: {g['price_diff']['min'].values}\"\n    #g['time'].dt.strftime(date_format='%Y-%m-%d').values\n)\ndata = [trace]\n\nlayout= go.Layout(\n    autosize= True,\n    title= 'Top 10 months by standard deviation of price change within a day',\n    hovermode= 'closest',\n    yaxis=dict(\n        title= 'price_diff',\n        ticklen= 5,\n        gridwidth= 2,\n    ),\n    showlegend= False\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig,filename='scatter2010')", "processed": ["imput observ standard fluctuat"]}, {"markdown": ["Assumption is that (intuition) this variable should be standard normally distributed. But some of the values (min max) are problematic, let us get rid of them."], "code": "# Reference: https://www.kaggle.com/code/zikazika/predicting-stock-movement\n\nnoOutliers = market_train_df[(market_train_df['returnsOpenNextMktres10'] < 1) &  (market_train_df['returnsOpenNextMktres10'] > -1)]\ntrace1 = go.Histogram(\n    x = noOutliers.sample(n=10000)['returnsOpenNextMktres10'].values\n)\n\nlayout = dict(title = \"returnsOpenNextMktres10 (random 10.000 sample; without outliers)\")\ndata = [trace1]\n\npy.iplot(dict(data=data, layout=layout), filename='basic-line')", "processed": ["assumpt intuit variabl standard normal distribut valu min max problemat let u get rid"]}, {"markdown": ["As a final EDA analysis we are going to plot mean values of all of the return variables"], "code": "# Reference: https://www.kaggle.com/code/zikazika/predicting-stock-movement\n\ndata = []\nfor col in ['returnsClosePrevRaw1', 'returnsOpenPrevRaw1',\n       'returnsClosePrevMktres1', 'returnsOpenPrevMktres1',\n       'returnsClosePrevRaw10', 'returnsOpenPrevRaw10',\n       'returnsClosePrevMktres10', 'returnsOpenPrevMktres10',\n       'returnsOpenNextMktres10']:\n    df = market_train_df.groupby('time')[col].mean().reset_index()\n    data.append(go.Scatter(\n        x = df['time'].dt.strftime(date_format='%Y-%m-%d').values,\n        y = df[col].values,\n        name = col\n    ))\n    \nlayout = go.Layout(dict(title = \"Treand of mean values\",\n                  xaxis = dict(title = 'Month'),\n                  yaxis = dict(title = 'Price (USD)'),\n                  ),legend=dict(\n                orientation=\"h\"),)\npy.iplot(dict(data=data, layout=layout), filename='basic-line')", "processed": ["final eda analysi go plot mean valu return variabl"]}, {"markdown": ["## Recordist\n\nRecordist is the user who provided the recordings."], "code": "# Reference: https://www.kaggle.com/code/parulpandey/eda-and-audio-processing-with-python\n\n# Total number of people who provided the recordings\ntrain['recordist'].nunique()\n# Top 10 recordists in terms of the number of recordings done\ntrain['recordist'].value_counts()[:10].sort_values().iplot(kind='barh',color='#3780BF')", "processed": ["recordist recordist user provid record"]}, {"markdown": ["## Ratings"], "code": "# Reference: https://www.kaggle.com/code/parulpandey/eda-and-audio-processing-with-python\n\ntrain['rating'].value_counts().iplot(kind='bar',color='#3780BF')", "processed": ["rate"]}, {"markdown": ["## Date of recordings"], "code": "# Reference: https://www.kaggle.com/code/parulpandey/eda-and-audio-processing-with-python\n\n# Convert string to datetime64\ntrain['date'] = train['date'].apply(pd.to_datetime,format='%Y-%m-%d', errors='coerce')\n#train.set_index('date',inplace=True)\ntrain['date'].value_counts().plot(figsize=(12,8))", "processed": ["date record"]}, {"markdown": ["## Species"], "code": "# Reference: https://www.kaggle.com/code/parulpandey/eda-and-audio-processing-with-python\n\n# Total no of unique species in the dataset\nprint(len(train['species'].value_counts().index))\n\ntrain['species'].value_counts()\ntrain['species'].value_counts().iplot()", "processed": ["speci"]}, {"markdown": ["The original train data has 21,375 recordings and the extended train data has an additional 22,015 recordings for 253 out of 264 species which more than doubles the training data size."], "code": "# Reference: https://www.kaggle.com/code/parulpandey/eda-and-audio-processing-with-python\n\ndf_original = train.groupby(\"species\")[\"filename\"].count().reset_index().rename(columns = {\"filename\": \"original_recordings\"})\ndf_extended = train_ext.groupby(\"species\")[\"filename\"].count().reset_index().rename(columns = {\"filename\": \"extended_recordings\"})\n\ndf = df_original.merge(df_extended, on = \"species\", how = \"left\").fillna(0)\ndf[\"total_recordings\"] = df.original_recordings + df.extended_recordings\ndf = df.sort_values(\"total_recordings\").reset_index().sort_values('total_recordings',ascending=False)\ndf.head()\n# Plot the total recordings\nf, ax = plt.subplots(figsize=(10, 50))\n\nsns.set_color_codes(\"pastel\")\nsns.barplot(x=\"total_recordings\", y=\"species\", data=df,\n            label=\"total_recordings\", color=\"r\")\n\n# Plot the original recordings\nsns.set_color_codes(\"muted\")\nsns.barplot(x=\"original_recordings\", y=\"species\", data=df,\n            label=\"original_recordings\", color=\"g\")\n\n# Add a legend and informative axis label\nax.legend(ncol=2, loc=\"lower right\", frameon=True)\nax.set(xlim=(0, 2000), ylabel=\"\",\n       xlabel=\"Count\")\nsns.despine(left=True, bottom=True)", "processed": ["origin train data 21 375 record extend train data addit 22 015 record 253 264 speci doubl train data size"]}, {"markdown": ["## Visualizing Audio\n\nWe can plot the audio array using librosa.display.waveplot. Let's plot the  amplitude envelope of a waveform.\n"], "code": "# Reference: https://www.kaggle.com/code/parulpandey/eda-and-audio-processing-with-python\n\nplt.figure(figsize=(14, 5))\nlibrosa.display.waveplot(x, sr=sr)", "processed": ["visual audio plot audio array use librosa display waveplot let plot amplitud envelop waveform"]}, {"markdown": ["### 2.Feature extraction\n\nEvery audio signal consists of many features. However, we must extract the characteristics that are relevant to the problem we are trying to solve. The process of extracting features to use them for analysis is called feature extraction. Let us study about few of the features in detail.", "###  Zero Crossing Rate\nThe zero crossing rate is the rate of sign-changes along a signal, i.e., the rate at which the signal changes from positive to negative or back. \nLet us calculate the zero crossing rate for our example audio clip."], "code": "# Reference: https://www.kaggle.com/code/parulpandey/eda-and-audio-processing-with-python\n\nplt.figure(figsize=(14, 5))\nlibrosa.display.waveplot(x, sr=sr)\n# Zooming in\nn0 = 9000\nn1 = 9100\nplt.figure(figsize=(14, 5))\nplt.plot(x[n0:n1])\nplt.grid()\nzero_crossings = librosa.zero_crossings(x[n0:n1], pad=False)\nprint(sum(zero_crossings))", "processed": ["2 featur extract everi audio signal consist mani featur howev must extract characterist relev problem tri solv process extract featur use analysi call featur extract let u studi featur detail", "zero cross rate zero cross rate rate sign chang along signal e rate signal chang posit neg back let u calcul zero cross rate exampl audio clip"]}, {"markdown": ["### Spectral Centroid\nIt indicates where the \u201dcentre of mass\u201d for a sound is located and is calculated as the weighted mean of the frequencies present in the sound. Consider two songs, one from a blues genre and the other belonging to metal. \n"], "code": "# Reference: https://www.kaggle.com/code/parulpandey/eda-and-audio-processing-with-python\n\nspectral_centroids = librosa.feature.spectral_centroid(x, sr=sr)[0]\nspectral_centroids.shape\n\n# Computing the time variable for visualization\nframes = range(len(spectral_centroids))\nt = librosa.frames_to_time(frames)\n# Normalising the spectral centroid for visualisation\ndef normalize(x, axis=0):\n    return sklearn.preprocessing.minmax_scale(x, axis=axis)\n#Plotting the Spectral Centroid along the waveform\nlibrosa.display.waveplot(x, sr=sr, alpha=0.4)\nplt.plot(t, normalize(spectral_centroids), color='r')", "processed": ["spectral centroid indic centr mass sound locat calcul weight mean frequenc present sound consid two song one blue genr belong metal"]}, {"markdown": ["### Spectral Rolloff\nIt is a measure of the shape of the signal. It represents the frequency below which a specified percentage of the total spectral energy, e.g. 85%, lies."], "code": "# Reference: https://www.kaggle.com/code/parulpandey/eda-and-audio-processing-with-python\n\nspectral_rolloff = librosa.feature.spectral_rolloff(x+0.01, sr=sr)[0]\nlibrosa.display.waveplot(x, sr=sr, alpha=0.4)\nplt.plot(t, normalize(spectral_rolloff), color='r')", "processed": ["spectral rolloff measur shape signal repres frequenc specifi percentag total spectral energi e g 85 lie"]}, {"markdown": ["### Mel-Frequency Cepstral Coefficients\nThe Mel frequency cepstral coefficients (MFCCs) of a signal are a small set of features (usually about 10\u201320) which concisely describe the overall shape of a spectral envelope. It models the characteristics of the human voice."], "code": "# Reference: https://www.kaggle.com/code/parulpandey/eda-and-audio-processing-with-python\n\nx, fs = librosa.load('../input/birdsong-recognition/train_audio/nutwoo/XC161356.mp3')\nlibrosa.display.waveplot(x, sr=sr)\n\nmfccs = librosa.feature.mfcc(x, sr=fs)\nprint(mfccs.shape)\n\n#Displaying  the MFCCs:\nlibrosa.display.specshow(mfccs, sr=sr, x_axis='time')\n", "processed": ["mel frequenc cepstral coeffici mel frequenc cepstral coeffici mfcc signal small set featur usual 1020 concis describ overal shape spectral envelop model characterist human voic"]}, {"markdown": ["# Anchor Point Mystery\n\nHelp me solve this mystery.\n\nDuring the ion challenge I found something interesting in the data that I still can't explain. Hopefully someone has a good explaination for it.\n\nMy attempt during the competition was to shift the \"drift\" sections of the data to maximize these points. I was successful but it didn't end up providing any fruitful to my model."], "code": "import pandas as pd\nimport numpy as np\nimport matplotlib.pylab as plt\nfrom scipy.optimize import minimize\ntrain = pd.read_csv('../input/data-without-drift/train_clean.csv')\ntest = pd.read_csv('../input/data-without-drift/test_clean.csv')\ntt = pd.concat([train, test], sort=False)\ndef add_model_groups(tt):\n    tt.loc[(tt['time'] > 0) & (tt['time'] <= 10), 'sbatch'] = 0\n    tt.loc[(tt['time'] > 10) & (tt['time'] <= 50), 'sbatch'] = 1\n    tt.loc[(tt['time'] > 50) & (tt['time'] <= 60), 'sbatch'] = 2\n    tt.loc[(tt['time'] > 60) & (tt['time'] <= 100), 'sbatch'] = 3\n    tt.loc[(tt['time'] > 100) & (tt['time'] <= 150), 'sbatch'] = 4\n    tt.loc[(tt['time'] > 150) & (tt['time'] <= 200), 'sbatch'] = 5\n    tt.loc[(tt['time'] > 200) & (tt['time'] <= 250), 'sbatch'] = 6\n    tt.loc[(tt['time'] > 250) & (tt['time'] <= 300), 'sbatch'] = 7\n    tt.loc[(tt['time'] > 300) & (tt['time'] <= 350), 'sbatch'] = 8\n    tt.loc[(tt['time'] > 350) & (tt['time'] <= 400), 'sbatch'] = 9\n    tt.loc[(tt['time'] > 400) & (tt['time'] <= 450), 'sbatch'] = 10\n    tt.loc[(tt['time'] > 450) & (tt['time'] <= 500), 'sbatch'] = 11\n    # Test\n    tt.loc[(tt['time'] > 500) & (tt['time'] <= 510), 'sbatch'] = 12\n    tt.loc[(tt['time'] > 510) & (tt['time'] <= 520), 'sbatch'] = 13\n    tt.loc[(tt['time'] > 520) & (tt['time'] <= 530), 'sbatch'] = 14\n    tt.loc[(tt['time'] > 530) & (tt['time'] <= 540), 'sbatch'] = 15\n    tt.loc[(tt['time'] > 540) & (tt['time'] <= 550), 'sbatch'] = 16\n    tt.loc[(tt['time'] > 550) & (tt['time'] <= 560), 'sbatch'] = 17\n    tt.loc[(tt['time'] > 560) & (tt['time'] <= 570), 'sbatch'] = 18\n    tt.loc[(tt['time'] > 570) & (tt['time'] <= 580), 'sbatch'] = 19\n    tt.loc[(tt['time'] > 580) & (tt['time'] <= 590), 'sbatch'] = 20\n    tt.loc[(tt['time'] > 590) & (tt['time'] <= 600), 'sbatch'] = 21\n    tt.loc[(tt['time'] > 600) & (tt['time'] <= 610), 'sbatch'] = 22\n    tt.loc[(tt['time'] > 610) & (tt['time'] <= 630), 'sbatch'] = 23\n    tt.loc[(tt['time'] > 630) & (tt['time'] <= 650), 'sbatch'] = 24\n    tt.loc[(tt['time'] > 650) & (tt['time'] <= 670), 'sbatch'] = 25\n    tt.loc[(tt['time'] > 670) & (tt['time'] <= 700), 'sbatch'] = 26\n    return tt\n\ndef had_drift(tt):\n    \"\"\"\n    I dentify if section had drift in the original dataset\n    \"\"\"\n    tt.loc[(tt['time'] > 0) & (tt['time'] <= 10), 'drift'] = False\n    tt.loc[(tt['time'] > 10) & (tt['time'] <= 50), 'drift'] = False\n    tt.loc[(tt['time'] > 50) & (tt['time'] <= 60), 'drift'] = True\n    tt.loc[(tt['time'] > 60) & (tt['time'] <= 100), 'drift'] = False\n    tt.loc[(tt['time'] > 100) & (tt['time'] <= 150), 'drift'] = False\n    tt.loc[(tt['time'] > 150) & (tt['time'] <= 200), 'drift'] = False\n    tt.loc[(tt['time'] > 200) & (tt['time'] <= 250), 'drift'] = False\n    tt.loc[(tt['time'] > 250) & (tt['time'] <= 300), 'drift'] = False\n    tt.loc[(tt['time'] > 300) & (tt['time'] <= 350), 'drift'] = True\n    tt.loc[(tt['time'] > 350) & (tt['time'] <= 400), 'drift'] = True\n    tt.loc[(tt['time'] > 400) & (tt['time'] <= 450), 'drift'] = True\n    tt.loc[(tt['time'] > 450) & (tt['time'] <= 500), 'drift'] = True\n    # Test\n    tt.loc[(tt['time'] > 500) & (tt['time'] <= 510), 'drift'] = True\n    tt.loc[(tt['time'] > 510) & (tt['time'] <= 520), 'drift'] = True\n    tt.loc[(tt['time'] > 520) & (tt['time'] <= 530), 'drift'] = False\n    tt.loc[(tt['time'] > 530) & (tt['time'] <= 540), 'drift'] = False\n    tt.loc[(tt['time'] > 540) & (tt['time'] <= 550), 'drift'] = True\n    tt.loc[(tt['time'] > 550) & (tt['time'] <= 560), 'drift'] = False\n    tt.loc[(tt['time'] > 560) & (tt['time'] <= 570), 'drift'] = True\n    tt.loc[(tt['time'] > 570) & (tt['time'] <= 580), 'drift'] = True\n    tt.loc[(tt['time'] > 580) & (tt['time'] <= 590), 'drift'] = True\n    tt.loc[(tt['time'] > 590) & (tt['time'] <= 600), 'drift'] = False\n    tt.loc[(tt['time'] > 600) & (tt['time'] <= 610), 'drift'] = True\n    tt.loc[(tt['time'] > 610) & (tt['time'] <= 630), 'drift'] = True\n    tt.loc[(tt['time'] > 630) & (tt['time'] <= 650), 'drift'] = True\n    tt.loc[(tt['time'] > 650) & (tt['time'] <= 670), 'drift'] = False\n    tt.loc[(tt['time'] > 670) & (tt['time'] <= 700), 'drift'] = False\n    return tt\ntt = had_drift(tt)\ntt = add_model_groups(tt)\nFILTER_TRAIN = '(time <= 47.6 or time > 48) and (time <= 364 or time > 382.4)'\ntt = tt.query(FILTER_TRAIN)\ntt['drift'] = tt['drift'].astype('bool')\nfor i, d in tt.groupby('open_channels'):\n    d.query('not drift')['signal'].value_counts() \\\n        .plot(figsize=(15, 5), style='.', label=i,\n              title='Value Counts by Signal (Excluding Drift Sections)')\nplt.legend()\nplt.show()", "processed": ["anchor point mysteri help solv mysteri ion challeng found someth interest data still explain hope someon good explain attempt competit shift drift section data maxim point success end provid fruit model"]}, {"markdown": ["# Plotting only the anchor points"], "code": "# Reference: https://www.kaggle.com/code/robikscube/ion-challenge-anchor-point-mystery\n\nfig, ax = plt.subplots()\ntt.query('drift == False and signal in @anchors') \\\n    .groupby('open_channels') \\\n    .plot(x='time', y='signal', style='.', figsize=(15, 5), ax=ax)\nplt.show()", "processed": ["plot anchor point"]}, {"markdown": ["# Lets plot the drift sections before and after this shift:"], "code": "# Reference: https://www.kaggle.com/code/robikscube/ion-challenge-anchor-point-mystery\n\nfor i, d in tt.groupby('open_channels'):\n    d.query('drift')['signal'].round(4) \\\n        .value_counts() \\\n        .plot(figsize=(15, 5), style='.', label=i,\n              title='Unique Value Counts in Drift Segments before shifting')\nplt.show()", "processed": ["let plot drift section shift"]}, {"markdown": ["This looks much cleaner, right?"], "code": "# Reference: https://www.kaggle.com/code/robikscube/ion-challenge-anchor-point-mystery\n\nfor i, d in tt.groupby('open_channels'):\n    d.query('drift')['signal_round4'] \\\n        .round(4).value_counts() \\\n        .plot(figsize=(15, 5), style='.', label=i,\n             title='Unique Value Counts in Drift Data after Shifting to Optimize Anchors')\nplt.legend()\nplt.show()", "processed": ["look much cleaner right"]}, {"markdown": ["## All function used in this kernel"], "code": "# Reference: https://www.kaggle.com/code/artgor/using-meta-features-to-improve-model\n\nimport os\nimport time\nimport datetime\nimport json\nimport gc\nfrom numba import jit\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm_notebook\n\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom catboost import CatBoostRegressor, CatBoostClassifier\nfrom sklearn import metrics\n\nfrom itertools import product\n\nimport altair as alt\nfrom altair.vega import v5\nfrom IPython.display import HTML\n\n# using ideas from this kernel: https://www.kaggle.com/notslush/altair-visualization-2018-stackoverflow-survey\ndef prepare_altair():\n    \"\"\"\n    Helper function to prepare altair for working.\n    \"\"\"\n\n    vega_url = 'https://cdn.jsdelivr.net/npm/vega@' + v5.SCHEMA_VERSION\n    vega_lib_url = 'https://cdn.jsdelivr.net/npm/vega-lib'\n    vega_lite_url = 'https://cdn.jsdelivr.net/npm/vega-lite@' + alt.SCHEMA_VERSION\n    vega_embed_url = 'https://cdn.jsdelivr.net/npm/vega-embed@3'\n    noext = \"?noext\"\n    \n    paths = {\n        'vega': vega_url + noext,\n        'vega-lib': vega_lib_url + noext,\n        'vega-lite': vega_lite_url + noext,\n        'vega-embed': vega_embed_url + noext\n    }\n    \n    workaround = f\"\"\"    requirejs.config({{\n        baseUrl: 'https://cdn.jsdelivr.net/npm/',\n        paths: {paths}\n    }});\n    \"\"\"\n    \n    return workaround\n    \n\ndef add_autoincrement(render_func):\n    # Keep track of unique <div/> IDs\n    cache = {}\n    def wrapped(chart, id=\"vega-chart\", autoincrement=True):\n        if autoincrement:\n            if id in cache:\n                counter = 1 + cache[id]\n                cache[id] = counter\n            else:\n                cache[id] = 0\n            actual_id = id if cache[id] == 0 else id + '-' + str(cache[id])\n        else:\n            if id not in cache:\n                cache[id] = 0\n            actual_id = id\n        return render_func(chart, id=actual_id)\n    # Cache will stay outside and \n    return wrapped\n           \n\n@add_autoincrement\ndef render(chart, id=\"vega-chart\"):\n    \"\"\"\n    Helper function to plot altair visualizations.\n    \"\"\"\n    chart_str = \"\"\"\n    <div id=\"{id}\"></div><script>\n    require([\"vega-embed\"], function(vg_embed) {{\n        const spec = {chart};     \n        vg_embed(\"#{id}\", spec, {{defaultStyle: true}}).catch(console.warn);\n        console.log(\"anything?\");\n    }});\n    console.log(\"really...anything?\");\n    </script>\n    \"\"\"\n    return HTML(\n        chart_str.format(\n            id=id,\n            chart=json.dumps(chart) if isinstance(chart, dict) else chart.to_json(indent=None)\n        )\n    )\n    \n\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                c_prec = df[col].apply(lambda x: np.finfo(x).precision).max()\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max and c_prec == np.finfo(np.float16).precision:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max and c_prec == np.finfo(np.float32).precision:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df\n    \n\n@jit\ndef fast_auc(y_true, y_prob):\n    \"\"\"\n    fast roc_auc computation: https://www.kaggle.com/c/microsoft-malware-prediction/discussion/76013\n    \"\"\"\n    y_true = np.asarray(y_true)\n    y_true = y_true[np.argsort(y_prob)]\n    nfalse = 0\n    auc = 0\n    n = len(y_true)\n    for i in range(n):\n        y_i = y_true[i]\n        nfalse += (1 - y_i)\n        auc += y_i * nfalse\n    auc /= (nfalse * (n - nfalse))\n    return auc\n\n\ndef eval_auc(y_true, y_pred):\n    \"\"\"\n    Fast auc eval function for lgb.\n    \"\"\"\n    return 'auc', fast_auc(y_true, y_pred), True\n\n\ndef group_mean_log_mae(y_true, y_pred, types, floor=1e-9):\n    \"\"\"\n    Fast metric computation for this competition: https://www.kaggle.com/c/champs-scalar-coupling\n    Code is from this kernel: https://www.kaggle.com/uberkinder/efficient-metric\n    \"\"\"\n    maes = (y_true-y_pred).abs().groupby(types).mean()\n    return np.log(maes.map(lambda x: max(x, floor))).mean()\n    \n\ndef train_model_regression(X, X_test, y, params, folds, model_type='lgb', eval_metric='mae', columns=None, plot_feature_importance=False, model=None,\n                               verbose=10000, early_stopping_rounds=200, n_estimators=50000):\n    \"\"\"\n    A function to train a variety of regression models.\n    Returns dictionary with oof predictions, test predictions, scores and, if necessary, feature importances.\n    \n    :params: X - training data, can be pd.DataFrame or np.ndarray (after normalizing)\n    :params: X_test - test data, can be pd.DataFrame or np.ndarray (after normalizing)\n    :params: y - target\n    :params: folds - folds to split data\n    :params: model_type - type of model to use\n    :params: eval_metric - metric to use\n    :params: columns - columns to use. If None - use all columns\n    :params: plot_feature_importance - whether to plot feature importance of LGB\n    :params: model - sklearn model, works only for \"sklearn\" model type\n    \n    \"\"\"\n    columns = X.columns if columns is None else columns\n    X_test = X_test[columns]\n    \n    # to set up scoring parameters\n    metrics_dict = {'mae': {'lgb_metric_name': 'mae',\n                        'catboost_metric_name': 'MAE',\n                        'sklearn_scoring_function': metrics.mean_absolute_error},\n                    'group_mae': {'lgb_metric_name': 'mae',\n                        'catboost_metric_name': 'MAE',\n                        'scoring_function': group_mean_log_mae},\n                    'mse': {'lgb_metric_name': 'mse',\n                        'catboost_metric_name': 'MSE',\n                        'sklearn_scoring_function': metrics.mean_squared_error}\n                    }\n\n    \n    result_dict = {}\n    \n    # out-of-fold predictions on train data\n    oof = np.zeros(len(X))\n    \n    # averaged predictions on train data\n    prediction = np.zeros(len(X_test))\n    \n    # list of scores on folds\n    scores = []\n    feature_importance = pd.DataFrame()\n    \n    # split and train on folds\n    for fold_n, (train_index, valid_index) in enumerate(folds.split(X)):\n        print(f'Fold {fold_n + 1} started at {time.ctime()}')\n        if type(X) == np.ndarray:\n            X_train, X_valid = X[columns][train_index], X[columns][valid_index]\n            y_train, y_valid = y[train_index], y[valid_index]\n        else:\n            X_train, X_valid = X[columns].iloc[train_index], X[columns].iloc[valid_index]\n            y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n            \n        if model_type == 'lgb':\n            model = lgb.LGBMRegressor(**params, n_estimators = n_estimators, n_jobs = -1)\n            model.fit(X_train, y_train, \n                    eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric=metrics_dict[eval_metric]['lgb_metric_name'],\n                    verbose=verbose, early_stopping_rounds=early_stopping_rounds)\n            \n            y_pred_valid = model.predict(X_valid)\n            y_pred = model.predict(X_test, num_iteration=model.best_iteration_)\n            \n        if model_type == 'xgb':\n            train_data = xgb.DMatrix(data=X_train, label=y_train, feature_names=X.columns)\n            valid_data = xgb.DMatrix(data=X_valid, label=y_valid, feature_names=X.columns)\n\n            watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]\n            model = xgb.train(dtrain=train_data, num_boost_round=20000, evals=watchlist, early_stopping_rounds=200, verbose_eval=verbose, params=params)\n            y_pred_valid = model.predict(xgb.DMatrix(X_valid, feature_names=X.columns), ntree_limit=model.best_ntree_limit)\n            y_pred = model.predict(xgb.DMatrix(X_test, feature_names=X.columns), ntree_limit=model.best_ntree_limit)\n        \n        if model_type == 'sklearn':\n            model = model\n            model.fit(X_train, y_train)\n            \n            y_pred_valid = model.predict(X_valid).reshape(-1,)\n            score = metrics_dict[eval_metric]['sklearn_scoring_function'](y_valid, y_pred_valid)\n            print(f'Fold {fold_n}. {eval_metric}: {score:.4f}.')\n            print('')\n            \n            y_pred = model.predict(X_test).reshape(-1,)\n        \n        if model_type == 'cat':\n            model = CatBoostRegressor(iterations=20000,  eval_metric=metrics_dict[eval_metric]['catboost_metric_name'], **params,\n                                      loss_function=metrics_dict[eval_metric]['catboost_metric_name'])\n            model.fit(X_train, y_train, eval_set=(X_valid, y_valid), cat_features=[], use_best_model=True, verbose=False)\n\n            y_pred_valid = model.predict(X_valid)\n            y_pred = model.predict(X_test)\n        \n        oof[valid_index] = y_pred_valid.reshape(-1,)\n        if eval_metric != 'group_mae':\n            scores.append(metrics_dict[eval_metric]['sklearn_scoring_function'](y_valid, y_pred_valid))\n        else:\n            scores.append(metrics_dict[eval_metric]['scoring_function'](y_valid, y_pred_valid, X_valid['type']))\n\n        prediction += y_pred    \n        \n        if model_type == 'lgb' and plot_feature_importance:\n            # feature importance\n            fold_importance = pd.DataFrame()\n            fold_importance[\"feature\"] = columns\n            fold_importance[\"importance\"] = model.feature_importances_\n            fold_importance[\"fold\"] = fold_n + 1\n            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n\n    prediction /= folds.n_splits\n    \n    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n    \n    result_dict['oof'] = oof\n    result_dict['prediction'] = prediction\n    result_dict['scores'] = scores\n    \n    if model_type == 'lgb':\n        if plot_feature_importance:\n            feature_importance[\"importance\"] /= folds.n_splits\n            cols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n                by=\"importance\", ascending=False)[:50].index\n\n            best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n\n            plt.figure(figsize=(16, 12));\n            sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False));\n            plt.title('LGB Features (avg over folds)');\n            \n            result_dict['feature_importance'] = feature_importance\n        \n    return result_dict\n    \n\n\ndef train_model_classification(X, X_test, y, params, folds, model_type='lgb', eval_metric='auc', columns=None, plot_feature_importance=False, model=None,\n                               verbose=10000, early_stopping_rounds=200, n_estimators=50000):\n    \"\"\"\n    A function to train a variety of regression models.\n    Returns dictionary with oof predictions, test predictions, scores and, if necessary, feature importances.\n    \n    :params: X - training data, can be pd.DataFrame or np.ndarray (after normalizing)\n    :params: X_test - test data, can be pd.DataFrame or np.ndarray (after normalizing)\n    :params: y - target\n    :params: folds - folds to split data\n    :params: model_type - type of model to use\n    :params: eval_metric - metric to use\n    :params: columns - columns to use. If None - use all columns\n    :params: plot_feature_importance - whether to plot feature importance of LGB\n    :params: model - sklearn model, works only for \"sklearn\" model type\n    \n    \"\"\"\n    columns = X.columns if columns == None else columns\n    X_test = X_test[columns]\n    \n    # to set up scoring parameters\n    metrics_dict = {'auc': {'lgb_metric_name': eval_auc,\n                        'catboost_metric_name': 'AUC',\n                        'sklearn_scoring_function': metrics.roc_auc_score},\n                    }\n    \n    result_dict = {}\n    \n    # out-of-fold predictions on train data\n    oof = np.zeros((len(X), len(set(y.values))))\n    \n    # averaged predictions on train data\n    prediction = np.zeros((len(X_test), oof.shape[1]))\n    \n    # list of scores on folds\n    scores = []\n    feature_importance = pd.DataFrame()\n    \n    # split and train on folds\n    for fold_n, (train_index, valid_index) in enumerate(folds.split(X)):\n        print(f'Fold {fold_n + 1} started at {time.ctime()}')\n        if type(X) == np.ndarray:\n            X_train, X_valid = X[columns][train_index], X[columns][valid_index]\n            y_train, y_valid = y[train_index], y[valid_index]\n        else:\n            X_train, X_valid = X[columns].iloc[train_index], X[columns].iloc[valid_index]\n            y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n            \n        if model_type == 'lgb':\n            model = lgb.LGBMClassifier(**params, n_estimators=n_estimators, n_jobs = -1)\n            model.fit(X_train, y_train, \n                    eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric=metrics_dict[eval_metric]['lgb_metric_name'],\n                    verbose=verbose, early_stopping_rounds=early_stopping_rounds)\n            \n            y_pred_valid = model.predict_proba(X_valid)\n            y_pred = model.predict_proba(X_test, num_iteration=model.best_iteration_)\n            \n        if model_type == 'xgb':\n            train_data = xgb.DMatrix(data=X_train, label=y_train, feature_names=X.columns)\n            valid_data = xgb.DMatrix(data=X_valid, label=y_valid, feature_names=X.columns)\n\n            watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]\n            model = xgb.train(dtrain=train_data, num_boost_round=n_estimators, evals=watchlist, early_stopping_rounds=early_stopping_rounds, verbose_eval=verbose, params=params)\n            y_pred_valid = model.predict(xgb.DMatrix(X_valid, feature_names=X.columns), ntree_limit=model.best_ntree_limit)\n            y_pred = model.predict(xgb.DMatrix(X_test, feature_names=X.columns), ntree_limit=model.best_ntree_limit)\n        \n        if model_type == 'sklearn':\n            model = model\n            model.fit(X_train, y_train)\n            \n            y_pred_valid = model.predict(X_valid).reshape(-1,)\n            score = metrics_dict[eval_metric]['sklearn_scoring_function'](y_valid, y_pred_valid)\n            print(f'Fold {fold_n}. {eval_metric}: {score:.4f}.')\n            print('')\n            \n            y_pred = model.predict_proba(X_test)\n        \n        if model_type == 'cat':\n            model = CatBoostClassifier(iterations=n_estimators, eval_metric=metrics_dict[eval_metric]['catboost_metric_name'], **params,\n                                      loss_function=metrics_dict[eval_metric]['catboost_metric_name'])\n            model.fit(X_train, y_train, eval_set=(X_valid, y_valid), cat_features=[], use_best_model=True, verbose=False)\n\n            y_pred_valid = model.predict(X_valid)\n            y_pred = model.predict(X_test)\n        \n        oof[valid_index] = y_pred_valid\n        scores.append(metrics_dict[eval_metric]['sklearn_scoring_function'](y_valid, y_pred_valid[:, 1]))\n\n        prediction += y_pred    \n        \n        if model_type == 'lgb' and plot_feature_importance:\n            # feature importance\n            fold_importance = pd.DataFrame()\n            fold_importance[\"feature\"] = columns\n            fold_importance[\"importance\"] = model.feature_importances_\n            fold_importance[\"fold\"] = fold_n + 1\n            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n\n    prediction /= folds.n_splits\n    \n    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n    \n    result_dict['oof'] = oof\n    result_dict['prediction'] = prediction\n    result_dict['scores'] = scores\n    \n    if model_type == 'lgb':\n        if plot_feature_importance:\n            feature_importance[\"importance\"] /= folds.n_splits\n            cols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n                by=\"importance\", ascending=False)[:50].index\n\n            best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n\n            plt.figure(figsize=(16, 12));\n            sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False));\n            plt.title('LGB Features (avg over folds)');\n            \n            result_dict['feature_importance'] = feature_importance\n        \n    return result_dict\n\n# setting up altair\nworkaround = prepare_altair()\nHTML(\"\".join((\n    \"<script>\",\n    workaround,\n    \"</script>\",\n)))", "processed": ["function use kernel"]}, {"markdown": ["Here we have a direct relationship between this file and train - each line in train has line in this file. We have 4 additional variables here, let's see what is written in competition description for this file:\n```\nThe scalar coupling constants in train.csv are a sum of four terms. scalar_coupling_contributions.csv contain all these terms.\n* fc is the Fermi Contact contribution\n* sd is the Spin-dipolar contribution\n* pso is the Paramagnetic spin-orbit contribution\n* dso is the Diamagnetic spin-orbit contribution. \n```\n\nIt seems that these 4 features should be highly correlated with the target variable, so using them should improve our models!\nLet's merge these file and have a look at them!"], "code": "# Reference: https://www.kaggle.com/code/artgor/using-meta-features-to-improve-model\n\ntrain = pd.merge(train, scalar_coupling_contributions, how = 'left',\n                  left_on  = ['molecule_name', 'atom_index_0', 'atom_index_1', 'type'],\n                  right_on = ['molecule_name', 'atom_index_0', 'atom_index_1', 'type'])\nfig, ax = plt.subplots(figsize = (20, 10))\nfor i, t in enumerate(train['type'].unique()):\n    plt.subplot(2, 4, i + 1);\n    plt.scatter(train.loc[train['type'] == t, 'fc'], train.loc[train['type'] == t, 'scalar_coupling_constant'], label=t);\n    plt.title(f'fc vs target \\n for {t} type');", "processed": ["direct relationship file train line train line file 4 addit variabl let see written competit descript file scalar coupl constant train csv sum four term scalar coupl contribut csv contain term fc fermi contact contribut sd spin dipolar contribut pso paramagnet spin orbit contribut dso diamagnet spin orbit contribut seem 4 featur highli correl target variabl use improv model let merg file look"]}, {"markdown": ["## On importance of the generated features\n\nAfter the training below you'll see that some of the most important variables are these: `molecule_atom_index_0_dist_max`, `molecule_atom_index_0_dist_mean`, `molecule_atom_index_0_dist_std`. Let's plot them!"], "code": "# Reference: https://www.kaggle.com/code/artgor/using-meta-features-to-improve-model\n\nfig, ax = plt.subplots(figsize = (20, 10))\nfor i, t in enumerate(train['type'].unique()):\n    plt.subplot(2, 4, i + 1);\n    plt.scatter(train.loc[train['type'] == t, 'dist'], train.loc[train['type'] == t, 'scalar_coupling_constant'], label=t);\n    plt.title(f'dist vs target \\n for {t} type');\n    plt.xlim(train['dist'].min(), train['dist'].max());\nfig, ax = plt.subplots(figsize = (20, 10))\nfor i, t in enumerate(train['type'].unique()):\n    plt.subplot(2, 4, i + 1);\n    plt.scatter(train.loc[train['type'] == t, 'molecule_atom_index_0_dist_max'], train.loc[train['type'] == t, 'scalar_coupling_constant'], label=t);\n    plt.title(f'molecule_atom_index_0_dist_max vs target \\n for {t} type');\n    plt.xlim(train['molecule_atom_index_0_dist_max'].min(), train['molecule_atom_index_0_dist_max'].max());\nfig, ax = plt.subplots(figsize = (20, 10))\nfor i, t in enumerate(train['type'].unique()):\n    plt.subplot(2, 4, i + 1);\n    plt.scatter(train.loc[train['type'] == t, 'molecule_atom_index_0_dist_mean'], train.loc[train['type'] == t, 'scalar_coupling_constant'], label=t);\n    plt.title(f'molecule_atom_index_0_dist_mean vs target \\n for {t} type');\n    plt.xlim(train['molecule_atom_index_0_dist_mean'].min(), train['molecule_atom_index_0_dist_mean'].max());\nfig, ax = plt.subplots(figsize = (20, 10))\nfor i, t in enumerate(train['type'].unique()):\n    plt.subplot(2, 4, i + 1);\n    plt.scatter(train.loc[train['type'] == t, 'molecule_atom_index_0_dist_std'], train.loc[train['type'] == t, 'scalar_coupling_constant'], label=t);\n    plt.title(f'molecule_atom_index_0_dist_std vs target \\n for {t} type');\n    plt.xlim(train['molecule_atom_index_0_dist_std'].min(), train['molecule_atom_index_0_dist_std'].max());\ndel train, test\ngc.collect()", "processed": ["import gener featur train see import variabl molecul atom index 0 dist max molecul atom index 0 dist mean molecul atom index 0 dist std let plot"]}, {"markdown": ["The Seed value consists of a 3 character Identifier. The first character denotes the region and the last two denote the seed in that region. Let's merge the Team's name from the Wteams file."], "code": "# Reference: https://www.kaggle.com/code/parulpandey/decoding-march-madness\n\nWseeds = pd.merge(Wseeds, Wteams,on='TeamID')\nWseeds.head()\n\n# Separating the regions from the Seeds\n\nWseeds['Region'] = Wseeds['Seed'].apply(lambda x: x[0][:1])\nWseeds['Seed'] = Wseeds['Seed'].apply(lambda x: int(x[1:3]))\nprint(Wseeds.head())\nprint(Wseeds.shape)\n# Teams with maximum top seeds\nfig = plt.gcf()\nfig.set_size_inches(10, 6)\ncolors = ['dodgerblue', 'plum', '#F0A30A','#8c564b','orange','green','yellow'] \n\nWseeds[Wseeds['Seed'] ==1]['TeamName'].value_counts()[:10].plot(kind='bar',color=colors,linewidth=2,edgecolor='black')\nplt.xlabel('Number of times in Top seeded positions')", "processed": ["seed valu consist 3 charact identifi first charact denot region last two denot seed region let merg team name wteam file"]}, {"markdown": ["Connecticut/UConn has been the top seeded team for the maximum no of times"], "code": "# Reference: https://www.kaggle.com/code/parulpandey/decoding-march-madness\n\n# Teams with maximum lowest seeds\nfig = plt.gcf()\nfig.set_size_inches(10, 6)\n\nWseeds[Wseeds['Seed'] ==16]['TeamName'].value_counts()[:10].plot(kind='bar',color=colors,edgecolor='black',linewidth=1)\nplt.xlabel('Number of times in bottom seeded positions')", "processed": ["connecticut uconn top seed team maximum time"]}, {"markdown": ["where \n* WScore -  the numberof points scored by the winning team.\n* WTeamID - the id number of the team that won the game\n* LTeamID - the id number of the team that lost the game.\n* LScore - the number of points scored by the losing team. \n"], "code": "# Reference: https://www.kaggle.com/code/parulpandey/decoding-march-madness\n\n# Winning and Losing score Average over the years\nx = rg_season_compact_results.groupby('Season')[['WScore','LScore']].mean()\n\nfig = plt.gcf()\nfig.set_size_inches(14, 6)\nplt.plot(x.index,x['WScore'],marker='o', markerfacecolor='green', markersize=12, color='green', linewidth=4)\nplt.plot(x.index,x['LScore'],marker=7, markerfacecolor='red', markersize=12, color='red', linewidth=4)\nplt.legend()\n", "processed": ["wscore numberof point score win team wteamid id number team game lteamid id number team lost game lscore number point score lose team"]}, {"markdown": ["### Is there a home team advantage?"], "code": "# Reference: https://www.kaggle.com/code/parulpandey/decoding-march-madness\n\nax = sns.countplot(x=tourney_compact_results['WLoc'])\nax.set_title(\"Win Locations\")\nax.set_xlabel(\"Location\")\nax.set_ylabel(\"Frequency\");", "processed": ["home team advantag"]}, {"markdown": ["Again let's checkput if there is a home team advantage in the tournaments?"], "code": "# Reference: https://www.kaggle.com/code/parulpandey/decoding-march-madness\n\nax = sns.countplot(x=tourney_detailed_results['WLoc'])\nax.set_title(\"Win Locations\")\nax.set_xlabel(\"Location\")\nax.set_ylabel(\"Frequency\");\n\n\ngames_stats = []\n\nfor row in tourney_detailed_results.to_dict('records'):\n    game = {}\n    game['Season'] =  row['Season']\n    game['DayNum'] = row['DayNum']\n    game['TeamID'] = row['WTeamID']\n    game['OpponentID'] = row['LTeamID']\n    game['FGM'] = row['WFGM']\n    game['Loc'] = row['WLoc']\n    game['Won'] = 1\n    game['Score'] = row['WScore']\n    game['FGA'] = row['WFGA']\n    game['FGM3'] = row['WFGM3']\n    game['FGA3'] = row['WFGA3']\n    game['FTM'] = row['WFTM']\n    game['FTA'] = row['WFTA']\n    game['OR'] = row['WOR']\n    game['DR'] = row['WDR']\n    game['AST'] = row['WAst']\n    game['TO'] = row['WTO']\n    game['STL'] = row['WStl']\n    game['BLK'] = row['WBlk']\n    game['PF'] = row['WPF']\n    games_stats.append(game)\n    game = {}\n    game['Season'] = row['Season']\n    game['DayNum'] = row['DayNum']\n    game['TeamID'] = row['LTeamID']\n    game['OpponentID'] = row['WTeamID']\n    game['FGM'] = row['LFGM']\n    game['Loc'] = row['WLoc']\n    game['Won']= 0\n    game['Score'] = row['LScore']\n    game['FGA'] = row['LFGA']\n    game['FGM3'] = row['LFGM3']\n    game['FGA3'] = row['LFGA3']\n    game['FTM'] = row['LFTM']\n    game['FTA'] = row['LFTA']\n    game['OR'] = row['LOR']\n    game['DR'] = row['LDR']\n    game['AST'] = row['LAst']\n    game['TO'] = row['LTO']\n    game['STL'] = row['LStl']\n    game['BLK'] = row['LBlk']\n    game['PF'] = row['LPF']\n    games_stats.append(game)\n\n    \n\n\n# Separating winners and losers using Won Column which is set to 1 for winner and 0 for loser\n\ntournament = pd.DataFrame(games_stats)\ntournament.head()", "processed": ["let checkput home team advantag tournament"]}, {"markdown": ["## Analysis of the Winning Teams"], "code": "# Reference: https://www.kaggle.com/code/parulpandey/decoding-march-madness\n\nwinning_Teams.head().T\n# Most successful teams\nfig = plt.gcf()\nfig.set_size_inches(10, 6)\n\ncolors = ['dodgerblue', 'plum', '#F0A30A','#8c564b','orange','green','yellow'] \nwinning_Teams['Team'].value_counts()[:10].plot(kind='bar',color=colors,edgecolor='black',linewidth=1 )\n\nplt.title('Most successful Teams')\nplt.tight_layout(h_pad=2)\n# How seed ranking affect game stats for winners\nsns.pairplot(winning_Teams[['FGA3','FGM3','AST','BLK','DR','FTA','FTM','OR','Team_Seed',]], hue='Team_Seed',kind=\"scatter\",plot_kws=dict(s=80, edgecolor=\"white\", linewidth=2.5))\n# Features Correlated with Wins\nf,ax = plt.subplots(figsize=(20,15))\ncorr = tournament_df2.corr()\nsns.heatmap(corr, cmap='inferno', annot=True)\n", "processed": ["analysi win team"]}, {"markdown": ["We can check the number of occurrence of each of the author to see if the classes are balanced. "], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-feature-engg-notebook-spooky-author\n\ncnt_srs = train_df['author'].value_counts()\n\nplt.figure(figsize=(8,4))\nsns.barplot(cnt_srs.index, cnt_srs.values, alpha=0.8)\nplt.ylabel('Number of Occurrences', fontsize=12)\nplt.xlabel('Author Name', fontsize=12)\nplt.show()", "processed": ["check number occurr author see class balanc"]}, {"markdown": ["Let us now plot some of our new variables to see of they will be helpful in predictions."], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-feature-engg-notebook-spooky-author\n\ntrain_df['num_words'].loc[train_df['num_words']>80] = 80 #truncation for better visuals\nplt.figure(figsize=(12,8))\nsns.violinplot(x='author', y='num_words', data=train_df)\nplt.xlabel('Author Name', fontsize=12)\nplt.ylabel('Number of words in text', fontsize=12)\nplt.title(\"Number of words by author\", fontsize=15)\nplt.show()", "processed": ["let u plot new variabl see help predict"]}, {"markdown": ["EAP seems slightly lesser number of words than MWS and HPL. "], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-feature-engg-notebook-spooky-author\n\ntrain_df['num_punctuations'].loc[train_df['num_punctuations']>10] = 10 #truncation for better visuals\nplt.figure(figsize=(12,8))\nsns.violinplot(x='author', y='num_punctuations', data=train_df)\nplt.xlabel('Author Name', fontsize=12)\nplt.ylabel('Number of puntuations in text', fontsize=12)\nplt.title(\"Number of punctuations by author\", fontsize=15)\nplt.show()", "processed": ["eap seem slightli lesser number word mw hpl"]}, {"markdown": ["# <a id='4'>New features exploration</a> \n\n\n## Aggregated features\n\nLet's visualize the new features distributions. The graphs below shows the distplot (histograms and density plots) for all the new features, for train (<font color=\"green\">green</font>) and test (<font color=\"blue\">blue</font>) data."], "code": "# Reference: https://www.kaggle.com/code/gpreda/lanl-earthquake-new-approach-eda\n\ndef plot_distplot(feature):\n    plt.figure(figsize=(16,6))\n    plt.title(\"Distribution of {} values in the train and test set\".format(feature))\n    sns.distplot(train_X[feature],color=\"green\", kde=True,bins=120, label='train')\n    sns.distplot(test_X[feature],color=\"blue\", kde=True,bins=120, label='test')\n    plt.legend()\n    plt.show()\ndef plot_distplot_features(features, nlines=4, colors=['green', 'blue'], df1=train_X, df2=test_X):\n    i = 0\n    plt.figure()\n    fig, ax = plt.subplots(nlines,2,figsize=(16,4*nlines))\n    for feature in features:\n        i += 1\n        plt.subplot(nlines,2,i)\n        sns.distplot(df1[feature],color=colors[0], kde=True,bins=40, label='train')\n        sns.distplot(df2[feature],color=colors[1], kde=True,bins=40, label='test')\n    plt.show()\nfeatures = ['mean', 'std', 'max', 'min', 'sum', 'mad', 'kurt', 'skew']\nplot_distplot_features(features)\nfeatures = ['med','abs_mean', 'q95', 'q99', 'q05', 'q01']\nplot_distplot_features(features,3)\nfeatures = ['Rmean', 'Rstd', 'Rmax','Rmin', 'Imean', 'Istd', 'Imax', 'Imin']\nplot_distplot_features(features)\nfeatures = ['std_first_50000', 'std_last_50000', 'std_first_25000','std_last_25000', 'std_first_10000','std_last_10000']\nplot_distplot_features(features,3)", "processed": ["id 4 new featur explor aggreg featur let visual new featur distribut graph show distplot histogram densiti plot new featur train font color green green font test font color blue blue font data"]}, {"markdown": ["## Aggregated features and time to failure\n\nLet's also show aggregated features and time to failure on the same graph. "], "code": "# Reference: https://www.kaggle.com/code/gpreda/lanl-earthquake-new-approach-eda\n\ndef plot_acc_agg_ttf_data(feature, title=\"Averaged accoustic data and ttf\"):\n    fig, ax1 = plt.subplots(figsize=(16, 8))\n    plt.title('Averaged accoustic data ({}) and time to failure'.format(feature))\n    plt.plot(train_X[feature], color='r')\n    ax1.set_xlabel('training samples')\n    ax1.set_ylabel('acoustic data ({})'.format(feature), color='r')\n    plt.legend(['acoustic data ({})'.format(feature)], loc=(0.01, 0.95))\n    ax2 = ax1.twinx()\n    plt.plot(train_y, color='b')\n    ax2.set_ylabel('time to failure', color='b')\n    plt.legend(['time to failure'], loc=(0.01, 0.9))\n    plt.grid(True)\nplot_acc_agg_ttf_data('mean')\nplot_acc_agg_ttf_data('std')\nplot_acc_agg_ttf_data('max')\nplot_acc_agg_ttf_data('min')\nplot_acc_agg_ttf_data('sum')\nplot_acc_agg_ttf_data('mad')\nplot_acc_agg_ttf_data('kurt')\nplot_acc_agg_ttf_data('skew')\nplot_acc_agg_ttf_data('std_first_50000')\nplot_acc_agg_ttf_data('std_last_50000')\nplot_acc_agg_ttf_data('std_first_25000')\nplot_acc_agg_ttf_data('std_last_25000')\nplot_acc_agg_ttf_data('std_first_10000')\nplot_acc_agg_ttf_data('std_last_10000')", "processed": ["aggreg featur time failur let also show aggreg featur time failur graph"]}, {"markdown": ["Fundamental_5 has the most number of missing values followed by fundamental_38.\n\n**Distribution plot:**\n\nNow let us look at the distribution plot of some of the numeric variables. \n\nUnivariate analysis from [this notebook][1] reveals some important variables. So let us look at the plots of  top 4 variables.\n\n- technical_30\n- technical_20\n- fundamental_11\n- technical_19\n\n  [1]: https://www.kaggle.com/sudalairajkumar/two-sigma-financial-modeling/univariate-analysis-regression-lb-0-006"], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-notebook-5\n\ncols_to_use = ['technical_30', 'technical_20', 'fundamental_11', 'technical_19']\nfig = plt.figure(figsize=(8, 20))\nplot_count = 0\nfor col in cols_to_use:\n    plot_count += 1\n    plt.subplot(4, 1, plot_count)\n    plt.scatter(range(df.shape[0]), df[col].values)\n    plt.title(\"Distribution of \"+col)\nplt.show()", "processed": ["fundament 5 number miss valu follow fundament 38 distribut plot let u look distribut plot numer variabl univari analysi notebook 1 reveal import variabl let u look plot top 4 variabl technic 30 technic 20 fundament 11 technic 19 1 http www kaggl com sudalairajkumar two sigma financi model univari analysi regress lb 0 006"]}, {"markdown": ["Target values range between -0.086 to 0.093. \n\nAs we can see the target graph is more darker at the middle, suggesting more values are concentrated in those region. \n\nAlso there seems to be some hard stop at both the ends (probably capping the target to remain within the limits?!) - this could be inferred from the two dark lines at the top and bottom.\n\nAlso there seems to be some change in the target distribution with respect to time. As we move from left to right, initially the target is evenly distributed in the given range (-0.08 to 0.09) and then in the middle it is not so.\n\n**Timestamp:**\n\nNow let us look at the counts for each of the timestamps present in the data."], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-notebook-5\n\nfig = plt.figure(figsize=(12, 6))\nsns.countplot(x='timestamp', data=df)\nplt.show()", "processed": ["target valu rang 0 086 0 093 see target graph darker middl suggest valu concentr region also seem hard stop end probabl cap target remain within limit could infer two dark line top bottom also seem chang target distribut respect time move left right initi target evenli distribut given rang 0 08 0 09 middl timestamp let u look count timestamp present data"]}, {"markdown": ["So we have 1424 unique assets in the dataset. As we can see from the previous plot of timestamp, ~1100 assets is the maximum number of assets at any given timestamp. So there are few assets that are dropped in between.\n\nNow we can check the 'y' distribution of some of the assets. Let us first look at ids with high negative mean target values. "], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-notebook-5\n\ntemp_df = df.groupby('id')['y'].agg('mean').reset_index().sort_values(by='y')\ntemp_df.head()\nid_to_use = [1431, 93, 882, 1637, 1118]\nfig = plt.figure(figsize=(8, 25))\nplot_count = 0\nfor id_val in id_to_use:\n    plot_count += 1\n    plt.subplot(5, 1, plot_count)\n    temp_df = df.ix[df['id']==id_val,:]\n    plt.plot(temp_df.timestamp.values, temp_df.y.values)\n    plt.plot(temp_df.timestamp.values, temp_df.y.cumsum())\n    plt.title(\"Asset ID : \"+str(id_val))\n    \nplt.show()", "processed": ["1424 uniqu asset dataset see previou plot timestamp 1100 asset maximum number asset given timestamp asset drop check distribut asset let u first look id high neg mean target valu"]}, {"markdown": ["Blue line represents the distribution of 'y' variable in the given time stamp. Green line represents the cumulative 'y' value\n\nSo 4 out these 5 assets are dropped (as they are not present till the last time stamp which is 1812), when the cumulative negative target value falls steeply. \n\nNow let us take the assets with high positive mean target value and see their distribution."], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-notebook-5\n\ntemp_df = df.groupby('id')['y'].agg('mean').reset_index().sort_values(by='y')\ntemp_df.tail()\nid_to_use = [767, 226, 824, 1809, 1089]\nfig = plt.figure(figsize=(8, 25))\nplot_count = 0\nfor id_val in id_to_use:\n    plot_count += 1\n    plt.subplot(5, 1, plot_count)\n    temp_df = df.ix[df['id']==id_val,:]\n    plt.plot(temp_df.timestamp.values, temp_df.y.values)\n    plt.plot(temp_df.timestamp.values, temp_df.y.cumsum())\n    plt.title(\"Asset ID : \"+str(id_val))\nplt.show()", "processed": ["blue line repres distribut variabl given time stamp green line repres cumul valu 4 5 asset drop present till last time stamp 1812 cumul neg target valu fall steepli let u take asset high posit mean target valu see distribut"]}, {"markdown": ["Interestingly 2 of these 5 good performing assets are also dropped (Assets 824 and 1089). Not sure about the reasons though.\n\nNow let us take some assets which are present across all the timestamps and see their distribution."], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-notebook-5\n\ntemp_df = df.groupby('id')['y'].agg('count').reset_index().sort_values(by='y')\ntemp_df.tail()\nid_to_use = [1548, 699, 697, 704, 1066]\nfig = plt.figure(figsize=(8, 25))\nplot_count = 0\nfor id_val in id_to_use:\n    plot_count += 1\n    plt.subplot(5, 1, plot_count)\n    temp_df = df.ix[df['id']==id_val,:]\n    plt.plot(temp_df.timestamp.values, temp_df.y.values)\n    plt.plot(temp_df.timestamp.values, temp_df.y.cumsum())\n    plt.title(\"Asset ID : \"+str(id_val))\nplt.show()", "processed": ["interestingli 2 5 good perform asset also drop asset 824 1089 sure reason though let u take asset present across timestamp see distribut"]}, {"markdown": ["Seems like a pretty nice normal-looking distribution, except for the few anomalous elements at teh far left. They will have to be dealt with separately.\n\nLet's look at the \"violin\" version of the same plot. "], "code": "# Reference: https://www.kaggle.com/code/tunguz/eloda-with-feature-engineering-and-stacking\n\nsns.set_style(\"whitegrid\")\nax = sns.violinplot(x=train.target.values)\nplt.show()", "processed": ["seem like pretti nice normal look distribut except anomal element teh far left dealt separ let look violin version plot"]}, {"markdown": ["## EDA & FE by the XGB, LGBM, Logistic Regression and Linear Regression"], "code": "# Reference: https://www.kaggle.com/code/vbmokin/cfec-fe-model-selection-xgb-lgbm-logr-linr\n\n# Building the feature importance diagrams and prediction by 4 models\n# Thanks to https://www.kaggle.com/vbmokin/feature-importance-xgb-lgbm-logreg-linreg\n  \n# LGBM\n#%% split training set to validation set\nXtrain, Xval, Ztrain, Zval = train_test_split(train, labels, test_size=0.2, random_state=0)\ntrain_set = lgbm.Dataset(Xtrain, Ztrain, silent=False)\nvalid_set = lgbm.Dataset(Xval, Zval, silent=False)\nparams = {\n        'boosting_type':'gbdt',\n        'objective': 'regression',\n        'num_leaves': 31,\n        'learning_rate': 0.05,\n        'max_depth': -1,\n        'subsample': 0.8,\n        'bagging_fraction' : 1,\n        'max_bin' : 5000 ,\n        'bagging_freq': 20,\n        'colsample_bytree': 0.6,\n        'metric': 'rmse',\n        'min_split_gain': 0.5,\n        'min_child_weight': 1,\n        'min_child_samples': 10,\n        'scale_pos_weight':1,\n        'zero_as_missing': True,\n        'seed':0,        \n    }\n\nmodelL = lgbm.train(params, train_set = train_set, num_boost_round=1000,\n                   early_stopping_rounds=50, verbose_eval=50, valid_sets=valid_set)\nfeature_score = pd.DataFrame(train.columns, columns = ['feature'])\npred_train = pd.DataFrame()\npred_test = pd.DataFrame()\nfeature_score['score_lgb'] = modelL.feature_importance()\npred_train['lgb_pred'] = modelL.predict(train)\npred_test['lgb_pred'] = modelL.predict(test)\nacc.append(round(r2_score(labels, pred_train['lgb_pred']) * 100, 2))\n\n#Plot FI\nfig =  plt.figure(figsize = (15,15))\naxes = fig.add_subplot(111)\nlgbm.plot_importance(modelL,ax = axes,height = 0.5)\nplt.show();plt.close()\n#XGBM\n#%% split training set to validation set \ndata_tr  = xgb.DMatrix(Xtrain, label=Ztrain)\ndata_cv  = xgb.DMatrix(Xval   , label=Zval)\ndata_train = xgb.DMatrix(train)\ndata_test = xgb.DMatrix(test)\nevallist = [(data_tr, 'train'), (data_cv, 'valid')]\nparms = {'max_depth':8, #maximum depth of a tree\n         'objective':'reg:logistic',\n         'eta'      :0.3,\n         'subsample':0.8,#SGD will use this percentage of data\n         'lambda '  :4, #L2 regularization term,>1 more conservative \n         'colsample_bytree ':0.9,\n         'colsample_bylevel':1,\n         'min_child_weight': 10}\nmodelx = xgb.train(parms, data_tr, num_boost_round=200, evals = evallist,\n                  early_stopping_rounds=30, maximize=False, \n                  verbose_eval=10)\nfeature_score['score_xgb'] = feature_score['feature'].map(modelx.get_score(importance_type='weight'))\npred_train['xgb_pred'] = modelx.predict(data_train)\npred_test['xgb_pred'] = modelx.predict(data_test)\nacc.append(round(r2_score(labels, pred_train['xgb_pred']) * 100, 2))\n\n#Plot FI\nfig =  plt.figure(figsize = (15,15))\naxes = fig.add_subplot(111)\nxgb.plot_importance(modelx,ax = axes,height = 0.5)\nplt.show();plt.close()\n# Regression models\n# Standardization for regression models\ntrain2 = pd.DataFrame(preprocessing.MinMaxScaler().fit_transform(train),\n    columns=train.columns, index=train.index)\n# Logistic Regression\nlogreg = LogisticRegression()\nlogreg.fit(train2, labels)\ncoeff_logreg = pd.DataFrame(train2.columns.delete(0))\ncoeff_logreg.columns = ['feature']\ncoeff_logreg[\"score_logreg\"] = pd.Series(logreg.coef_[0])\npred_train['log_pred'] = logreg.predict(train)\npred_test['log_pred'] = logreg.predict(test)\nacc.append(round(r2_score(labels, pred_train['log_pred']) * 100, 2))\ncoeff_logreg.sort_values(by='score_logreg', ascending=False)\ncoeff_logreg[\"score_logreg\"] = coeff_logreg[\"score_logreg\"].abs() # the level of importance of features is not associated with the sign\nfeature_score = pd.merge(feature_score, coeff_logreg, on='feature')\n# Linear Regression\nlinreg = LinearRegression()\nlinreg.fit(train2, labels)\ncoeff_linreg = pd.DataFrame(train2.columns.delete(0))\ncoeff_linreg.columns = ['feature']\ncoeff_linreg[\"score_linreg\"] = pd.Series(linreg.coef_)\npred_train['lin_pred'] = linreg.predict(train)\npred_test['lin_pred'] = linreg.predict(test)\nacc.append(round(r2_score(labels, pred_train['lin_pred']) * 100, 2))\ncoeff_linreg.sort_values(by='score_linreg', ascending=False)\ncoeff_linreg[\"score_linreg\"] = coeff_linreg[\"score_linreg\"].abs() # the level of importance of features is not associated with the sign\nfeature_score = pd.merge(feature_score, coeff_linreg, on='feature')\n# Comparison of the all feature importance diagrams\nfeature_score = feature_score.fillna(0)\nfeature_score = feature_score.set_index('feature')\n\n# MinMax scale all importances\nfeature_score = pd.DataFrame(preprocessing.MinMaxScaler().fit_transform(feature_score),\n    columns=feature_score.columns, index=feature_score.index)\n\n# Create mean column\nfeature_score['mean'] = feature_score.mean(axis=1)\npred_train['mean'] = pred_train.mean(axis=1)\npred_test['mean'] = pred_test.mean(axis=1)\nacc.append(round(r2_score(labels, pred_train['mean']) * 100, 2))\n\n# Create total column with different weights\nfL = 0.2\nflog = 0.5\nflin = 0.15\nfx = 1-fL-flog-flin\nfeature_score['total'] = fL*feature_score['score_lgb'] + fx*feature_score['score_xgb'] \\\n                       + flog*feature_score['score_logreg'] + flin*feature_score['score_linreg']\npred_train['total'] = fL*pred_train['lgb_pred'] + fx*pred_train['xgb_pred'] + \\\n                    + flog*pred_train['log_pred'] + flin*pred_train['lin_pred']\npred_test['total'] = fL*pred_test['lgb_pred'] + fx*pred_test['xgb_pred'] + \\\n                    + flog*pred_test['log_pred'] + flin*pred_test['lin_pred']\nacc.append(round(r2_score(labels, pred_train['total']) * 100, 2))\n\n# Plot the feature importances\nfeature_score.sort_values('total', ascending=False).plot(kind='bar', figsize=(20, 10))\nfeature_score.sort_values('score_logreg', ascending=False)\nacc\n# to_drop = feature_score[(feature_score['score_logreg'] < 0.0025)].index.tolist()\n# to_drop\n# data = data.drop(to_drop,axis=1)\ndata.info()", "processed": ["eda fe xgb lgbm logist regress linear regress"]}, {"markdown": ["### Display results in a barplot"], "code": "# Reference: https://www.kaggle.com/code/ogrellier/simple-feature-scorer\n\n# Initialize the matplotlib figure\nf, ax = plt.subplots(figsize=(10, 20))\n\n# Plot the total crashes\nsns.set_color_codes(\"pastel\")\n\nsns.barplot(x=\"score\", y=\"feature\", \n            data=f_scores,\n            palette=mpl.cm.ScalarMappable(cmap='viridis_r').to_rgba((100 * f_scores[\"score\"] / .2)))\nplt.xlabel(\"Score\")\nplt.ylabel(\"Feature\")\nplt.title(\"Raw Feature Gini Score\")", "processed": ["display result barplot"]}, {"markdown": ["# 2. Plotting Half Court"], "code": "# Reference: https://www.kaggle.com/code/robikscube/ncaa-basketball-court-plot-helper-functions\n\ndef create_ncaa_half_court(ax=None, three_line='mens', court_color='#dfbb85',\n                           lw=3, lines_color='black', lines_alpha=0.5,\n                           paint_fill='blue', paint_alpha=0.4,\n                          inner_arc=False):\n    \"\"\"\n    Version 2020.2.19\n\n    Creates NCAA Basketball Half Court\n    Dimensions are in feet (Court is 97x50 ft)\n    Created by: Rob Mulla / https://github.com/RobMulla\n\n    * Note that this function uses \"feet\" as the unit of measure.\n    * NCAA Data is provided on a x range: 0, 100 and y-range 0 to 100\n    * To plot X/Y positions first convert to feet like this:\n    ```\n    Events['X_'] = (Events['X'] * (94/100))\n    Events['Y_'] = (Events['Y'] * (50/100))\n    ```\n    ax: matplotlib axes if None gets current axes using `plt.gca`\n    \n    three_line: 'mens', 'womens' or 'both' defines 3 point line plotted\n    court_color : (hex) Color of the court\n    lw : line width\n    lines_color : Color of the lines\n    lines_alpha : transparency of lines\n    paint_fill : Color inside the paint\n    paint_alpha : transparency of the \"paint\"\n    inner_arc : paint the dotted inner arc\n    \"\"\"\n    if ax is None:\n        ax = plt.gca()\n\n    # Create Pathes for Court Lines\n    center_circle = Circle((50/2, 94/2), 6,\n                           linewidth=lw, color=lines_color, lw=lw,\n                           fill=False, alpha=lines_alpha)\n    hoop = Circle((50/2, 5.25), 1.5 / 2,\n                       linewidth=lw, color=lines_color, lw=lw,\n                       fill=False, alpha=lines_alpha)\n\n    # Paint - 18 Feet 10 inches which converts to 18.833333 feet - gross!\n    paint = Rectangle(((50/2)-6, 0), 12, 18.833333,\n                           fill=paint_fill, alpha=paint_alpha,\n                           lw=lw, edgecolor=None)\n    \n    paint_boarder = Rectangle(((50/2)-6, 0), 12, 18.833333,\n                           fill=False, alpha=lines_alpha,\n                           lw=lw, edgecolor=lines_color)\n    \n    arc = Arc((50/2, 18.833333), 12, 12, theta1=-\n                   0, theta2=180, color=lines_color, lw=lw,\n                   alpha=lines_alpha)\n    \n    block1 = Rectangle(((50/2)-6-0.666, 7), 0.666, 1, \n                           fill=True, alpha=lines_alpha,\n                           lw=0, edgecolor=lines_color,\n                           facecolor=lines_color)\n    block2 = Rectangle(((50/2)+6, 7), 0.666, 1, \n                           fill=True, alpha=lines_alpha,\n                           lw=0, edgecolor=lines_color,\n                           facecolor=lines_color)\n    ax.add_patch(block1)\n    ax.add_patch(block2)\n    \n    l1 = Rectangle(((50/2)-6-0.666, 11), 0.666, 0.166,\n                           fill=True, alpha=lines_alpha,\n                           lw=0, edgecolor=lines_color,\n                           facecolor=lines_color)\n    l2 = Rectangle(((50/2)-6-0.666, 14), 0.666, 0.166,\n                           fill=True, alpha=lines_alpha,\n                           lw=0, edgecolor=lines_color,\n                           facecolor=lines_color)\n    l3 = Rectangle(((50/2)-6-0.666, 17), 0.666, 0.166,\n                           fill=True, alpha=lines_alpha,\n                           lw=0, edgecolor=lines_color,\n                           facecolor=lines_color)\n    ax.add_patch(l1)\n    ax.add_patch(l2)\n    ax.add_patch(l3)\n    l4 = Rectangle(((50/2)+6, 11), 0.666, 0.166,\n                           fill=True, alpha=lines_alpha,\n                           lw=0, edgecolor=lines_color,\n                           facecolor=lines_color)\n    l5 = Rectangle(((50/2)+6, 14), 0.666, 0.166,\n                           fill=True, alpha=lines_alpha,\n                           lw=0, edgecolor=lines_color,\n                           facecolor=lines_color)\n    l6 = Rectangle(((50/2)+6, 17), 0.666, 0.166,\n                           fill=True, alpha=lines_alpha,\n                           lw=0, edgecolor=lines_color,\n                           facecolor=lines_color)\n    ax.add_patch(l4)\n    ax.add_patch(l5)\n    ax.add_patch(l6)\n    \n    # 3 Point Line\n    if (three_line == 'mens') | (three_line == 'both'):\n        # 22' 1.75\" distance to center of hoop\n        three_pt = Arc((50/2, 6.25), 44.291, 44.291, theta1=12,\n                            theta2=168, color=lines_color, lw=lw,\n                            alpha=lines_alpha)\n\n        # 4.25 feet max to sideline for mens\n        ax.plot((3.34, 3.34), (0, 11.20),\n                color=lines_color, lw=lw, alpha=lines_alpha)\n        ax.plot((50-3.34, 50-3.34), (0, 11.20),\n                color=lines_color, lw=lw, alpha=lines_alpha)\n        ax.add_patch(three_pt)\n\n    if (three_line == 'womens') | (three_line == 'both'):\n        # womens 3\n        three_pt_w = Arc((50/2, 6.25), 20.75 * 2, 20.75 * 2, theta1=5,\n                              theta2=175, color=lines_color, lw=lw, alpha=lines_alpha)\n        # 4.25 inches max to sideline for mens\n        ax.plot( (4.25, 4.25), (0, 8), color=lines_color,\n                lw=lw, alpha=lines_alpha)\n        ax.plot((50-4.25, 50-4.25), (0, 8.1),\n                color=lines_color, lw=lw, alpha=lines_alpha)\n\n        ax.add_patch(three_pt_w)\n\n    # Add Patches\n    ax.add_patch(paint)\n    ax.add_patch(paint_boarder)\n    ax.add_patch(center_circle)\n    ax.add_patch(hoop)\n    ax.add_patch(arc)\n    \n    if inner_arc:\n        inner_arc = Arc((50/2, 18.833333), 12, 12, theta1=180,\n                             theta2=0, color=lines_color, lw=lw,\n                       alpha=lines_alpha, ls='--')\n        ax.add_patch(inner_arc)\n\n    # Restricted Area Marker\n    restricted_area = Arc((50/2, 6.25), 8, 8, theta1=0,\n                        theta2=180, color=lines_color, lw=lw,\n                        alpha=lines_alpha)\n    ax.add_patch(restricted_area)\n    \n    # Backboard\n    ax.plot(((50/2) - 3, (50/2) + 3), (4, 4),\n            color=lines_color, lw=lw*1.5, alpha=lines_alpha)\n    ax.plot( (50/2, 50/2), (4.3, 4), color=lines_color,\n            lw=lw, alpha=lines_alpha)\n\n    # Half Court Line\n    ax.axhline(94/2, color=lines_color, lw=lw, alpha=lines_alpha)\n\n    \n    # Plot Limit\n    ax.set_xlim(0, 50)\n    ax.set_ylim(0, 94/2 + 2)\n    ax.set_facecolor(court_color)\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.set_xlabel('')\n    return ax\n\nfig, ax = plt.subplots(figsize=(11, 11.2))\ncreate_ncaa_half_court(ax,\n                       three_line='both',\n                       paint_alpha=0.4,\n                       inner_arc=True)\nplt.show()\nfig, axs = plt.subplots(2, 3, figsize=(15, 10))\ncolor_schemes = [['#93B7BE', '#048A81', '#2D3047'], # court, paint, lines\n                ['#BFC0C0', '#7DC95E', '#648767'],\n                ['#DDA448', '#BB342F', '#8D6A9F'],\n                ['#13505B', '#ED4848', '#ED4848'],\n                ['#161A32', '#D9DCD6', '#EAF2EF'],\n                ['#020202', '#E54424', '#FFFFFF']]\nidx = 0\nfor ax in axs.reshape(-1):\n    create_ncaa_half_court(ax,\n                           three_line='both',\n                           paint_alpha=0.1,\n                           inner_arc=True,\n                           court_color=color_schemes[idx][0],\n                           paint_fill=color_schemes[idx][1],\n                           lines_color=color_schemes[idx][2],\n                           lw=1.5)\n    idx += 1\n\nplt.tight_layout()\nplt.show()\n# Half Court Example\nfig, ax = plt.subplots(figsize=(13.8, 14))\nMEvents \\\n    .query('Y_ != 0') \\\n    .plot(x='Y_half_', y='X_half_', style='.',\n          kind='scatter', ax=ax,\n          color='orange', alpha=0.05)\ncreate_ncaa_half_court(ax, court_color='black',\n                       lines_color='white', paint_alpha=0,\n                       inner_arc=True)\nplt.show()\n# Made and Missed shots dataframes\nWMadeShots = WEvents.query('EventType == \"made2\" or EventType == \"made3\"')\nWMissedShots = WEvents.query('EventType == \"miss2\" or EventType == \"miss3\"')\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 7))\nax1 = create_ncaa_half_court(ax=ax1,\n                             three_line='womens',\n                             court_color='white',\n                             paint_alpha=0,\n                             inner_arc=True)\nax2 = create_ncaa_half_court(ax=ax2,\n                             three_line='womens',\n                             court_color='white',\n                             paint_alpha=0,\n                             inner_arc=True)\nhb1 = ax1.hexbin(x=WMadeShots.query('Y_ != 0')['Y_half_'],\n                 y=WMadeShots.query('Y_ != 0')['X_half_'],\n                 gridsize=20, bins='log', cmap='inferno')\nhb2 = ax2.hexbin(x=WMissedShots.query('Y_ != 0')['Y_half_'],\n                 y=WMissedShots.query('Y_ != 0')['X_half_'],\n                 gridsize=20, bins='log', cmap='inferno')\nax1.set_title('Womens NCAA Made Shots', size=15)\nax2.set_title('Womens NCAA Missed Shots', size=15)\ncb1 = fig.colorbar(hb1, ax=ax1)\ncb1 = fig.colorbar(hb2, ax=ax2)\nplt.tight_layout()\nplt.show()", "processed": ["2 plot half court"]}, {"markdown": ["# 3. Plotting Full Court"], "code": "# Reference: https://www.kaggle.com/code/robikscube/ncaa-basketball-court-plot-helper-functions\n\ndef create_ncaa_full_court(ax=None, three_line='mens', court_color='#dfbb85',\n                           lw=3, lines_color='black', lines_alpha=0.5,\n                           paint_fill='blue', paint_alpha=0.4,\n                           inner_arc=False):\n    \"\"\"\n    Version 2020.2.19\n    Creates NCAA Basketball Court\n    Dimensions are in feet (Court is 97x50 ft)\n    Created by: Rob Mulla / https://github.com/RobMulla\n\n    * Note that this function uses \"feet\" as the unit of measure.\n    * NCAA Data is provided on a x range: 0, 100 and y-range 0 to 100\n    * To plot X/Y positions first convert to feet like this:\n    ```\n    Events['X_'] = (Events['X'] * (94/100))\n    Events['Y_'] = (Events['Y'] * (50/100))\n    ```\n    \n    ax: matplotlib axes if None gets current axes using `plt.gca`\n\n\n    three_line: 'mens', 'womens' or 'both' defines 3 point line plotted\n    court_color : (hex) Color of the court\n    lw : line width\n    lines_color : Color of the lines\n    lines_alpha : transparency of lines\n    paint_fill : Color inside the paint\n    paint_alpha : transparency of the \"paint\"\n    inner_arc : paint the dotted inner arc\n    \"\"\"\n    if ax is None:\n        ax = plt.gca()\n\n    # Create Pathes for Court Lines\n    center_circle = Circle((94/2, 50/2), 6,\n                           linewidth=lw, color=lines_color, lw=lw,\n                           fill=False, alpha=lines_alpha)\n    hoop_left = Circle((5.25, 50/2), 1.5 / 2,\n                       linewidth=lw, color=lines_color, lw=lw,\n                       fill=False, alpha=lines_alpha)\n    hoop_right = Circle((94-5.25, 50/2), 1.5 / 2,\n                        linewidth=lw, color=lines_color, lw=lw,\n                        fill=False, alpha=lines_alpha)\n\n    # Paint - 18 Feet 10 inches which converts to 18.833333 feet - gross!\n    left_paint = Rectangle((0, (50/2)-6), 18.833333, 12,\n                           fill=paint_fill, alpha=paint_alpha,\n                           lw=lw, edgecolor=None)\n    right_paint = Rectangle((94-18.83333, (50/2)-6), 18.833333,\n                            12, fill=paint_fill, alpha=paint_alpha,\n                            lw=lw, edgecolor=None)\n    \n    left_paint_boarder = Rectangle((0, (50/2)-6), 18.833333, 12,\n                           fill=False, alpha=lines_alpha,\n                           lw=lw, edgecolor=lines_color)\n    right_paint_boarder = Rectangle((94-18.83333, (50/2)-6), 18.833333,\n                            12, fill=False, alpha=lines_alpha,\n                            lw=lw, edgecolor=lines_color)\n\n    left_arc = Arc((18.833333, 50/2), 12, 12, theta1=-\n                   90, theta2=90, color=lines_color, lw=lw,\n                   alpha=lines_alpha)\n    right_arc = Arc((94-18.833333, 50/2), 12, 12, theta1=90,\n                    theta2=-90, color=lines_color, lw=lw,\n                    alpha=lines_alpha)\n    \n    leftblock1 = Rectangle((7, (50/2)-6-0.666), 1, 0.666,\n                           fill=True, alpha=lines_alpha,\n                           lw=0, edgecolor=lines_color,\n                           facecolor=lines_color)\n    leftblock2 = Rectangle((7, (50/2)+6), 1, 0.666,\n                           fill=True, alpha=lines_alpha,\n                           lw=0, edgecolor=lines_color,\n                           facecolor=lines_color)\n    ax.add_patch(leftblock1)\n    ax.add_patch(leftblock2)\n    \n    left_l1 = Rectangle((11, (50/2)-6-0.666), 0.166, 0.666,\n                           fill=True, alpha=lines_alpha,\n                           lw=0, edgecolor=lines_color,\n                           facecolor=lines_color)\n    left_l2 = Rectangle((14, (50/2)-6-0.666), 0.166, 0.666,\n                           fill=True, alpha=lines_alpha,\n                           lw=0, edgecolor=lines_color,\n                           facecolor=lines_color)\n    left_l3 = Rectangle((17, (50/2)-6-0.666), 0.166, 0.666,\n                           fill=True, alpha=lines_alpha,\n                           lw=0, edgecolor=lines_color,\n                           facecolor=lines_color)\n    ax.add_patch(left_l1)\n    ax.add_patch(left_l2)\n    ax.add_patch(left_l3)\n    left_l4 = Rectangle((11, (50/2)+6), 0.166, 0.666,\n                           fill=True, alpha=lines_alpha,\n                           lw=0, edgecolor=lines_color,\n                           facecolor=lines_color)\n    left_l5 = Rectangle((14, (50/2)+6), 0.166, 0.666,\n                           fill=True, alpha=lines_alpha,\n                           lw=0, edgecolor=lines_color,\n                           facecolor=lines_color)\n    left_l6 = Rectangle((17, (50/2)+6), 0.166, 0.666,\n                           fill=True, alpha=lines_alpha,\n                           lw=0, edgecolor=lines_color,\n                           facecolor=lines_color)\n    ax.add_patch(left_l4)\n    ax.add_patch(left_l5)\n    ax.add_patch(left_l6)\n    \n    rightblock1 = Rectangle((94-7-1, (50/2)-6-0.666), 1, 0.666,\n                           fill=True, alpha=lines_alpha,\n                           lw=0, edgecolor=lines_color,\n                           facecolor=lines_color)\n    rightblock2 = Rectangle((94-7-1, (50/2)+6), 1, 0.666,\n                           fill=True, alpha=lines_alpha,\n                           lw=0, edgecolor=lines_color,\n                           facecolor=lines_color)\n    ax.add_patch(rightblock1)\n    ax.add_patch(rightblock2)\n\n    right_l1 = Rectangle((94-11, (50/2)-6-0.666), 0.166, 0.666,\n                           fill=True, alpha=lines_alpha,\n                           lw=0, edgecolor=lines_color,\n                           facecolor=lines_color)\n    right_l2 = Rectangle((94-14, (50/2)-6-0.666), 0.166, 0.666,\n                           fill=True, alpha=lines_alpha,\n                           lw=0, edgecolor=lines_color,\n                           facecolor=lines_color)\n    right_l3 = Rectangle((94-17, (50/2)-6-0.666), 0.166, 0.666,\n                           fill=True, alpha=lines_alpha,\n                           lw=0, edgecolor=lines_color,\n                           facecolor=lines_color)\n    ax.add_patch(right_l1)\n    ax.add_patch(right_l2)\n    ax.add_patch(right_l3)\n    right_l4 = Rectangle((94-11, (50/2)+6), 0.166, 0.666,\n                           fill=True, alpha=lines_alpha,\n                           lw=0, edgecolor=lines_color,\n                           facecolor=lines_color)\n    right_l5 = Rectangle((94-14, (50/2)+6), 0.166, 0.666,\n                           fill=True, alpha=lines_alpha,\n                           lw=0, edgecolor=lines_color,\n                           facecolor=lines_color)\n    right_l6 = Rectangle((94-17, (50/2)+6), 0.166, 0.666,\n                           fill=True, alpha=lines_alpha,\n                           lw=0, edgecolor=lines_color,\n                           facecolor=lines_color)\n    ax.add_patch(right_l4)\n    ax.add_patch(right_l5)\n    ax.add_patch(right_l6)\n    \n    # 3 Point Line\n    if (three_line == 'mens') | (three_line == 'both'):\n        # 22' 1.75\" distance to center of hoop\n        three_pt_left = Arc((6.25, 50/2), 44.291, 44.291, theta1=-78,\n                            theta2=78, color=lines_color, lw=lw,\n                            alpha=lines_alpha)\n        three_pt_right = Arc((94-6.25, 50/2), 44.291, 44.291,\n                             theta1=180-78, theta2=180+78,\n                             color=lines_color, lw=lw, alpha=lines_alpha)\n\n        # 4.25 feet max to sideline for mens\n        ax.plot((0, 11.25), (3.34, 3.34),\n                color=lines_color, lw=lw, alpha=lines_alpha)\n        ax.plot((0, 11.25), (50-3.34, 50-3.34),\n                color=lines_color, lw=lw, alpha=lines_alpha)\n        ax.plot((94-11.25, 94), (3.34, 3.34),\n                color=lines_color, lw=lw, alpha=lines_alpha)\n        ax.plot((94-11.25, 94), (50-3.34, 50-3.34),\n                color=lines_color, lw=lw, alpha=lines_alpha)\n        ax.add_patch(three_pt_left)\n        ax.add_patch(three_pt_right)\n\n    if (three_line == 'womens') | (three_line == 'both'):\n        # womens 3\n        three_pt_left_w = Arc((6.25, 50/2), 20.75 * 2, 20.75 * 2, theta1=-85,\n                              theta2=85, color=lines_color, lw=lw, alpha=lines_alpha)\n        three_pt_right_w = Arc((94-6.25, 50/2), 20.75 * 2, 20.75 * 2,\n                               theta1=180-85, theta2=180+85,\n                               color=lines_color, lw=lw, alpha=lines_alpha)\n\n        # 4.25 inches max to sideline for mens\n        ax.plot((0, 8.3), (4.25, 4.25), color=lines_color,\n                lw=lw, alpha=lines_alpha)\n        ax.plot((0, 8.3), (50-4.25, 50-4.25),\n                color=lines_color, lw=lw, alpha=lines_alpha)\n        ax.plot((94-8.3, 94), (4.25, 4.25),\n                color=lines_color, lw=lw, alpha=lines_alpha)\n        ax.plot((94-8.3, 94), (50-4.25, 50-4.25),\n                color=lines_color, lw=lw, alpha=lines_alpha)\n\n        ax.add_patch(three_pt_left_w)\n        ax.add_patch(three_pt_right_w)\n\n    # Add Patches\n    ax.add_patch(left_paint)\n    ax.add_patch(left_paint_boarder)\n    ax.add_patch(right_paint)\n    ax.add_patch(right_paint_boarder)\n    ax.add_patch(center_circle)\n    ax.add_patch(hoop_left)\n    ax.add_patch(hoop_right)\n    ax.add_patch(left_arc)\n    ax.add_patch(right_arc)\n    \n    if inner_arc:\n        left_inner_arc = Arc((18.833333, 50/2), 12, 12, theta1=90,\n                             theta2=-90, color=lines_color, lw=lw,\n                       alpha=lines_alpha, ls='--')\n        right_inner_arc = Arc((94-18.833333, 50/2), 12, 12, theta1=-90,\n                        theta2=90, color=lines_color, lw=lw,\n                        alpha=lines_alpha, ls='--')\n        ax.add_patch(left_inner_arc)\n        ax.add_patch(right_inner_arc)\n\n    # Restricted Area Marker\n    restricted_left = Arc((6.25, 50/2), 8, 8, theta1=-90,\n                        theta2=90, color=lines_color, lw=lw,\n                        alpha=lines_alpha)\n    restricted_right = Arc((94-6.25, 50/2), 8, 8,\n                         theta1=180-90, theta2=180+90,\n                         color=lines_color, lw=lw, alpha=lines_alpha)\n    ax.add_patch(restricted_left)\n    ax.add_patch(restricted_right)\n    \n    # Backboards\n    ax.plot((4, 4), ((50/2) - 3, (50/2) + 3),\n            color=lines_color, lw=lw*1.5, alpha=lines_alpha)\n    ax.plot((94-4, 94-4), ((50/2) - 3, (50/2) + 3),\n            color=lines_color, lw=lw*1.5, alpha=lines_alpha)\n    ax.plot((4, 4.6), (50/2, 50/2), color=lines_color,\n            lw=lw, alpha=lines_alpha)\n    ax.plot((94-4, 94-4.6), (50/2, 50/2),\n            color=lines_color, lw=lw, alpha=lines_alpha)\n\n    # Half Court Line\n    ax.axvline(94/2, color=lines_color, lw=lw, alpha=lines_alpha)\n\n    # Boarder\n    boarder = Rectangle((0.3,0.3), 94-0.4, 50-0.4, fill=False, lw=3, color='black', alpha=lines_alpha)\n    ax.add_patch(boarder)\n    \n    # Plot Limit\n    ax.set_xlim(0, 94)\n    ax.set_ylim(0, 50)\n    ax.set_facecolor(court_color)\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.set_xlabel('')\n    return ax\n# Example to make plot\nfig, ax = plt.subplots(figsize=(15, 8))\ncreate_ncaa_full_court(ax,\n                       three_line='both',\n                       paint_alpha=0.4,\n                       inner_arc=True)\nplt.show()\n# Example adding data \nfig, ax = plt.subplots(figsize=(15, 8))\ncreate_ncaa_full_court(ax,\n                       three_line='both',\n                       paint_alpha=0.4,\n                       inner_arc=True)\nfor i, d in MEvents.query('PlayerID == 13061 and X_ != 0').groupby('EventType'):\n    d.plot(x='X_', y='Y_', style='X', ax=ax, label=i, alpha=1)\n    plt.legend()\nplt.show()\n# Example with different color schemes\nfig, axs = plt.subplots(3, 2, figsize=(15, 13))\ncolor_schemes = [['#93B7BE', '#048A81', '#2D3047'], # court, paint, lines\n                ['#BFC0C0', '#7DC95E', '#648767'],\n                ['#DDA448', '#BB342F', '#8D6A9F'],\n                ['#13505B', '#ED4848', '#ED4848'],\n                ['#161A32', '#D9DCD6', '#EAF2EF'],\n                ['#020202', '#E54424', '#FFFFFF']]\nidx = 0\nfor ax in axs.reshape(-1):\n    create_ncaa_full_court(ax,\n                           three_line='both',\n                           paint_alpha=0.1,\n                           inner_arc=True,\n                           court_color=color_schemes[idx][0],\n                           paint_fill=color_schemes[idx][1],\n                           lines_color=color_schemes[idx][2],\n                           lw=1.5)\n    idx += 1\n\nplt.tight_layout()\nplt.show()", "processed": ["3 plot full court"]}, {"markdown": ["## Evaluation", "Unhide below to see helper function `display_training_curves`:"], "code": "# Reference: https://www.kaggle.com/code/xhlulu/plant-pathology-very-concise-tpu-efficientnet\n\ndef display_training_curves(training, validation, title, subplot):\n    \"\"\"\n    Source: https://www.kaggle.com/mgornergoogle/getting-started-with-100-flowers-on-tpu\n    \"\"\"\n    if subplot%10==1: # set up the subplots on the first call\n        plt.subplots(figsize=(10,10), facecolor='#F0F0F0')\n        plt.tight_layout()\n    ax = plt.subplot(subplot)\n    ax.set_facecolor('#F8F8F8')\n    ax.plot(training)\n    ax.plot(validation)\n    ax.set_title('model '+ title)\n    ax.set_ylabel(title)\n    #ax.set_ylim(0.28,1.05)\n    ax.set_xlabel('epoch')\n    ax.legend(['train', 'valid.'])\ndisplay_training_curves(\n    history.history['loss'], \n    history.history['val_loss'], \n    'loss', 211)\ndisplay_training_curves(\n    history.history['categorical_accuracy'], \n    history.history['val_categorical_accuracy'], \n    'accuracy', 212)", "processed": ["evalu", "unhid see helper function display train curv"]}, {"markdown": ["In `TeamSpellings` we have info about all spellings of all teams. Let's use this as a feature!"], "code": "# Reference: https://www.kaggle.com/code/artgor/ncaa-men-s-eda-and-models\n\nteam_counts = data_dict['TeamSpellings'].groupby('TeamID')['TeamNameSpelling'].count().reset_index()\nteam_counts.columns = ['TeamID', 'TeamSpellingCount']\nplt.title('Count of team counts');\nteam_counts['TeamSpellingCount'].value_counts().sort_index().plot(kind='barh', color='teal');", "processed": ["teamspel info spell team let use featur"]}, {"markdown": ["### Regular season results\n\nLet's try using some info from regular seasons"], "code": "# Reference: https://www.kaggle.com/code/artgor/ncaa-men-s-eda-and-models\n\ndata_dict['RegularSeasonCompactResults'].head()\nplt.title('Mean scores of winning teams by season');\ndata_dict['RegularSeasonCompactResults'].groupby(['Season'])['WScore'].mean().plot();", "processed": ["regular season result let tri use info regular season"]}, {"markdown": ["## Regular seasons detailed results"], "code": "# Reference: https://www.kaggle.com/code/artgor/ncaa-men-s-eda-and-models\n\ndata_dict['RegularSeasonDetailedResults'].head()\ndata_dict['RegularSeasonDetailedResults']['Season_join'] = data_dict['RegularSeasonDetailedResults']['Season'] + 1\nplt.title('Mean number of field goals made by winning teams by season');\ndata_dict['RegularSeasonDetailedResults'].groupby(['Season'])['WFGM'].mean().plot();\nplt.title('Mean number of field goals attempted by winning teams by season');\ndata_dict['RegularSeasonDetailedResults'].groupby(['Season'])['WFGA'].mean().plot();\nplt.title('Mean number of three pointers made by winning teams by season');\ndata_dict['RegularSeasonDetailedResults'].groupby(['Season'])['WFGM3'].mean().plot();\nplt.title('Mean number of three pointers attempted by winning teams by season');\ndata_dict['RegularSeasonDetailedResults'].groupby(['Season'])['WFGA3'].mean().plot();\nplt.title('Mean number of free throws made by winning teams by season');\ndata_dict['RegularSeasonDetailedResults'].groupby(['Season'])['WFTM'].mean().plot();\nplt.title('Mean number of free throws made by winning teams by season');\ndata_dict['RegularSeasonDetailedResults'].groupby(['Season'])['WFTA'].mean().plot();\nplt.title('Mean number of offensive rebounds pulled by winning teams by season');\ndata_dict['RegularSeasonDetailedResults'].groupby(['Season'])['WOR'].mean().plot();\nplt.title('Mean number of defensive rebounds pulled by winning teams by season');\ndata_dict['RegularSeasonDetailedResults'].groupby(['Season'])['WDR'].mean().plot();\nplt.title('Mean number of assists by winning teams by season');\ndata_dict['RegularSeasonDetailedResults'].groupby(['Season'])['WAst'].mean().plot();\nplt.title('Mean number of turnovers committed by winning teams by season');\ndata_dict['RegularSeasonDetailedResults'].groupby(['Season'])['WTO'].mean().plot();\nplt.title('Mean number of steals accomplished by winning teams by season');\ndata_dict['RegularSeasonDetailedResults'].groupby(['Season'])['WStl'].mean().plot();\nplt.title('Mean number of blocks accomplished by winning teams by season');\ndata_dict['RegularSeasonDetailedResults'].groupby(['Season'])['WBlk'].mean().plot();\nplt.title('Mean number of personal fouls committed by winning teams by season');\ndata_dict['RegularSeasonDetailedResults'].groupby(['Season'])['WPF'].mean().plot();", "processed": ["regular season detail result"]}, {"markdown": ["## Preparing data for training"], "code": "# Reference: https://www.kaggle.com/code/artgor/ncaa-men-s-eda-and-models\n\nnew_df = pd.merge(new_df, team_counts, how='left', left_on='Team1', right_on='TeamID')\nnew_df = new_df.drop(['TeamID'], axis=1)\nnew_df = pd.merge(new_df, team_counts, how='left', left_on='Team2', right_on='TeamID')\nnew_df = new_df.drop(['TeamID'], axis=1)\n\nsub = pd.merge(sub, team_counts, how='left', left_on='Team1', right_on='TeamID')\nsub = sub.drop(['TeamID'], axis=1)\nsub = pd.merge(sub, team_counts, how='left', left_on='Team2', right_on='TeamID')\nsub = sub.drop(['TeamID'], axis=1)\n\nnew_df = new_df.drop(['Season', 'Team1', 'Team2'], axis=1)\nsub = sub.drop(['Pred', 'Season', 'Team1', 'Team2', 'TeamID_x', 'TeamID_y', 'WTeamID_x', 'WTeamID_y', 'LTeamID_x', 'LTeamID_y'], axis=1)\nsub.columns = ['ID', 'Seed_1', 'Seed_2', 'WScore_1', 'LScore_1', 'LScore_2', 'WScore_2', 'seed_diff', 'TeamSpellingCount_x', 'TeamSpellingCount_y']\nsub = sub[['ID', 'Seed_1', 'Seed_2', 'seed_diff', 'WScore_1', 'LScore_1', 'LScore_2', 'WScore_2', 'TeamSpellingCount_x', 'TeamSpellingCount_y']]\nnew_df = new_df.fillna(0)\nsub = sub.fillna(0)\nX = new_df.drop(['target'], axis=1)\ny = new_df['target']\nX_test = sub.drop(['ID'], axis=1)\nn_fold = 5\nfolds = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=42)\ndef train_model(X, X_test, y, params, folds, model_type='lgb', plot_feature_importance=False, averaging='usual', model=None):\n    oof = np.zeros(len(X))\n    prediction = np.zeros(len(X_test))\n    scores = []\n    feature_importance = pd.DataFrame()\n    for fold_n, (train_index, valid_index) in enumerate(folds.split(X, y)):\n        print('Fold', fold_n, 'started at', time.ctime())\n        X_train, X_valid = X.values[train_index], X.values[valid_index]\n        y_train, y_valid = y.values[train_index], y.values[valid_index]\n        \n        if model_type == 'lgb':\n            train_data = lgb.Dataset(X_train, label=y_train)\n            valid_data = lgb.Dataset(X_valid, label=y_valid)\n            \n            model = lgb.train(params,\n                    train_data,\n                    num_boost_round=20000,\n                    valid_sets = [train_data, valid_data],\n                    verbose_eval=1000,\n                    early_stopping_rounds = 200)\n            \n            y_pred_valid = model.predict(X_valid)\n            y_pred = model.predict(X_test, num_iteration=model.best_iteration)\n            \n        if model_type == 'xgb':\n            train_data = xgb.DMatrix(data=X_train, label=y_train, feature_names=X_train.columns)\n            valid_data = xgb.DMatrix(data=X_valid, label=y_valid, feature_names=X_train.columns)\n\n            watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]\n            model = xgb.train(dtrain=train_data, num_boost_round=20000, evals=watchlist, early_stopping_rounds=200, verbose_eval=500, params=params)\n            y_pred_valid = model.predict(xgb.DMatrix(X_valid, feature_names=X_train.columns), ntree_limit=model.best_ntree_limit)\n            y_pred = model.predict(xgb.DMatrix(X_test, feature_names=X_train.columns), ntree_limit=model.best_ntree_limit)\n        \n        if model_type == 'sklearn':\n            model = model\n            model.fit(X_train, y_train)\n            y_pred_valid = model.predict_proba(X_valid)[:, 1].reshape(-1,)\n            score = roc_auc_score(y_valid, y_pred_valid)\n            # print(f'Fold {fold_n}. AUC: {score:.4f}.')\n            # print('')\n            \n            y_pred = model.predict_proba(X_test)[:, 1]\n            \n        if model_type == 'glm':\n            model = sm.GLM(y_train, X_train, family=sm.families.Binomial())\n            model_results = model.fit()\n            model_results.predict(X_test)\n            y_pred_valid = model_results.predict(X_valid).reshape(-1,)\n            score = roc_auc_score(y_valid, y_pred_valid)\n            \n            y_pred = model_results.predict(X_test)\n            \n        if model_type == 'cat':\n            model = CatBoostClassifier(iterations=20000, learning_rate=0.1, loss_function='Logloss',  eval_metric='AUC', **params)\n            model.fit(X_train, y_train, eval_set=(X_valid, y_valid), cat_features=[], use_best_model=True, verbose=False)\n\n            y_pred_valid = model.predict_proba(X_valid)\n            y_pred = model.predict_proba(X_test)\n            \n        oof[valid_index] = y_pred_valid.reshape(-1,)\n        scores.append(log_loss(y_valid, y_pred_valid))\n\n        if averaging == 'usual':\n            prediction += y_pred\n        elif averaging == 'rank':\n            prediction += pd.Series(y_pred).rank().values  \n        \n        if model_type == 'lgb':\n            # feature importance\n            fold_importance = pd.DataFrame()\n            fold_importance[\"feature\"] = X.columns\n            fold_importance[\"importance\"] = model.feature_importance()\n            fold_importance[\"fold\"] = fold_n + 1\n            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n\n    prediction /= n_fold\n    \n    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n    \n    if model_type == 'lgb':\n        feature_importance[\"importance\"] /= n_fold\n        if plot_feature_importance:\n            cols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n                by=\"importance\", ascending=False)[:50].index\n\n            best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n\n            plt.figure(figsize=(16, 12));\n            sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False));\n            plt.title('LGB Features (avg over folds)');\n        \n            return oof, prediction, feature_importance\n        return oof, prediction, scores\n    \n    else:\n        return oof, prediction, scores\nX1 = X.copy()\nX_test1 = X_test.copy()", "processed": ["prepar data train"]}, {"markdown": [">## PLOT ALL YOUR DATA!!!\nPlot all your data you say? This is going to be ugly....."], "code": "# Reference: https://www.kaggle.com/code/robikscube/eda-lets-have-fun-exploring-santander-data\n\n# Create a list of all the features\nfeatures = train.columns.tolist()[2:]\nlen(features) # That's a lot of features\n# Lets just try the first 10 features\nfor feature in features[:10]:\n    train.plot(x='target', y=feature, figsize=(10,1), kind='scatter', title=feature)\n    plt.axis('off')", "processed": ["plot data plot data say go ugli"]}, {"markdown": ["<h2>Visualing the distribuition of the unique values by each feature</h2>"], "code": "# Reference: https://www.kaggle.com/code/kabure/extensive-eda-of-deal-probability\n\nplt.figure(figsize=(16, 5))\n\ncols = df_train.columns\n\nuniques = [len(df_train[col].unique()) for col in cols]\nsns.set(font_scale=1.2)\nax = sns.barplot(cols, uniques, palette='hls', log=True)\nax.set(xlabel='Feature', ylabel='log(unique count)', title='Number of unique per feature')\nfor p, uniq in zip(ax.patches, uniques):\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()/2.,\n            height + 10,\n            uniq,\n            ha=\"center\") \nax.set_xticklabels(ax.get_xticklabels(),rotation=45)\nplt.show()", "processed": ["h2 visual distribuit uniqu valu featur h2"]}, {"markdown": ["", "Let's start exploring the distribuition of price and deal probability that will be one of the most important elements to guide our exploration", "<h2>I will start taking a look at  the Deal Probability that is the target feature </h2>"], "code": "# Reference: https://www.kaggle.com/code/kabure/extensive-eda-of-deal-probability\n\nplt.figure(figsize=(14,5))\n\nplt.subplot(1,2,1)\nax = sns.distplot(df_train[\"deal_probability\"].values, bins=100, kde=False)\nax.set_xlabel('Deal Probility', fontsize=15)\nax.set_ylabel('Deal Probility', fontsize=15)\nax.set_title(\"Deal Probability Histogram\", fontsize=20)\n\nplt.subplot(1,2,2)\nplt.scatter(range(df_train.shape[0]), np.sort(df_train['deal_probability'].values))\nplt.xlabel('Index', fontsize=15)\nplt.ylabel('Deal Probability', fontsize=15)\nplt.title(\"Deal Probability Distribution\", fontsize=20)\nplt.xticks(rotation=45)\nplt.show()\n", "processed": ["", "let start explor distribuit price deal probabl one import element guid explor", "h2 start take look deal probabl target featur h2"]}, {"markdown": ["<h2>Now, let's do a count of this categorical feature to see the distribuition of each value </h2>"], "code": "# Reference: https://www.kaggle.com/code/kabure/extensive-eda-of-deal-probability\n\nprob_cat_percent = df_train[\"deal_prob_cat\"].value_counts() / len(df_train['deal_probability'])* 100\n\nplt.figure(figsize=(12,5))\ng = sns.barplot(prob_cat_percent.index, prob_cat_percent.values)\ng.set_xlabel('The Deal Probability Categorical Dist',fontsize=16)\ng.set_ylabel('% of frequency',fontsize=16)\ng.set_title('Deal Probability Categorical % Frequency',fontsize= 20)\nplt.show()", "processed": ["h2 let count categor featur see distribuit valu h2"]}, {"markdown": ["We can see that that more than 65% of our target have from zero to .02%of Deal Probability", "<h2>Deal Probability x Price Log Distribuition </h2>", "Now, let's use the categorical probability to verify if the price have the same behavior to each value in our categories'"], "code": "# Reference: https://www.kaggle.com/code/kabure/extensive-eda-of-deal-probability\n\ndf_train['price_log'] = np.log(df_train['price'] + 1)\n\nplt.figure(figsize=(12,5))\n\ng = sns.boxplot(x='deal_prob_cat', y='price_log', data=df_train)\ng.set_xlabel('The Deal Probability Categorical Dist',fontsize=16)\ng.set_ylabel('Price Log Dist',fontsize=16)\ng.set_title('Looking the Price Log of each deal_prob_cat',fontsize= 20)\n\nplt.show()", "processed": ["see 65 target zero 02 deal probabl", "h2 deal probabl x price log distribuit h2", "let use categor probabl verifi price behavior valu categori"]}, {"markdown": ["We can see an interesting behavior of price on the lowest deal probabilities category that is different of another two lowest values. Also, we can clearly see that low probabilities have a lowest prices and is the most frequent deal probability interval in the dataset.  Later we will explore this further.", "<h2>Let's take a first look at Price Feature</h2>"], "code": "# Reference: https://www.kaggle.com/code/kabure/extensive-eda-of-deal-probability\n\nplt.figure(figsize=(12,5))\n\nplt.subplot(1,2,1)\ng = sns.distplot(np.log(df_train['price'].dropna() + 1))\ng.set_xlabel('Price Log', fontsize=15)\ng.set_ylabel('Probility', fontsize=15)\ng.set_title(\"Price Histogram\", fontsize=20)\n\nplt.subplot(1,2,2)\nplt.scatter(range(df_train.shape[0]), np.sort(np.log(df_train['price']+1).values))\nplt.xlabel('Index ', fontsize=15)\nplt.ylabel('Price Log', fontsize=15)\nplt.title(\"Price Log Distribution\", fontsize=20)\nplt.xticks(rotation=45)\nplt.show()\n\nplt.show()", "processed": ["see interest behavior price lowest deal probabl categori differ anoth two lowest valu also clearli see low probabl lowest price frequent deal probabl interv dataset later explor", "h2 let take first look price featur h2"]}, {"markdown": ["We can see that a great part of our data is under 10 Price log ", "<h2>Taking a look at user type feature</h2>"], "code": "# Reference: https://www.kaggle.com/code/kabure/extensive-eda-of-deal-probability\n\nprint(\"User Type % Proportion\")\nprint(round(df_train['user_type'].value_counts() / len(df_train) * 100, 2))\n\nplt.figure(figsize=(16,5))\n\nplt.subplot(1,3,1)\ng = sns.countplot(x='user_type', data=df_train, )\ng.set_xlabel('User Type',fontsize=16)\ng.set_ylabel('Count',fontsize=16)\ng.set_title('User Type Count',fontsize= 20)\n\nplt.subplot(1,3,2)\ng1 = sns.boxplot(x='user_type', y='deal_probability', data=df_train)\ng1.set_xlabel('User Type',fontsize=16)\ng1.set_ylabel('Deal probability',fontsize=16)\ng1.set_title('User Type x Prob Dist',fontsize= 20)\n\nplt.subplot(1,3,3)\ng1 = sns.boxplot(x='user_type', y='price_log', data=df_train)\ng1.set_xlabel('User Type',fontsize=16)\ng1.set_ylabel('Price Log',fontsize=16)\ng1.set_title('User Type x Price Log',fontsize= 20)\n\nplt.show()", "processed": ["see great part data 10 price log", "h2 take look user type featur h2"]}, {"markdown": ["Intetresting value distribuition...  We have a highest ", "<h2>Parent Category Name Feature: </h2>\n- Count\n- Crossed with deal prob\n- Crossed with price"], "code": "# Reference: https://www.kaggle.com/code/kabure/extensive-eda-of-deal-probability\n\nplt.figure(figsize=(12,14))\n\nplt.subplot(3,1,1)\ng = sns.countplot(x='parent_category_name_en', data=df_train)\ng.set_xlabel('User Type',fontsize=16)\ng.set_ylabel('Count',fontsize=16)\ng.set_title('Category of Ad',fontsize= 20)\ng.set_xticklabels(g.get_xticklabels(),rotation=45)\n\nplt.subplot(3,1,2)\ng1 = sns.boxplot(x='parent_category_name_en',y='deal_probability', data=df_train)\ng1.set_xlabel(\"Category's Name\",fontsize=16)\ng1.set_ylabel('Deal Probability',fontsize=16)\ng1.set_title('Category of Ad',fontsize= 20)\ng1.set_xticklabels(g.get_xticklabels(),rotation=45)\n\nplt.subplot(3,1,3)\ng2 = sns.boxplot(x='parent_category_name_en', y='price_log', data=df_train)\ng2.set_xlabel(\"Category's Name\",fontsize=16)\ng2.set_ylabel('Price Log',fontsize=16)\ng2.set_title('Category of Ad',fontsize= 20)\ng2.set_xticklabels(g1.get_xticklabels(),rotation=45)\n\nplt.subplots_adjust(hspace = 0.7,top = 0.9)\n\nplt.show()", "processed": ["intetrest valu distribuit highest", "h2 parent categori name featur h2 count cross deal prob cross price"]}, {"markdown": ["Very interesting and meaningful crosstab.", "<h2>Region Feature</h2>\n- Count\n- Crossed with deal prob\n- Crossed with price"], "code": "# Reference: https://www.kaggle.com/code/kabure/extensive-eda-of-deal-probability\n\nplt.figure(figsize=(16,20))\nplt.subplot(3,1,1)\ng = sns.countplot(x='region_en', data=df_train)\ng.set_xlabel('Ad Regions',fontsize=16)\ng.set_ylabel('Count',fontsize=16)\ng.set_title('Ad Regions Count',fontsize= 20)\ng.set_xticklabels(g.get_xticklabels(),rotation=90)\n\nplt.subplot(3,1,2)\ng1 = sns.boxplot(x='region_en', y='deal_probability',data=df_train, orient='')\ng1.set_xlabel('Ad Regions',fontsize=16)\ng1.set_ylabel('Deal Probability',fontsize=16)\ng1.set_title('Ad Regions Deal Prob Distribuition',fontsize= 20)\ng1.set_xticklabels(g1.get_xticklabels(),rotation=90)\n\nplt.subplot(3,1,3)\ng2 = sns.boxplot(x='region_en', y='price_log',data=df_train, orient='')\ng2.set_xlabel('Ad Regions',fontsize=16)\ng2.set_ylabel('Price Log Distribuition',fontsize=16)\ng2.set_title('Ad Regions Price Distribuition',fontsize= 20)\ng2.set_xticklabels(g2.get_xticklabels(),rotation=90)\n\nplt.subplots_adjust(hspace = 0.7,top = 0.9)\n\nplt.show()", "processed": ["interest meaning crosstab", "h2 region featur h2 count cross deal prob cross price"]}, {"markdown": ["## I will do a test showing the count of citys with high and lowest deal probability to we verify if they are the same"], "code": "# Reference: https://www.kaggle.com/code/kabure/extensive-eda-of-deal-probability\n\nlower_probs = ['0 -.02%', '.02%-.05%', '.05-.10']\nhigher_probs = ['.70-.85', '.85+']\n\nplt.figure(figsize=(15,12))\n\nplt.subplot(2,1,1)\ng = sns.countplot(x='region_en', data=df_train[df_train.deal_prob_cat.isin(lower_probs)])\ng.set_title(\"Region with lower deal probs\", fontsize=20)\ng.set_xticklabels(g.get_xticklabels(),rotation=90)\ng.set_xlabel(\"Regions\", fontsize=15)\ng.set_ylabel(\"Count\", fontsize=15)\n\n\nplt.subplot(2,1,2)\ng1 = sns.countplot(x='region_en', data=df_train[df_train.deal_prob_cat.isin(higher_probs)])\ng1.set_title(\"Regions with higher deal probability\", fontsize=20)\ng1.set_xticklabels(g1.get_xticklabels(),rotation=90)\ng1.set_xlabel(\"Regions\",fontsize=15)\ng1.set_ylabel(\"Count\", fontsize=15)\n\nplt.subplots_adjust(hspace = 0.8,top = 0.9)\n\nplt.show()", "processed": ["test show count citi high lowest deal probabl verifi"]}, {"markdown": ["Very interesting graphic.  We can see that the regions are differents and also have different distribuitions", "  ## Taking Advantage, let's take quick look at city feature\n"], "code": "# Reference: https://www.kaggle.com/code/kabure/extensive-eda-of-deal-probability\n\ncity_count = df_train['city'].value_counts()[:35].index.values\n\nplt.figure(figsize=(15,6))\n\ng = sns.countplot(x='city', data=df_train[df_train.city.isin(city_count)])\ng.set_xlabel('Ad Citys',fontsize=16)\ng.set_ylabel('Count',fontsize=16)\ng.set_title(\"Ad City's Count\",fontsize= 20)\ng.set_xticklabels(g.get_xticklabels(),rotation=90)\n\nplt.show()\ncities_top_35 = ['\u041a\u0440\u0430\u0441\u043d\u043e\u0434\u0430\u0440', '\u0415\u043a\u0430\u0442\u0435\u0440\u0438\u043d\u0431\u0443\u0440\u0433', '\u041d\u043e\u0432\u043e\u0441\u0438\u0431\u0438\u0440\u0441\u043a', '\u0420\u043e\u0441\u0442\u043e\u0432-\u043d\u0430-\u0414\u043e\u043d\u0443',\n       '\u041d\u0438\u0436\u043d\u0438\u0439 \u041d\u043e\u0432\u0433\u043e\u0440\u043e\u0434', '\u0427\u0435\u043b\u044f\u0431\u0438\u043d\u0441\u043a', '\u041f\u0435\u0440\u043c\u044c', '\u041a\u0430\u0437\u0430\u043d\u044c', '\u0421\u0430\u043c\u0430\u0440\u0430', '\u041e\u043c\u0441\u043a',\n       '\u0423\u0444\u0430', '\u041a\u0440\u0430\u0441\u043d\u043e\u044f\u0440\u0441\u043a', '\u0412\u043e\u0440\u043e\u043d\u0435\u0436', '\u0412\u043e\u043b\u0433\u043e\u0433\u0440\u0430\u0434', '\u0421\u0430\u0440\u0430\u0442\u043e\u0432', '\u0422\u044e\u043c\u0435\u043d\u044c',\n       '\u041a\u0430\u043b\u0438\u043d\u0438\u043d\u0433\u0440\u0430\u0434', '\u0411\u0430\u0440\u043d\u0430\u0443\u043b', '\u042f\u0440\u043e\u0441\u043b\u0430\u0432\u043b\u044c', '\u0418\u0440\u043a\u0443\u0442\u0441\u043a', '\u041e\u0440\u0435\u043d\u0431\u0443\u0440\u0433', '\u0421\u043e\u0447\u0438',\n       '\u0418\u0436\u0435\u0432\u0441\u043a', '\u0422\u043e\u043b\u044c\u044f\u0442\u0442\u0438', '\u041a\u0435\u043c\u0435\u0440\u043e\u0432\u043e', '\u0411\u0435\u043b\u0433\u043e\u0440\u043e\u0434', '\u0422\u0443\u043b\u0430', '\u0421\u0442\u0430\u0432\u0440\u043e\u043f\u043e\u043b\u044c',\n       '\u041d\u0430\u0431\u0435\u0440\u0435\u0436\u043d\u044b\u0435 \u0427\u0435\u043b\u043d\u044b', '\u041d\u043e\u0432\u043e\u043a\u0443\u0437\u043d\u0435\u0446\u043a', '\u0412\u043b\u0430\u0434\u0438\u043c\u0438\u0440', '\u0421\u0443\u0440\u0433\u0443\u0442', '\u041c\u0430\u0433\u043d\u0438\u0442\u043e\u0433\u043e\u0440\u0441\u043a',\n       '\u041d\u0438\u0436\u043d\u0438\u0439 \u0422\u0430\u0433\u0438\u043b', '\u041d\u043e\u0432\u043e\u0440\u043e\u0441\u0441\u0438\u0439\u0441\u043a']", "processed": ["interest graphic see region differ also differ distribuit", "take advantag let take quick look citi featur"]}, {"markdown": ["## Let's take a look at the distribuition of Deal Probability and Price of the top 35 Cities"], "code": "# Reference: https://www.kaggle.com/code/kabure/extensive-eda-of-deal-probability\n\nplt.figure(figsize=(16,10))\n\nplt.subplot(2,1,1)\ng = sns.boxplot(x='city', y='deal_probability', data=df_train[df_train.city.isin(cities_top_35)])\ng.set_xlabel(\"\", fontsize=15)\ng.set_ylabel(\"Deal Probability\", fontsize=15)\ng.set_title(\"Deal Probability of TOP 35 City's\", fontsize=20)\ng.set_xticklabels(g.get_xticklabels(),rotation=90)\n\nplt.subplot(2,1,2)\ng1 = sns.boxplot(x='city', y='price_log', data=df_train[df_train.city.isin(cities_top_35)])\ng1.set_xlabel(\"TOP 35 City's\", fontsize=15)\ng1.set_ylabel(\"Price Log\", fontsize=15)\ng1.set_title(\"Price Log of TOP 35 City's\", fontsize=20)\ng1.set_xticklabels(g1.get_xticklabels(),rotation=90)\n\nplt.subplots_adjust(hspace = 0.7,top = 0.9)\n\nplt.show()", "processed": ["let take look distribuit deal probabl price top 35 citi"]}, {"markdown": ["We can see that just on city have a different pattern at pricces, but in deal probability we can consider a normal distribuition\n", "<h2> Category name distribuitions </h2>"], "code": "# Reference: https://www.kaggle.com/code/kabure/extensive-eda-of-deal-probability\n\nplt.figure(figsize=(16,12))\n\nplt.subplot(2,1,1)\ng = sns.countplot(x='category_name_en', data=df_train)\ng.set_xticklabels(g.get_xticklabels(),rotation=90)\ng.set_xlabel('Category Names', fontsize=15)\ng.set_ylabel('Count', fontsize=15)\ng.set_title('Category Name Count', fontsize=20)\n\nplt.subplot(2,1,2)\ng1 = sns.boxplot(x='category_name_en', y='price_log', data=df_train)\ng1.set_xticklabels(g1.get_xticklabels(),rotation=90)\ng1.set_xlabel('Category Names', fontsize=15)\ng1.set_ylabel('Price log Dist', fontsize=15)\ng1.set_title('Category Name Count', fontsize=20)\n\nplt.subplots_adjust(hspace = 0.9,top = 0.9)\n\nplt.show()", "processed": ["see citi differ pattern pricc deal probabl consid normal distribuit", "h2 categori name distribuit h2"]}, {"markdown": ["very Interesting and meaningful heat table. I will create a subset of the principal values", "## Now we will know take a look at param_1 feature"], "code": "# Reference: https://www.kaggle.com/code/kabure/extensive-eda-of-deal-probability\n\nparams = df_train.param_1.value_counts().head(35)\n\nparams.index = [\"Women's Clothing\", 'For Girls', 'For Boys', 'Selling',\n                'With mileage', 'Accessories', \"Men's clothing\", 'Other', 'Toys',\n                'Baby carriages', 'Rent', 'Repair, construction', 'Building materials',\n                'iPhone', 'Beds, sofas and armchairs', 'Tools', 'For the kitchen',\n                'Accessories', \"Children's Furniture\", 'Cabinets and Chests', \n                'Devices and accessories', 'For the house', 'Transport, transportation',\n                'Nursing Items', 'Samsung', 'Hire', 'Books',\n                'TVs and projectors', 'Bicycles and scooters',\n                'Interior items, art', 'Other', 'Cosmetics',\n                'Bedding', 'Farm animals', 'Tables and chairs']\nplt.figure(figsize=(16,5))\n\ng = sns.barplot(x=params.index, y=params.values)\ng.set_xlabel(\"Translated params\", fontsize=15)\ng.set_ylabel(\"Count\", fontsize=15)\ng.set_title(\"Most Frequent params in Ads\", fontsize=20)\ng.set_xticklabels(g.get_xticklabels(),rotation=90)\n\nplt.show()", "processed": ["interest meaning heat tabl creat subset princip valu", "know take look param 1 featur"]}, {"markdown": ["## Visualing the top 35 param_1 values by Prices and deal probability"], "code": "# Reference: https://www.kaggle.com/code/kabure/extensive-eda-of-deal-probability\n\nplt.figure(figsize=(16,12))\nplt.subplot(2,1,1)\ng = sns.boxplot(x='param_en', y='price_log', data=subset_param)\ng.set_xlabel(\"\", fontsize=15)\ng.set_ylabel(\"Price Dist(log)\", fontsize=15)\ng.set_title(\"Price of TOP 35 params_1\", fontsize=20)\ng.set_xticklabels(g.get_xticklabels(),rotation=90)\n\nplt.subplot(2,1,2)\ng1 = sns.boxplot(x='param_en', y='deal_probability', data=subset_param)\ng1.set_xlabel(\"TOP 35 Params\", fontsize=15)\ng1.set_ylabel(\"Deal Probability\", fontsize=15)\ng1.set_title(\"Deal Probability of TOP 35 params_1\", fontsize=20)\ng1.set_xticklabels(g.get_xticklabels(),rotation=90)\n\nplt.subplots_adjust(hspace = 0.9,top = 0.9)\n\nplt.show()", "processed": ["visual top 35 param 1 valu price deal probabl"]}, {"markdown": ["## Let's take a look at the Activation Date' - How we have a little number of dates, I will extract just the day."], "code": "# Reference: https://www.kaggle.com/code/kabure/extensive-eda-of-deal-probability\n\ndf_train['day'] = df_train['activation_date'].dt.day\n\ntime_count = df_train['day'].value_counts()\n\nplt.figure(figsize=(16,5))\n\ng = sns.barplot(time_count.index, time_count.values)\ng.set_xlabel(\"Date Distribuition of Dataset\", fontsize=15)\ng.set_ylabel(\"Count\", fontsize=15)\ng.set_title(\"Ads Date of Avitos dataset\", fontsize=20)\n\nplt.show()", "processed": ["let take look activ date littl number date extract day"]}, {"markdown": ["### Plot the distribuition of our new features"], "code": "# Reference: https://www.kaggle.com/code/kabure/extensive-eda-of-deal-probability\n\nplt.figure(figsize = (12,18))\n\nplt.subplot(421)\ng1 = sns.distplot(np.log(df_train['count_word']), \n                  hist=False, label='Title')\ng1 = sns.distplot(np.log(df_train['count_word_desc']), \n                  hist=False, label='Description')\ng1.set_title(\"COUNT WORDS DISTRIBUITION\", fontsize=16)\n\nplt.subplot(422)\ng2 = sns.distplot(np.log(df_train['count_unique_word']),\n                  hist=False, label='Title')\ng2 = sns.distplot(np.log(df_train['count_unique_word_desc']), \n                  hist=False, label='Description')\ng2.set_title(\"COUNT UNIQUE DISTRIBUITION\", fontsize=16)\n\nplt.subplot(423)\ng3 = sns.distplot(np.log(df_train['count_letters']), \n                  hist=False, label='Title')\ng3 = sns.distplot(np.log(df_train['count_letters_desc']), \n                  hist=False, label='Description')\ng3.set_title(\"COUNT LETTERS DISTRIBUITION\", fontsize=16)\n\nplt.subplot(424)\ng4 = sns.distplot(np.log(df_train[\"count_punctuations\"]), \n                  hist=False, label='Title')\ng4 = sns.distplot(np.log(df_train[\"count_punctuations_desc\"]), \n                  hist=False, label='Description')\ng4.set_xlim([-2,50])\ng4.set_title('COUNT PONCTUATIONS DISTRIBUITION', fontsize=16)\n\nplt.subplot(425)\ng5 = sns.distplot(np.log(df_train[\"count_words_upper\"] + 1) , \n                  hist=False, label='Title')\ng5 = sns.distplot(np.log(df_train[\"count_words_upper_desc\"] + 1) , \n                  hist=False, label='Description')\ng5.set_title('COUNT WORDS UPPER DISTRIBUITION', fontsize=16)\n\nplt.subplot(426)\ng6 = sns.distplot(np.log(df_train[\"count_words_title\"] + 1), \n                  hist=False, label='Title')\ng6 = sns.distplot(np.log(df_train[\"count_words_title_desc\"]  + 1), \n                  hist=False, label='Tags')\ng6.set_title('WORDS DISTRIBUITION', fontsize=16)\n\nplt.subplot(427)\ng7 = sns.distplot(np.log(df_train[\"mean_word_len\"]  + 1), \n                  hist=False, label='Title')\ng7 = sns.distplot(np.log(df_train[\"mean_word_len_desc\"] + 1), \n                  hist=False, label='Description')\ng7.set_xlim([-2,100])\ng7.set_title('MEAN WORD LEN DISTRIBUITION', fontsize=16)\n\nplt.subplots_adjust(wspace = 0.2, hspace = 0.4,top = 0.9)\nplt.legend()\nplt.show()", "processed": ["plot distribuit new featur"]}, {"markdown": ["- Cool word clouds =D", "## Let's explore further the description and title features, because might it can be interesting to our purpose'"], "code": "# Reference: https://www.kaggle.com/code/kabure/extensive-eda-of-deal-probability\n\ntitle_freq = df_train.title.value_counts()[:35]\n\ntitle_freq.index = [\"Dress\",\"Shoes\",\"Jacket\",\"Coat\",\"Jeans\",\"Overalls\",\"Sneakers\",\"Costume\",\n                    \"Boots\",\"Sandals\",\"Skirt\",\"Blouse\",\"Windbreaker\",\"I'll rent one apartment\",\n                    \"Boots\",\"Wedding Dress\",\"Shirt\",\"A bag\",\"Stroller\",\"Blouse\",\"Sandals\",\"Sofa\",\n                    \"Pants\",\"Cloak\",\"Ankle Booties\",\"A bike\",\"The plot is 10 hundred. (IZhS)\",\"Sneakers\",\n                    \"Jacket demi-season\",\"Hire a house\",\"Selling dress\",\"A jacket\",\n                    \"I'll rent a 2-room apartment\",\"T-shirt\",\"Footwear\"]\nplt.figure(figsize=(16,6))\n\ng = sns.barplot(title_freq.index, title_freq.values)\ng.set_xlabel(\"TOP 35 Titles\", fontsize=15)\ng.set_ylabel(\"Count\", fontsize=15)\ng.set_title(\"TOP 35 titles frequency\", fontsize=20)\ng.set_xticklabels(g.get_xticklabels(),rotation=90)\n\nplt.show()", "processed": ["cool word cloud", "let explor descript titl featur might interest purpos"]}, {"markdown": ["- The top 5 values are: <br>\n1 - Dress <br>\n2 - Shoes  <br>\n3 - Jacket <br>\n4 - Coat  <br>\n5 - Jeans <br>\n\nIt can clearly show to us that clothing have a high influence in deal probability of Avito's Store", "## Looking the title by deal probability and Price"], "code": "# Reference: https://www.kaggle.com/code/kabure/extensive-eda-of-deal-probability\n\ntitle_freq = df_train.title.value_counts()[:35]\n\nplt.figure(figsize=(16,12))\n\nplt.subplot(2,1,1)\ng = sns.boxplot(x='title', y='price_log', \n                data=df_train[df_train.title.isin(title_freq.index.values)])\ng.set_xlabel(\"\", fontsize=15)\ng.set_ylabel(\"Price Log\", fontsize=15)\ng.set_title(\"TOP 35 titles by Price_log\", fontsize=20)\ng.set_xticklabels(g.get_xticklabels(),rotation=90)\n\nplt.subplot(2,1,2)\ng1 = sns.boxplot(x='title', y='deal_probability', \n                data=df_train[df_train.title.isin(title_freq.index.values)])\ng1.set_xlabel(\"TOP 35 Titles\", fontsize=15)\ng1.set_ylabel(\"Deal Probability\", fontsize=15)\ng1.set_title(\"TOP 35 titles by Deal Probability\", fontsize=20)\ng1.set_xticklabels(g1.get_xticklabels(),rotation=90)\n\nplt.subplots_adjust(hspace = 0.7,top = 0.9)\n\nplt.show()", "processed": ["top 5 valu br 1 dress br 2 shoe br 3 jacket br 4 coat br 5 jean br clearli show u cloth high influenc deal probabl avito store", "look titl deal probabl price"]}, {"markdown": ["Very cool heat table. I will try convert this names to a better understant.", "<h2>Ploting a squarify of Parent Category name and deal probability mean by each category</h2>"], "code": "# Reference: https://www.kaggle.com/code/kabure/extensive-eda-of-deal-probability\n\nimport squarify \n\nplt.figure(figsize = (16,5)) \n\nplt.subplot(1,2,1)\ngrouped_prob_cat = np.log1p(df_train.groupby(['parent_category_name_en']).mean())\ngrouped_prob_cat['cat'] = grouped_prob_cat.index\ncurrent_palette = sns.color_palette()\nsquarify.plot(sizes = grouped_prob_cat['deal_probability'], \\\n              label = grouped_prob_cat.index, alpha = 0.8,color = current_palette)\nplt.rc('font', size = 12)\nplt.axis('off')\nplt.title(\"SQUARIFY OF CATEGORY AND DEAL PROB MEAN \")\n\nplt.subplot(1,2,2)\ngrouped_prob_cat = np.log1p(df_train.groupby(['parent_category_name_en']).mean())\ngrouped_prob_cat['cat'] = grouped_prob_cat.index\ncurrent_palette = sns.color_palette()\nsquarify.plot(sizes = grouped_prob_cat['price'], \\\n              label = grouped_prob_cat.index, alpha = 0.8,color = current_palette)\nplt.rc('font', size = 12)\nplt.axis('off')\nplt.title(\"SQUARIFY OF CATEGORY AND PRICE MEAN \")\nplt.show()", "processed": ["cool heat tabl tri convert name better underst", "h2 plote squarifi parent categori name deal probabl mean categori h2"]}, {"markdown": ["##    Ploting the user_type mean deal probability and mean price by each "], "code": "# Reference: https://www.kaggle.com/code/kabure/extensive-eda-of-deal-probability\n\nplt.figure(figsize = (16,5)) \n\nplt.subplot(1,2,1)\ngrouped_prob_cat = np.log1p(df_train.groupby(['user_type']).mean())\ngrouped_prob_cat['cat'] = grouped_prob_cat.index\ncurrent_palette = sns.color_palette()\nsquarify.plot(sizes = grouped_prob_cat['deal_probability'].values, \n              label = grouped_prob_cat.index, alpha = 0.8,color = current_palette)\nplt.rc('font', size = 12)\nplt.axis('off')\nplt.title(\"SQUARIFY OF USER TYPE filled by Deal probability\")\n\nplt.subplot(1,2,2)\ngrouped_prob_cat = np.log1p(df_train.groupby(['user_type']).sum())\ngrouped_prob_cat['cat'] = grouped_prob_cat.index\ncurrent_palette = sns.color_palette()\nsquarify.plot(sizes = grouped_prob_cat['price'].values, \n              label = grouped_prob_cat.index, alpha = 0.8,color = current_palette)\nplt.rc('font', size = 12)\nplt.axis('off')\nplt.title(\"SQUARIFY weighted by Mean Price\")\n\nplt.show()", "processed": ["plote user type mean deal probabl mean price"]}, {"markdown": ["## Data overview"], "code": "# Reference: https://www.kaggle.com/code/artgor/basic-eda-and-baseline-pytorch-model\n\ntrain = pd.read_csv('../input/aptos2019-blindness-detection/train.csv')\ntest = pd.read_csv('../input/aptos2019-blindness-detection/test.csv')\nsample_submission = pd.read_csv('../input/aptos2019-blindness-detection/sample_submission.csv')\ntrain.head()\ntrain['diagnosis'].value_counts().plot(kind='bar');\nplt.title('Class counts');", "processed": ["data overview"]}, {"markdown": ["We have a slight disbalance in data."], "code": "# Reference: https://www.kaggle.com/code/artgor/basic-eda-and-baseline-pytorch-model\n\nfig = plt.figure(figsize=(25, 16))\n# display 10 images from each class\nfor class_id in sorted(train['diagnosis'].unique()):\n    for i, (idx, row) in enumerate(train.loc[train['diagnosis'] == class_id].sample(10).iterrows()):\n        ax = fig.add_subplot(5, 10, class_id * 10 + i + 1, xticks=[], yticks=[])\n        im = Image.open(f\"../input/aptos2019-blindness-detection/train_images/{row['id_code']}.png\")\n        plt.imshow(im)\n        ax.set_title(f'Label: {class_id}')", "processed": ["slight disbal data"]}, {"markdown": ["## 6. Models comparison <a class=\"anchor\" id=\"6\"></a>\n\n[Back to Table of Contents](#0.1)", "We can now compare our models and to choose the best one for our problem."], "code": "# Reference: https://www.kaggle.com/code/vbmokin/qa-prediction-answer-plausible\n\nmodels = pd.DataFrame({\n    'Model': ['Linear Regression', 'Support Vector Machines', 'Linear SVR', \n              'MLPRegressor', 'Stochastic Gradient Decent', \n              'Decision Tree Regressor', 'Random Forest',  'XGB', 'LGBM',\n              'GradientBoostingRegressor', 'RidgeRegressor', 'BaggingRegressor', 'ExtraTreesRegressor', \n              'AdaBoostRegressor', 'VotingRegressor'],\n    \n    'r2_train': acc_train_r2,\n    'r2_test': acc_test_r2,\n    'd_train': acc_train_d,\n    'd_test': acc_test_d,\n    'rmse_train': acc_train_rmse,\n    'rmse_test': acc_test_rmse\n                     })\npd.options.display.float_format = '{:,.2f}'.format\nprint('Prediction accuracy for models by R2 criterion - r2_test')\nmodels.sort_values(by=['r2_test', 'r2_train'], ascending=False)\nprint('Prediction accuracy for models by relative error - d_test')\nmodels.sort_values(by=['d_test', 'd_train'], ascending=True)\nprint('Prediction accuracy for models by RMSE - rmse_test')\nmodels.sort_values(by=['rmse_test', 'rmse_train'], ascending=True)\n# Plot\nplt.figure(figsize=[25,6])\nxx = models['Model']\nplt.tick_params(labelsize=14)\nplt.plot(xx, models['r2_train'], label = 'r2_train')\nplt.plot(xx, models['r2_test'], label = 'r2_test')\nplt.legend()\nplt.title('R2-criterion for 15 popular models for train and test datasets')\nplt.xlabel('Models')\nplt.ylabel('R2-criterion, %')\nplt.xticks(xx, rotation='vertical')\nplt.savefig('graph.png')\nplt.show()\n# Plot\nplt.figure(figsize=[25,6])\nxx = models['Model']\nplt.tick_params(labelsize=14)\nplt.plot(xx, models['d_train'], label = 'd_train')\nplt.plot(xx, models['d_test'], label = 'd_test')\nplt.legend()\nplt.title('Relative errors for 15 popular models for train and test datasets')\nplt.xlabel('Models')\nplt.ylabel('Relative error, %')\nplt.xticks(xx, rotation='vertical')\nplt.savefig('graph.png')\nplt.show()\n# Plot\nplt.figure(figsize=[25,6])\nxx = models['Model']\nplt.tick_params(labelsize=14)\nplt.plot(xx, models['rmse_train'], label = 'rmse_train')\nplt.plot(xx, models['rmse_test'], label = 'rmse_test')\nplt.legend()\nplt.title('RMSE for 15 popular models for train and test datasets')\nplt.xlabel('Models')\nplt.ylabel('RMSE, %')\nplt.xticks(xx, rotation='vertical')\nplt.savefig('graph.png')\nplt.show()", "processed": ["6 model comparison class anchor id 6 back tabl content 0 1", "compar model choos best one problem"]}, {"markdown": ["# First, lets explore the distribution of the target\nI feel that most of the time a histogram is the best way to get an understanding of a single feature."], "code": "# Reference: https://www.kaggle.com/code/robikscube/exploring-the-target-scalar-coupling-constant\n\n# Distribution of the target\ntrain_df['scalar_coupling_constant'].plot(kind='hist', figsize=(20, 5), bins=1000, title='Distribution of the scalar coupling constant (target)')\nplt.show()", "processed": ["first let explor distribut target feel time histogram best way get understand singl featur"]}, {"markdown": ["## Plot Target by Observation Kind"], "code": "# Reference: https://www.kaggle.com/code/robikscube/exploring-the-target-scalar-coupling-constant\n\ncolor_index = 0\naxes_index = 0\nfig, axes = plt.subplots(8, 1, figsize=(20, 30), sharex=True)\nfor mtype, d in train_df.groupby('type'):\n    d['scalar_coupling_constant'].plot(kind='hist',\n                  bins=1000,\n                  title='Distribution of Distance Feature for {}'.format(mtype),\n                  color=color_pal[color_index],\n                  ax=axes[axes_index])\n    if color_index == 6:\n        color_index = 0\n    else:\n        color_index += 1\n    axes_index += 1\nplt.show()", "processed": ["plot target observ kind"]}, {"markdown": ["## Distributions in the same plot.\nIt's pretty clear that the target is closely related to the type of coupling. All of the ~80+ `scalar_coupling_constant` values are type: 1JHC."], "code": "# Reference: https://www.kaggle.com/code/robikscube/exploring-the-target-scalar-coupling-constant\n\ntrain_df.groupby('type')['scalar_coupling_constant'].plot(kind='hist',\n                                                          bins=1000,\n                                                          figsize=(20, 5),\n                                                          alpha=0.8,\n                                                         title='scalar_coupling_constant by coupling type')\nplt.legend()\nplt.show()", "processed": ["distribut plot pretti clear target close relat type coupl 80 scalar coupl constant valu type 1jhc"]}, {"markdown": ["# NOTE\nI know what are you thinking, how much sense does it make to take a mean of a moving average and other derivations of it but according to the feature importance of lbgm it does. In a contrast if we were to take all 150 000 rows and calculate what we have calculated for each of them. Than (approximately) 9 minutes times 150 000 is around\n# **ONE LIFETIME**\n worth of time of calculations. Now I do understand that it is actually not that since we are taking the mean for every one ofthe variables etc.. but STILL it is a long time!", "<a id=\"3\"></a> <br>\n# **Model**\n**Model** xbgoost, lgbm and NuSVR with some parameter tweeking"], "code": "# Reference: https://www.kaggle.com/code/zikazika/useful-new-features-and-a-optimised-model\n\nn_fold = 5\nfolds = KFold(n_splits=n_fold, shuffle=True, random_state=11)\ndef train_model(X=X_train_scaled, X_test=X_test_scaled, y=y_tr, params=None, folds=folds, model_type='lgb', plot_feature_importance=False, model=None):\n\n    oof = np.zeros(len(X))\n    prediction = np.zeros(len(X_test))\n    scores = []\n    feature_importance = pd.DataFrame()\n    for fold_n, (train_index, valid_index) in enumerate(folds.split(X)):\n        print('Fold', fold_n, 'started at', time.ctime())\n        X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n        y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n        \n        if model_type == 'lgb':\n            model = lgb.LGBMRegressor(**params, n_estimators = 50000, n_jobs = -1)\n            model.fit(X_train, y_train, \n                    eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric='mae',\n                    verbose=10000, early_stopping_rounds=200)\n            \n            y_pred_valid = model.predict(X_valid)\n            y_pred = model.predict(X_test, num_iteration=model.best_iteration_)\n            \n        if model_type == 'xgb':\n            train_data = xgb.DMatrix(data=X_train, label=y_train, feature_names=X_tr.columns)\n            valid_data = xgb.DMatrix(data=X_valid, label=y_valid, feature_names=X_tr.columns)\n\n            watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]\n            model = xgb.train(dtrain=train_data, num_boost_round=20000, evals=watchlist, early_stopping_rounds=200, verbose_eval=500, params=params)\n            y_pred_valid = model.predict(xgb.DMatrix(X_valid, feature_names=X_tr.columns), ntree_limit=model.best_ntree_limit)\n            y_pred = model.predict(xgb.DMatrix(X_test, feature_names=X_tr.columns), ntree_limit=model.best_ntree_limit)\n            \n        if model_type == 'rcv':\n            model = RidgeCV(alphas=(0.01, 0.1, 1.0, 10.0, 100.0, 1000.0), scoring='neg_mean_absolute_error', cv=5)\n            model.fit(X_train, y_train)\n            print(model.alpha_)\n\n            y_pred_valid = model.predict(X_valid).reshape(-1,)\n            score = mean_absolute_error(y_valid, y_pred_valid)\n            print(f'Fold {fold_n}. MAE: {score:.4f}.')\n            print('')\n            \n            y_pred = model.predict(X_test).reshape(-1,)\n        \n        if model_type == 'sklearn':\n            model = model\n            model.fit(X_train, y_train)\n            \n            y_pred_valid = model.predict(X_valid).reshape(-1,)\n            score = mean_absolute_error(y_valid, y_pred_valid)\n            print(f'Fold {fold_n}. MAE: {score:.4f}.')\n            print('')\n            \n            y_pred = model.predict(X_test).reshape(-1,)\n        \n        if model_type == 'cat':\n            model = CatBoostRegressor(iterations=20000,  eval_metric='MAE', **params)\n            model.fit(X_train, y_train, eval_set=(X_valid, y_valid), cat_features=[], use_best_model=True, verbose=False)\n\n            y_pred_valid = model.predict(X_valid)\n            y_pred = model.predict(X_test)\n        \n        oof[valid_index] = y_pred_valid.reshape(-1,)\n        scores.append(mean_absolute_error(y_valid, y_pred_valid))\n\n        prediction += y_pred    \n        \n        if model_type == 'lgb':\n            # feature importance\n            fold_importance = pd.DataFrame()\n            fold_importance[\"feature\"] = X.columns\n            fold_importance[\"importance\"] = model.feature_importances_\n            fold_importance[\"fold\"] = fold_n + 1\n            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n\n    prediction /= n_fold\n    \n    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n    \n    if model_type == 'lgb':\n        feature_importance[\"importance\"] /= n_fold\n        if plot_feature_importance:\n            cols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n                by=\"importance\", ascending=False)[:50].index\n\n            best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n\n            plt.figure(figsize=(16, 12));\n            sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False));\n            plt.title('LGB Features (avg over folds)');\n        \n            return oof, prediction, feature_importance\n        return oof, prediction\n    \n    else:\n        return oof, prediction\nfrom bayes_opt import BayesianOptimization", "processed": ["note know think much sen make take mean move averag deriv accord featur import lbgm contrast take 150 000 row calcul calcul approxim 9 minut time 150 000 around one lifetim worth time calcul understand actual sinc take mean everi one ofth variabl etc still long time", "id 3 br model model xbgoost lgbm nusvr paramet tweek"]}, {"markdown": ["Looks okay to me. Also since the metric is RMSLE, I think it is okay to have it as such. However if needed, one can truncate the high values. \n\nWe can now bin the 'price_doc' and plot it."], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-notebook-sberbank\n\nplt.figure(figsize=(12,8))\nsns.distplot(train_df.price_doc.values, bins=50, kde=True)\nplt.xlabel('price', fontsize=12)\nplt.show()", "processed": ["look okay also sinc metric rmsle think okay howev need one truncat high valu bin price doc plot"]}, {"markdown": ["Certainly a very long right tail. Since our metric is Root Mean Square **Logarithmic** error, let us plot the log of price_doc variable."], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-notebook-sberbank\n\nplt.figure(figsize=(12,8))\nsns.distplot(np.log(train_df.price_doc.values), bins=50, kde=True)\nplt.xlabel('price', fontsize=12)\nplt.show()", "processed": ["certainli long right tail sinc metric root mean squar logarithm error let u plot log price doc variabl"]}, {"markdown": ["This looks much better than the previous one. \n\nNow let us see how the median housing price change with time. "], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-notebook-sberbank\n\ntrain_df['yearmonth'] = train_df['timestamp'].apply(lambda x: x[:4]+x[5:7])\ngrouped_df = train_df.groupby('yearmonth')['price_doc'].aggregate(np.median).reset_index()\nplt.figure(figsize=(12,8))\nsns.barplot(grouped_df.yearmonth.values, grouped_df.price_doc.values, alpha=0.8, color=color[2])\nplt.ylabel('Median Price', fontsize=12)\nplt.xlabel('Year Month', fontsize=12)\nplt.xticks(rotation='vertical')\nplt.show()", "processed": ["look much better previou one let u see median hous price chang time"]}, {"markdown": ["So the top 5 variables and their description from the data dictionary are:\n\n 1. full_sq - total area in square meters, including loggias, balconies and other non-residential areas\n 2. life_sq - living area in square meters, excluding loggias, balconies and other non-residential areas\n 3. floor - for apartments, floor of the building\n 4. max_floor - number of floors in the building\n 5. build_year - year built\n\nNow let us see how these important variables are distributed with respect to target variable.\n\n**Total area in square meters:**"], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-notebook-sberbank\n\nulimit = np.percentile(train_df.price_doc.values, 99.5)\nllimit = np.percentile(train_df.price_doc.values, 0.5)\ntrain_df['price_doc'].ix[train_df['price_doc']>ulimit] = ulimit\ntrain_df['price_doc'].ix[train_df['price_doc']<llimit] = llimit\n\ncol = \"full_sq\"\nulimit = np.percentile(train_df[col].values, 99.5)\nllimit = np.percentile(train_df[col].values, 0.5)\ntrain_df[col].ix[train_df[col]>ulimit] = ulimit\ntrain_df[col].ix[train_df[col]<llimit] = llimit\n\nplt.figure(figsize=(12,12))\nsns.jointplot(x=np.log1p(train_df.full_sq.values), y=np.log1p(train_df.price_doc.values), size=10)\nplt.ylabel('Log of Price', fontsize=12)\nplt.xlabel('Log of Total area in square metre', fontsize=12)\nplt.show()", "processed": ["top 5 variabl descript data dictionari 1 full sq total area squar meter includ loggia balconi non residenti area 2 life sq live area squar meter exclud loggia balconi non residenti area 3 floor apart floor build 4 max floor number floor build 5 build year year built let u see import variabl distribut respect target variabl total area squar meter"]}, {"markdown": ["**Living area in square meters:**"], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-notebook-sberbank\n\ncol = \"life_sq\"\ntrain_df[col].fillna(0, inplace=True)\nulimit = np.percentile(train_df[col].values, 95)\nllimit = np.percentile(train_df[col].values, 5)\ntrain_df[col].ix[train_df[col]>ulimit] = ulimit\ntrain_df[col].ix[train_df[col]<llimit] = llimit\n\nplt.figure(figsize=(12,12))\nsns.jointplot(x=np.log1p(train_df.life_sq.values), y=np.log1p(train_df.price_doc.values), \n              kind='kde', size=10)\nplt.ylabel('Log of Price', fontsize=12)\nplt.xlabel('Log of living area in square metre', fontsize=12)\nplt.show()", "processed": ["live area squar meter"]}, {"markdown": ["**Floor:**\n\nWe will see the count plot of floor variable."], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-notebook-sberbank\n\nplt.figure(figsize=(12,8))\nsns.countplot(x=\"floor\", data=train_df)\nplt.ylabel('Count', fontsize=12)\nplt.xlabel('floor number', fontsize=12)\nplt.xticks(rotation='vertical')\nplt.show()", "processed": ["floor see count plot floor variabl"]}, {"markdown": ["The distribution is right skewed. There are some good drops in between (5 to 6, 9 to 10, 12 to 13, 17 to 18). Now let us see how the price changes with respect to floors."], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-notebook-sberbank\n\ngrouped_df = train_df.groupby('floor')['price_doc'].aggregate(np.median).reset_index()\nplt.figure(figsize=(12,8))\nsns.pointplot(grouped_df.floor.values, grouped_df.price_doc.values, alpha=0.8, color=color[2])\nplt.ylabel('Median Price', fontsize=12)\nplt.xlabel('Floor number', fontsize=12)\nplt.xticks(rotation='vertical')\nplt.show()", "processed": ["distribut right skew good drop 5 6 9 10 12 13 17 18 let u see price chang respect floor"]}, {"markdown": ["This shows an overall increasing trend (individual houses seems to be costlier as well - check price of 0 floor houses). \nA sudden increase in the house price is also observed at floor 18.\n\n**Max floor:**\n\nTotal number of floors in the building is one another important variable. So let us plot that one and see."], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-notebook-sberbank\n\nplt.figure(figsize=(12,8))\nsns.countplot(x=\"max_floor\", data=train_df)\nplt.ylabel('Count', fontsize=12)\nplt.xlabel('Max floor number', fontsize=12)\nplt.xticks(rotation='vertical')\nplt.show()", "processed": ["show overal increas trend individu hous seem costlier well check price 0 floor hous sudden increas hous price also observ floor 18 max floor total number floor build one anoth import variabl let u plot one see"]}, {"markdown": ["We could see that there are few tall bars in between (at 5,9,12,17 - similar to drop in floors in the previous graph). May be there are some norms / restrictions on the number of maximum floors present(?). \n\nNow let us see how the median prices vary with the max floors. "], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-notebook-sberbank\n\nplt.figure(figsize=(12,8))\nsns.boxplot(x=\"max_floor\", y=\"price_doc\", data=train_df)\nplt.ylabel('Median Price', fontsize=12)\nplt.xlabel('Max Floor number', fontsize=12)\nplt.xticks(rotation='vertical')\nplt.show()", "processed": ["could see tall bar 5 9 12 17 similar drop floor previou graph may norm restrict number maximum floor present let u see median price vari max floor"]}, {"markdown": ["# Tracking the Public Leaderboard for the NFL Competition"], "code": "import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport matplotlib.pylab as plt\nimport plotly\nimport plotly.graph_objs as go\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\nfrom sklearn.linear_model import LinearRegression\nimport datetime\nimport colorlover as cl\nplt.style.use('ggplot')\ncolor_pal = [x['color'] for x in plt.rcParams['axes.prop_cycle']]\n\n# Format the data\ndf = pd.read_csv('../input/nflbigdatabowl2020lb/nfl-big-data-bowl-2020-publicleaderboard_11_16_2019.csv')\ndf['SubmissionDate'] = pd.to_datetime(df['SubmissionDate'])\ndf = df.set_index(['TeamName','SubmissionDate'])['Score'].unstack(-1).T\ndf.columns = [name for name in df.columns]\n\nFIFTEENTH_SCORE = df.min().sort_values(ascending=True)[15]\nFIFTYTH_SCORE = df.min().sort_values(ascending=True)[50]\nTOP_SCORE = df.min().sort_values(ascending=True)[0]\n# Interative Plotly\nmypal = cl.scales['9']['div']['Spectral']\ncolors = cl.interp( mypal, 15 )\nannotations = []\ninit_notebook_mode(connected=True)\nTOP_TEAMS = df.min().loc[df.min() < FIFTEENTH_SCORE].index.values\ndf_filtered = df[TOP_TEAMS].ffill()\ndf_filtered = df_filtered.iloc[df_filtered.index >= '10-01-2019']\nteam_ordered = df_filtered.min(axis=0) \\\n    .sort_values(ascending=True).index.tolist()\n\ndata = []\ni = 0\nfor col in df_filtered[team_ordered].columns:\n    data.append(go.Scatter(\n                        x = df_filtered.index,\n                        y = df_filtered[col],\n                        name=col,\n                        line=dict(color=colors[i], width=2),)\n               )\n    i += 1\n\nannotations.append(dict(xref='paper', yref='paper', x=0.0, y=1.05,\n                              xanchor='left', yanchor='bottom',\n                              text='NFL Big Data Bowl Leaderboard',\n                              font=dict(family='Arial',\n                                        size=30,\n                                        color='rgb(37,37,37)'),\n                              showarrow=False))\n\nlayout = go.Layout(yaxis=dict(range=[TOP_SCORE-0.0001, 0.015]),\n                   hovermode='x',\n                   plot_bgcolor='white',\n                  annotations=annotations,\n                  )\nfig = go.Figure(data=data, layout=layout)\nfig.update_layout(\n    legend=go.layout.Legend(\n        traceorder=\"normal\",\n        font=dict(\n            family=\"sans-serif\",\n            size=12,\n            color=\"black\"\n        ),\n        bgcolor=\"LightSteelBlue\",\n        bordercolor=\"Black\",\n        borderwidth=2,\n    )\n)\n\nfig.update_layout(legend_orientation=\"h\")\nfig.update_layout(template=\"plotly_white\")\n#fig.update_yaxes(showgrid=True, gridwidth=0.5, gridcolor='LightGrey')\nfig.update_xaxes(showgrid=False)\n\niplot(fig)\n# Scores of top teams over time\nplt.rcParams[\"font.size\"] = \"12\"\nALL_TEAMS = df.columns.values\ndf_ffill = df[ALL_TEAMS].ffill()\n\n# This is broken\ndf_ffill.plot(figsize=(20, 10),\n                           color=color_pal[0],\n                           legend=False,\n                           alpha=0.05,\n                           ylim=(TOP_SCORE-0.0001, 0.02),\n                           title='All Teams Public LB Scores over Time')\n\ndf.ffill().min(axis=1).plot(color=color_pal[1], label='1st Place Public LB', legend=True)\nplt.show()", "processed": ["track public leaderboard nfl competit"]}, {"markdown": ["## Teams By Date"], "code": "# Reference: https://www.kaggle.com/code/robikscube/the-race-for-nfl-big-data-bowl-2020\n\nplt.rcParams[\"font.size\"] = \"13\"\nax = df.ffill() \\\n    .count(axis=1) \\\n    .plot(figsize=(20, 8),\n          title='Number of Teams in the Competition by Date',\n          color=color_pal[5], lw=5)\nax.set_ylabel('Number of Teams')\n#ax.set_ylim('2019-10-01','2019-11-30')\nplt.axvline('11-20-2019', color='orange', linestyle='-.')\n#plt.text('11-20-2019', 0.1,'Merger Deadline',rotation=-90)\nplt.axvline('11-27-2019', color='orange', linestyle='-.')\n#plt.text('11-27-2019', 40,'Deadline',rotation=-90)\nplt.show()\nplt.style.use('ggplot')\nplt.rcParams[\"font.size\"] = \"25\"\nteam_over_time = df.ffill() \\\n    .count(axis=1)\n\nlr = LinearRegression()\n_ = lr.fit(np.array(pd.to_numeric(team_over_time.index).tolist()).reshape(-1, 1),\n           team_over_time.values)\n\nteamcount_df = pd.DataFrame(team_over_time)\n\nteamcount_pred_df = pd.DataFrame(index=pd.date_range('10-09-2019','11-30-2019'))\nteamcount_pred_df['Forecast Using All Data'] = lr.predict(np.array(pd.to_numeric(teamcount_pred_df.index).tolist()).reshape(-1, 1))\n\nlr = LinearRegression()\n_ = lr.fit(np.array(pd.to_numeric(team_over_time[-100:].index).tolist()).reshape(-1, 1),\n           team_over_time[-100:].values)\n\nteamcount_pred_df['Forecast Using Recent Data'] = lr.predict(np.array(pd.to_numeric(teamcount_pred_df.index).tolist()).reshape(-1, 1))\n\nplt.rcParams[\"font.size\"] = \"12\"\nax =df.ffill() \\\n    .count(axis=1) \\\n    .plot(figsize=(20, 8),\n          title='Forecasting the Final Number of Teams',\n         color=color_pal[5], lw=5,\n         xlim=('10-01-2019','11-30-2019'))\nteamcount_pred_df['Forecast Using All Data'].plot(ax=ax, style='.-.', alpha=0.5, label='Regression Using All Data')\nteamcount_pred_df['Forecast Using Recent Data'].plot(ax=ax, style='.-.', alpha=0.5, label='Regression Using last 1000 observations')\nax.set_ylabel('Number of Teams')\nteamcount_pred_df.plot(ax=ax, style='.-.', alpha=0.5)\nplt.axvline('11-20-2019', color='orange', linestyle='-.')\nplt.text('11-20-2019', 900,'Merger Deadline',rotation=-90)\nplt.axvline('11-27-2019', color='orange', linestyle='-.')\nplt.text('11-27-2019', 500,'Deadline',rotation=-90)\nplt.show()", "processed": ["team date"]}, {"markdown": ["# How many samples for each class are there in the dataset?"], "code": "# Reference: https://www.kaggle.com/code/mobassir/in-depth-melanoma-with-modeling\n\n# Get the counts for each class\ncases_count = df_train['target'].value_counts()\nprint(cases_count)\n\n# Plot the results \nplt.figure(figsize=(10,8))\nsns.barplot(x=cases_count.index, y= cases_count.values)\nplt.title('Number of cases', fontsize=14)\nplt.xlabel('Case type', fontsize=12)\nplt.ylabel('Count', fontsize=12)\nplt.xticks(range(len(cases_count.index)), ['Normal(0)', 'melanoma(1)'])\nplt.show()", "processed": ["mani sampl class dataset"]}, {"markdown": ["Lets use plotly to make interactive plot counting the number of words for each of the 3 authors."], "code": "# Reference: https://www.kaggle.com/code/zikazika/tutorial-on-topic-modelling-lda-nlp\n\nz = {'EAP': 'Edgar Allen Poe', 'MWS': 'Mary Shelley', 'HPL': 'HP Lovecraft'}\ndata = [go.Bar(\n            x = train.author.map(z).unique(),\n            y = train.author.value_counts().values,\n            marker= dict(colorscale='Viridis',\n                         color = train.author.value_counts().values\n                        ),\n            text='Text entries attributed to Author'\n    )]\n\nlayout = go.Layout(\n    title='Target variable distribution'\n)\n\nfig = go.Figure(data=data, layout=layout)\n\npy.iplot(fig, filename='basic-bar')", "processed": ["let use plotli make interact plot count number word 3 author"]}, {"markdown": ["Now that we have dataset in desired form, i.e. counts of data for every word, now we can use it to plot the most frequent ones."], "code": "# Reference: https://www.kaggle.com/code/zikazika/tutorial-on-topic-modelling-lda-nlp\n\nall_words = train['text'].str.split(expand=True).unstack().value_counts()\ndata = [go.Bar(\n            x = all_words.index.values[2:50],\n            y = all_words.values[2:50],\n            marker= dict(colorscale='Viridis',\n                         color = all_words.values[2:100]\n                        ),\n            text='Word counts'\n    )]\n\nlayout = go.Layout(\n    title='Top 50 (Uncleaned) Word frequencies in the training dataset'\n)\n\nfig = go.Figure(data=data, layout=layout)\n\npy.iplot(fig, filename='basic-bar')", "processed": ["dataset desir form e count data everi word use plot frequent one"]}, {"markdown": ["Now for the worldcloud, notice that hpl is for example our text from above (one of the 3 authors). It is one-liner to implement it, and mask changes the image."], "code": "# Reference: https://www.kaggle.com/code/zikazika/tutorial-on-topic-modelling-lda-nlp\n\n# The wordcloud of Cthulhu/squidy thing for HP Lovecraft\nplt.figure(figsize=(16,13))\nwc = WordCloud(background_color=\"black\", max_words=10000, \n               mask=hcmask3, stopwords=STOPWORDS, max_font_size= 40)\nwc.generate(\" \".join(hpl))\nplt.title(\"HP Lovecraft (Cthulhu-Squidy)\", fontsize=20)\n# plt.imshow(wc.recolor( colormap= 'Pastel1_r' , random_state=17), alpha=0.98)\nplt.imshow(wc.recolor( colormap= 'pink_r' , random_state=17), alpha=0.98)\nplt.axis('off')\nplt.figure(figsize=(20,18))\n# The wordcloud of the raven for Edgar Allen Poe\nplt.subplot(211)\nwc = WordCloud(background_color=\"black\", \n               max_words=10000, \n               mask=hcmask, \n               stopwords=STOPWORDS, \n               max_font_size= 40)\nwc.generate(\" \".join(eap))\nplt.title(\"Edgar Allen Poe (The Raven)\")\nplt.imshow(wc.recolor( colormap= 'PuBu' , random_state=17), alpha=0.9)\nplt.axis('off')\nplt.figure(figsize=(20,18))\nwc = WordCloud(background_color=\"black\", \n               max_words=10000, \n               mask=hcmask2, \n               stopwords=STOPWORDS, \n               max_font_size= 40)\nwc.generate(\" \".join(mws))\nplt.title(\"Mary Shelley (Frankenstein's Monster)\", fontsize= 18)\nplt.imshow(wc.recolor( colormap= 'pink_r' , random_state=17), alpha=0.9)\nplt.axis('off')", "processed": ["worldcloud notic hpl exampl text one 3 author one liner implement mask chang imag"]}, {"markdown": ["Now that we have done the pre-processing we can view frequency of our words, and really have some predictive power. Top 100 and bottom 50 plots, again with interactive feature plotly"], "code": "# Reference: https://www.kaggle.com/code/zikazika/tutorial-on-topic-modelling-lda-nlp\n\nfeature_names = tf_vectorizer.get_feature_names()\ncount_vec = np.asarray(tf.sum(axis=0)).ravel()\nzipped = list(zip(feature_names, count_vec))\nx, y = (list(x) for x in zip(*sorted(zipped, key=lambda x: x[1], reverse=True)))\n# Now I want to extract out on the top 15 and bottom 15 words\nY = np.concatenate([y[0:15], y[-16:-1]])\nX = np.concatenate([x[0:15], x[-16:-1]])\n\n# Plotting the Plot.ly plot for the Top 50 word frequencies\ndata = [go.Bar(\n            x = x[0:50],\n            y = y[0:50],\n            marker= dict(colorscale='Viridis',\n                         color = y[0:50]\n                        ),\n            text='Word counts'\n    )]\n\nlayout = go.Layout(\n    title='Top 50 Word frequencies after Preprocessing'\n)\n\nfig = go.Figure(data=data, layout=layout)\n\npy.iplot(fig, filename='basic-bar')\n\n# Plotting the Plot.ly plot for the Top 50 word frequencies\ndata = [go.Bar(\n            x = x[-100:],\n            y = y[-100:],\n            marker= dict(colorscale='Jet',\n                         color = y[-100:]\n                        ),\n            text='Word counts'\n    )]\n\nlayout = go.Layout(\n    title='Bottom 100 Word frequencies after Preprocessing'\n)\n\nfig = go.Figure(data=data, layout=layout)\n\npy.iplot(fig, filename='basic-bar')", "processed": ["done pre process view frequenc word realli predict power top 100 bottom 50 plot interact featur plotli"]}, {"markdown": ["# Google Safe Browsing Report\n[Google][1] publishes information about malware on the internet over time. Let's load this externel dataset, display some rows, merge it with `df_train`, and perform EDA.  \n  \n[1]: https://transparencyreport.google.com/safe-browsing/overview\n"], "code": "# Reference: https://www.kaggle.com/code/cdeotte/external-data-malware-0-50\n\ndata = pd.read_csv('../input/google-safe-browsing-transparency-report-data/data.csv')\ndata['WeekOf'] = data['WeekOf'].map(lambda x: datetime.strptime(x,'%Y-%m-%d').date())\ndatedictAS = np.load('../input/malware-timestamps/AvSigVersionTimestamps.npy')[()]\nweekdictAS={}\nfor x in datedictAS: \n    weekdictAS[x] = (datedictAS[x] - timedelta(days= -7+1+datedictAS[x].weekday())).date()\ndf_train['WeekOf'] = df_train['AvSigVersion'].map(weekdictAS)\ndf_train = pd.merge(df_train, data, on='WeekOf', how='left')\nprint('GOOGLE DATA')\ndata.sample(5)\nimport math, calendar\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\n\n# PARAMETERS\n# data : pandas.DataFrame : your data to plot\n# col  : str : which column to plot histogram for left y-axis\n# target : str : which column for mean/rate on right y-axis\n# bars : int : how many histogram bars to show (or less if you set show or min)\n# show : float : stop displaying bars after 100*show% of data is showing\n# minn : float : don't display bars containing under 100*minn% of data\n# sortby : str : either 'frequency', 'category', or 'rate'\n# verbose : int : display text summary 1=yes, 0=no\n# top : int : give this many bars nice color (and matches a subsequent dynamicPlot)\n# title : str : title of plot\n# asc : boolean : sort ascending (for category and rate)\n# dropna : boolean : include missing data as a category or not\n\ndef staticPlot(data, col, target='HasDetections', bars=10, show=1.0, sortby='frequency'\n               , verbose=1, top=5, title='',asc=False, dropna=False, minn=0.0):\n    # calcuate density and detection rate\n    cv = data[col].value_counts(dropna=dropna)\n    cvd = cv.to_dict()\n    nm = cv.index.values; lnn = len(nm); lnn2 = lnn\n    th = show * len(data)\n    th2 = minn * len(data)\n    sum = 0; lnn2 = 0\n    for x in nm[0:bars]:\n        lnn2 += 1\n        try: sum += cvd[x]\n        except: sum += cv[x]\n        if sum>th:\n            break\n        try:\n            if cvd[x]<th2: break\n        except:\n            if cv[x]<th2: break\n    if lnn2<bars: bars = lnn2\n    pct = round(100.0*sum/len(data),2)\n    lnn = min(lnn,lnn2)\n    ratio = [0.0]*lnn; lnn3 = lnn\n    if sortby =='frequency': lnn3 = min(lnn3,bars)\n    elif sortby=='category': lnn3 = 0\n    for i in range(lnn3):\n        if target not in data:\n            ratio[i] = np.nan\n        elif nan_check(nm[i]):\n            ratio[i] = data[target][data[col].isna()].mean()\n        else:\n            ratio[i] = data[target][data[col]==nm[i]].mean()\n    try: all = pd.DataFrame( {'category':nm[0:lnn],'frequency':[cvd[x] for x in nm[0:lnn]],'rate':ratio} )\n    except: all = pd.DataFrame( {'category':nm[0:lnn],'frequency':[cv[x] for x in nm[0:lnn]],'rate':ratio} )\n    if sortby=='rate': \n        all = all.sort_values(sortby, ascending=asc)\n    elif sortby=='category':\n        try: \n            all['temp'] = all['category'].astype('float')\n            all = all.sort_values('temp', ascending=asc)\n        except:\n            all = all.sort_values('category', ascending=asc)\n    if bars<lnn: all = all[0:bars]\n    if verbose==1 and target in data:\n        print('TRAIN.CSV variable',col,'has',len(nm),'categories')\n        print('The',min(bars,lnn),'bars displayed here contain',pct,'% of data.')\n        mlnn = data[col].isna().sum()\n        print(\"The data has %.1f %% NA. The plot is sorted by \" % (100.0*mlnn/len(data)) + sortby )\n    \n    # plot density and detection rate\n    fig = plt.figure(1,figsize=(15,3))\n    ax1 = fig.add_subplot(1,1,1)\n    clrs = ['red', 'green', 'blue', 'yellow', 'magenta']\n    barss = ax1.bar([str(x) for x in all['category']],[x/float(len(data)) for x in all['frequency']],color=clrs)\n    for i in range(len(all)-top):\n        barss[top+i].set_color('cyan')\n    if target in data:\n        ax2 = ax1.twinx()\n        if sortby!='category': infected = all['rate'][0:lnn]\n        else:\n            infected=[]\n            for x in all['category']:\n                if nan_check(x): infected.append( data[ data[col].isna() ][target].mean() )\n                elif cvd[x]!=0: infected.append( data[ data[col]==x ][target].mean() )\n                else: infected.append(-1)\n        ax2.plot([str(x) for x in all['category']],infected[0:lnn],'k:o')\n        #ax2.set_ylim(a,b)\n        ax2.spines['left'].set_color('red')\n        ax2.set_ylabel('Detection Rate', color='k')\n    ax1.spines['left'].set_color('red')\n    ax1.yaxis.label.set_color('red')\n    ax1.tick_params(axis='y', colors='red')\n    ax1.set_ylabel('Category Proportion', color='r')\n    if title!='': plt.title(title)\n    plt.show()\n    if verbose==1 and target not in data:\n        print('TEST.CSV variable',col,'has',len(nm),'categories')\n        print('The',min(bars,lnn),'bars displayed here contain',pct,'% of the data.')\n        mlnn = data[col].isna().sum()\n        print(\"The data has %.1f %% NA. The plot is sorted by \" % (100.0*mlnn/len(data)) + sortby )\n        \n# CHECK FOR NAN\ndef nan_check(x):\n    if isinstance(x,float):\n        if math.isnan(x):\n            return True\n    return False\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport seaborn as sns, matplotlib.pyplot as plt\nfrom matplotlib import gridspec\n\n# KERNEL DENSITY BANDWITHS\nn = [1000,1000,50000,25000,50,1000,1e6,1e6,5,2]\n# DENSITY X RANGES\ns = [(1,10000),(0,0),(0,0),(700000,950000),(1,750),(1,10000),(1,2e7),(1,2e7),(0,0),(0,0)]\n# REMOVE NAN\ndf_train = df_train[ df_train['AvSigVersion']!='0.0.0.0' ]\n# TEXT FORMATTING\ndef cc(l):\n    for i in range(len(l)): \n        l[i] = '< ('+str(i)+') < '+str(l[i])\n    return l\n# DISCRETIZING FUNCTION FROM \n# https://www.kaggle.com/guoday/nffm-baseline-0-690-on-lb\ndef make_bucket(data,num=10):\n    data.sort()\n    bins=[]\n    for i in range(num):\n        bins.append(data[int(len(data)*(i+1)//num)-1])\n    return bins\n\nct=0\nfor col in list(data.columns)[1:]:\n    print('###############################################')\n    print('###     '+col)\n    print('###############################################')\n    \n    # TIME SERIES PLOT\n    plt.figure(figsize=(15,5))\n    plt.plot(data['WeekOf'],data[col])\n    plt.xlabel('Time')\n    plt.ylabel(col)\n    plt.title('\"'+col+'\" versus time')\n    plt.show()\n    \n    # HASDETECTION HISTOGRAM\n    bins = make_bucket(df_train[col].copy().values,num=20)\n    df_train['P']=np.digitize(df_train[col],bins=bins)\n    staticPlot(df_train[ df_train['P']!=20 ],'P',sortby='category',asc=True,bars=20,verbose=0,\n               title='HasDetections Rate versus \"'+col+'\" (Bars use left y-axis. Dotted line uses right)')\n    print('KEY TO BINS:',col,cc(bins))\n    \n    # DENSITY PLOT AND BOXPLOT\n    lines = [1, 0]\n    plt.figure(figsize=(15,5))\n    gs = gridspec.GridSpec(1, 2, width_ratios=[3, 1]) \n    plt.subplot(gs[0])\n    for line in lines:\n        subset = df_train[ (df_train['HasDetections'] == line) & (~df_train[col].isna())]\n        sns.kdeplot(subset[col], bw=n[ct],\n                     shade=True, linewidth=3, \n                     label = line)\n    plt.legend(prop={'size': 16}, title = 'HasDetections')\n    plt.title('Density Plot of HasDetections versus \"'+col+'\"')\n    plt.xlabel(col)\n    if s[ct][0]!=0: plt.xlim((s[ct][0],s[ct][1]))\n    plt.ylabel('Density')\n    ax = plt.subplot(gs[1])\n    df_train2 = df_train[ ~df_train[col].isna() ]\n    plt.boxplot([df_train2[ df_train2['HasDetections']==0][col],df_train2[ df_train2['HasDetections']==1][col]])\n    plt.title('Boxplot of \"'+col+'\"')\n    if s[ct][0]!=0: plt.ylim((s[ct][0],s[ct][1]))\n    ct += 1\n    plt.xlabel('HasDetections')\n    ax.set_xticklabels([0,1])\n    plt.show()\ndel df_train['P']", "processed": ["googl safe brow report googl 1 publish inform malwar internet time let load externel dataset display row merg df train perform eda 1 http transparencyreport googl com safe brow overview"]}, {"markdown": ["# AvSigVersion Threats\n[Microsoft][1] publishes malware threats. For each `AvSigVersion`, this external dataset lists all the known malware that was threatening it. (Note that for each point in time, all (95%) of computers use the same `AvSigVersion`, so threats on `AvSigVersion` can be interpreted as threats at a given time point.) There is a lot of information in this dataset, we will only add one new feature to `df_train`. For each `AvSigVersion`, we will count how many things were threatening it, and add a new varable, `ThreatCount`.  \n  \n[1]: https://www.microsoft.com/en-us/wdsi/definitions/antimalware-definition-release-notes?RequestVersion=1.277.43.0"], "code": "# Reference: https://www.kaggle.com/code/cdeotte/external-data-malware-0-50\n\ndata2 = pd.read_csv('../input/malware-avsigversion-threats/AvSigversion_Threats.csv')\ncv = pd.DataFrame(data2.groupby('AvSigVersion')['index'].count()).rename({'index':'ThreatCount'},axis=1)\ndf_train = pd.merge(df_train,cv,on='AvSigVersion',how='left')\ndf_train['ThreatCount'].fillna(0,inplace=True)\nprint('THREAT DATA')\ndata2.sample(10)\ndf_train[['AvSigVersion','OsBuildLab','Census_OSVersion','HasDetections','ThreatCount']].sample(5)\nstaticPlot(df_train,'ThreatCount',sortby='category',asc=True,bars=10\n           ,title='Threat Count, 10 most frequent (65% of all data)(Bars use left y-axis. Dotted line uses right)',verbose=False)\nstaticPlot(df_train,'ThreatCount',sortby='category',asc=True,bars=100\n           ,title='Threat Count, 100 most frequent (90% of all data)(Bars use left y-axis. Dotted line uses right)',verbose=False)\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport seaborn as sns, matplotlib.pyplot as plt\nfrom matplotlib import gridspec\n\n# DENSITY PLOT AND BOXPLOT\ncol = 'ThreatCount'\nlines = [1, 0]\nplt.figure(figsize=(15,5))\ngs = gridspec.GridSpec(1, 2, width_ratios=[3, 1]) \nplt.subplot(gs[0])\nfor line in lines:\n    subset = df_train[ (df_train['HasDetections'] == line) & (~df_train[col].isna())]\n    sns.kdeplot(subset[col], bw=10,\n                   shade=True, linewidth=3, \n                   label = line)\nplt.legend(prop={'size': 16}, title = 'HasDetections')\nplt.title('Density Plot of HasDetections versus \"'+col+'\"')\nplt.xlabel(col)\nplt.xlim((-50,500))\nplt.ylabel('Density')\nax = plt.subplot(gs[1])\ndf_train2 = df_train[ ~df_train[col].isna() ]\nplt.boxplot([df_train2[ df_train2['HasDetections']==0][col],df_train2[ df_train2['HasDetections']==1][col]])\nplt.title('Boxplot of \"'+col+'\"')\nplt.ylim((-50,500))\nplt.xlabel('HasDetections')\nax.set_xticklabels([0,1])\nplt.show()", "processed": ["avsigvers threat microsoft 1 publish malwar threat avsigvers extern dataset list known malwar threaten note point time 95 comput use avsigvers threat avsigvers interpret threat given time point lot inform dataset add one new featur df train avsigvers count mani thing threaten add new varabl threatcount 1 http www microsoft com en u wdsi definit antimalwar definit releas note requestvers 1 277 43 0"]}, {"markdown": ["# LGBM Feature Importance\nWe will now train an LGBM model on just variables from the 2 external datasets, Google data and Threat data. First we will delete all other variables."], "code": "# Reference: https://www.kaggle.com/code/cdeotte/external-data-malware-0-50\n\ndel df_train['DateAS'], df_train['DateOS'], df_train['DateBL'], df_train['WeekOf'] \ndel df_train['AvSigVersion'], df_train['OsBuildLab'], df_train['Census_OSVersion']\nprint('TRAIN DATA')\ndf_train.sample(5)\nimport lightgbm as lgb,gc\n\n# CREATE TRAIN AND VALIDATE\nX_train = df_train.sample(frac=0.5)\nY_train = X_train['HasDetections']\nX_val = df_train[ ~df_train.index.isin(X_train.index) ]\nY_val = X_val['HasDetections']\ndel X_train['HasDetections'], X_val['HasDetections'], df_train\nx=gc.collect()\n\n# TRAIN LGBM\nmodel = lgb.LGBMClassifier(n_estimators=10000, colsample_bytree=0.7, objective='binary', num_leaves=32,\n            max_depth=-1, learning_rate=0.1)\nh=model.fit(X_train, Y_train, eval_metric='auc', eval_set=[(X_val, Y_val)], verbose=50,\n            early_stopping_rounds=100)\n\n# FEATURE IMPORTANCE\ndf = pd.DataFrame({\"mean\" : model.feature_importances_, \"feature\" : X_train.columns })\ndf.sort_values(\"mean\", inplace=True)\nax = df.plot(x=\"feature\", y=\"mean\", kind='barh',color='green', figsize=(12, 20)\n             , title='External Data LGBM Feature Importance')", "processed": ["lgbm featur import train lgbm model variabl 2 extern dataset googl data threat data first delet variabl"]}, {"markdown": ["This looks nice with some outliers at both the ends.! \n\nLet us remove the outliers and then do a histogram plot on the same."], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-notebook-zillow-prize\n\nulimit = np.percentile(train_df.logerror.values, 99)\nllimit = np.percentile(train_df.logerror.values, 1)\ntrain_df['logerror'].ix[train_df['logerror']>ulimit] = ulimit\ntrain_df['logerror'].ix[train_df['logerror']<llimit] = llimit\n\nplt.figure(figsize=(12,8))\nsns.distplot(train_df.logerror.values, bins=50, kde=False)\nplt.xlabel('logerror', fontsize=12)\nplt.show()", "processed": ["look nice outlier end let u remov outlier histogram plot"]}, {"markdown": ["Wow. nice normal distribution on the log error.\n\n**Transaction Date:**\n\nNow let us explore the date field. Let us first check the number of transactions in each month. "], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-notebook-zillow-prize\n\ntrain_df['transaction_month'] = train_df['transactiondate'].dt.month\n\ncnt_srs = train_df['transaction_month'].value_counts()\nplt.figure(figsize=(12,6))\nsns.barplot(cnt_srs.index, cnt_srs.values, alpha=0.8, color=color[3])\nplt.xticks(rotation='vertical')\nplt.xlabel('Month of transaction', fontsize=12)\nplt.ylabel('Number of Occurrences', fontsize=12)\nplt.show()", "processed": ["wow nice normal distribut log error transact date let u explor date field let u first check number transact month"]}, {"markdown": ["Let us explore the latitude and longitude variable to begin with."], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-notebook-zillow-prize\n\nplt.figure(figsize=(12,12))\nsns.jointplot(x=prop_df.latitude.values, y=prop_df.longitude.values, size=10)\nplt.ylabel('Longitude', fontsize=12)\nplt.xlabel('Latitude', fontsize=12)\nplt.show()", "processed": ["let u explor latitud longitud variabl begin"]}, {"markdown": ["The important variables themselves are very highly correlated.! Let us now look at each of them.\n\n**Finished SquareFeet 12:**\n\nLet us seee how the finished square feet 12 varies with the log error."], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-notebook-zillow-prize\n\ncol = \"finishedsquarefeet12\"\nulimit = np.percentile(train_df[col].values, 99.5)\nllimit = np.percentile(train_df[col].values, 0.5)\ntrain_df[col].ix[train_df[col]>ulimit] = ulimit\ntrain_df[col].ix[train_df[col]<llimit] = llimit\n\nplt.figure(figsize=(12,12))\nsns.jointplot(x=train_df.finishedsquarefeet12.values, y=train_df.logerror.values, size=10, color=color[4])\nplt.ylabel('Log Error', fontsize=12)\nplt.xlabel('Finished Square Feet 12', fontsize=12)\nplt.title(\"Finished square feet 12 Vs Log error\", fontsize=15)\nplt.show()", "processed": ["import variabl highli correl let u look finish squarefeet 12 let u seee finish squar foot 12 vari log error"]}, {"markdown": ["Seems the range of logerror narrows down with increase in finished square feet 12 variable. Probably larger houses are easy to predict?\n\n**Calculated finished square feet:**"], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-notebook-zillow-prize\n\ncol = \"calculatedfinishedsquarefeet\"\nulimit = np.percentile(train_df[col].values, 99.5)\nllimit = np.percentile(train_df[col].values, 0.5)\ntrain_df[col].ix[train_df[col]>ulimit] = ulimit\ntrain_df[col].ix[train_df[col]<llimit] = llimit\n\nplt.figure(figsize=(12,12))\nsns.jointplot(x=train_df.calculatedfinishedsquarefeet.values, y=train_df.logerror.values, size=10, color=color[5])\nplt.ylabel('Log Error', fontsize=12)\nplt.xlabel('Calculated finished square feet', fontsize=12)\nplt.title(\"Calculated finished square feet Vs Log error\", fontsize=15)\nplt.show()", "processed": ["seem rang logerror narrow increas finish squar foot 12 variabl probabl larger hous easi predict calcul finish squar foot"]}, {"markdown": ["Here as well the distribution is very similar to the previous one. No wonder the correlation between the two variables are also high.\n\n**Bathroom Count:**"], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-notebook-zillow-prize\n\nplt.figure(figsize=(12,8))\nsns.countplot(x=\"bathroomcnt\", data=train_df)\nplt.ylabel('Count', fontsize=12)\nplt.xlabel('Bathroom', fontsize=12)\nplt.xticks(rotation='vertical')\nplt.title(\"Frequency of Bathroom count\", fontsize=15)\nplt.show()", "processed": ["well distribut similar previou one wonder correl two variabl also high bathroom count"]}, {"markdown": ["There is an interesting 2.279 value in the bathroom count.\n\nEdit: As MihwaHan pointed in the comments, this is the mean value :)\n\nNow let us check how the log error changes based on this."], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-notebook-zillow-prize\n\nplt.figure(figsize=(12,8))\nsns.boxplot(x=\"bathroomcnt\", y=\"logerror\", data=train_df)\nplt.ylabel('Log error', fontsize=12)\nplt.xlabel('Bathroom Count', fontsize=12)\nplt.xticks(rotation='vertical')\nplt.title(\"How log error changes with bathroom count?\", fontsize=15)\nplt.show()", "processed": ["interest 2 279 valu bathroom count edit mihwahan point comment mean valu let u check log error chang base"]}, {"markdown": ["**Bedroom count:**"], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-notebook-zillow-prize\n\nplt.figure(figsize=(12,8))\nsns.countplot(x=\"bedroomcnt\", data=train_df)\nplt.ylabel('Frequency', fontsize=12)\nplt.xlabel('Bedroom Count', fontsize=12)\nplt.xticks(rotation='vertical')\nplt.title(\"Frequency of Bedroom count\", fontsize=15)\nplt.show()", "processed": ["bedroom count"]}, {"markdown": ["3.03 is the mean value with which we replaced the Null values."], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-notebook-zillow-prize\n\ntrain_df['bedroomcnt'].ix[train_df['bedroomcnt']>7] = 7\nplt.figure(figsize=(12,8))\nsns.violinplot(x='bedroomcnt', y='logerror', data=train_df)\nplt.xlabel('Bedroom count', fontsize=12)\nplt.ylabel('Log Error', fontsize=12)\nplt.show()\ncol = \"taxamount\"\nulimit = np.percentile(train_df[col].values, 99.5)\nllimit = np.percentile(train_df[col].values, 0.5)\ntrain_df[col].ix[train_df[col]>ulimit] = ulimit\ntrain_df[col].ix[train_df[col]<llimit] = llimit\n\nplt.figure(figsize=(12,12))\nsns.jointplot(x=train_df['taxamount'].values, y=train_df['logerror'].values, size=10, color='g')\nplt.ylabel('Log Error', fontsize=12)\nplt.xlabel('Tax Amount', fontsize=12)\nplt.title(\"Tax Amount Vs Log error\", fontsize=15)\nplt.show()", "processed": ["3 03 mean valu replac null valu"]}, {"markdown": ["**YearBuilt:**\n\nLet us explore how the error varies with the yearbuilt variable."], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-notebook-zillow-prize\n\nfrom ggplot import *\nggplot(aes(x='yearbuilt', y='logerror'), data=train_df) + \\\n    geom_point(color='steelblue', size=1) + \\\n    stat_smooth()", "processed": ["yearbuilt let u explor error vari yearbuilt variabl"]}, {"markdown": ["There is a minor incremental trend seen with respect to built year.\n\nNow let us see how the logerror varies with respect to latitude and longitude."], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-notebook-zillow-prize\n\nggplot(aes(x='latitude', y='longitude', color='logerror'), data=train_df) + \\\n    geom_point() + \\\n    scale_color_gradient(low = 'red', high = 'blue')", "processed": ["minor increment trend seen respect built year let u see logerror vari respect latitud longitud"]}, {"markdown": ["There are no visible pockets as such with respect to latitude or longitude atleast with the naked eye.\n\nLet us take the variables with highest positive correlation and highest negative correlation to see if we can see some visible patterns."], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-notebook-zillow-prize\n\nggplot(aes(x='finishedsquarefeet12', y='taxamount', color='logerror'), data=train_df) + \\\n    geom_point(alpha=0.7) + \\\n    scale_color_gradient(low = 'pink', high = 'blue')", "processed": ["visibl pocket respect latitud longitud atleast nake eye let u take variabl highest posit correl highest neg correl see see visibl pattern"]}, {"markdown": ["There are no visible patterns here as well. So this is going to be a hard competition to predict I guess.\n\nJust for fun, we will let the machine form some arbitrary pattern for us :D"], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-notebook-zillow-prize\n\nggplot(aes(x='finishedsquarefeet12', y='taxamount', color='logerror'), data=train_df) + \\\n    geom_now_its_art()", "processed": ["visibl pattern well go hard competit predict guess fun let machin form arbitrari pattern u"]}, {"markdown": ["We also have some null values in the dataset. So one feature idea could be to use the count of nulls in the row."], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/feature-engineering-validation-strategy\n\ntrain_df[\"null_count\"] = train_df.isnull().sum(axis=1)\ntest_df[\"null_count\"] = test_df.isnull().sum(axis=1)\n\nplt.figure(figsize=(14,12))\nsns.pointplot(x='null_count', y='price_doc', data=train_df)\nplt.ylabel('price_doc', fontsize=12)\nplt.xlabel('null_count', fontsize=12)\nplt.xticks(rotation='vertical')\nplt.show()", "processed": ["also null valu dataset one featur idea could use count null row"]}, {"markdown": ["We have a timestamp variable in the dataset and time could be one of an important factor determining the price. So let us extract some features out of the timestamp variable."], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/feature-engineering-validation-strategy\n\n# year and month #\ntrain_df[\"yearmonth\"] = train_df[\"timestamp\"].dt.year*100 + train_df[\"timestamp\"].dt.month\ntest_df[\"yearmonth\"] = test_df[\"timestamp\"].dt.year*100 + test_df[\"timestamp\"].dt.month\n\n# year and week #\ntrain_df[\"yearweek\"] = train_df[\"timestamp\"].dt.year*100 + train_df[\"timestamp\"].dt.weekofyear\ntest_df[\"yearweek\"] = test_df[\"timestamp\"].dt.year*100 + test_df[\"timestamp\"].dt.weekofyear\n\n# year #\ntrain_df[\"year\"] = train_df[\"timestamp\"].dt.year\ntest_df[\"year\"] = test_df[\"timestamp\"].dt.year\n\n# month of year #\ntrain_df[\"month_of_year\"] = train_df[\"timestamp\"].dt.month\ntest_df[\"month_of_year\"] = test_df[\"timestamp\"].dt.month\n\n# week of year #\ntrain_df[\"week_of_year\"] = train_df[\"timestamp\"].dt.weekofyear\ntest_df[\"week_of_year\"] = test_df[\"timestamp\"].dt.weekofyear\n\n# day of week #\ntrain_df[\"day_of_week\"] = train_df[\"timestamp\"].dt.weekday\ntest_df[\"day_of_week\"] = test_df[\"timestamp\"].dt.weekday\n\n\nplt.figure(figsize=(12,8))\nsns.pointplot(x='yearweek', y='price_doc', data=train_df)\nplt.ylabel('price_doc', fontsize=12)\nplt.xlabel('yearweek', fontsize=12)\nplt.title('Median Price distribution by year and week_num')\nplt.xticks(rotation='vertical')\nplt.show()\n\nplt.figure(figsize=(12,8))\nsns.boxplot(x='month_of_year', y='price_doc', data=train_df)\nplt.ylabel('price_doc', fontsize=12)\nplt.xlabel('month_of_year', fontsize=12)\nplt.title('Median Price distribution by month_of_year')\nplt.xticks(rotation='vertical')\nplt.show()\n\nplt.figure(figsize=(12,8))\nsns.pointplot(x='week_of_year', y='price_doc', data=train_df)\nplt.ylabel('price_doc', fontsize=12)\nplt.xlabel('week of the year', fontsize=12)\nplt.title('Median Price distribution by week of year')\nplt.xticks(rotation='vertical')\nplt.show()\n\nplt.figure(figsize=(12,8))\nsns.boxplot(x='day_of_week', y='price_doc', data=train_df)\nplt.ylabel('price_doc', fontsize=12)\nplt.xlabel('day_of_week', fontsize=12)\nplt.title('Median Price distribution by day of week')\nplt.xticks(rotation='vertical')\nplt.show()", "processed": ["timestamp variabl dataset time could one import factor determin price let u extract featur timestamp variabl"]}, {"markdown": ["Also from the [simple exploration notebook][1], we have seen that area of the property is the top variables. So creating some more features might be helpful.\n\nLet us create some ratio variables around it.\n\n\n  [1]: https://www.kaggle.com/sudalairajkumar/sberbank-russian-housing-market/simple-exploration-notebook-sberbank"], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/feature-engineering-validation-strategy\n\n# ratio of living area to full area #\ntrain_df[\"ratio_life_sq_full_sq\"] = train_df[\"life_sq\"] / np.maximum(train_df[\"full_sq\"].astype(\"float\"),1)\ntest_df[\"ratio_life_sq_full_sq\"] = test_df[\"life_sq\"] / np.maximum(test_df[\"full_sq\"].astype(\"float\"),1)\ntrain_df[\"ratio_life_sq_full_sq\"].ix[train_df[\"ratio_life_sq_full_sq\"]<0] = 0\ntrain_df[\"ratio_life_sq_full_sq\"].ix[train_df[\"ratio_life_sq_full_sq\"]>1] = 1\ntest_df[\"ratio_life_sq_full_sq\"].ix[test_df[\"ratio_life_sq_full_sq\"]<0] = 0\ntest_df[\"ratio_life_sq_full_sq\"].ix[test_df[\"ratio_life_sq_full_sq\"]>1] = 1\n\n# ratio of kitchen area to living area #\ntrain_df[\"ratio_kitch_sq_life_sq\"] = train_df[\"kitch_sq\"] / np.maximum(train_df[\"life_sq\"].astype(\"float\"),1)\ntest_df[\"ratio_kitch_sq_life_sq\"] = test_df[\"kitch_sq\"] / np.maximum(test_df[\"life_sq\"].astype(\"float\"),1)\ntrain_df[\"ratio_kitch_sq_life_sq\"].ix[train_df[\"ratio_kitch_sq_life_sq\"]<0] = 0\ntrain_df[\"ratio_kitch_sq_life_sq\"].ix[train_df[\"ratio_kitch_sq_life_sq\"]>1] = 1\ntest_df[\"ratio_kitch_sq_life_sq\"].ix[test_df[\"ratio_kitch_sq_life_sq\"]<0] = 0\ntest_df[\"ratio_kitch_sq_life_sq\"].ix[test_df[\"ratio_kitch_sq_life_sq\"]>1] = 1\n\n# ratio of kitchen area to full area #\ntrain_df[\"ratio_kitch_sq_full_sq\"] = train_df[\"kitch_sq\"] / np.maximum(train_df[\"full_sq\"].astype(\"float\"),1)\ntest_df[\"ratio_kitch_sq_full_sq\"] = test_df[\"kitch_sq\"] / np.maximum(test_df[\"full_sq\"].astype(\"float\"),1)\ntrain_df[\"ratio_kitch_sq_full_sq\"].ix[train_df[\"ratio_kitch_sq_full_sq\"]<0] = 0\ntrain_df[\"ratio_kitch_sq_full_sq\"].ix[train_df[\"ratio_kitch_sq_full_sq\"]>1] = 1\ntest_df[\"ratio_kitch_sq_full_sq\"].ix[test_df[\"ratio_kitch_sq_full_sq\"]<0] = 0\ntest_df[\"ratio_kitch_sq_full_sq\"].ix[test_df[\"ratio_kitch_sq_full_sq\"]>1] = 1\n\nplt.figure(figsize=(12,12))\nsns.jointplot(x=train_df.ratio_life_sq_full_sq.values, y=np.log1p(train_df.price_doc.values), size=10)\nplt.ylabel('Log of Price', fontsize=12)\nplt.xlabel('Ratio of living area to full area', fontsize=12)\nplt.title(\"Joint plot on log of living price to ratio_life_sq_full_sq\")\nplt.show()\n\nplt.figure(figsize=(12,12))\nsns.jointplot(x=train_df.ratio_life_sq_full_sq.values, y=np.log1p(train_df.price_doc.values), \n              kind='kde',size=10)\nplt.ylabel('Log of Price', fontsize=12)\nplt.xlabel('Ratio of kitchen area to living area', fontsize=12)\nplt.title(\"Joint plot on log of living price to ratio_kitch_sq_life_sq\")\nplt.show()\n\nplt.figure(figsize=(12,12))\nsns.jointplot(x=train_df.ratio_life_sq_full_sq.values, y=np.log1p(train_df.price_doc.values), \n              kind='kde',size=10)\nplt.ylabel('Log of Price', fontsize=12)\nplt.xlabel('Ratio of kitchen area to full area', fontsize=12)\nplt.title(\"Joint plot on log of living price to ratio_kitch_sq_full_sq\")\nplt.show()", "processed": ["also simpl explor notebook 1 seen area properti top variabl creat featur might help let u creat ratio variabl around 1 http www kaggl com sudalairajkumar sberbank russian hous market simpl explor notebook sberbank"]}, {"markdown": ["### Training and Validation"], "code": "# Reference: https://www.kaggle.com/code/mobassir/severstal-nested-unet-pytorch\n\nclass Trainer(object):\n    '''This class takes care of training and validation of our model'''\n    def __init__(self, model):\n        self.num_workers = 4\n        self.batch_size = {\"train\": 1, \"val\": 1}\n        self.accumulation_steps = 32 // self.batch_size['train']\n        self.lr = 0.00001\n        self.num_epochs = 1\n        self.best_loss = float(\"inf\")\n        self.phases = [\"train\", \"val\"]\n        self.device = torch.device(\"cuda:0\")\n        torch.set_default_tensor_type(\"torch.cuda.FloatTensor\")\n        self.net = model\n        self.criterion = torch.nn.BCEWithLogitsLoss()\n        self.optimizer = optim.Adam(self.net.parameters(), lr=self.lr)\n        self.scheduler = ReduceLROnPlateau(self.optimizer, mode=\"min\", patience=5, verbose=True)\n        self.net = self.net.to(self.device)\n        cudnn.benchmark = True\n        self.dataloaders = {\n            phase: provider(\n                data_folder=data_folder,\n                df_path=train_df_path,\n                phase=phase,\n                mean=(0.485, 0.456, 0.406),\n                std=(0.229, 0.224, 0.225),\n                batch_size=self.batch_size[phase],\n                num_workers=self.num_workers,\n            )\n            for phase in self.phases\n        }\n        self.losses = {phase: [] for phase in self.phases}\n        self.iou_scores = {phase: [] for phase in self.phases}\n        self.dice_scores = {phase: [] for phase in self.phases}\n        \n    def forward(self, images, targets):\n        images = images.to(self.device)\n        masks = targets.to(self.device)\n        outputs = self.net(images)\n        loss = self.criterion(outputs, masks)\n        return loss, outputs\n\n    def iterate(self, epoch, phase):\n        meter = Meter(phase, epoch)\n        start = time.strftime(\"%H:%M:%S\")\n        print(f\"Starting epoch: {epoch} | phase: {phase} | \u23f0: {start}\")\n        batch_size = self.batch_size[phase]\n        self.net.train(phase == \"train\")\n        dataloader = self.dataloaders[phase]\n        running_loss = 0.0\n        total_batches = len(dataloader)\n#         tk0 = tqdm(dataloader, total=total_batches)\n        self.optimizer.zero_grad()\n        for itr, batch in enumerate(dataloader): # replace `dataloader` with `tk0` for tqdm\n            images, targets = batch\n            loss, outputs = self.forward(images, targets)\n            loss = loss / self.accumulation_steps\n            if phase == \"train\":\n                loss.backward()\n                if (itr + 1 ) % self.accumulation_steps == 0:\n                    self.optimizer.step()\n                    self.optimizer.zero_grad()\n            running_loss += loss.item()\n            outputs = outputs.detach().cpu()\n            meter.update(targets, outputs)\n#             tk0.set_postfix(loss=(running_loss / ((itr + 1))))\n        epoch_loss = (running_loss * self.accumulation_steps) / total_batches\n        dice, iou = epoch_log(phase, epoch, epoch_loss, meter, start)\n        self.losses[phase].append(epoch_loss)\n        self.dice_scores[phase].append(dice)\n        self.iou_scores[phase].append(iou)\n        torch.cuda.empty_cache()\n        return epoch_loss\n\n    def start(self):\n        for epoch in range(self.num_epochs):\n            self.iterate(epoch, \"train\")\n            state = {\n                \"epoch\": epoch,\n                \"best_loss\": self.best_loss,\n                \"state_dict\": self.net.state_dict(),\n                \"optimizer\": self.optimizer.state_dict(),\n            }\n            with torch.no_grad():\n                val_loss = self.iterate(epoch, \"val\")\n                self.scheduler.step(val_loss)\n            if val_loss < self.best_loss:\n                print(\"******** New optimal found, saving state ********\")\n                state[\"best_loss\"] = self.best_loss = val_loss\n                torch.save(state, \"./model.pth\")\n            print()\n\nsample_submission_path = '../input/severstal-steel-defect-detection/sample_submission.csv'\ntrain_df_path = '../input/severstal-steel-defect-detection/train.csv'\ndata_folder = \"../input/severstal-steel-defect-detection/\"\ntest_data_folder = \"../input/severstal-steel-defect-detection/test_images\"\nmodel_trainer = Trainer(model)\nmodel_trainer.start()\n# PLOT TRAINING\nlosses = model_trainer.losses\ndice_scores = model_trainer.dice_scores # overall dice\niou_scores = model_trainer.iou_scores\n\ndef plot(scores, name):\n    plt.figure(figsize=(15,5))\n    plt.plot(range(len(scores[\"train\"])), scores[\"train\"], label=f'train {name}')\n    plt.plot(range(len(scores[\"train\"])), scores[\"val\"], label=f'val {name}')\n    plt.title(f'{name} plot'); plt.xlabel('Epoch'); plt.ylabel(f'{name}');\n    plt.legend(); \n    plt.show()\n\nplot(losses, \"BCE loss\")\nplot(dice_scores, \"Dice score\")\nplot(iou_scores, \"IoU score\")", "processed": ["train valid"]}, {"markdown": ["Aha! We can see that they are not the same and that pandas has rounded them off. And we can see that the time seems to decrease. Let's plot the time to get more familiar with this pattern:"], "code": "# Reference: https://www.kaggle.com/code/allunia/shaking-earth\n\nfig, ax = plt.subplots(2,1, figsize=(20,12))\nax[0].plot(train.index.values, train.quaketime.values, c=\"darkred\")\nax[0].set_title(\"Quaketime of 10 Mio rows\")\nax[0].set_xlabel(\"Index\")\nax[0].set_ylabel(\"Quaketime in ms\");\nax[1].plot(train.index.values, train.signal.values, c=\"mediumseagreen\")\nax[1].set_title(\"Signal of 10 Mio rows\")\nax[1].set_xlabel(\"Index\")\nax[1].set_ylabel(\"Acoustic Signal\");", "processed": ["aha see panda round see time seem decreas let plot time get familiar pattern"]}, {"markdown": ["### Take-Away\n\n* We can see only one time in 10 Mio rows when quaketime goes to 0. This is a timepoint where an earthquake in the lab occurs. \n* There are many small oscillations until a heavy peak of the signal occurs. Then it takes some time with smaller oscillations and the earthquake occurs.\n\n\nIf we take a look at the first 50000 indizes we can see that there is a second pattern of quaketime that may has something to do with the resolution of the experimental equipment:"], "code": "# Reference: https://www.kaggle.com/code/allunia/shaking-earth\n\nfig, ax = plt.subplots(3,1,figsize=(20,18))\nax[0].plot(train.index.values[0:50000], train.quaketime.values[0:50000], c=\"Red\")\nax[0].set_xlabel(\"Index\")\nax[0].set_ylabel(\"Time to quake\")\nax[0].set_title(\"How does the second quaketime pattern look like?\")\nax[1].plot(train.index.values[0:49999], np.diff(train.quaketime.values[0:50000]))\nax[1].set_xlabel(\"Index\")\nax[1].set_ylabel(\"Difference between quaketimes\")\nax[1].set_title(\"Are the jumps always the same?\")\nax[2].plot(train.index.values[0:4000], train.quaketime.values[0:4000])\nax[2].set_xlabel(\"Index from 0 to 4000\")\nax[2].set_ylabel(\"Quaketime\")\nax[2].set_title(\"How does the quaketime changes within the first block?\");", "processed": ["take away see one time 10 mio row quaketim goe 0 timepoint earthquak lab occur mani small oscil heavi peak signal occur take time smaller oscil earthquak occur take look first 50000 indiz see second pattern quaketim may someth resolut experiment equip"]}, {"markdown": ["Ok. How does the signal of the test data look like?"], "code": "# Reference: https://www.kaggle.com/code/allunia/shaking-earth\n\nfig, ax = plt.subplots(4,1, figsize=(20,25))\n\nfor n in range(4):\n    seg = pd.read_csv(test_path  + test_files[n])\n    ax[n].plot(seg.acoustic_data.values, c=\"mediumseagreen\")\n    ax[n].set_xlabel(\"Index\")\n    ax[n].set_ylabel(\"Signal\")\n    ax[n].set_ylim([-300, 300])\n    ax[n].set_title(\"Test {}\".format(test_files[n]));", "processed": ["ok signal test data look like"]}, {"markdown": ["* We can see that the mean is shifted towards higher values due to the earthquake. In addition we can see that the 25% up to 75% quartils are looking very discrete.\n* Looking at the quaketime we can't say much about it.", "### The train signal distribution"], "code": "# Reference: https://www.kaggle.com/code/allunia/shaking-earth\n\nfig, ax = plt.subplots(1,2, figsize=(20,5))\nsns.distplot(train.signal.values, ax=ax[0], color=\"Red\", bins=100, kde=False)\nax[0].set_xlabel(\"Signal\")\nax[0].set_ylabel(\"Density\")\nax[0].set_title(\"Signal distribution\")\n\nlow = train.signal.mean() - 3 * train.signal.std()\nhigh = train.signal.mean() + 3 * train.signal.std() \nsns.distplot(train.loc[(train.signal >= low) & (train.signal <= high), \"signal\"].values,\n             ax=ax[1],\n             color=\"Orange\",\n             bins=150, kde=False)\nax[1].set_xlabel(\"Signal\")\nax[1].set_ylabel(\"Density\")\nax[1].set_title(\"Signal distribution without peaks\");", "processed": ["see mean shift toward higher valu due earthquak addit see 25 75 quartil look discret look quaketim say much", "train signal distribut"]}, {"markdown": ["## Rolling features\n\nI sometimes get stuck in too much details that may not be neccessary or are more fun to explore by yourself. As this is just a starter, I like to continue with some ideas and visualisations that have been used in initial work of Bertrand Rouet-Leduc and the LANL-group. One of the ideas was to use features extracted by a rolling window approach. Let's do the same and make some visualisations what goes on with these features until the first lab earthquake occurs.\n\n### Window size\n\nI don't know in adcance which kind of window size would be an appropriate choice and I think it's an hyperparameter we should try to optimize. But to start, let's try out some different sizes and the mean and standard deviation to select one that may be sufficient to play around:"], "code": "# Reference: https://www.kaggle.com/code/allunia/shaking-earth\n\nwindow_sizes = [10, 50, 100, 1000]\nfor window in window_sizes:\n    train[\"rolling_mean_\" + str(window)] = train.signal.rolling(window=window).mean()\n    train[\"rolling_std_\" + str(window)] = train.signal.rolling(window=window).std()\nfig, ax = plt.subplots(len(window_sizes),1,figsize=(20,6*len(window_sizes)))\n\nn = 0\nfor col in train.columns.values:\n    if \"rolling_\" in col:\n        if \"mean\" in col:\n            mean_df = train.iloc[4435000:4445000][col]\n            ax[n].plot(mean_df, label=col, color=\"mediumseagreen\")\n        if \"std\" in col:\n            std = train.iloc[4435000:4445000][col].values\n            ax[n].fill_between(mean_df.index.values,\n                               mean_df.values-std, mean_df.values+std,\n                               facecolor='lightgreen',\n                               alpha = 0.5, label=col)\n            ax[n].legend()\n            n+=1\n", "processed": ["roll featur sometim get stuck much detail may neccessari fun explor starter like continu idea visualis use initi work bertrand rouet leduc lanl group one idea use featur extract roll window approach let make visualis goe featur first lab earthquak occur window size know adcanc kind window size would appropri choic think hyperparamet tri optim start let tri differ size mean standard deviat select one may suffici play around"]}, {"markdown": ["From the above plot, we can see that the maximum number of audio samples per species in the training data is <code>100</code>. There are many species with less than <code>100</code> samples in the training data, such as <code>redhea</code>, <code>buffle</code>, and <code>coshum</code> with <code>9</code>, <code>15</code>, and <code>19</code> samples respectively.", "## duration <a id=\"2.2\"></a>\n\nThe <code>duration</code> is the length of the audio clip in seconds."], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/birdcall-identification-spectrogram-resnet\n\nnums_1 = train_df.duration\nnums_1 = nums_1.fillna(nums_1.mean())\n\nfig = ff.create_distplot(hist_data=[nums_1],\n                         group_labels=[\"0\"],\n                         colors=[\"seagreen\"], show_hist=False)\n\nfig.update_layout(title_text=\"Duration distribution\", xaxis_title=\"Duration\",\n                  template=\"plotly_white\", paper_bgcolor=\"#edebeb\")\nfig.show()", "processed": ["plot see maximum number audio sampl per speci train data code 100 code mani speci le code 100 code sampl train data code redhea code code buffl code code coshum code code 9 code code 15 code code 19 code sampl respect", "durat id 2 2 code durat code length audio clip second"]}, {"markdown": ["### Visualize melspectrogram\n\nNext we visualize the <code>melspectrogram</code> feature maps for sample signals to get an better understanding. We can see that the <code>melspectrogram</code> contains visual information about the trends (<code>frequency</code> and <code>amplitude</code>) in the audio signal over time."], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/birdcall-identification-spectrogram-resnet\n\ndef viz(species):\n    data = train_df.query('ebird_code == \"{}\"'.format(species))\n    \n    data = data.reset_index(drop=True)\n    ebird_code, filename = data.ebird_code.loc[0], data.filename.loc[0]\n    \n    ipd.Audio(TRAIN_AUDIO_PATH + species + '/' + filename)\n\n    f, pth = (30, 5), TRAIN_AUDIO_PATH\n    fig, ax = plt.subplots(nrows=CHUNKS, ncols=2, figsize=f)\n\n    ax[0].set_title(species + \" signal\", fontsize=16)\n    ax[1].set_title(species + \" melspectrogram\", fontsize=16)\n\n    sr, data = read(pth + ebird_code + '/' + filename)\n    signals, melsp_features = get_signal(data), get_melsp_img(data)\n\n    values = zip(signals, melsp_features)\n    for i, (signal, melsp_feature) in enumerate(values):\n        ax[0].plot(signal, 'crimson'); ax[1].imshow(cv2.resize(melsp_feature, (4096, 1024)))\n    \n    display(ipd.Audio(pth + ebird_code + '/' + filename))\n    plt.show()", "processed": ["visual melspectrogram next visual code melspectrogram code featur map sampl signal get better understand see code melspectrogram code contain visual inform trend code frequenc code code amplitud code audio signal time"]}, {"markdown": ["# Visualizing the data for a single item\n- Lets take a random item that sell a lot and see how it's sales look across the training data.\n- `FOODS_3_090_CA_3_validation` sells a lot\n- Note there are days where it appears the item is unavailable and sales flatline"], "code": "# Reference: https://www.kaggle.com/code/robikscube/m5-forecasting-starter-data-exploration\n\nd_cols = [c for c in stv.columns if 'd_' in c] # sales data columns\n\n# Below we are chaining the following steps in pandas:\n# 1. Select the item.\n# 2. Set the id as the index, Keep only sales data columns\n# 3. Transform so it's a column\n# 4. Plot the data\nstv.loc[stv['id'] == 'FOODS_3_090_CA_3_validation'] \\\n    .set_index('id')[d_cols] \\\n    .T \\\n    .plot(figsize=(15, 5),\n          title='FOODS_3_090_CA_3 sales by \"d\" number',\n          color=next(color_cycle))\nplt.legend('')\nplt.show()", "processed": ["visual data singl item let take random item sell lot see sale look across train data food 3 090 ca 3 valid sell lot note day appear item unavail sale flatlin"]}, {"markdown": ["## Merging the data with real dates\n- We are given a calendar with additional information about past and future dates.\n- The calendar data can be merged with our days data\n- From this we can find weekly and annual trends"], "code": "# Reference: https://www.kaggle.com/code/robikscube/m5-forecasting-starter-data-exploration\n\n# Calendar data looks like this (only showing columns we care about for now)\ncal[['d','date','event_name_1','event_name_2',\n     'event_type_1','event_type_2', 'snap_CA']].head()\n# Merge calendar on our items' data\nexample = stv.loc[stv['id'] == 'FOODS_3_090_CA_3_validation'][d_cols].T\nexample = example.rename(columns={8412:'FOODS_3_090_CA_3'}) # Name it correctly\nexample = example.reset_index().rename(columns={'index': 'd'}) # make the index \"d\"\nexample = example.merge(cal, how='left', validate='1:1')\nexample.set_index('date')['FOODS_3_090_CA_3'] \\\n    .plot(figsize=(15, 5),\n          color=next(color_cycle),\n          title='FOODS_3_090_CA_3 sales by actual sale dates')\nplt.show()\n\n# Select more top selling examples\nexample2 = stv.loc[stv['id'] == 'HOBBIES_1_234_CA_3_validation'][d_cols].T\nexample2 = example2.rename(columns={6324:'HOBBIES_1_234_CA_3'}) # Name it correctly\nexample2 = example2.reset_index().rename(columns={'index': 'd'}) # make the index \"d\"\nexample2 = example2.merge(cal, how='left', validate='1:1')\n\nexample3 = stv.loc[stv['id'] == 'HOUSEHOLD_1_118_CA_3_validation'][d_cols].T\nexample3 = example3.rename(columns={6776:'HOUSEHOLD_1_118_CA_3'}) # Name it correctly\nexample3 = example3.reset_index().rename(columns={'index': 'd'}) # make the index \"d\"\nexample3 = example3.merge(cal, how='left', validate='1:1')", "processed": ["merg data real date given calendar addit inform past futur date calendar data merg day data find weekli annual trend"]}, {"markdown": ["# Sales broken down by time variables\n- Now that we have our example item lets see how it sells by:\n    - Day of the week\n    - Month\n    - Year"], "code": "# Reference: https://www.kaggle.com/code/robikscube/m5-forecasting-starter-data-exploration\n\nexamples = ['FOODS_3_090_CA_3','HOBBIES_1_234_CA_3','HOUSEHOLD_1_118_CA_3']\nexample_df = [example, example2, example3]\nfor i in [0, 1, 2]:\n    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 3))\n    example_df[i].groupby('wday').mean()[examples[i]] \\\n        .plot(kind='line',\n              title='average sale: day of week',\n              lw=5,\n              color=color_pal[0],\n              ax=ax1)\n    example_df[i].groupby('month').mean()[examples[i]] \\\n        .plot(kind='line',\n              title='average sale: month',\n              lw=5,\n              color=color_pal[4],\n\n              ax=ax2)\n    example_df[i].groupby('year').mean()[examples[i]] \\\n        .plot(kind='line',\n              lw=5,\n              title='average sale: year',\n              color=color_pal[2],\n\n              ax=ax3)\n    fig.suptitle(f'Trends for item: {examples[i]}',\n                 size=20,\n                 y=1.1)\n    plt.tight_layout()\n    plt.show()", "processed": ["sale broken time variabl exampl item let see sell day week month year"]}, {"markdown": ["# Lets look at a lot of different items!\n- Lets put it all together to plot 20 different items and their sales\n- Some observations from these plots:\n    - It is common to see an item unavailable for a period of time.\n    - Some items only sell 1 or less in a day, making it very hard to predict.\n    - Other items show spikes in their demand (super bowl sunday?) possibly the \"events\" provided to us could help with these."], "code": "# Reference: https://www.kaggle.com/code/robikscube/m5-forecasting-starter-data-exploration\n\ntwenty_examples = stv.sample(20, random_state=529) \\\n        .set_index('id')[d_cols] \\\n    .T \\\n    .merge(cal.set_index('d')['date'],\n           left_index=True,\n           right_index=True,\n            validate='1:1') \\\n    .set_index('date')\nfig, axs = plt.subplots(10, 2, figsize=(15, 20))\naxs = axs.flatten()\nax_idx = 0\nfor item in twenty_examples.columns:\n    twenty_examples[item].plot(title=item,\n                              color=next(color_cycle),\n                              ax=axs[ax_idx])\n    ax_idx += 1\nplt.tight_layout()\nplt.show()", "processed": ["let look lot differ item let put togeth plot 20 differ item sale observ plot common see item unavail period time item sell 1 le day make hard predict item show spike demand super bowl sunday possibl event provid u could help"]}, {"markdown": ["# Combined Sales over Time by Type\n- We have several item types:\n    - Hobbies\n    - Household\n    - Foods\n- Lets plot the total demand over time for each type"], "code": "# Reference: https://www.kaggle.com/code/robikscube/m5-forecasting-starter-data-exploration\n\nstv['cat_id'].unique()\nstv.groupby('cat_id').count()['id'] \\\n    .sort_values() \\\n    .plot(kind='barh', figsize=(15, 5), title='Count of Items by Category')\nplt.show()\npast_sales = stv.set_index('id')[d_cols] \\\n    .T \\\n    .merge(cal.set_index('d')['date'],\n           left_index=True,\n           right_index=True,\n            validate='1:1') \\\n    .set_index('date')\n\n\nfor i in stv['cat_id'].unique():\n    items_col = [c for c in past_sales.columns if i in c]\n    past_sales[items_col] \\\n        .sum(axis=1) \\\n        .plot(figsize=(15, 5),\n              alpha=0.8,\n              title='Total Sales by Item Type')\nplt.legend(stv['cat_id'].unique())\nplt.show()", "processed": ["combin sale time type sever item type hobbi household food let plot total demand time type"]}, {"markdown": ["# Rollout of items being sold.\n- We can see the some items come into supply that previously didn't exist. Similarly some items stop being sold completely.\n- Lets plot the sales, but only count if item is selling or not selling (0 -> not selling, >0 -> selling)\n- This plot shows us that many items are being slowly introduced into inventory, so many of them will not register a sale at the beginning of the provided data."], "code": "# Reference: https://www.kaggle.com/code/robikscube/m5-forecasting-starter-data-exploration\n\npast_sales_clipped = past_sales.clip(0, 1)\nfor i in stv['cat_id'].unique():\n    items_col = [c for c in past_sales.columns if i in c]\n    (past_sales_clipped[items_col] \\\n        .mean(axis=1) * 100) \\\n        .plot(figsize=(15, 5),\n              alpha=0.8,\n              title='Inventory Sale Percentage by Date',\n              style='.')\nplt.ylabel('% of Inventory with at least 1 sale')\nplt.legend(stv['cat_id'].unique())\nplt.show()", "processed": ["rollout item sold see item come suppli previous exist similarli item stop sold complet let plot sale count item sell sell 0 sell 0 sell plot show u mani item slowli introduc inventori mani regist sale begin provid data"]}, {"markdown": ["# Sales by Store\nWe are provided data for 10 unique stores. What are the total sales by stores?\n- Note that some stores are more steady than others.\n- CA_2 seems to have a big change occur in 2015"], "code": "# Reference: https://www.kaggle.com/code/robikscube/m5-forecasting-starter-data-exploration\n\nstore_list = sellp['store_id'].unique()\nfor s in store_list:\n    store_items = [c for c in past_sales.columns if s in c]\n    past_sales[store_items] \\\n        .sum(axis=1) \\\n        .rolling(90).mean() \\\n        .plot(figsize=(15, 5),\n              alpha=0.8,\n              title='Rolling 90 Day Average Total Sales (10 stores)')\nplt.legend(store_list)\nplt.show()", "processed": ["sale store provid data 10 uniqu store total sale store note store steadi other ca 2 seem big chang occur 2015"]}, {"markdown": ["Looking at the same data a different way, we can plot a rolling 7 day total demand count by store. Note clearly that some stores have abrupt changes in their demand, it could be that the store expanded or a new competitor was built near by. Either way this is imporant to note when creating predictive models about demand pattern. "], "code": "# Reference: https://www.kaggle.com/code/robikscube/m5-forecasting-starter-data-exploration\n\nfig, axes = plt.subplots(5, 2, figsize=(15, 10), sharex=True)\naxes = axes.flatten()\nax_idx = 0\nfor s in store_list:\n    store_items = [c for c in past_sales.columns if s in c]\n    past_sales[store_items] \\\n        .sum(axis=1) \\\n        .rolling(7).mean() \\\n        .plot(alpha=1,\n              ax=axes[ax_idx],\n              title=s,\n              lw=3,\n              color=next(color_cycle))\n    ax_idx += 1\n# plt.legend(store_list)\nplt.suptitle('Weekly Sale Trends by Store ID')\nplt.tight_layout()\nplt.show()", "processed": ["look data differ way plot roll 7 day total demand count store note clearli store abrupt chang demand could store expand new competitor built near either way impor note creat predict model demand pattern"]}, {"markdown": ["Some interesting things to note from these heatmaps:\n- Food tends to have lower number of purchases as the month goes on. Could this be because people get their paychecks early in the month?\n- Household and Hobby items sell much less in January - after the Holiday season is over.\n- Cleary weekends are more popular shopping days regardless of the item category.", "# Sale Prices\nWe are given historical sale prices of each item. Lets take a look at our example item from before.\n- It looks to me like the price of this item is growing.\n- Different stores have different selling prices."], "code": "# Reference: https://www.kaggle.com/code/robikscube/m5-forecasting-starter-data-exploration\n\nfig, ax = plt.subplots(figsize=(15, 5))\nstores = []\nfor store, d in sellp.query('item_id == \"FOODS_3_090\"').groupby('store_id'):\n    d.plot(x='wm_yr_wk',\n          y='sell_price',\n          style='.',\n          color=next(color_cycle),\n          figsize=(15, 5),\n          title='FOODS_3_090 sale price over time',\n         ax=ax,\n          legend=store)\n    stores.append(store)\n    plt.legend()\nplt.legend(stores)\nplt.show()\nsellp['Category'] = sellp['item_id'].str.split('_', expand=True)[0]\nfig, axs = plt.subplots(1, 3, figsize=(15, 4))\ni = 0\nfor cat, d in sellp.groupby('Category'):\n    ax = d['sell_price'].apply(np.log1p) \\\n        .plot(kind='hist',\n                         bins=20,\n                         title=f'Distribution of {cat} prices',\n                         ax=axs[i],\n                                         color=next(color_cycle))\n    ax.set_xlabel('Log(price)')\n    i += 1\nplt.tight_layout()", "processed": ["interest thing note heatmap food tend lower number purchas month goe could peopl get paycheck earli month household hobbi item sell much le januari holiday season cleari weekend popular shop day regardless item categori", "sale price given histor sale price item let take look exampl item look like price item grow differ store differ sell price"]}, {"markdown": ["A lot of columns are binary, in fact you could say that there were several categorical features and they were one-hot encoded.", "## Feature analysis\n\nLet's work with features. It is important to remember that some features show data for each individual and others show data for the whole household, so they have the same value for each individual in the household.", "### Fixing target\nYou can see in [this](https://www.kaggle.com/c/costa-rican-household-poverty-prediction/discussion/61403#358941) discussion that some targets of individual rows could be wrong, let's correct them."], "code": "# Reference: https://www.kaggle.com/code/artgor/extensive-poverty-eda-feature-engineering-and-lgb\n\nidhogars = train.groupby(['idhogar']).agg({'Target': ['count', 'min', 'max']}).reset_index()\nidhogars[idhogars['Target']['min'] != idhogars['Target']['max']].head(20)\nfor i in idhogars[idhogars['Target']['min'] != idhogars['Target']['max']]['idhogar'].unique():\n    correct_value = train.loc[(train['idhogar'] == i) & (train['parentesco1'] == 1), 'Target'].values[0]\n    train.loc[train['idhogar'] == i, 'Target'] = correct_value\nsns.countplot(x=\"Target\", data=train);", "processed": ["lot column binari fact could say sever categor featur one hot encod", "featur analysi let work featur import rememb featur show data individu other show data whole household valu individu household", "fix target see http www kaggl com c costa rican household poverti predict discus 61403 358941 discus target individu row could wrong let correct"]}, {"markdown": ["Now all households have a single value for target. We can see that most of the rows in train set have target 4, so this is imbalanced classification problem.", "### v2a1\nMonthly rent payment.\n\nI suppose that empty values mean that family owns the house and therfore doesn't pay rent."], "code": "# Reference: https://www.kaggle.com/code/artgor/extensive-poverty-eda-feature-engineering-and-lgb\n\n# Let's create a short train set which contains only one line per household for correct analysis and visualization of household features.\ntrain_short = train.drop_duplicates('idhogar')\ntrain_short.groupby('Target')['v2a1'].mean()\nsns.boxplot(x=\"Target\", y=\"v2a1\", data=train_short);", "processed": ["household singl valu target see row train set target 4 imbalanc classif problem", "v2a1 monthli rent payment suppos empti valu mean famili own hous therfor pay rent"]}, {"markdown": ["We can see that poor households indeed can only afford lower rents than non vulnerable households.", "ddddTo be continued"], "code": "# Reference: https://www.kaggle.com/code/artgor/extensive-poverty-eda-feature-engineering-and-lgb\n\nprint('Mean monthly rate for households with different sizes of household separately by poverty level.')\nsns.factorplot(x=\"tamhog\", y=\"v2a1\", col=\"Target\", data=train_short, kind=\"bar\");\nprint('Counts of households with different sizes of household separately by poverty level.')\nsns.factorplot(x=\"tamhog\", col=\"Target\", data=train_short, kind=\"count\");", "processed": ["see poor household inde afford lower rent non vulner household", "ddddto continu"]}, {"markdown": ["Almost all houses have toilets and refrigerators, but sadly most don't have tablets."], "code": "# Reference: https://www.kaggle.com/code/artgor/extensive-poverty-eda-feature-engineering-and-lgb\n\nsns.boxplot(x='Target', y='escolari', data = train);\nplt.title('Years of schooling per household poverty level.')", "processed": ["almost hous toilet refriger sadli tablet"]}, {"markdown": ["We can see that people in non vulnerable households have better education. It is a question which comes the first: is it more difficult to get better education for poor people or does lower education cause poverty?", "## Combining ohe-hot enoded columns\nSeveral columns were one-hot encoded and separated into separate columns. While some machine learning models will enjoy it, some others won't. And it is easier to visualize a single column. "], "code": "# Reference: https://www.kaggle.com/code/artgor/extensive-poverty-eda-feature-engineering-and-lgb\n\ndef combine_features(data, cols=[], name=''):\n    df = data.copy()\n    for i, col in enumerate(cols):\n        print(i + 1, col)\n    df[cols] = df[cols].multiply([i for i in range(1, len(cols) + 1)], axis=1)\n    df[name] = df[cols].sum(axis=1)\n    df.drop(cols, axis=1, inplace=True)\n    return df\ntrain_new = combine_features(train, cols=[col for col in train.columns if col.startswith('pared')], name='wall')\nprint('Wall type count by target.');\nsns.factorplot(\"wall\", col=\"Target\", col_wrap=4, data=train_new, kind=\"count\");", "processed": ["see peopl non vulner household better educ question come first difficult get better educ poor peopl lower educ caus poverti", "combin ohe hot enod column sever column one hot encod separ separ column machin learn model enjoy other easier visual singl column"]}, {"markdown": ["Most walls are made from bricks/blocks or cement. But poor households sometimes leave in wooden houses.", "## Floor material"], "code": "# Reference: https://www.kaggle.com/code/artgor/extensive-poverty-eda-feature-engineering-and-lgb\n\ntrain_new = combine_features(train_new, cols=[col for col in train_new.columns if col.startswith('piso')], name='floor')\nprint('Floor type count by target.');\nsns.factorplot(\"floor\", col=\"Target\", col_wrap=4, data=train_new, kind=\"count\");\ntrain_new = combine_features(train_new, cols=[col for col in train_new.columns if col.startswith('techo')], name='roof')\nprint('Roof type count by target.');\nsns.factorplot(\"roof\", col=\"Target\", col_wrap=4, data=train_new, kind=\"count\");\ntrain_new = combine_features(train_new, cols=[col for col in train_new.columns if col.startswith('abasta')], name='water')\nprint('Water provision type count by target.');\nsns.factorplot(\"water\", col=\"Target\", col_wrap=4, data=train_new, kind=\"count\");", "processed": ["wall made brick block cement poor household sometim leav wooden hous", "floor materi"]}, {"markdown": ["Most households have water provision inside their dwellings."], "code": "# Reference: https://www.kaggle.com/code/artgor/extensive-poverty-eda-feature-engineering-and-lgb\n\ntrain_new = combine_features(train_new, cols=['public', 'planpri', 'noelec', 'coopele'], name='electricity')\nprint('Electricity source type count by target.');\nsns.factorplot(\"electricity\", col=\"Target\", col_wrap=4, data=train_new, kind=\"count\");", "processed": ["household water provis insid dwell"]}, {"markdown": ["Wow, most households have electricity from private plants!"], "code": "# Reference: https://www.kaggle.com/code/artgor/extensive-poverty-eda-feature-engineering-and-lgb\n\ntrain_new = combine_features(train_new, cols=[col for col in train_new.columns if col.startswith('sanitario')], name='toilet')\nprint('Toilet connection type count by target.');\nsns.factorplot(\"toilet\", col=\"Target\", col_wrap=4, data=train_new, kind=\"count\");", "processed": ["wow household electr privat plant"]}, {"markdown": ["Most of the toilets are connected to septic tanks."], "code": "# Reference: https://www.kaggle.com/code/artgor/extensive-poverty-eda-feature-engineering-and-lgb\n\ntrain_new = combine_features(train_new, cols=[col for col in train_new.columns if col.startswith('energcocinar')], name='cooking')\nprint('Cooking sourse energy type count by target.');\nsns.factorplot(\"cooking\", col=\"Target\", col_wrap=4, data=train_new, kind=\"count\");", "processed": ["toilet connect septic tank"]}, {"markdown": ["Most use electricity or gas."], "code": "# Reference: https://www.kaggle.com/code/artgor/extensive-poverty-eda-feature-engineering-and-lgb\n\ntrain_new = combine_features(train_new, cols=[col for col in train_new.columns if col.startswith('elimbasu')], name='rubbish')\nprint('Rubbish disposal type count by target.');\nsns.factorplot(\"rubbish\", col=\"Target\", col_wrap=4, data=train_new, kind=\"count\");", "processed": ["use electr ga"]}, {"markdown": ["Most of the rubbish is disposed using tanker truck."], "code": "# Reference: https://www.kaggle.com/code/artgor/extensive-poverty-eda-feature-engineering-and-lgb\n\ntrain_new = combine_features(train_new, cols=[col for col in train_new.columns if col.startswith('epared')], name='wall_quality')\nprint('Wall quality type count by target.');\nsns.factorplot(\"wall_quality\", col=\"Target\", col_wrap=4, data=train_new, kind=\"count\");\ntrain_new = combine_features(train_new, cols=[col for col in train_new.columns if col.startswith('etecho')], name='roof_quality')\nprint('Roof quality type count by target.');\nsns.factorplot(\"roof_quality\", col=\"Target\", col_wrap=4, data=train_new, kind=\"count\");\ntrain_new = combine_features(train_new, cols=[col for col in train_new.columns if col.startswith('eviv')], name='floor_quality')\nprint('Floor quality type count by target.');\nsns.factorplot(\"floor_quality\", col=\"Target\", col_wrap=4, data=train_new, kind=\"count\");", "processed": ["rubbish dispos use tanker truck"]}, {"markdown": ["Most of non vulnurable households have good houses, but more than a third have regular quality. Poor houselds tend to have houses with problems."], "code": "# Reference: https://www.kaggle.com/code/artgor/extensive-poverty-eda-feature-engineering-and-lgb\n\ntrain_new = combine_features(train_new, cols=[col for col in train_new.columns if col.startswith('estadocivil')], name='family')\nprint('Family status count by target.');\nsns.factorplot(\"family\", col=\"Target\", col_wrap=4, data=train_new, kind=\"count\");\ntrain_new = combine_features(train_new, cols=[col for col in train_new.columns if col.startswith('parentesco')], name='family_status')\ntrain_new['family_status'].value_counts().plot(kind='bar');\nplt.title('Family status count.');\ntrain_new = combine_features(train_new, cols=[col for col in train_new.columns if col.startswith('instlevel')], name='education')\nprint('Education level count by target.');\nsns.factorplot(\"education\", col=\"Target\", col_wrap=4, data=train_new, kind=\"count\");\ntrain_new = combine_features(train_new, cols=[col for col in train_new.columns if col.startswith('tipovivi')], name='home_own')\nprint('Home ownership type count by target.');\nsns.factorplot(\"home_own\", col=\"Target\", col_wrap=4, data=train_new, kind=\"count\");", "processed": ["non vulnur household good hous third regular qualiti poor houseld tend hous problem"]}, {"markdown": ["Most of the households own houses, as we could saw from rent payment amount."], "code": "# Reference: https://www.kaggle.com/code/artgor/extensive-poverty-eda-feature-engineering-and-lgb\n\ntrain_new = combine_features(train_new, cols=[col for col in train_new.columns if col.startswith('lugar')], name='region')\nprint('Region count by target.');\nsns.factorplot(\"region\", col=\"Target\", col_wrap=4, data=train_new, kind=\"count\");", "processed": ["household hous could saw rent payment amount"]}, {"markdown": ["# Description of Datsets\nPer the competition description [here](https://www.kaggle.com/c/deepfake-detection-challenge/overview/getting-started):\n\n*There are 4 groups of datasets associated with this competition.*\n\n**Training Set:** *This dataset, containing labels for the target, is available for download outside of Kaggle for competitors to build their models. It is broken up into 50 files, for ease of access and download. Due to its large size, it must be accessed through a GCS bucket which is only made available to participants after accepting the competition\u2019s rules. Please read the rules fully before accessing the dataset, as they contain important details about the dataset\u2019s permitted use. It is expected and encouraged that you train your models outside of Kaggle\u2019s notebooks environment and submit to Kaggle by uploading the trained model as an external data source.*\n\n**Public Validation Set:** *When you commit your Kaggle notebook, the submission file output that is generated will be based on the small set of 400 videos/ids contained within this Public Validation Set. This is available on the Kaggle Data page as test_videos.zip*\n\n**Public Test Set:** *This dataset is completely withheld and is what Kaggle\u2019s platform computes the public leaderboard against. When you \u201cSubmit to Competition\u201d from the \u201cOutput\u201d file of a committed notebook that contains the competition\u2019s dataset, your code will be re-run in the background against this Public Test Set. When the re-run is complete, the score will be posted to the public leaderboard. If the re-run fails, you will see an error reflected in your \u201cMy Submissions\u201d page. Unfortunately, we are unable to surface any details about your error, so as to prevent error-probing. You are limited to 2 submissions per day, including submissions which error.*\n\n**Private Test Set:** *This dataset is privately held outside of Kaggle\u2019s platform, and is used to compute the private leaderboard. It contains videos with a similar format and nature as the Training and Public Validation/Test Sets, but are real, organic videos with and without deepfakes. After the competition deadline, Kaggle transfers your 2 final selected submissions\u2019 code to the host. They will re-run your code against this private dataset and return prediction submissions back to Kaggle for computing your final private leaderboard scores.*", "# Review of Data Files Accessable within kernel\n\n### Files\n- **train_sample_videos.zip** - a ZIP file containing a sample set of training videos and a metadata.json with labels. the full set of training videos is available through the links provided above.\n- **sample_submission.csv** - a sample submission file in the correct format.\n- **test_videos.zip** - a zip file containing a small set of videos to be used as a public validation set.\nTo understand the datasets available for this competition, review the Getting Started information.\n\n### Metadata Columns\n- **filename** - the filename of the video\n- **label** - whether the video is REAL or FAKE\n- **original** - in the case that a train set video is FAKE, the original video is listed here\n- **split** - this is always equal to \"train\"."], "code": "# Reference: https://www.kaggle.com/code/robikscube/kaggle-deepfake-detection-introduction\n\ntrain_sample_metadata = pd.read_json('../input/deepfake-detection-challenge/train_sample_videos/metadata.json').T\ntrain_sample_metadata.head()\ntrain_sample_metadata.groupby('label')['label'].count().plot(figsize=(15, 5), kind='bar', title='Distribution of Labels in the Training Set')\nplt.show()", "processed": ["descript datset per competit descript http www kaggl com c deepfak detect challeng overview get start 4 group dataset associ competit train set dataset contain label target avail download outsid kaggl competitor build model broken 50 file ea access download due larg size must access gc bucket made avail particip accept competit rule plea read rule fulli access dataset contain import detail dataset permit use expect encourag train model outsid kaggl notebook environ submit kaggl upload train model extern data sourc public valid set commit kaggl notebook submiss file output gener base small set 400 video id contain within public valid set avail kaggl data page test video zip public test set dataset complet withheld kaggl platform comput public leaderboard submit competit output file commit notebook contain competit dataset code run background public test set run complet score post public leaderboard run fail see error reflect submiss page unfortun unabl surfac detail error prevent error probe limit 2 submiss per day includ submiss error privat test set dataset privat held outsid kaggl platform use comput privat leaderboard contain video similar format natur train public valid test set real organ video without deepfak competit deadlin kaggl transfer 2 final select submiss code host run code privat dataset return predict submiss back kaggl comput final privat leaderboard score", "review data file access within kernel file train sampl video zip zip file contain sampl set train video metadata json label full set train video avail link provid sampl submiss csv sampl submiss file correct format test video zip zip file contain small set video use public valid set understand dataset avail competit review get start inform metadata column filenam filenam video label whether video real fake origin case train set video fake origin video list split alway equal train"]}, {"markdown": ["## **Feature Importance**\n\n- Decision Trees models which are based on ensembles (eg. Extra Trees and Random Forest) can be used to rank the importance of the different features. Knowing which features our model is giving most importance can be of vital importance to understand how our model is making it\u2019s predictions (therefore making it more explainable). At the same time, we can get rid of the features which do not bring any benefit to our model."], "code": "# Reference: https://www.kaggle.com/code/prashant111/comprehensive-guide-on-feature-selection\n\n# visualize feature importance\n\nplt.figure(num=None, figsize=(10,8), dpi=80, facecolor='w', edgecolor='k')\n\nfeat_importances = pd.Series(clf.feature_importances_, index= X.columns)\n\nfeat_importances.nlargest(7).plot(kind='barh')", "processed": ["featur import decis tree model base ensembl eg extra tree random forest use rank import differ featur know featur model give import vital import understand model make predict therefor make explain time get rid featur bring benefit model"]}, {"markdown": ["# Home Credit Default Risk - Exploration + Baseline Model\n\nMany people struggle to get loans due to insufficient or non-existent credit histories. And, unfortunately, this population is often taken advantage of by untrustworthy lenders. Home Credit strives to broaden financial inclusion for the unbanked population by providing a positive and safe borrowing experience. In order to make sure this underserved population has a positive loan experience, Home Credit makes use of a variety of alternative data--including telco and transactional information--to predict their clients' repayment abilities.\n\nWhile Home Credit is currently using various statistical and machine learning methods to make these predictions, they're challenging Kagglers to help them unlock the full potential of their data. Doing so will ensure that clients capable of repayment are not rejected and that loans are given with a principal, maturity, and repayment calendar that will empower their clients to be successful.\n\nThis is a simple notebook on exploration and baseline model of home credit default risk data \n\n**Contents**   \n[1. Dataset Preparation](#1)    \n[2. Exploration - Applications Train](#2)  \n&nbsp;&nbsp;&nbsp;&nbsp; [2.1 Snapshot - Application Train](#2.1)    \n&nbsp;&nbsp;&nbsp;&nbsp; [2.2 Distribution of Target Variable](#2.2)    \n&nbsp;&nbsp;&nbsp;&nbsp; [2.3 Applicant's Gender Type](#2.3)    \n&nbsp;&nbsp;&nbsp;&nbsp; [2.4 Family Status of Applicants who takes the loan](#2.4)  \n&nbsp;&nbsp;&nbsp;&nbsp; [2.5 Does applicants own Real Estate or Car](#2.5)    \n&nbsp;&nbsp;&nbsp;&nbsp; [2.6 Suite Type and Income Type of Applicants](#2.6)   \n&nbsp;&nbsp;&nbsp;&nbsp; [2.7 Applicants Contract Type](#2.7)   \n&nbsp;&nbsp;&nbsp;&nbsp; [2.8 Education Type and Occupation Type](#2.8)   \n&nbsp;&nbsp;&nbsp;&nbsp; [2.9 Organization Type and Occupation Type](#2.9)   \n&nbsp;&nbsp;&nbsp;&nbsp; [2.10 Walls Material, Foundation and House Type](#2.10)   \n&nbsp;&nbsp;&nbsp;&nbsp; [2.11 Amount Credit Distribution](#2.11)    \n&nbsp;&nbsp;&nbsp;&nbsp; [2.12 Amount Annuity Distribution - Distribution](#2.12)  \n&nbsp;&nbsp;&nbsp;&nbsp; [2.13 Amount Goods Price - Distribution](#2.13)   \n&nbsp;&nbsp;&nbsp;&nbsp; [2.14 Amount Region Population Relative](#2.14)    \n&nbsp;&nbsp;&nbsp;&nbsp; [2.15 Days Birth - Distribution](#2.15)   \n&nbsp;&nbsp;&nbsp;&nbsp; [2.16 Days Employed - Distribution](#2.16)    \n&nbsp;&nbsp;&nbsp;&nbsp; [2.17 Distribution of Num Days Registration](#2.17)  \n&nbsp;&nbsp;&nbsp;&nbsp; [2.18 Applicants Number of Family Members](#2.18)  \n&nbsp;&nbsp;&nbsp;&nbsp; [2.19 Applicants Number of Children](#2.19)  \n[3. Exploration - Bureau Data](#3)  \n&nbsp;&nbsp;&nbsp;&nbsp; [3.1 Snapshot - Bureau Data](#3)    \n[4. Exploration - Bureau Balance Data](#4)  \n&nbsp;&nbsp;&nbsp;&nbsp; [4.1 Snapshot - Bureau Balance Data](#3)     \n[5. Exploration - Credit Card Balance Data](#5)   \n&nbsp;&nbsp;&nbsp;&nbsp; [5.1 Snapshot - Credit Card Balance Data](#3)   \n[6. Exploration - POS Cash Balance Data](#6)   \n&nbsp;&nbsp;&nbsp;&nbsp; [6.1 Snapshot - POS Cash Balance Data](#3)   \n[7. Exploration - Previous Application Data](#7)   \n&nbsp;&nbsp;&nbsp;&nbsp; [7.1 Snapshot - Previous Application Data](#7.1)  \n&nbsp;&nbsp;&nbsp;&nbsp; [7.2 Contract Status Distribution - Previous Applications](#7.2)  \n&nbsp;&nbsp;&nbsp;&nbsp; [7.3 Suite Type Distribution - Previous Application](#7.3)    \n&nbsp;&nbsp;&nbsp;&nbsp; [7.4 Client Type Distribution  - Previous Application](#7.4)    \n&nbsp;&nbsp;&nbsp;&nbsp; [7.5 Channel Type Distribution - Previous Applications](#7.5)  \n[8. Exploration - Installation Payments](#8)  \n&nbsp;&nbsp;&nbsp;&nbsp; [8.1 Snapshot of Installation Payments](#3)  \n[9. Baseline Model](#9)  \n&nbsp;&nbsp;&nbsp;&nbsp; [9.1 Dataset Preparation](#9.1)  \n&nbsp;&nbsp;&nbsp;&nbsp; [9.2 Handelling Categorical Features](#9.2)     \n&nbsp;&nbsp;&nbsp;&nbsp; [9.3 Create Flat Dataset](#9.3)     \n&nbsp;&nbsp;&nbsp;&nbsp; [9.4 Validation Sets Preparation](#9.4)    \n&nbsp;&nbsp;&nbsp;&nbsp; [9.5 Model Fitting](#9.5)    \n&nbsp;&nbsp;&nbsp;&nbsp; [9.6 Feature Importance](#9.6)    \n&nbsp;&nbsp;&nbsp;&nbsp; [9.7 Prediction](#9.7)   \n\n\n\n## <a id=\"1\">1. Dataset Preparation </a>"], "code": "from plotly.offline import init_notebook_mode, iplot\nfrom wordcloud import WordCloud\nimport plotly.graph_objs as go\nimport matplotlib.pyplot as plt\nimport plotly.plotly as py\nfrom plotly import tools\nfrom datetime import date\nimport pandas as pd\nimport numpy as np \nimport seaborn as sns\nimport random \nimport warnings\nwarnings.filterwarnings(\"ignore\")\ninit_notebook_mode(connected=True)\n\npath = \"../input/\"\n\ndef bar_hor(df, col, title, color, w=None, h=None, lm=0, limit=100, return_trace=False, rev=False, xlb = False):\n    cnt_srs = df[col].value_counts()\n    yy = cnt_srs.head(limit).index[::-1] \n    xx = cnt_srs.head(limit).values[::-1] \n    if rev:\n        yy = cnt_srs.tail(limit).index[::-1] \n        xx = cnt_srs.tail(limit).values[::-1] \n    if xlb:\n        trace = go.Bar(y=xlb, x=xx, orientation = 'h', marker=dict(color=color))\n    else:\n        trace = go.Bar(y=yy, x=xx, orientation = 'h', marker=dict(color=color))\n    if return_trace:\n        return trace \n    layout = dict(title=title, margin=dict(l=lm), width=w, height=h)\n    data = [trace]\n    fig = go.Figure(data=data, layout=layout)\n    iplot(fig)\n\ndef bar_hor_noagg(x, y, title, color, w=None, h=None, lm=0, limit=100, rt=False):\n    trace = go.Bar(y=x, x=y, orientation = 'h', marker=dict(color=color))\n    if rt:\n        return trace\n    layout = dict(title=title, margin=dict(l=lm), width=w, height=h)\n    data = [trace]\n    fig = go.Figure(data=data, layout=layout)\n    iplot(fig)\n\n\ndef bar_ver_noagg(x, y, title, color, w=None, h=None, lm=0, rt = False):\n    trace = go.Bar(y=y, x=x, marker=dict(color=color))\n    if rt:\n        return trace\n    layout = dict(title=title, margin=dict(l=lm), width=w, height=h)\n    data = [trace]\n    fig = go.Figure(data=data, layout=layout)\n    iplot(fig)\n    \ndef gp(col, title):\n    df1 = app_train[app_train[\"TARGET\"] == 1]\n    df0 = app_train[app_train[\"TARGET\"] == 0]\n    a1 = df1[col].value_counts()\n    b1 = df0[col].value_counts()\n    \n    total = dict(app_train[col].value_counts())\n    x0 = a1.index\n    x1 = b1.index\n    \n    y0 = [float(x)*100 / total[x0[i]] for i,x in enumerate(a1.values)]\n    y1 = [float(x)*100 / total[x1[i]] for i,x in enumerate(b1.values)]\n\n    trace1 = go.Bar(x=a1.index, y=y0, name='Target : 1', marker=dict(color=\"#96D38C\"))\n    trace2 = go.Bar(x=b1.index, y=y1, name='Target : 0', marker=dict(color=\"#FEBFB3\"))\n    return trace1, trace2 ", "processed": ["home credit default risk explor baselin model mani peopl struggl get loan due insuffici non exist credit histori unfortun popul often taken advantag untrustworthi lender home credit strive broaden financi inclus unbank popul provid posit safe borrow experi order make sure underserv popul posit loan experi home credit make use varieti altern data includ telco transact inform predict client repay abil home credit current use variou statist machin learn method make predict challeng kaggler help unlock full potenti data ensur client capabl repay reject loan given princip matur repay calendar empow client success simpl notebook explor baselin model home credit default risk data content 1 dataset prepar 1 2 explor applic train 2 nbsp nbsp nbsp nbsp 2 1 snapshot applic train 2 1 nbsp nbsp nbsp nbsp 2 2 distribut target variabl 2 2 nbsp nbsp nbsp nbsp 2 3 applic gender type 2 3 nbsp nbsp nbsp nbsp 2 4 famili statu applic take loan 2 4 nbsp nbsp nbsp nbsp 2 5 applic real estat car 2 5 nbsp nbsp nbsp nbsp 2 6 suit type incom type applic 2 6 nbsp nbsp nbsp nbsp 2 7 applic contract type 2 7 nbsp nbsp nbsp nbsp 2 8 educ type occup type 2 8 nbsp nbsp nbsp nbsp 2 9 organ type occup type 2 9 nbsp nbsp nbsp nbsp 2 10 wall materi foundat hous type 2 10 nbsp nbsp nbsp nbsp 2 11 amount credit distribut 2 11 nbsp nbsp nbsp nbsp 2 12 amount annuiti distribut distribut 2 12 nbsp nbsp nbsp nbsp 2 13 amount good price distribut 2 13 nbsp nbsp nbsp nbsp 2 14 amount region popul rel 2 14 nbsp nbsp nbsp nbsp 2 15 day birth distribut 2 15 nbsp nbsp nbsp nbsp 2 16 day employ distribut 2 16 nbsp nbsp nbsp nbsp 2 17 distribut num day registr 2 17 nbsp nbsp nbsp nbsp 2 18 applic number famili member 2 18 nbsp nbsp nbsp nbsp 2 19 applic number child 2 19 3 explor bureau data 3 nbsp nbsp nbsp nbsp 3 1 snapshot bureau data 3 4 explor bureau balanc data 4 nbsp nbsp nbsp nbsp 4 1 snapshot bureau balanc data 3 5 explor credit card balanc data 5 nbsp nbsp nbsp nbsp 5 1 snapshot credit card balanc data 3 6 explor po cash balanc data 6 nbsp nbsp nbsp nbsp 6 1 snapshot po cash balanc data 3 7 explor previou applic data 7 nbsp nbsp nbsp nbsp 7 1 snapshot previou applic data 7 1 nbsp nbsp nbsp nbsp 7 2 contract statu distribut previou applic 7 2 nbsp nbsp nbsp nbsp 7 3 suit type distribut previou applic 7 3 nbsp nbsp nbsp nbsp 7 4 client type distribut previou applic 7 4 nbsp nbsp nbsp nbsp 7 5 channel type distribut previou applic 7 5 8 explor instal payment 8 nbsp nbsp nbsp nbsp 8 1 snapshot instal payment 3 9 baselin model 9 nbsp nbsp nbsp nbsp 9 1 dataset prepar 9 1 nbsp nbsp nbsp nbsp 9 2 handel categor featur 9 2 nbsp nbsp nbsp nbsp 9 3 creat flat dataset 9 3 nbsp nbsp nbsp nbsp 9 4 valid set prepar 9 4 nbsp nbsp nbsp nbsp 9 5 model fit 9 5 nbsp nbsp nbsp nbsp 9 6 featur import 9 6 nbsp nbsp nbsp nbsp 9 7 predict 9 7 id 1 1 dataset prepar"]}, {"markdown": ["> The target variable is slightly imbalance with the majority of loans has the target equals to 0 which indicates that individuals did not had any problems in paying installments in given time. There are about 91% loans which is equal to about 282K with target = 0, While only 9% of the total loans (about 24K applicants) in this dataset involved the applicants having problems in repaying the loan / making installments.  \n\n### <a href=\"2.3\">2.3 Gender Type of Applicants </a>"], "code": "# Reference: https://www.kaggle.com/code/shivamb/homecreditrisk-extensive-eda-baseline-0-772\n\ntr0 = bar_hor(app_train, \"CODE_GENDER\", \"Distribution of CODE_GENDER Variable\" ,\"#f975ae\", w=700, lm=100, return_trace= True)\ntr1, tr2 = gp('CODE_GENDER', 'Distribution of Target with Applicant Gender')\n\nfig = tools.make_subplots(rows=1, cols=3, print_grid=False, subplot_titles = [\"Gender Distribution\" , \"Gender, Target=1\" ,\"Gender, Target=0\"])\nfig.append_trace(tr0, 1, 1);\nfig.append_trace(tr1, 1, 2);\nfig.append_trace(tr2, 1, 3);\nfig['layout'].update(height=350, showlegend=False, margin=dict(l=50));\niplot(fig);", "processed": ["target variabl slightli imbal major loan target equal 0 indic individu problem pay instal given time 91 loan equal 282k target 0 9 total loan 24k applic dataset involv applic problem repay loan make instal href 2 3 2 3 gender type applic"]}, {"markdown": ["> In the applicant's data women have applied for a larger majority of loans which is almost the double as the men. In total, there are about 202,448 loan applications filed by females in contrast to about 105,059 applications filed by males. However, a larger percentage (about 10% of the total) of men had the problems in paying the loan or making installments within time as compared to women applicants (about 7%). \n\n### <a href=\"2.4\">2.4 Family Status of Applicants </a>"], "code": "# Reference: https://www.kaggle.com/code/shivamb/homecreditrisk-extensive-eda-baseline-0-772\n\ntr0 = bar_hor(app_train, \"NAME_FAMILY_STATUS\", \"Distribution of CODE_GENDER Variable\" ,\"#f975ae\", w=700, lm=100, return_trace= True)\ntr1, tr2 = gp('NAME_FAMILY_STATUS', 'Distribution of Target with Applicant Gender')\n\nfig = tools.make_subplots(rows=1, cols=3, print_grid=False, subplot_titles = [\"Family Status Distribution\" , \"Family Status, Target = 1\" ,\"Family Status, Target = 0\"])\nfig.append_trace(tr0, 1, 1);\nfig.append_trace(tr1, 1, 2);\nfig.append_trace(tr2, 1, 3);\nfig['layout'].update(height=350, showlegend=False, margin=dict(l=120));\niplot(fig);", "processed": ["applic data woman appli larger major loan almost doubl men total 202 448 loan applic file femal contrast 105 059 applic file male howev larger percentag 10 total men problem pay loan make instal within time compar woman applic 7 href 2 4 2 4 famili statu applic"]}, {"markdown": ["> Married people have applied for a larger number of loan applications about 196K, However, people having Civil Marriage has the highest percentage (about 10%) of loan problems and challenges. \n\n### <a href=\"2.5\">2.5. Does applicants own Real Estate or Car ?</a>"], "code": "# Reference: https://www.kaggle.com/code/shivamb/homecreditrisk-extensive-eda-baseline-0-772\n\n## real estate \nt = app_train['FLAG_OWN_REALTY'].value_counts()\nlabels = t.index\nvalues = t.values\ncolors = ['#96D38C','#FEBFB3']\ntrace = go.Pie(labels=labels, values=values,\n               hoverinfo='', textinfo='',\n               textfont=dict(size=12),\n               marker=dict(colors=colors,\n                           line=dict(color='#fff', width=2)))\nlayout = go.Layout(title='Applicants Owning Real Estate', height=400)\nfig = go.Figure(data=[trace], layout=layout)\niplot(fig)\n\n\nt = app_train['FLAG_OWN_CAR'].value_counts()\nlabels = t.index\nvalues = t.values\ncolors = ['#FEBFB3','#96D38C']\ntrace = go.Pie(labels=labels, values=values,\n               hoverinfo='', textinfo='',\n               textfont=dict(size=12),\n               marker=dict(colors=colors,\n                           line=dict(color='#fff', width=2)))\nlayout = go.Layout(title='Applicants Owning Car', height=400)\nfig = go.Figure(data=[trace], layout=layout)\niplot(fig)\n\n\ntr1, tr2 = gp('FLAG_OWN_REALTY', 'Applicants Owning Real Estate wrt Target Variable')\ntr3, tr4 = gp('FLAG_OWN_CAR', 'Applicants Owning Car wrt Target Variable')\nfig = tools.make_subplots(rows=1, cols=2, print_grid=False, \n                          subplot_titles = [\"% Applicants with RealEstate and Target = 1\", \"% Applicants with Car and Target = 1\"])\nfig.append_trace(tr1, 1, 1);\nfig.append_trace(tr3, 1, 2);\nfig['layout'].update(height=350, showlegend=False, margin=dict(l=120));\niplot(fig);", "processed": ["marri peopl appli larger number loan applic 196k howev peopl civil marriag highest percentag 10 loan problem challeng href 2 5 2 5 applic real estat car"]}, {"markdown": ["> About 70% of the applicants own Real Estate, while only 34% of applicants own Car who had applied for the loan in the past years. However, a higher percentage of people having payment difficulties was observed with applicants which did not owned Car or which did not owned Real Estate. \n\n### <a href=\"2.6\">2.6 Suite Type and Income Type of Applicants </a>"], "code": "# Reference: https://www.kaggle.com/code/shivamb/homecreditrisk-extensive-eda-baseline-0-772\n\ntr0 = bar_hor(app_train, \"NAME_TYPE_SUITE\", \"Distribution of CODE_GENDER Variable\" ,\"#f975ae\", w=700, lm=100, return_trace= True)\ntr1 = bar_hor(app_train, \"NAME_INCOME_TYPE\", \"Distribution of CODE_GENDER Variable\" ,\"#f975ae\", w=700, lm=100, return_trace= True)\n\nfig = tools.make_subplots(rows=1, cols=2, print_grid=False, subplot_titles = ['Applicants Suite Type' , 'Applicants Income Type'])\nfig.append_trace(tr0, 1, 1);\nfig.append_trace(tr1, 1, 2);\nfig['layout'].update(height=400, showlegend=False, margin=dict(l=100));\niplot(fig);", "processed": ["70 applic real estat 34 applic car appli loan past year howev higher percentag peopl payment difficulti observ applic own car own real estat href 2 6 2 6 suit type incom type applic"]}, {"markdown": ["> Top 3 Type Suites which applies for loan are the houses which are:  \n    - Unaccompanined (about 248K applicants) \n    - Family (about 40K applicants)  \n    - Spouse, partner (about 11K applicants)    \n> The income type of people who applies for loan include about 8 categroes, top ones are : \n    - Working Class (158K)\n    - Commercial Associate (71K)\n    - Pensiner (55K)\n\n### <a id=\"2.6.1\">2.6.1 How does Target Varies with Suite and Income Type of Applicants </a>"], "code": "# Reference: https://www.kaggle.com/code/shivamb/homecreditrisk-extensive-eda-baseline-0-772\n\ntr1, tr2 = gp('NAME_TYPE_SUITE', 'Applicants Type Suites which repayed the loan')\nfig = tools.make_subplots(rows=1, cols=2, print_grid=False, \n                          subplot_titles = [\"Applicants Type Suites distribution when Target = 1\", \"Applicants Type Suites distribution when Target = 0\"])\nfig.append_trace(tr1, 1, 1);\nfig.append_trace(tr2, 1, 2);\nfig['layout'].update(height=350, showlegend=False, margin=dict(l=120));\niplot(fig);\n\n\ntr1, tr2 = gp('NAME_INCOME_TYPE', 'Applicants Income Types which repayed the loan')\nfig = tools.make_subplots(rows=1, cols=2, print_grid=False, \n                          subplot_titles = [\"Applicants Income Types when Target = 1\", \"Applicants Income Type When Target = 0\"])\nfig.append_trace(tr1, 1, 1);\nfig.append_trace(tr2, 1, 2);\nfig['layout'].update(height=350, showlegend=False, margin=dict(l=120));\niplot(fig);", "processed": ["top 3 type suit appli loan hous unaccompanin 248k applic famili 40k applic spous partner 11k applic incom type peopl appli loan includ 8 categro top one work class 158k commerci associ 71k pensin 55k id 2 6 1 2 6 1 target vari suit incom type applic"]}, {"markdown": ["> We see that Applicants having Income Types : Maternity Leaves and UnEmployed has the highest percentage (about 40% and 36% approx) of Target = 1 ie. having more payment problems, while Pensioners have the least (about 5.3%). \n\n### <a id=\"2.7\">2.7. Applicant's Contract Type</a>"], "code": "# Reference: https://www.kaggle.com/code/shivamb/homecreditrisk-extensive-eda-baseline-0-772\n\nt = app_train['NAME_CONTRACT_TYPE'].value_counts()\nlabels = t.index\nvalues = t.values\ncolors = ['#FEBFB3','#96D38C']\ntrace = go.Pie(labels=labels, values=values,\n               hoverinfo='', textinfo='',\n               textfont=dict(size=12),\n               marker=dict(colors=colors,\n                           line=dict(color='#fff', width=2)))\nlayout = go.Layout(title='Applicants Contract Type', height=400)\nfig = go.Figure(data=[trace], layout=layout)\niplot(fig)", "processed": ["see applic incom type matern leav unemploy highest percentag 40 36 approx target 1 ie payment problem pension least 5 3 id 2 7 2 7 applic contract type"]}, {"markdown": ["> Cash loans with about 278K loans contributes to a majorty of total lonas in this dataset. Revolving loans has significantly lesser number equal to about 29K as compared to Cash loans. \n\n### <a id=\"2.8\">2.8 Education Type and Housing Type </a>"], "code": "# Reference: https://www.kaggle.com/code/shivamb/homecreditrisk-extensive-eda-baseline-0-772\n\ntr1 = bar_hor(app_train, \"NAME_EDUCATION_TYPE\", \"Distribution of \" ,\"#f975ae\", w=700, lm=100, return_trace= True)\ntr2 = bar_hor(app_train, \"NAME_HOUSING_TYPE\", \"Distribution of \" ,\"#f975ae\", w=700, lm=100, return_trace = True)\n\nfig = tools.make_subplots(rows=1, cols=2, print_grid=False, subplot_titles = ['Applicants Education Type', 'Applicants Housing Type' ])\nfig.append_trace(tr1, 1, 1);\nfig.append_trace(tr2, 1, 2);\nfig['layout'].update(height=400, showlegend=False, margin=dict(l=100));\niplot(fig);\n\n\ntr1, tr2 = gp('NAME_EDUCATION_TYPE', 'Applicants Income Types which repayed the loan')\ntr3, tr4 = gp('NAME_HOUSING_TYPE', 'Applicants Income Types which repayed the loan')\nfig = tools.make_subplots(rows=1, cols=2, print_grid=False, \n                          subplot_titles = [\"Applicants Education Types, Target=1\", \"Applicants Housing Type, Target=1\"])\nfig.append_trace(tr1, 1, 1);\nfig.append_trace(tr3, 1, 2);\nfig['layout'].update(height=350, showlegend=False, margin=dict(l=30));\niplot(fig);", "processed": ["cash loan 278k loan contribut majorti total lona dataset revolv loan significantli lesser number equal 29k compar cash loan id 2 8 2 8 educ type hous type"]}, {"markdown": ["> A large number of applications (218K) are filed by people having secondary education followed by people with Higher Education with 75K applications. Applicants living in House / apartments has the highest number of loan apllications equal to 272K. While we see that the applicants with Lower Secondary education status has the highest percentage of payment related problems. Also, Applicants living in apartments or living with parents also shows the same trend. ", "### <a id=\"2.9\">2.9. Which Organization and Occupation Type applies for loan and which repays </a>"], "code": "# Reference: https://www.kaggle.com/code/shivamb/homecreditrisk-extensive-eda-baseline-0-772\n\ntr1 = bar_hor(app_train, \"ORGANIZATION_TYPE\", \"Distribution of \" ,\"#f975ae\", w=700, lm=100, return_trace= True)\ntr2 = bar_hor(app_train, \"OCCUPATION_TYPE\", \"Distribution of \" ,\"#f975ae\", w=700, lm=100, return_trace = True)\nfig = tools.make_subplots(rows=1, cols=2, print_grid=False, subplot_titles = ['Applicants Organization Type', 'Applicants Occupation Type' ])\nfig.append_trace(tr1, 1, 1);\nfig.append_trace(tr2, 1, 2);\nfig['layout'].update(height=600, showlegend=False, margin=dict(l=150));\niplot(fig);", "processed": ["larg number applic 218k file peopl secondari educ follow peopl higher educ 75k applic applic live hous apart highest number loan apllic equal 272k see applic lower secondari educ statu highest percentag payment relat problem also applic live apart live parent also show trend", "id 2 9 2 9 organ occup type appli loan repay"]}, {"markdown": ["> Top Applicant's who applied for loan : Laborers - Approx 55 K, Sales Staff - Approx 32 K, Core staff - Approx 28 K. Entity Type 3 type organizations have filed maximum number of loans equal to approx 67K\n\n### <a id=\"2.9.1\">2.9.1 Target Variable with respect to Organization and Occupation Type </a>"], "code": "# Reference: https://www.kaggle.com/code/shivamb/homecreditrisk-extensive-eda-baseline-0-772\n\ntr1, tr2 = gp('ORGANIZATION_TYPE', 'Applicants Income Types which repayed the loan')\ntr3, tr4 = gp('OCCUPATION_TYPE', 'Applicants Income Types which repayed the loan')\nfig = tools.make_subplots(rows=1, cols=2, print_grid=False, \n                          subplot_titles = [\"Applicants Organization Types - Repayed\", \"Applicants Occupation Type - Repayed\"])\nfig.append_trace(tr1, 1, 1);\nfig.append_trace(tr3, 1, 2);\nfig['layout'].update(height=350, showlegend=False, margin=dict(l=120));\niplot(fig);", "processed": ["top applic appli loan labor approx 55 k sale staff approx 32 k core staff approx 28 k entiti type 3 type organ file maximum number loan equal approx 67k id 2 9 1 2 9 1 target variabl respect organ occup type"]}, {"markdown": ["### <a id=\"2.10\">2.10 Walls Material, Foundation, and House Type </a>"], "code": "# Reference: https://www.kaggle.com/code/shivamb/homecreditrisk-extensive-eda-baseline-0-772\n\ntr1 = bar_hor(app_train, \"FONDKAPREMONT_MODE\", \"Distribution of FLAG_OWN_REALTY\" ,\"#639af2\", w=700, lm=100, return_trace= True)\ntr2 = bar_hor(app_train, \"WALLSMATERIAL_MODE\", \"Distribution of FLAG_OWN_CAR\" ,\"#a4c5f9\", w=700, lm=100, return_trace = True)\ntr1 = bar_hor(app_train, \"HOUSETYPE_MODE\", \"Distribution of FLAG_OWN_CAR\" ,\"#a4c5f9\", w=700, lm=100, return_trace = True)\n\nfig = tools.make_subplots(rows=1, cols=2, print_grid=False, subplot_titles = [ 'House Type', 'Walls Material'])\nfig.append_trace(tr1, 1, 1);\nfig.append_trace(tr2, 1, 2);\n# fig.append_trace(tr3, 1, 3);\n\nfig['layout'].update(height=400, showlegend=False, margin=dict(l=100));\niplot(fig);", "processed": ["id 2 10 2 10 wall materi foundat hous type"]}, {"markdown": ["> - \"Blocks and Flats\" related house types have filed the largest number of loan applications equal to about 150K, rest of the other categories : Specific Housing and Terraced house have less than 1500 applications. Similarly houses having Panel and Stone Brick type walls material have filed the largest applciations close to 120K combined. \n\n### <a id=\"2.10.1\">2.10.1 Target Variable with respect to Walls Material, Fondkappremont, House Type </a>"], "code": "# Reference: https://www.kaggle.com/code/shivamb/homecreditrisk-extensive-eda-baseline-0-772\n\ntr1, tr2 = gp('HOUSETYPE_MODE', 'Applicants Income Types which repayed the loan')\ntr3, tr4 = gp('WALLSMATERIAL_MODE', 'Applicants Income Types which repayed the loan')\nfig = tools.make_subplots(rows=1, cols=2, print_grid=False, subplot_titles = [\"HouseTypes - Repayed\", \"WallsMaterial - Repayed\"])\nfig.append_trace(tr1, 1, 1);\nfig.append_trace(tr3, 1, 2);\nfig['layout'].update(height=350, showlegend=False, margin=dict(l=120));\niplot(fig);", "processed": ["block flat relat hous type file largest number loan applic equal 150k rest categori specif hous terrac hous le 1500 applic similarli hous panel stone brick type wall materi file largest applciat close 120k combin id 2 10 1 2 10 1 target variabl respect wall materi fondkappremont hous type"]}, {"markdown": ["### <a id=\"2.11\">2.11. Distribution of Amount Credit </a>"], "code": "# Reference: https://www.kaggle.com/code/shivamb/homecreditrisk-extensive-eda-baseline-0-772\n\nplt.figure(figsize=(12,5))\nplt.title(\"Distribution of AMT_CREDIT\")\nax = sns.distplot(app_train[\"AMT_CREDIT\"])", "processed": ["id 2 11 2 11 distribut amount credit"]}, {"markdown": ["### <a id=\"2.12\">2.12 Distribution of Amount AMT_ANNUITY </a>"], "code": "# Reference: https://www.kaggle.com/code/shivamb/homecreditrisk-extensive-eda-baseline-0-772\n\nplt.figure(figsize=(12,5))\nplt.title(\"Distribution of AMT_ANNUITY\")\nax = sns.distplot(app_train[\"AMT_ANNUITY\"].dropna())", "processed": ["id 2 12 2 12 distribut amount amt annuiti"]}, {"markdown": ["### <a id=\"2.13\">2.13 Distribution of Amount AMT_GOODS_PRICE </a>"], "code": "# Reference: https://www.kaggle.com/code/shivamb/homecreditrisk-extensive-eda-baseline-0-772\n\nplt.figure(figsize=(12,5))\nplt.title(\"Distribution of AMT_GOODS_PRICE\")\nax = sns.distplot(app_train[\"AMT_GOODS_PRICE\"].dropna())", "processed": ["id 2 13 2 13 distribut amount amt good price"]}, {"markdown": ["### <a id=\"2.14\">2.14 Distribution of Amount REGION_POPULATION_RELATIVE </a>"], "code": "# Reference: https://www.kaggle.com/code/shivamb/homecreditrisk-extensive-eda-baseline-0-772\n\nplt.figure(figsize=(12,5))\nplt.title(\"Distribution of REGION_POPULATION_RELATIVE\")\nax = sns.distplot(app_train[\"REGION_POPULATION_RELATIVE\"])", "processed": ["id 2 14 2 14 distribut amount region popul rel"]}, {"markdown": ["### <a id=\"2.15\">2.15 Distribution of Amount DAYS_BIRTH </a>"], "code": "# Reference: https://www.kaggle.com/code/shivamb/homecreditrisk-extensive-eda-baseline-0-772\n\nplt.figure(figsize=(12,5))\nplt.title(\"Distribution of DAYS_BIRTH\")\nax = sns.distplot(app_train[\"DAYS_BIRTH\"])", "processed": ["id 2 15 2 15 distribut amount day birth"]}, {"markdown": ["### <a id=\"2.16\">2.16 Distribution of Amount DAYS_EMPLOYED </a>"], "code": "# Reference: https://www.kaggle.com/code/shivamb/homecreditrisk-extensive-eda-baseline-0-772\n\nplt.figure(figsize=(12,5))\nplt.title(\"Distribution of DAYS_EMPLOYED\")\nax = sns.distplot(app_train[\"DAYS_EMPLOYED\"])", "processed": ["id 2 16 2 16 distribut amount day employ"]}, {"markdown": ["### <a id=\"2.17\">2.17 Distribution of Number of Days for Registration</a>"], "code": "# Reference: https://www.kaggle.com/code/shivamb/homecreditrisk-extensive-eda-baseline-0-772\n\nplt.figure(figsize=(12,5))\nplt.title(\"Distribution of DAYS_REGISTRATION\")\nax = sns.distplot(app_train[\"DAYS_REGISTRATION\"])", "processed": ["id 2 17 2 17 distribut number day registr"]}, {"markdown": ["### <a id=\"2.18\">2.18 How many Family Members does the applicants has </a>"], "code": "# Reference: https://www.kaggle.com/code/shivamb/homecreditrisk-extensive-eda-baseline-0-772\n\nt = app_train[\"CNT_FAM_MEMBERS\"].value_counts()\nt1 = pd.DataFrame()\nt1['x'] = t.index \nt1['y'] = t.values \n\nplt.figure(figsize=(12,5));\nplt.title(\"Distribution of Applicant's Family Members Count\");\nax = sns.barplot(data=t1, x=\"x\", y=\"y\", color=\"#f975ae\");\nax.spines['right'].set_visible(False);\nax.spines['top'].set_visible(False);\n\nax.set_ylabel('');    \nax.set_xlabel('');", "processed": ["id 2 18 2 18 mani famili member applic"]}, {"markdown": ["> Most of the applicants who applied for loan had 2 family members in total\n\n### <a id=\"2.19\"> 2.19 How many Children does the applicants have </a>"], "code": "# Reference: https://www.kaggle.com/code/shivamb/homecreditrisk-extensive-eda-baseline-0-772\n\nt = app_train[\"CNT_CHILDREN\"].value_counts()\nt1 = pd.DataFrame()\nt1['x'] = t.index \nt1['y'] = t.values \n\nplt.figure(figsize=(12,5));\nplt.title(\"Distribution of Applicant's Number of Children\");\nax = sns.barplot(data=t1, x=\"x\", y=\"y\", color=\"#f975ae\");\nax.spines['right'].set_visible(False);\nax.spines['top'].set_visible(False);\n\nax.set_ylabel('');    \nax.set_xlabel('');", "processed": ["applic appli loan 2 famili member total id 2 19 2 19 mani child applic"]}, {"markdown": ["### <a id=\"7.2\">7.2 Contract Status Distribution in Previously Filed Applications</a>"], "code": "# Reference: https://www.kaggle.com/code/shivamb/homecreditrisk-extensive-eda-baseline-0-772\n\nt = previous_application['NAME_CONTRACT_STATUS'].value_counts()\nlabels = t.index\nvalues = t.values\n\ncolors = ['#FEBFB3', '#E1396C', '#96D38C', '#D0F9B1']\n\ntrace = go.Pie(labels=labels, values=values,\n               hoverinfo='', textinfo='',\n               textfont=dict(size=12),\n               marker=dict(colors=colors,\n                           line=dict(color='#fff', width=2)))\n\nlayout = go.Layout(title='Name Contract Status in Previous Applications', height=400)\nfig = go.Figure(data=[trace], layout=layout)\niplot(fig)", "processed": ["id 7 2 7 2 contract statu distribut previous file applic"]}, {"markdown": ["> - A large number of people (about 62%) had their previous applications approved, while about 19% of them had cancelled and other 17% were resued. \n\n### <a id=\"7.3\">7.3 Suite Type Distribution of Previous Applications</a>"], "code": "# Reference: https://www.kaggle.com/code/shivamb/homecreditrisk-extensive-eda-baseline-0-772\n\nt = previous_application['NAME_TYPE_SUITE'].value_counts()\nlabels = t.index\nvalues = t.values\n\ncolors = ['#FEBFB3', '#E1396C', '#96D38C', '#D0F9B1']\n\ntrace = go.Pie(labels=labels, values=values,\n               hoverinfo='', textinfo='',\n               textfont=dict(size=12),\n               marker=dict(colors=colors,\n                           line=dict(color='#fff', width=2)))\n\nlayout = go.Layout(title='Suite Type in Previous Application Distribution', height=400)\nfig = go.Figure(data=[trace], layout=layout)\niplot(fig)", "processed": ["larg number peopl 62 previou applic approv 19 cancel 17 resu id 7 3 7 3 suit type distribut previou applic"]}, {"markdown": ["> - A majority of applicants had previous applications having Unaccompanied Suite Type (about 60%) followed by Family related suite type (about 25%)\n\n### <a id=\"7.4\">7.4 Client Type of Previous Applications</a>"], "code": "# Reference: https://www.kaggle.com/code/shivamb/homecreditrisk-extensive-eda-baseline-0-772\n\nt = previous_application['NAME_CLIENT_TYPE'].value_counts()\nlabels = t.index\nvalues = t.values\n\ncolors = ['#FEBFB3', '#E1396C', '#96D38C', '#D0F9B1']\n\ntrace = go.Pie(labels=labels, values=values,\n               hoverinfo='', textinfo='',\n               textfont=dict(size=12),\n               marker=dict(colors=colors,\n                           line=dict(color='#fff', width=2)))\n\nlayout = go.Layout(title='Client Type in Previous Applications', height=400)\nfig = go.Figure(data=[trace], layout=layout)\niplot(fig)", "processed": ["major applic previou applic unaccompani suit type 60 follow famili relat suit type 25 id 7 4 7 4 client type previou applic"]}, {"markdown": ["> - About 74% of the previous applications were Repeater Clients, while only 18% are new. About 8% are refreshed. \n\n### <a id=\"7.5\">7.5 Channel Type - Previous Applications </a>"], "code": "# Reference: https://www.kaggle.com/code/shivamb/homecreditrisk-extensive-eda-baseline-0-772\n\nt = previous_application['CHANNEL_TYPE'].value_counts()\nlabels = t.index\nvalues = t.values\n\ncolors = ['#FEBFB3', '#E1396C', '#96D38C', '#D0F9B1']\n\ntrace = go.Pie(labels=labels, values=values,\n               hoverinfo='', textinfo='',\n               textfont=dict(size=12),\n               marker=dict(colors=colors,\n                           line=dict(color='#fff', width=2)))\n\nlayout = go.Layout(title='Channel Type in Previous Applications', height=400)\nfig = go.Figure(data=[trace], layout=layout)\niplot(fig)", "processed": ["74 previou applic repeat client 18 new 8 refresh id 7 5 7 5 channel type previou applic"]}, {"markdown": ["We need the same for our test data later:"], "code": "# Reference: https://www.kaggle.com/code/allunia/rsna-ih-detection-eda\n\ntestdf = submission.ID.str.rsplit(\"_\", n=1, expand=True)\ntestdf = testdf.rename({0: \"id\", 1: \"subtype\"}, axis=1)\ntestdf.loc[:, \"label\"] = 0\ntestdf.head()\nmulti_target_count = traindf.groupby(\"id\").label.sum()\n\nfig, ax = plt.subplots(1,3,figsize=(20,5))\n\nsns.countplot(traindf.label, ax=ax[0], palette=\"Reds\")\nax[0].set_xlabel(\"Binary label\")\nax[0].set_title(\"How often do we observe a positive label?\");\n\nsns.countplot(multi_target_count, ax=ax[1])\nax[1].set_xlabel(\"Numer of targets per image\")\nax[1].set_ylabel(\"Frequency\")\nax[1].set_title(\"Multi-Hot occurences\")\n\nsns.barplot(x=subtype_counts.index, y=subtype_counts.values, ax=ax[2], palette=\"Set2\")\nplt.xticks(rotation=45); \nax[2].set_title(\"How much binary imbalance do we have?\")\nax[2].set_ylabel(\"% of positive occurences (1)\");\n", "processed": ["need test data later"]}, {"markdown": ["Ok, I learnt... \n\n* that hounsfield units are a measurement to describe radiodensity. \n* different tissues have different HUs.\n* Our eye can only detect ~6% change in greyscale (16 shades of grey).\n* Given 2000 HU of one image (-1000 to 1000), this means that 1 greyscale covers 8 HUs.\n* Consequently there can happen a change of 120 HUs unit our eye is able to detect an intensity change in the image.\n* The example of a hemorrhage in the brain shows relevant HUs in the range of 8-70. We won't be able to see important changes in the intensity to detect the hemorrhage. \n* This is the reason why we have to focus 256 shades of grey into a small range/window of HU units. (WINDOW)\n* The level means where this window is centered.\n", "## What is covered by raw pixel values? <a class=\"anchor\" id=\"pixelarray\"></a>\n\nNo! If we browse through the dicom files, we can see that this is not true. Let's see how different dicom datasets differ in the distribution of pixel array values:"], "code": "# Reference: https://www.kaggle.com/code/allunia/rsna-ih-detection-eda\n\nfig, ax = plt.subplots(2,1,figsize=(20,10))\nfor file in train_files[0:10]:\n    dataset = pydicom.dcmread(train_dir + file)\n    image = dataset.pixel_array.flatten()\n    rescaled_image = image * dataset.RescaleSlope + dataset.RescaleIntercept\n    sns.distplot(image.flatten(), ax=ax[0]);\n    sns.distplot(rescaled_image.flatten(), ax=ax[1])\nax[0].set_title(\"Raw pixel array distributions for 10 examples\")\nax[1].set_title(\"HU unit distributions for 10 examples\");", "processed": ["ok learnt hounsfield unit measur describ radiodens differ tissu differ hu eye detect 6 chang greyscal 16 shade grey given 2000 hu one imag 1000 1000 mean 1 greyscal cover 8 hu consequ happen chang 120 hu unit eye abl detect intens chang imag exampl hemorrhag brain show relev hu rang 8 70 abl see import chang intens detect hemorrhag reason focu 256 shade grey small rang window hu unit window level mean window center", "cover raw pixel valu class anchor id pixelarray brow dicom file see true let see differ dicom dataset differ distribut pixel array valu"]}, {"markdown": ["Yeah! Great! We can see that both cases (-2000 and -3000) correspond to the outside region ot cylindrical CT scanners. Consequently we can set these values to -1000 (air in HU) without worries.", "## What does pixel spacing mean? <a class=\"anchor\" id=\"pixelspacing\"></a>\n\nWhen browsing through the dicom files, we can see that a value called pixel spacing changes as well! I don't know what that means, but perhaps we can understand it by looking at some extremes."], "code": "# Reference: https://www.kaggle.com/code/allunia/rsna-ih-detection-eda\n\npixelspacing_w = []\npixelspacing_h = []\nspacing_filenames = []\nfor file in train_files[0:1000]:\n    dataset = pydicom.dcmread(train_dir + file)\n    spacing = dataset.PixelSpacing\n    pixelspacing_w.append(spacing[0])\n    pixelspacing_h.append(spacing[1])\n    spacing_filenames.append(file)\n\nfig, ax = plt.subplots(1,2,figsize=(20,5))\nsns.distplot(pixelspacing_w, ax=ax[0], color=\"Limegreen\", kde=False)\nax[0].set_title(\"Pixel spacing width \\n distribution\")\nax[0].set_ylabel(\"Frequency given 1000 images\")\nsns.distplot(pixelspacing_h, ax=ax[1], color=\"Mediumseagreen\", kde=False)\nax[1].set_title(\"Pixel spacing height \\n distribution\");\nax[1].set_ylabel(\"Frequency given 1000 images\");\nmin_file = spacing_filenames[np.argmin(pixelspacing_w)]\nmax_file = spacing_filenames[np.argmax(pixelspacing_w)]\ndef rescale_pixelarray(dataset):\n    image = dataset.pixel_array\n    rescaled_image = image * dataset.RescaleSlope + dataset.RescaleIntercept\n    rescaled_image[rescaled_image < -1024] = -1024\n    return rescaled_image\nfig, ax = plt.subplots(1,2,figsize=(20,10))\n\ndataset_min = pydicom.dcmread(train_dir + min_file)\nimage_min = rescale_pixelarray(dataset_min)\n\ndataset_max = pydicom.dcmread(train_dir + max_file)\nimage_max = rescale_pixelarray(dataset_max)\n\nax[0].imshow(image_min, cmap=\"Spectral\")\nax[0].set_title(\"Pixel spacing w: \" + str(np.min(pixelspacing_w)))\nax[1].imshow(image_max, cmap=\"Spectral\");\nax[1].set_title(\"Pixel spacing w: \" + str(np.max(pixelspacing_w)))\nax[0].grid(False)\nax[1].grid(False)", "processed": ["yeah great see case 2000 3000 correspond outsid region ot cylindr ct scanner consequ set valu 1000 air hu without worri", "pixel space mean class anchor id pixelspac brow dicom file see valu call pixel space chang well know mean perhap understand look extrem"]}, {"markdown": ["And in the maximum case it's ~500 mm, consequently 50 cm. \n\nWhat does this mean for us? It means that small heads do not automatically mean that this is a child. It could also be an adult but zoomed out. I'm expected that the dataset holds patients with varying true head sizes ranging from childrens to adults. But the resolution might differ from scan to scan even if patients show same head sizes. Nonetheless our model should still be able to detect the different types of hemorrhage. I would say it's worth to add zooming as image augmentation technique to our workflow later. What do you think? ", "## The doctors windows <a class=\"anchor\" id=\"docwindows\"></a>\n\nTaking a look at the dicom dataset again we can see that there is already a window center and width given for us. But what does that mean? Was this done by a doctor who set the range to visualise the hemorrhage? Is this important for our algorithm? **We should be very careful now. If we would simply put 256 shades of grey into one window this would differ from patient to patient as the given window ranges are different. Consequently we would introduce a source of variation that is not given by original HU units per image.** What to do instead?\n\nI would like to collect window centers and width of ~1000 images to see the varity of doctos favorite windows. Then I would like to **setup a fixed window level and width that covers the majority of all window properties**. This way we can compare if a fixed custom window size is better suited that individual doctor window sizes. \n\n"], "code": "# Reference: https://www.kaggle.com/code/allunia/rsna-ih-detection-eda\n\ndef get_window_value(feature):\n    if type(feature) == pydicom.multival.MultiValue:\n        return np.int(feature[0])\n    else:\n        return np.int(feature)\nwindow_widths = []\nwindow_levels = []\nspacing_filenames = []\nfor file in train_files[0:1000]:\n    dataset = pydicom.dcmread(train_dir + file)\n    win_width = get_window_value(dataset.WindowWidth)\n    win_center = get_window_value(dataset.WindowCenter)\n    window_widths.append(win_width)\n    window_levels.append(win_center)\n    spacing_filenames.append(file)\nfig, ax = plt.subplots(2,2,figsize=(20,15))\n\nsns.distplot(window_widths, kde=False, ax=ax[0,0], color=\"Tomato\")\nax[0,0].set_title(\"Window width distribution \\n of 1000 images\")\nax[0,0].set_xlabel(\"Window width\")\nax[0,0].set_ylabel(\"Frequency\")\n\nsns.distplot(window_levels, kde=False, ax=ax[0,1], color=\"Firebrick\")\nax[0,1].set_title(\"Window level distribution \\n of 1000 images\")\nax[0,1].set_xlabel(\"Window level\")\nax[0,1].set_ylabel(\"Frequency\")\n\nsns.distplot(np.log(window_widths), kde=False, ax=ax[1,0], color=\"Tomato\")\nax[1,0].set_title(\"Log window width distribution \\n of 1000 images\")\nax[1,0].set_xlabel(\"Log window width\")\nax[1,0].set_ylabel(\"Frequency\")\n\nsns.distplot(np.log(window_levels), kde=False, ax=ax[1,1], color=\"Firebrick\")\nax[1,1].set_title(\"Log window level distribution \\n of 1000 images\")\nax[1,1].set_xlabel(\"Log window level\")\nax[1,1].set_ylabel(\"Frequency\");", "processed": ["maximum case 500 mm consequ 50 cm mean u mean small head automat mean child could also adult zoom expect dataset hold patient vari true head size rang child adult resolut might differ scan scan even patient show head size nonetheless model still abl detect differ type hemorrhag would say worth add zoom imag augment techniqu workflow later think", "doctor window class anchor id docwindow take look dicom dataset see alreadi window center width given u mean done doctor set rang visualis hemorrhag import algorithm care would simpli put 256 shade grey one window would differ patient patient given window rang differ consequ would introduc sourc variat given origin hu unit per imag instead would like collect window center width 1000 imag see variti docto favorit window would like setup fix window level width cover major window properti way compar fix custom window size better suit individu doctor window size"]}, {"markdown": ["### Insights\n\n* The first extreme case seems to be faulty. After windowing to a center of 40 and width of 150 we can't see the same nice patterns as for the min and the median cases.\n* By windowing to the same window we can't see differences between the median and the min case. But pixel values differ! ", "## The image shape <a class=\"anchor\" id=\"imageshape\"></a>"], "code": "# Reference: https://www.kaggle.com/code/allunia/rsna-ih-detection-eda\n\nnum_rows = []\nnum_cols = []\nspacing_filenames = []\nfor file in train_files[0:1000]:\n    dataset = pydicom.dcmread(train_dir + file)\n    num_rows.append(dataset.Rows)\n    num_cols.append(dataset.Columns)\n    spacing_filenames.append(file)\n\nfig, ax = plt.subplots(1,2,figsize=(20,5))\nsns.distplot(num_rows, ax=ax[0], color=\"Purple\", kde=False)\nax[0].set_title(\"Number of rows \\n distribution\")\nax[0].set_ylabel(\"Frequency given 1000 train images\")\nsns.distplot(num_cols, ax=ax[1], color=\"Violet\", kde=False)\nax[1].set_title(\"Number of columns \\n distribution\");\nax[1].set_ylabel(\"Frequency given 1000 train images\");", "processed": ["insight first extrem case seem faulti window center 40 width 150 see nice pattern min median case window window see differ median min case pixel valu differ", "imag shape class anchor id imageshap"]}, {"markdown": ["Ok it seems that most images are of shape 512x512. But I don't know if we can be sure about it. Is it possible that this can change from stage 1 to stage 2? What about the test data?"], "code": "# Reference: https://www.kaggle.com/code/allunia/rsna-ih-detection-eda\n\nnum_rows = []\nnum_cols = []\nspacing_filenames = []\nfor file in test_files[0:1000]:\n    dataset = pydicom.dcmread(test_dir + file)\n    num_rows.append(dataset.Rows)\n    num_cols.append(dataset.Columns)\n    spacing_filenames.append(file)\n\nfig, ax = plt.subplots(1,2,figsize=(20,5))\nsns.distplot(num_rows, ax=ax[0], color=\"Purple\", kde=False)\nax[0].set_title(\"Number of rows \\n distribution\")\nax[0].set_ylabel(\"Frequency given 1000 test images\")\nsns.distplot(num_cols, ax=ax[1], color=\"Violet\", kde=False)\nax[1].set_title(\"Number of columns \\n distribution\");\nax[1].set_ylabel(\"Frequency given 1000 test images\");", "processed": ["ok seem imag shape 512x512 know sure possibl chang stage 1 stage 2 test data"]}, {"markdown": ["## Functions used in this kernel\nThey are in the hidden cell below."], "code": "# Reference: https://www.kaggle.com/code/artgor/validation-feature-selection-interpretation-etc\n\nimport os\nimport time\nimport datetime\nimport json\nimport gc\nfrom numba import jit\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm_notebook\n\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom catboost import CatBoostRegressor, CatBoostClassifier\nfrom sklearn import metrics\n\nfrom itertools import product\n\nimport altair as alt\nfrom altair.vega import v5\nfrom IPython.display import HTML\n\n# using ideas from this kernel: https://www.kaggle.com/notslush/altair-visualization-2018-stackoverflow-survey\ndef prepare_altair():\n    \"\"\"\n    Helper function to prepare altair for working.\n    \"\"\"\n\n    vega_url = 'https://cdn.jsdelivr.net/npm/vega@' + v5.SCHEMA_VERSION\n    vega_lib_url = 'https://cdn.jsdelivr.net/npm/vega-lib'\n    vega_lite_url = 'https://cdn.jsdelivr.net/npm/vega-lite@' + alt.SCHEMA_VERSION\n    vega_embed_url = 'https://cdn.jsdelivr.net/npm/vega-embed@3'\n    noext = \"?noext\"\n    \n    paths = {\n        'vega': vega_url + noext,\n        'vega-lib': vega_lib_url + noext,\n        'vega-lite': vega_lite_url + noext,\n        'vega-embed': vega_embed_url + noext\n    }\n    \n    workaround = f\"\"\"    requirejs.config({{\n        baseUrl: 'https://cdn.jsdelivr.net/npm/',\n        paths: {paths}\n    }});\n    \"\"\"\n    \n    return workaround\n    \n\ndef add_autoincrement(render_func):\n    # Keep track of unique <div/> IDs\n    cache = {}\n    def wrapped(chart, id=\"vega-chart\", autoincrement=True):\n        if autoincrement:\n            if id in cache:\n                counter = 1 + cache[id]\n                cache[id] = counter\n            else:\n                cache[id] = 0\n            actual_id = id if cache[id] == 0 else id + '-' + str(cache[id])\n        else:\n            if id not in cache:\n                cache[id] = 0\n            actual_id = id\n        return render_func(chart, id=actual_id)\n    # Cache will stay outside and \n    return wrapped\n           \n\n@add_autoincrement\ndef render(chart, id=\"vega-chart\"):\n    \"\"\"\n    Helper function to plot altair visualizations.\n    \"\"\"\n    chart_str = \"\"\"\n    <div id=\"{id}\"></div><script>\n    require([\"vega-embed\"], function(vg_embed) {{\n        const spec = {chart};     \n        vg_embed(\"#{id}\", spec, {{defaultStyle: true}}).catch(console.warn);\n        console.log(\"anything?\");\n    }});\n    console.log(\"really...anything?\");\n    </script>\n    \"\"\"\n    return HTML(\n        chart_str.format(\n            id=id,\n            chart=json.dumps(chart) if isinstance(chart, dict) else chart.to_json(indent=None)\n        )\n    )\n    \n\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                c_prec = df[col].apply(lambda x: np.finfo(x).precision).max()\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max and c_prec == np.finfo(np.float16).precision:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max and c_prec == np.finfo(np.float32).precision:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df\n    \n\n@jit\ndef fast_auc(y_true, y_prob):\n    \"\"\"\n    fast roc_auc computation: https://www.kaggle.com/c/microsoft-malware-prediction/discussion/76013\n    \"\"\"\n    y_true = np.asarray(y_true)\n    y_true = y_true[np.argsort(y_prob)]\n    nfalse = 0\n    auc = 0\n    n = len(y_true)\n    for i in range(n):\n        y_i = y_true[i]\n        nfalse += (1 - y_i)\n        auc += y_i * nfalse\n    auc /= (nfalse * (n - nfalse))\n    return auc\n\n\ndef eval_auc(y_true, y_pred):\n    \"\"\"\n    Fast auc eval function for lgb.\n    \"\"\"\n    return 'auc', fast_auc(y_true, y_pred), True\n\n\ndef group_mean_log_mae(y_true, y_pred, types, floor=1e-9):\n    \"\"\"\n    Fast metric computation for this competition: https://www.kaggle.com/c/champs-scalar-coupling\n    Code is from this kernel: https://www.kaggle.com/uberkinder/efficient-metric\n    \"\"\"\n    maes = (y_true-y_pred).abs().groupby(types).mean()\n    return np.log(maes.map(lambda x: max(x, floor))).mean()\n    \n\ndef train_model_regression(X, X_test, y, params, folds=None, model_type='lgb', eval_metric='mae', columns=None, plot_feature_importance=False, model=None,\n                               verbose=10000, early_stopping_rounds=200, n_estimators=50000, splits=None, n_folds=3):\n    \"\"\"\n    A function to train a variety of regression models.\n    Returns dictionary with oof predictions, test predictions, scores and, if necessary, feature importances.\n    \n    :params: X - training data, can be pd.DataFrame or np.ndarray (after normalizing)\n    :params: X_test - test data, can be pd.DataFrame or np.ndarray (after normalizing)\n    :params: y - target\n    :params: folds - folds to split data\n    :params: model_type - type of model to use\n    :params: eval_metric - metric to use\n    :params: columns - columns to use. If None - use all columns\n    :params: plot_feature_importance - whether to plot feature importance of LGB\n    :params: model - sklearn model, works only for \"sklearn\" model type\n    \n    \"\"\"\n    columns = X.columns if columns is None else columns\n    X_test = X_test[columns]\n    splits = folds.split(X) if splits is None else splits\n    n_splits = folds.n_splits if splits is None else n_folds\n    \n    # to set up scoring parameters\n    metrics_dict = {'mae': {'lgb_metric_name': 'mae',\n                        'catboost_metric_name': 'MAE',\n                        'sklearn_scoring_function': metrics.mean_absolute_error},\n                    'group_mae': {'lgb_metric_name': 'mae',\n                        'catboost_metric_name': 'MAE',\n                        'scoring_function': group_mean_log_mae},\n                    'mse': {'lgb_metric_name': 'mse',\n                        'catboost_metric_name': 'MSE',\n                        'sklearn_scoring_function': metrics.mean_squared_error}\n                    }\n\n    \n    result_dict = {}\n    \n    # out-of-fold predictions on train data\n    oof = np.zeros(len(X))\n    \n    # averaged predictions on train data\n    prediction = np.zeros(len(X_test))\n    \n    # list of scores on folds\n    scores = []\n    feature_importance = pd.DataFrame()\n    \n    # split and train on folds\n    for fold_n, (train_index, valid_index) in enumerate(splits):\n        if verbose:\n            print(f'Fold {fold_n + 1} started at {time.ctime()}')\n        if type(X) == np.ndarray:\n            X_train, X_valid = X[columns][train_index], X[columns][valid_index]\n            y_train, y_valid = y[train_index], y[valid_index]\n        else:\n            X_train, X_valid = X[columns].iloc[train_index], X[columns].iloc[valid_index]\n            y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n            \n        if model_type == 'lgb':\n            model = lgb.LGBMRegressor(**params, n_estimators = n_estimators, n_jobs = -1)\n            model.fit(X_train, y_train, \n                    eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric=metrics_dict[eval_metric]['lgb_metric_name'],\n                    verbose=verbose, early_stopping_rounds=early_stopping_rounds)\n            \n            y_pred_valid = model.predict(X_valid)\n            y_pred = model.predict(X_test, num_iteration=model.best_iteration_)\n            \n        if model_type == 'xgb':\n            train_data = xgb.DMatrix(data=X_train, label=y_train, feature_names=X.columns)\n            valid_data = xgb.DMatrix(data=X_valid, label=y_valid, feature_names=X.columns)\n\n            watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]\n            model = xgb.train(dtrain=train_data, num_boost_round=20000, evals=watchlist, early_stopping_rounds=200, verbose_eval=verbose, params=params)\n            y_pred_valid = model.predict(xgb.DMatrix(X_valid, feature_names=X.columns), ntree_limit=model.best_ntree_limit)\n            y_pred = model.predict(xgb.DMatrix(X_test, feature_names=X.columns), ntree_limit=model.best_ntree_limit)\n        \n        if model_type == 'sklearn':\n            model = model\n            model.fit(X_train, y_train)\n            \n            y_pred_valid = model.predict(X_valid).reshape(-1,)\n            score = metrics_dict[eval_metric]['sklearn_scoring_function'](y_valid, y_pred_valid)\n            print(f'Fold {fold_n}. {eval_metric}: {score:.4f}.')\n            print('')\n            \n            y_pred = model.predict(X_test).reshape(-1,)\n        \n        if model_type == 'cat':\n            model = CatBoostRegressor(iterations=20000,  eval_metric=metrics_dict[eval_metric]['catboost_metric_name'], **params,\n                                      loss_function=metrics_dict[eval_metric]['catboost_metric_name'])\n            model.fit(X_train, y_train, eval_set=(X_valid, y_valid), cat_features=[], use_best_model=True, verbose=False)\n\n            y_pred_valid = model.predict(X_valid)\n            y_pred = model.predict(X_test)\n        \n        oof[valid_index] = y_pred_valid.reshape(-1,)\n        if eval_metric != 'group_mae':\n            scores.append(metrics_dict[eval_metric]['sklearn_scoring_function'](y_valid, y_pred_valid))\n        else:\n            scores.append(metrics_dict[eval_metric]['scoring_function'](y_valid, y_pred_valid, X_valid['type']))\n\n        prediction += y_pred    \n        \n        if model_type == 'lgb' and plot_feature_importance:\n            # feature importance\n            fold_importance = pd.DataFrame()\n            fold_importance[\"feature\"] = columns\n            fold_importance[\"importance\"] = model.feature_importances_\n            fold_importance[\"fold\"] = fold_n + 1\n            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n\n    prediction /= n_splits\n    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n    \n    result_dict['oof'] = oof\n    result_dict['prediction'] = prediction\n    result_dict['scores'] = scores\n    \n    if model_type == 'lgb':\n        if plot_feature_importance:\n            feature_importance[\"importance\"] /= n_splits\n            cols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n                by=\"importance\", ascending=False)[:50].index\n\n            best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n\n            plt.figure(figsize=(16, 12));\n            sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False));\n            plt.title('LGB Features (avg over folds)');\n            \n            result_dict['feature_importance'] = feature_importance\n        \n    return result_dict\n    \n\n\ndef train_model_classification(X, X_test, y, params, folds, model_type='lgb', eval_metric='auc', columns=None, plot_feature_importance=False, model=None,\n                               verbose=10000, early_stopping_rounds=200, n_estimators=50000, splits=None, n_folds=3):\n    \"\"\"\n    A function to train a variety of regression models.\n    Returns dictionary with oof predictions, test predictions, scores and, if necessary, feature importances.\n    \n    :params: X - training data, can be pd.DataFrame or np.ndarray (after normalizing)\n    :params: X_test - test data, can be pd.DataFrame or np.ndarray (after normalizing)\n    :params: y - target\n    :params: folds - folds to split data\n    :params: model_type - type of model to use\n    :params: eval_metric - metric to use\n    :params: columns - columns to use. If None - use all columns\n    :params: plot_feature_importance - whether to plot feature importance of LGB\n    :params: model - sklearn model, works only for \"sklearn\" model type\n    \n    \"\"\"\n    columns = X.columns if columns is None else columns\n    n_splits = folds.n_splits if splits is None else n_folds\n    X_test = X_test[columns]\n    \n    # to set up scoring parameters\n    metrics_dict = {'auc': {'lgb_metric_name': eval_auc,\n                        'catboost_metric_name': 'AUC',\n                        'sklearn_scoring_function': metrics.roc_auc_score},\n                    }\n    \n    result_dict = {}\n    \n    # out-of-fold predictions on train data\n    oof = np.zeros((len(X), len(set(y.values))))\n    \n    # averaged predictions on train data\n    prediction = np.zeros((len(X_test), oof.shape[1]))\n    \n    # list of scores on folds\n    scores = []\n    feature_importance = pd.DataFrame()\n    \n    # split and train on folds\n    for fold_n, (train_index, valid_index) in enumerate(folds.split(X)):\n        print(f'Fold {fold_n + 1} started at {time.ctime()}')\n        if type(X) == np.ndarray:\n            X_train, X_valid = X[columns][train_index], X[columns][valid_index]\n            y_train, y_valid = y[train_index], y[valid_index]\n        else:\n            X_train, X_valid = X[columns].iloc[train_index], X[columns].iloc[valid_index]\n            y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n            \n        if model_type == 'lgb':\n            model = lgb.LGBMClassifier(**params, n_estimators=n_estimators, n_jobs = -1)\n            model.fit(X_train, y_train, \n                    eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric=metrics_dict[eval_metric]['lgb_metric_name'],\n                    verbose=verbose, early_stopping_rounds=early_stopping_rounds)\n            \n            y_pred_valid = model.predict_proba(X_valid)\n            y_pred = model.predict_proba(X_test, num_iteration=model.best_iteration_)\n            \n        if model_type == 'xgb':\n            train_data = xgb.DMatrix(data=X_train, label=y_train, feature_names=X.columns)\n            valid_data = xgb.DMatrix(data=X_valid, label=y_valid, feature_names=X.columns)\n\n            watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]\n            model = xgb.train(dtrain=train_data, num_boost_round=n_estimators, evals=watchlist, early_stopping_rounds=early_stopping_rounds, verbose_eval=verbose, params=params)\n            y_pred_valid = model.predict(xgb.DMatrix(X_valid, feature_names=X.columns), ntree_limit=model.best_ntree_limit)\n            y_pred = model.predict(xgb.DMatrix(X_test, feature_names=X.columns), ntree_limit=model.best_ntree_limit)\n        \n        if model_type == 'sklearn':\n            model = model\n            model.fit(X_train, y_train)\n            \n            y_pred_valid = model.predict(X_valid).reshape(-1,)\n            score = metrics_dict[eval_metric]['sklearn_scoring_function'](y_valid, y_pred_valid)\n            print(f'Fold {fold_n}. {eval_metric}: {score:.4f}.')\n            print('')\n            \n            y_pred = model.predict_proba(X_test)\n        \n        if model_type == 'cat':\n            model = CatBoostClassifier(iterations=n_estimators, eval_metric=metrics_dict[eval_metric]['catboost_metric_name'], **params,\n                                      loss_function=metrics_dict[eval_metric]['catboost_metric_name'])\n            model.fit(X_train, y_train, eval_set=(X_valid, y_valid), cat_features=[], use_best_model=True, verbose=False)\n\n            y_pred_valid = model.predict(X_valid)\n            y_pred = model.predict(X_test)\n        \n        oof[valid_index] = y_pred_valid\n        scores.append(metrics_dict[eval_metric]['sklearn_scoring_function'](y_valid, y_pred_valid[:, 1]))\n\n        prediction += y_pred    \n        \n        if model_type == 'lgb' and plot_feature_importance:\n            # feature importance\n            fold_importance = pd.DataFrame()\n            fold_importance[\"feature\"] = columns\n            fold_importance[\"importance\"] = model.feature_importances_\n            fold_importance[\"fold\"] = fold_n + 1\n            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n\n    prediction /= n_splits\n    \n    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n    \n    result_dict['oof'] = oof\n    result_dict['prediction'] = prediction\n    result_dict['scores'] = scores\n    \n    if model_type == 'lgb':\n        if plot_feature_importance:\n            feature_importance[\"importance\"] /= n_splits\n            cols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n                by=\"importance\", ascending=False)[:50].index\n\n            best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n\n            plt.figure(figsize=(16, 12));\n            sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False));\n            plt.title('LGB Features (avg over folds)');\n            \n            result_dict['feature_importance'] = feature_importance\n        \n    return result_dict\n\n# setting up altair\nworkaround = prepare_altair()\nHTML(\"\".join((\n    \"<script>\",\n    workaround,\n    \"</script>\",\n)))", "processed": ["function use kernel hidden cell"]}, {"markdown": ["I have already done EDA in my previous [kernel](https://www.kaggle.com/artgor/molecular-properties-eda-and-models), so no need to repeat it.\n\nStill I want to show two plots again to explain what I'll do further in my kernel."], "code": "# Reference: https://www.kaggle.com/code/artgor/validation-feature-selection-interpretation-etc\n\ntype_count = train['type'].value_counts().reset_index().rename(columns={'type': 'count', 'index': 'type'})\nchart = alt.Chart(type_count).mark_bar().encode(\n    x=alt.X(\"type:N\", axis=alt.Axis(title='type')),\n    y=alt.Y('count:Q', axis=alt.Axis(title='Count')),\n    tooltip=['type', 'count']\n).properties(title=\"Counts of type\", width=350).interactive()\nrender(chart)\nsns.violinplot(x='type', y='scalar_coupling_constant', data=train);\nplt.title('Violinplot of scalar_coupling_constant by type');", "processed": ["alreadi done eda previou kernel http www kaggl com artgor molecular properti eda model need repeat still want show two plot explain kernel"]}, {"markdown": ["## Validation\n\nCreating a stable validation is one of serious challenges in each competition. Let's compare several cross-validation schemes and see how they work. I'll try the following schemes:\n\n- simple KFold with shuffling and without it;\n- strafitied KFold by by atom_index_0;\n- group KFold by molecules names and by atom_index_0;\n\nFor each scheme I'll train models and then compare their scores on cross-validation to see which one is more stable. Another way is to make a holdout set and compare model quality on cross-validation and on this holdout set. Or even better - compare it to the leaderboard."], "code": "# Reference: https://www.kaggle.com/code/artgor/validation-feature-selection-interpretation-etc\n\nval_folds = {'kfold_shuffle': KFold(n_splits=n_fold, shuffle=True, random_state=11),\n             'kfold_no_shuffle': KFold(n_splits=n_fold, shuffle=False, random_state=11),\n             'stratified_atom_index_0': StratifiedKFold(n_splits=n_fold, shuffle=False, random_state=11),\n             'group_molecules': GroupKFold(n_splits=n_fold),\n             'group_atom_index_0': GroupKFold(n_splits=n_fold)}\n\nparams = {'num_leaves': 128,\n          'min_child_samples': 79,\n          'objective': 'regression',\n          'max_depth': 9,\n          'learning_rate': 0.2,\n          \"boosting_type\": \"gbdt\",\n          \"subsample_freq\": 1,\n          \"subsample\": 0.9,\n          \"bagging_seed\": 11,\n          \"metric\": 'mae',\n          \"verbosity\": -1,\n          'reg_alpha': 0.1,\n          'reg_lambda': 0.3,\n          'colsample_bytree': 1.0\n         }\nval_scores = {}\nfor t in ['1JHN', '2JHN', '3JHN']:\n    val_scores[t] = {}\n    print(f'Type: {t}. {time.ctime()}')\n    \n    X = train.loc[train['type'] == t, [col for col in good_columns if col not in bad_advers_columns[t]]]\n    y = train.loc[train['type'] == t, 'scalar_coupling_constant']\n    X_test = test.loc[test['type'] == t, [col for col in good_columns if col not in bad_advers_columns[t]]]\n\n    for k, v in val_folds.items():\n        print(f'Validation type: {k}')\n        if 'kfold' in k:\n            splits = v.split(X)\n        elif 'stratified' in k:\n            if 'molecules' in k:\n                splits = v.split(X, train.loc[train['type'] == t, 'molecule_name'])\n            else:\n                splits = v.split(X, train.loc[train['type'] == t, 'atom_index_0'])\n        elif 'group' in k:\n            if 'molecules' in k:\n                splits = v.split(X, y, train.loc[train['type'] == t, 'molecule_name'])\n            else:\n                splits = v.split(X, y, train.loc[train['type'] == t, 'atom_index_0'])\n\n        result_dict_lgb = train_model_regression(X=X, X_test=X_test, y=y, params=params, model_type='lgb', eval_metric='mae', plot_feature_importance=False,\n                                                          verbose=False, early_stopping_rounds=100, n_estimators=500, splits=splits, n_folds=n_fold)\n\n        val_scores[t][k] = result_dict_lgb['scores']\n        print()\nfig, ax = plt.subplots(figsize = (18, 6))\nfor i, k in enumerate(val_scores.keys()):\n    val_scores_df = pd.DataFrame(val_scores[k]).T.unstack().reset_index().rename(columns={'level_1': 'validation_scheme', 0: 'scores'})\n    plt.subplot(1, 3, i + 1);\n    sns.boxplot(data=val_scores_df, x='validation_scheme', y='scores');\n    plt.xticks(rotation=45);\n    plt.title(k);", "processed": ["valid creat stabl valid one seriou challeng competit let compar sever cross valid scheme see work tri follow scheme simpl kfold shuffl without strafiti kfold atom index 0 group kfold molecul name atom index 0 scheme train model compar score cross valid see one stabl anoth way make holdout set compar model qualiti cross valid holdout set even better compar leaderboard"]}, {"markdown": ["This is quite interesting!\n\n- stratified splits have a wide range of values as well as group kfold on `atom_index_0`. This feature has some correlation with the target, but it seems that splitting by it isn't really useful;\n- strangely kfold without shuffling works quite bad. I suppose this is because we need to have diverse information about different molecules in each fold;\n- kfold with shuffling and group kfold by molecules work well!\n\nSo we can should try group kfold by molecules. But for now let's still use simple kfold", "## Comparing several models"], "code": "# Reference: https://www.kaggle.com/code/artgor/validation-feature-selection-interpretation-etc\n\nX_1JHN = train.loc[train['type'] == '1JHN', [col for col in good_columns if col not in bad_advers_columns['1JHN']]].fillna(-999)\ny_1JHN = train.loc[train['type'] == '1JHN', 'scalar_coupling_constant']\nX_test_1JHN = test.loc[test['type'] == '1JHN', [col for col in good_columns if col not in bad_advers_columns['1JHN']]].fillna(-999)\n\nfolds = KFold(n_splits=n_fold, shuffle=True, random_state=11)\nfrom sklearn.ensemble import ExtraTreesRegressor, AdaBoostRegressor, RandomForestRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.neighbors import NearestNeighbors\n%%time\netr = ExtraTreesRegressor()\n\nparameter_grid = {'n_estimators': [100, 300],\n                  'max_depth': [15, 50]\n                 }\n\ngrid_search = GridSearchCV(etr, param_grid=parameter_grid, cv=folds, scoring='neg_mean_absolute_error', n_jobs=-1)\ngrid_search.fit(X_1JHN, y_1JHN, groups=train.loc[train['type'] == '1JHN', 'molecule_name'])\nprint('Best score: {}'.format(grid_search.best_score_))\nprint('Best parameters: {}'.format(grid_search.best_params_))\netr = ExtraTreesRegressor(**grid_search.best_params_)\nresult_dict_etr = train_model_regression(X=X_1JHN, X_test=X_test_1JHN, y=y_1JHN, params=params, model_type='sklearn', model=etr, eval_metric='mae', plot_feature_importance=False,\n                                                          verbose=False, early_stopping_rounds=100, n_estimators=500, folds=folds)\n%%time\nada = AdaBoostRegressor()\n\nparameter_grid = {'n_estimators': [50, 200]}\n\ngrid_search = GridSearchCV(ada, param_grid=parameter_grid, cv=folds, scoring='neg_mean_absolute_error', n_jobs=-1)\ngrid_search.fit(X_1JHN, y_1JHN, groups=train.loc[train['type'] == '1JHN', 'molecule_name'])\nprint('Best score: {}'.format(grid_search.best_score_))\nprint('Best parameters: {}'.format(grid_search.best_params_))\nada = AdaBoostRegressor(**grid_search.best_params_)\nresult_dict_ada = train_model_regression(X=X_1JHN, X_test=X_test_1JHN, y=y_1JHN, params=params, model_type='sklearn', model=ada, eval_metric='mae', plot_feature_importance=False,\n                                                          verbose=False, early_stopping_rounds=100, n_estimators=500, folds=folds)\n%%time\nrfr = RandomForestRegressor()\n\nparameter_grid = {'n_estimators': [100, 300],\n                  'max_depth': [15, 50]\n                 }\n\ngrid_search = GridSearchCV(rfr, param_grid=parameter_grid, cv=folds, scoring='neg_mean_absolute_error', n_jobs=-1)\ngrid_search.fit(X_1JHN, y_1JHN, groups=train.loc[train['type'] == '1JHN', 'molecule_name'])\nprint('Best score: {}'.format(grid_search.best_score_))\nprint('Best parameters: {}'.format(grid_search.best_params_))\nrfr = RandomForestRegressor(**grid_search.best_params_)\nresult_dict_rfr = train_model_regression(X=X_1JHN, X_test=X_test_1JHN, y=y_1JHN, params=params, folds=folds, model_type='sklearn', model=rfr, eval_metric='mae', plot_feature_importance=False,\n                                                          verbose=10, early_stopping_rounds=100, n_estimators=500)\nX_1JHN = train.loc[train['type'] == '1JHN', [col for col in good_columns if col not in bad_advers_columns['1JHN']]].fillna(0)\ny_1JHN = train.loc[train['type'] == '1JHN', 'scalar_coupling_constant']\nX_test_1JHN = test.loc[test['type'] == '1JHN', [col for col in good_columns if col not in bad_advers_columns['1JHN']]].fillna(0)\n%%time\nridge = linear_model.Ridge(normalize=True)\n\nparameter_grid = {'alpha': [0.01, 0.1, 1.0]}\n\ngrid_search = GridSearchCV(ridge, param_grid=parameter_grid, cv=folds, scoring='neg_mean_absolute_error', n_jobs=-1)\ngrid_search.fit(X_1JHN, y_1JHN, groups=train.loc[train['type'] == '1JHN', 'molecule_name'])\nprint('Best score: {}'.format(grid_search.best_score_))\nprint('Best parameters: {}'.format(grid_search.best_params_))\nridge = linear_model.Ridge(**grid_search.best_params_, normalize=True)\nresult_dict_ridge = train_model_regression(X=X_1JHN, X_test=X_test_1JHN, y=y_1JHN, params=params, folds=folds, model_type='sklearn', model=ridge, eval_metric='mae', plot_feature_importance=False,\n                                                          verbose=10, early_stopping_rounds=100, n_estimators=500)\n%%time\nlasso = linear_model.Lasso(normalize=True)\n\nparameter_grid = {'alpha': [0.01, 0.1, 1.0]}\n\ngrid_search = GridSearchCV(lasso, param_grid=parameter_grid, cv=folds, scoring='neg_mean_absolute_error', n_jobs=-1)\ngrid_search.fit(X_1JHN, y_1JHN, groups=train.loc[train['type'] == '1JHN', 'molecule_name'])\nprint('Best score: {}'.format(grid_search.best_score_))\nprint('Best parameters: {}'.format(grid_search.best_params_))\nlasso = linear_model.Lasso(**grid_search.best_params_, normalize=True)\nresult_dict_lasso = train_model_regression(X=X_1JHN, X_test=X_test_1JHN, y=y_1JHN, params=params, model_type='sklearn', folds=folds, model=lasso, eval_metric='mae', plot_feature_importance=False,\n                                                          verbose=False, early_stopping_rounds=100, n_estimators=500)\nplt.figure(figsize=(12, 8));\nscores_df = pd.DataFrame({'RandomForestRegressor': result_dict_rfr['scores']})\nscores_df['ExtraTreesRegressor'] = result_dict_etr['scores']\nscores_df['AdaBoostRegressor'] = result_dict_ada['scores']\n# scores_df['KNN'] = result_dict_knn['scores']\nscores_df['Ridge'] = result_dict_ridge['scores']\nscores_df['Lasso'] = result_dict_lasso['scores']\n\nsns.boxplot(data=scores_df);\nplt.xticks(rotation=45);", "processed": ["quit interest stratifi split wide rang valu well group kfold atom index 0 featur correl target seem split realli use strang kfold without shuffl work quit bad suppos need diver inform differ molecul fold kfold shuffl group kfold molecul work well tri group kfold molecul let still use simpl kfold", "compar sever model"]}, {"markdown": ["## Time Series Competition\n\nIt's important to note that the data provided is `time series` in nature. We are given one year of data (2016) and are asked to predict 2 years of meter readings.\n\nPer the description:\n\n**This competition challenges you to build these counterfactual models across four energy types based on historic usage rates and observed weather. The dataset includes three years of hourly meter readings from over one thousand buildings at several different sites around the world.**"], "code": "# Reference: https://www.kaggle.com/code/robikscube/ashrae-energy-consumption-starter-kit-eda\n\nmeter_mapping = {0: 'electricity', 1: 'chilledwater', 2: 'steam', 3: 'hotwater'}\ntrain['meter_type'] = train['meter'].map(meter_mapping)\ntest['meter_type'] = test['meter'].map(meter_mapping)\ntrain.groupby(['timestamp','meter_type'])['meter_reading'] \\\n    .median() \\\n    .reset_index().set_index('timestamp') \\\n    .groupby('meter_type')['meter_reading'] \\\n    .plot(figsize=(15, 5), title='Median Meter Reading by Meter Type (Test Set)')\nplt.legend()\nplt.show()\ntrain['train'] = 1\ntest['train'] = 0\ntt = pd.concat([train, test], axis=0, sort=True)\n\ntt.groupby(['timestamp','meter_type'])['meter_reading'] \\\n    .median() \\\n    .reset_index().set_index('timestamp') \\\n    .groupby('meter_type')['meter_reading'] \\\n    .plot(figsize=(15, 5), title='Median Meter Reading by Meter Type (train and test timeframe)')\nplt.legend()\nplt.show()", "processed": ["time seri competit import note data provid time seri natur given one year data 2016 ask predict 2 year meter read per descript competit challeng build counterfactu model across four energi type base histor usag rate observ weather dataset includ three year hourli meter read one thousand build sever differ site around world"]}, {"markdown": ["## Plotting the distribution of the target.\nFirst thing we notice here is the extremely skewed distribution due to a few values that are very very large...."], "code": "# Reference: https://www.kaggle.com/code/robikscube/ashrae-energy-consumption-starter-kit-eda\n\ntrain['meter_reading'].plot(kind='hist',\n                        bins=50,\n                        figsize=(15, 2),\n                       title='Distribution of Target Variable (meter_reading)')\nplt.show()", "processed": ["plot distribut target first thing notic extrem skew distribut due valu larg"]}, {"markdown": ["Removing the high values we can get a better idea about the distribution of values. We may want to create different models for different buildings."], "code": "# Reference: https://www.kaggle.com/code/robikscube/ashrae-energy-consumption-starter-kit-eda\n\ntrain.query('meter_reading < 5000')['meter_reading'] \\\n    .plot(kind='hist',\n          figsize=(15, 3),\n          title='Distribution of meter_reading, excluding values greater than 5000',\n          bins=200)\nplt.show()\ntrain.query('meter_reading < 500')['meter_reading'] \\\n    .plot(kind='hist',\n          figsize=(15, 3),\n          title='Distribution of meter_reading, excluding values greater than 500',\n         bins=200)\nplt.show()\ntrain.query('meter_reading < 100')['meter_reading'] \\\n    .plot(kind='hist',\n          figsize=(15, 3),\n          title='Distribution of meter_reading, excluding values greater than 100',\n         bins=100)\nplt.show()", "processed": ["remov high valu get better idea distribut valu may want creat differ model differ build"]}, {"markdown": ["## Target for a single building /w Multiple Meters. Viewing over Time.\nOn inspection of the data over time, we can see that this data is very messy. There appears to be times when the values drop to zero."], "code": "# Reference: https://www.kaggle.com/code/robikscube/ashrae-energy-consumption-starter-kit-eda\n\ntrain.query('building_id == 0 and meter == 0') \\\n    .set_index('timestamp')['meter_reading'].plot(figsize=(15, 3),\n                                                 title='Building 0 - Meter 0')\n\nplt.show()\ntrain.query('building_id == 753').set_index('timestamp').groupby('meter')['meter_reading'].plot(figsize=(15, 3),\n                                                 title='Building 753 - Meters 0-3')\nplt.show()\ntrain.query('building_id == 1322').set_index('timestamp').groupby('meter')['meter_reading'].plot(figsize=(15, 3),\n                                                 title='Building 1322 - Meters 0-3')\nplt.show()", "processed": ["target singl build w multipl meter view time inspect data time see data messi appear time valu drop zero"]}, {"markdown": ["- We see that there is spike in buildings that were built in the year 1976\n- 774 buildings have no year built information"], "code": "# Reference: https://www.kaggle.com/code/robikscube/ashrae-energy-consumption-starter-kit-eda\n\nbmd.groupby('year_built')['site_id'] \\\n    .count() \\\n    .plot(figsize=(15, 5),\n          style='.-',\n          title='Building Meta Data - Count by Year Built')\nplt.show()\nprint('{} Buildings have no year data.'.format(np.sum(bmd['year_built'].isna())))", "processed": ["see spike build built year 1976 774 build year built inform"]}, {"markdown": ["## Building Primary Use\n- Education is the most common type of building use, with office second.\n- There is a steep drop off in number of buildings after Lodging."], "code": "# Reference: https://www.kaggle.com/code/robikscube/ashrae-energy-consumption-starter-kit-eda\n\nbmd.groupby('primary_use') \\\n    .count()['site_id'] \\\n    .sort_values() \\\n    .plot(kind='barh',\n          figsize=(15, 5),\n          title='Count of Buildings by Primary Use')\nplt.show()\n# Aggregate some meter reading stats\nmeter_reading_stats = train.groupby('building_id')['meter_reading'].agg(['mean','max','min']).reset_index()\nbmd_with_stats = pd.merge(bmd, meter_reading_stats, on=['building_id']).rename(columns={'mean':'mean_meter_reading',\n                                                                       'max':'max_meter_reading',\n                                                                       'min':'min_meter_reading'})", "processed": ["build primari use educ common type build use offic second steep drop number build lodg"]}, {"markdown": ["## Building Type and Meter Reading "], "code": "# Reference: https://www.kaggle.com/code/robikscube/ashrae-energy-consumption-starter-kit-eda\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=RuntimeWarning) \nsns.pairplot(bmd_with_stats.dropna(),\n             vars=['mean_meter_reading','min_meter_reading',\n                   'max_meter_reading','square_feet','year_built'],\n             hue='primary_use')\nplt.show()", "processed": ["build type meter read"]}, {"markdown": ["In order to properly visualize the data, we can normalize the meter reading by type. This allows us to compare how the time series features impact each meter reading type, but on the same scale. The normalized value shows the value in relation to the meter type's average."], "code": "# Reference: https://www.kaggle.com/code/robikscube/ashrae-energy-consumption-starter-kit-eda\n\ntrain['normalized_meter_reading_type'] = \\\n    train.groupby('meter_type')['meter_reading'] \\\n        .transform(lambda x: (x - x.mean()) / x.std())\nfig, ax = plt.subplots(figsize=(15, 5))\nsns.barplot(data=train.groupby(['Weekday_Name','meter_type']).mean().reset_index(),\n            x='Weekday_Name',\n            y='normalized_meter_reading_type',\n            hue='meter_type',\n            ax=ax)\nplt.title('Day of Week vs. Normalized Meter Reading')\nplt.show()\nfig, ax = plt.subplots(figsize=(15, 5))\nsns.barplot(data=train.groupby(['Month','meter_type']).mean().reset_index(),\n            x='Month',\n            y='normalized_meter_reading_type',\n            hue='meter_type',\n            ax=ax)\nplt.title('Month vs. Normalized Meter Reading')\nplt.show()\nfig, ax = plt.subplots(figsize=(15, 5))\nsns.barplot(data=train.groupby(['Hour','meter_type']).mean().reset_index(),\n            x='Hour',\n            y='normalized_meter_reading_type',\n            hue='meter_type',\n            ax=ax)\nplt.title('Hour within Day vs. Normalized Meter Reading')\nplt.show()\nfig, ax = plt.subplots(figsize=(15, 5))\nsns.lineplot(data=train.groupby(['DayofYear','meter_type']).mean().reset_index(),\n            x='DayofYear',\n            y='normalized_meter_reading_type',\n            hue='meter_type',\n            ax=ax)\n# plt.title('Day of Year vs. Normalized Meter Reading')\nplt.show()", "processed": ["order properli visual data normal meter read type allow u compar time seri featur impact meter read type scale normal valu show valu relat meter type averag"]}, {"markdown": ["# Load Clean Competition Data\nWe will use the Kaggle dataset [here][1] containing this competition's train and test data with drift removed. Drift is explained [here][2]. And visualized [here][3]. This competition's data is believed to be `data = computer generated + real life noise + synthetic drift`. Drift was found in training data batches 2, 7, 8, 9, 10. And test batches 1, 2, 3. [Markus][5] demonstrated in his great notebook [here][4] that drift can be cleanly removed with 4th order approximation functions. \n\nClean data allows to produce better EDA and allows us to use models like kNN.\n\n[1]: https://www.kaggle.com/cdeotte/data-without-drift\n[2]: https://www.kaggle.com/c/liverpool-ion-switching/discussion/133874\n[3]: https://www.kaggle.com/cdeotte/one-feature-model-0-930\n[4]: https://www.kaggle.com/friedchips/clean-removal-of-data-drift\n[5]: https://www.kaggle.com/friedchips"], "code": "# Reference: https://www.kaggle.com/code/cdeotte/rapids-knn-30-seconds-0-938\n\ntrain = pd.read_csv('/kaggle/input/data-without-drift/train_clean.csv')\ntrain['group'] = -1\nx = [(0,500000),(1000000,1500000),(1500000,2000000),(2500000,3000000),(2000000,2500000)]\nfor k in range(5): train.iloc[x[k][0]:x[k][1],3] = k\n    \nres = 1000\nplt.figure(figsize=(20,5))\nplt.plot(train.time[::res],train.signal[::res])\nplt.plot(train.time,train.group,color='black')\nplt.title('Clean Train Data. Blue line is signal. Black line is group number.')\nplt.xlabel('time'); plt.ylabel('signal')\nplt.show()\ntest = pd.read_csv('/kaggle/input/data-without-drift/test_clean.csv')\ntest['group'] = -1\nx = [[(0,100000),(300000,400000),(800000,900000),(1000000,2000000)],[(400000,500000)], \n     [(100000,200000),(900000,1000000)],[(200000,300000),(600000,700000)],[(500000,600000),(700000,800000)]]\nfor k in range(5):\n    for j in range(len(x[k])): test.iloc[x[k][j][0]:x[k][j][1],2] = k\n        \nres = 400\nplt.figure(figsize=(20,5))\nplt.plot(test.time[::res],test.signal[::res])\nplt.plot(test.time,test.group,color='black')\nplt.title('Clean Test Data. Blue line is signal. Black line is group number.')\nplt.xlabel('time'); plt.ylabel('signal')\nplt.show()", "processed": ["load clean competit data use kaggl dataset 1 contain competit train test data drift remov drift explain 2 visual 3 competit data believ data comput gener real life nois synthet drift drift found train data batch 2 7 8 9 10 test batch 1 2 3 marku 5 demonstr great notebook 4 drift cleanli remov 4th order approxim function clean data allow produc better eda allow u use model like knn 1 http www kaggl com cdeott data without drift 2 http www kaggl com c liverpool ion switch discus 133874 3 http www kaggl com cdeott one featur model 0 930 4 http www kaggl com friedchip clean remov data drift 5 http www kaggl com friedchip"]}, {"markdown": ["# One Feature Model\nWith clean data, we can now make a one feature model. A full explanation of the one feature model is [here][1]. Summary of one feature model is below. If you have an unknown test row, then first you determine which group that row belongs to. Next you find the signal from that row on the x axis below and find the corresponding open channels prediction on the y axis below. This model has CV 0.926 and LB 0.929.\n\n[1]: https://www.kaggle.com/cdeotte/one-feature-model-0-930"], "code": "# Reference: https://www.kaggle.com/code/cdeotte/rapids-knn-30-seconds-0-938\n\nstep = 0.2\npt = [[],[],[],[],[]]\ncuts = [[],[],[],[],[]]\nfor g in range(5):\n    mn = train.loc[train.group==g].signal.min()\n    mx = train.loc[train.group==g].signal.max()\n    old = 0\n    for x in np.arange(mn,mx+step,step):\n        sg = train.loc[(train.group==g)&(train.signal>x-step/2)&(train.signal<x+step/2)].open_channels.values\n        if len(sg)>100:\n            m = mode(sg)[0][0]\n            pt[g].append((x,m))\n            if m!=old: cuts[g].append(x-step/2)\n            old = m\n    pt[g] = np.vstack(pt[g])\n    \nmodels = ['1 channel low prob','1 channel high prob','3 channel','5 channel','10 channel']\nplt.figure(figsize=(15,8))\nfor g in range(5):\n    plt.plot(pt[g][:,0],pt[g][:,1],'-o',label='Group %i (%s model)'%(g,models[g]))\nplt.legend()\nplt.title('Traing Data Open Channels versus Clean Signal Value',size=16)\nplt.xlabel('Clean Signal Value',size=16)\nplt.ylabel('Open Channels',size=16)\nplt.show()", "processed": ["one featur model clean data make one featur model full explan one featur model 1 summari one featur model unknown test row first determin group row belong next find signal row x axi find correspond open channel predict axi model cv 0 926 lb 0 929 1 http www kaggl com cdeott one featur model 0 930"]}, {"markdown": ["# Two Feature Model\nWe can improve the one feature model by adding a second feature. For every unknown test row, we will also use the preceeding row's signal value. This model has CV 0.9295 and LB 0.932. Below are plots of the two feature model. When given an unknown test row. Just find the most similar training row that has the same signal and preceeding row signal.\n\nSpecifically, here is how to use the plots below to predict an unknown test row. First determine what group the test row is in and find the corresponding group plots below. Next match the signal to the y axis and the preceeding row's signal to the x axis. Whatever color is beneath your point is the number of open channels you will predict."], "code": "# Reference: https://www.kaggle.com/code/cdeotte/rapids-knn-30-seconds-0-938\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfor g in range(5):\n    if g==0: res = 100\n    else: res = 10\n\n    plt.figure(figsize=(16,8))\n    plt.subplot(1,2,1)\n    for k in range(0,11):\n        idx = np.array( train.loc[(train.open_channels==k) & (train.group==g)].index )\n        if len(idx)==0: continue\n        plt.scatter(train.signal[idx-1],train.signal[idx],s=0.01,label='%i open channels'%k)\n    plt.xlabel('Previous Signal Value',size=14)\n    plt.ylabel('Signal Value',size=14)\n    lgnd = plt.legend(numpoints=1, fontsize=10)\n    for k in range( len(lgnd.legendHandles) ):\n        lgnd.legendHandles[k]._sizes = [30]\n    \n    data = test.loc[test.group==g]\n    #plt.scatter(data.signal[:-1][::res],data.signal[1:][::res],s=0.1,color='black')\n    xx = plt.xlim(); yy = plt.ylim()\n    for k in range(len(cuts[g])):\n        if (g!=4)|(k!=0): plt.plot([xx[0],xx[1]],[cuts[g][k],cuts[g][k]],':',color='black')\n    plt.title('Train Data in group %i'%g,size=16)\n    \n    plt.subplot(1,2,2)\n    plt.scatter(data.signal[:-1][::res],data.signal[1:][::res],s=0.1,color='black')\n    plt.xlim(xx); plt.ylim(yy)\n    for k in range(len(cuts[g])):\n        if (g!=4)|(k!=0): plt.plot([xx[0],xx[1]],[cuts[g][k],cuts[g][k]],':',color='black')\n        if (g==4)&(k!=0): plt.text(xx[0]+1,cuts[g][k],'%i open channels'%(k+2),size=12)\n        elif g!=4: plt.text(xx[0]+1,cuts[g][k],'%i open channels'%(k+1),size=14)\n    plt.xlabel('Previous Signal Value',size=14)\n    plt.ylabel('Signal Value',size=14)\n    plt.title('Unknown Test Data in group %i'%g,size=16)\n\n    plt.show()", "processed": ["two featur model improv one featur model ad second featur everi unknown test row also use preceed row signal valu model cv 0 9295 lb 0 932 plot two featur model given unknown test row find similar train row signal preceed row signal specif use plot predict unknown test row first determin group test row find correspond group plot next match signal axi preceed row signal x axi whatev color beneath point number open channel predict"]}, {"markdown": ["# Seven Feature Model\nNow if we wish to predict a row of the test data, we will use the three rows before and three rows after. In total we will use 7 features. We will use simple kNN compared with the train data. We can visualize a row of test data as a \"wiggle\". The green dot is the unknown test row and it's signal value can be found on the y axis. The red dots are the 3 rows before and the blue dots are the 3 rows after. Given a unknown test data \"wiggle\", our job is to find the most similar looking training data \"wiggle\"."], "code": "# Reference: https://www.kaggle.com/code/cdeotte/rapids-knn-30-seconds-0-938\n\ndef wiggle(df, row, plt, xx=None, yy=None):\n    plt.plot([-3,-2,-1,0,1,2,3],df.loc[df.index[row-3:row+4],'signal'],'-')\n    sizes = np.array([1,2,3,12,3,2,1])*50\n    colors = ['red','red','red','green','blue','blue','blue']\n    for k in range(7):\n        plt.scatter(k-3,df.loc[df.index[row+k-3],'signal'],s=sizes[k],color=colors[k])\n    if xx!=None: plt.xlim(xx)\n    if yy!=None: plt.ylim(yy)\n    return plt.xlim(),plt.ylim()\n\nrow=2; col=4;\nnp.random.seed(42)\nplt.figure(figsize=(4*col,4*row))\nfor k in range(row*col):\n    plt.subplot(row,col,k+1)\n    r = np.random.randint(2e6)\n    wiggle(test,r,plt)\n    if k%col==0: plt.ylabel('signal')\n    g = test.loc[r,'group']\n    plt.title('Test row %i group %i'%(r,g))\nplt.tight_layout(pad=3.0)\nplt.show()", "processed": ["seven featur model wish predict row test data use three row three row total use 7 featur use simpl knn compar train data visual row test data wiggl green dot unknown test row signal valu found axi red dot 3 row blue dot 3 row given unknown test data wiggl job find similar look train data wiggl"]}, {"markdown": ["# Visualize kNN\nBelow are some random rows from test data group 4 and the 4 closest kNN training rows. We then use the most common known training label to predict the unknown test label."], "code": "# Reference: https://www.kaggle.com/code/cdeotte/rapids-knn-30-seconds-0-938\n\ndata1 = test.loc[test.group==4].iloc[3:]\ndata1.reset_index(inplace=True)\n\ndata2 = train.loc[train.group==4].iloc[3:]\ndata2.reset_index(inplace=True)\n\nfor j in range(5):\n    r = np.random.randint(data1.shape[0])\n    distances, indices = model.kneighbors(X_test[r:r+1,])\n\n    row=2; \n    plt.figure(figsize=(16,row*4))\n    for k in range(row*4):\n        if k in [1,2,3]: continue\n        plt.subplot(row,4,k+1)\n        if k==0: \n            xx,yy = wiggle(data1,r,plt)\n            g = data1.loc[r,'group']\n            rw = data1.loc[r,'index']\n            plt.title('UNKNOWN Test row %i group %i'%(rw,g))\n        else:\n            r=indices[0,k-4].astype('int')\n            wiggle(data2,r,plt,xx,yy)\n            g = data2.loc[r,'group']\n            rw = data2.loc[r,'index']\n            t = data2.loc[r,'open_channels']\n            plt.title('LABEL = %i. Train row %i group %i'%(t,rw,g))\n        if k%4==0: plt.ylabel('signal')\n    plt.tight_layout(pad=3.0)\n    plt.show()", "processed": ["visual knn random row test data group 4 4 closest knn train row use common known train label predict unknown test label"]}, {"markdown": ["In contrast we can find multiple images for one patient!"], "code": "# Reference: https://www.kaggle.com/code/allunia/don-t-turn-into-a-smoothie-after-the-shake-up\n\npatient_counts_train = train_info.patient_id.value_counts()\npatient_counts_test = test_info.patient_id.value_counts()\n\nfig, ax = plt.subplots(2,2,figsize=(20,12))\n\nsns.distplot(patient_counts_train, ax=ax[0,0], color=\"orangered\", kde=True);\nax[0,0].set_xlabel(\"Counts\")\nax[0,0].set_ylabel(\"Frequency\")\nax[0,0].set_title(\"Patient id value counts in train\");\n\nsns.distplot(patient_counts_test, ax=ax[0,1], color=\"lightseagreen\", kde=True);\nax[0,1].set_xlabel(\"Counts\")\nax[0,1].set_ylabel(\"Frequency\")\nax[0,1].set_title(\"Patient id value counts in test\");\n\nsns.boxplot(patient_counts_train, ax=ax[1,0], color=\"orangered\");\nax[1,0].set_xlim(0, 250)\nsns.boxplot(patient_counts_test, ax=ax[1,1], color=\"lightseagreen\");\nax[1,1].set_xlim(0, 250);\nnp.quantile(patient_counts_train, 0.75) - np.quantile(patient_counts_train, 0.25)\nnp.quantile(patient_counts_train, 0.5)\nprint(np.quantile(patient_counts_train, 0.95))\nprint(np.quantile(patient_counts_test, 0.95))", "processed": ["contrast find multipl imag one patient"]}, {"markdown": ["Ok, that's great! There seem to be no patients in train that can be found in test as well. We can't be sure as we don't know how the naming assignment process was designed. We might better check the images themselves as well!", "## Gender counts  <a class=\"anchor\" id=\"gender_counts\"></a>"], "code": "# Reference: https://www.kaggle.com/code/allunia/don-t-turn-into-a-smoothie-after-the-shake-up\n\nfig, ax = plt.subplots(1,2,figsize=(20,5))\nsns.countplot(train_info.sex, palette=\"Reds_r\", ax=ax[0]);\nax[0].set_xlabel(\"\")\nax[0].set_title(\"Gender counts\");\n\nsns.countplot(test_info.sex, palette=\"Blues_r\", ax=ax[1]);\nax[1].set_xlabel(\"\")\nax[1].set_title(\"Gender counts\");", "processed": ["ok great seem patient train found test well sure know name assign process design might better check imag well", "gender count class anchor id gender count"]}, {"markdown": ["### Insights\n\n* We observe more males than females in both train and test data.\n* The surplus of males is even higher in test than in train!", "## Age distributions <a class=\"anchor\" id=\"age_distributions\"></a>"], "code": "# Reference: https://www.kaggle.com/code/allunia/don-t-turn-into-a-smoothie-after-the-shake-up\n\nfig, ax = plt.subplots(1,2,figsize=(20,5))\n\nsns.countplot(train_info.age_approx, color=\"orangered\", ax=ax[0]);\nlabels = ax[0].get_xticklabels();\nax[0].set_xticklabels(labels, rotation=90);\nax[0].set_xlabel(\"\");\nax[0].set_title(\"Age distribution in train\");\n\nsns.countplot(test_info.age_approx, color=\"lightseagreen\", ax=ax[1]);\nlabels = ax[1].get_xticklabels();\nax[1].set_xticklabels(labels, rotation=90);\nax[1].set_xlabel(\"\");\nax[1].set_title(\"Age distribution in test\");", "processed": ["insight observ male femal train test data surplu male even higher test train", "age distribut class anchor id age distribut"]}, {"markdown": ["### Insights\n\n* The age distribution in train looks almost normally distributed.\n* In contrast, the age distribution in test shows multiple modes and interesting peaks at the ageof 55 and 70!\n* We can observe more older patients in test than in train! This kind of imbalance can be important for our model performance if the age is an important feature.  ", "## Image location <a class=\"anchor\" id=\"image_location\"></a>"], "code": "# Reference: https://www.kaggle.com/code/allunia/don-t-turn-into-a-smoothie-after-the-shake-up\n\nfig, ax = plt.subplots(1,2,figsize=(20,5))\n\nimage_locations_train = train_info.anatom_site_general_challenge.value_counts().sort_values(ascending=False)\nimage_locations_test = test_info.anatom_site_general_challenge.value_counts().sort_values(ascending=False)\n\nsns.barplot(x=image_locations_train.index.values, y=image_locations_train.values, ax=ax[0], color=\"orangered\");\nax[0].set_xlabel(\"\");\nlabels = ax[0].get_xticklabels();\nax[0].set_xticklabels(labels, rotation=90);\nax[0].set_title(\"Image locations in train\");\n\nsns.barplot(x=image_locations_test.index.values, y=image_locations_test.values, ax=ax[1], color=\"lightseagreen\");\nax[1].set_xlabel(\"\");\nlabels = ax[1].get_xticklabels();\nax[1].set_xticklabels(labels, rotation=90);\nax[1].set_title(\"Image locations in test\");", "processed": ["insight age distribut train look almost normal distribut contrast age distribut test show multipl mode interest peak ageof 55 70 observ older patient test train kind imbal import model perform age import featur", "imag locat class anchor id imag locat"]}, {"markdown": ["### Insights\n\n* The distributions of image locations in train and test look very similar. \n* Most images are related to the torso or to the lower extremity.", "## Target distribution <a class=\"anchor\" id=\"target_distribution\"></a>"], "code": "# Reference: https://www.kaggle.com/code/allunia/don-t-turn-into-a-smoothie-after-the-shake-up\n\nfig, ax = plt.subplots(1,2, figsize=(20,5))\n\nsns.countplot(x=train_info.diagnosis, orient=\"v\", ax=ax[0], color=\"Orangered\")\nax[0].set_xlabel(\"\")\nlabels = ax[0].get_xticklabels();\nax[0].set_xticklabels(labels, rotation=90);\nax[0].set_title(\"Diagnosis\");\n\nsns.countplot(train_info.benign_malignant, ax=ax[1], palette=\"Reds_r\");\nax[1].set_xlabel(\"\")\nax[1].set_title(\"Type\");", "processed": ["insight distribut imag locat train test look similar imag relat torso lower extrem", "target distribut class anchor id target distribut"]}, {"markdown": ["### Insights\n\n* Be careful with interpreting these heatmaps: \n    * **The patients are soreted by value_counts**. Patients with the most number of images are given on the left and those with only a few or a single images are on the righthand side.\n    * The color represents how much percentage of the images for one patient is covered by a given age. \n    * For example the most left patient in test is the one with almost 250 images. The related images are not spread over a wide range of different ages and are very concentrated at an old age. (Dark blue color at the age of 70).    \n* We can conclude that more images does not mean that there are multiple ages involved! \n* It's possible that multiple images are spread over a wide range of ages but it's also possible that multiple images are concentrated at one age.", "## Age and gender <a class=\"anchor\" id=\"age_gender\"></a>", "Looking at the sex per patient (excluding the multiple counts due to multiple images) we can observe that we still have more males than females."], "code": "# Reference: https://www.kaggle.com/code/allunia/don-t-turn-into-a-smoothie-after-the-shake-up\n\nfig, ax = plt.subplots(2,2,figsize=(20,15))\n\nsns.boxplot(train_info.sex, train_info.age_approx, ax=ax[0,0], palette=\"Reds_r\");\nax[0,0].set_title(\"Age per gender in train\");\n\nsns.boxplot(test_info.sex, test_info.age_approx, ax=ax[0,1], palette=\"Blues_r\");\nax[0,1].set_title(\"Age per gender in test\");\n\nsns.countplot(train_info.age_approx, hue=train_info.sex, ax=ax[1,0], palette=\"Reds_r\");\nsns.countplot(test_info.age_approx, hue=test_info.sex, ax=ax[1,1], palette=\"Blues_r\");", "processed": ["insight care interpret heatmap patient soret valu count patient number imag given left singl imag righthand side color repres much percentag imag one patient cover given age exampl left patient test one almost 250 imag relat imag spread wide rang differ age concentr old age dark blue color age 70 conclud imag mean multipl age involv possibl multipl imag spread wide rang age also possibl multipl imag concentr one age", "age gender class anchor id age gender", "look sex per patient exclud multipl count due multipl imag observ still male femal"]}, {"markdown": ["### Insights\nThere are some significant differences in train and test regarding the gender per age level:\n\n* At the ages between 25 and 35 we have much more females than males in train but a balanced count in test!\n* We can observe a high surplus of males in the ages 45 to 50 and 70, 75 in train and test but in test we can find even more males of high age > 75.", "## Age, gender and cancer <a class=\"anchor\" id=\"age_gender_cancer\"></a>"], "code": "# Reference: https://www.kaggle.com/code/allunia/don-t-turn-into-a-smoothie-after-the-shake-up\n\nsex_and_cancer_map = train_info.groupby(\n    [\"benign_malignant\", \"sex\"]\n).size().unstack(level=0) / train_info.groupby(\"benign_malignant\").size() * 100\n\ncancer_sex_map = train_info.groupby(\n    [\"benign_malignant\", \"sex\"]\n).size().unstack(level=1) / train_info.groupby(\"sex\").size() * 100\n\n\nfig, ax = plt.subplots(1,3,figsize=(20,5))\n\nsns.boxplot(train_info.benign_malignant, train_info.age_approx, ax=ax[0], palette=\"Greens\");\nax[0].set_title(\"Age and cancer\");\nax[0].set_xlabel(\"\");\n\nsns.heatmap(sex_and_cancer_map, annot=True, cmap=\"Greens\", cbar=False, ax=ax[1])\nax[1].set_xlabel(\"\")\nax[1].set_ylabel(\"\");\n\nsns.heatmap(cancer_sex_map, annot=True, cmap=\"Greens\", cbar=False, ax=ax[2])\nax[2].set_xlabel(\"\")\nax[2].set_ylabel(\"\");", "processed": ["insight signific differ train test regard gender per age level age 25 35 much femal male train balanc count test observ high surplu male age 45 50 70 75 train test test find even male high age 75", "age gender cancer class anchor id age gender cancer"]}, {"markdown": ["### Insights\n\n* We have more malignant cases of higher age than benign cases.\n* 62 % of the malignant cases belong to males and only 38 % to females.\n* Roughly 2 % of the males in the train dataset show malignant cases, but only 1.4 % of the females.\n\nWe have to be very careful!!! As we have a surpus of males with ages above 70 and 75 it's unclear if the sex is really an important feature for having melanoma or not. It could also be that the age is most important and that we only have more malignant cases for males due to their higher age!  "], "code": "# Reference: https://www.kaggle.com/code/allunia/don-t-turn-into-a-smoothie-after-the-shake-up\n\nfig, ax = plt.subplots(2,2,figsize=(20,15))\n\nsns.countplot(train_info[train_info.benign_malignant==\"benign\"].age_approx, hue=train_info.sex, palette=\"Purples_r\", ax=ax[0,0])\nax[0,0].set_title(\"Benign cases in train\");\n\nsns.countplot(train_info[train_info.benign_malignant==\"malignant\"].age_approx, hue=train_info.sex, palette=\"Oranges_r\", ax=ax[0,1])\nax[0,1].set_title(\"Malignant cases in train\");\n\nsns.violinplot(train_info.sex, train_info.age_approx, hue=train_info.benign_malignant, split=True, ax=ax[1,0], palette=\"Greens_r\");\nsns.violinplot(train_info.benign_malignant, train_info.age_approx, hue=train_info.sex, split=True, ax=ax[1,1], palette=\"RdPu\");", "processed": ["insight malign case higher age benign case 62 malign case belong male 38 femal roughli 2 male train dataset show malign case 1 4 femal care surpu male age 70 75 unclear sex realli import featur melanoma could also age import malign case male due higher age"]}, {"markdown": ["### Insights\n\n* For the benign cases we can see that there is still a surplus of males in the ages of 45 and 70, but the other ones look quite good and balanced.\n* **In contrast we can find a high gender imbalance for a wide range of ages for the malignant cases!** That's really interesting and the features age and gender as well as their interaction with cancer are definitely some to play with during modelling.", "## Individual patient information <a class=\"anchor\" id=\"patient_information\"></a>\n\nLet's collect some information for each patient:\n\n* the number of recorded images\n* the gender\n* the min, max age and the age span\n* the number of benign & malignant cases\n* the minimum and maximum age of a patient with malignant cases"], "code": "# Reference: https://www.kaggle.com/code/allunia/don-t-turn-into-a-smoothie-after-the-shake-up\n\npatient_gender_train = train_info.groupby(\"patient_id\").sex.unique().apply(lambda l: l[0])\npatient_gender_test = test_info.groupby(\"patient_id\").sex.unique().apply(lambda l: l[0])\n\ntrain_patients = pd.DataFrame(index=patient_gender_train.index.values, data=patient_gender_train.values, columns=[\"sex\"])\ntest_patients = pd.DataFrame(index=patient_gender_test.index.values, data=patient_gender_test.values, columns=[\"sex\"])\n\ntrain_patients.loc[:, \"num_images\"] = train_info.groupby(\"patient_id\").size()\ntest_patients.loc[:, \"num_images\"] = test_info.groupby(\"patient_id\").size()\n\ntrain_patients.loc[:, \"min_age\"] = train_info.groupby(\"patient_id\").age_approx.min()\ntrain_patients.loc[:, \"max_age\"] = train_info.groupby(\"patient_id\").age_approx.max()\ntest_patients.loc[:, \"min_age\"] = test_info.groupby(\"patient_id\").age_approx.min()\ntest_patients.loc[:, \"max_age\"] = test_info.groupby(\"patient_id\").age_approx.max()\n\ntrain_patients.loc[:, \"age_span\"] = train_patients[\"max_age\"] - train_patients[\"min_age\"]\ntest_patients.loc[:, \"age_span\"] = test_patients[\"max_age\"] - test_patients[\"min_age\"]\n\ntrain_patients.loc[:, \"benign_cases\"] = train_info.groupby([\"patient_id\", \"benign_malignant\"]).size().loc[:, \"benign\"]\ntrain_patients.loc[:, \"malignant_cases\"] = train_info.groupby([\"patient_id\", \"benign_malignant\"]).size().loc[:, \"malignant\"]\ntrain_patients[\"min_age_malignant\"] = train_info.groupby([\"patient_id\", \"benign_malignant\"]).age_approx.min().loc[:, \"malignant\"]\ntrain_patients[\"max_age_malignant\"] = train_info.groupby([\"patient_id\", \"benign_malignant\"]).age_approx.max().loc[:, \"malignant\"]\ntrain_patients.sort_values(by=\"malignant_cases\", ascending=False).head()\nfig, ax = plt.subplots(2,2,figsize=(20,12))\nsns.countplot(train_patients.sex, ax=ax[0,0], palette=\"Reds\")\nax[0,0].set_title(\"Gender counts with unique patient ids in train\")\nsns.countplot(test_patients.sex, ax=ax[0,1], palette=\"Blues\");\nax[0,1].set_title(\"Gender counts with unique patient ids in test\");\n\ntrain_age_span_perc = train_patients.age_span.value_counts() / train_patients.shape[0] * 100\ntest_age_span_perc = test_patients.age_span.value_counts() / test_patients.shape[0] * 100\n\nsns.barplot(train_age_span_perc.index, train_age_span_perc.values, ax=ax[1,0], color=\"Orangered\");\nsns.barplot(test_age_span_perc.index, test_age_span_perc.values, ax=ax[1,1], color=\"Lightseagreen\");\nax[1,0].set_title(\"Patients age span in train\")\nax[1,1].set_title(\"Patients age span in test\")\nfor n in range(2):\n    ax[1,n].set_ylabel(\"% in data\")\n    ax[1,n].set_xlabel(\"age span\");", "processed": ["insight benign case see still surplu male age 45 70 one look quit good balanc contrast find high gender imbal wide rang age malign case realli interest featur age gender well interact cancer definit play model", "individu patient inform class anchor id patient inform let collect inform patient number record imag gender min max age age span number benign malign case minimum maximum age patient malign case"]}, {"markdown": ["If you set plot_test to False the following scatter plot will show statistics of all training examples instead:"], "code": "# Reference: https://www.kaggle.com/code/allunia/don-t-turn-into-a-smoothie-after-the-shake-up\n\nif plot_test:\n    N = test_image_stats.shape[0]\n    selected_data = test_image_stats\n    my_title = \"Test image statistics\"\nelse:\n    N = train_image_stats.shape[0]\n    selected_data = train_image_stats\n    my_title = \"Train image statistics\"\n\ntrace1 = go.Scatter3d(\n    x=selected_data.img_mean.values[0:N], \n    y=selected_data.img_std.values[0:N],\n    z=selected_data.img_skew.values[0:N],\n    mode='markers',\n    text=selected_data[\"rows\"].values[0:N],\n    marker=dict(\n        color=selected_data[\"columns\"].values[0:N],\n        colorscale = \"Jet\",\n        colorbar=dict(thickness=10, title=\"image columns\", len=0.8),\n        opacity=0.4,\n        size=2\n    )\n)\n\nfigure_data = [trace1]\nlayout = go.Layout(\n    title = my_title,\n    scene = dict(\n        xaxis = dict(title=\"Image mean\"),\n        yaxis = dict(title=\"Image standard deviation\"),\n        zaxis = dict(title=\"Image skewness\"),\n    ),\n    margin=dict(\n        l=0,\n        r=0,\n        b=0,\n        t=0\n    ),\n    showlegend=True\n)\n\nfig = go.Figure(data=figure_data, layout=layout)\npy.iplot(fig, filename='simple-3d-scatter')", "processed": ["set plot test fals follow scatter plot show statist train exampl instead"]}, {"markdown": ["Ok, now we have created a hold out dataset that only consists of one type of image group. We need to fill it up with further training samples of all other groups. To be similar to the test set we should try to reach a 33% split.  "], "code": "# Reference: https://www.kaggle.com/code/allunia/don-t-turn-into-a-smoothie-after-the-shake-up\n\ntest_info.shape[0] / train_info.shape[0]\nreduced_train_df, add_to_hold_out_df = train_test_split(\n    reduced_train_df, test_size=0.163, stratify=reduced_train_df.target.values)\nhold_out_df = hold_out_df.append(add_to_hold_out_df)\nprint(hold_out_df.shape[0] / reduced_train_df.shape[0])\nprint(hold_out_df.shape[0], reduced_train_df.shape[0])\nfig, ax = plt.subplots(1,2,figsize=(20,5))\n\nh_target_perc = hold_out_df.target.value_counts() / hold_out_df.shape[0] * 100\nrt_target_perc = reduced_train_df.target.value_counts() / reduced_train_df.shape[0] * 100 \n\nsns.barplot(h_target_perc.index, h_target_perc.values, ax=ax[0], palette=\"Oranges_r\")\nsns.barplot(rt_target_perc.index, rt_target_perc.values, ax=ax[1], palette=\"Purples_r\");\n\nax[0].set_title(\"Target distribution of \\n hold-out\");\nax[1].set_title(\"Target distribution of \\n reduced train\");\nfor n in range(2):\n    ax[n].set_ylabel(\"% in data\")\n    ax[n].set_xlabel(\"Target\")", "processed": ["ok creat hold dataset consist one type imag group need fill train sampl group similar test set tri reach 33 split"]}, {"markdown": ["As we are using a hold-out dataset to simulate what happens when there is a image group in test that is missed in train, we need to selected the proper indices:"], "code": "# Reference: https://www.kaggle.com/code/allunia/don-t-turn-into-a-smoothie-after-the-shake-up\n\nhold_out_indices = hold_out_df.index.values\nreduced_train_indices = reduced_train_df.index.values\n#hold_out_dataset_1 = ResizedNpyMelanomaDataset(train_npy, hold_out_indices, df=hold_out_df,\n#                                              transform=transform_fun(RESIZE_SHAPE, key=\"dev\", plot=True))\n#hold_out_dataset_2 = MelanomaDataset(hold_out_df, transform=transform_fun(RESIZE_SHAPE, key=\"dev\", plot=True))\n\n\n#idx = 10\n\n#hold_out_example_1 = hold_out_dataset_1.__getitem__(idx)\n#hold_out_example_2 = hold_out_dataset_2.__getitem__(idx)\n\n#fig, ax = plt.subplots(1,4,figsize=(20,5))\n#ax[0].imshow(hold_out_example_1[\"image\"])\n#ax[0].axis(\"off\")\n#ax[0].set_title(hold_out_example_1[\"target\"]);\n#sns.distplot(hold_out_example_1[\"image\"], ax=ax[1])\n#ax[2].imshow(hold_out_example_2[\"image\"])\n#ax[2].axis(\"off\")\n#ax[2].set_title(hold_out_example_2[\"target\"]);\n#sns.distplot(hold_out_example_1[\"image\"], ax=ax[3])", "processed": ["use hold dataset simul happen imag group test miss train need select proper indic"]}, {"markdown": ["Let's take a look at the different target distributions:"], "code": "# Reference: https://www.kaggle.com/code/allunia/don-t-turn-into-a-smoothie-after-the-shake-up\n\nfig, ax = plt.subplots(1,2,figsize=(20,5))\nsns.countplot(train_info.target, ax=ax[0], palette=\"Reds_r\")\nsns.countplot(external_train.target, ax=ax[1], palette=\"Reds_r\")\nax[1].set_title(\"Target imbalance in external train\")\nax[0].set_title(\"Target imbalance in original train\");", "processed": ["let take look differ target distribut"]}, {"markdown": ["Personally I find it a bit easier to use weighted cross entropy loss but perhaps with tuning the hyperparameters properly the focal loss could be a good choice as well. Try to start with $\\gamma=0$ as the focal loss would turn into weighted cross entropy loss in this case. I'm also working on a better way to set $\\alpha$. The class weights are not working well and there seems to be a way to set them based on the effective number of samples ([look at this paper](https://arxiv.org/abs/1901.05555)). I would like to try it out. "], "code": "# Reference: https://www.kaggle.com/code/allunia/don-t-turn-into-a-smoothie-after-the-shake-up\n\nfig, ax = plt.subplots(3,2,figsize=(20,18))\n\nrates = results[0].results[\"train\"].learning_rates\nf1_score = results[0].results[\"train\"].f1_scores\nprecision = results[0].results[\"train\"].precision\nrecall = results[0].results[\"train\"].recall\nlosses = results[0].results[\"train\"].losses\nepoch_losses = results[0].results[\"train\"].epoch_losses\n\nax[0,0].plot(rates, f1_score, '.-', c=\"maroon\", label=\"f1-score\")\nax[0,0].plot(rates, precision, '.-', c=\"salmon\", label=\"precision\")\nax[0,0].plot(rates, recall, '.-', c=\"lightsalmon\", label=\"recall\")\n\n\nax[0,0].legend();\nax[0,0].set_xlabel(\"Learning rate\")\nax[0,0].set_ylabel(\"Score values\")\nax[0,0].set_title(\"Evaluation scores for learning rate search within {} epochs\".format(NUM_EPOCHS));\n\nax[1,1].plot(rates, precision, '.-', c=\"salmon\", label=\"precision\")\nax[1,1].set_title(\"Precision\")\nax[1,1].set_xlabel(\"learning rates\")\nax[1,1].set_ylabel(\"precision\")\n\nax[1,0].plot(rates, recall, '.-', c=\"lightsalmon\", label=\"recall\")\nax[1,0].set_title(\"Recall\")\nax[1,0].set_xlabel(\"learning rates\")\nax[1,0].set_ylabel(\"recall\")\n\nax[0,1].plot(rates, f1_score, '.-', c=\"maroon\", label=\"f1-score\")\nax[0,1].set_title(\"F1-score\")\nax[0,1].set_xlabel(\"learning rates\")\nax[0,1].set_ylabel(\"f1-score\")\n\nax[2,0].plot(rates, losses, 'o-', c=\"deepskyblue\")\nax[2,0].set_title(\"Loss change with rates\")\nax[2,0].set_ylabel(\"loss\")\nax[2,0].set_xlabel(\"Learning rates\")\n\nax[2,1].set_title(\"Learning rate increase\")\nax[2,1].plot(rates, 'o', c=\"mediumseagreen\");\nax[2,1].set_ylabel(\"learning rate\")\nax[2,1].set_xlabel(\"Iteration step\");", "processed": ["person find bit easier use weight cross entropi loss perhap tune hyperparamet properli focal loss could good choic well tri start gamma 0 focal loss would turn weight cross entropi loss case also work better way set alpha class weight work well seem way set base effect number sampl look paper http arxiv org ab 1901 05555 would like tri"]}, {"markdown": ["### Insights\n\n* You may like to play with the smallest learning rate first. During my experiments I found that how small the first one is definitely influences the success of these curves and how much you can increase the max learning rate.\n* One can also see that the loss may go up and down a bit even though the scores are still increasing!", "## Running a model <a class=\"anchor\" id=\"running\"></a>\n\nLet's now check wheater everything works as expected:"], "code": "# Reference: https://www.kaggle.com/code/allunia/don-t-turn-into-a-smoothie-after-the-shake-up\n\ncheck_workflow = False\nsave_folder = \"check_workflow\"\nload_folder = \"../input/melanomaclassificationsmoothiestarter/check_workflow\"\nNUM_EPOCHS = 10\nLR = 0.01\nmin_lr = 0.0001\nmax_lr = 0.25\nfind_lr=False\nif check_workflow:\n    \n    results = {}\n    \n    train_df, dev_df = train_test_split(external_train,\n                                        stratify=external_train.target.values,\n                                        test_size=0.3,\n                                        random_state=0)\n    \n    train_dataset = AlbuMelanomaDataset(train_df, albu_transform_fun(RESIZE_SHAPE, key=\"train\"))\n    dev_dataset = AlbuMelanomaDataset(dev_df, albu_transform_fun(key=\"dev\"))\n    \n    train_dataloader = DataLoader(train_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=True, drop_last=True)\n    dev_dataloader = DataLoader(dev_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=False, drop_last=True)\n    dataloaders_dict = {\"train\": train_dataloader, \"dev\": dev_dataloader}\n    \n    model = build_model(my_model)\n    model.apply(init_weights)\n    model = model.to(device)\n    \n    criterion = MulticlassFocalLoss(gamma=2)\n    #criterion = get_wce_loss(train_df.target.values)\n    if \"efficientnet\" in my_model:\n        optimiser = torch.optim.SGD(model._fc.parameters(), lr=LR)\n    else:\n        optimiser = torch.optim.SGD(model.fc.parameters(), lr=LR)\n    \n    stepsize = 2*len(train_dataloader)\n    scheduler = get_scheduler(optimiser, min_lr, max_lr, stepsize)\n    \n    single_results = train(model=model,\n                           model_kind=my_model,\n                           criterion=criterion,\n                           optimiser=optimiser,\n                           num_epochs=NUM_EPOCHS,\n                           dataloaders_dict=dataloaders_dict,\n                           fold_num=0,\n                           scheduler=scheduler, \n                           patience=1,\n                           find_lr=find_lr)\n    \n    results = {0: single_results}\n    save_results(results, save_folder)\n\nelse:\n    \n    train_df, dev_df = train_test_split(external_train,\n                                        stratify=external_train.target.values,\n                                        test_size=0.3,\n                                        random_state=0)\n    \n    train_dataset = AlbuMelanomaDataset(train_df, albu_transform_fun(RESIZE_SHAPE, key=\"train\"))\n    dev_dataset = AlbuMelanomaDataset(dev_df, albu_transform_fun(key=\"dev\"))\n    \n    train_dataloader = DataLoader(train_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=True, drop_last=True)\n    dev_dataloader = DataLoader(dev_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=False, drop_last=True)\n    dataloaders_dict = {\"train\": train_dataloader, \"dev\": dev_dataloader}\n    \n    criterion = MulticlassFocalLoss(gamma=2)\n    \n    results = load_results(load_folder,\n                           total_folds=1,\n                           model_kind=my_model,\n                           lr=LR,\n                           num_epochs=NUM_EPOCHS,\n                           len_train=len(train_dataloader),\n                           min_lr=min_lr,\n                           max_lr=max_lr,\n                           for_inference=True)\nsave_results(results, save_folder)\nfig, ax = plt.subplots(3,2,figsize=(20,15))\n\nrates = results[0].results[\"train\"].learning_rates\nf1_score = results[0].results[\"train\"].f1_scores\nprecision = results[0].results[\"train\"].precision\nrecall = results[0].results[\"train\"].recall\n\ntrain_losses = results[0].results[\"train\"].losses\ndev_losses = results[0].results[\"dev\"].losses\n\ntrain_epoch_losses = results[0].results[\"train\"].epoch_losses\ndev_epoch_losses = results[0].results[\"dev\"].epoch_losses\ntrain_epoch_auc = results[0].results[\"train\"].epoch_scores\ndev_epoch_auc = results[0].results[\"dev\"].epoch_scores\n\nax[0,0].plot(f1_score, '.-', c=\"maroon\", label=\"f1-score\")\nax[0,0].plot(precision, '.-', c=\"salmon\", label=\"precision\")\nax[0,0].plot(recall, '.-', c=\"lightsalmon\", label=\"recall\")\n\nax[0,0].legend();\nax[0,0].set_xlabel(\"Learning rate\")\nax[0,0].set_ylabel(\"Score values\")\nax[0,0].set_title(\"Evaluation scores for learning rate search within {} epochs\".format(NUM_EPOCHS));\n\nax[0,1].plot(rates)\nax[0,1].set_title(\"Learning rates\")\n\nax[1,0].plot(train_losses, label=\"train\")\n\nax[1,1].plot(dev_losses, label=\"dev\");\nax[1,1].legend()\nax[1,1].set_title(\"Losses\")\n\nax[2,0].plot(train_epoch_losses, label=\"train\")\nax[2,0].plot(dev_epoch_losses, label=\"dev\")\nax[2,0].set_title(\"Epoch losses\")\n\nax[2,1].plot(train_epoch_auc)\nax[2,1].plot(dev_epoch_auc)\nax[2,1].set_title(\"Epoch AUC\");", "processed": ["insight may like play smallest learn rate first experi found small first one definit influenc success curv much increas max learn rate one also see loss may go bit even though score still increas", "run model class anchor id run let check wheater everyth work expect"]}, {"markdown": ["## Submission <a class=\"anchor\" id=\"submission\"></a>"], "code": "# Reference: https://www.kaggle.com/code/allunia/don-t-turn-into-a-smoothie-after-the-shake-up\n\nexternal_test_path = \"../input/melanoma-external-malignant-256/test/test/\"\ntest_info[\"image_path\"] = external_test_path + test_info.image_name +\".jpg\"\nTEST_BATCH_SIZE=68\ntest_dataset = AlbuMelanomaDataset(test_info, albu_transform_fun(RESIZE_SHAPE, \"test\"))\ntest_dataloader = DataLoader(test_dataset, batch_size=TEST_BATCH_SIZE, shuffle=False, drop_last=False)\npreds, probas = predict(results, test_dataloader)\nsubmission = pd.read_csv(basepath + \"sample_submission.csv\")\nsubmission.target = probas[:,1]\nsubmission.head()\nsns.distplot(submission.target)\nsubmission.to_csv(\"submission.csv\", index=False)", "processed": ["submiss class anchor id submiss"]}, {"markdown": ["There are one nominal feature (the **id**), 20 binary values, 21 real (or float numbers), 16 categorical features - all these being as well **input** values and one **target** value, which is as well **binary**, the **target**.", "# Data analysis and statistics\n\n", "## Target variable"], "code": "# Reference: https://www.kaggle.com/code/gpreda/porto-seguro-exploratory-analysis-and-prediction\n\nplt.figure()\nfig, ax = plt.subplots(figsize=(6,6))\nx = trainset['target'].value_counts().index.values\ny = trainset[\"target\"].value_counts().values\n# Bar plot\n# Order the bars descending on target mean\nsns.barplot(ax=ax, x=x, y=y)\nplt.ylabel('Number of values', fontsize=12)\nplt.xlabel('Target value', fontsize=12)\nplt.tick_params(axis='both', which='major', labelsize=12)\nplt.show();", "processed": ["one nomin featur id 20 binari valu 21 real float number 16 categor featur well input valu one target valu well binari target", "data analysi statist", "target variabl"]}, {"markdown": ["### Features with missing values\n\n**ps_reg_o3**, **ps_car_12**, **ps_car_14** have missing values (their minimum value is -1)\n\n\n### Registration features\n\n**ps_reg_01** and **ps_reg_02** are fractions with denominator 10 (values of 0.1, 0.2, 0.3 )\n\n### Car features\n\n**ps_car_12** are (with some approximations) square roots (divided by 10) of natural numbers whilst **ps_car_15** are square roots of natural numbers. Let's represent the values using *pairplot*.\n\n\n"], "code": "# Reference: https://www.kaggle.com/code/gpreda/porto-seguro-exploratory-analysis-and-prediction\n\nsample = trainset.sample(frac=0.05)\nvar = ['ps_car_12', 'ps_car_15', 'target']\nsample = sample[var]\nsns.pairplot(sample,  hue='target', palette = 'Set1', diag_kind='kde')\nplt.show()", "processed": ["featur miss valu p reg o3 p car 12 p car 14 miss valu minimum valu 1 registr featur p reg 01 p reg 02 fraction denomin 10 valu 0 1 0 2 0 3 car featur p car 12 approxim squar root divid 10 natur number whilst p car 15 squar root natur number let repres valu use pairplot"]}, {"markdown": ["### Calculated features\n\nThe features **ps_calc_01**, **ps_calc_02** and **ps_calc_03** have very similar distributions and could be some kind of ratio, since the maximum value is for all three 0.9. The other calculated values have maximum value an integer value (5,6,7, 10,12). ", "Let's visualize the real features distribution using density plot."], "code": "# Reference: https://www.kaggle.com/code/gpreda/porto-seguro-exploratory-analysis-and-prediction\n\nvar = metadata[(metadata.type == 'real') & (metadata.preserve)].index\ni = 0\nt1 = trainset.loc[trainset['target'] != 0]\nt0 = trainset.loc[trainset['target'] == 0]\n\nsns.set_style('whitegrid')\nplt.figure()\nfig, ax = plt.subplots(3,4,figsize=(16,12))\n\nfor feature in var:\n    i += 1\n    plt.subplot(3,4,i)\n    sns.kdeplot(t1[feature], bw=0.5,label=\"target = 1\")\n    sns.kdeplot(t0[feature], bw=0.5,label=\"target = 0\")\n    plt.ylabel('Density plot', fontsize=12)\n    plt.xlabel(feature, fontsize=12)\n    locs, labels = plt.xticks()\n    plt.tick_params(axis='both', which='major', labelsize=12)\nplt.show();", "processed": ["calcul featur featur p calc 01 p calc 02 p calc 03 similar distribut could kind ratio sinc maximum valu three 0 9 calcul valu maximum valu integ valu 5 6 7 10 12", "let visual real featur distribut use densiti plot"]}, {"markdown": ["Let's visualize the plots of the variables with strong correlations. These are:\n\n* ps_reg_01 with ps_reg_02 (0.47);  \n* ps_reg_01 with ps_reg_03 (0.64);  \n* ps_reg_02 with ps_reg_03 (0.52);  \n* ps_car_12 with ps_car_13 (0.67);  \n* ps_car_13 with ps_car_15 (0.53);  \n\n\nTo show the pairs of values that are correlated we use *pairplot*. Before representing the pairs, we subsample the data, using only 2% in the sample.\n\n"], "code": "# Reference: https://www.kaggle.com/code/gpreda/porto-seguro-exploratory-analysis-and-prediction\n\nsample = trainset.sample(frac=0.05)\nvar = ['ps_reg_01', 'ps_reg_02', 'ps_reg_03', 'ps_car_12', 'ps_car_13', 'ps_car_15', 'target']\nsample = sample[var]\nsns.pairplot(sample,  hue='target', palette = 'Set1', diag_kind='kde')\nplt.show()", "processed": ["let visual plot variabl strong correl p reg 01 p reg 02 0 47 p reg 01 p reg 03 0 64 p reg 02 p reg 03 0 52 p car 12 p car 13 0 67 p car 13 p car 15 0 53 show pair valu correl use pairplot repres pair subsampl data use 2 sampl"]}, {"markdown": ["Let's plot the distribution of the binary data in the training dataset. With `blue` we represent the percent of `0` and with `red` the percent of `1`."], "code": "# Reference: https://www.kaggle.com/code/gpreda/porto-seguro-exploratory-analysis-and-prediction\n\nbin_col = [col for col in trainset.columns if '_bin' in col]\nzero_list = []\none_list = []\nfor col in bin_col:\n    zero_list.append((trainset[col]==0).sum()/trainset.shape[0]*100)\n    one_list.append((trainset[col]==1).sum()/trainset.shape[0]*100)\nplt.figure()\nfig, ax = plt.subplots(figsize=(6,6))\n# Bar plot\np1 = sns.barplot(ax=ax, x=bin_col, y=zero_list, color=\"blue\")\np2 = sns.barplot(ax=ax, x=bin_col, y=one_list, bottom= zero_list, color=\"red\")\nplt.ylabel('Percent of zero/one [%]', fontsize=12)\nplt.xlabel('Binary features', fontsize=12)\nlocs, labels = plt.xticks()\nplt.setp(labels, rotation=90)\nplt.tick_params(axis='both', which='major', labelsize=12)\nplt.legend((p1, p2), ('Zero', 'One'))\nplt.show();", "processed": ["let plot distribut binari data train dataset blue repres percent 0 red percent 1"]}, {"markdown": ["**ps_ind_10_bin**, **ps_ind_11_bin**, **ps_ind_12_bin** and **ps_ind_13_bin** have very small number of  values `1` (lesss than 0.5%) whilst the number of  value `1` is very large for **ps_ind_16_bin** and **ps_cals_16_bin** (more than 60%).\n\nLet's see now the distribution of binary data and the corresponding values of **target** variable.\n"], "code": "# Reference: https://www.kaggle.com/code/gpreda/porto-seguro-exploratory-analysis-and-prediction\n\nvar = metadata[(metadata.type == 'binary') & (metadata.preserve)].index\nvar = [col for col in trainset.columns if '_bin' in col]\ni = 0\nt1 = trainset.loc[trainset['target'] != 0]\nt0 = trainset.loc[trainset['target'] == 0]\n\nsns.set_style('whitegrid')\nplt.figure()\nfig, ax = plt.subplots(6,3,figsize=(12,24))\n\nfor feature in var:\n    i += 1\n    plt.subplot(6,3,i)\n    sns.kdeplot(t1[feature], bw=0.5,label=\"target = 1\")\n    sns.kdeplot(t0[feature], bw=0.5,label=\"target = 0\")\n    plt.ylabel('Density plot', fontsize=12)\n    plt.xlabel(feature, fontsize=12)\n    locs, labels = plt.xticks()\n    plt.tick_params(axis='both', which='major', labelsize=12)\nplt.show();", "processed": ["p ind 10 bin p ind 11 bin p ind 12 bin p ind 13 bin small number valu 1 le 0 5 whilst number valu 1 larg p ind 16 bin p cal 16 bin 60 let see distribut binari data correspond valu target variabl"]}, {"markdown": ["**ps_ind_06_bin**, **ps_ind_07_bin**, **ps_ind_16_bin**, **ps_ind_17_bin**  shows high inbalance between distribution of values of `1` and `0` for values of target equals with `1` and `0`, **ps_ind_08_bin** shows a small inbalance while the other features are well balanced, having similar density plots.", "## Categorical features", "We will represent the distribution on `categorical` data in two ways. \nFirst, we calculate the percentage of `target=1` per category value and represent these percentages\nusing bar plots."], "code": "# Reference: https://www.kaggle.com/code/gpreda/porto-seguro-exploratory-analysis-and-prediction\n\nvar = metadata[(metadata.type == 'categorical') & (metadata.preserve)].index\n\nfor feature in var:\n    fig, ax = plt.subplots(figsize=(6,6))\n    # Calculate the percentage of target=1 per category value\n    cat_perc = trainset[[feature, 'target']].groupby([feature],as_index=False).mean()\n    cat_perc.sort_values(by='target', ascending=False, inplace=True)\n    # Bar plot\n    # Order the bars descending on target mean\n    sns.barplot(ax=ax,x=feature, y='target', data=cat_perc, order=cat_perc[feature])\n    plt.ylabel('Percent of target with value 1 [%]', fontsize=12)\n    plt.xlabel(feature, fontsize=12)\n    plt.tick_params(axis='both', which='major', labelsize=12)\n    plt.show();", "processed": ["p ind 06 bin p ind 07 bin p ind 16 bin p ind 17 bin show high inbal distribut valu 1 0 valu target equal 1 0 p ind 08 bin show small inbal featur well balanc similar densiti plot", "categor featur", "repres distribut categor data two way first calcul percentag target 1 per categori valu repres percentag use bar plot"]}, {"markdown": ["Alternativelly we represent the `categorical` features using density plot. We select values with `target=0` and `target=1` and represent both density plots on the same graphic."], "code": "# Reference: https://www.kaggle.com/code/gpreda/porto-seguro-exploratory-analysis-and-prediction\n\nvar = metadata[(metadata.type == 'categorical') & (metadata.preserve)].index\ni = 0\nt1 = trainset.loc[trainset['target'] != 0]\nt0 = trainset.loc[trainset['target'] == 0]\n\nsns.set_style('whitegrid')\nplt.figure()\nfig, ax = plt.subplots(4,4,figsize=(16,16))\n\nfor feature in var:\n    i += 1\n    plt.subplot(4,4,i)\n    sns.kdeplot(t1[feature], bw=0.5,label=\"target = 1\")\n    sns.kdeplot(t0[feature], bw=0.5,label=\"target = 0\")\n    plt.ylabel('Density plot', fontsize=12)\n    plt.xlabel(feature, fontsize=12)\n    locs, labels = plt.xticks()\n    plt.tick_params(axis='both', which='major', labelsize=12)\nplt.show();", "processed": ["alternativelli repres categor featur use densiti plot select valu target 0 target 1 repres densiti plot graphic"]}, {"markdown": ["**ps_car_03_cat**, **ps_car_05_cat** shows the most different density plot between values associated with `target=0` and `target=1`.", "## Data unbalance between train and test data ", "Let's compare the distribution of the features in the train and test datasets. \n\nWe start with the `reg` or `registration` features."], "code": "# Reference: https://www.kaggle.com/code/gpreda/porto-seguro-exploratory-analysis-and-prediction\n\nvar = metadata[(metadata.category == 'registration') & (metadata.preserve)].index\n\n# Bar plot\nsns.set_style('whitegrid')\n\nplt.figure()\nfig, ax = plt.subplots(1,3,figsize=(12,4))\ni = 0\nfor feature in var:\n    i = i + 1\n    plt.subplot(1,3,i)\n    sns.kdeplot(trainset[feature], bw=0.5, label=\"train\")\n    sns.kdeplot(testset[feature], bw=0.5, label=\"test\")\n    plt.ylabel('Distribution', fontsize=12)\n    plt.xlabel(feature, fontsize=12)\n    locs, labels = plt.xticks()\n    #plt.setp(labels, rotation=90)\n    plt.tick_params(axis='both', which='major', labelsize=12)\nplt.show();", "processed": ["p car 03 cat p car 05 cat show differ densiti plot valu associ target 0 target 1", "data unbal train test data", "let compar distribut featur train test dataset start reg registr featur"]}, {"markdown": ["All `reg` features shows well balanced train and test sets.\n\nLet's continue with `car` features."], "code": "# Reference: https://www.kaggle.com/code/gpreda/porto-seguro-exploratory-analysis-and-prediction\n\nvar = metadata[(metadata.category == 'car') & (metadata.preserve)].index\n\n# Bar plot\nsns.set_style('whitegrid')\n\nplt.figure()\nfig, ax = plt.subplots(4,4,figsize=(20,16))\ni = 0\nfor feature in var:\n    i = i + 1\n    plt.subplot(4,4,i)\n    sns.kdeplot(trainset[feature], bw=0.5, label=\"train\")\n    sns.kdeplot(testset[feature], bw=0.5, label=\"test\")\n    plt.ylabel('Distribution', fontsize=12)\n    plt.xlabel(feature, fontsize=12)\n    locs, labels = plt.xticks()\n    #plt.setp(labels, rotation=90)\n    plt.tick_params(axis='both', which='major', labelsize=12)\nplt.show();", "processed": ["reg featur show well balanc train test set let continu car featur"]}, {"markdown": ["From the `car` features, all variables looks well balanced between `train` and `test` set.\n\nLet's look now to the `ind` (`individual`) values."], "code": "# Reference: https://www.kaggle.com/code/gpreda/porto-seguro-exploratory-analysis-and-prediction\n\nvar = metadata[(metadata.category == 'individual') & (metadata.preserve)].index\n\n# Bar plot\nsns.set_style('whitegrid')\n\nplt.figure()\nfig, ax = plt.subplots(5,4,figsize=(20,16))\ni = 0\nfor feature in var:\n    i = i + 1\n    plt.subplot(5,4,i)\n    sns.kdeplot(trainset[feature], bw=0.5, label=\"train\")\n    sns.kdeplot(testset[feature], bw=0.5, label=\"test\")\n    plt.ylabel('Distribution', fontsize=12)\n    plt.xlabel(feature, fontsize=12)\n    locs, labels = plt.xticks()\n    #plt.setp(labels, rotation=90)\n    plt.tick_params(axis='both', which='major', labelsize=12)\nplt.show();", "processed": ["car featur variabl look well balanc train test set let look ind individu valu"]}, {"markdown": ["All `ind` features are well balanced between `train` and `test` sets.\n\nLet's check now `calc` features."], "code": "# Reference: https://www.kaggle.com/code/gpreda/porto-seguro-exploratory-analysis-and-prediction\n\nvar = metadata[(metadata.category == 'calculated') & (metadata.preserve)].index\n\n# Bar plot\nsns.set_style('whitegrid')\n\nplt.figure()\nfig, ax = plt.subplots(5,4,figsize=(20,16))\ni = 0\nfor feature in var:\n    i = i + 1\n    plt.subplot(5,4,i)\n    sns.kdeplot(trainset[feature], bw=0.5, label=\"train\")\n    sns.kdeplot(testset[feature], bw=0.5, label=\"test\")\n    plt.ylabel('Distribution', fontsize=12)\n    plt.xlabel(feature, fontsize=12)\n    locs, labels = plt.xticks()\n    #plt.setp(labels, rotation=90)\n    plt.tick_params(axis='both', which='major', labelsize=12)\nplt.show();", "processed": ["ind featur well balanc train test set let check calc featur"]}, {"markdown": ["### 2.2 Missing Values Percentage\n\nFrom the snapshot we can observe that there are many missing values in the dataset. Let's plot the missing values percentage for columns having missing values. \n\n> The following graph shows only those columns having missing values, all other columns are fine. "], "code": "# Reference: https://www.kaggle.com/code/shivamb/exploratory-analysis-ga-customer-revenue\n\nmiss_per = {}\nfor k, v in dict(train.isna().sum(axis=0)).items():\n    if v == 0:\n        continue\n    miss_per[k] = 100 * float(v) / len(train)\n    \nimport operator \nsorted_x = sorted(miss_per.items(), key=operator.itemgetter(1), reverse=True)\nprint (\"There are \" + str(len(miss_per)) + \" columns with missing values\")\n\nkys = [_[0] for _ in sorted_x][::-1]\nvls = [_[1] for _ in sorted_x][::-1]\ntrace1 = go.Bar(y = kys, orientation=\"h\" , x = vls, marker=dict(color=\"#d6a5ff\"))\nlayout = go.Layout(title=\"Missing Values Percentage\", \n                   xaxis=dict(title=\"Missing Percentage\"), \n                   height=400, margin=dict(l=300, r=300))\nfigure = go.Figure(data = [trace1], layout = layout)\niplot(figure)", "processed": ["2 2 miss valu percentag snapshot observ mani miss valu dataset let plot miss valu percentag column miss valu follow graph show column miss valu column fine"]}, {"markdown": ["> - So we can observe that there are some columns in the dataset having very large number of missing values. \n\n## 3. Exploration - Univariate Analysis \n\nLets perform the univariate analysis and plot some distributions of variables in the dataset\n\n### 3.1 Device Attributes\n\nLets plot the distribution of device attributes"], "code": "# Reference: https://www.kaggle.com/code/shivamb/exploratory-analysis-ga-customer-revenue\n\ndevice_cols = [\"device_browser\", \"device_deviceCategory\", \"device_operatingSystem\"]\n\ncolors = [\"#d6a5ff\", \"#fca6da\", \"#f4d39c\", \"#a9fcca\"]\ntraces = []\nfor i, col in enumerate(device_cols):\n    t = train[col].value_counts()\n    traces.append(go.Bar(marker=dict(color=colors[i]),orientation=\"h\", y = t.index[:15][::-1], x = t.values[:15][::-1]))\n\nfig = tools.make_subplots(rows=1, cols=3, subplot_titles=[\"Visits: Category\", \"Visits: Browser\",\"Visits: OS\"], print_grid=False)\nfig.append_trace(traces[1], 1, 1)\nfig.append_trace(traces[0], 1, 2)\nfig.append_trace(traces[2], 1, 3)\n\nfig['layout'].update(height=400, showlegend=False, title=\"Visits by Device Attributes\")\niplot(fig)\n\n## convert transaction revenue to float\ntrain[\"totals_transactionRevenue\"] = train[\"totals_transactionRevenue\"].astype('float')\n\ndevice_cols = [\"device_browser\", \"device_deviceCategory\", \"device_operatingSystem\"]\n\nfig = tools.make_subplots(rows=1, cols=3, subplot_titles=[\"Mean Revenue: Category\", \"Mean Revenue: Browser\",\"Mean Revenue: OS\"], print_grid=False)\n\ncolors = [\"red\", \"green\", \"purple\"]\ntrs = []\nfor i, col in enumerate(device_cols):\n    tmp = train.groupby(col).agg({\"totals_transactionRevenue\": \"mean\"}).reset_index().rename(columns={\"totals_transactionRevenue\" : \"Mean Revenue\"})\n    tmp = tmp.dropna().sort_values(\"Mean Revenue\", ascending = False)\n    tr = go.Bar(x = tmp[\"Mean Revenue\"][::-1], orientation=\"h\", marker=dict(opacity=0.5, color=colors[i]), y = tmp[col][::-1])\n    trs.append(tr)\n\nfig.append_trace(trs[1], 1, 1)\nfig.append_trace(trs[0], 1, 2)\nfig.append_trace(trs[2], 1, 3)\nfig['layout'].update(height=400, showlegend=False, title=\"Mean Revenue by Device Attributes\")\niplot(fig)", "processed": ["observ column dataset larg number miss valu 3 explor univari analysi let perform univari analysi plot distribut variabl dataset 3 1 devic attribut let plot distribut devic attribut"]}, {"markdown": ["> - There is a significant difference in visits from mobile and tablets, but mean revenue for both of them is very close.  \n> - Interesting to note that maximum visits are from Chrome browser however maximum revenue is collected from visits throught firefox. \n> - Chrome OS users has generated maximum revenue though maximum visits are from windows and macintosh users  \n\n### 3.2 GeoNetwork Attributes "], "code": "# Reference: https://www.kaggle.com/code/shivamb/exploratory-analysis-ga-customer-revenue\n\ngeo_cols = ['geoNetwork_city', 'geoNetwork_continent','geoNetwork_country',\n            'geoNetwork_metro', 'geoNetwork_networkDomain', 'geoNetwork_region','geoNetwork_subContinent']\ngeo_cols = ['geoNetwork_continent','geoNetwork_subContinent']\n\ncolors = [\"#d6a5ff\", \"#fca6da\"]\nfig = tools.make_subplots(rows=1, cols=2, subplot_titles=[\"Visits : GeoNetwork Continent\", \"Visits : GeoNetwork subContinent\"], print_grid=False)\ntrs = []\nfor i,col in enumerate(geo_cols):\n    t = train[col].value_counts()\n    tr = go.Bar(x = t.index[:20], marker=dict(color=colors[i]), y = t.values[:20])\n    trs.append(tr)\n\nfig.append_trace(trs[0], 1, 1)\nfig.append_trace(trs[1], 1, 2)\nfig['layout'].update(height=400, margin=dict(b=150), showlegend=False)\niplot(fig)\n\n\n\n\ngeo_cols = ['geoNetwork_continent','geoNetwork_subContinent']\nfig = tools.make_subplots(rows=1, cols=2, subplot_titles=[\"Mean Revenue: Continent\", \"Mean Revenue: SubContinent\"], print_grid=False)\n\ncolors = [\"blue\", \"orange\"]\ntrs = []\nfor i, col in enumerate(geo_cols):\n    tmp = train.groupby(col).agg({\"totals_transactionRevenue\": \"mean\"}).reset_index().rename(columns={\"totals_transactionRevenue\" : \"Mean Revenue\"})\n    tmp = tmp.dropna().sort_values(\"Mean Revenue\", ascending = False)\n    tr = go.Bar(y = tmp[\"Mean Revenue\"], orientation=\"v\", marker=dict(opacity=0.5, color=colors[i]), x= tmp[col])\n    trs.append(tr)\n\nfig.append_trace(trs[0], 1, 1)\nfig.append_trace(trs[1], 1, 2)\nfig['layout'].update(height=450, margin=dict(b=200), showlegend=False)\niplot(fig)\ntmp = train[\"geoNetwork_country\"].value_counts()\n\n# plotly globe credits - https://www.kaggle.com/arthurtok/generation-unemployed-interactive-plotly-visuals\ncolorscale = [[0, 'rgb(102,194,165)'], [0.005, 'rgb(102,194,165)'], \n              [0.01, 'rgb(171,221,164)'], [0.02, 'rgb(230,245,152)'], \n              [0.04, 'rgb(255,255,191)'], [0.05, 'rgb(254,224,139)'], \n              [0.10, 'rgb(253,174,97)'], [0.25, 'rgb(213,62,79)'], [1.0, 'rgb(158,1,66)']]\n\ndata = [ dict(\n        type = 'choropleth',\n        autocolorscale = False,\n        colorscale = colorscale,\n        showscale = True,\n        locations = tmp.index,\n        z = tmp.values,\n        locationmode = 'country names',\n        text = tmp.values,\n        marker = dict(\n            line = dict(color = '#fff', width = 2)) )           ]\n\nlayout = dict(\n    height=500,\n    title = 'Visits by Country',\n    geo = dict(\n        showframe = True,\n        showocean = True,\n        oceancolor = '#222',\n        projection = dict(\n        type = 'orthographic',\n            rotation = dict(\n                    lon = 60,\n                    lat = 10),\n        ),\n        lonaxis =  dict(\n                showgrid = False,\n                gridcolor = 'rgb(102, 102, 102)'\n            ),\n        lataxis = dict(\n                showgrid = False,\n                gridcolor = 'rgb(102, 102, 102)'\n                )\n            ),\n        )\nfig = dict(data=data, layout=layout)\niplot(fig)\n\n\ntmp = train.groupby(\"geoNetwork_country\").agg({\"totals_transactionRevenue\" : \"mean\"}).reset_index()\n\n\n\n# plotly globe credits - https://www.kaggle.com/arthurtok/generation-unemployed-interactive-plotly-visuals\ncolorscale = [[0, 'rgb(102,194,165)'], [0.005, 'rgb(102,194,165)'], \n              [0.01, 'rgb(171,221,164)'], [0.02, 'rgb(230,245,152)'], \n              [0.04, 'rgb(255,255,191)'], [0.05, 'rgb(254,224,139)'], \n              [0.10, 'rgb(253,174,97)'], [0.25, 'rgb(213,62,79)'], [1.0, 'rgb(158,1,66)']]\n\ndata = [ dict(\n        type = 'choropleth',\n        autocolorscale = False,\n        colorscale = colorscale,\n        showscale = True,\n        locations = tmp.geoNetwork_country,\n        z = tmp.totals_transactionRevenue,\n        locationmode = 'country names',\n        text = tmp.totals_transactionRevenue,\n        marker = dict(\n            line = dict(color = '#fff', width = 2)) )           ]\n\nlayout = dict(\n    height=500,\n    title = 'Mean Revenue by Countries',\n    geo = dict(\n        showframe = True,\n        showocean = True,\n        oceancolor = '#222',\n        projection = dict(\n        type = 'orthographic',\n            rotation = dict(\n                    lon = 60,\n                    lat = 10),\n        ),\n        lonaxis =  dict(\n                showgrid = False,\n                gridcolor = 'rgb(102, 102, 102)'\n            ),\n        lataxis = dict(\n                showgrid = False,\n                gridcolor = 'rgb(102, 102, 102)'\n                )\n            ),\n        )\nfig = dict(data=data, layout=layout)\niplot(fig)\n", "processed": ["signific differ visit mobil tablet mean revenu close interest note maximum visit chrome browser howev maximum revenu collect visit throught firefox chrome o user gener maximum revenu though maximum visit window macintosh user 3 2 geonetwork attribut"]}, {"markdown": ["### 3.3 Traffic Attributes\n\nLets now plot the traffic attributes"], "code": "# Reference: https://www.kaggle.com/code/shivamb/exploratory-analysis-ga-customer-revenue\n\nfig = tools.make_subplots(rows=1, cols=2, subplot_titles=[\"TrafficSource Campaign (not-set removed)\", \"TrafficSource Medium\"], print_grid=False)\n\ncolors = [\"#d6a5ff\", \"#fca6da\", \"#f4d39c\", \"#a9fcca\"]\nt1 = train[\"trafficSource_campaign\"].value_counts()\nt2 = train[\"trafficSource_medium\"].value_counts()\ntr1 = go.Bar(x = t1.index, y = t1.values, marker=dict(color=colors[3]))\ntr2 = go.Bar(x = t2.index, y = t2.values, marker=dict(color=colors[2]))\ntr3 = go.Bar(x = t1.index[1:], y = t1.values[1:], marker=dict(color=colors[0]))\ntr4 = go.Bar(x = t2.index[1:], y = t2.values[1:])\n\nfig.append_trace(tr3, 1, 1)\nfig.append_trace(tr2, 1, 2)\nfig['layout'].update(height=400, margin=dict(b=100), showlegend=False)\niplot(fig)", "processed": ["3 3 traffic attribut let plot traffic attribut"]}, {"markdown": ["### 3.4 Channel Grouping"], "code": "# Reference: https://www.kaggle.com/code/shivamb/exploratory-analysis-ga-customer-revenue\n\ntmp = train[\"channelGrouping\"].value_counts()\ncolors = [\"#8d44fc\", \"#ed95d5\", \"#caadf7\", \"#6161b7\", \"#7e7eba\", \"#babad1\"]\ntrace = go.Pie(labels=tmp.index, values=tmp.values, marker=dict(colors=colors))\nlayout = go.Layout(title=\"Channel Grouping\", height=400)\nfig = go.Figure(data = [trace], layout = layout)\niplot(fig, filename='basic_pie_chart')", "processed": ["3 4 channel group"]}, {"markdown": ["### 3.5 Visits by date, month and day"], "code": "# Reference: https://www.kaggle.com/code/shivamb/exploratory-analysis-ga-customer-revenue\n\ndef _add_date_features(df):\n    df['date'] = df['date'].astype(str)\n    df[\"date\"] = df[\"date\"].apply(lambda x : x[:4] + \"-\" + x[4:6] + \"-\" + x[6:])\n    df[\"date\"] = pd.to_datetime(df[\"date\"])\n    \n    df[\"month\"]   = df['date'].dt.month\n    df[\"day\"]     = df['date'].dt.day\n    df[\"weekday\"] = df['date'].dt.weekday\n    return df \n\ntrain = _add_date_features(train)\n\ntmp = train['date'].value_counts().to_frame().reset_index().sort_values('index')\ntmp = tmp.rename(columns = {\"index\" : \"dateX\", \"date\" : \"visits\"})\n\ntr = go.Scatter(mode=\"lines\", x = tmp[\"dateX\"].astype(str), y = tmp[\"visits\"])\nlayout = go.Layout(title=\"Visits by date\", height=400)\nfig = go.Figure(data = [tr], layout = layout)\niplot(fig)\n\n\ntmp = train.groupby(\"date\").agg({\"totals_transactionRevenue\" : \"mean\"}).reset_index()\ntmp = tmp.rename(columns = {\"date\" : \"dateX\", \"totals_transactionRevenue\" : \"mean_revenue\"})\ntr = go.Scatter(mode=\"lines\", x = tmp[\"dateX\"].astype(str), y = tmp[\"mean_revenue\"])\nlayout = go.Layout(title=\"MonthlyRevenue by date\", height=400)\nfig = go.Figure(data = [tr], layout = layout)\niplot(fig)\nfig = tools.make_subplots(rows=1, cols=3, subplot_titles=[\"Visits by Month\", \"Visits by MonthDay\", \"Visits by WeekDay\"], print_grid=False)\ntrs = []\nfor i,col in enumerate([\"month\", \"day\", \"weekday\"]):\n    t = train[col].value_counts()\n    tr = go.Bar(x = t.index, marker=dict(color=colors[i]), y = t.values)\n    trs.append(tr)\n\nfig.append_trace(trs[0], 1, 1)\nfig.append_trace(trs[1], 1, 2)\nfig.append_trace(trs[2], 1, 3)\nfig['layout'].update(height=400, showlegend=False)\niplot(fig)\n\n\n\ntmp1 = train.groupby('month').agg({\"totals_transactionRevenue\" : \"mean\"}).reset_index()\ntmp2 = train.groupby('day').agg({\"totals_transactionRevenue\" : \"mean\"}).reset_index()\ntmp3 = train.groupby('weekday').agg({\"totals_transactionRevenue\" : \"mean\"}).reset_index()\n\nfig = tools.make_subplots(rows=1, cols=3, subplot_titles=[\"MeanRevenue by Month\", \"MeanRevenue by MonthDay\", \"MeanRevenue by WeekDay\"], print_grid=False)\ntr1 = go.Bar(x = tmp1.month, marker=dict(color=\"red\", opacity=0.5), y = tmp1.totals_transactionRevenue)\ntr2 = go.Bar(x = tmp2.day, marker=dict(color=\"orange\", opacity=0.5), y = tmp2.totals_transactionRevenue)\ntr3 = go.Bar(x = tmp3.weekday, marker=dict(color=\"green\", opacity=0.5), y = tmp3.totals_transactionRevenue)\n\nfig.append_trace(tr1, 1, 1)\nfig.append_trace(tr2, 1, 2)\nfig.append_trace(tr3, 1, 3)\nfig['layout'].update(height=400, showlegend=False)\niplot(fig)", "processed": ["3 5 visit date month day"]}, {"markdown": ["### 3.6 Visit Number Frequency"], "code": "# Reference: https://www.kaggle.com/code/shivamb/exploratory-analysis-ga-customer-revenue\n\nvn = train[\"visitNumber\"].value_counts()\ndef vn_bins(x):\n    if x == 1:\n        return \"1\" \n    elif x < 5:\n        return \"2-5\"\n    elif x < 10:\n        return \"5-10\"\n    elif x < 50:\n        return \"10-50\"\n    elif x < 100:\n        return \"50-100\"\n    else:\n        return \"100+\"\n    \nvn = train[\"visitNumber\"].apply(vn_bins).value_counts()\n\ntrace1 = go.Bar(y = vn.index[::-1], orientation=\"h\" , x = vn.values[::-1], marker=dict(color=\"#7af9ad\"))\nlayout = go.Layout(title=\"Visit Numbers Distribution\", \n                   xaxis=dict(title=\"Frequency\"),yaxis=dict(title=\"VisitNumber\") ,\n                   height=400, margin=dict(l=300, r=300))\nfigure = go.Figure(data = [trace1], layout = layout)\niplot(figure)", "processed": ["3 6 visit number frequenc"]}, {"markdown": ["### 4.2 Total Transactions Revenue"], "code": "# Reference: https://www.kaggle.com/code/shivamb/exploratory-analysis-ga-customer-revenue\n\nnon_zero = tmp[tmp[\"totals_transactionRevenue\"] > 0][\"totals_transactionRevenue\"]\nprint (\"There are \" + str(len(non_zero)) + \" visitors in the train dataset having non zero total transaction revenue\")\n\nplt.figure(figsize=(12,6))\nsns.distplot(non_zero)\nplt.title(\"Distribution of Non Zero Total Transactions\");\nplt.xlabel(\"Total Transactions\");", "processed": ["4 2 total transact revenu"]}, {"markdown": ["Lets take the natural log on the transactions"], "code": "# Reference: https://www.kaggle.com/code/shivamb/exploratory-analysis-ga-customer-revenue\n\nplt.figure(figsize=(12,6))\nsns.distplot(np.log1p(non_zero))\nplt.title(\"Log Distribution of Non Zero Total Transactions\");\nplt.xlabel(\"Log - Total Transactions\");", "processed": ["let take natur log transact"]}, {"markdown": ["### 4.3 Visitor Profile Attributes"], "code": "# Reference: https://www.kaggle.com/code/shivamb/exploratory-analysis-ga-customer-revenue\n\ndef getbin_hits(x):\n    if x < 5:\n        return \"1-5\"\n    elif x < 10:\n        return \"5-10\"\n    elif x < 30:\n        return \"10-30\"\n    elif x < 50:\n        return \"30-50\"\n    elif x < 100:\n        return \"50-100\"\n    else:\n        return \"100+\"\n\ntmp[\"total_hits_bin\"] = tmp[\"totals_hits\"].apply(getbin_hits)\ntmp[\"totals_bounces_bin\"] = tmp[\"totals_bounces\"].apply(lambda x : str(x) if x <= 5 else \"5+\")\ntmp[\"totals_pageviews_bin\"] = tmp[\"totals_pageviews\"].apply(lambda x : str(x) if x <= 50 else \"50+\")\n\nt1 = tmp[\"total_hits_bin\"].value_counts()\nt2 = tmp[\"totals_bounces_bin\"].value_counts()\nt3 = tmp[\"totals_newVisits\"].value_counts()\nt4 = tmp[\"totals_pageviews_bin\"].value_counts()\n\nfig = tools.make_subplots(rows=2, cols=2, subplot_titles=[\"Total Hits per User\", \"Total Bounces per User\", \n                                                         \"Total NewVistits per User\", \"Total PageViews per User\"], print_grid=False)\n\ntr1 = go.Bar(x = t1.index[:20], y = t1.values[:20])\ntr2 = go.Bar(x = t2.index[:20], y = t2.values[:20])\ntr3 = go.Bar(x = t3.index[:20], y = t3.values[:20])\ntr4 = go.Bar(x = t4.index, y = t4.values)\n\nfig.append_trace(tr1, 1, 1)\nfig.append_trace(tr2, 1, 2)\nfig.append_trace(tr3, 2, 1)\nfig.append_trace(tr4, 2, 2)\n\nfig['layout'].update(height=700, showlegend=False)\niplot(fig)", "processed": ["4 3 visitor profil attribut"]}, {"markdown": ["# Display Predictions\nFirst we will display a histogram and next display a time series plot. In the time series plot, you can see how the predictions for the final days November 20, 21, 22, 23, 24, 25 have `HasDetections=0`. Those are the outliers which have been corrected for."], "code": "# Reference: https://www.kaggle.com/code/cdeotte/high-scoring-lgbm-malware-0-702-0-775\n\nimport matplotlib.pyplot as plt    \nb = plt.hist(df_test['HasDetections'], bins=200)\nimport calendar, math\n\ndef dynamicPlot(data,col, target='HasDetections', start=datetime(2018,4,1), end=datetime(2018,12,1)\n                ,inc_hr=0,inc_dy=7,inc_mn=0,show=0.99,top=5,top2=4,title='',legend=1,z=0,dots=False):\n    # check for timestamps\n    if 'Date' not in data:\n        print('Error dynamicPlot: DataFrame needs column Date of datetimes')\n        return\n    \n    # remove detection line if category density is too small\n    cv = data[(data['Date']>start) & (data['Date']<end)][col].value_counts(dropna=False)\n    cvd = cv.to_dict()\n    nm = cv.index.values\n    th = show * len(data)\n    sum = 0; lnn2 = 0\n    for x in nm:\n        lnn2 += 1\n        sum += cvd[x]\n        if sum>th:\n            break\n    top = min(top,len(nm))\n    top2 = min(top2,len(nm),lnn2,top)\n\n    # calculate rate within each time interval\n    diff = (end-start).days*24*3600 + (end-start).seconds\n    size = diff//(3600*((inc_mn * 28 + inc_dy) * 24 + inc_hr)) + 5\n    data_counts = np.zeros([size,2*top+1],dtype=float)\n    idx=0; idx2 = {}\n    for i in range(top):\n        idx2[nm[i]] = i+1\n    low = start\n    high = add_time(start,inc_mn,inc_dy,inc_hr)\n    data_times = [low+(high-low)/2]\n    while low<end:\n        slice = data[ (data['Date']<high) & (data['Date']>=low) ]\n        #data_counts[idx,0] = len(slice)\n        data_counts[idx,0] = 5000*len(slice['AvSigVersion'].unique())\n        for key in idx2:\n            if nan_check(key): slice2 = slice[slice[col].isna()]\n            else: slice2 = slice[slice[col]==key]\n            data_counts[idx,idx2[key]] = len(slice2)\n            if target in data:\n                data_counts[idx,top+idx2[key]] = slice2['HasDetections'].mean()\n        low = high\n        high = add_time(high,inc_mn,inc_dy,inc_hr)\n        data_times.append(low+(high-low)/2)\n        idx += 1\n\n    # plot lines\n    fig = plt.figure(1,figsize=(15,3))\n    cl = ['r','g','b','y','m']\n    ax3 = fig.add_subplot(1,1,1)\n    lines = []; labels = []\n    if z==1: ax3.plot(data_times,data_counts[0:idx+1,0],'k')\n    for i in range(top):\n        tmp, = ax3.plot(data_times,data_counts[0:idx+1,i+1],cl[i%5])\n        if dots: ax3.plot(data_times,data_counts[0:idx+1,i+1],cl[i%5]+'o')\n        lines.append(tmp)\n        labels.append(str(nm[i]))\n    ax3.spines['left'].set_color('red')\n    ax3.yaxis.label.set_color('red')\n    ax3.tick_params(axis='y', colors='red')\n    if col!='ones': ax3.set_ylabel('Category Density', color='r')\n    else: ax3.set_ylabel('Data Density', color='r')\n    #ax3.set_yticklabels([])\n    if target in data:\n        ax4 = ax3.twinx()\n        for i in range(top2):\n            ax4.plot(data_times,data_counts[0:idx+1,i+1+top],cl[i%5]+\":\")\n            if dots: ax4.plot(data_times,data_counts[0:idx+1,i+1+top],cl[i%5]+\"o\")\n        ax4.spines['left'].set_color('red')\n        ax4.set_ylabel('Detection Rate', color='k')\n    if title!='': plt.title(title)\n    if legend==1: plt.legend(lines,labels,loc=2)\n    plt.show()\n        \n# INCREMENT A DATETIME\ndef add_time(sdate,months=0,days=0,hours=0):\n    month = sdate.month -1 + months\n    year = sdate.year + month // 12\n    month = month % 12 + 1\n    day = sdate.day + days\n    if day>calendar.monthrange(year,month)[1]:\n        day -= calendar.monthrange(year,month)[1]\n        month += 1\n        if month>12:\n            month = 1\n            year += 1\n    hour = sdate.hour + hours\n    if hour>23:\n        hour = 0\n        day += 1\n        if day>calendar.monthrange(year,month)[1]:\n            day -= calendar.monthrange(year,month)[1]\n            month += 1\n            if month>12:\n                month = 1\n                year += 1\n    return datetime(year,month,day,hour,sdate.minute)\n\n# CHECK FOR NAN\ndef nan_check(x):\n    if isinstance(x,float):\n        if math.isnan(x):\n            return True\n    return False\ndf_test['ones'] = 1\ndynamicPlot(df_test, 'ones', inc_dy=2, legend=0,\n        title='Test Predictions. (Dotted line uses right y-axis. Solid uses left.)')", "processed": ["display predict first display histogram next display time seri plot time seri plot see predict final day novemb 20 21 22 23 24 25 hasdetect 0 outlier correct"]}, {"markdown": ["## Data overview"], "code": "# Reference: https://www.kaggle.com/code/artgor/detecting-cactus-with-kekas\n\nlabels = pd.read_csv('../input/train.csv')\nfig = plt.figure(figsize=(25, 8))\ntrain_imgs = os.listdir(\"../input/train/train\")\nfor idx, img in enumerate(np.random.choice(train_imgs, 20)):\n    ax = fig.add_subplot(4, 20//4, idx+1, xticks=[], yticks=[])\n    im = Image.open(\"../input/train/train/\" + img)\n    plt.imshow(im)\n    lab = labels.loc[labels['id'] == img, 'has_cactus'].values[0]\n    ax.set_title(f'Label: {lab}')", "processed": ["data overview"]}, {"markdown": ["Main point is gathering and automising as much as possible. So we will plot all of the variables together (modifying the code for different problems), than \"zoom in\" in case of suspicion. Since there are a lot of indicators I only took some of them to speed up the computation. We can see distributions as well as plots in relationship with other variables."], "code": "# Reference: https://www.kaggle.com/code/zikazika/reusable-insightful-eda\n\nsns.pairplot(train_df.iloc[:,197:])", "processed": ["main point gather automis much possibl plot variabl togeth modifi code differ problem zoom case suspicion sinc lot indic took speed comput see distribut well plot relationship variabl"]}, {"markdown": ["Same for test set."], "code": "# Reference: https://www.kaggle.com/code/zikazika/reusable-insightful-eda\n\nsns.pairplot(test_df.iloc[:,198:])", "processed": ["test set"]}, {"markdown": ["We should also plot other variables in dependence to the dependent variable--**target**. Thats the first column so lets just take a couple of the first columns. (run-time!!!)"], "code": "# Reference: https://www.kaggle.com/code/zikazika/reusable-insightful-eda\n\nsns.pairplot(train_df.iloc[:,:5])", "processed": ["also plot variabl depend depend variabl target that first column let take coupl first column run time"]}, {"markdown": ["BUT, this is a boiler plat code that can be re-used later on different projects!"], "code": "# Reference: https://www.kaggle.com/code/zikazika/reusable-insightful-eda\n\nsns.pairplot(pd.isna(train_df.iloc[:,198:]))", "processed": ["boiler plat code use later differ project"]}, {"markdown": ["Another way to look at the outliers but also in the same time get some more information about distribution (IQR, median, mean etc...) is with the box-plot. But we need to do it efficiently:\n"], "code": "# Reference: https://www.kaggle.com/code/zikazika/reusable-insightful-eda\n\nmelted = pd.melt(train_df.iloc[:,194:])\nmelted[\"value\"] = pd.to_numeric(melted[\"value\"])\n\nsns_plot1=sns.boxplot(x=\"variable\", y=\"value\", data=melted)\nsns_plot1.set_xticklabels(sns_plot1.get_xticklabels(), rotation = 90, fontsize = 10)", "processed": ["anoth way look outlier also time get inform distribut iqr median mean etc box plot need effici"]}, {"markdown": ["As we noticed from the first plots (scattered ones) there is not really much correlation between the variables.", "So what are some other insights that can be gathered using EDA about the data? One interesting thing is the distribution (density) plot of different predicators when in contrast to different classes (0 or 1)."], "code": "# Reference: https://www.kaggle.com/code/zikazika/reusable-insightful-eda\n\ndef plot_feature_distribution(df1, df2, label1, label2, features):\n    i = 0\n    sns.set_style('whitegrid')\n    plt.figure()\n    fig, ax = plt.subplots(5,10,figsize=(18,22))\n\n    for feature in features:\n        i += 1\n        plt.subplot(5,10,i)\n        sns.kdeplot(df1[feature], bw=0.5,label=label1)\n        sns.kdeplot(df2[feature], bw=0.5,label=label2)\n        plt.xlabel(feature, fontsize=9)\n        locs, labels = plt.xticks()\n        plt.tick_params(axis='x', which='major', labelsize=6, pad=-6)\n        plt.tick_params(axis='y', which='major', labelsize=6)\n    plt.show();\nt0 = train_df.loc[train_df['target'] == 0]\nt1 = train_df.loc[train_df['target'] == 1]\nfeatures = train_df.columns.values[2:52]\nplot_feature_distribution(t0, t1, '0', '1', features)", "processed": ["notic first plot scatter one realli much correl variabl", "insight gather use eda data one interest thing distribut densiti plot differ predic contrast differ class 0 1"]}, {"markdown": ["## 6. Models comparison <a class=\"anchor\" id=\"6\"></a>\n\n[Back to Table of Contents](#0.1)", "We can now compare our models and to choose the best one for our problem."], "code": "# Reference: https://www.kaggle.com/code/vbmokin/qa-prediction-question-not-really-a-question\n\nmodels = pd.DataFrame({\n    'Model': ['Linear Regression', 'Support Vector Machines', 'Linear SVR', \n              'MLPRegressor', 'Stochastic Gradient Decent', \n              'Decision Tree Regressor', 'Random Forest',  'XGB', 'LGBM',\n              'GradientBoostingRegressor', 'RidgeRegressor', 'BaggingRegressor', 'ExtraTreesRegressor', \n              'AdaBoostRegressor', 'VotingRegressor'],\n    \n    'r2_train': acc_train_r2,\n    'r2_test': acc_test_r2,\n    'd_train': acc_train_d,\n    'd_test': acc_test_d,\n    'rmse_train': acc_train_rmse,\n    'rmse_test': acc_test_rmse\n                     })\npd.options.display.float_format = '{:,.2f}'.format\nprint('Prediction accuracy for models by R2 criterion - r2_test')\nmodels.sort_values(by=['r2_test', 'r2_train'], ascending=False)\nprint('Prediction accuracy for models by relative error - d_test')\nmodels.sort_values(by=['d_test', 'd_train'], ascending=True)\nprint('Prediction accuracy for models by RMSE - rmse_test')\nmodels.sort_values(by=['rmse_test', 'rmse_train'], ascending=True)\n# Plot\nplt.figure(figsize=[25,6])\nxx = models['Model']\nplt.tick_params(labelsize=14)\nplt.plot(xx, models['r2_train'], label = 'r2_train')\nplt.plot(xx, models['r2_test'], label = 'r2_test')\nplt.legend()\nplt.title('R2-criterion for 15 popular models for train and test datasets')\nplt.xlabel('Models')\nplt.ylabel('R2-criterion, %')\nplt.xticks(xx, rotation='vertical')\nplt.savefig('graph.png')\nplt.show()\n# Plot\nplt.figure(figsize=[25,6])\nxx = models['Model']\nplt.tick_params(labelsize=14)\nplt.plot(xx, models['d_train'], label = 'd_train')\nplt.plot(xx, models['d_test'], label = 'd_test')\nplt.legend()\nplt.title('Relative errors for 15 popular models for train and test datasets')\nplt.xlabel('Models')\nplt.ylabel('Relative error, %')\nplt.xticks(xx, rotation='vertical')\nplt.savefig('graph.png')\nplt.show()\n# Plot\nplt.figure(figsize=[25,6])\nxx = models['Model']\nplt.tick_params(labelsize=14)\nplt.plot(xx, models['rmse_train'], label = 'rmse_train')\nplt.plot(xx, models['rmse_test'], label = 'rmse_test')\nplt.legend()\nplt.title('RMSE for 15 popular models for train and test datasets')\nplt.xlabel('Models')\nplt.ylabel('RMSE, %')\nplt.xticks(xx, rotation='vertical')\nplt.savefig('graph.png')\nplt.show()", "processed": ["6 model comparison class anchor id 6 back tabl content 0 1", "compar model choos best one problem"]}, {"markdown": ["### Visualize distribution of scalar coupling coefficient"], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/champs-competition-chemistry-background-and-eda\n\nsns.distplot(train['scalar_coupling_constant'], color='orangered')\nplt.show()", "processed": ["visual distribut scalar coupl coeffici"]}, {"markdown": ["From the above graph, we can see that the distribution of *scalar_coupling_coefficient* is skewed to the left, but the distribution is not perfectly unimodal. There is a significantly smaller peak close to 100 making it bimodal. The mode and mean is approximately 0."], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/champs-competition-chemistry-background-and-eda\n\nplt.figure(figsize=(26, 24))\nfor i, col in enumerate(typelist):\n    plt.subplot(4,2, i + 1)\n    sns.distplot(train[train['type']==col]['scalar_coupling_constant'],color ='indigo')\n    plt.title(col)", "processed": ["graph see distribut scalar coupl coeffici skew left distribut perfectli unimod significantli smaller peak close 100 make bimod mode mean approxim 0"]}, {"markdown": ["### Visualize the distribution of dipole moments in X, Y and Z directions"], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/champs-competition-chemistry-background-and-eda\n\nsns.distplot(dipole_moments.X, color='mediumseagreen')\nplt.title('Dipole moment along X-axis')\nplt.show()\nsns.distplot(dipole_moments.Y, color='seagreen')\nplt.title('Dipole moment along Y-axis')\nplt.show()\nsns.distplot(dipole_moments.Z, color='green')\nplt.title('Dipole moment along Z-axis')\nplt.show()", "processed": ["visual distribut dipol moment x z direct"]}, {"markdown": ["The distributions of dipole moment along the X and Y axes are approximately normal with a mean of 0, with the X-axis distribution having a greater standard deviation and range. On the other hand, the dipole moment along the Z-axis has a slightly skewed distribution (skewed to the right), with a secondary peak around 1 in addition to the primary peak (mode) above 0.", "### Visualize the distribution of dipole moments in all directions for each molecule type"], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/champs-competition-chemistry-background-and-eda\n\nplt.figure(figsize=(26, 24))\nfor i, col in enumerate(typelist):\n    plt.subplot(4,2, i + 1)\n    sns.distplot(dipole_moments[train['type']==col]['X'],color = 'orange', kde=False)\n    sns.distplot(dipole_moments[train['type']==col]['Y'],color = 'red', kde=False)\n    sns.distplot(dipole_moments[train['type']==col]['Z'],color = 'blue', kde=False)\n    plt.title(col)", "processed": ["distribut dipol moment along x axe approxim normal mean 0 x axi distribut greater standard deviat rang hand dipol moment along z axi slightli skew distribut skew right secondari peak around 1 addit primari peak mode 0", "visual distribut dipol moment direct molecul type"]}, {"markdown": ["### Visualize the distribution of potential energy"], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/champs-competition-chemistry-background-and-eda\n\nsns.distplot(potential_energy.potential_energy, color='darkblue', kde=False)\nplt.show()", "processed": ["visual distribut potenti energi"]}, {"markdown": ["The distribution of potential energy of the molecules is approximately normal with a mean of around -400.", "### Visualize the distribution of potential energy for each molecule type"], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/champs-competition-chemistry-background-and-eda\n\nplt.figure(figsize=(26, 24))\nfor i, col in enumerate(typelist):\n    plt.subplot(4,2, i + 1)\n    sns.distplot(potential_energy[train['type']==col]['potential_energy'], color = 'orangered')\n    plt.title(col)", "processed": ["distribut potenti energi molecul approxim normal mean around 400", "visual distribut potenti energi molecul type"]}, {"markdown": ["### Visualize the magnetic shielding in each direction combination"], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/champs-competition-chemistry-background-and-eda\n\nsns.distplot(magnetic_shielding_tensors.XX[~is_outlier(magnetic_shielding_tensors.XX)], color='red')\nplt.title('Magnetic Shielding (XX)')\nplt.show()\nsns.distplot(magnetic_shielding_tensors.XY[~is_outlier(magnetic_shielding_tensors.XY)], color='orangered')\nplt.title('Magnetic Shielding (XY)')\nplt.show()\nsns.distplot(magnetic_shielding_tensors.XZ[~is_outlier(magnetic_shielding_tensors.XZ)], color='orange')\nplt.title('Magnetic Shielding (XZ)')\nplt.show()\nsns.distplot(magnetic_shielding_tensors.YX[~is_outlier(magnetic_shielding_tensors.YX)], color='yellow')\nplt.title('Magnetic Shielding (YX)')\nplt.show()\nsns.distplot(magnetic_shielding_tensors.YY[~is_outlier(magnetic_shielding_tensors.YY)], color='green')\nplt.title('Magnetic Shielding (YY)')\nplt.show()\nsns.distplot(magnetic_shielding_tensors.YZ[~is_outlier(magnetic_shielding_tensors.YZ)], color='blue')\nplt.title('Magnetic Shielding (YZ)')\nplt.show()\nsns.distplot(magnetic_shielding_tensors.ZX[~is_outlier(magnetic_shielding_tensors.ZX)], color='darkblue')\nplt.title('Magnetic Shielding (ZX)')\nplt.show()\nsns.distplot(magnetic_shielding_tensors.ZY[~is_outlier(magnetic_shielding_tensors.ZY)], color='indigo')\nplt.title('Magnetic Shielding (ZY)')\nplt.show()\nsns.distplot(magnetic_shielding_tensors.ZZ[~is_outlier(magnetic_shielding_tensors.ZZ)], color='darkviolet')\nplt.title('Magnetic Shielding (ZZ)')\nplt.show()", "processed": ["visual magnet shield direct combin"]}, {"markdown": ["### Visualize distribution of mulliken charge"], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/champs-competition-chemistry-background-and-eda\n\nsns.distplot(mulliken_charges.mulliken_charge, color = 'seagreen')\nplt.show()", "processed": ["visual distribut mulliken charg"]}, {"markdown": ["The distribution of mulliken charges of the molecules peaks at around 0.175 and it is clearly unimodal. But, the distribution has a very uneven (small peaks and valleys) appearance on the tails. Additionally, there is a clear rightward skew.", "### Visualize distribution of mulliken charge for each atom index"], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/champs-competition-chemistry-background-and-eda\n\nsns.distplot(mulliken_charges.loc[mulliken_charges.atom_index == 0].mulliken_charge, color = 'blue')\nplt.title('Atom index 0')\nplt.show()\nsns.distplot(mulliken_charges.loc[mulliken_charges.atom_index == 1].mulliken_charge, color = 'darkblue')\nplt.title('Atom index 1')\nplt.show()\nsns.distplot(mulliken_charges.loc[mulliken_charges.atom_index == 2].mulliken_charge, color = 'blueviolet')\nplt.title('Atom index 2')\nplt.show()\nsns.distplot(mulliken_charges.loc[mulliken_charges.atom_index == 3].mulliken_charge, color = 'purple')\nplt.title('Atom index 3')\nplt.show()\nsns.distplot(mulliken_charges.loc[mulliken_charges.atom_index == 3].mulliken_charge, color = 'indigo')\nplt.title('Atom index 4')\nplt.show()", "processed": ["distribut mulliken charg molecul peak around 0 175 clearli unimod distribut uneven small peak valley appear tail addit clear rightward skew", "visual distribut mulliken charg atom index"]}, {"markdown": ["Wow, AUC of 0.9999 is as large as it gets! Let's see which columns are the most responsible for this discrepancy."], "code": "# Reference: https://www.kaggle.com/code/tunguz/m5-adversarial-validation\n\nfeature_imp = pd.DataFrame(sorted(zip(clf.feature_importance(),features)), columns=['Value','Feature'])\n\nplt.figure(figsize=(20, 20))\nsns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False).head(100))\nplt.title('LightGBM Features')\nplt.tight_layout()\nplt.show()\nplt.savefig('lgbm_importances-01.png')", "processed": ["wow auc 0 9999 larg get let see column respons discrep"]}, {"markdown": ["Seems that the temporal features are the most disperate between the two models, which may not be surprising for a time-series problem. ", "Now we will repeat the same procedure, but now we'll drop 'week' from the features."], "code": "# Reference: https://www.kaggle.com/code/tunguz/m5-adversarial-validation\n\n%%time\ndel train, test, clf\n\ngc.collect()\ngc.collect()\n\nfeatures = ['item_id', 'dept_id', 'store_id', 'cat_id', 'state_id', 'wday', 'month',\n       'year', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2',\n       'snap_CA', 'snap_TX', 'snap_WI', 'sell_price', 'lag_7', 'lag_28',\n       'rmean_7_7', 'rmean_28_7', 'rmean_7_28', 'rmean_28_28',\n       'quarter', 'mday']\n\ntest = pd.read_csv('../input/best-features-only/X_test.csv', usecols=features)\ntrain = pd.read_csv('../input/best-features-only/X_train.csv', usecols=features)\n\ntrain = pd.concat([train, test], axis =0)\ndel test\ngc.collect()\n\ntarget = np.hstack([np.zeros(trs,), np.ones(tes,)])\n\ntrain, test, y_train, y_test = model_selection.train_test_split(train, target, test_size=0.33, random_state=42, shuffle=True)\ndel target\ngc.collect()\n\ntrain = lgb.Dataset(train, label=y_train)\ntest = lgb.Dataset(test, label=y_test)\n\nclf = lgb.train(param, train, num_round, valid_sets = [train, test], verbose_eval=50, early_stopping_rounds = 50)\n\nfeature_imp = pd.DataFrame(sorted(zip(clf.feature_importance(),features)), columns=['Value','Feature'])\n\nplt.figure(figsize=(20, 20))\nsns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False).head(100))\nplt.title('LightGBM Features')\nplt.tight_layout()\nplt.show()\nplt.savefig('lgbm_importances-02.png')", "processed": ["seem tempor featur disper two model may surpris time seri problem", "repeat procedur drop week featur"]}, {"markdown": ["Well, the AUC hardly changed. Let's see what happens if we leave out 'month' as well."], "code": "# Reference: https://www.kaggle.com/code/tunguz/m5-adversarial-validation\n\n%%time\ndel train, test, clf\n\ngc.collect()\ngc.collect()\n\nfeatures = ['item_id', 'dept_id', 'store_id', 'cat_id', 'state_id', 'wday',\n       'year', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2',\n       'snap_CA', 'snap_TX', 'snap_WI', 'sell_price', 'lag_7', 'lag_28',\n       'rmean_7_7', 'rmean_28_7', 'rmean_7_28', 'rmean_28_28',\n       'quarter', 'mday']\n\ntest = pd.read_csv('../input/best-features-only/X_test.csv', usecols=features)\ntrain = pd.read_csv('../input/best-features-only/X_train.csv', usecols=features)\n\ntrain = pd.concat([train, test], axis =0)\ndel test\ngc.collect()\n\ntarget = np.hstack([np.zeros(trs,), np.ones(tes,)])\n\ntrain, test, y_train, y_test = model_selection.train_test_split(train, target, test_size=0.33, random_state=42, shuffle=True)\ndel target\ngc.collect()\n\ntrain = lgb.Dataset(train, label=y_train)\ntest = lgb.Dataset(test, label=y_test)\n\nclf = lgb.train(param, train, num_round, valid_sets = [train, test], verbose_eval=50, early_stopping_rounds = 50)\n\nfeature_imp = pd.DataFrame(sorted(zip(clf.feature_importance(),features)), columns=['Value','Feature'])\n\nplt.figure(figsize=(20, 20))\nsns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False).head(100))\nplt.title('LightGBM Features')\nplt.tight_layout()\nplt.show()\nplt.savefig('lgbm_importances-03.png')", "processed": ["well auc hardli chang let see happen leav month well"]}, {"markdown": ["I begin with finding the optimal learning rate. The following function runs training with different lr and records the loss. Increase of the loss indicates onset of divergence of training. The optimal lr lies in the vicinity of the minimum of the curve but before the onset of divergence. Based on the following plot, for the current setup the divergence starts at ~0.01, and the recommended learning rate is ~0.002.", "I'm going to load a model trained in the original version of the kernel, so I comment lines corresponding to training. Please see the original kernel for more details."], "code": "# Reference: https://www.kaggle.com/code/iafoss/fine-tuning-resnet34-on-ship-detection-new-data\n\n#learn.lr_find()\n#learn.sched.plot()", "processed": ["begin find optim learn rate follow function run train differ lr record loss increas loss indic onset diverg train optim lr lie vicin minimum curv onset diverg base follow plot current setup diverg start 0 01 recommend learn rate 0 002", "go load model train origin version kernel comment line correspond train plea see origin kernel detail"]}, {"markdown": ["## Model training"], "code": "# Reference: https://www.kaggle.com/code/artgor/dcgan-baseline\n\n# Training Loop\n\n# Lists to keep track of progress\n\nG_losses = []\nD_losses = []\niters = 0\n\nvalid_loss_min = np.Inf\npatience = 5\n# current number of epochs, where validation loss didn't increase\np = 0\n# whether training should be stopped\nstop = False\n\n\nprint(\"Starting Training Loop...\")\n# For each epoch\nfor epoch in range(num_epochs):\n    # For each batch in the dataloader\n    for i, data in enumerate(dataloader, 0):\n\n        ## Train with all-real batch\n        netD.zero_grad()\n        # Format batch\n        real_cpu = data[0].to(device)\n        b_size = real_cpu.size(0)\n        label = torch.full((b_size,), real_label, device=device)\n        # Forward pass real batch through D\n        output = netD(real_cpu).view(-1)\n        # Calculate loss on all-real batch\n        errD_real = criterion(output, label)\n        # Calculate gradients for D in backward pass\n        errD_real.backward()\n        D_x = output.mean().item()\n\n        ## Train with all-fake batch\n        # Generate batch of latent vectors\n        noise = torch.randn(b_size, nz, 1, 1, device=device)\n        # Generate fake image batch with G\n        fake = netG(noise)\n        label.fill_(fake_label)\n        # Classify all fake batch with D\n        output = netD(fake.detach()).view(-1)\n        # Calculate D's loss on the all-fake batch\n        errD_fake = criterion(output, label)\n        # Calculate the gradients for this batch\n        errD_fake.backward()\n        D_G_z1 = output.mean().item()\n        # Add the gradients from the all-real and all-fake batches\n        errD = errD_real + errD_fake\n        # Update D\n        optimizerD.step()\n\n        ############################\n        # (2) Update G network: maximize log(D(G(z)))\n        ###########################\n        netG.zero_grad()\n        label.fill_(real_label)  # fake labels are real for generator cost\n        # Since we just updated D, perform another forward pass of all-fake batch through D\n        output = netD(fake).view(-1)\n        # Calculate G's loss based on this output\n        errG = criterion(output, label)\n        # Calculate gradients for G\n        errG.backward()\n        D_G_z2 = output.mean().item()\n        # Update G\n        optimizerG.step()\n\n        # Save Losses for plotting later\n        G_losses.append(errG.item())\n        D_losses.append(errD.item())\n\n        iters += 1\n    scheduler.step(errD.item())\n    \n    print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n          % (epoch, num_epochs, i, len(dataloader),\n             errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n    \n#     loss = errG.item()\n#     if loss <= valid_loss_min:\n#         print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n#         valid_loss_min,\n#         loss))\n#         torch.save(netD.state_dict(), 'model.pt')\n#         valid_loss_min = loss\n#         p = 0\n\n#     # check if validation loss didn't improve\n#     if loss > valid_loss_min:\n#         p += 1\n#         print(f'{p} epochs of increasing loss')\n#         if p > patience:\n#             print('Stopping training')\n#             stop = True\n#             break        \n            \n#     if stop:\n#         break\nplt.figure(figsize=(10,5))\nplt.title(\"Generator and Discriminator Loss During Training\")\nplt.plot(G_losses,label=\"G\")\nplt.plot(D_losses,label=\"D\")\nplt.xlabel(\"iterations\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.show()", "processed": ["model train"]}, {"markdown": ["# Conditional Dog Breed GAN\nThis kernel is a A-C-Ra-LS-DC-GAN. Whoa that's a lot of letters! The A if for [Auxiliary Classifier][8]. The C is for [Conditional GAN][1]. The Ra is for [Relativistic Average GAN][2]. The LS is for [Least Squares GAN][3]. The DC is for [Deep Convolutional GAN][4]!\n![image](http://playagricola.com/Kaggle/GAN2.jpg)\n\nConditional GANs are fun! When we train our GAN, we can associate each image (and seed) with one or more labels (classes). Afterwards, we can request our Generator to draw a dog with certain labels. For example, we can label every training image as either \"facing left\", \"facing center\", or \"facing right\". This is categorical feature one. Next we label every training image as either \"short hair\", \"long hair\", or \"no hair\". This is categorical feature two. Then we can ask our Generator to draw a dog that is \"facing left\" and has \"long hair\". Fun, right?!\n\nIn this kernel, we will use one categorial feature, namely breed. After training our GAN, we can ask our Generator to draw a specific breed of dog! Keep in mind that this kernel is a **work in progress**. The GAN architecture and/or hyperparameters are not neccessarily optimal. Similar to most of you, I'm learning this stuff too. I encourage everyone to fork this kernel and improve it.\n# UPDATE v17\nKernel version 16 scores LB 100. This version scores LB 52. In addition to small changes, the following big changes were made:\n* Crop original images with square inside bounding box plus padding (80x80)\n* Use data augmentation, random crops (64x64)\n* Use dense layer of 121 sigmoid units before output unit\n* Compute classification error on dense layer and add to discriminator's loss\n* Compile training loop as `tf.function` for 2x speedup\n\n# Load and Crop Images\n\n[1]: https://arxiv.org/abs/1411.1784\n[2]: https://arxiv.org/pdf/1807.00734.pdf\n[3]: https://arxiv.org/abs/1611.04076\n[4]: https://arxiv.org/abs/1511.06434\n[5]: https://www.kaggle.com/c/generative-dog-images/discussion/99215\n[6]: https://www.kaggle.com/c/generative-dog-images/discussion/99485\n[7]: https://www.kaggle.com/cdeotte/dog-memorizer-gan\n[8]: https://arxiv.org/abs/1610.09585"], "code": "import numpy as np, pandas as pd, os, time, gc\nimport xml.etree.ElementTree as ET , time\nimport matplotlib.pyplot as plt, zipfile \nfrom PIL import Image \n\n# STOP KERNEL IF IT RUNS FOR MORE THAN 8 HOURS\nkernel_start = time.time()\nLIMIT = 8\n\n# PREPROCESS IMAGES\nComputeLB = True\nDogsOnly = True\nROOT = '../input/generative-dog-images/'\nif not ComputeLB: ROOT = '../input/'\nIMAGES = os.listdir(ROOT + 'all-dogs/all-dogs/')\nbreeds = os.listdir(ROOT + 'annotation/Annotation/') \n\nidxIn = 0; namesIn = []\nimagesIn = np.zeros((25000,80,80,3))\n\n# CROP WITH BOUNDING BOXES TO GET DOGS ONLY\n# https://www.kaggle.com/paulorzp/show-annotations-and-breeds\nif DogsOnly:\n    for breed in breeds:\n        for dog in os.listdir(ROOT+'annotation/Annotation/'+breed):\n            try: img = Image.open(ROOT+'all-dogs/all-dogs/'+dog+'.jpg') \n            except: continue           \n            ww,hh = img.size\n            tree = ET.parse(ROOT+'annotation/Annotation/'+breed+'/'+dog)\n            root = tree.getroot()\n            objects = root.findall('object')\n            for o in objects:\n                bndbox = o.find('bndbox') \n                xmin = int(bndbox.find('xmin').text)\n                ymin = int(bndbox.find('ymin').text)\n                xmax = int(bndbox.find('xmax').text)\n                ymax = int(bndbox.find('ymax').text)\n                w = np.min((xmax - xmin, ymax - ymin))\n                # ADD PADDING TO CROPS\n                EXTRA = w//8\n                a1 = EXTRA; a2 = EXTRA; b1 = EXTRA; b2 = EXTRA\n                a1 = np.min((a1,xmin)); a2 = np.min((a2,ww-xmin-w))\n                b1 = np.min((b1,ymin)); b2 = np.min((b2,hh-ymin-w))\n                img2 = img.crop((xmin-a1, ymin-b1, xmin+w+a2, ymin+w+b2))\n                img2 = img2.resize((80,80), Image.ANTIALIAS)\n                imagesIn[idxIn,:,:,:] = np.asarray(img2)\n                namesIn.append(breed)                \n                #if idxIn%1000==0: print(idxIn)\n                idxIn += 1\n                \n    idx = np.arange(idxIn)\n    np.random.shuffle(idx)\n    imagesIn = imagesIn[idx,:,:,:]\n    namesIn = np.array(namesIn)[idx]\n    \n# RANDOMLY CROP FULL IMAGES\nelse:\n    for k in range(len(IMAGES)):\n        img = Image.open(ROOT + 'all-dogs/all-dogs/' + IMAGES[k])\n        w = img.size[0]\n        h = img.size[1]\n        sz = np.min((w,h))\n        a=0; b=0\n        if w<h: b = (h-sz)//2\n        else: a = (w-sz)//2\n        img = img.crop((0+a, 0+b, sz+a, sz+b))  \n        img = img.resize((64,64), Image.ANTIALIAS)   \n        imagesIn[idxIn,:,:,:] = np.asarray(img)\n        namesIn.append(IMAGES[k])               \n        #if (idxIn%1000==0): print(idxIn)\n        idxIn += 1 \n    \n# DISPLAY CROPPED IMAGES\nx = np.random.randint(0,idxIn,25)\nfor k in range(3):\n    plt.figure(figsize=(15,3))\n    for j in range(5):\n        plt.subplot(1,5,j+1)\n        img = Image.fromarray( imagesIn[x[k*5+j],:,:,:].astype('uint8') )\n        plt.axis('off')\n        if not DogsOnly: plt.title(namesIn[x[k*5+j]],fontsize=11)\n        else: plt.title(namesIn[x[k*5+j]].split('-')[1],fontsize=11)\n        plt.imshow(img)\n    plt.show()", "processed": ["condit dog breed gan kernel c ra l dc gan whoa lot letter auxiliari classifi 8 c condit gan 1 ra relativist averag gan 2 l least squar gan 3 dc deep convolut gan 4 imag http playagricola com kaggl gan2 jpg condit gan fun train gan associ imag seed one label class afterward request gener draw dog certain label exampl label everi train imag either face left face center face right categor featur one next label everi train imag either short hair long hair hair categor featur two ask gener draw dog face left long hair fun right kernel use one categori featur name breed train gan ask gener draw specif breed dog keep mind kernel work progress gan architectur hyperparamet neccessarili optim similar learn stuff encourag everyon fork kernel improv updat v17 kernel version 16 score lb 100 version score lb 52 addit small chang follow big chang made crop origin imag squar insid bound box plu pad 80x80 use data augment random crop 64x64 use den layer 121 sigmoid unit output unit comput classif error den layer add discrimin loss compil train loop tf function 2x speedup load crop imag 1 http arxiv org ab 1411 1784 2 http arxiv org pdf 1807 00734 pdf 3 http arxiv org ab 1611 04076 4 http arxiv org ab 1511 06434 5 http www kaggl com c gener dog imag discus 99215 6 http www kaggl com c gener dog imag discus 99485 7 http www kaggl com cdeott dog memor gan 8 http arxiv org ab 1610 09585"]}, {"markdown": ["# Training Loop"], "code": "# Reference: https://www.kaggle.com/code/cdeotte/dog-breed-acgan-lb-52\n\nDISPLAY_EVERY = 10\n\ndef display_images(model, test_input, labs):\n    predictions = model([test_input,labs], training=False)\n    fig = plt.figure(figsize=(16,4))\n    for i in range(predictions.shape[0]):\n        plt.subplot(2, 8, i+1)\n        plt.imshow( (predictions[i, :, :, :]+1.)/2. )\n        plt.axis('off')\n    plt.show()\n    \ndef generate_latent_points(latent_dim, n_samples):\n    return tf.random.truncated_normal((n_samples,latent_dim))\n\ndef train(dataset, epochs):\n    all_gl = np.array([]); all_dl = np.array([])\n    \n    for epoch in range(epochs):\n        start = time.time()\n        gl = []; dl = []\n           \n        # TENSOR FLOW DATA.DATASET HAS A BUG AND WONT SHUFFLE ON ITS OWN :-(\n        # https://github.com/tensorflow/tensorflow/issues/27680\n        idx = np.arange(idxIn)\n        np.random.shuffle(idx)\n        dataset = (tf.data.Dataset.from_tensor_slices((imagesIn[idx,:,:,:],namesIn[idx]))\n            .map(flip).map(crop).batch(BATCH_SIZE,drop_remainder=True))\n        \n        # TRAIN ACGAN\n        for i,image_batch in enumerate(dataset):\n            gg,dd = train_step(image_batch,generator,discriminator,\n                        generator_optimizer, discriminator_optimizer)\n            gl.append(gg); dl.append(dd)\n        all_gl = np.append(all_gl,np.array([gl]))\n        all_dl = np.append(all_dl,np.array([dl]))\n        \n        # EXPONENTIALLY DECAY LEARNING RATES\n        if epoch>180: learning_rate.assign(learning_rate*0.95)\n        \n        # DISPLAY PROGRESS\n        if epoch%DISPLAY_EVERY==0:\n            # PLOT EPOCH LOSS\n            plt.figure(figsize=(16,2))\n            plt.plot(np.arange(len(gl)),gl,label='Gen_loss')\n            plt.plot(np.arange(len(dl)),dl,label='Disc_loss')\n            plt.legend()\n            plt.title('Epoch '+str(epoch)+' Loss')\n            ymax = plt.ylim()[1]\n            plt.show()\n            \n            # PLOT ALL TIME LOSS\n            plt.figure(figsize=(16,2))\n            plt.plot(np.arange(len(all_gl)),all_gl,label='Gen_loss')\n            plt.plot(np.arange(len(all_dl)),all_dl,label='Disc_loss')\n            plt.legend()\n            plt.ylim((0,np.min([1.1*np.max(all_gl),2*ymax])))\n            plt.title('All Time Loss')\n            plt.show()\n\n            # DISPLAY IMAGES FROM TRAIN PROGRESS\n            seed = generate_latent_points(noise_dim, num_examples)\n            labs = tf.cast(120*tf.random.uniform((num_examples,1)),tf.int8)\n            display_images(generator, seed, labs)\n            \n            # PRINT STATS\n            print('EPOCH',epoch,'took',np.round(time.time()-start,1),'sec')\n            print('Gen_loss mean=',np.mean(gl),'std=',np.std(gl))\n            print('Disc_loss mean=',np.mean(dl),'std=',np.std(dl))\n            print('Learning rate = ',end='')\n            tf.print(discriminator_optimizer._lr)\n            \n        x = gc.collect()\n        tt = np.round( (time.time() - kernel_start)/60,1 )\n        if tt > LIMIT*60: break\nEPOCHS = 250\nnum_examples = 16\n\n@ tf.function\ndef train_step(images,generator,discriminator,generator_optimizer,discriminator_optimizer):\n        \n    bce = tf.keras.losses.BinaryCrossentropy(from_logits=True,label_smoothing=0.4)\n    bce2 = tf.keras.losses.BinaryCrossentropy(from_logits=False,label_smoothing=0.4)\n    noise = tf.random.normal((32,128)) # update noise_dim here\n    labs = tf.cast(120*tf.random.uniform((32,)),tf.int32)\n    \n    # USE GRADIENT TAPE TO CALCULATE GRADIENTS\n    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:       \n        generated_images = generator([noise,labs], training=True)\n        real_cat, real_output = discriminator([images[0],images[1]], training=True)\n        fake_cat, fake_output = discriminator([generated_images,labs], training=True)\n    \n        # GENERATOR LOSS \n        gen_loss = (tf.reduce_mean( (real_output - tf.reduce_mean(fake_output,0) + tf.ones_like(real_output))**2,0 )\n        + tf.reduce_mean( (fake_output - tf.reduce_mean(real_output,0) - tf.ones_like(real_output))**2,0 ) )/2.\n        \n        # DISCRIMINATOR LOSS\n        disc_loss = bce(tf.ones_like(real_output), real_output) + bce(tf.zeros_like(fake_output), fake_output)           \n        real_cat2 = tf.one_hot(tf.cast(images[1],tf.int32),121,dtype=tf.int32)\n        fake_cat2 = tf.one_hot(120*tf.ones((32,),tf.int32),121,dtype=tf.int32)\n        disc_loss += bce2(real_cat2,real_cat) + bce2(fake_cat2,fake_cat) \n        \n    # BACK PROPAGATE ERROR\n    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n       \n    return gen_loss, disc_loss\n\nprint('Training started. Displaying every '+str(DISPLAY_EVERY)+'th epoch.')\ntrain(ds, EPOCHS)", "processed": ["train loop"]}, {"markdown": ["# Display Generated Dog Breeds\nFor each breed, we will ask our Generator to draw us 10 dog pictures of that breed by feeding into our Generator both 10 random seeds `Z` of length 100 and a breed number `C` from 0 to 119 inclusive. Because this CGAN isn't optimal yet, some breeds may only output 1 image repeatedly. This is called `Mode Collapse`. Below we won't display the breeds with mode collapse. For each breed below we display one row of real pictures above two row of fake pictures. Pretty cool, huh?!"], "code": "# Reference: https://www.kaggle.com/code/cdeotte/dog-breed-acgan-lb-52\n\nmse = tf.keras.losses.MeanSquaredError()\n\nprint('Display Random Dogs by Breed')\nprint()\nfor j in np.random.randint(0,120,25):\n    # GENERATE DOGS\n    seed = generate_latent_points(noise_dim, 10)\n    labs = tf.cast( j*np.ones((10,1)), tf.int8)\n    predictions = generator([seed,labs], training=False); d = 0   \n    # GET BREED NAME    \n    br = np.argwhere( namesIn==j ).flatten()\n    bd = le.inverse_transform(np.array([j]))[0].capitalize()\n    # CALCULATE VARIETY\n    for k in range(4): d += mse(predictions[k,:,:,:],predictions[k+1,:,:,:]) \n    d = np.round( np.array(d),1 )\n    if d<1.0: \n        print(bd,'had mode collapse. No display. (variety =',d,')')\n        continue\n    # DISPLAY DOGS\n    print(bd,'REAL DOGS on top. FAKE DOGS on bottom. (variety =',d,')')\n    plt.figure(figsize=(15,9))\n    for i in range(5):\n        plt.subplot(3,5,i+1)\n        plt.imshow( (imagesIn[br[i],:,:,:]+1.)/2. )\n        plt.axis('off')\n    for i in range(10):\n        plt.subplot(3,5,i+6)\n        plt.imshow( (predictions[i,:,:,:]+1.)/2. )\n        plt.axis('off')\n    plt.show()", "processed": ["display gener dog breed breed ask gener draw u 10 dog pictur breed feed gener 10 random seed z length 100 breed number c 0 119 inclus cgan optim yet breed may output 1 imag repeatedli call mode collaps display breed mode collaps breed display one row real pictur two row fake pictur pretti cool huh"]}, {"markdown": ["# Submit to Kaggle\nWe will ask our Generative Network to draw 10000 random dog images from whatever random breeds. Alternatively we could ask for specifically 83 of each breed to guarentee variety and increased FID score."], "code": "# Reference: https://www.kaggle.com/code/cdeotte/dog-breed-acgan-lb-52\n\nseed = generate_latent_points(noise_dim, 100)\nlabs = tf.cast(120*tf.random.uniform((100,1)),tf.int8)\npredictions = generator([seed,labs], training=False)\nplt.figure(figsize=(20,20))\nplt.subplots_adjust(wspace=0,hspace=0)\nfor k in range(100):\n    plt.subplot(10,10,k+1)\n    plt.imshow( (predictions[k,:,:,:]+1.)/2. )\n    plt.axis('off')\nplt.show()\n# SUBMIT 84 IMAGES OF EACH OF 119 BREEDS and 4 of breed 120\nz = zipfile.PyZipFile('images.zip', mode='w')\nfor i in range(120):\n    ct = 84\n    if i==119: ct=4\n    seed = generate_latent_points(noise_dim, ct)\n    labs = tf.cast( i*np.ones((ct,1)), tf.int8)\n    predictions = generator([seed,labs], training=False)\n    predictions = 255*((np.array(predictions)+1.)/2.)\n    for j in range(ct):\n        img = Image.fromarray( predictions[j,:,:,:].astype('uint8') )\n        f = str(i*84+j)+'.png'\n        img.save(f,'PNG'); z.write(f); os.remove(f)\n    #if (i%10==0)|(i==119): print(i*84)\nz.close()", "processed": ["submit kaggl ask gener network draw 10000 random dog imag whatev random breed altern could ask specif 83 breed guarente varieti increas fid score"]}, {"markdown": ["## Evaluation", "Unhide below to see helper function `display_training_curves`:"], "code": "# Reference: https://www.kaggle.com/code/xhlulu/alaska2-efficientnet-on-tpus\n\ndef display_training_curves(training, validation, title, subplot):\n    \"\"\"\n    Source: https://www.kaggle.com/mgornergoogle/getting-started-with-100-flowers-on-tpu\n    \"\"\"\n    if subplot%10==1: # set up the subplots on the first call\n        plt.subplots(figsize=(10,10), facecolor='#F0F0F0')\n        plt.tight_layout()\n    ax = plt.subplot(subplot)\n    ax.set_facecolor('#F8F8F8')\n    ax.plot(training)\n    ax.plot(validation)\n    ax.set_title('model '+ title)\n    ax.set_ylabel(title)\n    #ax.set_ylim(0.28,1.05)\n    ax.set_xlabel('epoch')\n    ax.legend(['train', 'valid.'])\ndisplay_training_curves(\n    history.history['loss'], \n    history.history['val_loss'], \n    'loss', 211)\ndisplay_training_curves(\n    history.history['accuracy'], \n    history.history['val_accuracy'], \n    'accuracy', 212)", "processed": ["evalu", "unhid see helper function display train curv"]}, {"markdown": ["## **4.3 Pair Plot**  <a class=\"anchor\" id=\"4.3\"></a> \n\n[Table of Contents](#0.1)\n\n\nWe will draw pairplot to visualize relationship between the target variables. "], "code": "# Reference: https://www.kaggle.com/code/prashant111/trends-insights-1st-submission\n\nsns.pairplot(train_df[cols], kind='scatter', diag_kind='hist', palette='Rainbow')\nplt.show()", "processed": ["4 3 pair plot class anchor id 4 3 tabl content 0 1 draw pairplot visual relationship target variabl"]}, {"markdown": ["Using KFold with shuffle set to True I would expect target to be mixed up in both training and validation sets. So let's try this and plot the obtained target data."], "code": "# Reference: https://www.kaggle.com/code/ogrellier/kfold-or-stratifiedkfold\n\n# Create folds\nfolds = KFold(n_splits=3, shuffle=True, random_state = 45)\n# Go through folds\nplt.figure(figsize=(10,10))\nfor trn_idx, val_idx in folds.split(target, target):\n    # Stack training target and validation target and plot them\n    plt.plot(np.hstack((target.iloc[trn_idx].values, target.iloc[val_idx].values)))\nplt.title(\"KFold Shuffle=True ?\")", "processed": ["use kfold shuffl set true would expect target mix train valid set let tri plot obtain target data"]}, {"markdown": ["So in fact zeros and ones are absolutely not mixed. The original sortation is kept!\n\nTo understand what KFold exactly does I will use a simple index and display and 2-fold KFold with and without shuffling\n"], "code": "# Reference: https://www.kaggle.com/code/ogrellier/kfold-or-stratifiedkfold\n\nplt.figure(figsize=(15,7))\nfrom matplotlib.gridspec import GridSpec\ngs = GridSpec(1, 2)\nax1 = plt.subplot(gs[0, 0])\n# Create folds\nkfolds = KFold(n_splits=2, shuffle=False, random_state=2645312378)\n# Go through folds\nidx = np.arange(len(target))\nfor trn_idx, val_idx in kfolds.split(trn.values):\n    # Stack training target and validation target and plot them\n    ax1.plot(np.hstack((idx[trn_idx], idx[val_idx])))\nax1.set_title(\"KFold Shuffle=False\")\n\nax2 = plt.subplot(gs[0, 1])\n# Create folds\nkfolds = KFold(n_splits=2, shuffle=True, random_state=2645312378)\n# Go through folds\nidx = np.arange(len(target))\nfor I, (trn_idx, val_idx) in enumerate(kfolds.split(trn.values)):\n    # Stack training target and validation target and plot them\n    ax2.plot(np.hstack((idx[trn_idx], idx[val_idx])) + 20000 * I)\nax2.set_title(\"KFold Shuffle=False\")", "processed": ["fact zero one absolut mix origin sortat kept understand kfold exactli use simpl index display 2 fold kfold without shuffl"]}, {"markdown": ["On the right hand plot,  the second split has been shifted to show both splits are printed. \n\nWhen shuffle is False the plot is as expected. On the first split train and validation indexes stay in place, in the second split the second half of the indices come first and the first half is used for validation.\n\nWhen shuffle is True, indices of training and validation folds are taken from the full range of indices but they remain in ascending order. So it won't change the order in which samples appear...\n\nLet's see if StratifiedKFold has a different behaviour."], "code": "# Reference: https://www.kaggle.com/code/ogrellier/kfold-or-stratifiedkfold\n\n# Create folds\nplt.figure(figsize=(10,10))\nfolds = StratifiedKFold(n_splits=3, shuffle=True, random_state = 5)\n# Go through folds\nfor trn_idx, val_idx in folds.split(target, target):\n    # Stack training target and validation target and plot them\n    plt.plot(np.hstack((target.iloc[trn_idx].values, target.iloc[val_idx].values)))\nplt.title(\"StratifiedKFold Shuffle=True ?\")", "processed": ["right hand plot second split shift show split print shuffl fals plot expect first split train valid index stay place second split second half indic come first first half use valid shuffl true indic train valid fold taken full rang indic remain ascend order chang order sampl appear let see stratifiedkfold differ behaviour"]}, {"markdown": ["Again zeros and ones are not mixed up !\n\nIn short, if you want the original sample sortation being mixed up you have to do something like this :"], "code": "# Reference: https://www.kaggle.com/code/ogrellier/kfold-or-stratifiedkfold\n\nidx = target.index.values\nnp.random.shuffle(idx)\nfolds = StratifiedKFold(n_splits=3, shuffle=True, random_state = 5)\n# Go through folds\nplt.figure(figsize=(10,10))\nfor trn_idx, val_idx in folds.split(target, target):\n    # Stack training target and validation target and plot them\n    plt.plot(np.hstack((target.loc[idx[trn_idx]].values, \n                        target.loc[idx[val_idx]].values)))\nplt.title(\"StratifiedKFold Shuffle=True ?\")", "processed": ["zero one mix short want origin sampl sortat mix someth like"]}, {"markdown": ["From the above graph, we can clearly see that the number distribution has a string positive skew. Most numbers in the matrices are clearly 0. This is reflected by the dominance of black color in most matrices.", "## Matrix mean values <a id=\"matrix-mean-values\"></a>"], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/arc-competition-eda-pytorch-cnn\n\nmeans = [np.mean(X) for X in matrices]\nfig = ff.create_distplot([means], group_labels=[\"Means\"], colors=[\"green\"])\nfig.update_layout(title_text=\"Distribution of matrix mean values\")", "processed": ["graph clearli see number distribut string posit skew number matric clearli 0 reflect domin black color matric", "matrix mean valu id matrix mean valu"]}, {"markdown": ["From the above graph, we can see that lower means are more common than higher means. The graph, once again, has a strong positive skew. This is further proof that black is the most dominant color in the matrices.", "## Matrix heights <a id=\"matrix-heights\"></a>"], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/arc-competition-eda-pytorch-cnn\n\nheights = [np.shape(matrix)[0] for matrix in matrices]\nwidths = [np.shape(matrix)[1] for matrix in matrices]\nfig = ff.create_distplot([heights], group_labels=[\"Height\"], colors=[\"magenta\"])\nfig.update_layout(title_text=\"Distribution of matrix heights\")", "processed": ["graph see lower mean common higher mean graph strong posit skew proof black domin color matric", "matrix height id matrix height"]}, {"markdown": ["From the above graph, we can see that matrix heights have a much more uniform distribution (with significantly less skew). The distribution is somewhat normal with a mean of approximately 15.", "## Matrix widths <a id=\"matrix-widths\"></a>"], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/arc-competition-eda-pytorch-cnn\n\nfig = ff.create_distplot([widths], group_labels=[\"Width\"], colors=[\"red\"])\nfig.update_layout(title_text=\"Distribution of matrix widths\")", "processed": ["graph see matrix height much uniform distribut significantli le skew distribut somewhat normal mean approxim 15", "matrix width id matrix width"]}, {"markdown": ["From the above graph, we can see that matrix widths also have a uniform distribution (with significantly less skew). The distribution is also somewhat uniform with a mean of approximately 16.", "## Height vs. Width <a id=\"height-vs-width\"></a>"], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/arc-competition-eda-pytorch-cnn\n\nplot = sns.jointplot(widths, heights, kind=\"kde\", color=\"blueviolet\")\nplot.set_axis_labels(\"Width\", \"Height\", fontsize=14)\nplt.show(plot)\nplot = sns.jointplot(widths, heights, kind=\"reg\", color=\"blueviolet\")\nplot.set_axis_labels(\"Width\", \"Height\", fontsize=14)\nplt.show(plot)", "processed": ["graph see matrix width also uniform distribut significantli le skew distribut also somewhat uniform mean approxim 16", "height v width id height v width"]}, {"markdown": ["So there are 8 folders present inside the train folder, one for each species.\n\nNow let us check the number of files present in each of these sub folders. "], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-notebook-nda-compliant\n\nsub_folders = check_output([\"ls\", \"../input/train/\"]).decode(\"utf8\").strip().split('\\n')\ncount_dict = {}\nfor sub_folder in sub_folders:\n    num_of_files = len(check_output([\"ls\", \"../input/train/\"+sub_folder]).decode(\"utf8\").strip().split('\\n'))\n    print(\"Number of files for the species\",sub_folder,\":\",num_of_files)\n    count_dict[sub_folder] = num_of_files\n    \nplt.figure(figsize=(12,4))\nsns.barplot(list(count_dict.keys()), list(count_dict.values()), alpha=0.8)\nplt.xlabel('Fish Species', fontsize=12)\nplt.ylabel('Number of Images', fontsize=12)\nplt.show()\n    ", "processed": ["8 folder present insid train folder one speci let u check number file present sub folder"]}, {"markdown": ["**Image Size:**\n\nNow let us look at the image size of each of the files and see what different sizes are available."], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-notebook-nda-compliant\n\ntrain_path = \"../input/train/\"\nsub_folders = check_output([\"ls\", train_path]).decode(\"utf8\").strip().split('\\n')\ndifferent_file_sizes = {}\nfor sub_folder in sub_folders:\n    file_names = check_output([\"ls\", train_path+sub_folder]).decode(\"utf8\").strip().split('\\n')\n    for file_name in file_names:\n        im_array = imread(train_path+sub_folder+\"/\"+file_name)\n        size = \"_\".join(map(str,list(im_array.shape)))\n        different_file_sizes[size] = different_file_sizes.get(size,0) + 1\n\nplt.figure(figsize=(12,4))\nsns.barplot(list(different_file_sizes.keys()), list(different_file_sizes.values()), alpha=0.8)\nplt.xlabel('Image size', fontsize=12)\nplt.ylabel('Number of Images', fontsize=12)\nplt.title(\"Image size present in train dataset\")\nplt.xticks(rotation='vertical')\nplt.show()", "processed": ["imag size let u look imag size file see differ size avail"]}, {"markdown": ["So 720_1280_3 is the most common image size available in the train data and 10 different sizes are available. \n\n720_1244_3 is the smallest size of the available images in train set and 974_1732_3 is the largest one.\n\nNow let us look at the distribution in test dataset as well."], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-notebook-nda-compliant\n\ntest_path = \"../input/test_stg1/\"\nfile_names = check_output([\"ls\", test_path]).decode(\"utf8\").strip().split('\\n')\ndifferent_file_sizes = {}\nfor file_name in file_names:\n        size = \"_\".join(map(str,list(imread(test_path+file_name).shape)))\n        different_file_sizes[size] = different_file_sizes.get(size,0) + 1\n\nplt.figure(figsize=(12,4))\nsns.barplot(list(different_file_sizes.keys()), list(different_file_sizes.values()), alpha=0.8)\nplt.xlabel('File size', fontsize=12)\nplt.ylabel('Number of Images', fontsize=12)\nplt.xticks(rotation='vertical')\nplt.title(\"Image size present in test dataset\")\nplt.show()", "processed": ["720 1280 3 common imag size avail train data 10 differ size avail 720 1244 3 smallest size avail imag train set 974 1732 3 largest one let u look distribut test dataset well"]}, {"markdown": ["Let us see the count of occurrences of each of the customers in train set"], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-notebook-v3-0\n\nnum_occur = train.groupby('ncodpers').agg('size').value_counts()\n\nplt.figure(figsize=(8,4))\nsns.barplot(num_occur.index, num_occur.values, alpha=0.8, color=color[0])\nplt.xlabel('Number of Occurrences of the customer', fontsize=12)\nplt.ylabel('Number of customers', fontsize=12)\nplt.show()", "processed": ["let u see count occurr custom train set"]}, {"markdown": ["**Target Variables distribution:**\n\nThere are 24 target variables present in this dataset are as follows:\n\n1. ind_ahor_fin_ult1\t  - Saving Account\n\n2. ind_aval_fin_ult1\t  - Guarantees\n\n3. ind_cco_fin_ult1\t  - Current Accounts\n\n4. ind_cder_fin_ult1\t  - Derivada Account\n\n5. ind_cno_fin_ult1\t  - Payroll Account\n\n6. ind_ctju_fin_ult1\t  - Junior Account\n\n7. ind_ctma_fin_ult1 - M\u00e1s particular Account\n\n8. ind_ctop_fin_ult1 - particular Account\n\n9. ind_ctpp_fin_ult1 - particular Plus Account\n\n10. ind_deco_fin_ult1 - Short-term deposits\n\n11. ind_deme_fin_ult1 - Medium-term deposits\n\n12. ind_dela_fin_ult1 - Long-term deposits\n\n13. ind_ecue_fin_ult1 - e-account\n\n14. ind_fond_fin_ult1 - Funds\n\n15. ind_hip_fin_ult1 - Mortgage\n\n16. ind_plan_fin_ult1 - Pensions\n\n17. ind_pres_fin_ult1 - Loans\n\n18. ind_reca_fin_ult1 - Taxes\n\n19. ind_tjcr_fin_ult1 - Credit Card\n\n20. ind_valo_fin_ult1 - Securities\n\n21. ind_viv_fin_ult1 - Home Account\n\n22. ind_nomina_ult1 - Payroll\n\n23. ind_nom_pens_ult1 - Pensions\n\n24. ind_recibo_ult1 - Direct Debit\n\nLet us check the number of times the given product has been bought in the train dataset"], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-notebook-v3-0\n\ntrain = pd.read_csv(data_path+\"train_ver2.csv\", dtype='float16', \n                    usecols=['ind_ahor_fin_ult1', 'ind_aval_fin_ult1', \n                             'ind_cco_fin_ult1', 'ind_cder_fin_ult1',\n                             'ind_cno_fin_ult1', 'ind_ctju_fin_ult1',\n                             'ind_ctma_fin_ult1', 'ind_ctop_fin_ult1',\n                             'ind_ctpp_fin_ult1', 'ind_deco_fin_ult1',\n                             'ind_deme_fin_ult1', 'ind_dela_fin_ult1',\n                             'ind_ecue_fin_ult1', 'ind_fond_fin_ult1',\n                             'ind_hip_fin_ult1', 'ind_plan_fin_ult1',\n                             'ind_pres_fin_ult1', 'ind_reca_fin_ult1',\n                             'ind_tjcr_fin_ult1', 'ind_valo_fin_ult1',\n                             'ind_viv_fin_ult1', 'ind_nomina_ult1',\n                             'ind_nom_pens_ult1', 'ind_recibo_ult1'])\ntarget_counts = train.astype('float64').sum(axis=0)\n#print(target_counts)\nplt.figure(figsize=(8,4))\nsns.barplot(target_counts.index, target_counts.values, alpha=0.8, color=color[0])\nplt.xlabel('Product Name', fontsize=12)\nplt.ylabel('Number of Occurrences', fontsize=12)\nplt.xticks(rotation='vertical')\nplt.show()", "processed": ["target variabl distribut 24 target variabl present dataset follow 1 ind ahor fin ult1 save account 2 ind aval fin ult1 guarante 3 ind cco fin ult1 current account 4 ind cder fin ult1 derivada account 5 ind cno fin ult1 payrol account 6 ind ctju fin ult1 junior account 7 ind ctma fin ult1 m particular account 8 ind ctop fin ult1 particular account 9 ind ctpp fin ult1 particular plu account 10 ind deco fin ult1 short term deposit 11 ind deme fin ult1 medium term deposit 12 ind dela fin ult1 long term deposit 13 ind ecu fin ult1 e account 14 ind fond fin ult1 fund 15 ind hip fin ult1 mortgag 16 ind plan fin ult1 pension 17 ind pre fin ult1 loan 18 ind reca fin ult1 tax 19 ind tjcr fin ult1 credit card 20 ind valo fin ult1 secur 21 ind viv fin ult1 home account 22 ind nomina ult1 payrol 23 ind nom pen ult1 pension 24 ind recibo ult1 direct debit let u check number time given product bought train dataset"]}, {"markdown": ["Product \"ind_cco_fin_ult1 \" is the most bought one and \"ind_aval_fin_ult1\" is the least bought one.", "**Exploring Dates:**\n\nLet us explore the dates now and see if there are any insights. There are 2 date fields present in the data.\n\n1. fecha_dato - The date of observation\n2. fecha_alta - The date in which the customer became as the first holder of a contract in the bank"], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-notebook-v3-0\n\ntrain = pd.read_csv(data_path+\"train_ver2.csv\", usecols=['fecha_dato', 'fecha_alta'], parse_dates=['fecha_dato', 'fecha_alta'])\ntrain['fecha_dato_yearmonth'] = train['fecha_dato'].apply(lambda x: (100*x.year) + x.month)\nyearmonth = train['fecha_dato_yearmonth'].value_counts()\n\nplt.figure(figsize=(8,4))\nsns.barplot(yearmonth.index, yearmonth.values, alpha=0.8, color=color[0])\nplt.xlabel('Year and month of observation', fontsize=12)\nplt.ylabel('Number of customers', fontsize=12)\nplt.xticks(rotation='vertical')\nplt.show()", "processed": ["product ind cco fin ult1 bought one ind aval fin ult1 least bought one", "explor date let u explor date see insight 2 date field present data 1 fecha dato date observ 2 fecha alta date custom becam first holder contract bank"]}, {"markdown": ["For the first six months of the given train data, the number of customers / observations remain almost same and then there is a sudden spike in the number of customers / observations during July 2015."], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-notebook-v3-0\n\ntrain['fecha_alta_yearmonth'] = train['fecha_alta'].apply(lambda x: (100*x.year) + x.month)\nyearmonth = train['fecha_alta_yearmonth'].value_counts()\nprint(\"Minimum value of fetcha_alta : \", min(yearmonth.index))\nprint(\"Maximum value of fetcha_alta : \", max(yearmonth.index))\n\nplt.figure(figsize=(12,4))\nsns.barplot(yearmonth.index, yearmonth.values, alpha=0.8, color=color[1])\nplt.xlabel('Year and month of joining', fontsize=12)\nplt.ylabel('Number of customers', fontsize=12)\nplt.xticks(rotation='vertical')\nplt.show()", "processed": ["first six month given train data number custom observ remain almost sudden spike number custom observ juli 2015"]}, {"markdown": ["So the first holder date starts from January 1995. But as we can see, the number is high during the recent years.! \n\nAlso it seems there are some seasonal peaks in the data. Let us have a close look at them.!"], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-notebook-v3-0\n\nyear_month = yearmonth.sort_index().reset_index()\nyear_month = year_month.ix[185:]\nyear_month.columns = ['yearmonth', 'number_of_customers']\n\nplt.figure(figsize=(12,4))\nsns.barplot(year_month.yearmonth.astype('int'), year_month.number_of_customers, alpha=0.8, color=color[2])\nplt.xlabel('Year and month of joining', fontsize=12)\nplt.ylabel('Number of customers', fontsize=12)\nplt.xticks(rotation='vertical')\nplt.show()", "processed": ["first holder date start januari 1995 see number high recent year also seem season peak data let u close look"]}, {"markdown": ["We can now convert the field to dtype 'float' and then get the counts"], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-notebook-v3-0\n\ntrain['age'] = train['age'].astype('float64')\n\nage_series = train.age.value_counts()\nplt.figure(figsize=(12,4))\nsns.barplot(age_series.index.astype('int'), age_series.values, alpha=0.8)\nplt.ylabel('Number of Occurrences of the customer', fontsize=12)\nplt.xlabel('Age', fontsize=12)\nplt.xticks(rotation='vertical')\nplt.show()", "processed": ["convert field dtype float get count"]}, {"markdown": ["We have 27734 missing values and the mean age is 40. We could probably do a mean imputation here. \n\nWe could look at test set age distribution to confirm both train and test have same distribution."], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-notebook-v3-0\n\ntest = pd.read_csv(test_file, usecols=['age'])\ntest['age'] = test['age'].replace(to_replace=[' NA'], value=np.nan)\ntest['age'] = test['age'].astype('float64')\n\nage_series = test.age.value_counts()\nplt.figure(figsize=(12,4))\nsns.barplot(age_series.index.astype('int'), age_series.values, alpha=0.8)\nplt.ylabel('Number of Occurrences of the customer', fontsize=12)\nplt.xlabel('Age', fontsize=12)\nplt.xticks(rotation='vertical')\nplt.show()", "processed": ["27734 miss valu mean age 40 could probabl mean imput could look test set age distribut confirm train test distribut"]}, {"markdown": ["We have 38 special values. If we use a tree based model, we could probably leave it as such or if we use a linear model, we need to map it to mean or some value in the range of 0 to 256.\n\nNow we can see the distribution plot of this variable."], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-notebook-v3-0\n\ncol_series = train.antiguedad.value_counts()\nplt.figure(figsize=(12,4))\nsns.barplot(col_series.index.astype('int'), col_series.values, alpha=0.8)\nplt.ylabel('Number of Occurrences of the customer', fontsize=12)\nplt.xlabel('Customer Seniority', fontsize=12)\nplt.xticks(rotation='vertical')\nplt.show()", "processed": ["38 special valu use tree base model could probabl leav use linear model need map mean valu rang 0 256 see distribut plot variabl"]}, {"markdown": ["There are few peaks and troughs in the plot but there are no visible gaps or anything as such which is alarming (atleast to me.!)\n\nSo we can also see whether test follows a similar pattern and if it does then we are good."], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-notebook-v3-0\n\ntest = pd.read_csv(test_file, usecols=['antiguedad'])\ntest['antiguedad'] = test['antiguedad'].replace(to_replace=[' NA'], value=np.nan)\ntest['antiguedad'] = test['antiguedad'].astype('float64')\n\ncol_series = test.antiguedad.value_counts()\nplt.figure(figsize=(12,4))\nsns.barplot(col_series.index.astype('int'), col_series.values, alpha=0.8)\nplt.ylabel('Number of Occurrences of the customer', fontsize=12)\nplt.xlabel('Customer Seniority', fontsize=12)\nplt.xticks(rotation='vertical')\nplt.show()", "processed": ["peak trough plot visibl gap anyth alarm atleast also see whether test follow similar pattern good"]}, {"markdown": ["There are quite a few number of missing values present in this field.! We can do some form of imputation for the same. One very good idea is given by Alan in this [script][1].\n\nWe can check the quantile distribution to see how the value changes in the last percentile.\n\n\n  [1]: https://www.kaggle.com/apryor6/santander-product-recommendation/detailed-cleaning-visualization-python"], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-notebook-v3-0\n\ntrain.fillna(101850., inplace=True) #filling NA as median for now\nquantile_series = train.renta.quantile(np.arange(0.99,1,0.001))\nplt.figure(figsize=(12,4))\nsns.barplot((quantile_series.index*100), quantile_series.values, alpha=0.8)\nplt.ylabel('Rent value', fontsize=12)\nplt.xlabel('Quantile value', fontsize=12)\nplt.xticks(rotation='vertical')\nplt.show()", "processed": ["quit number miss valu present field form imput one good idea given alan script 1 check quantil distribut see valu chang last percentil 1 http www kaggl com apryor6 santand product recommend detail clean visual python"]}, {"markdown": ["As we can see there is a sudden increase in the rent value from 99.9% to 100%. So let us max cap the rent values at 99.9% and then get a box plot."], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-notebook-v3-0\n\nrent_max_cap = train.renta.quantile(0.999)\ntrain['renta'][train['renta']>rent_max_cap] = 101850.0 # assigining median value \nsns.boxplot(train.renta.values)\nplt.show()", "processed": ["see sudden increas rent valu 99 9 100 let u max cap rent valu 99 9 get box plot"]}, {"markdown": ["*Please note that there is a new value '   NA' present in the test data set while it is not in train data.*\n\nThe distribution looks similar to train though."], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-notebook-v3-0\n\ntest.renta.mean()\ntest.fillna(101850., inplace=True) #filling NA as median for now\nquantile_series = test.renta.quantile(np.arange(0.99,1,0.001))\nplt.figure(figsize=(12,4))\nsns.barplot((quantile_series.index*100), quantile_series.values, alpha=0.8)\nplt.ylabel('Rent value', fontsize=12)\nplt.xlabel('Quantile value', fontsize=12)\nplt.xticks(rotation='vertical')\nplt.show()\ntest['renta'][test['renta']>rent_max_cap] = 101850.0 # assigining median value \nsns.boxplot(test.renta.values)\nplt.show()", "processed": ["plea note new valu na present test data set train data distribut look similar train though"]}, {"markdown": ["So box and quantile plots are similar to that of the train dataset for rent.!\n\n**Numerical variables Vs Target variables:**\n\nNow let us see how the targets are distributed based on the numerical variables present in the data. Let us subset the first 100K rows for the same. "], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-notebook-v3-0\n\ntrain = pd.read_csv(data_path+\"train_ver2.csv\", nrows=100000)\ntarget_cols = ['ind_cco_fin_ult1', 'ind_cder_fin_ult1',\n                             'ind_cno_fin_ult1', 'ind_ctju_fin_ult1',\n                             'ind_ctma_fin_ult1', 'ind_ctop_fin_ult1',\n                             'ind_ctpp_fin_ult1', 'ind_deco_fin_ult1',\n                             'ind_deme_fin_ult1', 'ind_dela_fin_ult1',\n                             'ind_ecue_fin_ult1', 'ind_fond_fin_ult1',\n                             'ind_hip_fin_ult1', 'ind_plan_fin_ult1',\n                             'ind_pres_fin_ult1', 'ind_reca_fin_ult1',\n                             'ind_tjcr_fin_ult1', 'ind_valo_fin_ult1',\n                             'ind_viv_fin_ult1', 'ind_nomina_ult1',\n                             'ind_nom_pens_ult1', 'ind_recibo_ult1']\ntrain[target_cols] = (train[target_cols].fillna(0))\ntrain[\"age\"] = train['age'].map(str.strip).replace(['NA'], value=0).astype('float')\ntrain[\"antiguedad\"] = train[\"antiguedad\"].map(str.strip)\ntrain[\"antiguedad\"] = train['antiguedad'].replace(['NA'], value=0).astype('float')\ntrain[\"antiguedad\"].ix[train[\"antiguedad\"]>65] = 65 # there is one very high skewing the graph\ntrain[\"renta\"].ix[train[\"renta\"]>1e6] = 1e6 # capping the higher values for better visualisation\ntrain.fillna(-1, inplace=True)\nfig = plt.figure(figsize=(16, 120))\nnumeric_cols = ['age', 'antiguedad', 'renta']\n#for ind1, numeric_col in enumerate(numeric_cols):\nplot_count = 0\nfor ind, target_col in enumerate(target_cols):\n    for numeric_col in numeric_cols:\n        plot_count += 1\n        plt.subplot(22, 3, plot_count)\n        sns.boxplot(x=target_col, y=numeric_col, data=train)\n        plt.title(numeric_col+\" Vs \"+target_col)\nplt.show()", "processed": ["box quantil plot similar train dataset rent numer variabl v target variabl let u see target distribut base numer variabl present data let u subset first 100k row"]}, {"markdown": ["Seems all these numerical variables have some predictive power since they show some different behavior between 0's and 1's.\n\n**Exploring categorical fields:**\n\nNow let us look at the distribution of categorical fields present in the data by using the first 1 million rows."], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-notebook-v3-0\n\ncols = [\"ind_empleado\",\"pais_residencia\",\"sexo\",\"ind_nuevo\",\"indrel\",\"ult_fec_cli_1t\",\"indrel_1mes\",\"tiprel_1mes\",\"indresi\",\"indext\",\"conyuemp\",\"canal_entrada\",\"indfall\",\"tipodom\",\"cod_prov\",\"nomprov\",\"ind_actividad_cliente\",\"segmento\"]\nfor col in cols:\n    train = pd.read_csv(\"../input/train_ver2.csv\", usecols = [\"ncodpers\", col], nrows=1000000)\n    train = train.fillna(-99)\n    len_unique = len(train[col].unique())\n    print(\"Number of unique values in \",col,\" : \",len_unique)\n    if len_unique < 200:\n        agg_df = train[col].value_counts()\n        plt.figure(figsize=(12,6))\n        sns.barplot(agg_df.index, np.log1p(agg_df.values), alpha=0.8, color=color[0])\n        plt.xlabel(col, fontsize=12)\n        plt.ylabel('Log(Number of customers)', fontsize=12)\n        plt.xticks(rotation='vertical')\n        plt.show()\n    print()\n    \n       ", "processed": ["seem numer variabl predict power sinc show differ behavior 0 1 explor categor field let u look distribut categor field present data use first 1 million row"]}, {"markdown": ["## Target\nDifferent surfaces. Most common is concrete. Least common is hard tiles."], "code": "# Reference: https://www.kaggle.com/code/robikscube/navigate-robots-first-look-and-eda\n\ny_train['count'] = 1\ny_train.groupby('surface').sum()['count'] \\\n    .sort_values(ascending=True) \\\n    .plot(kind='barh', color='grey', figsize=(15, 5), title='Count of Surface Type')\nplt.show()", "processed": ["target differ surfac common concret least common hard tile"]}, {"markdown": ["# Test Full versus Train Full\nTo compare the distributions of TEST.csv and TRAIN.csv, I wrote a special Python function. (If interested, click the button 'see code' to view.) The visual may be confusing at first, so let me explain. Basically the plot is two histograms on top of each other. Instead of using bars, it uses a line which follows where the tops of the bars would be. The x-axis are the category variable's possible values. They have been ordered from most frequent in TRAIN to less frequent in TRAIN. Then values containing less than 0.1% of data are removed. Then they are relabeled 0, 1, 2, ... n.\n\nThe solid blue line is TRAIN and the solid green line is TEST. If the distributions are the same then the blue and green line would coincide. The dotted blue lines indicate 4x more than TRAIN and 4x less than TRAIN. Therefore if the green line crosses outside the dotted lines, then TEST has a value that is 4x more or 4x less than TRAIN. Let's plot `CountryIdentifier` for TRAIN.csv versus TEST.csv. "], "code": "# Reference: https://www.kaggle.com/code/cdeotte/time-split-validation-malware-0-68\n\n# COMPARE VALUE DENSITIES FROM TWO DIFFERENT DATAFRAMES\n#\n# PARAMETERS\n# df1: pandas.DataFrame containing variable\n# df2: pandas.DataFrame containing variable\n# col: column to compare between df1 and df2\n# override: set to False to prevent display when variables similar\n# verbose: display text summary\n# scale: zooms y-axis\n# title: plot title\n# lab1: legend label for df1\n# lab2: legend label for df2\n# prefix: pre text for verbose summary\n#\ndef comparePlot(df1, df2, col, factor=4, override=True, verbose=True, scale=0.5, title='',\n                lab1='', lab2='', prefix=''):\n    cv1 = pd.DataFrame(df1[col].value_counts(normalize=True).reset_index().rename({col:'train'},axis=1))\n    cv2 = pd.DataFrame(df2[col].value_counts(normalize=True).reset_index().rename({col:'test'},axis=1))\n    cv3 = pd.merge(cv1,cv2,on='index',how='outer')\n    cv3['train'].fillna(0,inplace=True)\n    cv3['test'].fillna(0,inplace=True)\n    cv3 = cv3.iloc[np.lexsort((cv3['test'], -cv3['train']))]\n    cv3['total'] = cv3['train']+cv3['test']\n    cv3['trainMX'] = cv3['train']*factor\n    cv3['trainMN'] = cv3['train']/factor\n    cv3 = cv3[cv3['total']>0.0001]\n    if (len(cv3)<5): return\n    cv3.reset_index(inplace=True)\n    MX = (cv3['test'] > cv3['trainMX'])\n    mxSum = round(100*cv3.loc[MX,'test'].sum(),1)\n    MN = (cv3['test'] < cv3['trainMN'])\n    mnSum = round(100*cv3.loc[MN,'test'].sum(),1)\n    #if override | (MX.sum()+MN.sum()>0):\n    if override | (mxSum + mnSum > 1):\n        plt.figure(figsize=(15,5))\n        if lab1=='': lab1='Train'\n        if lab2=='': lab2='Test'\n        plt.plot(cv3.index,cv3['train'],linewidth=3,alpha=0.7,color='b',label=lab1)\n        plt.plot(cv3.index,cv3['trainMX'],linewidth=2,alpha=1.0,linestyle=':',color='b',label=str())\n        plt.plot(cv3.index,cv3['trainMN'],linewidth=2,alpha=1.0,linestyle=':',color='b',label=str())\n        #plt.bar(cv3.index,cv3['test'],linewidth=3,alpha=0.7,color='g', label='Test.csv')\n        plt.plot(cv3.index,cv3['test'],linewidth=3,alpha=0.7,color='g',label=lab2)\n        plt.legend()\n        if title=='': plt.title(col)\n        else: plt.title(col+' - '+title)\n        plt.xlabel(col+' values (ordered by train frequency and relabeled)')\n        plt.ylabel('Frequency')\n        mx = max(cv3['train'].max(),cv3['test'].max())\n        #plt.ylim(0,mx*1.05)\n        plt.ylim(0,mx*scale)\n        plt.show()\n        tempMX = cv3.loc[MX.values,['index','test']].sort_values('test',ascending=False)['index']\n        tempMN = cv3.loc[MN.values,['index','test']].sort_values('test',ascending=False)['index']\n        if verbose:\n            if MX.sum()>0:    \n                print(prefix+'Test.csv',col,'has',MX.sum(),'values 4x MORE freq than Train.csv. (',mxSum,'% of data)')\n            if MX.sum()>10: print('  Top 10 by test freq:',list(tempMX)[:10])\n            elif MX.sum()>0: print(list(tempMX)[:10])\n            if MN.sum()>0:\n                print(prefix+'Test.csv',col,'has',MN.sum(),'values 4x LESS freq than Train.csv. (',mnSum,'% of data)')\n            if MN.sum()>10: print('  Top 10 by test freq:',list(tempMN)[:10])\n            elif MN.sum()>0: print(list(tempMN)[:10])\n    return\ncomparePlot(df_train, df_test, 'CountryIdentifier', verbose=False, title='Test vs. Train')", "processed": ["test full versu train full compar distribut test csv train csv wrote special python function interest click button see code view visual may confus first let explain basic plot two histogram top instead use bar use line follow top bar would x axi categori variabl possibl valu order frequent train le frequent train valu contain le 0 1 data remov relabel 0 1 2 n solid blue line train solid green line test distribut blue green line would coincid dot blue line indic 4x train 4x le train therefor green line cross outsid dot line test valu 4x 4x le train let plot countryidentifi train csv versu test csv"]}, {"markdown": ["## General information\n\nCorrectly predicting earthquakes is very important for preventing deaths and damage to infrastructure. In this competition we try to predict time left to the next laboratory earthquake based on seismic signal data.\nTraining data represents one huge signal, but in test data we have many separate chunks, for each of which we need to predict time to failure.\n\nThis is my second kernel for this competition, here is the [link](https://www.kaggle.com/artgor/seismic-data-eda-and-baseline) to the first one.\n\nIn this kernel I'll try to create more useful features and generate more data for training.\n\n![](https://i.cbc.ca/1.4972912.1547133821!/fileImage/httpImage/image.jpg_gen/derivatives/16x9_780/new-brunswick-earthquake.jpg)"], "code": "# Reference: https://www.kaggle.com/code/artgor/earthquakes-fe-more-features-and-samples\n\nimport numpy as np\nimport pandas as pd\nimport os\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom tqdm import tqdm_notebook\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import NuSVR, SVR\nfrom sklearn.metrics import mean_absolute_error\npd.options.display.precision = 15\n\nimport lightgbm as lgb\nimport xgboost as xgb\nimport time\nimport datetime\nfrom catboost import CatBoostRegressor\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.linear_model import LinearRegression\nimport gc\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom scipy.signal import hilbert\nfrom scipy.signal import hann\nfrom scipy.signal import convolve\nfrom scipy import stats\nfrom sklearn.kernel_ridge import KernelRidge\n%%time\ntrain = pd.read_csv('../input/train.csv', dtype={'acoustic_data': np.int16, 'time_to_failure': np.float32})\ntrain_acoustic_data_small = train['acoustic_data'].values[::50]\ntrain_time_to_failure_small = train['time_to_failure'].values[::50]\n\nfig, ax1 = plt.subplots(figsize=(16, 8))\nplt.title(\"Trends of acoustic_data and time_to_failure. 2% of data (sampled)\")\nplt.plot(train_acoustic_data_small, color='b')\nax1.set_ylabel('acoustic_data', color='b')\nplt.legend(['acoustic_data'])\nax2 = ax1.twinx()\nplt.plot(train_time_to_failure_small, color='g')\nax2.set_ylabel('time_to_failure', color='g')\nplt.legend(['time_to_failure'], loc=(0.875, 0.9))\nplt.grid(False)\n\ndel train_acoustic_data_small\ndel train_time_to_failure_small", "processed": ["gener inform correctli predict earthquak import prevent death damag infrastructur competit tri predict time left next laboratori earthquak base seismic signal data train data repres one huge signal test data mani separ chunk need predict time failur second kernel competit link http www kaggl com artgor seismic data eda baselin first one kernel tri creat use featur gener data train http cbc ca 1 4972912 1547133821 fileimag httpimag imag jpg gen deriv 16x9 780 new brunswick earthquak jpg"]}, {"markdown": ["### On sampling\n\nI tried to randomly sample 150000 rows 1k-10k times and add these samples to training data, but it severely decreased my score."], "code": "# Reference: https://www.kaggle.com/code/artgor/earthquakes-fe-more-features-and-samples\n\nnp.abs(X_tr.corrwith(y_tr['time_to_failure'])).sort_values(ascending=False).head(12)\nplt.figure(figsize=(44, 24))\ncols = list(np.abs(X_tr.corrwith(y_tr['time_to_failure'])).sort_values(ascending=False).head(24).index)\nfor i, col in enumerate(cols):\n    plt.subplot(6, 4, i + 1)\n    plt.plot(X_tr[col], color='blue')\n    plt.title(col)\n    ax1.set_ylabel(col, color='b')\n\n    ax2 = ax1.twinx()\n    plt.plot(y_tr, color='g')\n    ax2.set_ylabel('time_to_failure', color='g')\n    plt.legend([col, 'time_to_failure'], loc=(0.875, 0.9))\n    plt.grid(False)\nmeans_dict = {}\nfor col in X_tr.columns:\n    if X_tr[col].isnull().any():\n        print(col)\n        mean_value = X_tr.loc[X_tr[col] != -np.inf, col].mean()\n        X_tr.loc[X_tr[col] == -np.inf, col] = mean_value\n        X_tr[col] = X_tr[col].fillna(mean_value)\n        means_dict[col] = mean_value\nscaler = StandardScaler()\nscaler.fit(X_tr)\nX_train_scaled = pd.DataFrame(scaler.transform(X_tr), columns=X_tr.columns)", "processed": ["sampl tri randomli sampl 150000 row 1k 10k time add sampl train data sever decreas score"]}, {"markdown": ["### Reading test dat"], "code": "# Reference: https://www.kaggle.com/code/artgor/earthquakes-fe-more-features-and-samples\n\nsubmission = pd.read_csv('../input/sample_submission.csv', index_col='seg_id')\nX_test = pd.DataFrame(columns=X_tr.columns, dtype=np.float64, index=submission.index)\nplt.figure(figsize=(22, 16))\n\nfor i, seg_id in enumerate(tqdm_notebook(X_test.index)):\n    seg = pd.read_csv('../input/test/' + seg_id + '.csv')\n    \n    x = pd.Series(seg['acoustic_data'].values)\n    X_test.loc[seg_id, 'mean'] = x.mean()\n    X_test.loc[seg_id, 'std'] = x.std()\n    X_test.loc[seg_id, 'max'] = x.max()\n    X_test.loc[seg_id, 'min'] = x.min()\n        \n    X_test.loc[seg_id, 'mean_change_abs'] = np.mean(np.diff(x))\n    X_test.loc[seg_id, 'mean_change_rate'] = calc_change_rate(x)\n    X_test.loc[seg_id, 'abs_max'] = np.abs(x).max()\n    X_test.loc[seg_id, 'abs_min'] = np.abs(x).min()\n    \n    X_test.loc[seg_id, 'std_first_50000'] = x[:50000].std()\n    X_test.loc[seg_id, 'std_last_50000'] = x[-50000:].std()\n    X_test.loc[seg_id, 'std_first_10000'] = x[:10000].std()\n    X_test.loc[seg_id, 'std_last_10000'] = x[-10000:].std()\n    \n    X_test.loc[seg_id, 'avg_first_50000'] = x[:50000].mean()\n    X_test.loc[seg_id, 'avg_last_50000'] = x[-50000:].mean()\n    X_test.loc[seg_id, 'avg_first_10000'] = x[:10000].mean()\n    X_test.loc[seg_id, 'avg_last_10000'] = x[-10000:].mean()\n    \n    X_test.loc[seg_id, 'min_first_50000'] = x[:50000].min()\n    X_test.loc[seg_id, 'min_last_50000'] = x[-50000:].min()\n    X_test.loc[seg_id, 'min_first_10000'] = x[:10000].min()\n    X_test.loc[seg_id, 'min_last_10000'] = x[-10000:].min()\n    \n    X_test.loc[seg_id, 'max_first_50000'] = x[:50000].max()\n    X_test.loc[seg_id, 'max_last_50000'] = x[-50000:].max()\n    X_test.loc[seg_id, 'max_first_10000'] = x[:10000].max()\n    X_test.loc[seg_id, 'max_last_10000'] = x[-10000:].max()\n    \n    X_test.loc[seg_id, 'max_to_min'] = x.max() / np.abs(x.min())\n    X_test.loc[seg_id, 'max_to_min_diff'] = x.max() - np.abs(x.min())\n    X_test.loc[seg_id, 'count_big'] = len(x[np.abs(x) > 500])\n    X_test.loc[seg_id, 'sum'] = x.sum()\n    \n    X_test.loc[seg_id, 'mean_change_rate_first_50000'] = calc_change_rate(x[:50000])\n    X_test.loc[seg_id, 'mean_change_rate_last_50000'] = calc_change_rate(x[-50000:])\n    X_test.loc[seg_id, 'mean_change_rate_first_10000'] = calc_change_rate(x[:10000])\n    X_test.loc[seg_id, 'mean_change_rate_last_10000'] = calc_change_rate(x[-10000:])\n    \n    X_test.loc[seg_id, 'q95'] = np.quantile(x,0.95)\n    X_test.loc[seg_id, 'q99'] = np.quantile(x,0.99)\n    X_test.loc[seg_id, 'q05'] = np.quantile(x,0.05)\n    X_test.loc[seg_id, 'q01'] = np.quantile(x,0.01)\n    \n    X_test.loc[seg_id, 'abs_q95'] = np.quantile(np.abs(x), 0.95)\n    X_test.loc[seg_id, 'abs_q99'] = np.quantile(np.abs(x), 0.99)\n    X_test.loc[seg_id, 'abs_q05'] = np.quantile(np.abs(x), 0.05)\n    X_test.loc[seg_id, 'abs_q01'] = np.quantile(np.abs(x), 0.01)\n    \n    X_test.loc[seg_id, 'trend'] = add_trend_feature(x)\n    X_test.loc[seg_id, 'abs_trend'] = add_trend_feature(x, abs_values=True)\n    X_test.loc[seg_id, 'abs_mean'] = np.abs(x).mean()\n    X_test.loc[seg_id, 'abs_std'] = np.abs(x).std()\n    \n    X_test.loc[seg_id, 'mad'] = x.mad()\n    X_test.loc[seg_id, 'kurt'] = x.kurtosis()\n    X_test.loc[seg_id, 'skew'] = x.skew()\n    X_test.loc[seg_id, 'med'] = x.median()\n    \n    X_test.loc[seg_id, 'Hilbert_mean'] = np.abs(hilbert(x)).mean()\n    X_test.loc[seg_id, 'Hann_window_mean'] = (convolve(x, hann(150), mode='same') / sum(hann(150))).mean()\n    X_test.loc[seg_id, 'classic_sta_lta1_mean'] = classic_sta_lta(x, 500, 10000).mean()\n    X_test.loc[seg_id, 'classic_sta_lta2_mean'] = classic_sta_lta(x, 5000, 100000).mean()\n    X_test.loc[seg_id, 'classic_sta_lta3_mean'] = classic_sta_lta(x, 3333, 6666).mean()\n    X_test.loc[seg_id, 'classic_sta_lta4_mean'] = classic_sta_lta(x, 10000, 25000).mean()\n    X_test.loc[seg_id, 'classic_sta_lta5_mean'] = classic_sta_lta(x, 50, 1000).mean()\n    X_test.loc[seg_id, 'classic_sta_lta6_mean'] = classic_sta_lta(x, 100, 5000).mean()\n    X_test.loc[seg_id, 'classic_sta_lta7_mean'] = classic_sta_lta(x, 333, 666).mean()\n    X_test.loc[seg_id, 'classic_sta_lta8_mean'] = classic_sta_lta(x, 4000, 10000).mean()\n    X_test.loc[seg_id, 'Moving_average_700_mean'] = x.rolling(window=700).mean().mean(skipna=True)\n    ewma = pd.Series.ewm\n    X_test.loc[seg_id, 'exp_Moving_average_300_mean'] = (ewma(x, span=300).mean()).mean(skipna=True)\n    X_test.loc[seg_id, 'exp_Moving_average_3000_mean'] = ewma(x, span=3000).mean().mean(skipna=True)\n    X_test.loc[seg_id, 'exp_Moving_average_30000_mean'] = ewma(x, span=30000).mean().mean(skipna=True)\n    no_of_std = 3\n    X_test.loc[seg_id, 'MA_700MA_std_mean'] = x.rolling(window=700).std().mean()\n    X_test.loc[seg_id,'MA_700MA_BB_high_mean'] = (X_test.loc[seg_id, 'Moving_average_700_mean'] + no_of_std * X_test.loc[seg_id, 'MA_700MA_std_mean']).mean()\n    X_test.loc[seg_id,'MA_700MA_BB_low_mean'] = (X_test.loc[seg_id, 'Moving_average_700_mean'] - no_of_std * X_test.loc[seg_id, 'MA_700MA_std_mean']).mean()\n    X_test.loc[seg_id, 'MA_400MA_std_mean'] = x.rolling(window=400).std().mean()\n    X_test.loc[seg_id,'MA_400MA_BB_high_mean'] = (X_test.loc[seg_id, 'Moving_average_700_mean'] + no_of_std * X_test.loc[seg_id, 'MA_400MA_std_mean']).mean()\n    X_test.loc[seg_id,'MA_400MA_BB_low_mean'] = (X_test.loc[seg_id, 'Moving_average_700_mean'] - no_of_std * X_test.loc[seg_id, 'MA_400MA_std_mean']).mean()\n    X_test.loc[seg_id, 'MA_1000MA_std_mean'] = x.rolling(window=1000).std().mean()\n    X_test.drop('Moving_average_700_mean', axis=1, inplace=True)\n    \n    X_test.loc[seg_id, 'iqr'] = np.subtract(*np.percentile(x, [75, 25]))\n    X_test.loc[seg_id, 'q999'] = np.quantile(x,0.999)\n    X_test.loc[seg_id, 'q001'] = np.quantile(x,0.001)\n    X_test.loc[seg_id, 'ave10'] = stats.trim_mean(x, 0.1)\n    \n    for windows in [10, 100, 1000]:\n        x_roll_std = x.rolling(windows).std().dropna().values\n        x_roll_mean = x.rolling(windows).mean().dropna().values\n        \n        X_test.loc[seg_id, 'ave_roll_std_' + str(windows)] = x_roll_std.mean()\n        X_test.loc[seg_id, 'std_roll_std_' + str(windows)] = x_roll_std.std()\n        X_test.loc[seg_id, 'max_roll_std_' + str(windows)] = x_roll_std.max()\n        X_test.loc[seg_id, 'min_roll_std_' + str(windows)] = x_roll_std.min()\n        X_test.loc[seg_id, 'q01_roll_std_' + str(windows)] = np.quantile(x_roll_std, 0.01)\n        X_test.loc[seg_id, 'q05_roll_std_' + str(windows)] = np.quantile(x_roll_std, 0.05)\n        X_test.loc[seg_id, 'q95_roll_std_' + str(windows)] = np.quantile(x_roll_std, 0.95)\n        X_test.loc[seg_id, 'q99_roll_std_' + str(windows)] = np.quantile(x_roll_std, 0.99)\n        X_test.loc[seg_id, 'av_change_abs_roll_std_' + str(windows)] = np.mean(np.diff(x_roll_std))\n        X_test.loc[seg_id, 'av_change_rate_roll_std_' + str(windows)] = np.mean(np.nonzero((np.diff(x_roll_std) / x_roll_std[:-1]))[0])\n        X_test.loc[seg_id, 'abs_max_roll_std_' + str(windows)] = np.abs(x_roll_std).max()\n        \n        X_test.loc[seg_id, 'ave_roll_mean_' + str(windows)] = x_roll_mean.mean()\n        X_test.loc[seg_id, 'std_roll_mean_' + str(windows)] = x_roll_mean.std()\n        X_test.loc[seg_id, 'max_roll_mean_' + str(windows)] = x_roll_mean.max()\n        X_test.loc[seg_id, 'min_roll_mean_' + str(windows)] = x_roll_mean.min()\n        X_test.loc[seg_id, 'q01_roll_mean_' + str(windows)] = np.quantile(x_roll_mean, 0.01)\n        X_test.loc[seg_id, 'q05_roll_mean_' + str(windows)] = np.quantile(x_roll_mean, 0.05)\n        X_test.loc[seg_id, 'q95_roll_mean_' + str(windows)] = np.quantile(x_roll_mean, 0.95)\n        X_test.loc[seg_id, 'q99_roll_mean_' + str(windows)] = np.quantile(x_roll_mean, 0.99)\n        X_test.loc[seg_id, 'av_change_abs_roll_mean_' + str(windows)] = np.mean(np.diff(x_roll_mean))\n        X_test.loc[seg_id, 'av_change_rate_roll_mean_' + str(windows)] = np.mean(np.nonzero((np.diff(x_roll_mean) / x_roll_mean[:-1]))[0])\n        X_test.loc[seg_id, 'abs_max_roll_mean_' + str(windows)] = np.abs(x_roll_mean).max()\n    \n    if i < 12:\n        plt.subplot(6, 4, i + 1)\n        plt.plot(seg['acoustic_data'])\n        plt.title(seg_id)\n\nfor col in X_test.columns:\n    if X_test[col].isnull().any():\n        X_test.loc[X_test[col] == -np.inf, col] = means_dict[col]\n        X_test[col] = X_test[col].fillna(means_dict[col])\n        \nX_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)", "processed": ["read test dat"]}, {"markdown": ["## Building models"], "code": "# Reference: https://www.kaggle.com/code/artgor/earthquakes-fe-more-features-and-samples\n\nn_fold = 5\nfolds = KFold(n_splits=n_fold, shuffle=True, random_state=11)\ndef train_model(X=X_train_scaled, X_test=X_test_scaled, y=y_tr, params=None, folds=folds, model_type='lgb', plot_feature_importance=False, model=None):\n\n    oof = np.zeros(len(X))\n    prediction = np.zeros(len(X_test))\n    scores = []\n    feature_importance = pd.DataFrame()\n    for fold_n, (train_index, valid_index) in enumerate(folds.split(X)):\n        print('Fold', fold_n, 'started at', time.ctime())\n        X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n        y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n        \n        if model_type == 'lgb':\n            model = lgb.LGBMRegressor(**params, n_estimators = 50000, n_jobs = -1)\n            model.fit(X_train, y_train, \n                    eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric='mae',\n                    verbose=10000, early_stopping_rounds=200)\n            \n            y_pred_valid = model.predict(X_valid)\n            y_pred = model.predict(X_test, num_iteration=model.best_iteration_)\n            \n        if model_type == 'xgb':\n            train_data = xgb.DMatrix(data=X_train, label=y_train, feature_names=X.columns)\n            valid_data = xgb.DMatrix(data=X_valid, label=y_valid, feature_names=X.columns)\n\n            watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]\n            model = xgb.train(dtrain=train_data, num_boost_round=20000, evals=watchlist, early_stopping_rounds=200, verbose_eval=500, params=params)\n            y_pred_valid = model.predict(xgb.DMatrix(X_valid, feature_names=X.columns), ntree_limit=model.best_ntree_limit)\n            y_pred = model.predict(xgb.DMatrix(X_test, feature_names=X.columns), ntree_limit=model.best_ntree_limit)\n        \n        if model_type == 'sklearn':\n            model = model\n            model.fit(X_train, y_train)\n            \n            y_pred_valid = model.predict(X_valid).reshape(-1,)\n            score = mean_absolute_error(y_valid, y_pred_valid)\n            print(f'Fold {fold_n}. MAE: {score:.4f}.')\n            print('')\n            \n            y_pred = model.predict(X_test).reshape(-1,)\n        \n        if model_type == 'cat':\n            model = CatBoostRegressor(iterations=20000,  eval_metric='MAE', **params)\n            model.fit(X_train, y_train, eval_set=(X_valid, y_valid), cat_features=[], use_best_model=True, verbose=False)\n\n            y_pred_valid = model.predict(X_valid)\n            y_pred = model.predict(X_test)\n        \n        oof[valid_index] = y_pred_valid.reshape(-1,)\n        scores.append(mean_absolute_error(y_valid, y_pred_valid))\n\n        prediction += y_pred    \n        \n        if model_type == 'lgb':\n            # feature importance\n            fold_importance = pd.DataFrame()\n            fold_importance[\"feature\"] = X.columns\n            fold_importance[\"importance\"] = model.feature_importances_\n            fold_importance[\"fold\"] = fold_n + 1\n            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n\n    prediction /= n_fold\n    \n    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n    \n    if model_type == 'lgb':\n        feature_importance[\"importance\"] /= n_fold\n        if plot_feature_importance:\n            cols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n                by=\"importance\", ascending=False)[:50].index\n\n            best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n\n            plt.figure(figsize=(16, 12));\n            sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False));\n            plt.title('LGB Features (avg over folds)');\n        \n            return oof, prediction, feature_importance\n        return oof, prediction\n    \n    else:\n        return oof, prediction\nparams = {'num_leaves': 128,\n          'min_data_in_leaf': 79,\n          'objective': 'huber',\n          'max_depth': -1,\n          'learning_rate': 0.01,\n          \"boosting\": \"gbdt\",\n          \"bagging_freq\": 5,\n          \"bagging_fraction\": 0.8126672064208567,\n          \"bagging_seed\": 11,\n          \"metric\": 'mae',\n          \"verbosity\": -1,\n          'reg_alpha': 0.1302650970728192,\n          'reg_lambda': 0.3603427518866501\n         }\noof_lgb, prediction_lgb, feature_importance = train_model(params=params, model_type='lgb', plot_feature_importance=True)\ntop_cols = list(feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n                by=\"importance\", ascending=False)[:50].index)\n# Taking less columns seriously decreases score.\n# X_train_scaled = X_train_scaled[top_cols]\n# X_test_scaled = X_test_scaled[top_cols]\noof_lgb, prediction_lgb, feature_importance = train_model(X=X_train_scaled, X_test=X_test_scaled, params=params, model_type='lgb', plot_feature_importance=True)\nxgb_params = {'eta': 0.03,\n              'max_depth': 9,\n              'subsample': 0.85,\n              'objective': 'reg:linear',\n              'eval_metric': 'mae',\n              'silent': True,\n              'nthread': 4}\noof_xgb, prediction_xgb = train_model(X=X_train_scaled, X_test=X_test_scaled, params=xgb_params, model_type='xgb')\nmodel = NuSVR(gamma='scale', nu=0.9, C=10.0, tol=0.01)\noof_svr, prediction_svr = train_model(X=X_train_scaled, X_test=X_test_scaled, params=None, model_type='sklearn', model=model)\nmodel = NuSVR(gamma='scale', nu=0.7, tol=0.01, C=1.0)\noof_svr1, prediction_svr1 = train_model(X=X_train_scaled, X_test=X_test_scaled, params=None, model_type='sklearn', model=model)\nparams = {'loss_function':'MAE'}\noof_cat, prediction_cat = train_model(X=X_train_scaled, X_test=X_test_scaled, params=params, model_type='cat')\nmodel = KernelRidge(kernel='rbf', alpha=0.1, gamma=0.01)\noof_r, prediction_r = train_model(X=X_train_scaled, X_test=X_test_scaled, params=None, model_type='sklearn', model=model)", "processed": ["build model"]}, {"markdown": ["Now let's see how do our models perform"], "code": "# Reference: https://www.kaggle.com/code/artgor/earthquakes-fe-more-features-and-samples\n\nplt.figure(figsize=(18, 8))\nplt.subplot(2, 3, 1)\nplt.plot(y_tr, color='g', label='y_train')\nplt.plot(oof_lgb, color='b', label='lgb')\nplt.legend(loc=(1, 0.5));\nplt.title('lgb');\nplt.subplot(2, 3, 2)\nplt.plot(y_tr, color='g', label='y_train')\nplt.plot(oof_xgb, color='teal', label='xgb')\nplt.legend(loc=(1, 0.5));\nplt.title('xgb');\nplt.subplot(2, 3, 3)\nplt.plot(y_tr, color='g', label='y_train')\nplt.plot(oof_svr, color='red', label='svr')\nplt.legend(loc=(1, 0.5));\nplt.title('svr');\nplt.subplot(2, 3, 4)\nplt.plot(y_tr, color='g', label='y_train')\nplt.plot(oof_cat, color='b', label='cat')\nplt.legend(loc=(1, 0.5));\nplt.title('cat');\nplt.subplot(2, 3, 5)\nplt.plot(y_tr, color='g', label='y_train')\nplt.plot(oof_lgb_stack, color='gold', label='stack')\nplt.legend(loc=(1, 0.5));\nplt.title('blend');\nplt.legend(loc=(1, 0.5));\nplt.suptitle('Predictions vs actual');\nplt.subplot(2, 3, 6)\nplt.plot(y_tr, color='g', label='y_train')\nplt.plot((oof_lgb + oof_xgb + oof_svr + oof_svr1 + oof_r + oof_cat) / 6, color='gold', label='blend')\nplt.legend(loc=(1, 0.5));\nplt.title('blend');\nplt.legend(loc=(1, 0.5));\nplt.suptitle('Predictions vs actual');", "processed": ["let see model perform"]}, {"markdown": ["# <a id='3'>Data exploration</a>  \n\nThe dimmension of the data is quite large, in excess of 600 millions rows of data.  \nThe two columns in the train dataset have the following meaning:   \n*  accoustic_data: is the accoustic signal measured in the laboratory experiment;  \n* time to failure: this gives the time until a failure will occurs.\n\nLet's plot 1% of the data. For this we will sample every 100 points of data.  "], "code": "# Reference: https://www.kaggle.com/code/gpreda/lanl-earthquake-eda-and-prediction\n\ntrain_ad_sample_df = train_df['acoustic_data'].values[::100]\ntrain_ttf_sample_df = train_df['time_to_failure'].values[::100]\n\ndef plot_acc_ttf_data(train_ad_sample_df, train_ttf_sample_df, title=\"Acoustic data and time to failure: 1% sampled data\"):\n    fig, ax1 = plt.subplots(figsize=(12, 8))\n    plt.title(title)\n    plt.plot(train_ad_sample_df, color='r')\n    ax1.set_ylabel('acoustic data', color='r')\n    plt.legend(['acoustic data'], loc=(0.01, 0.95))\n    ax2 = ax1.twinx()\n    plt.plot(train_ttf_sample_df, color='b')\n    ax2.set_ylabel('time to failure', color='b')\n    plt.legend(['time to failure'], loc=(0.01, 0.9))\n    plt.grid(True)\n\nplot_acc_ttf_data(train_ad_sample_df, train_ttf_sample_df)\ndel train_ad_sample_df\ndel train_ttf_sample_df", "processed": ["id 3 data explor dimmens data quit larg excess 600 million row data two column train dataset follow mean accoust data accoust signal measur laboratori experi time failur give time failur occur let plot 1 data sampl everi 100 point data"]}, {"markdown": ["## Features importance\n\nLet's print features importance."], "code": "# Reference: https://www.kaggle.com/code/gpreda/lanl-earthquake-eda-and-prediction\n\ncols = (feature_importance_df[[\"Feature\", \"importance\"]]\n        .groupby(\"Feature\")\n        .mean()\n        .sort_values(by=\"importance\", ascending=False)[:200].index)\nbest_features = feature_importance_df.loc[feature_importance_df.Feature.isin(cols)]\n\nplt.figure(figsize=(14,26))\nsns.barplot(x=\"importance\", y=\"Feature\", data=best_features.sort_values(by=\"importance\",ascending=False))\nplt.title('LightGBM Features (averaged over folds)')\nplt.tight_layout()\nplt.savefig('lgbm_importances.png')", "processed": ["featur import let print featur import"]}, {"markdown": ["Now let us see the bivariate distribution of these numerical variables. Thanks to this script."], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-notebook-1\n\nplt.figure(figsize=(12,12))\nsns.pairplot(train[numerical_cols+['type']], hue=\"type\")\nplt.title(\"Bivariate plot on numerical features\")\nplt.show()", "processed": ["let u see bivari distribut numer variabl thank script"]}, {"markdown": ["We can now see how the classes are distributed in each of the categories"], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-notebook-1\n\nplt.figure(figsize=(12,6))\nsns.countplot(x='color', hue='type', data=train)\nplt.show()", "processed": ["see class distribut categori"]}, {"markdown": ["**Bivariate plots:**\n\nNow let us look at the bi-variate plots of numerical variables with respect to the categorical variable"], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-notebook-1\n\nplt.figure(figsize=(12,6))\nsns.swarmplot(x=\"color\", y=\"bone_length\", hue=\"type\", data=train)\nplt.title(\"Color Vs BoneLength\")\nplt.show()\nplt.figure(figsize=(12,6))\nsns.boxplot(x=\"color\", y=\"rotting_flesh\", hue=\"type\", data=train)\nplt.title(\"Color Vs Rotting Flesh\")\nplt.show()\nplt.figure(figsize=(12,6))\nsns.violinplot(x=\"color\", y=\"hair_length\", hue=\"type\", data=train)\nplt.title(\"Color Vs Hair Length\")\nplt.show()\nplt.figure(figsize=(12,6))\nsns.pointplot(x=\"color\", y=\"has_soul\", hue=\"type\", data=train)\nplt.title(\"Color Vs Has Soul\")\nplt.show()", "processed": ["bivari plot let u look bi variat plot numer variabl respect categor variabl"]}, {"markdown": ["# Display Predictions\nFirst we will display a histogram and next display a time series plot."], "code": "# Reference: https://www.kaggle.com/code/cdeotte/embeddings-network-malware-0-697-0-773\n\nimport matplotlib.pyplot as plt    \nb = plt.hist(pred, bins=200)\nimport calendar, math\n\ndef dynamicPlot(data,col, target='HasDetections', start=datetime(2018,4,1), end=datetime(2018,12,1)\n                ,inc_hr=0,inc_dy=7,inc_mn=0,show=0.99,top=5,top2=4,title='',legend=1,z=0,dots=False):\n    # check for timestamps\n    if 'Date' not in data:\n        print('Error dynamicPlot: DataFrame needs column Date of datetimes')\n        return\n    \n    # remove detection line if category density is too small\n    cv = data[(data['Date']>start) & (data['Date']<end)][col].value_counts(dropna=False)\n    cvd = cv.to_dict()\n    nm = cv.index.values\n    th = show * len(data)\n    sum = 0; lnn2 = 0\n    for x in nm:\n        lnn2 += 1\n        sum += cvd[x]\n        if sum>th:\n            break\n    top = min(top,len(nm))\n    top2 = min(top2,len(nm),lnn2,top)\n\n    # calculate rate within each time interval\n    diff = (end-start).days*24*3600 + (end-start).seconds\n    size = diff//(3600*((inc_mn * 28 + inc_dy) * 24 + inc_hr)) + 5\n    data_counts = np.zeros([size,2*top+1],dtype=float)\n    idx=0; idx2 = {}\n    for i in range(top):\n        idx2[nm[i]] = i+1\n    low = start\n    high = add_time(start,inc_mn,inc_dy,inc_hr)\n    data_times = [low+(high-low)/2]\n    while low<end:\n        slice = data[ (data['Date']<high) & (data['Date']>=low) ]\n        #data_counts[idx,0] = len(slice)\n        data_counts[idx,0] = 5000*len(slice['AvSigVersion'].unique())\n        for key in idx2:\n            if nan_check(key): slice2 = slice[slice[col].isna()]\n            else: slice2 = slice[slice[col]==key]\n            data_counts[idx,idx2[key]] = len(slice2)\n            if target in data:\n                data_counts[idx,top+idx2[key]] = slice2['HasDetections'].mean()\n        low = high\n        high = add_time(high,inc_mn,inc_dy,inc_hr)\n        data_times.append(low+(high-low)/2)\n        idx += 1\n\n    # plot lines\n    fig = plt.figure(1,figsize=(15,3))\n    cl = ['r','g','b','y','m']\n    ax3 = fig.add_subplot(1,1,1)\n    lines = []; labels = []\n    if z==1: ax3.plot(data_times,data_counts[0:idx+1,0],'k')\n    for i in range(top):\n        tmp, = ax3.plot(data_times,data_counts[0:idx+1,i+1],cl[i%5])\n        if dots: ax3.plot(data_times,data_counts[0:idx+1,i+1],cl[i%5]+'o')\n        lines.append(tmp)\n        labels.append(str(nm[i]))\n    ax3.spines['left'].set_color('red')\n    ax3.yaxis.label.set_color('red')\n    ax3.tick_params(axis='y', colors='red')\n    if col!='ones': ax3.set_ylabel('Category Density', color='r')\n    else: ax3.set_ylabel('Data Density', color='r')\n    #ax3.set_yticklabels([])\n    if target in data:\n        ax4 = ax3.twinx()\n        for i in range(top2):\n            ax4.plot(data_times,data_counts[0:idx+1,i+1+top],cl[i%5]+\":\")\n            if dots: ax4.plot(data_times,data_counts[0:idx+1,i+1+top],cl[i%5]+\"o\")\n        ax4.spines['left'].set_color('red')\n        ax4.set_ylabel('Detection Rate', color='k')\n    if title!='': plt.title(title)\n    if legend==1: plt.legend(lines,labels,loc=2)\n    plt.show()\n        \n# INCREMENT A DATETIME\ndef add_time(sdate,months=0,days=0,hours=0):\n    month = sdate.month -1 + months\n    year = sdate.year + month // 12\n    month = month % 12 + 1\n    day = sdate.day + days\n    if day>calendar.monthrange(year,month)[1]:\n        day -= calendar.monthrange(year,month)[1]\n        month += 1\n        if month>12:\n            month = 1\n            year += 1\n    hour = sdate.hour + hours\n    if hour>23:\n        hour = 0\n        day += 1\n        if day>calendar.monthrange(year,month)[1]:\n            day -= calendar.monthrange(year,month)[1]\n            month += 1\n            if month>12:\n                month = 1\n                year += 1\n    return datetime(year,month,day,hour,sdate.minute)\n\n# CHECK FOR NAN\ndef nan_check(x):\n    if isinstance(x,float):\n        if math.isnan(x):\n            return True\n    return False\ndf_test['ones'] = 1\ndynamicPlot(df_test, 'ones', inc_dy=2, legend=0,\n        title='Test.csv HasDetections Predictions. (Dotted line uses right y-axis. Solid uses left.)')", "processed": ["display predict first display histogram next display time seri plot"]}, {"markdown": ["## Inspecting Memory Usage"], "code": "# Reference: https://www.kaggle.com/code/xhlulu/load-entire-dataset-with-7-gb-ram-fork\n\nram_usage = dict(psutil.virtual_memory()._asdict())\nram_usage\nram_usage_df = pd.Series(ram_usage)\nram_usage_df/= 2**30\nram_usage_df.plot(kind='bar', rot=45)", "processed": ["inspect memori usag"]}, {"markdown": ["They are quite a few. Since the values are constant, we can just drop them from our feature list and save some memory and time in our modeling process. \n\n**Device Information:**"], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-baseline-ga-customer-revenue\n\ndef horizontal_bar_chart(cnt_srs, color):\n    trace = go.Bar(\n        y=cnt_srs.index[::-1],\n        x=cnt_srs.values[::-1],\n        showlegend=False,\n        orientation = 'h',\n        marker=dict(\n            color=color,\n        ),\n    )\n    return trace\n\n# Device Browser\ncnt_srs = train_df.groupby('device.browser')['totals.transactionRevenue'].agg(['size', 'count', 'mean'])\ncnt_srs.columns = [\"count\", \"count of non-zero revenue\", \"mean\"]\ncnt_srs = cnt_srs.sort_values(by=\"count\", ascending=False)\ntrace1 = horizontal_bar_chart(cnt_srs[\"count\"].head(10), 'rgba(50, 171, 96, 0.6)')\ntrace2 = horizontal_bar_chart(cnt_srs[\"count of non-zero revenue\"].head(10), 'rgba(50, 171, 96, 0.6)')\ntrace3 = horizontal_bar_chart(cnt_srs[\"mean\"].head(10), 'rgba(50, 171, 96, 0.6)')\n\n# Device Category\ncnt_srs = train_df.groupby('device.deviceCategory')['totals.transactionRevenue'].agg(['size', 'count', 'mean'])\ncnt_srs.columns = [\"count\", \"count of non-zero revenue\", \"mean\"]\ncnt_srs = cnt_srs.sort_values(by=\"count\", ascending=False)\ntrace4 = horizontal_bar_chart(cnt_srs[\"count\"].head(10), 'rgba(71, 58, 131, 0.8)')\ntrace5 = horizontal_bar_chart(cnt_srs[\"count of non-zero revenue\"].head(10), 'rgba(71, 58, 131, 0.8)')\ntrace6 = horizontal_bar_chart(cnt_srs[\"mean\"].head(10), 'rgba(71, 58, 131, 0.8)')\n\n# Operating system\ncnt_srs = train_df.groupby('device.operatingSystem')['totals.transactionRevenue'].agg(['size', 'count', 'mean'])\ncnt_srs.columns = [\"count\", \"count of non-zero revenue\", \"mean\"]\ncnt_srs = cnt_srs.sort_values(by=\"count\", ascending=False)\ntrace7 = horizontal_bar_chart(cnt_srs[\"count\"].head(10), 'rgba(246, 78, 139, 0.6)')\ntrace8 = horizontal_bar_chart(cnt_srs[\"count of non-zero revenue\"].head(10),'rgba(246, 78, 139, 0.6)')\ntrace9 = horizontal_bar_chart(cnt_srs[\"mean\"].head(10),'rgba(246, 78, 139, 0.6)')\n\n# Creating two subplots\nfig = tools.make_subplots(rows=3, cols=3, vertical_spacing=0.04, \n                          subplot_titles=[\"Device Browser - Count\", \"Device Browser - Non-zero Revenue Count\", \"Device Browser - Mean Revenue\",\n                                          \"Device Category - Count\",  \"Device Category - Non-zero Revenue Count\", \"Device Category - Mean Revenue\", \n                                          \"Device OS - Count\", \"Device OS - Non-zero Revenue Count\", \"Device OS - Mean Revenue\"])\n\nfig.append_trace(trace1, 1, 1)\nfig.append_trace(trace2, 1, 2)\nfig.append_trace(trace3, 1, 3)\nfig.append_trace(trace4, 2, 1)\nfig.append_trace(trace5, 2, 2)\nfig.append_trace(trace6, 2, 3)\nfig.append_trace(trace7, 3, 1)\nfig.append_trace(trace8, 3, 2)\nfig.append_trace(trace9, 3, 3)\n\nfig['layout'].update(height=1200, width=1200, paper_bgcolor='rgb(233,233,233)', title=\"Device Plots\")\npy.iplot(fig, filename='device-plots')", "processed": ["quit sinc valu constant drop featur list save memori time model process devic inform"]}, {"markdown": ["Inferences:\n* Device browser distribution looks similar on both the count and count of non-zero revenue plots\n* On the device category front, desktop seem to have higher percentage of non-zero revenue counts compared to mobile devices.\n* In device operating system, though the number of counts is more from windows, the number of counts where revenue is not zero is more for Macintosh.\n* Chrome OS also has higher percentage of non-zero revenue counts\n* On the mobile OS side, iOS has more percentage of non-zero revenue counts compared to Android \n\n**Date Exploration:**"], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-baseline-ga-customer-revenue\n\nimport datetime\n\ndef scatter_plot(cnt_srs, color):\n    trace = go.Scatter(\n        x=cnt_srs.index[::-1],\n        y=cnt_srs.values[::-1],\n        showlegend=False,\n        marker=dict(\n            color=color,\n        ),\n    )\n    return trace\n\ntrain_df['date'] = train_df['date'].apply(lambda x: datetime.date(int(str(x)[:4]), int(str(x)[4:6]), int(str(x)[6:])))\ncnt_srs = train_df.groupby('date')['totals.transactionRevenue'].agg(['size', 'count'])\ncnt_srs.columns = [\"count\", \"count of non-zero revenue\"]\ncnt_srs = cnt_srs.sort_index()\n#cnt_srs.index = cnt_srs.index.astype('str')\ntrace1 = scatter_plot(cnt_srs[\"count\"], 'red')\ntrace2 = scatter_plot(cnt_srs[\"count of non-zero revenue\"], 'blue')\n\nfig = tools.make_subplots(rows=2, cols=1, vertical_spacing=0.08,\n                          subplot_titles=[\"Date - Count\", \"Date - Non-zero Revenue count\"])\nfig.append_trace(trace1, 1, 1)\nfig.append_trace(trace2, 2, 1)\nfig['layout'].update(height=800, width=800, paper_bgcolor='rgb(233,233,233)', title=\"Date Plots\")\npy.iplot(fig, filename='date-plots')", "processed": ["infer devic browser distribut look similar count count non zero revenu plot devic categori front desktop seem higher percentag non zero revenu count compar mobil devic devic oper system though number count window number count revenu zero macintosh chrome o also higher percentag non zero revenu count mobil o side io percentag non zero revenu count compar android date explor"]}, {"markdown": ["Inferences:\n* We have data from 1 Aug, 2016 to 31 July, 2017 in our training dataset\n* In Nov 2016, though there is an increase in the count of visitors, there is no increase in non-zero revenue counts during that time period (relative to the mean)."], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-baseline-ga-customer-revenue\n\ntest_df['date'] = test_df['date'].apply(lambda x: datetime.date(int(str(x)[:4]), int(str(x)[4:6]), int(str(x)[6:])))\ncnt_srs = test_df.groupby('date')['fullVisitorId'].size()\n\n\ntrace = scatter_plot(cnt_srs, 'red')\n\nlayout = go.Layout(\n    height=400,\n    width=800,\n    paper_bgcolor='rgb(233,233,233)',\n    title='Dates in Test set'\n)\n\ndata = [trace]\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename=\"ActivationDate\")", "processed": ["infer data 1 aug 2016 31 juli 2017 train dataset nov 2016 though increas count visitor increas non zero revenu count time period rel mean"]}, {"markdown": ["In the test set, we have dates from 2 Aug, 2017 to 30 Apr, 2018. So there are no common dates between train and test set. So it might be a good idea to do time based validation for this dataset.\n\n**Geographic Information:**"], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-baseline-ga-customer-revenue\n\n# Continent\ncnt_srs = train_df.groupby('geoNetwork.continent')['totals.transactionRevenue'].agg(['size', 'count', 'mean'])\ncnt_srs.columns = [\"count\", \"count of non-zero revenue\", \"mean\"]\ncnt_srs = cnt_srs.sort_values(by=\"count\", ascending=False)\ntrace1 = horizontal_bar_chart(cnt_srs[\"count\"].head(10), 'rgba(58, 71, 80, 0.6)')\ntrace2 = horizontal_bar_chart(cnt_srs[\"count of non-zero revenue\"].head(10), 'rgba(58, 71, 80, 0.6)')\ntrace3 = horizontal_bar_chart(cnt_srs[\"mean\"].head(10), 'rgba(58, 71, 80, 0.6)')\n\n# Sub-continent\ncnt_srs = train_df.groupby('geoNetwork.subContinent')['totals.transactionRevenue'].agg(['size', 'count', 'mean'])\ncnt_srs.columns = [\"count\", \"count of non-zero revenue\", \"mean\"]\ncnt_srs = cnt_srs.sort_values(by=\"count\", ascending=False)\ntrace4 = horizontal_bar_chart(cnt_srs[\"count\"], 'orange')\ntrace5 = horizontal_bar_chart(cnt_srs[\"count of non-zero revenue\"], 'orange')\ntrace6 = horizontal_bar_chart(cnt_srs[\"mean\"], 'orange')\n\n# Network domain\ncnt_srs = train_df.groupby('geoNetwork.networkDomain')['totals.transactionRevenue'].agg(['size', 'count', 'mean'])\ncnt_srs.columns = [\"count\", \"count of non-zero revenue\", \"mean\"]\ncnt_srs = cnt_srs.sort_values(by=\"count\", ascending=False)\ntrace7 = horizontal_bar_chart(cnt_srs[\"count\"].head(10), 'blue')\ntrace8 = horizontal_bar_chart(cnt_srs[\"count of non-zero revenue\"].head(10), 'blue')\ntrace9 = horizontal_bar_chart(cnt_srs[\"mean\"].head(10), 'blue')\n\n# Creating two subplots\nfig = tools.make_subplots(rows=3, cols=3, vertical_spacing=0.08, horizontal_spacing=0.15, \n                          subplot_titles=[\"Continent - Count\", \"Continent - Non-zero Revenue Count\", \"Continent - Mean Revenue\",\n                                          \"Sub Continent - Count\",  \"Sub Continent - Non-zero Revenue Count\", \"Sub Continent - Mean Revenue\",\n                                          \"Network Domain - Count\", \"Network Domain - Non-zero Revenue Count\", \"Network Domain - Mean Revenue\"])\n\nfig.append_trace(trace1, 1, 1)\nfig.append_trace(trace2, 1, 2)\nfig.append_trace(trace3, 1, 3)\nfig.append_trace(trace4, 2, 1)\nfig.append_trace(trace5, 2, 2)\nfig.append_trace(trace6, 2, 3)\nfig.append_trace(trace7, 3, 1)\nfig.append_trace(trace8, 3, 2)\nfig.append_trace(trace9, 3, 3)\n\nfig['layout'].update(height=1500, width=1200, paper_bgcolor='rgb(233,233,233)', title=\"Geography Plots\")\npy.iplot(fig, filename='geo-plots')", "processed": ["test set date 2 aug 2017 30 apr 2018 common date train test set might good idea time base valid dataset geograph inform"]}, {"markdown": ["Inferences:\n* On the continent plot, we can see that America has both higher number of counts as well as highest number of counts where the revenue is non-zero\n* Though Asia and Europe has high number of counts, the number of non-zero revenue counts from these continents are comparatively low. \n* We can infer the first two points from the sub-continents plot too.\n* If the network domain is \"unknown.unknown\" rather than \"(not set)\", then the number of counts with non-zero revenue tend to be lower. \n\n**Traffic Source:**\n"], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-baseline-ga-customer-revenue\n\n# Continent\ncnt_srs = train_df.groupby('trafficSource.source')['totals.transactionRevenue'].agg(['size', 'count', 'mean'])\ncnt_srs.columns = [\"count\", \"count of non-zero revenue\", \"mean\"]\ncnt_srs = cnt_srs.sort_values(by=\"count\", ascending=False)\ntrace1 = horizontal_bar_chart(cnt_srs[\"count\"].head(10), 'green')\ntrace2 = horizontal_bar_chart(cnt_srs[\"count of non-zero revenue\"].head(10), 'green')\ntrace3 = horizontal_bar_chart(cnt_srs[\"mean\"].head(10), 'green')\n\n# Sub-continent\ncnt_srs = train_df.groupby('trafficSource.medium')['totals.transactionRevenue'].agg(['size', 'count', 'mean'])\ncnt_srs.columns = [\"count\", \"count of non-zero revenue\", \"mean\"]\ncnt_srs = cnt_srs.sort_values(by=\"count\", ascending=False)\ntrace4 = horizontal_bar_chart(cnt_srs[\"count\"], 'purple')\ntrace5 = horizontal_bar_chart(cnt_srs[\"count of non-zero revenue\"], 'purple')\ntrace6 = horizontal_bar_chart(cnt_srs[\"mean\"], 'purple')\n\n# Creating two subplots\nfig = tools.make_subplots(rows=2, cols=3, vertical_spacing=0.08, horizontal_spacing=0.15, \n                          subplot_titles=[\"Traffic Source - Count\", \"Traffic Source - Non-zero Revenue Count\", \"Traffic Source - Mean Revenue\",\n                                          \"Traffic Source Medium - Count\",  \"Traffic Source Medium - Non-zero Revenue Count\", \"Traffic Source Medium - Mean Revenue\"\n                                          ])\n\nfig.append_trace(trace1, 1, 1)\nfig.append_trace(trace2, 1, 2)\nfig.append_trace(trace3, 1, 3)\nfig.append_trace(trace4, 2, 1)\nfig.append_trace(trace5, 2, 2)\nfig.append_trace(trace6, 2, 3)\n\nfig['layout'].update(height=1000, width=1200, paper_bgcolor='rgb(233,233,233)', title=\"Traffic Source Plots\")\npy.iplot(fig, filename='traffic-source-plots')", "processed": ["infer contin plot see america higher number count well highest number count revenu non zero though asia europ high number count number non zero revenu count contin compar low infer first two point sub contin plot network domain unknown unknown rather set number count non zero revenu tend lower traffic sourc"]}, {"markdown": ["Inferences:\n* In the traffic source plot, though Youtube has high number of counts in the dataset, the number of non-zero revenue counts are very less. \n* Google plex has a high ratio of non-zero revenue count to total count in the traffic source plot. \n* On the traffic source medium, \"referral\" has more number of non-zero revenue count compared to \"organic\" medium.\n\n**Visitor Profile:**\n\nNow let us look at the visitor profile variables like number of pageviews by the visitor, number of hits by the visitor and see how they look."], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-baseline-ga-customer-revenue\n\n\n# Page views\ncnt_srs = train_df.groupby('totals.pageviews')['totals.transactionRevenue'].agg(['size', 'count', 'mean'])\ncnt_srs.columns = [\"count\", \"count of non-zero revenue\", \"mean\"]\ncnt_srs = cnt_srs.sort_values(by=\"count\", ascending=False)\ntrace1 = horizontal_bar_chart(cnt_srs[\"count\"].head(60), 'cyan')\ntrace2 = horizontal_bar_chart(cnt_srs[\"count of non-zero revenue\"].head(60), 'cyan')\ntrace5 = horizontal_bar_chart(cnt_srs[\"mean\"].head(60), 'cyan')\n\n# Hits\ncnt_srs = train_df.groupby('totals.hits')['totals.transactionRevenue'].agg(['size', 'count', 'mean'])\ncnt_srs.columns = [\"count\", \"count of non-zero revenue\", 'mean']\ncnt_srs = cnt_srs.sort_values(by=\"count\", ascending=False)\ntrace3 = horizontal_bar_chart(cnt_srs[\"count\"].head(60), 'black')\ntrace4 = horizontal_bar_chart(cnt_srs[\"count of non-zero revenue\"].head(60), 'black')\ntrace6 = horizontal_bar_chart(cnt_srs[\"mean\"].head(60), 'black')\n\n# Creating two subplots\nfig = tools.make_subplots(rows=2, cols=3, vertical_spacing=0.08, horizontal_spacing=0.15, \n                          subplot_titles=[\"Total Pageviews - Count\", \"Total Pageviews - Non-zero Revenue Count\", \"Total Pageviews - Mean Revenue\",\n                                          \"Total Hits - Count\",  \"Total Hits - Non-zero Revenue Count\", \"Total Hits - Mean Revenue\"])\n\nfig.append_trace(trace1, 1, 1)\nfig.append_trace(trace2, 1, 2)\nfig.append_trace(trace5, 1, 3)\nfig.append_trace(trace3, 2, 1)\nfig.append_trace(trace4, 2, 2)\nfig.append_trace(trace6, 2, 3)\n\nfig['layout'].update(height=1200, width=900, paper_bgcolor='rgb(233,233,233)', title=\"Visitor Profile Plots\")\npy.iplot(fig, filename='visitor-profile-plots')", "processed": ["infer traffic sourc plot though youtub high number count dataset number non zero revenu count le googl plex high ratio non zero revenu count total count traffic sourc plot traffic sourc medium referr number non zero revenu count compar organ medium visitor profil let u look visitor profil variabl like number pageview visitor number hit visitor see look"]}, {"markdown": ["## Basic EDA", "### Number of characters in the sentence"], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/jigsaw-competition-eda-and-modeling\n\nlengths = train_df[TEXT_COL].apply(len)\ntrain_df['lengths'] = lengths\nlengths = train_df.loc[train_df['lengths']<1125]['lengths']\nsns.distplot(lengths, color='r')\nplt.show()", "processed": ["basic eda", "number charact sentenc"]}, {"markdown": ["Here, we seem to have a bimodal distribution of character length in the data. Although the lengths seem to be heavily skewed to the lower lengths, we see another clear peak around the 1000 character mark.", "### Number of words in the sentence"], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/jigsaw-competition-eda-and-modeling\n\nwords = train_df[TEXT_COL].apply(lambda x: len(x) - len(''.join(x.split())) + 1)\ntrain_df['words'] = words\nwords = train_df.loc[train_df['words']<200]['words']\nsns.distplot(words, color='g')\nplt.show()", "processed": ["seem bimod distribut charact length data although length seem heavili skew lower length see anoth clear peak around 1000 charact mark", "number word sentenc"]}, {"markdown": ["It looks like we have a clear unimodal left-skewed distribution of the number of words in the data.", "### Average Word Length"], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/jigsaw-competition-eda-and-modeling\n\navg_word_len = train_df[TEXT_COL].apply(lambda x: 1.0*len(''.join(x.split()))/(len(x) - len(''.join(x.split())) + 1))\ntrain_df['avg_word_len'] = avg_word_len\navg_word_len = train_df.loc[train_df['avg_word_len']<10]['avg_word_len']\nsns.distplot(avg_word_len, color='b')\nplt.show()", "processed": ["look like clear unimod left skew distribut number word data", "averag word length"]}, {"markdown": ["We have a simple bell-shaped normal distribution of the average word length with a mean of around 4.5", "### Sentiment (negativity)\n#### The orange distribution is label 0 (non-toxic) and the purple distribution is label 1 (toxic)."], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/jigsaw-competition-eda-and-modeling\n\n# SIA = SentimentIntensityAnalyzer()\n# polarity_0 = train_df.loc[train_df.target<0.5][TEXT_COL].apply(lambda x: SIA.polarity_scores(x))\n# polarity_1 = train_df.loc[train_df.target>0.5][TEXT_COL].apply(lambda x: SIA.polarity_scores(x))\n# sns.distplot([polarity['neg'] for polarity in polarity_0], color='darkorange')\n# sns.distplot([polarity['neg'] for polarity in polarity_1], color='purple')\n# plt.show()", "processed": ["simpl bell shape normal distribut averag word length mean around 4 5", "sentiment neg orang distribut label 0 non toxic purpl distribut label 1 toxic"]}, {"markdown": ["\nClearly, the purple distribution has a higher mean than the orange distribution. Both distributions have a somewhat equal spread (standard deviation). Although both distributions are skewed leftwards, the orange distribution has a stronger leftward skew.", "This shows that the toxic comments generally tend to be more negative on average. This is probably because most comments considered \"toxic\" generally spread negative emotions like hate, anger or insult against certain people, beliefs or opinions.", "### Sentiment (positivity)"], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/jigsaw-competition-eda-and-modeling\n\n# sns.distplot([polarity['pos'] for polarity in polarity_0], color='darkorange')\n# sns.distplot([polarity['pos'] for polarity in polarity_1], color='purple')\n# plt.show()", "processed": ["clearli purpl distribut higher mean orang distribut distribut somewhat equal spread standard deviat although distribut skew leftward orang distribut stronger leftward skew", "show toxic comment gener tend neg averag probabl comment consid toxic gener spread neg emot like hate anger insult certain peopl belief opinion", "sentiment posit"]}, {"markdown": ["Both the orange and purple distributions are very similar in every way : bimodality, shape, skew, mean etc.", "This shows that there is no significant difference between positivity in toxic and non-toxic comments.", "### Sentiment (neutrality)"], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/jigsaw-competition-eda-and-modeling\n\n# sns.distplot([polarity['neu'] for polarity in polarity_0], color='darkorange')\n# sns.distplot([polarity['neu'] for polarity in polarity_1], color='purple')\n# plt.show()", "processed": ["orang purpl distribut similar everi way bimod shape skew mean etc", "show signific differ posit toxic non toxic comment", "sentiment neutral"]}, {"markdown": ["The orange distribution clearly has a higher mean than the purple distribution. Both distributions have a somewhat equal spread (standard deviation). Although both distributions are skewed rightwards, the orange distribution has a stronger rightward skew.", "This shows that the non-toxic samples generally tend to be more neutral on average. This is probably because non-toxic comments generally do not have extreme emotions (positive or negative), but many toxic comments do.", "### Sentiment (compoundness / complexity of comment)"], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/jigsaw-competition-eda-and-modeling\n\n# sns.distplot([polarity['compound'] for polarity in polarity_0], color='darkorange')\n# sns.distplot([polarity['compound'] for polarity in polarity_1], color='purple')\n# plt.show()", "processed": ["orang distribut clearli higher mean purpl distribut distribut somewhat equal spread standard deviat although distribut skew rightward orang distribut stronger rightward skew", "show non toxic sampl gener tend neutral averag probabl non toxic comment gener extrem emot posit neg mani toxic comment", "sentiment compound complex comment"]}, {"markdown": ["## Exploring the Target column"], "code": "# Reference: https://www.kaggle.com/code/parulpandey/melanoma-classification-eda-starter\n\ntrain_df['target'].value_counts()\ntrain_df['target'].value_counts(normalize=True).iplot(kind='bar',\n                                                      yTitle='Percentage', \n                                                      linecolor='black', \n                                                      opacity=0.7,\n                                                      color='red',\n                                                      theme='pearl',\n                                                      bargap=0.8,\n                                                      gridcolor='white',\n                                                     \n                                                      title='Distribution of the Target column in the training set')", "processed": ["explor target column"]}, {"markdown": ["## Gender wise distribution\n"], "code": "# Reference: https://www.kaggle.com/code/parulpandey/melanoma-classification-eda-starter\n\ntrain_df['sex'].value_counts(normalize=True)\ntrain_df['sex'].value_counts(normalize=True).iplot(kind='bar',\n                                                      yTitle='Percentage', \n                                                      linecolor='black', \n                                                      opacity=0.7,\n                                                      color='green',\n                                                      theme='pearl',\n                                                      bargap=0.8,\n                                                      gridcolor='white',\n                                                     \n                                                      title='Distribution of the Sex column in the training set')", "processed": ["gender wise distribut"]}, {"markdown": ["## Gender vs Target"], "code": "# Reference: https://www.kaggle.com/code/parulpandey/melanoma-classification-eda-starter\n\nz=train_df.groupby(['target','sex'])['benign_malignant'].count().to_frame().reset_index()\nz.style.background_gradient(cmap='Reds')  \nsns.catplot(x='target',y='benign_malignant', hue='sex',data=z,kind='bar')\nplt.ylabel('Count')\nplt.xlabel('benign:0 vs malignant:1')", "processed": ["gender v target"]}, {"markdown": ["## Location of imaged site"], "code": "# Reference: https://www.kaggle.com/code/parulpandey/melanoma-classification-eda-starter\n\ntrain_df['anatom_site_general_challenge'].value_counts(normalize=True).sort_values()\ntrain_df['anatom_site_general_challenge'].value_counts(normalize=True).sort_values().iplot(kind='barh',\n                                                      xTitle='Percentage', \n                                                      linecolor='black', \n                                                      opacity=0.7,\n                                                      color='#FB8072',\n                                                      theme='pearl',\n                                                      bargap=0.2,\n                                                      gridcolor='white',\n                                                      title='Distribution of the imaged site in the training set')", "processed": ["locat imag site"]}, {"markdown": ["## Location of imaged site w.r.t gender"], "code": "# Reference: https://www.kaggle.com/code/parulpandey/melanoma-classification-eda-starter\n\n\nz1=train_df.groupby(['sex','anatom_site_general_challenge'])['benign_malignant'].count().to_frame().reset_index()\nz1.style.background_gradient(cmap='Reds')\nsns.catplot(x='anatom_site_general_challenge',y='benign_malignant', hue='sex',data=z1,kind='bar')\nplt.gcf().set_size_inches(10,8)\nplt.xlabel('location of imaged site')\nplt.xticks(rotation=45,fontsize='10', horizontalalignment='right')\nplt.ylabel('count of melanoma cases')\n", "processed": ["locat imag site w r gender"]}, {"markdown": ["## Age Distribution of patients"], "code": "# Reference: https://www.kaggle.com/code/parulpandey/melanoma-classification-eda-starter\n\ntrain_df['age_approx'].iplot(kind='hist',bins=30,color='orange',xTitle='Age distribution',yTitle='Count')", "processed": ["age distribut patient"]}, {"markdown": ["## Visualising Age KDEs\nSummarizing the data with Density plots to see where the mass of the data is located. [A kernel density estimate plot](https://chemicalstatistician.wordpress.com/2013/06/09/exploratory-data-analysis-kernel-density-estimation-in-r-on-ozone-pollution-data-in-new-york-and-ozonopolis/) shows the distribution of a single variable and can be thought of as a smoothed histogram (it is created by computing a kernel, usually a Gaussian, at each data point and then averaging all the individual kernels to develop a single smooth curve). We will use the seaborn kdeplot for this graph.\n\n### Distribution of Ages w.r.t Target"], "code": "# Reference: https://www.kaggle.com/code/parulpandey/melanoma-classification-eda-starter\n\n# KDE plot of age that were diagnosed as benign\nsns.kdeplot(train_df.loc[train_df['target'] == 0, 'age_approx'], label = 'Benign',shade=True)\n\n# KDE plot of age that were diagnosed as malignant\nsns.kdeplot(train_df.loc[train_df['target'] == 1, 'age_approx'], label = 'Malignant',shade=True)\n\n# Labeling of plot\nplt.xlabel('Age (years)'); plt.ylabel('Density'); plt.title('Distribution of Ages');", "processed": ["visualis age kde summar data densiti plot see mass data locat kernel densiti estim plot http chemicalstatistician wordpress com 2013 06 09 exploratori data analysi kernel densiti estim r ozon pollut data new york ozonopoli show distribut singl variabl thought smooth histogram creat comput kernel usual gaussian data point averag individu kernel develop singl smooth curv use seaborn kdeplot graph distribut age w r target"]}, {"markdown": ["\n### Distribution of Ages w.r.t gender"], "code": "# Reference: https://www.kaggle.com/code/parulpandey/melanoma-classification-eda-starter\n\n# KDE plot of age that were diagnosed as benign\nsns.kdeplot(train_df.loc[train_df['sex'] == 'male', 'age_approx'], label = 'Male',shade=True)\n\n# KDE plot of age that were diagnosed as malignant\nsns.kdeplot(train_df.loc[train_df['sex'] == 'female', 'age_approx'], label = 'Female',shade=True)\n\n# Labeling of plot\nplt.xlabel('Age (years)'); plt.ylabel('Density'); plt.title('Distribution of Ages');\n", "processed": ["distribut age w r gender"]}, {"markdown": ["## Distribution of Diagnosis"], "code": "# Reference: https://www.kaggle.com/code/parulpandey/melanoma-classification-eda-starter\n\ntrain_df['diagnosis'].value_counts()\ntrain_df['diagnosis'].value_counts(normalize=True).sort_values().iplot(kind='barh',\n                                                      xTitle='Percentage', \n                                                      linecolor='black', \n                                                      opacity=0.7,\n                                                      color='blue',\n                                                      theme='pearl',\n                                                      bargap=0.2,\n                                                      gridcolor='white',\n                                                      title='Distribution in the training set')", "processed": ["distribut diagnosi"]}, {"markdown": ["# 4. Visualising Images : JPEG\n\n## Visualizing a random selection of images"], "code": "# Reference: https://www.kaggle.com/code/parulpandey/melanoma-classification-eda-starter\n\nimages = train_df['image_name'].values\n\n# Extract 9 random images from it\nrandom_images = [np.random.choice(images+'.jpg') for i in range(9)]\n\n# Location of the image dir\nimg_dir = IMAGE_PATH+'/jpeg/train'\n\nprint('Display Random Images')\n\n# Adjust the size of your images\nplt.figure(figsize=(10,8))\n\n# Iterate and plot random images\nfor i in range(9):\n    plt.subplot(3, 3, i + 1)\n    img = plt.imread(os.path.join(img_dir, random_images[i]))\n    plt.imshow(img, cmap='gray')\n    plt.axis('off')\n    \n# Adjust subplot parameters to give specified padding\nplt.tight_layout()   ", "processed": ["4 visualis imag jpeg visual random select imag"]}, {"markdown": ["We do see that the JPEG format images vary in sizes", "## Visualizing Images with benign lesions"], "code": "# Reference: https://www.kaggle.com/code/parulpandey/melanoma-classification-eda-starter\n\nbenign = train_df[train_df['benign_malignant']=='benign']\nmalignant = train_df[train_df['benign_malignant']=='malignant']\nimages = benign['image_name'].values\n\n# Extract 9 random images from it\nrandom_images = [np.random.choice(images+'.jpg') for i in range(9)]\n\n# Location of the image dir\nimg_dir = IMAGE_PATH+'/jpeg/train'\n\nprint('Display benign Images')\n\n# Adjust the size of your images\nplt.figure(figsize=(10,8))\n\n# Iterate and plot random images\nfor i in range(9):\n    plt.subplot(3, 3, i + 1)\n    img = plt.imread(os.path.join(img_dir, random_images[i]))\n    plt.imshow(img, cmap='gray')\n    plt.axis('off')\n    \n# Adjust subplot parameters to give specified padding\nplt.tight_layout()   ", "processed": ["see jpeg format imag vari size", "visual imag benign lesion"]}, {"markdown": ["## Visualizing Images with Malignant lesions"], "code": "# Reference: https://www.kaggle.com/code/parulpandey/melanoma-classification-eda-starter\n\nimages = malignant['image_name'].values\n\n# Extract 9 random images from it\nrandom_images = [np.random.choice(images+'.jpg') for i in range(9)]\n\n# Location of the image dir\nimg_dir = IMAGE_PATH+'/jpeg/train'\n\nprint('Display malignant Images')\n\n# Adjust the size of your images\nplt.figure(figsize=(10,8))\n\n# Iterate and plot random images\nfor i in range(9):\n    plt.subplot(3, 3, i + 1)\n    img = plt.imread(os.path.join(img_dir, random_images[i]))\n    plt.imshow(img, cmap='gray')\n    plt.axis('off')\n    \n# Adjust subplot parameters to give specified padding\nplt.tight_layout()   ", "processed": ["visual imag malign lesion"]}, {"markdown": ["## Histograms\n\nHistograms are a graphical representation showing how frequently various color values occur in the image i.e frequency of pixels intensity values. In a RGB color space, pixel values range from 0 to 255 where 0 stands for black and 255 stands for white. Analysis of a histogram can help us understand thee brightness, contrast and intensity distribution of an image. Now let's look at the histogram of a random selected sample from each category.\n\n### Benign category"], "code": "# Reference: https://www.kaggle.com/code/parulpandey/melanoma-classification-eda-starter\n\nf = plt.figure(figsize=(16,8))\nf.add_subplot(1,2, 1)\n\nsample_img = benign['image_name'][0]+'.jpg'\nraw_image = plt.imread(os.path.join(img_dir, sample_img))\nplt.imshow(raw_image, cmap='gray')\nplt.colorbar()\nplt.title('Benign Image')\nprint(f\"Image dimensions:  {raw_image.shape[0],raw_image.shape[1]}\")\nprint(f\"Maximum pixel value : {raw_image.max():.1f} ; Minimum pixel value:{raw_image.min():.1f}\")\nprint(f\"Mean value of the pixels : {raw_image.mean():.1f} ; Standard deviation : {raw_image.std():.1f}\")\n\nf.add_subplot(1,2, 2)\n\n#_ = plt.hist(raw_image.ravel(),bins = 256, color = 'orange',)\n_ = plt.hist(raw_image[:, :, 0].ravel(), bins = 256, color = 'red', alpha = 0.5)\n_ = plt.hist(raw_image[:, :, 1].ravel(), bins = 256, color = 'Green', alpha = 0.5)\n_ = plt.hist(raw_image[:, :, 2].ravel(), bins = 256, color = 'Blue', alpha = 0.5)\n_ = plt.xlabel('Intensity Value')\n_ = plt.ylabel('Count')\n_ = plt.legend(['Red_Channel', 'Green_Channel', 'Blue_Channel'])\nplt.show()", "processed": ["histogram histogram graphic represent show frequent variou color valu occur imag e frequenc pixel intens valu rgb color space pixel valu rang 0 255 0 stand black 255 stand white analysi histogram help u understand thee bright contrast intens distribut imag let look histogram random select sampl categori benign categori"]}, {"markdown": ["### Malignant category"], "code": "# Reference: https://www.kaggle.com/code/parulpandey/melanoma-classification-eda-starter\n\nf = plt.figure(figsize=(16,8))\nf.add_subplot(1,2, 1)\n\nsample_img = malignant['image_name'][235]+'.jpg'\nraw_image = plt.imread(os.path.join(img_dir, sample_img))\nplt.imshow(raw_image, cmap='gray')\nplt.colorbar()\nplt.title('Malignant Image')\nprint(f\"Image dimensions:  {raw_image.shape[0],raw_image.shape[1]}\")\nprint(f\"Maximum pixel value : {raw_image.max():.1f} ; Minimum pixel value:{raw_image.min():.1f}\")\nprint(f\"Mean value of the pixels : {raw_image.mean():.1f} ; Standard deviation : {raw_image.std():.1f}\")\n\nf.add_subplot(1,2, 2)\n\n#_ = plt.hist(raw_image.ravel(),bins = 256, color = 'orange',)\n_ = plt.hist(raw_image[:, :, 0].ravel(), bins = 256, color = 'red', alpha = 0.5)\n_ = plt.hist(raw_image[:, :, 1].ravel(), bins = 256, color = 'Green', alpha = 0.5)\n_ = plt.hist(raw_image[:, :, 2].ravel(), bins = 256, color = 'Blue', alpha = 0.5)\n_ = plt.xlabel('Intensity Value')\n_ = plt.ylabel('Count')\n_ = plt.legend(['Red_Channel', 'Green_Channel', 'Blue_Channel'])\nplt.show()", "processed": ["malign categori"]}, {"markdown": ["## Functions used in this kernel\nThey are in the hidden cell below."], "code": "# Reference: https://www.kaggle.com/code/artgor/eda-and-models\n\nimport os\nimport time\nimport datetime\nimport json\nimport gc\nfrom numba import jit\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm_notebook\n\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom catboost import CatBoostRegressor, CatBoostClassifier\nfrom sklearn import metrics\n\nfrom itertools import product\n\nimport altair as alt\nfrom altair.vega import v5\nfrom IPython.display import HTML\n\n# using ideas from this kernel: https://www.kaggle.com/notslush/altair-visualization-2018-stackoverflow-survey\ndef prepare_altair():\n    \"\"\"\n    Helper function to prepare altair for working.\n    \"\"\"\n\n    vega_url = 'https://cdn.jsdelivr.net/npm/vega@' + v5.SCHEMA_VERSION\n    vega_lib_url = 'https://cdn.jsdelivr.net/npm/vega-lib'\n    vega_lite_url = 'https://cdn.jsdelivr.net/npm/vega-lite@' + alt.SCHEMA_VERSION\n    vega_embed_url = 'https://cdn.jsdelivr.net/npm/vega-embed@3'\n    noext = \"?noext\"\n    \n    paths = {\n        'vega': vega_url + noext,\n        'vega-lib': vega_lib_url + noext,\n        'vega-lite': vega_lite_url + noext,\n        'vega-embed': vega_embed_url + noext\n    }\n    \n    workaround = f\"\"\"    requirejs.config({{\n        baseUrl: 'https://cdn.jsdelivr.net/npm/',\n        paths: {paths}\n    }});\n    \"\"\"\n    \n    return workaround\n    \n\ndef add_autoincrement(render_func):\n    # Keep track of unique <div/> IDs\n    cache = {}\n    def wrapped(chart, id=\"vega-chart\", autoincrement=True):\n        if autoincrement:\n            if id in cache:\n                counter = 1 + cache[id]\n                cache[id] = counter\n            else:\n                cache[id] = 0\n            actual_id = id if cache[id] == 0 else id + '-' + str(cache[id])\n        else:\n            if id not in cache:\n                cache[id] = 0\n            actual_id = id\n        return render_func(chart, id=actual_id)\n    # Cache will stay outside and \n    return wrapped\n           \n\n@add_autoincrement\ndef render(chart, id=\"vega-chart\"):\n    \"\"\"\n    Helper function to plot altair visualizations.\n    \"\"\"\n    chart_str = \"\"\"\n    <div id=\"{id}\"></div><script>\n    require([\"vega-embed\"], function(vg_embed) {{\n        const spec = {chart};     \n        vg_embed(\"#{id}\", spec, {{defaultStyle: true}}).catch(console.warn);\n        console.log(\"anything?\");\n    }});\n    console.log(\"really...anything?\");\n    </script>\n    \"\"\"\n    return HTML(\n        chart_str.format(\n            id=id,\n            chart=json.dumps(chart) if isinstance(chart, dict) else chart.to_json(indent=None)\n        )\n    )\n    \n\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage(deep=True).sum() / 1024**2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                c_prec = df[col].apply(lambda x: np.finfo(x).precision).max()\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max and c_prec == np.finfo(np.float32).precision:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage(deep=True).sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df\n    \n\n@jit\ndef fast_auc(y_true, y_prob):\n    \"\"\"\n    fast roc_auc computation: https://www.kaggle.com/c/microsoft-malware-prediction/discussion/76013\n    \"\"\"\n    y_true = np.asarray(y_true)\n    y_true = y_true[np.argsort(y_prob)]\n    nfalse = 0\n    auc = 0\n    n = len(y_true)\n    for i in range(n):\n        y_i = y_true[i]\n        nfalse += (1 - y_i)\n        auc += y_i * nfalse\n    auc /= (nfalse * (n - nfalse))\n    return auc\n\n\ndef eval_auc(y_true, y_pred):\n    \"\"\"\n    Fast auc eval function for lgb.\n    \"\"\"\n    return 'auc', fast_auc(y_true, y_pred), True\n\n\ndef group_mean_log_mae(y_true, y_pred, types, floor=1e-9):\n    \"\"\"\n    Fast metric computation for this competition: https://www.kaggle.com/c/champs-scalar-coupling\n    Code is from this kernel: https://www.kaggle.com/uberkinder/efficient-metric\n    \"\"\"\n    maes = (y_true-y_pred).abs().groupby(types).mean()\n    return np.log(maes.map(lambda x: max(x, floor))).mean()\n    \n\ndef train_model_regression(X, X_test, y, params, folds=None, model_type='lgb', eval_metric='mae', columns=None, plot_feature_importance=False, model=None,\n                               verbose=10000, early_stopping_rounds=200, n_estimators=50000, splits=None, n_folds=3):\n    \"\"\"\n    A function to train a variety of regression models.\n    Returns dictionary with oof predictions, test predictions, scores and, if necessary, feature importances.\n    \n    :params: X - training data, can be pd.DataFrame or np.ndarray (after normalizing)\n    :params: X_test - test data, can be pd.DataFrame or np.ndarray (after normalizing)\n    :params: y - target\n    :params: folds - folds to split data\n    :params: model_type - type of model to use\n    :params: eval_metric - metric to use\n    :params: columns - columns to use. If None - use all columns\n    :params: plot_feature_importance - whether to plot feature importance of LGB\n    :params: model - sklearn model, works only for \"sklearn\" model type\n    \n    \"\"\"\n    columns = X.columns if columns is None else columns\n    X_test = X_test[columns]\n    splits = folds.split(X) if splits is None else splits\n    n_splits = folds.n_splits if splits is None else n_folds\n    \n    # to set up scoring parameters\n    metrics_dict = {'mae': {'lgb_metric_name': 'mae',\n                        'catboost_metric_name': 'MAE',\n                        'sklearn_scoring_function': metrics.mean_absolute_error},\n                    'group_mae': {'lgb_metric_name': 'mae',\n                        'catboost_metric_name': 'MAE',\n                        'scoring_function': group_mean_log_mae},\n                    'mse': {'lgb_metric_name': 'mse',\n                        'catboost_metric_name': 'MSE',\n                        'sklearn_scoring_function': metrics.mean_squared_error}\n                    }\n\n    \n    result_dict = {}\n    \n    # out-of-fold predictions on train data\n    oof = np.zeros(len(X))\n    \n    # averaged predictions on train data\n    prediction = np.zeros(len(X_test))\n    \n    # list of scores on folds\n    scores = []\n    feature_importance = pd.DataFrame()\n    \n    # split and train on folds\n    for fold_n, (train_index, valid_index) in enumerate(splits):\n        if verbose:\n            print(f'Fold {fold_n + 1} started at {time.ctime()}')\n        if type(X) == np.ndarray:\n            X_train, X_valid = X[columns][train_index], X[columns][valid_index]\n            y_train, y_valid = y[train_index], y[valid_index]\n        else:\n            X_train, X_valid = X[columns].iloc[train_index], X[columns].iloc[valid_index]\n            y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n            \n        if model_type == 'lgb':\n            model = lgb.LGBMRegressor(**params, n_estimators = n_estimators, n_jobs = -1)\n            model.fit(X_train, y_train, \n                    eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric=metrics_dict[eval_metric]['lgb_metric_name'],\n                    verbose=verbose, early_stopping_rounds=early_stopping_rounds)\n            \n            y_pred_valid = model.predict(X_valid)\n            y_pred = model.predict(X_test, num_iteration=model.best_iteration_)\n            \n        if model_type == 'xgb':\n            train_data = xgb.DMatrix(data=X_train, label=y_train, feature_names=X.columns)\n            valid_data = xgb.DMatrix(data=X_valid, label=y_valid, feature_names=X.columns)\n\n            watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]\n            model = xgb.train(dtrain=train_data, num_boost_round=20000, evals=watchlist, early_stopping_rounds=200, verbose_eval=verbose, params=params)\n            y_pred_valid = model.predict(xgb.DMatrix(X_valid, feature_names=X.columns), ntree_limit=model.best_ntree_limit)\n            y_pred = model.predict(xgb.DMatrix(X_test, feature_names=X.columns), ntree_limit=model.best_ntree_limit)\n        \n        if model_type == 'sklearn':\n            model = model\n            model.fit(X_train, y_train)\n            \n            y_pred_valid = model.predict(X_valid).reshape(-1,)\n            score = metrics_dict[eval_metric]['sklearn_scoring_function'](y_valid, y_pred_valid)\n            print(f'Fold {fold_n}. {eval_metric}: {score:.4f}.')\n            print('')\n            \n            y_pred = model.predict(X_test).reshape(-1,)\n        \n        if model_type == 'cat':\n            model = CatBoostRegressor(iterations=20000,  eval_metric=metrics_dict[eval_metric]['catboost_metric_name'], **params,\n                                      loss_function=metrics_dict[eval_metric]['catboost_metric_name'])\n            model.fit(X_train, y_train, eval_set=(X_valid, y_valid), cat_features=[], use_best_model=True, verbose=False)\n\n            y_pred_valid = model.predict(X_valid)\n            y_pred = model.predict(X_test)\n        \n        oof[valid_index] = y_pred_valid.reshape(-1,)\n        if eval_metric != 'group_mae':\n            scores.append(metrics_dict[eval_metric]['sklearn_scoring_function'](y_valid, y_pred_valid))\n        else:\n            scores.append(metrics_dict[eval_metric]['scoring_function'](y_valid, y_pred_valid, X_valid['type']))\n\n        prediction += y_pred    \n        \n        if model_type == 'lgb' and plot_feature_importance:\n            # feature importance\n            fold_importance = pd.DataFrame()\n            fold_importance[\"feature\"] = columns\n            fold_importance[\"importance\"] = model.feature_importances_\n            fold_importance[\"fold\"] = fold_n + 1\n            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n\n    prediction /= n_splits\n    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n    \n    result_dict['oof'] = oof\n    result_dict['prediction'] = prediction\n    result_dict['scores'] = scores\n    \n    if model_type == 'lgb':\n        if plot_feature_importance:\n            feature_importance[\"importance\"] /= n_splits\n            cols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n                by=\"importance\", ascending=False)[:50].index\n\n            best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n\n            plt.figure(figsize=(16, 12));\n            sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False));\n            plt.title('LGB Features (avg over folds)');\n            \n            result_dict['feature_importance'] = feature_importance\n        \n    return result_dict\n    \n\n\ndef train_model_classification(X, X_test, y, params, folds, model_type='lgb', eval_metric='auc', columns=None, plot_feature_importance=False, model=None,\n                               verbose=10000, early_stopping_rounds=200, n_estimators=50000, splits=None, n_folds=3, averaging='usual', n_jobs=-1):\n    \"\"\"\n    A function to train a variety of classification models.\n    Returns dictionary with oof predictions, test predictions, scores and, if necessary, feature importances.\n    \n    :params: X - training data, can be pd.DataFrame or np.ndarray (after normalizing)\n    :params: X_test - test data, can be pd.DataFrame or np.ndarray (after normalizing)\n    :params: y - target\n    :params: folds - folds to split data\n    :params: model_type - type of model to use\n    :params: eval_metric - metric to use\n    :params: columns - columns to use. If None - use all columns\n    :params: plot_feature_importance - whether to plot feature importance of LGB\n    :params: model - sklearn model, works only for \"sklearn\" model type\n    \n    \"\"\"\n    columns = X.columns if columns is None else columns\n    n_splits = folds.n_splits if splits is None else n_folds\n    X_test = X_test[columns]\n    \n    # to set up scoring parameters\n    metrics_dict = {'auc': {'lgb_metric_name': eval_auc,\n                        'catboost_metric_name': 'AUC',\n                        'sklearn_scoring_function': metrics.roc_auc_score},\n                    }\n    \n    result_dict = {}\n    if averaging == 'usual':\n        # out-of-fold predictions on train data\n        oof = np.zeros((len(X), 1))\n\n        # averaged predictions on train data\n        prediction = np.zeros((len(X_test), 1))\n        \n    elif averaging == 'rank':\n        # out-of-fold predictions on train data\n        oof = np.zeros((len(X), 1))\n\n        # averaged predictions on train data\n        prediction = np.zeros((len(X_test), 1))\n\n    \n    # list of scores on folds\n    scores = []\n    feature_importance = pd.DataFrame()\n    \n    # split and train on folds\n    for fold_n, (train_index, valid_index) in enumerate(folds.split(X)):\n        print(f'Fold {fold_n + 1} started at {time.ctime()}')\n        if type(X) == np.ndarray:\n            X_train, X_valid = X[columns][train_index], X[columns][valid_index]\n            y_train, y_valid = y[train_index], y[valid_index]\n        else:\n            X_train, X_valid = X[columns].iloc[train_index], X[columns].iloc[valid_index]\n            y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n            \n        if model_type == 'lgb':\n            model = lgb.LGBMClassifier(**params, n_estimators=n_estimators, n_jobs = n_jobs)\n            model.fit(X_train, y_train, \n                    eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric=metrics_dict[eval_metric]['lgb_metric_name'],\n                    verbose=verbose, early_stopping_rounds=early_stopping_rounds)\n            \n            y_pred_valid = model.predict_proba(X_valid)[:, 1]\n            y_pred = model.predict_proba(X_test, num_iteration=model.best_iteration_)[:, 1]\n            \n        if model_type == 'xgb':\n            train_data = xgb.DMatrix(data=X_train, label=y_train, feature_names=X.columns)\n            valid_data = xgb.DMatrix(data=X_valid, label=y_valid, feature_names=X.columns)\n\n            watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]\n            model = xgb.train(dtrain=train_data, num_boost_round=n_estimators, evals=watchlist, early_stopping_rounds=early_stopping_rounds, verbose_eval=verbose, params=params)\n            y_pred_valid = model.predict(xgb.DMatrix(X_valid, feature_names=X.columns), ntree_limit=model.best_ntree_limit)\n            y_pred = model.predict(xgb.DMatrix(X_test, feature_names=X.columns), ntree_limit=model.best_ntree_limit)\n        \n        if model_type == 'sklearn':\n            model = model\n            model.fit(X_train, y_train)\n            \n            y_pred_valid = model.predict(X_valid).reshape(-1,)\n            score = metrics_dict[eval_metric]['sklearn_scoring_function'](y_valid, y_pred_valid)\n            print(f'Fold {fold_n}. {eval_metric}: {score:.4f}.')\n            print('')\n            \n            y_pred = model.predict_proba(X_test)\n        \n        if model_type == 'cat':\n            model = CatBoostClassifier(iterations=n_estimators, eval_metric=metrics_dict[eval_metric]['catboost_metric_name'], **params,\n                                      loss_function=Logloss)\n            model.fit(X_train, y_train, eval_set=(X_valid, y_valid), cat_features=[], use_best_model=True, verbose=False)\n\n            y_pred_valid = model.predict(X_valid)\n            y_pred = model.predict(X_test)\n        \n        if averaging == 'usual':\n            \n            oof[valid_index] = y_pred_valid.reshape(-1, 1)\n            scores.append(metrics_dict[eval_metric]['sklearn_scoring_function'](y_valid, y_pred_valid))\n            \n            prediction += y_pred.reshape(-1, 1)\n\n        elif averaging == 'rank':\n                                  \n            oof[valid_index] = y_pred_valid.reshape(-1, 1)\n            scores.append(metrics_dict[eval_metric]['sklearn_scoring_function'](y_valid, y_pred_valid))\n                                  \n            prediction += pd.Series(y_pred).rank().values.reshape(-1, 1)        \n        \n        if model_type == 'lgb' and plot_feature_importance:\n            # feature importance\n            fold_importance = pd.DataFrame()\n            fold_importance[\"feature\"] = columns\n            fold_importance[\"importance\"] = model.feature_importances_\n            fold_importance[\"fold\"] = fold_n + 1\n            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n\n    prediction /= n_splits\n    \n    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n    \n    result_dict['oof'] = oof\n    result_dict['prediction'] = prediction\n    result_dict['scores'] = scores\n    \n    if model_type == 'lgb':\n        if plot_feature_importance:\n            feature_importance[\"importance\"] /= n_splits\n            cols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n                by=\"importance\", ascending=False)[:50].index\n\n            best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n\n            plt.figure(figsize=(16, 12));\n            sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False));\n            plt.title('LGB Features (avg over folds)');\n            \n            result_dict['feature_importance'] = feature_importance\n            result_dict['top_columns'] = cols\n        \n    return result_dict\n\n# setting up altair\nworkaround = prepare_altair()\nHTML(\"\".join((\n    \"<script>\",\n    workaround,\n    \"</script>\",\n)))", "processed": ["function use kernel hidden cell"]}, {"markdown": ["### Predict revenues at session level"], "code": "# Reference: https://www.kaggle.com/code/ogrellier/using-classification-for-predictions\n\ntrain_features = [_f for _f in train.columns if _f not in excluded_features] + ['non_zero_proba']\nprint(train_features)\n\noof_reg_preds = np.zeros(train.shape[0])\nsub_reg_preds = np.zeros(test.shape[0])\nimportances = pd.DataFrame()\n\nfor fold_, (trn_, val_) in enumerate(folds.split(y_reg, y_reg, groups=train['fullVisitorId'])):\n    trn_x, trn_y = train[train_features].iloc[trn_], y_reg.iloc[trn_].fillna(0)\n    val_x, val_y = train[train_features].iloc[val_], y_reg.iloc[val_].fillna(0)\n    \n    reg = lgb.LGBMRegressor(\n        num_leaves=31,\n        learning_rate=0.03,\n        n_estimators=1000,\n        subsample=.9,\n        colsample_bytree=.9,\n        random_state=1\n    )\n    reg.fit(\n        trn_x, np.log1p(trn_y),\n        eval_set=[(val_x, np.log1p(val_y))],\n        early_stopping_rounds=50,\n        verbose=50\n    )\n    imp_df = pd.DataFrame()\n    imp_df['feature'] = train_features\n    imp_df['gain'] = reg.booster_.feature_importance(importance_type='gain')\n    \n    imp_df['fold'] = fold_ + 1\n    importances = pd.concat([importances, imp_df], axis=0, sort=False)\n    \n    oof_reg_preds[val_] = reg.predict(val_x, num_iteration=reg.best_iteration_)\n    oof_reg_preds[oof_reg_preds < 0] = 0\n    _preds = reg.predict(test[train_features], num_iteration=reg.best_iteration_)\n    _preds[_preds < 0] = 0\n    sub_reg_preds += np.expm1(_preds) / folds.n_splits\n    \nmean_squared_error(np.log1p(y_reg.fillna(0)), oof_reg_preds) ** .5\nimport warnings\nwarnings.simplefilter('ignore', FutureWarning)\n\nimportances['gain_log'] = np.log1p(importances['gain'])\nmean_gain = importances[['gain', 'feature']].groupby('feature').mean()\nimportances['mean_gain'] = importances['feature'].map(mean_gain['gain'])\n\nplt.figure(figsize=(8, 12))\nsns.barplot(x='gain_log', y='feature', data=importances.sort_values('mean_gain', ascending=False))", "processed": ["predict revenu session level"]}, {"markdown": ["### Plot Actual Dollar estimates per dates"], "code": "# Reference: https://www.kaggle.com/code/ogrellier/using-classification-for-predictions\n\n# Go to actual revenues\ntrain['PredictedRevenue'] = np.expm1(oof_reg_preds)\ntest['PredictedRevenue'] = sub_reg_preds\ntrain['totals.transactionRevenue'] = y_reg\n\n# Sum by date on train and test\ntrn_group = train[['date', 'PredictedRevenue', 'totals.transactionRevenue']].groupby('date').sum().reset_index()\nsub_group = test[['date', 'PredictedRevenue']].groupby('date').sum().reset_index()\n\n# Now plot all this\nyears = mdates.YearLocator()   # every year\nmonths = mdates.MonthLocator()  # every month\nyearsFmt = mdates.DateFormatter('%Y-%m')\n\nfig, ax = plt.subplots(figsize=(15, 6))\nax.set_title('Actual Dollar Revenues - we are way off...', fontsize=15, fontweight='bold')\nax.plot(pd.to_datetime(trn_group['date']).values, trn_group['totals.transactionRevenue'].values)\nax.plot(pd.to_datetime(trn_group['date']).values, trn_group['PredictedRevenue'].values)\nax.plot(pd.to_datetime(sub_group['date']).values, sub_group['PredictedRevenue'].values)\n\n# # format the ticks\nax.xaxis.set_major_locator(months)\nax.xaxis.set_major_formatter(yearsFmt)\nax.xaxis.set_minor_locator(months)\n\nax.format_xdata = mdates.DateFormatter('%Y-%m-%d')\n# # ax.format_ydata = price\nax.grid(True)\n\n# rotates and right aligns the x labels, and moves the bottom of the\n# axes up to make room for them\nfig.autofmt_xdate()", "processed": ["plot actual dollar estim per date"]}, {"markdown": ["### Display using np.log1p"], "code": "# Reference: https://www.kaggle.com/code/ogrellier/using-classification-for-predictions\n\n# Go to actual revenues\ntrain['PredictedRevenue'] = np.expm1(oof_reg_preds)\ntest['PredictedRevenue'] = sub_reg_preds\ntrain['totals.transactionRevenue'] = y_reg\n\n# Sum by date on train and test\ntrn_group = train[['date', 'PredictedRevenue', 'totals.transactionRevenue']].groupby('date').sum().reset_index()\nsub_group = test[['date', 'PredictedRevenue']].groupby('date').sum().reset_index()\n\nyears = mdates.YearLocator()   # every year\nmonths = mdates.MonthLocator()  # every month\nyearsFmt = mdates.DateFormatter('%Y-%m')\n\nfig, ax = plt.subplots(figsize=(15, 6))\nax.set_title('We are also off in logs... or am I just stupid ?', fontsize=15, fontweight='bold')\nax.plot(pd.to_datetime(trn_group['date']).values, np.log1p(trn_group['totals.transactionRevenue'].values))\nax.plot(pd.to_datetime(trn_group['date']).values, np.log1p(trn_group['PredictedRevenue'].values))\nax.plot(pd.to_datetime(sub_group['date']).values, np.log1p(sub_group['PredictedRevenue'].values))\n\n# # format the ticks\nax.xaxis.set_major_locator(months)\nax.xaxis.set_major_formatter(yearsFmt)\nax.xaxis.set_minor_locator(months)\n\nax.format_xdata = mdates.DateFormatter('%Y-%m-%d')\n# # ax.format_ydata = price\nax.grid(True)\n\n# rotates and right aligns the x labels, and moves the bottom of the\n# axes up to make room for them\nfig.autofmt_xdate()", "processed": ["display use np log1p"]}, {"markdown": ["### Using sum of logs - no really ?"], "code": "# Reference: https://www.kaggle.com/code/ogrellier/using-classification-for-predictions\n\n# Keep amounts in logs\ntrain['PredictedRevenue'] = oof_reg_preds\ntest['PredictedRevenue'] = np.log1p(sub_reg_preds)\ntrain['totals.transactionRevenue'] = np.log1p(y_reg)\n\n# You really mean summing up the logs ???\ntrn_group = train[['date', 'PredictedRevenue', 'totals.transactionRevenue']].groupby('date').sum().reset_index()\nsub_group = test[['date', 'PredictedRevenue']].groupby('date').sum().reset_index()\n\nyears = mdates.YearLocator()   # every year\nmonths = mdates.MonthLocator()  # every month\nyearsFmt = mdates.DateFormatter('%Y-%m')\n\nfig, ax = plt.subplots(figsize=(15, 6))\nax.set_title('Summing up logs looks a lot better !?! Is the challenge to find the correct metric ???', fontsize=15, fontweight='bold')\nax.plot(pd.to_datetime(trn_group['date']).values, trn_group['totals.transactionRevenue'].values)\nax.plot(pd.to_datetime(trn_group['date']).values, trn_group['PredictedRevenue'].values)\nax.plot(pd.to_datetime(sub_group['date']).values, sub_group['PredictedRevenue'].values)\n\n# # format the ticks\nax.xaxis.set_major_locator(months)\nax.xaxis.set_major_formatter(yearsFmt)\nax.xaxis.set_minor_locator(months)\n\nax.format_xdata = mdates.DateFormatter('%Y-%m-%d')\n# # ax.format_ydata = price\nax.grid(True)\n\n# rotates and right aligns the x labels, and moves the bottom of the\n# axes up to make room for them\nfig.autofmt_xdate()", "processed": ["use sum log realli"]}, {"markdown": ["Whoa, that's a pretty significant AUC!  0.999996 adverserial AUC is the biggest one I've ever come across. I first thought I might be making a mistake, but re-run this script several times, and don't seem to find any bugs in it. But I am open to criticims/suggestions.\n\nLet's look now at the top 20 \"adversarial\" features."], "code": "# Reference: https://www.kaggle.com/code/tunguz/adversarial-ieee\n\nfeature_imp = pd.DataFrame(sorted(zip(clf.feature_importance(),features)), columns=['Value','Feature'])\n\nplt.figure(figsize=(20, 10))\nsns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False).head(20))\nplt.title('LightGBM Features')\nplt.tight_layout()\nplt.show()\nplt.savefig('lgbm_importances-01.png')", "processed": ["whoa pretti signific auc 0 999996 adverseri auc biggest one ever come across first thought might make mistak run script sever time seem find bug open criticim suggest let look top 20 adversari featur"]}, {"markdown": ["Seems that transaction date is the main \"culprit\".\n\nLet's see what happens when we remove time stamp."], "code": "# Reference: https://www.kaggle.com/code/tunguz/adversarial-ieee\n\ndel train, test, clf\ngc.collect()\n\ntrain = pd.read_csv('../input/standalone-train-and-test-preprocessing/train.csv')\n\n\ntest = pd.read_csv('../input/standalone-train-and-test-preprocessing/test.csv')\n\nfeatures = test.columns[1:]\ntrain = train[features]\ntest = test[features]\n\ntrain['target'] = 0\ntest['target'] = 1\n\ntrain_test = pd.concat([train, test], axis =0)\n\ndel train, test\n\ntarget = train_test['target'].values\n\n# Label Encoding\nfor f in object_columns:\n    lbl = preprocessing.LabelEncoder()\n    lbl.fit(list(train_test[f].values) )\n    train_test[f] = lbl.transform(list(train_test[f].values))\n    \ntrain, test = model_selection.train_test_split(train_test, test_size=0.33, random_state=42, shuffle=True)\n\ntrain_y = train['target'].values\ntest_y = test['target'].values\ndel train['target'], test['target']\ngc.collect()\n\ntrain = lgb.Dataset(train, label=train_y)\ntest = lgb.Dataset(test, label=test_y)\n\nclf = lgb.train(param, train, num_round, valid_sets = [train, test], verbose_eval=50, early_stopping_rounds = 50)\n\n\nfeature_imp = pd.DataFrame(sorted(zip(clf.feature_importance(),features)), columns=['Value','Feature'])\n\nplt.figure(figsize=(20, 10))\nsns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False).head(20))\nplt.title('LightGBM Features')\nplt.tight_layout()\nplt.show()\nplt.savefig('lgbm_importances-01.png')", "processed": ["seem transact date main culprit let see happen remov time stamp"]}, {"markdown": ["Also we have the data about sell prices in all 10 stores.", "Now, let's make some plots. I'd like to see sales over time in different shops."], "code": "# Reference: https://www.kaggle.com/code/artgor/it-is-time-for-m5-going-step-by-step\n\nplt.figure(figsize=(12, 4))\nfor i in range(10):\n    plt.plot(train_sales.loc[train_sales['item_id'] == 'HOBBIES_1_002'].iloc[i, 6:].values,\n             label=train_sales.loc[train_sales['item_id'] == 'HOBBIES_1_002'].iloc[i, 5]);\nplt.title('HOBBIES_1_002 sales')\nplt.legend();", "processed": ["also data sell price 10 store", "let make plot like see sale time differ shop"]}, {"markdown": ["Well... this doesn't look pretty. Let's make is smoother - I'll plot rolling mean over 30 days"], "code": "# Reference: https://www.kaggle.com/code/artgor/it-is-time-for-m5-going-step-by-step\n\nplt.figure(figsize=(12, 4))\nfor i in range(10):\n    plt.plot(train_sales.loc[train_sales['item_id'] == 'HOBBIES_1_002'].iloc[i, 6:].rolling(30).mean().values,\n             label=train_sales.loc[train_sales['item_id'] == 'HOBBIES_1_002'].iloc[i, 5]);\nplt.title('HOBBIES_1_002 sales, rolling mean 30 days')\nplt.legend();\n\nplt.figure(figsize=(12, 4))\nfor i in range(10):\n    plt.plot(train_sales.loc[train_sales['item_id'] == 'HOBBIES_1_002'].iloc[i, 6:].rolling(60).mean().values,\n             label=train_sales.loc[train_sales['item_id'] == 'HOBBIES_1_002'].iloc[i, 5]);\nplt.title('HOBBIES_1_002 sales, rolling mean 60 days')\nplt.legend();\n\nplt.figure(figsize=(12, 4))\nfor i in range(10):\n    plt.plot(train_sales.loc[train_sales['item_id'] == 'HOBBIES_1_002'].iloc[i, 6:].rolling(90).mean().values,\n             label=train_sales.loc[train_sales['item_id'] == 'HOBBIES_1_002'].iloc[i, 5]);\nplt.title('HOBBIES_1_002 sales, rolling mean 90 days')\nplt.legend();\n", "processed": ["well look pretti let make smoother plot roll mean 30 day"]}, {"markdown": ["So what do we see here?\n* there is a definite seasonality with several peaks;\n* the sales are more or less constant and quite low (max sales per day in one store is 11);\n* as a result, it could be difficult to predict such low values;"], "code": "# Reference: https://www.kaggle.com/code/artgor/it-is-time-for-m5-going-step-by-step\n\nitem_prices = sell_prices.loc[sell_prices['item_id'] == 'HOBBIES_2_001']\nfor s in item_prices['store_id'].unique():\n    small_df = item_prices.loc[item_prices['store_id'] == s]\n    plt.plot(small_df['wm_yr_wk'], small_df['sell_price'], label=s)\nplt.legend()\nplt.title('HOBBIES_2_001 sell prices');", "processed": ["see definit season sever peak sale le constant quit low max sale per day one store 11 result could difficult predict low valu"]}, {"markdown": ["This store (and I suppose other stores) have 3 categories: foods, hobbies and households, which have 2-3 departments."], "code": "# Reference: https://www.kaggle.com/code/artgor/it-is-time-for-m5-going-step-by-step\n\nplt.figure(figsize=(12, 4))\nfor d in ca_1_sales['dept_id'].unique():\n    store_sales = ca_1_sales.loc[ca_1_sales['dept_id'] == d]\n    store_sales.iloc[:, 6:].sum().rolling(30).mean().plot(label=d)\nplt.title('CA_1 sales by department, rolling mean 30 days')\nplt.legend(loc=(1.0, 0.5));", "processed": ["store suppos store 3 categori food hobbi household 2 3 depart"]}, {"markdown": ["Interesting that `FOODS_1` has much higher sales than any other department."], "code": "# Reference: https://www.kaggle.com/code/artgor/it-is-time-for-m5-going-step-by-step\n\nitem_prices = sell_prices.loc[sell_prices['item_id'] == 'HOBBIES_2_001']\nfor s in item_prices['store_id'].unique():\n    small_df = item_prices.loc[item_prices['store_id'] == s]\n    plt.plot(small_df['wm_yr_wk'], small_df['sell_price'], label=s)\nplt.legend()\nplt.title('HOBBIES_2_001 sell prices');\nca_1_prices = sell_prices.loc[sell_prices['store_id'] == 'CA_1']\nca_1_prices['dept_id'] = ca_1_prices['item_id'].apply(lambda x: x[:-4])\n\nplt.figure(figsize=(12, 6))\nfor d in ca_1_prices['dept_id'].unique():\n    small_df = ca_1_prices.loc[ca_1_prices['dept_id'] == d]\n    grouped = small_df.groupby(['wm_yr_wk'])['sell_price'].mean()\n    plt.plot(grouped.index, grouped.values, label=d)\nplt.legend(loc=(1.0, 0.5))\nplt.title('CA_1 mean sell prices by dept');", "processed": ["interest food 1 much higher sale depart"]}, {"markdown": ["We can see that there are 416 unique items in this department and they are sold in all stores."], "code": "# Reference: https://www.kaggle.com/code/artgor/it-is-time-for-m5-going-step-by-step\n\nsell_prices.loc[sell_prices['item_id'].str.contains('HOBBIES_1')]\nhobbies_1_sales = train_sales.loc[train_sales['dept_id'] == 'HOBBIES_1']\nplt.figure(figsize=(12, 6))\nfor d in hobbies_1_sales['store_id'].unique():\n    store_sales = hobbies_1_sales.loc[hobbies_1_sales['store_id'] == d]\n    store_sales.iloc[:, 6:].sum().rolling(30).mean().plot(label=d)\nplt.title('HOBBIES_1 sales by stores, rolling mean 30 days')\nplt.legend(loc=(1.0, 0.5));", "processed": ["see 416 uniqu item depart sold store"]}, {"markdown": ["We can see a definite increase of sales over time. And we can see that CA_1 and CA_3 stores have higher sales than other stores."], "code": "# Reference: https://www.kaggle.com/code/artgor/it-is-time-for-m5-going-step-by-step\n\nsell_prices.head()\nhobbies_1_prices = sell_prices.loc[sell_prices['item_id'].str.contains('HOBBIES_1')]\nplt.figure(figsize=(12, 6))\nfor d in hobbies_1_prices['store_id'].unique():\n    small_df = hobbies_1_prices.loc[hobbies_1_prices['store_id'] == d]\n    grouped = small_df.groupby(['wm_yr_wk'])['sell_price'].mean()\n    plt.plot(grouped.index, grouped.values, label=d)\nplt.legend(loc=(1.0, 0.5))\nplt.title('HOBBIES_1 mean sell prices by store');", "processed": ["see definit increas sale time see ca 1 ca 3 store higher sale store"]}, {"markdown": ["Not only sales grow over time, the prices also grow. We can see that there were several points of time when the price increased.", "### All info about a single state\n\nNow we can analyse how different items are sold in different stores of the same state."], "code": "# Reference: https://www.kaggle.com/code/artgor/it-is-time-for-m5-going-step-by-step\n\ntrain_sales.loc[train_sales['state_id'] == 'CA']\nfor col in ['item_id', 'dept_id', 'store_id']:\n    print(f\"{col} has {train_sales.loc[train_sales['state_id'] == 'CA', col].nunique()} unique values for CA state\")\nca_sales = train_sales.loc[train_sales['state_id'] == 'CA']\nplt.figure(figsize=(12, 6))\nfor d in ca_sales['store_id'].unique():\n    store_sales = ca_sales.loc[ca_sales['store_id'] == d]\n    store_sales.iloc[:, 6:].sum().rolling(30).mean().plot(label=d)\nplt.title('CA sales by stores, rolling mean 30 days')\nplt.legend(loc=(1.0, 0.5));", "processed": ["sale grow time price also grow see sever point time price increas", "info singl state analys differ item sold differ store state"]}, {"markdown": ["We can see a lot of interesting things:\n* ca_3 store always has higher sales;\n* ca_1 has a little increasing trend;\n* ca_2 had a long decline and then had a very steep increase in sales;"], "code": "# Reference: https://www.kaggle.com/code/artgor/it-is-time-for-m5-going-step-by-step\n\nca_prices = sell_prices.loc[sell_prices['store_id'].str.contains('CA')]\nplt.figure(figsize=(12, 6))\nfor d in ca_prices['store_id'].unique():\n    small_df = ca_prices.loc[ca_prices['store_id'] == d]\n    grouped = small_df.groupby(['wm_yr_wk'])['sell_price'].mean()\n    plt.plot(grouped.index, grouped.values, label=d)\nplt.legend(loc=(1.0, 0.5))\nplt.title('Mean sell prices by store in CA');", "processed": ["see lot interest thing ca 3 store alway higher sale ca 1 littl increas trend ca 2 long declin steep increas sale"]}, {"markdown": ["We can see that there were several points of time when the price increased.", "### Aggregations over department\n\nNow let's look at various aggregations over the data"], "code": "# Reference: https://www.kaggle.com/code/artgor/it-is-time-for-m5-going-step-by-step\n\ntrain_sales.head()\nplt.figure(figsize=(12, 8))\ndept_grouped_sales = train_sales.groupby(['dept_id']).sum()\nfor i, row in dept_grouped_sales.iterrows():\n    plt.plot(row.values, label=i);\nplt.legend(loc=(1.0, 0.5))\nplt.title('Sales by departments');\nplt.figure(figsize=(12, 4))\nfor i, row in dept_grouped_sales.iterrows():\n    plt.plot(row.rolling(30).mean().values, label=i);\nplt.title('Sales by department, rolling mean 30 days')\nplt.legend(loc=(1.0, 0.5));\n\nplt.figure(figsize=(12, 4))\nfor i, row in dept_grouped_sales.iterrows():\n    plt.plot(row.rolling(60).mean().values, label=i);\nplt.title('Sales by department, rolling mean 60 days')\nplt.legend(loc=(1.0, 0.5));\n\nplt.figure(figsize=(12, 4))\nfor i, row in dept_grouped_sales.iterrows():\n    plt.plot(row.rolling(90).mean().values, label=i);\nplt.title('Sales by department, rolling mean 90 days')\nplt.legend(loc=(1.0, 0.5));\n", "processed": ["see sever point time price increas", "aggreg depart let look variou aggreg data"]}, {"markdown": ["# Train Model", "### Loss function\n\nFocal loss is good for unbalanced datasets, like this one."], "code": "# Reference: https://www.kaggle.com/code/xhlulu/rsna-intracranial-simple-densenet-in-keras\n\ndef focal_loss(prediction_tensor, target_tensor, weights=None, alpha=0.25, gamma=2):\n    r\"\"\"Compute focal loss for predictions.\n        Multi-labels Focal loss formula:\n            FL = -alpha * (z-p)^gamma * log(p) -(1-alpha) * p^gamma * log(1-p)\n                 ,which alpha = 0.25, gamma = 2, p = sigmoid(x), z = target_tensor.\n    Args:\n     prediction_tensor: A float tensor of shape [batch_size, num_anchors,\n        num_classes] representing the predicted logits for each class\n     target_tensor: A float tensor of shape [batch_size, num_anchors,\n        num_classes] representing one-hot encoded classification targets\n     weights: A float tensor of shape [batch_size, num_anchors]\n     alpha: A scalar tensor for focal loss alpha hyper-parameter\n     gamma: A scalar tensor for focal loss gamma hyper-parameter\n    Returns:\n        loss: A (scalar) tensor representing the value of the loss function\n    \"\"\"\n    sigmoid_p = tf.nn.sigmoid(prediction_tensor)\n    zeros = array_ops.zeros_like(sigmoid_p, dtype=sigmoid_p.dtype)\n    \n    # For poitive prediction, only need consider front part loss, back part is 0;\n    # target_tensor > zeros <=> z=1, so poitive coefficient = z - p.\n    pos_p_sub = array_ops.where(target_tensor > zeros, target_tensor - sigmoid_p, zeros)\n    \n    # For negative prediction, only need consider back part loss, front part is 0;\n    # target_tensor > zeros <=> z=1, so negative coefficient = 0.\n    neg_p_sub = array_ops.where(target_tensor > zeros, zeros, sigmoid_p)\n    per_entry_cross_ent = - alpha * (pos_p_sub ** gamma) * tf.log(tf.clip_by_value(sigmoid_p, 1e-8, 1.0)) \\\n                          - (1 - alpha) * (neg_p_sub ** gamma) * tf.log(tf.clip_by_value(1.0 - sigmoid_p, 1e-8, 1.0))\n    return tf.reduce_sum(per_entry_cross_ent)\ndensenet = DenseNet121(\n    weights='../input/densenet-keras/DenseNet-BC-121-32-no-top.h5',\n    include_top=False,\n    input_shape=(224,224,3)\n)\ndef build_model():\n    model = Sequential()\n    model.add(densenet)\n    model.add(layers.GlobalAveragePooling2D())\n    model.add(layers.Dropout(0.5))\n#     model.add(layers.Dense(6, activation='sigmoid', \n#                            bias_initializer=Constant(value=-5.5)))\n    model.add(layers.Dense(6, activation='sigmoid'))\n    \n    model.compile(\n#         loss=focal_loss,\n        loss='categorical_crossentropy',\n        optimizer=Adam(lr=0.001),\n        metrics=['accuracy']\n    )\n    \n    return model\nmodel = build_model()\nmodel.summary()\ncheckpoint = ModelCheckpoint(\n    'model.h5', \n    monitor='val_loss', \n    verbose=0, \n    save_best_only=True, \n    save_weights_only=False,\n    mode='auto'\n)\n\ntotal_steps = sample_files.shape[0] / BATCH_SIZE\n\nhistory = model.fit_generator(\n    train_gen,\n    steps_per_epoch=2000,\n    validation_data=val_gen,\n    validation_steps=total_steps * 0.15,\n    callbacks=[checkpoint],\n    epochs=5\n)\nwith open('history.json', 'w') as f:\n    json.dump(history.history, f)\n\nhistory_df = pd.DataFrame(history.history)\nhistory_df[['loss', 'val_loss']].plot()\nhistory_df[['acc', 'val_acc']].plot()", "processed": ["train model", "loss function focal loss good unbalanc dataset like one"]}, {"markdown": ["## signal_to_noise feature"], "code": "# Reference: https://www.kaggle.com/code/robikscube/openvaccine-covid-19-mrna-starter-eda\n\nplt.style.use('ggplot')\nfig, ax = plt.subplots(figsize=(10, 3))\nax = sns.distplot(train['signal_to_noise'])\nax.set_title('Signal to Noise feature (train)')\nplt.show()", "processed": ["signal nois featur"]}, {"markdown": ["## seq_length\n\nTrain data consists of only 107 sequence length. The test data contains mostly 130 sequence lengths."], "code": "# Reference: https://www.kaggle.com/code/robikscube/openvaccine-covid-19-mrna-starter-eda\n\ntest['seq_length'].value_counts() \\\n    .plot(kind='bar', figsize=(10, 4),\n          color=color_pal[4],\n         title='Sequence Length in public test set')\nplt.show()", "processed": ["seq length train data consist 107 sequenc length test data contain mostli 130 sequenc length"]}, {"markdown": ["# Baseline Submission [0.47840 LB]\n## Predict the average value for each target column\nLets first calculate the average value for the target columns. And then create a 91 length vector as a baseline submission."], "code": "# Reference: https://www.kaggle.com/code/robikscube/openvaccine-covid-19-mrna-starter-eda\n\nfig, axs = plt.subplots(3, 1,\n                        figsize=(10, 6),\n                        sharex=True)\naxs = axs.flatten()\ntrain['mean_reactivity'] = train['reactivity'].apply(lambda x: np.mean(x))\ntrain['mean_deg_Mg_pH10'] = train['deg_Mg_pH10'].apply(lambda x: np.mean(x))\ntrain['mean_deg_Mg_50C'] = train['deg_Mg_50C'].apply(lambda x: np.mean(x))\n\ntrain['mean_reactivity'] \\\n    .plot(kind='hist',\n          bins=50,\n          color=color_pal[0],\n          title='Distribution of Mean Reactivity in training set',\n         ax=axs[0])\ntrain['mean_deg_Mg_pH10'] \\\n    .plot(kind='hist',\n          bins=50,\n          ax=axs[1],\n          color=color_pal[4],\n          title='Distribution of Mean deg_Mg_pH10 in training set')\ntrain['mean_deg_Mg_50C'] \\\n    .plot(kind='hist',\n          bins=50,\n          ax=axs[2],\n          color=color_pal[3],\n          title='Distribution of Mean deg_Mg_50C in training set')\nplt.tight_layout()\nplt.show()", "processed": ["baselin submiss 0 47840 lb predict averag valu target column let first calcul averag valu target column creat 91 length vector baselin submiss"]}, {"markdown": ["# Plot the Targets for Each Training Example\nFirst we need to split the list of 68 values for each target into their own columns. Then we can plot."], "code": "# Reference: https://www.kaggle.com/code/robikscube/openvaccine-covid-19-mrna-starter-eda\n\n# Split the 68 Reactivity values each into it's own column\nfor n in range(68):\n    train[f'reactivity_{n}'] = train['reactivity'].apply(lambda x: x[n])\n    \nREACTIVITY_COLS = [r for r in train.columns if 'reactivity_' in r and 'error' not in r]\n\nax = train.set_index('id')[REACTIVITY_COLS] \\\n    .T \\\n    .plot(color='black',\n          alpha=0.01,\n          ylim=(-0.5, 5),\n          title='reactivity of training set',\n          figsize=(15, 5))\nax.get_legend().remove()\nfor n in range(68):\n    train[f'deg_Mg_pH10_{n}'] = train['deg_Mg_pH10'].apply(lambda x: x[n])\n    \nDEG_MG_PH10_COLS = [r for r in train.columns if 'deg_Mg_pH10_' in r and 'error' not in r]\n\nax = train.set_index('id')[DEG_MG_PH10_COLS] \\\n    .T \\\n    .plot(color='c',\n          alpha=0.01,\n          ylim=(-0.5, 5),\n          title='Deg Mg Ph10 of training set',\n          figsize=(15, 5))\nax.get_legend().remove()\nfor n in range(68):\n    train[f'deg_Mg_50C_{n}'] = train['deg_Mg_50C'].apply(lambda x: x[n])\n    \nDEG_MG_50C_COLS = [r for r in train.columns if 'deg_Mg_50C_' in r and 'error' not in r]\n\nax = train.set_index('id')[DEG_MG_50C_COLS] \\\n    .T \\\n    .plot(color='m',\n          alpha=0.2,\n          ylim=(-2, 7),\n          title='Deg Mg 50C of training set',\n          figsize=(15, 5)\n         )\nax.get_legend().remove()", "processed": ["plot target train exampl first need split list 68 valu target column plot"]}, {"markdown": ["# Relationship between targets\nColored by `SN_filter` although I'm not clear from the data description what this column represents.\n"], "code": "# Reference: https://www.kaggle.com/code/robikscube/openvaccine-covid-19-mrna-starter-eda\n\nsns.pairplot(data=train,\n             vars=['mean_reactivity',\n                   'mean_deg_Mg_pH10',\n                    'mean_deg_Mg_50C'],\n            hue='SN_filter')\nplt.show()", "processed": ["relationship target color sn filter although clear data descript column repres"]}, {"markdown": ["## Map mean predictions to test"], "code": "# Reference: https://www.kaggle.com/code/robikscube/openvaccine-covid-19-mrna-starter-eda\n\nss['id'] = 'id_' + ss['id_seqpos'].str.split('_', expand=True)[1]\n\n# Merge my predicted average values\nss_new = ss. \\\n    drop(['reactivity','deg_Mg_pH10','deg_Mg_50C'], axis=1) \\\n    .merge(test[['id',\n               'mean_reactivity_pred',\n               'mean_deg_Mg_pH10_pred',\n               'mean_deg_Mg_50C_pred']] \\\n               .rename(columns={'mean_reactivity_pred' : 'reactivity',\n                                'mean_deg_Mg_pH10_pred': 'deg_Mg_pH10',\n                                'mean_deg_Mg_50C_pred' : 'deg_Mg_50C'}\n                      ),\n         on='id',\n        validate='m:1')\nTARGETS = ['reactivity','deg_Mg_pH10','deg_Mg_50C']\nfor i, t in enumerate(TARGETS):\n    ss_new[t].plot(kind='hist',\n                              figsize=(10, 3),\n                              bins=100,\n                              color=color_pal[i*3],\n                              title=f'Submission {t}')\n    plt.show()\nss_new.sample(10)\n# Make Submission\nss = pd.read_csv('../input/stanford-covid-vaccine/sample_submission.csv')\nss_new[ss.columns].to_csv('submission_lgbm_v1.csv', index=False)", "processed": ["map mean predict test"]}, {"markdown": ["# Improve Baseline by adding: **structure** and **predicted_loop_type** features [0.47520 LB]"], "code": "# Reference: https://www.kaggle.com/code/robikscube/openvaccine-covid-19-mrna-starter-eda\n\n# Expand Sequence Features\nfor n in range(107):\n    train[f'structure_{n}'] = train['structure'].apply(lambda x: x[n]).astype('category')\n    test[f'structure_{n}'] = test['structure'].apply(lambda x: x[n]).astype('category')\n    train[f'predicted_loop_type_{n}'] = train['predicted_loop_type'].apply(lambda x: x[n]).astype('category')\n    test[f'predicted_loop_type_{n}'] = test['predicted_loop_type'].apply(lambda x: x[n]).astype('category')\n    train[f'sequence_{n}'] = train['sequence'].apply(lambda x: x[n]).astype('category')\n    test[f'sequence_{n}'] = test['sequence'].apply(lambda x: x[n]).astype('category')\n\nSEQUENCE_COLS = [c for c in train.columns if 'sequence_' in c]\nSTRUCTURE_COLS = [c for c in train.columns if 'structure_' in c]\nPLT_COLS = [c for c in train.columns if 'predicted_loop_type_' in c]\n\nfor target in ['reactivity','deg_Mg_pH10','deg_Mg_50C']:\n\n    X = train[SEQUENCE_COLS + STRUCTURE_COLS + PLT_COLS]\n    y = train[f'mean_{target}']\n    X_test = test[SEQUENCE_COLS + STRUCTURE_COLS + PLT_COLS]\n\n    X_train, X_val, y_train, y_val = train_test_split(X, y)\n\n    reg = lgb.LGBMRegressor(n_estimators=10000,\n                            learning_rate=0.001,\n                            feature_fraction=0.8)\n    reg.fit(X_train, y_train,\n            eval_set=(X_val, y_val),\n           early_stopping_rounds=100,\n           verbose=1000)\n\n    test[f'mean_{target}_pred'] = reg.predict(X_test)\n    \nss['id'] = 'id_' + ss['id_seqpos'].str.split('_', expand=True)[1]\n\n# Merge my predicted average values\nss_new = ss. \\\n    drop(['reactivity','deg_Mg_pH10','deg_Mg_50C'], axis=1) \\\n    .merge(test[['id',\n               'mean_reactivity_pred',\n               'mean_deg_Mg_pH10_pred',\n               'mean_deg_Mg_50C_pred']] \\\n               .rename(columns={'mean_reactivity_pred' : 'reactivity',\n                                'mean_deg_Mg_pH10_pred': 'deg_Mg_pH10',\n                                'mean_deg_Mg_50C_pred' : 'deg_Mg_50C'}\n                      ),\n         on='id',\n        validate='m:1')\n\nss = pd.read_csv('../input/stanford-covid-vaccine/sample_submission.csv')\nss_new[ss.columns].to_csv('submission.csv', index=False)\n\nTARGETS = ['reactivity','deg_Mg_pH10','deg_Mg_50C']\nfor i, t in enumerate(TARGETS):\n    ss_new[t].plot(kind='hist',\n                              figsize=(10, 3),\n                              bins=100,\n                              color=color_pal[i*3],\n                              title=f'Submission {t}')\n    plt.show()", "processed": ["improv baselin ad structur predict loop type featur 0 47520 lb"]}, {"markdown": ["# Modeling approach. Fit Line for Reactivity?\nLets test and see what a regression line looks like for some example samples. Since we are only given 68 values in the training set and will predict 93 in the test, this might be a good idea for extending the trend beyond 93. "], "code": "# Reference: https://www.kaggle.com/code/robikscube/openvaccine-covid-19-mrna-starter-eda\n\nfig, axs = plt.subplots(5, 5, figsize=(12, 10))\naxs = axs.flatten()\ni = 0\nfor row in train.sample(25, random_state=42).iterrows():\n    myid = row[1]['id']\n    reactivity_array = row[1][REACTIVITY_COLS].values\n    sns.regplot(np.array(range(68)).reshape(-1, 1),\n                reactivity_array,\n                ax=axs[i],\n                color=next(color_cycle))\n    axs[i].set_title(myid)\n    i += 1\nfig.suptitle('Reactivity Array for 25 Train Examples with Regression Line',\n             fontsize=18,\n             y=1.02)\nplt.tight_layout()\nplt.show()\nfig, axs = plt.subplots(5, 5,\n                        figsize=(12, 10),\n                       sharex=True)\naxs = axs.flatten()\ni = 0\nfor row in train.sample(25, random_state=42).iterrows():\n    myid = row[1]['id']\n    reactivity_array = row[1][DEG_MG_50C_COLS].values\n    sns.regplot(np.array(range(68)).reshape(-1, 1),\n                reactivity_array,\n                ax=axs[i],\n                color=next(color_cycle))\n    axs[i].set_title(myid)\n    i += 1\nfig.suptitle('\"DEG_MG_50C\" Array for 25 Train Examples with Regression Line',\n             fontsize=18,\n             y=1.02)\nplt.tight_layout()\nplt.show()\nfig, axs = plt.subplots(5, 5, figsize=(12, 10))\naxs = axs.flatten()\ni = 0\nfor row in train.sample(25, random_state=42).iterrows():\n    myid = row[1]['id']\n    reactivity_array = row[1][DEG_MG_PH10_COLS].values\n    sns.regplot(np.array(range(68)).reshape(-1, 1),\n                reactivity_array,\n                ax=axs[i],\n                color=next(color_cycle))\n    axs[i].set_title(myid)\n    i += 1\nfig.suptitle('\"DEG_MG_PH10\" Array for 25 Train Examples with Regression Line',\n             fontsize=18,\n             y=1.02)\nplt.tight_layout()\nplt.show()", "processed": ["model approach fit line reactiv let test see regress line look like exampl sampl sinc given 68 valu train set predict 93 test might good idea extend trend beyond 93"]}, {"markdown": ["# Better LightGBM Model\n- Expanding for one row per prediction"], "code": "# Reference: https://www.kaggle.com/code/robikscube/openvaccine-covid-19-mrna-starter-eda\n\nimport pandas as pd\nimport numpy as np\nimport json\nfrom tqdm.notebook import tqdm\nfrom sklearn.model_selection import train_test_split\nimport lightgbm as lgb\nimport matplotlib.pylab as plt\n\ndef expand_columns(df):\n    df = df.copy()\n    df = df.drop('index', axis=1)\n    max_seq_length = df['seq_length'].max()\n    SEQUENCE_COLS = []; STRUCTURE_COLS = []; PRED_LOOP_TYPE_COLS = []\n    for s in range(130):\n        df[f'sequence_{s}'] = df['sequence'].str[s]\n        df[f'structure_{s}'] = df['structure'].str[s]\n        df[f'predicted_loop_type_{s}'] = df['predicted_loop_type'].str[s]\n        SEQUENCE_COLS.append(f'sequence_{s}')\n        STRUCTURE_COLS.append(f'structure_{s}')\n    return df, SEQUENCE_COLS, STRUCTURE_COLS\n\ndef parse_sample_submission(ss):\n    ss = ss.copy()\n    ss['id'] = ss['id_seqpos'].str.split('_', expand=True)[1]\n    ss['seqpos'] = ss['id_seqpos'].str.split('_', expand=True)[2].astype('int')\n    return ss\ndef get_train_long(train):\n    dfs = []\n\n    def pad(feat, tolen):\n        padded = np.pad(feat,\n                        (0, tolen-len(feat)),\n                        mode='constant',\n                        constant_values=np.nan)\n        return padded\n\n    for d in tqdm(train.itertuples(), total=len(train)):\n        sequence = [s for s in d[3]]\n        seq_len = len(sequence)\n        structure = [s for s in d[4]]\n        predicted_loop_type = [s for s in d[5]]\n        reactivity_error = pad([s for s in d[10]], seq_len)\n        deg_error_Mg_pH10 = pad([s for s in d[11]], seq_len)\n        deg_error_pH10 = pad([s for s in d[12]], seq_len)\n        deg_error_Mg_50C = pad([s for s in d[13]], seq_len)\n        deg_error_50C = pad([s for s in d[14]], seq_len)\n\n        reactivity = pad([s for s in d[15]], seq_len)\n        deg_Mg_pH10 = pad([s for s in d[16]], seq_len)\n        deg_pH10 = pad([s for s in d[17]], seq_len)\n        deg_Mg_50C = pad([s for s in d[18]], seq_len)\n        deg_50C = pad([s for s in d[10]], seq_len)\n        myid = [d[2]] * len(sequence)\n        seqpos = [c for c in range(len(sequence))]\n        dfs.append(pd.DataFrame(np.array([myid,\n                                          seqpos,\n                                          sequence,\n                                          structure,\n                                          predicted_loop_type,\n                                          reactivity_error,\n                                          deg_error_Mg_pH10,\n                                          deg_error_pH10,\n                                          deg_error_Mg_50C,\n                                          deg_error_50C,\n                                          reactivity,\n                                          deg_Mg_pH10,\n                                          deg_pH10,\n                                          deg_Mg_50C,\n                                         ]).T))\n    train_long = pd.concat(dfs)\n\n    train_long.columns=['id',\n               'seqpos',\n               'sequence',\n               'structure',\n               'predicted_loop_type',\n               'reactivity_error',\n               'deg_error_Mg_pH10',\n               'deg_error_pH10',\n               'deg_error_Mg_50C',\n               'deg_error_50C',\n               'reactivity',\n               'deg_Mg_pH10',\n               'deg_pH10',\n               'deg_Mg_50C']\n\n    return train_long\n\n\ndef get_test_long(test):\n    dfs = []\n\n    def pad(feat, tolen):\n        padded = np.pad(feat,\n                        (0, tolen-len(feat)),\n                        mode='constant',\n                        constant_values=np.nan)\n        return padded\n\n    for d in tqdm(test.itertuples(), total=len(test)):\n        sequence = [s for s in d[3]]\n        seq_len = len(sequence)\n        structure = [s for s in d[4]]\n        predicted_loop_type = [s for s in d[5]]\n        myid = [d[2]] * len(sequence)\n        seqpos = [c for c in range(len(sequence))]\n        dfs.append(pd.DataFrame(np.array([myid,\n                                          seqpos,\n                                          sequence,\n                                          structure,\n                                          predicted_loop_type,\n                                         ]).T))\n    test_long = pd.concat(dfs)\n\n    test_long.columns=['id',\n               'seqpos',\n               'sequence',\n               'structure',\n               'predicted_loop_type']\n\n    return test_long\n\ndef add_long_features(df):\n    df = df.copy()\n    df['seqpos'] = df['seqpos'].astype('int')\n    df = df.merge(df.query('seqpos <= 106') \\\n                    .groupby('id')['sequence'] \\\n                      .value_counts() \\\n                      .unstack() \\\n                      .reset_index(),\n             how='left',\n             on=['id'],\n             validate='m:1'\n            )\n    \n    df = df.merge(df.query('seqpos <= 106') \\\n                  .groupby('id')['structure'] \\\n                      .value_counts() \\\n                      .unstack() \\\n                      .reset_index(),\n             how='left',\n             on=['id'],\n             validate='m:1'\n            )\n\n    df = df.merge(df.query('seqpos <= 106') \\\n                  .groupby('id')['predicted_loop_type'] \\\n                      .value_counts() \\\n                      .unstack() \\\n                      .reset_index(),\n             how='left',\n             on=['id'],\n             validate='m:1'\n            )\n    for shift in [-5, -4, -3, -2 -1, 1, 2, 3, 4, 5]:\n        for f in ['sequence','structure','predicted_loop_type']:\n            df[f'{f}_shift{shift}'] = df.groupby('id')[f].shift(shift)\n    return df\ndef make_feature_types(df, features):\n    df = df.copy()\n    df = df.replace('nan', np.nan)\n    for f in features:\n        try:\n            df[f] = pd.to_numeric(df[f])\n        except ValueError:\n            df[f] = df[f].astype('category')\n    return df\ntrain = pd.read_json('../input/stanford-covid-vaccine/train.json', lines=True)\ntest = pd.read_json('../input/stanford-covid-vaccine/test.json', lines=True)\nss = pd.read_csv('../input/stanford-covid-vaccine/sample_submission.csv')\n\ntrain_expanded, SEQUENCE_COLS, STRUCTURE_COLS = expand_columns(train)\ntest_expanded, SEQUENCE_COLS, STRUCTURE_COLS = expand_columns(test)\nss = parse_sample_submission(ss)\n\ntrain_long = get_train_long(train)\ntest_long = get_test_long(test)\n\ntrain_long = add_long_features(train_long)\ntest_long = add_long_features(test_long)\n\nFEATURES = ['seqpos',\n            'sequence',\n            'structure',\n            'predicted_loop_type',\n            'A', 'C', 'G', 'U', '(', ')', '.', 'B', 'E',\n            'H', 'I', 'M', 'S', 'X',\n            'sequence_shift-5', 'structure_shift-5',\n            'predicted_loop_type_shift-5', 'sequence_shift-4', 'structure_shift-4',\n            'predicted_loop_type_shift-4', 'sequence_shift-3', 'structure_shift-3',\n            'predicted_loop_type_shift-3', 'sequence_shift1', 'structure_shift1',\n            'predicted_loop_type_shift1', 'sequence_shift2', 'structure_shift2',\n            'predicted_loop_type_shift2', 'sequence_shift3', 'structure_shift3',\n            'predicted_loop_type_shift3', 'sequence_shift4', 'structure_shift4',\n            'predicted_loop_type_shift4', 'sequence_shift5', 'structure_shift5',\n            'predicted_loop_type_shift5']\n\ntrain_long = make_feature_types(train_long, FEATURES)\ntest_long = make_feature_types(test_long, FEATURES)\n\ntrain_ids, val_ids = train_test_split(train['id'].unique())\n\nTARGETS = ['reactivity','deg_Mg_pH10','deg_Mg_50C']\nfis = []\nfor t in TARGETS:\n    print(f'==== Running for target {t} ====')\n    X_train = train_long.dropna(subset=[t]).loc[train_long['id'].isin(train_ids)][FEATURES].copy()\n    y_train = train_long.dropna(subset=[t]).loc[train_long['id'].isin(train_ids)][t].copy()\n    X_val = train_long.dropna(subset=[t]).loc[train_long['id'].isin(val_ids)][FEATURES].copy()\n    y_val = train_long.dropna(subset=[t]).loc[train_long['id'].isin(val_ids)][t].copy()\n    X_test = test_long[FEATURES].copy()\n    y_train = pd.to_numeric(y_train)\n    y_val = pd.to_numeric(y_val)\n    \n    reg = lgb.LGBMRegressor(n_estimators=10000,\n                            learning_rate=0.01,\n                            importance_type='gain')\n    reg.fit(X_train, y_train,\n            eval_set=(X_val, y_val),\n           verbose=1000,\n           early_stopping_rounds=500)\n\n    fi_df = pd.DataFrame(index=FEATURES, \n                 data=reg.feature_importances_,\n                 columns=[f'importance_{t}'])\n    \n    fi_df.sort_values(f'importance_{t}') \\\n        .plot(kind='barh', figsize=(8, 15), title=t)\n    plt.show()\n    fis.append(fi_df)\n    \n    test_long[f'{t}_pred'] = reg.predict(X_test)", "processed": ["better lightgbm model expand one row per predict"]}, {"markdown": ["# Custom LR scheduler\nFrom starter [kernel][1]\n\n[1]: https://www.kaggle.com/mgornergoogle/getting-started-with-100-flowers-on-tpu"], "code": "# Reference: https://www.kaggle.com/code/cdeotte/rotation-augmentation-gpu-tpu-0-96\n\n# Learning rate schedule for TPU, GPU and CPU.\n# Using an LR ramp up because fine-tuning a pre-trained model.\n# Starting with a high LR would break the pre-trained weights.\n\nLR_START = 0.00001\nLR_MAX = 0.00005 * strategy.num_replicas_in_sync\nLR_MIN = 0.00001\nLR_RAMPUP_EPOCHS = 5\nLR_SUSTAIN_EPOCHS = 0\nLR_EXP_DECAY = .8\n\ndef lrfn(epoch):\n    if epoch < LR_RAMPUP_EPOCHS:\n        lr = (LR_MAX - LR_START) / LR_RAMPUP_EPOCHS * epoch + LR_START\n    elif epoch < LR_RAMPUP_EPOCHS + LR_SUSTAIN_EPOCHS:\n        lr = LR_MAX\n    else:\n        lr = (LR_MAX - LR_MIN) * LR_EXP_DECAY**(epoch - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS) + LR_MIN\n    return lr\n    \nlr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose = True)\n\nrng = [i for i in range(25 if EPOCHS<25 else EPOCHS)]\ny = [lrfn(x) for x in rng]\nplt.plot(rng, y)\nprint(\"Learning rate schedule: {:.3g} to {:.3g} to {:.3g}\".format(y[0], max(y), y[-1]))", "processed": ["custom lr schedul starter kernel 1 1 http www kaggl com mgornergoogl get start 100 flower tpu"]}, {"markdown": ["# Display Example Augmentation\nBelow are examples of 3 training images where each is randomly augmented 12 different times."], "code": "# Reference: https://www.kaggle.com/code/cdeotte/rotation-augmentation-gpu-tpu-0-96\n\nrow = 3; col = 4;\nall_elements = get_training_dataset(load_dataset(TRAINING_FILENAMES),do_aug=False).unbatch()\none_element = tf.data.Dataset.from_tensors( next(iter(all_elements)) )\naugmented_element = one_element.repeat().map(transform).batch(row*col)\n\nfor (img,label) in augmented_element:\n    plt.figure(figsize=(15,int(15*row/col)))\n    for j in range(row*col):\n        plt.subplot(row,col,j+1)\n        plt.axis('off')\n        plt.imshow(img[j,])\n    plt.show()\n    break\nrow = 3; col = 4;\nall_elements = get_training_dataset(load_dataset(TRAINING_FILENAMES),do_aug=False).unbatch()\none_element = tf.data.Dataset.from_tensors( next(iter(all_elements)) )\naugmented_element = one_element.repeat().map(transform).batch(row*col)\n\nfor (img,label) in augmented_element:\n    plt.figure(figsize=(15,int(15*row/col)))\n    for j in range(row*col):\n        plt.subplot(row,col,j+1)\n        plt.axis('off')\n        plt.imshow(img[j,])\n    plt.show()\n    break\nrow = 3; col = 4;\nall_elements = get_training_dataset(load_dataset(TRAINING_FILENAMES),do_aug=False).unbatch()\none_element = tf.data.Dataset.from_tensors( next(iter(all_elements)) )\naugmented_element = one_element.repeat().map(transform).batch(row*col)\n\nfor (img,label) in augmented_element:\n    plt.figure(figsize=(15,int(15*row/col)))\n    for j in range(row*col):\n        plt.subplot(row,col,j+1)\n        plt.axis('off')\n        plt.imshow(img[j,])\n    plt.show()\n    break", "processed": ["display exampl augment exampl 3 train imag randomli augment 12 differ time"]}, {"markdown": ["# EDA \ud83d\udcca\n\nNow, I am going to use **seaborn** and **plotly** to visualize and analyze the data.", "<center><img src=\"https://i.imgur.com/HrT6xFJ.png\" width=\"300px\"></center>", "## X coordinate\n\nThese are the *x*-coordinates of the players on the field during the games.", "### Distribution plot"], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/nfl-competition-eda-pytorch-cnn\n\nfig = ff.create_distplot(hist_data=[train_df.sample(frac=0.025)[\"X\"]], group_labels=\"X\", colors=['rgb(26, 153, 0)'])\nfig.update_layout(title=\"X coordinate\", yaxis=dict(title=\"Probability Density\"), xaxis=dict(title=\"X coordinate\"))\nfig.show()", "processed": ["eda go use seaborn plotli visual analyz data", "center img src http imgur com hrt6xfj png width 300px center", "x coordin x coordin player field game", "distribut plot"]}, {"markdown": ["In the distribution plot above, we can see that the *x*-coordinates of the players in the dataset has a somewhat bimodal distribution. There are two major peaks in the probability density, at around 35 and 85 yards. This is probably where most players are concentrated on average: in between the midway line and their end of the field. The two peaks probably represent the two teams playing the game.", "## X coordinate vs. Yards"], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/nfl-competition-eda-pytorch-cnn\n\ndata = train_df.sample(frac=0.025)\nquantile = data[\"Yards\"].quantile(0.95)\ndata = data.loc[data[\"Yards\"] < quantile]\nplot = sns.jointplot(x=data[\"X\"], y=data[\"Yards\"], kind='kde', color='forestgreen', height=7)\nplot.set_axis_labels('X coordinate', 'Yards', fontsize=16)\nplt.show(plot)", "processed": ["distribut plot see x coordin player dataset somewhat bimod distribut two major peak probabl densiti around 35 85 yard probabl player concentr averag midway line end field two peak probabl repres two team play game", "x coordin v yard"]}, {"markdown": ["In the KDE plot above, we can see that the *x*-coordinate does not have any clear correlation or relationship with the number of yards gained in the play. The density is concentrated at the center, at around **X = 50** and **Yards = 3**. The probability density decreases rapidly as one moves away from this central region, where most of the data is concentrated.", "## Y coordinate\n\nThese are the *y*-coordinates of the players on the field during the games.", "### Distribution plot"], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/nfl-competition-eda-pytorch-cnn\n\nfig = ff.create_distplot(hist_data=[train_df.sample(frac=0.025)[\"Y\"]], group_labels=\"Y\", colors=['rgb(179, 0, 30)'])\nfig.update_layout(title=\"Y coordinate\", yaxis=dict(title=\"Probability Density\"), xaxis=dict(title=\"Y coordinate\"))\nfig.show()", "processed": ["kde plot see x coordin clear correl relationship number yard gain play densiti concentr center around x 50 yard 3 probabl densiti decreas rapidli one move away central region data concentr", "coordin coordin player field game", "distribut plot"]}, {"markdown": ["In the distribution plot above, we can see that the *y*-coordinates of the players in the dataset has a somewhat normal, unimodal distribution. There ais one major peaks in the probability density, at around **Y = 25** yards, which is near the center of the pitch. The two small peaks at the extreme ends of the distribution probably represent the players on the extreme left and right \"sides\" or \"flanks\" of the field.", "## Y coordinate vs. Yards"], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/nfl-competition-eda-pytorch-cnn\n\ndata = train_df.sample(frac=0.025)\nquantile = data[\"Yards\"].quantile(0.95)\ndata = data.loc[data[\"Yards\"] < quantile]\nplot = sns.jointplot(x=data[\"Y\"], y=data[\"Yards\"], kind='kde', color=(179/255, 0, 30/255), height=7)\nplot.set_axis_labels('Y coordinate', 'Yards', fontsize=16)\nplt.show(plot)", "processed": ["distribut plot see coordin player dataset somewhat normal unimod distribut ai one major peak probabl densiti around 25 yard near center pitch two small peak extrem end distribut probabl repres player extrem left right side flank field", "coordin v yard"]}, {"markdown": ["In the KDE plot above, we can see that the *y*-coordinate does not have any clear correlation or relationship with the number of yards gained in the play. The density is concentrated at the center, at around **Y = 25** and **Yards = 3**. The probability density decreases rapidly as one moves away from this central region, where most of the data is concentrated.", "## X coordinate vs. Y coordinate"], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/nfl-competition-eda-pytorch-cnn\n\ndata = train_df.sample(frac=0.025)\nplot = sns.jointplot(x=data[\"X\"], y=data[\"Y\"], kind='kde', color='mediumvioletred', height=7)\nplot.set_axis_labels('X coordinate', 'Y coordinate', fontsize=16)\nplt.show(plot)", "processed": ["kde plot see coordin clear correl relationship number yard gain play densiti concentr center around 25 yard 3 probabl densiti decreas rapidli one move away central region data concentr", "x coordin v coordin"]}, {"markdown": ["### Empty formation"], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/nfl-competition-eda-pytorch-cnn\n\nplot = sns.jointplot(x=empty_data[\"X\"], y=empty_data[\"Y\"], kind='kde', color=colors[0], height=7)\nplot.set_axis_labels('X coordinate', 'Y coordinate', fontsize=16)\nplt.show(plot)", "processed": ["empti format"]}, {"markdown": ["In the plot above (for the \"empty\" formation), we can see that the *x*-coordinates are concentrated heavily around the **X = 40** mark. Maybe this suggests that attacks of this form occur mainly from the left side of the field, because this value of **X** is closer to the left end of the field than the right end. But, there is also comparatively less dense region of probability density at around **X = 80**, suggesting that attacks can also take place from the other side of the field, but this is less likely.", "### \"I\" formation"], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/nfl-competition-eda-pytorch-cnn\n\nplot = sns.jointplot(x=iform_data[\"X\"], y=iform_data[\"Y\"], kind='kde', color=colors[1], height=7)\nplot.set_axis_labels('X coordinate', 'Y coordinate', fontsize=16)\nplt.show(plot)", "processed": ["plot empti format see x coordin concentr heavili around x 40 mark mayb suggest attack form occur mainli left side field valu x closer left end field right end also compar le den region probabl densiti around x 80 suggest attack also take place side field le like", "format"]}, {"markdown": ["In the plot above (for the \"I\" formation), we can see that most of the *xy*-coordinate data is concentrated around a flat rectangular region in the dead center of the field. The probability density decreases as we move away from this central rectangle. The rectange extends from **X = 10 to 110** and **Y = 15 to 35**. This suggests that most attacks of this type occur from the center of the field. But, there are also minor regions of density at the top and bottom extremes of the field, suggesting that such attacks can also take place from the left and right flanks of the field.", "### Jumbo formation"], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/nfl-competition-eda-pytorch-cnn\n\nplot = sns.jointplot(x=jumbo_data[\"X\"], y=jumbo_data[\"Y\"], kind='kde', color=colors[2], height=7)\nplot.set_axis_labels('X coordinate', 'Y coordinate', fontsize=16)\nplt.show(plot)", "processed": ["plot format see xy coordin data concentr around flat rectangular region dead center field probabl densiti decreas move away central rectangl rectang extend x 10 110 15 35 suggest attack type occur center field also minor region densiti top bottom extrem field suggest attack also take place left right flank field", "jumbo format"]}, {"markdown": ["In the plot above (for the \"jumbo\" formation), we can see that most of the *xy*-coordinate data is heavily concentrated at two regions on the field, at around **X = 10** and **X = 110**. The density in between these two regions is comparatively very low. This suggests that such attacks generally take place at the left and right extremes of the field, and not at the center.", "### Pistol formation"], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/nfl-competition-eda-pytorch-cnn\n\nplot = sns.jointplot(x=pistol_data[\"X\"], y=pistol_data[\"Y\"], kind='kde', color=colors[3], height=7)\nplot.set_axis_labels('X coordinate', 'Y coordinate', fontsize=16)\nplt.show(plot)", "processed": ["plot jumbo format see xy coordin data heavili concentr two region field around x 10 x 110 densiti two region compar low suggest attack gener take place left right extrem field center", "pistol format"]}, {"markdown": ["In the plot above (for the \"pistol\" formation), we can see that most of the *xy*-coordinate data is concentrated around a flat rectangular region in the dead center of the field. The probability density decreases as we move away from this central rectangle. The rectange extends from **X = 10 to 110** and **Y = 15 to 35**. This suggests that most attacks of this type occur from the center of the field. But, there are also minor regions of density at the top and bottom extremes of the field, suggesting that such attacks can also take place from the left and right flanks of the field.", "### Shotgun formation"], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/nfl-competition-eda-pytorch-cnn\n\nplot = sns.jointplot(x=shotgun_data[\"X\"], y=shotgun_data[\"Y\"], kind='kde', color=colors[4], height=7)\nplot.set_axis_labels('X coordinate', 'Y coordinate', fontsize=16)\nplt.show(plot)", "processed": ["plot pistol format see xy coordin data concentr around flat rectangular region dead center field probabl densiti decreas move away central rectangl rectang extend x 10 110 15 35 suggest attack type occur center field also minor region densiti top bottom extrem field suggest attack also take place left right flank field", "shotgun format"]}, {"markdown": ["Once again, in the plot above (for the \"shotgun\" formation), we can see that most of the *xy*-coordinate data is concentrated around a flat rectangular region in the dead center of the field. The probability density decreases as we move away from this central rectangle. The rectange extends from **X = 10 to 110** and **Y = 15 to 35**. This suggests that most attacks of this type occur from the center of the field. But, there are also minor regions of density at the top and bottom extremes of the field, suggesting that such attacks can also take place from the left and right flanks of the field.", "### Singleback formation"], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/nfl-competition-eda-pytorch-cnn\n\nplot = sns.jointplot(x=singleback_data[\"X\"], y=singleback_data[\"Y\"], kind='kde', color=colors[5], height=7)\nplot.set_axis_labels('X coordinate', 'Y coordinate', fontsize=16)\nplt.show(plot)", "processed": ["plot shotgun format see xy coordin data concentr around flat rectangular region dead center field probabl densiti decreas move away central rectangl rectang extend x 10 110 15 35 suggest attack type occur center field also minor region densiti top bottom extrem field suggest attack also take place left right flank field", "singleback format"]}, {"markdown": ["We can see a similar pattern to the \"pistol\" formation here. In the plot above (for the \"singleback\" formation), we can see that most of the *xy*-coordinate data is concentrated around a flat rectangular region in the dead center of the field. The probability density decreases as we move away from this central rectangle. The rectange extends from **X = 10 to 110** and **Y = 15 to 35**. This suggests that most attacks of this type occur from the center of the field. But, there are also minor regions of density at the top and bottom extremes of the field, suggesting that such attacks can also take place from the left and right flanks of the field.", "### Wilcat formation"], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/nfl-competition-eda-pytorch-cnn\n\nplot = sns.jointplot(x=wildcat_data[\"X\"], y=wildcat_data[\"Y\"], kind='kde', color=colors[6], height=7)\nplot.set_axis_labels('X coordinate', 'Y coordinate', fontsize=16)\nplt.show(plot)", "processed": ["see similar pattern pistol format plot singleback format see xy coordin data concentr around flat rectangular region dead center field probabl densiti decreas move away central rectangl rectang extend x 10 110 15 35 suggest attack type occur center field also minor region densiti top bottom extrem field suggest attack also take place left right flank field", "wilcat format"]}, {"markdown": ["In the plot above (for the \"wildcat\" formation), we can see that the *x*-coordinates are concentrated heavily around the **X = 20** and **X = 70** marks. Maybe this suggests that attacks of this form occur from both sides of the field. But, every few attacks of this form at values of **X > 110**.", "## Dir\n\n**Dir** is the angle of the player's motion during the play", "### Distribution plot"], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/nfl-competition-eda-pytorch-cnn\n\nfig = ff.create_distplot(hist_data=[train_df.sample(frac=0.025).query('Dir == Dir')[\"Dir\"]], group_labels=[\"Dir\"], colors=['rgb(255, 102, 25)'])\nfig.update_layout(title=\"Dir\", yaxis=dict(title=\"Probability Density\"), xaxis=dict(title=\"Dir\"))\nfig.show()", "processed": ["plot wildcat format see x coordin concentr heavili around x 20 x 70 mark mayb suggest attack form occur side field everi attack form valu x 110", "dir dir angl player motion play", "distribut plot"]}, {"markdown": ["From the above plot, we can see that the distribution of **Dir** is roughly trimodal (three peaks). One peak occurs approximately at the center at around **Dir = 190**, and the other two peaks occur at the extreme ends of the distribution, at **Dir = 0** and **Dir = 350**. This probably represents two types of motion in players: almost straight motion (0 or 350 degrees) and diagonal motion (190 degrees).", "## Dir vs. Yards"], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/nfl-competition-eda-pytorch-cnn\n\ndata = train_df.sample(frac=0.025)\nquantile = data[\"Yards\"].quantile(0.95)\ndata = data.loc[data[\"Yards\"] < quantile]\nplot = sns.jointplot(x=data[\"Dir\"], y=data[\"Yards\"], kind='kde', color=(255/255, 102/255, 25/255), height=7)\nplot.set_axis_labels('Dir', 'Yards', fontsize=16)\nplt.show(plot)", "processed": ["plot see distribut dir roughli trimod three peak one peak occur approxim center around dir 190 two peak occur extrem end distribut dir 0 dir 350 probabl repres two type motion player almost straight motion 0 350 degre diagon motion 190 degre", "dir v yard"]}, {"markdown": ["In the KDE plot above, we can see that the **Dir** value does not have any clear correlation or relationship with the number of yards gained in the play. The density is concentrated at three symmetric  areas around the center, at **Dir = 0, 190 and 350**. The probability density decreases rapidly as one moves away from these three region, where most of the data is concentrated.", "## Acceleration\n\n**A** is the acceleration of the player in yards per second per second.", "### Distribution plot"], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/nfl-competition-eda-pytorch-cnn\n\nfig = ff.create_distplot(hist_data=[train_df.sample(frac=0.025)[\"A\"]], group_labels=\"A\", colors=['rgb(0, 0, 230)'])\nfig.update_layout(title=\"A\", yaxis=dict(title=\"Probability Density\"), xaxis=dict(title=\"A\"))\nfig.show()", "processed": ["kde plot see dir valu clear correl relationship number yard gain play densiti concentr three symmetr area around center dir 0 190 350 probabl densiti decreas rapidli one move away three region data concentr", "acceler acceler player yard per second per second", "distribut plot"]}, {"markdown": ["In the plot above, we can see that the distribution of **A** is asymmetrical, unimodal, and heavily skewed to the right. The probability density of the acceleration peaks at around **A = 1** yard per second per second. "], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/nfl-competition-eda-pytorch-cnn\n\ndata = train_df.sample(frac=0.025)\nquantile = data[\"Yards\"].quantile(0.95)\ndata = data.loc[data[\"Yards\"] < quantile]\nplot = sns.jointplot(x=data[\"A\"], y=data[\"Yards\"], kind='kde', color=(0, 0, 230/255), height=7)\nplot.set_axis_labels('A', 'Yards', fontsize=16)\nplt.show(plot)", "processed": ["plot see distribut asymmetr unimod heavili skew right probabl densiti acceler peak around 1 yard per second per second"]}, {"markdown": ["In the KDE plot above, we can see that the acceleration does not have any clear correlation or relationship with the number of yards gained in the play. The density is concentrated at the left edge, at around **A = 1** and **Yards = 3**. The probability density decreases as one moves away from this central region, where most of the data is concentrated.", "## Speed", "**S** is the speed of the player in yards per second."], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/nfl-competition-eda-pytorch-cnn\n\nfig = ff.create_distplot(hist_data=[train_df.sample(frac=0.025)[\"S\"]], group_labels=\"S\", colors=['rgb(230, 0, 191)'])\nfig.update_layout(title=\"S\", yaxis=dict(title=\"Probability Density\"), xaxis=dict(title=\"S\"))\nfig.show()", "processed": ["kde plot see acceler clear correl relationship number yard gain play densiti concentr left edg around 1 yard 3 probabl densiti decreas one move away central region data concentr", "speed", "speed player yard per second"]}, {"markdown": ["In the plot above, we can see that the distribution of **S** is asymmetrical, unimodal, and heavily skewed to the right. The probability density of the speed peaks at around **S = 2** yard per second. "], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/nfl-competition-eda-pytorch-cnn\n\ndata = train_df.sample(frac=0.025)\nquantile = data[\"Yards\"].quantile(0.95)\ndata = data.loc[data[\"Yards\"] < quantile]\nplot = sns.jointplot(x=data[\"S\"], y=data[\"Yards\"], kind='kde', color=(230/255, 0, 191/255), height=7)\nplot.set_axis_labels('S', 'Yards', fontsize=16)\nplt.show(plot)", "processed": ["plot see distribut asymmetr unimod heavili skew right probabl densiti speed peak around 2 yard per second"]}, {"markdown": ["In the KDE plot above, we can see that the speed does not have any clear correlation or relationship with the number of yards gained in the play. The density is concentrated at the left edge, at around **S = 2** and **Yards = 3**. The probability density decreases as one moves away from this central region, where most of the data is concentrated.", "## Humidity\n\nHumidity is the percentage (from 0 to 100) of water vapour present in the air during the game.", "### Dsitribution plot"], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/nfl-competition-eda-pytorch-cnn\n\ndata = train_df.sample(frac=0.025)[\"Humidity\"]\nfig = ff.create_distplot(hist_data=[data.fillna(data.mean())], group_labels=[\"Humidity\"], colors=['rgb(0, 102, 102)'])\nfig.update_layout(title=\"Humidity\", yaxis=dict(title=\"Probability Density\"), xaxis=dict(title=\"Humidity\"))\nfig.show()", "processed": ["kde plot see speed clear correl relationship number yard gain play densiti concentr left edg around 2 yard 3 probabl densiti decreas one move away central region data concentr", "humid humid percentag 0 100 water vapour present air game", "dsitribut plot"]}, {"markdown": ["In the plot above, we can see that the distribution of humidity in the dataset has a slight leftward skew and is bimodal in nature. The distribution has two peaks at around **Humidity = 0 and 70**. The first peak, at **Humidity = 0** is very sudden and goes against the gentle leftward skew of the data."], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/nfl-competition-eda-pytorch-cnn\n\ndata = train_df.sample(frac=0.025)\nquantile = data[\"Yards\"].quantile(0.95)\ndata = data.loc[data[\"Yards\"] < quantile]\nplot = sns.jointplot(x=data[\"Humidity\"], y=data[\"Yards\"], kind='kde', color=(0/255, 77/255, 77/255), height=7)\nplot.set_axis_labels('Humidity', 'Yards', fontsize=16)\nplt.show(plot)", "processed": ["plot see distribut humid dataset slight leftward skew bimod natur distribut two peak around humid 0 70 first peak humid 0 sudden goe gentl leftward skew data"]}, {"markdown": ["In the KDE plot above, we can see that the humidity does not have any clear correlation or relationship with the number of yards gained in the play. The density is concentrated at the right edge, at around **Humidity = 70** and **Yards = 3**. There is another high density region at **Humidity = 0**. The probability density decreases as one moves away from these regions, where most of the data is concentrated.", "## Temperature\n\n**Temperature** is the temperature (in degrees Fahrenheit) during the game."], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/nfl-competition-eda-pytorch-cnn\n\ndata = train_df.sample(frac=0.025)[\"Temperature\"]\nfig = ff.create_distplot(hist_data=[data.fillna(data.mean())], group_labels=[\"Temperature\"], colors=['rgb(51, 34, 0)'])\nfig.update_layout(title=\"Temperature\", yaxis=dict(title=\"Probability Density\"), xaxis=dict(title=\"Temperature\"))", "processed": ["kde plot see humid clear correl relationship number yard gain play densiti concentr right edg around humid 70 yard 3 anoth high densiti region humid 0 probabl densiti decreas one move away region data concentr", "temperatur temperatur temperatur degre fahrenheit game"]}, {"markdown": ["In the plot above, we can see that the distribution of humidity in the dataset has a slight leftward skew and is roughly unimodal in nature. The distribution has one peak at around **Temperature = 60** degrees Fahrenheit."], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/nfl-competition-eda-pytorch-cnn\n\ndata = train_df.sample(frac=0.025)\nquantile = data[\"Yards\"].quantile(0.95)\ndata = data.loc[data[\"Yards\"] < quantile]\nplot = sns.jointplot(x=data[\"Temperature\"], y=data[\"Yards\"], kind='kde', color=(51/255, 34/255, 0), height=7)\nplot.set_axis_labels('Temperature', 'Yards', fontsize=16)\nplt.show(plot)", "processed": ["plot see distribut humid dataset slight leftward skew roughli unimod natur distribut one peak around temperatur 60 degre fahrenheit"]}, {"markdown": ["In the KDE plot above, we can see that the humidity does not have any clear correlation or relationship with the number of yards gained in the play. The density is concentrated towards the right side, at around **Temperature = 60** and **Yards = 3**. The probability density decreases as one moves away from this region, where most of the data is concentrated.", "## Team vs. Yards", "### Distribution plot"], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/nfl-competition-eda-pytorch-cnn\n\ndata = train_df.sample(frac=0.025)\nquantile = data[\"Yards\"].quantile(0.95)\ndata = data.loc[data[\"Yards\"] < quantile]\naway_data = data.query('Team == \"away\"')[\"Yards\"]\nhome_data = data.query('Team == \"home\"')[\"Yards\"]\n\nfig = ff.create_distplot(hist_data=[away_data, home_data],\n                         group_labels=[\"Away\", \"Home\"],\n                         show_hist=False)\n\nfig.update_layout(title=\"Team vs. Yards\", yaxis=dict(title=\"Probability Density\"), xaxis=dict(title=\"Yards\"))\nfig.show()", "processed": ["kde plot see humid clear correl relationship number yard gain play densiti concentr toward right side around temperatur 60 yard 3 probabl densiti decreas one move away region data concentr", "team v yard", "distribut plot"]}, {"markdown": ["The above plot, once gain, reinforces the fact that the distributions are very similar.", "## WindDirection vs. Yards", "### Distribution plot"], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/nfl-competition-eda-pytorch-cnn\n\ndata = train_df.sample(frac=0.025)\nquantile = data[\"Yards\"].quantile(0.95)\ndata = data.loc[data[\"Yards\"] < quantile]\nnorth_data = data.query('WindDirection == \"N\"')[\"Yards\"]\neast_data = data.query('WindDirection == \"E\"')[\"Yards\"]\nwest_data = data.query('WindDirection == \"W\"')[\"Yards\"]\nsouth_data = data.query('WindDirection == \"S\"')[\"Yards\"]\n\nfig = ff.create_distplot(hist_data=[north_data, east_data, west_data, south_data],\n                         group_labels=[\"North\", \"East\", \"West\", \"South\"],\n                         show_hist=False)\n\nfig.update_layout(title=\"WindDirection vs. Yards\", yaxis=dict(title=\"Probability Density\"), xaxis=dict(title=\"Yards\"))\nfig.show()", "processed": ["plot gain reinforc fact distribut similar", "winddirect v yard", "distribut plot"]}, {"markdown": ["The above plot, once gain, reinforces the fact that the distributions are very similar, but the **West** distribution has the highest mean and the **East** distribution has the lowest mean.", "## Offense formation vs. Yards", "### Distribution plot"], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/nfl-competition-eda-pytorch-cnn\n\ndata = train_df.sample(frac=0.025)\nquantile = data[\"Yards\"].quantile(0.95)\ndata = data.loc[data[\"Yards\"] < quantile]\nempty_data = data.query('OffenseFormation == \"EMPTY\"')[\"Yards\"]\niform_data = data.query('OffenseFormation == \"I_FORM\"')[\"Yards\"]\njumbo_data = data.query('OffenseFormation == \"JUMBO\"')[\"Yards\"]\npistol_data = data.query('OffenseFormation == \"PISTOL\"')[\"Yards\"]\nshotgun_data = data.query('OffenseFormation == \"SHOTGUN\"')[\"Yards\"]\nsingleback_data = data.query('OffenseFormation == \"SINGLEBACK\"')[\"Yards\"]\nwildcat_data = data.query('OffenseFormation == \"WILDCAT\"')[\"Yards\"]\n\nfig = ff.create_distplot(hist_data=[empty_data, iform_data, jumbo_data,\n                                    pistol_data, shotgun_data, singleback_data, wildcat_data],\n                         group_labels=[\"Empty\", \"I-Form\", \"Jumbo\", \"Pistol\",\n                                       \"Shotgun\", \"Singleback\", \"Wildcat\"],\n                         show_hist=False)\n\nfig.update_layout(title=\"OffenseFormation vs. Yards\", yaxis=dict(title=\"Probability Density\"), xaxis=dict(title=\"Yards\"))\nfig.show()", "processed": ["plot gain reinforc fact distribut similar west distribut highest mean east distribut lowest mean", "offens format v yard", "distribut plot"]}, {"markdown": ["The above plot, once gain, reinforces the fact that **Empty** formations generally result in higher-than-average yards gained, and **Jumbo** formations generally result in lower-than-average yards gained.", "## HomeTeamAbbr vs. Yards", "### Distribution plot"], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/nfl-competition-eda-pytorch-cnn\n\ndata = train_df.sample(frac=0.025)\nquantile = data[\"Yards\"].quantile(0.95)\ndata = data.loc[data[\"Yards\"] < quantile]\nhist_data = [data.loc[data[\"HomeTeamAbbr\"] == home_team_abbr][\"Yards\"] for home_team_abbr in set(data['HomeTeamAbbr'])]\n\nfig = ff.create_distplot(hist_data=hist_data, group_labels=list(set(data['HomeTeamAbbr'])), show_hist=False)\nfig.update_layout(title=\"HomeTeamAbbr vs. Yards\", yaxis=dict(title=\"Probability Density\"), xaxis=dict(title=\"Yards\"))\nfig.show()", "processed": ["plot gain reinforc fact empti format gener result higher averag yard gain jumbo format gener result lower averag yard gain", "hometeamabbr v yard", "distribut plot"]}, {"markdown": ["We can the same pattern in this plot.\n\n* **Highest Yards Gained**: Wahington Redskins, Oakland Raiders, and Green Bay Packers\n* **Lowest Yards Gained**: Chicago Bears and New York Jets", "## VisitorTeamAbbr vs. Yards", "### Distribution plot"], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/nfl-competition-eda-pytorch-cnn\n\ndata = train_df.sample(frac=0.025)\nquantile = data[\"Yards\"].quantile(0.95)\ndata = data.loc[data[\"Yards\"] < quantile]\nhist_data = [data.loc[data[\"VisitorTeamAbbr\"] == visitor_team_abbr][\"Yards\"] for visitor_team_abbr in set(data['VisitorTeamAbbr'])]\n\nfig = ff.create_distplot(hist_data=hist_data, group_labels=list(set(data['VisitorTeamAbbr'])), show_hist=False)\nfig.update_layout(title=\"VisitorTeamAbbr vs. Yards\", yaxis=dict(title=\"Probability Density\"), xaxis=dict(title=\"Yards\"))\nfig.show()", "processed": ["pattern plot highest yard gain wahington redskin oakland raider green bay packer lowest yard gain chicago bear new york jet", "visitorteamabbr v yard", "distribut plot"]}, {"markdown": ["# Statistics", "## Rolling features"], "code": "# Reference: https://www.kaggle.com/code/pestipeti/eda-ion-switching\n\nwindow_sizes = [10, 50, 100, 1000]\n\nfor window in window_sizes:\n    train_df[\"rolling_mean_\" + str(window)] = train_df['signal'].rolling(window=window).mean()\n    train_df[\"rolling_std_\" + str(window)] = train_df['signal'].rolling(window=window).std()\nfig, ax = plt.subplots(len(window_sizes),1,figsize=(20, 6 * len(window_sizes)))\n\nn = 0\nfor col in train_df.columns.values:\n    if \"rolling_\" in col:\n        if \"mean\" in col:\n            mean_df = train_df.iloc[2200000:2210000][col]\n            ax[n].plot(mean_df, label=col, color=\"mediumseagreen\")\n        if \"std\" in col:\n            std = train_df.iloc[2200000:2210000][col].values\n            ax[n].fill_between(mean_df.index.values,\n                               mean_df.values-std, mean_df.values+std,\n                               facecolor='lightgreen',\n                               alpha = 0.5, label=col)\n            ax[n].legend()\n            n+=1", "processed": ["statist", "roll featur"]}, {"markdown": ["## Clases of images\n\nLet's check the classes of images in train_df."], "code": "# Reference: https://www.kaggle.com/code/gpreda/iwildcam-2019-eda-and-prediction\n\ncnt_classes_images = train_df.classes_wild.nunique()\nprint(\"There are {} classes of images\".format(cnt_classes_images))\npd.DataFrame(train_df.classes_wild.value_counts()).transpose()\ndef plot_classes(feature, fs=8, show_percents=True, color_palette='Set3'):\n    f, ax = plt.subplots(1,1, figsize=(2*fs,4))\n    total = float(len(train_df))\n    g = sns.countplot(train_df[feature], order = train_df[feature].value_counts().index, palette=color_palette)\n    g.set_title(\"Number and percentage of labels for each class of {}\".format(feature))\n    if(show_percents):\n        for p in ax.patches:\n            height = p.get_height()\n            ax.text(p.get_x()+p.get_width()/2.,\n                    height + 3,\n                    '{:1.2f}%'.format(100*height/total),\n                    ha=\"center\") \n    plt.show()    \nplot_classes('classes_wild')", "processed": ["clase imag let check class imag train df"]}, {"markdown": ["All these visualizations suffers from one problem: the distribution of majority classes obscures the distribution of minority classes. We will try to create a heatmap for each species. Let's do this showing month and hour for each species, on a separate histogram.\n\n\n## Classes per hour and month"], "code": "# Reference: https://www.kaggle.com/code/gpreda/iwildcam-2019-eda-and-prediction\n\nclasses = train_df.classes_wild.unique()\nfig, ax = plt.subplots(7,2,figsize=(20,28))\ni = 0\nfor class_wild in classes:\n    i = i + 1\n    plt.subplot(7,2,i)\n    tmp = train_df[train_df['classes_wild'] == class_wild]\n    t = pd.DataFrame(tmp.groupby(['month', 'hour'])['seq_id'].count().reset_index())\n    m = t.pivot(index='hour', columns='month', values='seq_id')\n    s = sns.heatmap(m, linewidths=.1, linecolor='black', annot=False, cmap=\"Greens\")\n    if(i<13):\n        s.set_xlabel('')    \n    s.set_title(class_wild, size=12)\n\nplt.show()", "processed": ["visual suffer one problem distribut major class obscur distribut minor class tri creat heatmap speci let show month hour speci separ histogram class per hour month"]}, {"markdown": ["## Classes per rights holder and month"], "code": "# Reference: https://www.kaggle.com/code/gpreda/iwildcam-2019-eda-and-prediction\n\nclasses = train_df.classes_wild.unique()\nfig, ax = plt.subplots(7,2,figsize=(16,24))\ni = 0\nfor class_wild in classes:\n    i = i + 1\n    plt.subplot(7,2,i)\n    tmp = train_df[train_df['classes_wild'] == class_wild]\n    t = pd.DataFrame(tmp.groupby(['rights_holder', 'month'])['seq_id'].count().reset_index())\n    m = t.pivot(index='rights_holder', columns='month', values='seq_id')\n    s = sns.heatmap(m, linewidths=.1, linecolor='black', annot=False, cmap=\"Blues\")\n    if(i<13):\n        s.set_xlabel('')    \n    s.set_title(class_wild, size=12)\n\nplt.show()", "processed": ["class per right holder month"]}, {"markdown": ["## Validation"], "code": "# Reference: https://www.kaggle.com/code/gpreda/iwildcam-2019-eda-and-prediction\n\nwith open('history.json', 'w') as f:\n    json.dump(history.history, f)\nh_df = pd.DataFrame(history.history)\nh_df['val_f1'] = f1_metrics.val_f1s\nh_df['val_precision'] = f1_metrics.val_precisions\nh_df['val_recall'] = f1_metrics.val_recalls\nepochs = range(len(h_df['val_f1']))\nplt.figure()\nfig, ax = plt.subplots(1,3,figsize=(18,4))\nax[0].plot(epochs,h_df['loss'], label='Training loss')\nax[0].plot(epochs,h_df['val_loss'], label='Validation loss')\nax[0].set_title('Training and validation loss')\nax[0].legend()\nax[1].plot(epochs,h_df['acc'],label='Training accuracy')\nax[1].plot(epochs,h_df['val_acc'], label='Validation accuracy')\nax[1].set_title('Training and validation accuracy')\nax[1].legend()\nax[2].plot(epochs,h_df['val_f1'],label='Validation f1-score')\nax[2].plot(epochs,h_df['val_precision'],label='Validation precision')\nax[2].plot(epochs,h_df['val_recall'],label='Validation recall')\nax[2].set_title('Validation f1-score, precision & recall')\nax[2].legend()\nplt.show()", "processed": ["valid"]}, {"markdown": ["That's not too shabby - AUC of 0.61 between train and test sets indicates high variability between distinct features. Unfortunately most of our featues come from the transformers embedding space(s), so it will not be easy to interpret whcih ones are responsible for what. Nonetheless, let's try to take a quick look."], "code": "# Reference: https://www.kaggle.com/code/tunguz/adversarial-quest\n\nfeatures = ['feature_'+str(x) for x in range(3142)]\n\nfeature_imp = pd.DataFrame(sorted(zip(clf.feature_importance(),features)), columns=['Value','Feature'])\n\nplt.figure(figsize=(20, 20))\nsns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False).head(100))\nplt.title('LightGBM Features')\nplt.tight_layout()\nplt.show()\nplt.savefig('lgbm_importances-01.png')", "processed": ["shabbi auc 0 61 train test set indic high variabl distinct featur unfortun featu come transform embed space easi interpret whcih one respons nonetheless let tri take quick look"]}, {"markdown": ["Blue values show histograms for train data, green - test data."], "code": "# Reference: https://www.kaggle.com/code/artgor/where-do-the-robots-drive\n\nplt.figure(figsize=(26, 16))\nfor i, col in enumerate(train.columns[3:]):\n    plt.subplot(3, 4, i + 1)\n    plt.hist(train[col], color='blue', bins=100)\n    plt.hist(test[col], color='green', bins=100)\n    plt.title(col)", "processed": ["blue valu show histogram train data green test data"]}, {"markdown": ["Velocity and acceleration have normal distribution, orientation features seem to have normalized values (using tanh function).\n\nFeature distributions in train and test are quite similar.", "Let's have a look at the values of features in a single time-series"], "code": "# Reference: https://www.kaggle.com/code/artgor/where-do-the-robots-drive\n\nplt.figure(figsize=(26, 16))\nfor i, col in enumerate(train.columns[3:]):\n    plt.subplot(3, 4, i + 1)\n    plt.plot(train.loc[train['series_id'] == 1, col])\n    plt.title(col)", "processed": ["veloc acceler normal distribut orient featur seem normal valu use tanh function featur distribut train test quit similar", "let look valu featur singl time seri"]}, {"markdown": ["Function to train models:"], "code": "# Reference: https://www.kaggle.com/code/artgor/where-do-the-robots-drive\n\ndef eval_acc(preds, dtrain):\n    labels = dtrain.get_label()\n    return 'acc', accuracy_score(labels, preds.argmax(1)), True\n\ndef train_model(X, X_test, y, params=None, folds=folds, model_type='lgb', plot_feature_importance=False, model=None, groups=y['group_id']):\n\n    oof = np.zeros((len(X), 9))\n    prediction = np.zeros((len(X_test), 9))\n    scores = []\n    feature_importance = pd.DataFrame()\n    for fold_n, (train_index, valid_index) in enumerate(folds.split(X, y, groups)):\n        print('Fold', fold_n, 'started at', time.ctime())\n        X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n        y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n        \n        if model_type == 'lgb':\n            model = lgb.LGBMClassifier(**params, n_estimators = 10000, n_jobs = -1)\n            model.fit(X_train, y_train, \n                    eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric='multi_logloss',\n                    verbose=5000, early_stopping_rounds=200)\n            \n            y_pred_valid = model.predict_proba(X_valid)\n            y_pred = model.predict_proba(X_test, num_iteration=model.best_iteration_)\n            \n        if model_type == 'xgb':\n            train_data = xgb.DMatrix(data=X_train, label=y_train, feature_names=X.columns)\n            valid_data = xgb.DMatrix(data=X_valid, label=y_valid, feature_names=X.columns)\n\n            watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]\n            model = xgb.train(dtrain=train_data, num_boost_round=20000, evals=watchlist, early_stopping_rounds=200, verbose_eval=500, params=params)\n            y_pred_valid = model.predict(xgb.DMatrix(X_valid, feature_names=X.columns), ntree_limit=model.best_ntree_limit)\n            y_pred = model.predict(xgb.DMatrix(X_test, feature_names=X.columns), ntree_limit=model.best_ntree_limit)\n        \n        if model_type == 'sklearn':\n            model = model\n            model.fit(X_train, y_train)\n            \n            y_pred_valid = model.predict_proba(X_valid)\n            score = accuracy_score(y_valid, y_pred_valid.argmax(1))\n            print(f'Fold {fold_n}. Accuracy: {score:.4f}.')\n            print('')\n            \n            y_pred = model.predict_proba(X_test)\n        \n        if model_type == 'cat':\n            model = CatBoostClassifier(iterations=20000,  eval_metric='MAE', **params)\n            model.fit(X_train, y_train, eval_set=(X_valid, y_valid), cat_features=[], use_best_model=True, verbose=False)\n\n            y_pred_valid = model.predict(X_valid)\n            y_pred = model.predict(X_test)\n        \n        oof[valid_index] = y_pred_valid\n        scores.append(accuracy_score(y_valid, y_pred_valid.argmax(1)))\n\n        prediction += y_pred    \n        \n        if model_type == 'lgb':\n            # feature importance\n            fold_importance = pd.DataFrame()\n            fold_importance[\"feature\"] = X.columns\n            fold_importance[\"importance\"] = model.feature_importances_\n            fold_importance[\"fold\"] = fold_n + 1\n            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n\n    prediction /= n_fold\n    \n    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n    \n    if model_type == 'lgb':\n        feature_importance[\"importance\"] /= n_fold\n        if plot_feature_importance:\n            cols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n                by=\"importance\", ascending=False)[:50].index\n\n            best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n\n            plt.figure(figsize=(16, 12));\n            sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False));\n            plt.title('LGB Features (avg over folds)');\n        \n            return oof, prediction, feature_importance\n        return oof, prediction\n    \n    else:\n        return oof, prediction\nparams = {'num_leaves': 123,\n          'min_data_in_leaf': 12,\n          'objective': 'multiclass',\n          'max_depth': 22,\n          'learning_rate': 0.04680350949723872,\n          \"boosting\": \"gbdt\",\n          \"bagging_freq\": 5,\n          \"bagging_fraction\": 0.8933018355190274,\n          \"bagging_seed\": 11,\n          \"verbosity\": -1,\n          'reg_alpha': 0.9498109326932401,\n          'reg_lambda': 0.8058490960546196,\n          \"num_class\": 9,\n          'nthread': -1,\n          'min_split_gain': 0.009913227240564853,\n          'subsample': 0.9027358830703129\n         }\n\n\noof_lgb, prediction_lgb, feature_importance = train_model(X=train_df, X_test=test_df, y=y['surface'], params=params, model_type='lgb', plot_feature_importance=True)\n# I use code from this kernel: https://www.kaggle.com/theoviel/deep-learning-starter\nimport itertools\n\ndef plot_confusion_matrix(truth, pred, classes, normalize=False, title=''):\n    cm = confusion_matrix(truth, pred)\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n    \n    plt.figure(figsize=(10, 10))\n    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n    plt.title('Confusion matrix', size=15)\n    plt.colorbar(fraction=0.046, pad=0.04)\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.grid(False)\n    plt.tight_layout()\nplot_confusion_matrix(y['surface'], oof_lgb.argmax(1), le.classes_)", "processed": ["function train model"]}, {"markdown": ["# Load files and time stamps\nWe will load the output from  [Hung The Nguyen's][1] kernel [here][2]. And attach time stamps.\n\n[1]: https://www.kaggle.com/hung96ad\n[2]: https://www.kaggle.com/hung96ad/new-blend"], "code": "# Reference: https://www.kaggle.com/code/cdeotte/private-leaderboard-0-750\n\n# Lower detection rate for August and September computers\nAdjustPublicScore = True\n# 0=No, 1=Remove Nov 20,21,22,23,24,25, 2=Downward trend Nov 20,21,22,23,24,25\nAdjustPrivateScore = 2\n\ndtypes = {}\ndtypes['MachineIdentifier'] = 'str'\ndtypes['AvSigVersion'] = 'category'\ndtypes['HasDetections'] = 'int8'\n\n# LOAD TRAIN DATA\ndf_train = pd.read_csv('../input/microsoft-malware-prediction/train.csv', usecols=list(dtypes.keys()), dtype=dtypes)\nprint ('Loaded',len(df_train),'rows of train.CSV!')\n\n# LOAD TEST DATA\ndf_test = pd.read_csv('../input/microsoft-malware-prediction/test.csv', usecols=list(dtypes.keys())[0:-1], dtype=dtypes)\nprint ('Loaded',len(df_test),'rows of test.CSV!')\n\n# LOAD PREDICTIONS FROM PUBLIC KERNEL\n# https://www.kaggle.com/hung96ad/new-blend\ndf_test2 = pd.read_csv('../input/kagglebest/super_blend.csv')\nprint ('Loaded',len(df_test),'rows of super_blend.csv!')\n\n# ADD TIMESTAMPS\ndatedictAS = np.load('../input/malware-timestamps/AvSigVersionTimestamps.npy')[()]\ndf_test['Date'] = df_test['AvSigVersion'].map(datedictAS)\ndf_train['Date'] = df_train['AvSigVersion'].map(datedictAS)\ndf_test2 = pd.merge(df_test2, df_test, on='MachineIdentifier', how='left')\ndf_test2['AvSigVersion2'] = df_test2['AvSigVersion'].map(lambda x: np.int(x.split('.')[1]) )\nimport calendar, math\n\ndef staticPlot(data, col, target='HasDetections', bars=10, show=1.0, sortby='frequency'\n               , verbose=1, top=5, title='',asc=False, dropna=False, minn=0.0):\n    # calcuate density and detection rate\n    cv = data[col].value_counts(dropna=dropna)\n    cvd = cv.to_dict()\n    nm = cv.index.values; lnn = len(nm); lnn2 = lnn\n    th = show * len(data)\n    th2 = minn * len(data)\n    sum = 0; lnn2 = 0\n    for x in nm[0:bars]:\n        lnn2 += 1\n        try: sum += cvd[x]\n        except: sum += cv[x]\n        if sum>th:\n            break\n        try:\n            if cvd[x]<th2: break\n        except:\n            if cv[x]<th2: break\n    if lnn2<bars: bars = lnn2\n    pct = round(100.0*sum/len(data),2)\n    lnn = min(lnn,lnn2)\n    ratio = [0.0]*lnn; lnn3 = lnn\n    if sortby =='frequency': lnn3 = min(lnn3,bars)\n    elif sortby=='category': lnn3 = 0\n    for i in range(lnn3):\n        if target not in data:\n            ratio[i] = np.nan\n        elif nan_check(nm[i]):\n            ratio[i] = data[target][data[col].isna()].mean()\n        else:\n            ratio[i] = data[target][data[col]==nm[i]].mean()\n    try: all = pd.DataFrame( {'category':nm[0:lnn],'frequency':[cvd[x] for x in nm[0:lnn]],'rate':ratio} )\n    except: all = pd.DataFrame( {'category':nm[0:lnn],'frequency':[cv[x] for x in nm[0:lnn]],'rate':ratio} )\n    if sortby=='rate': \n        all = all.sort_values(sortby, ascending=asc)\n    elif sortby=='category':\n        try: \n            all['temp'] = all['category'].astype('float')\n            all = all.sort_values('temp', ascending=asc)\n        except:\n            all = all.sort_values('category', ascending=asc)\n    if bars<lnn: all = all[0:bars]\n    if verbose==1 and target in data:\n        print('TRAIN.CSV variable',col,'has',len(nm),'categories')\n        print('The',min(bars,lnn),'bars displayed here contain',pct,'% of data.')\n        mlnn = data[col].isna().sum()\n        print(\"The data has %.1f %% NA. The plot is sorted by \" % (100.0*mlnn/len(data)) + sortby )\n    \n    # plot density and detection rate\n    fig = plt.figure(1,figsize=(15,3))\n    ax1 = fig.add_subplot(1,1,1)\n    clrs = ['red', 'green', 'blue', 'yellow', 'magenta']\n    barss = ax1.bar([str(x) for x in all['category']],[x/float(len(data)) for x in all['frequency']],color=clrs)\n    for i in range(len(all)-top):\n        barss[top+i].set_color('cyan')\n    if target in data:\n        ax2 = ax1.twinx()\n        if sortby!='category': infected = all['rate'][0:lnn]\n        else:\n            infected=[]\n            for x in all['category']:\n                if nan_check(x): infected.append( data[ data[col].isna() ][target].mean() )\n                elif cvd[x]!=0: infected.append( data[ data[col]==x ][target].mean() )\n                else: infected.append(-1)\n        ax2.plot([str(x) for x in all['category']],infected[0:lnn],'k:o')\n        #ax2.set_ylim(a,b)\n        ax2.spines['left'].set_color('red')\n        ax2.set_ylabel('Detection Rate', color='k')\n    ax1.spines['left'].set_color('red')\n    ax1.yaxis.label.set_color('red')\n    ax1.tick_params(axis='y', colors='red')\n    ax1.set_ylabel('Category Proportion', color='r')\n    if title!='': plt.title(title)\n    plt.show()\n    if verbose==1 and target not in data:\n        print('TEST.CSV variable',col,'has',len(nm),'categories')\n        print('The',min(bars,lnn),'bars displayed here contain',pct,'% of the data.')\n        mlnn = data[col].isna().sum()\n        print(\"The data has %.1f %% NA. The plot is sorted by \" % (100.0*mlnn/len(data)) + sortby )\n\ndef dynamicPlot(data,col, target='HasDetections', start=datetime(2018,4,1), end=datetime(2018,12,1)\n                ,inc_hr=0,inc_dy=7,inc_mn=0,show=0.99,top=5,top2=4,title='',legend=1,z=0,dots=False):\n    # check for timestamps\n    if 'Date' not in data:\n        print('Error dynamicPlot: DataFrame needs column Date of datetimes')\n        return\n    \n    # remove detection line if category density is too small\n    cv = data[(data['Date']>start) & (data['Date']<end)][col].value_counts(dropna=False)\n    cvd = cv.to_dict()\n    nm = cv.index.values\n    th = show * len(data)\n    sum = 0; lnn2 = 0\n    for x in nm:\n        lnn2 += 1\n        sum += cvd[x]\n        if sum>th:\n            break\n    top = min(top,len(nm))\n    top2 = min(top2,len(nm),lnn2,top)\n\n    # calculate rate within each time interval\n    diff = (end-start).days*24*3600 + (end-start).seconds\n    size = diff//(3600*((inc_mn * 28 + inc_dy) * 24 + inc_hr)) + 5\n    data_counts = np.zeros([size,2*top+1],dtype=float)\n    idx=0; idx2 = {}\n    for i in range(top):\n        idx2[nm[i]] = i+1\n    low = start\n    high = add_time(start,inc_mn,inc_dy,inc_hr)\n    data_times = [low+(high-low)/2]\n    while low<end:\n        slice = data[ (data['Date']<high) & (data['Date']>=low) ]\n        #data_counts[idx,0] = len(slice)\n        data_counts[idx,0] = 5000*len(slice['AvSigVersion'].unique())\n        for key in idx2:\n            if nan_check(key): slice2 = slice[slice[col].isna()]\n            else: slice2 = slice[slice[col]==key]\n            data_counts[idx,idx2[key]] = len(slice2)\n            if target in data:\n                data_counts[idx,top+idx2[key]] = slice2['HasDetections'].mean()\n        low = high\n        high = add_time(high,inc_mn,inc_dy,inc_hr)\n        data_times.append(low+(high-low)/2)\n        idx += 1\n\n    # plot lines\n    fig = plt.figure(1,figsize=(15,3))\n    cl = ['r','g','b','y','m']\n    ax3 = fig.add_subplot(1,1,1)\n    lines = []; labels = []\n    if z==1: ax3.plot(data_times,data_counts[0:idx+1,0],'k')\n    for i in range(top):\n        tmp, = ax3.plot(data_times,data_counts[0:idx+1,i+1],cl[i%5])\n        if dots: ax3.plot(data_times,data_counts[0:idx+1,i+1],cl[i%5]+'o')\n        lines.append(tmp)\n        labels.append(str(nm[i]))\n    ax3.spines['left'].set_color('red')\n    ax3.yaxis.label.set_color('red')\n    ax3.tick_params(axis='y', colors='red')\n    if col!='ones': ax3.set_ylabel('Category Density', color='r')\n    else: ax3.set_ylabel('Data Density', color='r')\n    #ax3.set_yticklabels([])\n    if target in data:\n        ax4 = ax3.twinx()\n        for i in range(top2):\n            ax4.plot(data_times,data_counts[0:idx+1,i+1+top],cl[i%5]+\":\")\n            if dots: ax4.plot(data_times,data_counts[0:idx+1,i+1+top],cl[i%5]+\"o\")\n        ax4.spines['left'].set_color('red')\n        ax4.set_ylabel('Detection Rate', color='k')\n    if title!='': plt.title(title)\n    if legend==1: plt.legend(lines,labels,loc=2)\n    plt.show()\n        \n# INCREMENT A DATETIME\ndef add_time(sdate,months=0,days=0,hours=0):\n    month = sdate.month -1 + months\n    year = sdate.year + month // 12\n    month = month % 12 + 1\n    day = sdate.day + days\n    if day>calendar.monthrange(year,month)[1]:\n        day -= calendar.monthrange(year,month)[1]\n        month += 1\n        if month>12:\n            month = 1\n            year += 1\n    hour = sdate.hour + hours\n    if hour>23:\n        hour = 0\n        day += 1\n        if day>calendar.monthrange(year,month)[1]:\n            day -= calendar.monthrange(year,month)[1]\n            month += 1\n            if month>12:\n                month = 1\n                year += 1\n    return datetime(year,month,day,hour,sdate.minute)\n\n# CHECK FOR NAN\ndef nan_check(x):\n    if isinstance(x,float):\n        if math.isnan(x):\n            return True\n    return False", "processed": ["load file time stamp load output hung nguyen 1 kernel 2 attach time stamp 1 http www kaggl com hung96ad 2 http www kaggl com hung96ad new blend"]}, {"markdown": ["# Plot the Average vs Max Prediction Probability - Fake vs Real\nThe model appears to preform fairly poorly but shows that it is picking up on some signal.", "## c23 model results"], "code": "# Reference: https://www.kaggle.com/code/robikscube/faceforensics-baseline-dlib-no-internet\n\nfig, ax = plt.subplots(1,1, figsize=(10, 10))\nsns.scatterplot(x='avg_pred_c23', y='max_pred_c23', data=metadata.dropna(subset=['avg_pred_c23']), hue='label')\nplt.show()", "processed": ["plot averag v max predict probabl fake v real model appear preform fairli poorli show pick signal", "c23 model result"]}, {"markdown": ["## raw model results"], "code": "# Reference: https://www.kaggle.com/code/robikscube/faceforensics-baseline-dlib-no-internet\n\nfig, ax = plt.subplots(1,1, figsize=(10, 10))\nsns.scatterplot(x='avg_pred_raw', y='max_pred_raw', data=metadata.dropna(subset=['avg_pred_raw']), hue='label')\nplt.show()\nfor i, d in metadata.groupby('label'):\n    d['avg_pred_c23'].plot(kind='hist', figsize=(15, 5), bins=20, alpha=0.8, title='Average Prediction distribution c23')\n    plt.legend(['FAKE','REAL'])\nplt.show()\nfor i, d in metadata.groupby('label'):\n    d['max_pred_c23'].plot(kind='hist', figsize=(15, 5), bins=20, title='Max Prediction distribution c23', alpha=0.8)\n    plt.legend(['FAKE','REAL'])\nplt.show()\nfor i, d in metadata.groupby('label'):\n    d['avg_pred_raw'].plot(kind='hist',\n                           figsize=(15, 5),\n                           bins=20,\n                           alpha=0.8,\n                           title='Average Prediction distribution raw')\n    plt.legend(['FAKE','REAL'])\nplt.show()\nfor i, d in metadata.groupby('label'):\n    d['max_pred_raw'].plot(kind='hist',\n                           figsize=(15, 5),\n                           bins=20,\n                           title='Max Prediction distribution raw',\n                           alpha=0.8)\n    plt.legend(['FAKE','REAL'])\nplt.show()\nmetadata['max_pred_c23'] = metadata['max_pred_c23'].round(6)\nmetadata.dropna(subset=['max_pred_c23']).sort_values('label')\nmetadata['label_binary'] = 0\nmetadata.loc[metadata['label'] == \"FAKE\", 'label_binary'] = 1", "processed": ["raw model result"]}, {"markdown": ["At first let's take 10 random assets and plot them."], "code": "# Reference: https://www.kaggle.com/code/artgor/eda-feature-engineering-and-everything\n\ndata = []\nfor asset in np.random.choice(market_train_df['assetName'].unique(), 10):\n    asset_df = market_train_df[(market_train_df['assetName'] == asset)]\n\n    data.append(go.Scatter(\n        x = asset_df['time'].dt.strftime(date_format='%Y-%m-%d').values,\n        y = asset_df['close'].values,\n        name = asset\n    ))\nlayout = go.Layout(dict(title = \"Closing prices of 10 random assets\",\n                  xaxis = dict(title = 'Month'),\n                  yaxis = dict(title = 'Price (USD)'),\n                  ),legend=dict(\n                orientation=\"h\"))\npy.iplot(dict(data=data, layout=layout), filename='basic-line')", "processed": ["first let take 10 random asset plot"]}, {"markdown": ["I plot data for all periods because I'd like to show long-term trends.\nAssets are sampled randomly, but you should see that some companies' stocks started trading later, some dissappeared. Disappearence could be due to bankruptcy, acquisition or other reasons.", "Well, these were some random companies. But it would be more interesting to see general trends of prices."], "code": "# Reference: https://www.kaggle.com/code/artgor/eda-feature-engineering-and-everything\n\ndata = []\n#market_train_df['close'] = market_train_df['close'] / 20\nfor i in [0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95]:\n    price_df = market_train_df.groupby('time')['close'].quantile(i).reset_index()\n\n    data.append(go.Scatter(\n        x = price_df['time'].dt.strftime(date_format='%Y-%m-%d').values,\n        y = price_df['close'].values,\n        name = f'{i} quantile'\n    ))\nlayout = go.Layout(dict(title = \"Trends of closing prices by quantiles\",\n                  xaxis = dict(title = 'Month'),\n                  yaxis = dict(title = 'Price (USD)'),\n                  ),legend=dict(\n                orientation=\"h\"))\npy.iplot(dict(data=data, layout=layout), filename='basic-line')", "processed": ["plot data period like show long term trend asset sampl randomli see compani stock start trade later dissappear disappear could due bankruptci acquisit reason", "well random compani would interest see gener trend price"]}, {"markdown": ["It is cool to be able to see how markets fall and rise again.\nI have shown 4 events when there were serious stock price drops on the market.\nYou could also notice that higher quantile prices have increased with time and lower quantile prices decreased.\nMaybe the gap between poor and rich increases... on the other hand maybe more \"little\" companies are ready to go to market and prices of their shares isn't very high.", "Now, let's look at these price drops in details."], "code": "# Reference: https://www.kaggle.com/code/artgor/eda-feature-engineering-and-everything\n\nmarket_train_df['price_diff'] = market_train_df['close'] - market_train_df['open']\ngrouped = market_train_df.groupby('time').agg({'price_diff': ['std', 'min']}).reset_index()\nprint(f\"Average standard deviation of price change within a day in {grouped['price_diff']['std'].mean():.4f}.\")\ng = grouped.sort_values(('price_diff', 'std'), ascending=False)[:10]\ng['min_text'] = 'Maximum price drop: ' + (-1 * g['price_diff']['min']).astype(str)\ntrace = go.Scatter(\n    x = g['time'].dt.strftime(date_format='%Y-%m-%d').values,\n    y = g['price_diff']['std'].values,\n    mode='markers',\n    marker=dict(\n        size = g['price_diff']['std'].values,\n        color = g['price_diff']['std'].values,\n        colorscale='Portland',\n        showscale=True\n    ),\n    text = g['min_text'].values\n    #text = f\"Maximum price drop: {g['price_diff']['min'].values}\"\n    #g['time'].dt.strftime(date_format='%Y-%m-%d').values\n)\ndata = [trace]\n\nlayout= go.Layout(\n    autosize= True,\n    title= 'Top 10 months by standard deviation of price change within a day',\n    hovermode= 'closest',\n    yaxis=dict(\n        title= 'price_diff',\n        ticklen= 5,\n        gridwidth= 2,\n    ),\n    showlegend= False\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig,filename='scatter2010')", "processed": ["cool abl see market fall rise shown 4 event seriou stock price drop market could also notic higher quantil price increas time lower quantil price decreas mayb gap poor rich increas hand mayb littl compani readi go market price share high", "let look price drop detail"]}, {"markdown": ["Now let's try to build that graph again."], "code": "# Reference: https://www.kaggle.com/code/artgor/eda-feature-engineering-and-everything\n\nmarket_train_df['price_diff'] = market_train_df['close'] - market_train_df['open']\ngrouped = market_train_df.groupby(['time']).agg({'price_diff': ['std', 'min']}).reset_index()\ng = grouped.sort_values(('price_diff', 'std'), ascending=False)[:10]\ng['min_text'] = 'Maximum price drop: ' + (-1 * np.round(g['price_diff']['min'], 2)).astype(str)\ntrace = go.Scatter(\n    x = g['time'].dt.strftime(date_format='%Y-%m-%d').values,\n    y = g['price_diff']['std'].values,\n    mode='markers',\n    marker=dict(\n        size = g['price_diff']['std'].values * 5,\n        color = g['price_diff']['std'].values,\n        colorscale='Portland',\n        showscale=True\n    ),\n    text = g['min_text'].values\n    #text = f\"Maximum price drop: {g['price_diff']['min'].values}\"\n    #g['time'].dt.strftime(date_format='%Y-%m-%d').values\n)\ndata = [trace]\n\nlayout= go.Layout(\n    autosize= True,\n    title= 'Top 10 months by standard deviation of price change within a day',\n    hovermode= 'closest',\n    yaxis=dict(\n        title= 'price_diff',\n        ticklen= 5,\n        gridwidth= 2,\n    ),\n    showlegend= False\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig,filename='scatter2010')", "processed": ["let tri build graph"]}, {"markdown": ["Now the graph is much more reasonable.", "Now let's take a look at out target variable."], "code": "# Reference: https://www.kaggle.com/code/artgor/eda-feature-engineering-and-everything\n\ndata = []\nfor i in [0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95]:\n    price_df = market_train_df.groupby('time')['returnsOpenNextMktres10'].quantile(i).reset_index()\n\n    data.append(go.Scatter(\n        x = price_df['time'].dt.strftime(date_format='%Y-%m-%d').values,\n        y = price_df['returnsOpenNextMktres10'].values,\n        name = f'{i} quantile'\n    ))\nlayout = go.Layout(dict(title = \"Trends of returnsOpenNextMktres10 by quantiles\",\n                  xaxis = dict(title = 'Month'),\n                  yaxis = dict(title = 'Price (USD)'),\n                  ),legend=dict(\n                orientation=\"h\"),)\npy.iplot(dict(data=data, layout=layout), filename='basic-line')", "processed": ["graph much reason", "let take look target variabl"]}, {"markdown": ["We can see that quantiles have a high deviation, but mean value doesn't change much.\n\nNow I think it is time to throw an old part of dataset. Let's leave only data since 2010 year, this way we will get rid of the data of the biggest crisis.", "Let's look at the target variable now."], "code": "# Reference: https://www.kaggle.com/code/artgor/eda-feature-engineering-and-everything\n\ndata = []\nmarket_train_df = market_train_df.loc[market_train_df['time'] >= '2010-01-01 22:00:00+0000']\n\nprice_df = market_train_df.groupby('time')['returnsOpenNextMktres10'].mean().reset_index()\n\ndata.append(go.Scatter(\n    x = price_df['time'].dt.strftime(date_format='%Y-%m-%d').values,\n    y = price_df['returnsOpenNextMktres10'].values,\n    name = f'{i} quantile'\n))\nlayout = go.Layout(dict(title = \"Treand of returnsOpenNextMktres10 mean\",\n                  xaxis = dict(title = 'Month'),\n                  yaxis = dict(title = 'Price (USD)'),\n                  ),legend=dict(\n                orientation=\"h\"),)\npy.iplot(dict(data=data, layout=layout), filename='basic-line')", "processed": ["see quantil high deviat mean valu chang much think time throw old part dataset let leav data sinc 2010 year way get rid data biggest crisi", "let look target variabl"]}, {"markdown": ["Fluctuations seem to be high, but in fact they are lower that 8 percent. In fact it looks like a random noise...", "Now let's remember the description:\n```\nThe marketdata contains a variety of returns calculated over different timespans. All of the returns in this set of marketdata have these properties:\n\n    Returns are always calculated either open-to-open (from the opening time of one trading day to the open of another) or close-to-close (from the closing time of one trading day to the open of another).\n    Returns are either raw, meaning that the data is not adjusted against any benchmark, or market-residualized (Mktres), meaning that the movement of the market as a whole has been accounted for, leaving only movements inherent to the instrument.\n    Returns can be calculated over any arbitrary interval. Provided here are 1 day and 10 day horizons.\n    Returns are tagged with 'Prev' if they are backwards looking in time, or 'Next' if forwards looking.\n```\n\nLet's have a look at means of these variables."], "code": "# Reference: https://www.kaggle.com/code/artgor/eda-feature-engineering-and-everything\n\ndata = []\nfor col in ['returnsClosePrevRaw1', 'returnsOpenPrevRaw1',\n       'returnsClosePrevMktres1', 'returnsOpenPrevMktres1',\n       'returnsClosePrevRaw10', 'returnsOpenPrevRaw10',\n       'returnsClosePrevMktres10', 'returnsOpenPrevMktres10',\n       'returnsOpenNextMktres10']:\n    df = market_train_df.groupby('time')[col].mean().reset_index()\n    data.append(go.Scatter(\n        x = df['time'].dt.strftime(date_format='%Y-%m-%d').values,\n        y = df[col].values,\n        name = col\n    ))\n    \nlayout = go.Layout(dict(title = \"Treand of mean values\",\n                  xaxis = dict(title = 'Month'),\n                  yaxis = dict(title = 'Price (USD)'),\n                  ),legend=dict(\n                orientation=\"h\"),)\npy.iplot(dict(data=data, layout=layout), filename='basic-line')", "processed": ["fluctuat seem high fact lower 8 percent fact look like random nois", "let rememb descript marketdata contain varieti return calcul differ timespan return set marketdata properti return alway calcul either open open open time one trade day open anoth close close close time one trade day open anoth return either raw mean data adjust benchmark market residu mktre mean movement market whole account leav movement inher instrument return calcul arbitrari interv provid 1 day 10 day horizon return tag prev backward look time next forward look let look mean variabl"]}, {"markdown": ["The file is too huge to work with text directly, so let's see a wordcloud of the last 100000 headlines."], "code": "# Reference: https://www.kaggle.com/code/artgor/eda-feature-engineering-and-everything\n\ntext = ' '.join(news_train_df['headline'].str.lower().values[-1000000:])\nwordcloud = WordCloud(max_font_size=None, stopwords=stop, background_color='white',\n                      width=1200, height=1000).generate(text)\nplt.figure(figsize=(12, 8))\nplt.imshow(wordcloud)\nplt.title('Top words in headline')\nplt.axis(\"off\")\nplt.show()\n# Let's also limit the time period\nnews_train_df = news_train_df.loc[news_train_df['time'] >= '2010-01-01 22:00:00+0000']\n(news_train_df['urgency'].value_counts() / 1000000).plot('bar');\nplt.xticks(rotation=30);\nplt.title('Urgency counts (mln)');", "processed": ["file huge work text directli let see wordcloud last 100000 headlin"]}, {"markdown": ["Well, it seems that in fact urgency \"2\" is almost never used."], "code": "# Reference: https://www.kaggle.com/code/artgor/eda-feature-engineering-and-everything\n\nnews_train_df['sentence_word_count'] =  news_train_df['wordCount'] / news_train_df['sentenceCount']\nplt.boxplot(news_train_df['sentence_word_count'][news_train_df['sentence_word_count'] < 40]);", "processed": ["well seem fact urgenc 2 almost never use"]}, {"markdown": ["It isn't surprising that Reuters is the most common provider :)"], "code": "# Reference: https://www.kaggle.com/code/artgor/eda-feature-engineering-and-everything\n\n(news_train_df['headlineTag'].value_counts() / 1000)[:10].plot('barh');\nplt.title('headlineTag counts (thousands)');", "processed": ["surpris reuter common provid"]}, {"markdown": ["## 3. Download data, auxiliary functions and model tuning <a class=\"anchor\" id=\"3\"></a>\n\n[Back to Table of Contents](#0.1)", "From notebook https://www.kaggle.com/gandagorn/gru-lstm-mix-with-custom-loss\n\nMy upgrade: structure of model"], "code": "# Reference: https://www.kaggle.com/code/vbmokin/gru-lstm-mix-custom-loss-tuning-by-3d-visual\n\n# Download datasets\ntrain = pd.read_json('/kaggle/input/stanford-covid-vaccine/train.json', lines=True)\ntest = pd.read_json('/kaggle/input/stanford-covid-vaccine/test.json', lines=True)\nsample_sub = pd.read_csv(\"/kaggle/input/stanford-covid-vaccine/sample_submission.csv\")\n# Target columns \ntarget_cols = ['reactivity', 'deg_Mg_pH10', 'deg_pH10', 'deg_Mg_50C', 'deg_50C']\ntoken2int = {x:i for i, x in enumerate('().ACGUBEHIMSX')}\n\ndef get_pair_index_structure(structure):\n    structure = np.array([struc for struc in structure], dtype=\"<U4\")\n\n    open_index = np.where(structure == \"(\")[0]\n    closed_index = np.where(structure == \")\")[0]\n\n    structure[open_index] = range(0, len(open_index))\n    structure[closed_index] = range(len(open_index)-1, -1, -1)\n    structure[structure == \".\"] = -1\n    structure = structure.astype(int)\n\n    pair_structure = np.array([-1]*len(structure))\n    for i in range(len(open_index)):\n        start, end = np.where(structure == i)[0]\n        pair_structure[start] = end\n        pair_structure[end] = start    \n        \n    return pair_structure\ndef preprocess_inputs(df, cols=['sequence', 'structure', 'predicted_loop_type']):\n    return np.transpose(\n        np.array(\n            df[cols]\n            .applymap(lambda seq: [token2int[x] for x in seq])\n            .values\n            .tolist()\n        ),\n        (0, 2, 1)\n    )\n\ntrain_inputs_all = preprocess_inputs(train)\ntrain_labels_all = np.array(train[target_cols].values.tolist()).transpose((0, 2, 1))\n# Building model (with my upgrade)\n\ndef MCRMSE(y_true, y_pred):\n    colwise_mse = tf.reduce_mean(tf.square(y_true - y_pred), axis=1)\n    return tf.reduce_mean(tf.sqrt(colwise_mse), axis=1)\n\ndef gru_layer(hidden_dim, dropout):\n    return tf.keras.layers.Bidirectional(\n                                tf.keras.layers.GRU(hidden_dim,\n                                dropout=dropout,\n                                return_sequences=True,\n                                kernel_initializer = 'orthogonal'))\n\ndef lstm_layer(hidden_dim, dropout):\n    return tf.keras.layers.Bidirectional(\n                                tf.keras.layers.LSTM(hidden_dim,\n                                dropout=dropout,\n                                return_sequences=True,\n                                kernel_initializer = 'orthogonal'))\n\ndef build_model(model_type=1, seq_len=107, pred_len=68, embed_dim=100, \n                dropout=dropout_model, hidden_dim_first = hidden_dim_first, \n                hidden_dim_second = hidden_dim_second, hidden_dim_third = hidden_dim_third):\n    \n    inputs = tf.keras.layers.Input(shape=(seq_len, 3))\n\n    embed = tf.keras.layers.Embedding(input_dim=len(token2int), output_dim=embed_dim)(inputs)\n    reshaped = tf.reshape(\n        embed, shape=(-1, embed.shape[1],  embed.shape[2] * embed.shape[3]))\n    \n    reshaped = tf.keras.layers.SpatialDropout1D(.2)(reshaped)\n    \n    if model_type == 0:\n        hidden = gru_layer(hidden_dim_first, dropout)(reshaped)\n        hidden = gru_layer(hidden_dim_second, dropout)(hidden)\n        hidden = gru_layer(hidden_dim_third, dropout)(hidden)\n        \n    elif model_type == 1:\n        hidden = lstm_layer(hidden_dim_first, dropout)(reshaped)\n        hidden = lstm_layer(hidden_dim_second, dropout)(hidden)\n        hidden = lstm_layer(hidden_dim_third, dropout)(hidden)\n        \n    elif model_type == 2:\n        hidden = gru_layer(hidden_dim_first, dropout)(reshaped)\n        hidden = lstm_layer(hidden_dim_second, dropout)(hidden)\n        hidden = lstm_layer(hidden_dim_third, dropout)(hidden)\n        \n    elif model_type == 3:\n        hidden = lstm_layer(hidden_dim_first, dropout)(reshaped)\n        hidden = gru_layer(hidden_dim_second, dropout)(hidden)\n        hidden = gru_layer(hidden_dim_third, dropout)(hidden)\n\n    elif model_type == 4:\n        hidden = lstm_layer(hidden_dim_first, dropout)(reshaped)\n        hidden = gru_layer(hidden_dim_second, dropout)(hidden)\n        hidden = lstm_layer(hidden_dim_third, dropout)(hidden)\n    \n    truncated = hidden[:, :pred_len]\n\n    out = tf.keras.layers.Dense(5, activation='linear')(truncated)\n\n    model = tf.keras.Model(inputs=inputs, outputs=out)\n\n    adam = tf.optimizers.Adam()\n    model.compile(optimizer=adam, loss=MCRMSE)\n    \n    return model\n# Tunning model (with my upgrade)\n\ndef train_and_predict(n_folds=5, model_name=\"model\", model_type=0, epochs=90, debug=False,\n                      dropout_model=dropout_model, hidden_dim_first = hidden_dim_first, \n                      hidden_dim_second = hidden_dim_second, hidden_dim_third = hidden_dim_third,\n                      seed=seed):\n\n    print(\"Model:\", model_name)\n\n    ensemble_preds = pd.DataFrame(index=sample_sub.index, columns=target_cols).fillna(0) # test dataframe with 0 values\n    kf = KFold(n_folds, shuffle=True, random_state=seed)\n    skf = StratifiedKFold(n_folds, shuffle=True, random_state=seed)\n    val_losses = []\n    historys = []\n\n    for i, (train_index, val_index) in enumerate(skf.split(train_inputs_all, train['SN_filter'])):\n        print(\"Fold:\", str(i+1))\n\n        model_train = build_model(model_type=model_type, \n                                  dropout=dropout_model, \n                                  hidden_dim_first = hidden_dim_first, \n                                  hidden_dim_second = hidden_dim_second, \n                                  hidden_dim_third = hidden_dim_third)\n        model_short = build_model(model_type=model_type, seq_len=107, pred_len=107,\n                                  dropout=dropout_model, \n                                  hidden_dim_first = hidden_dim_first, \n                                  hidden_dim_second = hidden_dim_second, \n                                  hidden_dim_third = hidden_dim_third)\n        model_long = build_model(model_type=model_type, seq_len=130, pred_len=130,\n                                 dropout=dropout_model, \n                                 hidden_dim_first = hidden_dim_first, \n                                 hidden_dim_second = hidden_dim_second, \n                                 hidden_dim_third = hidden_dim_third)\n\n        train_inputs, train_labels = train_inputs_all[train_index], train_labels_all[train_index]\n        val_inputs, val_labels = train_inputs_all[val_index], train_labels_all[val_index]\n\n        checkpoint = tf.keras.callbacks.ModelCheckpoint(f'{model_name}.h5')\n\n        history = model_train.fit(\n            train_inputs , train_labels, \n            validation_data=(val_inputs,val_labels),\n            batch_size=64,\n            epochs=epochs, # changed 70\n            callbacks=[tf.keras.callbacks.ReduceLROnPlateau(), checkpoint],\n            verbose=2 if debug else 0\n        )\n\n        print(f\"{model_name} Min training loss={min(history.history['loss'])}, min validation loss={min(history.history['val_loss'])}\")\n\n        val_losses.append(min(history.history['val_loss']))\n        historys.append(history)\n\n        model_short.load_weights(f'{model_name}.h5')\n        model_long.load_weights(f'{model_name}.h5')\n\n        public_preds = model_short.predict(public_inputs)\n        private_preds = model_long.predict(private_inputs)\n\n        preds_model = []\n        for df, preds in [(public_df, public_preds), (private_df, private_preds)]:\n            for i, uid in enumerate(df.id):\n                single_pred = preds[i]\n\n                single_df = pd.DataFrame(single_pred, columns=target_cols)\n                single_df['id_seqpos'] = [f'{uid}_{x}' for x in range(single_df.shape[0])]\n\n                preds_model.append(single_df)\n\n        preds_model_df = pd.concat(preds_model)\n        ensemble_preds[target_cols] += preds_model_df[target_cols].values / n_folds\n\n        if debug:\n            print(\"Intermediate ensemble result\")\n            print(ensemble_preds[target_cols].head())\n\n    ensemble_preds[\"id_seqpos\"] = preds_model_df[\"id_seqpos\"].values\n    ensemble_preds = pd.merge(sample_sub[\"id_seqpos\"], ensemble_preds, on=\"id_seqpos\", how=\"left\")\n\n    print(\"Mean Validation loss:\", str(np.mean(val_losses)))\n\n    if debug:\n        fig, ax = plt.subplots(1, 1, figsize = (10, 5))\n        for i, history in enumerate(historys):\n            ax.plot(history.history['loss'])\n            ax.plot(history.history['val_loss'])\n            ax.set_title('model_'+str(i+1))\n            ax.set_ylabel('Loss')\n            ax.set_xlabel('Epoch')\n        plt.show()\n\n    return ensemble_preds\n\n\npublic_df = test.query(\"seq_length == 107\").copy()\nprivate_df = test.query(\"seq_length == 130\").copy()\npublic_inputs = preprocess_inputs(public_df)\nprivate_inputs = preprocess_inputs(private_df)\n\nensembles = []\n\nfor i in range(5):\n    model_name = \"model_\"+str(i+1)\n\n    ensemble = train_and_predict(n_folds=5, model_name=model_name, model_type=i, epochs=100,\n                                 dropout_model=dropout_model, hidden_dim_first = hidden_dim_first, \n                                 hidden_dim_second = hidden_dim_second, hidden_dim_third = hidden_dim_third,\n                                 seed=seed)\n    ensembles.append(ensemble)", "processed": ["3 download data auxiliari function model tune class anchor id 3 back tabl content 0 1", "notebook http www kaggl com gandagorn gru lstm mix custom loss upgrad structur model"]}, {"markdown": ["Let's have a look at some images."], "code": "# Reference: https://www.kaggle.com/code/artgor/simple-eda-and-model-in-pytorch\n\nfig = plt.figure(figsize=(25, 4))\n# display 20 images\ntrain_imgs = os.listdir(\"../input/train\")\nfor idx, img in enumerate(np.random.choice(train_imgs, 20)):\n    ax = fig.add_subplot(2, 20//2, idx+1, xticks=[], yticks=[])\n    im = Image.open(\"../input/train/\" + img)\n    plt.imshow(im)\n    lab = labels.loc[labels['id'] == img.split('.')[0], 'label'].values[0]\n    ax.set_title(f'Label: {lab}')", "processed": ["let look imag"]}, {"markdown": ["### Display big contributors"], "code": "# Reference: https://www.kaggle.com/code/ogrellier/plasticc-adversarial-validation\n\nmean_gain = importances[['gain', 'feature']].groupby('feature').mean()\nimportances['mean_gain'] = importances['feature'].map(mean_gain['gain'])\n\nplt.figure(figsize=(8, 12))\nsns.barplot(x='gain', y='feature', data=importances.sort_values('mean_gain', ascending=False))\nplt.tight_layout()", "processed": ["display big contributor"]}, {"markdown": ["### Display contributors"], "code": "# Reference: https://www.kaggle.com/code/ogrellier/plasticc-adversarial-validation\n\nmean_gain = importances[['gain', 'feature']].groupby('feature').mean()\nimportances['mean_gain'] = importances['feature'].map(mean_gain['gain'])\n\nplt.figure(figsize=(8, 12))\nsns.barplot(x='gain', y='feature', data=importances.sort_values('mean_gain', ascending=False))\nplt.tight_layout()", "processed": ["display contributor"]}, {"markdown": ["### Adding further information\n\n1. The CT-scan captures information about the radiodensity of an object or tissue exposed to x-rays. A transversal slice of a scan is reconstructed after taking measurements from several different directions.\n2. We need to transform to Hounsfield units as the spectral composition of the x-rays depends on the measurement settings like acquisition parameters and tube voltage. By normalizing to values of water and air (water has HU 0 and air -1000) the images of different measurements are becoming comparable.\n3. A ct-scanner yields roughly 4000 grey values that can't be captured by our eyes. This is why windowing is performed. This way the image is displayed in a HU range that suites most to the region of interest. ", "## Transforming to Hounsfield Units <a class=\"anchor\" id=\"hunits\"></a>", "Before starting, let's plot the pixelarray distribution of some dicom files to get an impression of the raw data:"], "code": "# Reference: https://www.kaggle.com/code/allunia/pulmonary-dicom-preprocessing\n\nfig, ax = plt.subplots(1,2,figsize=(20,5))\nfor n in range(10):\n    image = scans[n].pixel_array.flatten()\n    rescaled_image = image * scans[n].RescaleSlope + scans[n].RescaleIntercept\n    sns.distplot(image.flatten(), ax=ax[0]);\n    sns.distplot(rescaled_image.flatten(), ax=ax[1])\nax[0].set_title(\"Raw pixel array distributions for 10 examples\")\nax[1].set_title(\"HU unit distributions for 10 examples\");", "processed": ["ad inform 1 ct scan captur inform radiodens object tissu expo x ray transvers slice scan reconstruct take measur sever differ direct 2 need transform hounsfield unit spectral composit x ray depend measur set like acquisit paramet tube voltag normal valu water air water hu 0 air 1000 imag differ measur becom compar 3 ct scanner yield roughli 4000 grey valu captur eye window perform way imag display hu rang suit region interest", "transform hounsfield unit class anchor id hunit", "start let plot pixelarray distribut dicom file get impress raw data"]}, {"markdown": ["For some examples we can see that there are raw values at -2000. They correspond to images with a circular boundary within the image. The \"outside\" of this circle value is often set to -2000 (or in other competitions I found also -3000) by default."], "code": "# Reference: https://www.kaggle.com/code/allunia/pulmonary-dicom-preprocessing\n\ndef set_outside_scanner_to_air(raw_pixelarrays):\n    # in OSIC we find outside-scanner-regions with raw-values of -2000. \n    # Let's threshold between air (0) and this default (-2000) using -1000\n    raw_pixelarrays[raw_pixelarrays <= -1000] = 0\n    return raw_pixelarrays\ndef transform_to_hu(slices):\n    images = np.stack([file.pixel_array for file in slices])\n    images = images.astype(np.int16)\n\n    images = set_outside_scanner_to_air(images)\n    \n    # convert to HU\n    for n in range(len(slices)):\n        \n        intercept = slices[n].RescaleIntercept\n        slope = slices[n].RescaleSlope\n        \n        if slope != 1:\n            images[n] = slope * images[n].astype(np.float64)\n            images[n] = images[n].astype(np.int16)\n            \n        images[n] += np.int16(intercept)\n    \n    return np.array(images, dtype=np.int16)\nhu_scans = transform_to_hu(scans)\nfig, ax = plt.subplots(1,4,figsize=(20,3))\nax[0].set_title(\"Original CT-scan\")\nax[0].imshow(scans[0].pixel_array, cmap=\"bone\")\nax[1].set_title(\"Pixelarray distribution\");\nsns.distplot(scans[0].pixel_array.flatten(), ax=ax[1]);\n\nax[2].set_title(\"CT-scan in HU\")\nax[2].imshow(hu_scans[0], cmap=\"bone\")\nax[3].set_title(\"HU values distribution\");\nsns.distplot(hu_scans[0].flatten(), ax=ax[3]);\n\nfor m in [0,2]:\n    ax[m].grid(False)", "processed": ["exampl see raw valu 2000 correspond imag circular boundari within imag outsid circl valu often set 2000 competit found also 3000 default"]}, {"markdown": ["Just to speed up the computation, I have selected N patients to consider. Use N = train.shape[0] to do this for all patients in the dataset:"], "code": "# Reference: https://www.kaggle.com/code/allunia/pulmonary-dicom-preprocessing\n\ndef get_window_value(feature):\n    if type(feature) == pydicom.multival.MultiValue:\n        return np.int(feature[0])\n    else:\n        return np.int(feature)\n\npixelspacing_r = []\npixelspacing_c = []\nslice_thicknesses = []\npatient_id = []\npatient_pth = []\nrow_values = []\ncolumn_values = []\nwindow_widths = []\nwindow_levels = []\n\nif basepath == \"../input/osic-pulmonary-fibrosis-progression/\":\n    patients = train.Patient.unique()[0:N]\nelse:\n    patients = train.SeriesInstanceUID.unique()[0:N]\n\nfor patient in patients:\n    patient_id.append(patient)\n    if basepath == \"../input/osic-pulmonary-fibrosis-progression/\":\n        path = train[train.Patient == patient].dcm_path.values[0]\n    else:\n        path = train[train.SeriesInstanceUID == patient].dcm_path.values[0]\n    example_dcm = listdir(path)[0]\n    patient_pth.append(path)\n    dataset = pydicom.dcmread(path + \"/\" + example_dcm)\n    \n    window_widths.append(get_window_value(dataset.WindowWidth))\n    window_levels.append(get_window_value(dataset.WindowCenter))\n    \n    spacing = dataset.PixelSpacing\n    slice_thicknesses.append(dataset.SliceThickness)\n    \n    row_values.append(dataset.Rows)\n    column_values.append(dataset.Columns)\n    pixelspacing_r.append(spacing[0])\n    pixelspacing_c.append(spacing[1])\n    \nscan_properties = pd.DataFrame(data=patient_id, columns=[\"patient\"])\nscan_properties.loc[:, \"rows\"] = row_values\nscan_properties.loc[:, \"columns\"] = column_values\nscan_properties.loc[:, \"area\"] = scan_properties[\"rows\"] * scan_properties[\"columns\"]\nscan_properties.loc[:, \"pixelspacing_r\"] = pixelspacing_r\nscan_properties.loc[:, \"pixelspacing_c\"] = pixelspacing_c\nscan_properties.loc[:, \"pixelspacing_area\"] = scan_properties.pixelspacing_r * scan_properties.pixelspacing_c\nscan_properties.loc[:, \"slice_thickness\"] = slice_thicknesses\nscan_properties.loc[:, \"patient_pth\"] = patient_pth\nscan_properties.loc[:, \"window_width\"] = window_widths\nscan_properties.loc[:, \"window_level\"] = window_levels\nscan_properties.head()\nfig, ax = plt.subplots(1,2,figsize=(20,5))\nsns.distplot(pixelspacing_r, ax=ax[0], color=\"Limegreen\", kde=False)\nax[0].set_title(\"Pixel spacing distribution \\n in row direction \")\nax[0].set_ylabel(\"Counts in train\")\nax[0].set_xlabel(\"mm\")\nsns.distplot(pixelspacing_c, ax=ax[1], color=\"Mediumseagreen\", kde=False)\nax[1].set_title(\"Pixel spacing distribution \\n in column direction\");\nax[1].set_ylabel(\"Counts in train\");\nax[1].set_xlabel(\"mm\");", "processed": ["speed comput select n patient consid use n train shape 0 patient dataset"]}, {"markdown": ["We can see that the values really vary a lot from patient to patient! As they are given in mm and ct-scans usually cover 512 row and column values... **oh wait! we need to check this!** ... we can compute the minimum and maximum distance that is covered by the images:", "### Slice thickness and pixel area\n\nThe slice thickness tells us how much distance is covered in Z-direction by one slice. Let's plot the distribution of it as well. Furthermore the pixel_array of raw values covers a specific area given by row and column values. Let's take a look at it as well: "], "code": "# Reference: https://www.kaggle.com/code/allunia/pulmonary-dicom-preprocessing\n\ncounts = scan_properties.groupby([\"rows\", \"columns\"]).size()\ncounts = counts.unstack()\ncounts.fillna(0, inplace=True)\n\n\nfig, ax = plt.subplots(1,2,figsize=(20,5))\nsns.distplot(slice_thicknesses, color=\"orangered\", kde=False, ax=ax[0])\nax[0].set_title(\"Slice thicknesses of all patients\");\nax[0].set_xlabel(\"Slice thickness in mm\")\nax[0].set_ylabel(\"Counts in train\");\n\nfor n in counts.index.values:\n    for m in counts.columns.values:\n        ax[1].scatter(n, m, s=counts.loc[n,m], c=\"midnightblue\")\nax[1].set_xlabel(\"rows\")\nax[1].set_ylabel(\"columns\")\nax[1].set_title(\"Pixel area of ct-scan per patient\");", "processed": ["see valu realli vari lot patient patient given mm ct scan usual cover 512 row column valu oh wait need check comput minimum maximum distanc cover imag", "slice thick pixel area slice thick tell u much distanc cover z direct one slice let plot distribut well furthermor pixel array raw valu cover specif area given row column valu let take look well"]}, {"markdown": ["* Very thin slices allow more details to be shown. On the other hand thick slices contain less noise but are more prone to artifacts. Hmm... I'm very excited to see some examples here as well. \n* Even though it is common to have 512x512 pixel size areas, we can see that this is not always true!! We can find a lot of exceptions and even one or a few very large pixel areas (1300x1300)!!! (OSIC)\n* Proper preprocessing of these scans might be very important... we have to check it.", "## Physical area & slice volume covered by a single ct-scan", "Now, we know some important quantities to compute the physical distance covered by a ct-scan!"], "code": "# Reference: https://www.kaggle.com/code/allunia/pulmonary-dicom-preprocessing\n\nscan_properties[\"r_distance\"] = scan_properties.pixelspacing_r * scan_properties.rows\nscan_properties[\"c_distance\"] = scan_properties.pixelspacing_c * scan_properties[\"columns\"]\nscan_properties[\"area_cm2\"] = 0.1* scan_properties[\"r_distance\"] * 0.1*scan_properties[\"c_distance\"]\nscan_properties[\"slice_volume_cm3\"] = 0.1*scan_properties.slice_thickness * scan_properties.area_cm2\nfig, ax = plt.subplots(1,2,figsize=(20,5))\nsns.distplot(scan_properties.area_cm2, ax=ax[0], color=\"purple\")\nsns.distplot(scan_properties.slice_volume_cm3, ax=ax[1], color=\"magenta\")\nax[0].set_title(\"CT-slice area in $cm^{2}$\")\nax[1].set_title(\"CT-slice volume in $cm^{3}$\")\nax[0].set_xlabel(\"$cm^{2}$\")\nax[1].set_xlabel(\"$cm^{3}$\");", "processed": ["thin slice allow detail shown hand thick slice contain le nois prone artifact hmm excit see exampl well even though common 512x512 pixel size area see alway true find lot except even one larg pixel area 1300x1300 osic proper preprocess scan might import check", "physic area slice volum cover singl ct scan", "know import quantiti comput physic distanc cover ct scan"]}, {"markdown": ["### Insights\n\n* Taking a look at one slice of a scan with smallest and largest slice area, we can see that the large one has a lot of useless region covered. We could crop it.\n* Strange... in the second image with the large area the outside region of the scanner tube is not set to the value of air but rather to some value in the middle of the range -1000 to 1000."], "code": "# Reference: https://www.kaggle.com/code/allunia/pulmonary-dicom-preprocessing\n\nfig, ax = plt.subplots(1,2,figsize=(20,5))\nsns.distplot(max_hu_scans[np.int(len(max_hu_scans)/2)].flatten(), kde=False, ax=ax[1])\nax[1].set_title(\"Large area image\")\nsns.distplot(min_hu_scans[np.int(len(min_hu_scans)/2)].flatten(), kde=False, ax=ax[0])\nax[0].set_title(\"Small area image\")\nax[0].set_xlabel(\"HU values\")\nax[1].set_xlabel(\"HU values\");", "processed": ["insight take look one slice scan smallest largest slice area see larg one lot useless region cover could crop strang second imag larg area outsid region scanner tube set valu air rather valu middl rang 1000 1000"]}, {"markdown": ["Hmm I can't see a great difference. Perhaps the one with the large slice volume looks a bit more blurred. But as above there is an outside-scanner region that was set to the walue of water (HU value of 0) instead of air."], "code": "# Reference: https://www.kaggle.com/code/allunia/pulmonary-dicom-preprocessing\n\nfig, ax = plt.subplots(1,2,figsize=(20,5))\nsns.distplot(max_hu_scans[np.int(len(max_hu_scans)/2)].flatten(), kde=False, ax=ax[1])\nax[1].set_title(\"Large slice volume\")\nsns.distplot(min_hu_scans[np.int(len(min_hu_scans)/2)].flatten(), kde=False, ax=ax[0])\nax[0].set_title(\"Small slice volume\")\nax[0].set_xlabel(\"HU values\")\nax[1].set_xlabel(\"HU values\");", "processed": ["hmm see great differ perhap one larg slice volum look bit blur outsid scanner region set walu water hu valu 0 instead air"]}, {"markdown": ["## 3D-reconstruction of CT-scans <a class=\"anchor\" id=\"reconstruction\"></a>", "The plot_3d works well in the Data Science Bowl 2017 but in our case the results are not so well. It depends on the threshold but so far I don't know why our reconstructions often look blurred or show tube regions as well. :-("], "code": "# Reference: https://www.kaggle.com/code/allunia/pulmonary-dicom-preprocessing\n\ndef plot_3d(image, threshold=700, color=\"navy\"):\n    \n    # Position the scan upright, \n    # so the head of the patient would be at the top facing the camera\n    p = image.transpose(2,1,0)\n    \n    verts, faces,_,_ = measure.marching_cubes_lewiner(p, threshold)\n\n    fig = plt.figure(figsize=(10, 10))\n    ax = fig.add_subplot(111, projection='3d')\n\n    # Fancy indexing: `verts[faces]` to generate a collection of triangles\n    mesh = Poly3DCollection(verts[faces], alpha=0.2)\n    mesh.set_facecolor(color)\n    ax.add_collection3d(mesh)\n\n    ax.set_xlim(0, p.shape[0])\n    ax.set_ylim(0, p.shape[1])\n    ax.set_zlim(0, p.shape[2])\n\n    plt.show()\nplot_3d(max_hu_scans)", "processed": ["3d reconstruct ct scan class anchor id reconstruct", "plot 3d work well data scienc bowl 2017 case result well depend threshold far know reconstruct often look blur show tube region well"]}, {"markdown": ["Compare to above this one looks far better. Let's plot the distributions. Perhaps we can understand what's going wrong by taking a look at them:"], "code": "# Reference: https://www.kaggle.com/code/allunia/pulmonary-dicom-preprocessing\n\nplt.figure(figsize=(20,5))\nsns.distplot(old_distribution, label=\"weak 3d plot\", kde=False)\nsns.distplot(hu_scans.flatten(), label=\"strong 3d plot\", kde=False)\nplt.title(\"HU value distribution\")\nplt.legend();", "processed": ["compar one look far better let plot distribut perhap understand go wrong take look"]}, {"markdown": ["### Understanding the segmentation step by step:", "First of all we are separating between \"potentially lung\" with values smaller or equal to 320. Why 320? I just kept the value of Guidos old notebook. Try out your own values if you like. ;-) Before you do so, you make like to take a look at the hu_value distribution again:"], "code": "# Reference: https://www.kaggle.com/code/allunia/pulmonary-dicom-preprocessing\n\nplt.figure(figsize=(20,5))\nsns.distplot(hu_scans[20], kde=False)\nplt.title(\"Example HU value distribution\");\nplt.xlabel(\"HU-value\")\nplt.ylabel(\"count\")", "processed": ["understand segment step step", "first separ potenti lung valu smaller equal 320 320 kept valu guido old notebook tri valu like make like take look hu valu distribut"]}, {"markdown": ["For OSIC it's interesting that we have two different kind of patterns. In the RSNA competition all training images are of shape 512, 512. So you don't need to explore the image sizes further. But perhaps it's still useful to browse through the images to gain ideas for good augmentations."], "code": "# Reference: https://www.kaggle.com/code/allunia/pulmonary-dicom-preprocessing\n\nplt.figure(figsize=(8,8))\nfor n in counts.index.values:\n    for m in counts.columns.values:\n        plt.scatter(n, m, s=counts.loc[n,m], c=\"dodgerblue\", alpha=0.7)\nplt.xlabel(\"rows\")\nplt.ylabel(\"columns\")\nplt.title(\"Pixel area of ct-scan per patient\");\nplt.plot(np.arange(0,1400), '-.', c=\"purple\", label=\"squared\")\nplt.plot(888 * np.ones(1400), '-.', c=\"crimson\", label=\"888 rows\");\nplt.legend();", "processed": ["osic interest two differ kind pattern rsna competit train imag shape 512 512 need explor imag size perhap still use brow imag gain idea good augment"]}, {"markdown": ["## MY PLAN\n\n1. Using subset of the full data but with bit large image size 256x256\n2. Trying efficientnet (reason described below)\n3. Replacing batch normalization with group normalization for small batch size\n4. using radam instead of sgd or adam\n5. little bit of preprocessing", "## UPDATE\n\n**in version 15**\n- reducing image size down to 224x224\n- subsample size 400000\n- epochs = 5\n- increasing batch size from 32 to 64\n- total_steps = 2000\n", "## Recommendations \n* please don't forget to recommend any idea in the comment box to improve this kernel,your aid is highly highly appreciated,thanks in advance", "**I hope this kernel helpful and some <font color=\"RED\"><b>UPVOTES</b></font> would be very much appreciated****", "## why RSNA Intracranial Hemorrhage Detection is so important????", "*An intracranial hemorrhage (ICH) is a condition in which a blood vessel erupts inside the brain, causing internal bleeding. If not treated correctly and immediately, a brain hemorrhage can be deadly. The type of hemorrhage is usually diagnosed using a CT or MRI scan. Some hemorrhages are also accompanied by cerebral edema \u2013 an excess accumulation of fluid in the intracellular or extracellular spaces of the brain.Edema is exceedingly difficult to identify, appearing as a subtle darker area surrounding the hemorrhage; it sometimes requires analysis of multiple sequential scans. Implementation and execution of a successful segmentation model in these situations requires expertise in all computer vision methods \u2013 both classical and deep learning based. RSIP Vision leverages both deep learning and classical computer vision techniques to provide a fully automated solution to this segmentation problem.*\nto know more,watch [this video](https://www.youtube.com/watch?v=7RqjrCSR8TE) ", " <font color=\"GREEN\"><b> Typically Intracranial  Hemorrhage Detection is segmentation task like the diagram describing below </b></font>\n\n![](https://www.rsipvision.com/wp-content/uploads/2018/07/Hemorrhage-Slide.jpg)", "*source : https://www.rsipvision.com/intracranial-hemorrhage-and-edema-segmentation/*", "## Types of Intracranial", "![](http://pbs.twimg.com/media/BxqqVoyCQAAOE10.png)  ", "**Necessary imports**"], "code": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\n# Any results you write to the current directory are saved as output.\n\n\nimport seaborn as sns\nfrom sklearn.utils import shuffle\nfrom os.path import isfile, join\nimport keras\n\n# Standard dependencies\nimport cv2\nimport time\nimport scipy as sp\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom PIL import Image\nfrom functools import partial\nimport matplotlib.pyplot as plt\n\n# Machine Learning\nimport tensorflow as tf\nimport keras\nfrom keras import initializers\nfrom keras import regularizers\nfrom keras import constraints\nfrom keras import backend as K\nfrom keras.activations import elu\nfrom keras.optimizers import Adam\nfrom keras.models import Sequential\nfrom keras.engine import Layer, InputSpec\nfrom keras.utils.generic_utils import get_custom_objects\nfrom keras.callbacks import Callback, EarlyStopping, ReduceLROnPlateau\nfrom keras.layers import Dense, Conv2D, Flatten, GlobalAveragePooling2D, Dropout\nfrom sklearn.metrics import cohen_kappa_score\nimport pydicom\n\nimport json\n\nimport cv2\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport pydicom\nfrom keras import layers\nfrom keras.callbacks import Callback, ModelCheckpoint\nfrom keras.models import Sequential\nfrom keras.optimizers import Adam\nfrom tqdm import tqdm\n\n  \nfrom keras import backend as K\nimport tensorflow as tf\n!ls ../input\n\nos.listdir('/kaggle/input/rsna-intracranial-hemorrhage-detection/rsna-intracranial-hemorrhage-detection')\nBASE_PATH = '/kaggle/input/rsna-intracranial-hemorrhage-detection/rsna-intracranial-hemorrhage-detection/'\nTRAIN_DIR = 'stage_2_train/'\nTEST_DIR = 'stage_2_test/'\ntrain_df = pd.read_csv(BASE_PATH + 'stage_2_train.csv')\ntrain_df\ntrain_df.ID == 'ID_6431af929_intraparenchymal'\nsns.countplot(train_df.Label)\ntrain_df.Label.value_counts()", "processed": ["plan 1 use subset full data bit larg imag size 256x256 2 tri efficientnet reason describ 3 replac batch normal group normal small batch size 4 use radam instead sgd adam 5 littl bit preprocess", "updat version 15 reduc imag size 224x224 subsampl size 400000 epoch 5 increas batch size 32 64 total step 2000", "recommend plea forget recommend idea comment box improv kernel aid highli highli appreci thank advanc", "hope kernel help font color red b upvot b font would much appreci", "rsna intracrani hemorrhag detect import", "intracrani hemorrhag ich condit blood vessel erupt insid brain caus intern bleed treat correctli immedi brain hemorrhag deadli type hemorrhag usual diagnos use ct mri scan hemorrhag also accompani cerebr edema excess accumul fluid intracellular extracellular space brain edema exceedingli difficult identifi appear subtl darker area surround hemorrhag sometim requir analysi multipl sequenti scan implement execut success segment model situat requir expertis comput vision method classic deep learn base rsip vision leverag deep learn classic comput vision techniqu provid fulli autom solut segment problem know watch video http www youtub com watch v 7rqjrcsr8te", "font color green b typic intracrani hemorrhag detect segment task like diagram describ b font http www rsipvis com wp content upload 2018 07 hemorrhag slide jpg", "sourc http www rsipvis com intracrani hemorrhag edema segment", "type intracrani", "http pb twimg com medium bxqqvoycqaaoe10 png", "necessari import"]}, {"markdown": ["#### Data prosessing - my ORIGINAL research"], "code": "# Reference: https://www.kaggle.com/code/vbmokin/covid-19-week5-global-forecasting-eda-extratr\n\n# Find date start COVID19 growth\ncountry_list = df_trend['Country'].unique()\ncountry_stage = pd.DataFrame(columns = ['Country', 'COVID_start', 'COVID_max', 'COVID_now'])\nfor i in range(len(country_list)):\n    country_i = df_trend[df_trend['Country'] == country_list[i]].reset_index(drop=True)\n    country_stage.loc[i,'Country'] = country_list[i]                                                    # country name\n    country_stage.loc[i,'COVID_start'] = country_i[country_i['Cases']!=0]['Cases'].cumsum().idxmin()    # date of the first cases\n    country_stage.loc[i,'COVID_max'] = np.argmax(country_i['Cases'])                                    # date of the maximum\n    country_stage.loc[i,'COVID_now'] = country_i.Cases[len(country_i)-1]/country_i.Cases.max()          # % from maximum at the end date\ncountry_stage.sort_values(by='COVID_max')\ncountry_stage_now = country_stage[['Country','COVID_now', 'COVID_max']].sort_values(by='COVID_now', ascending=False)\nprint(\"Cases now as % from maximum in each country\")\nplt.figure(figsize=(20,10))\nplt.plot(range(len(country_stage_now.Country)), country_stage_now.COVID_now, marker='p');\ndata = country_stage[['COVID_start', 'COVID_max', 'COVID_now']]", "processed": ["data prose origin research"]}, {"markdown": ["#### Data clustering"], "code": "# Reference: https://www.kaggle.com/code/vbmokin/covid-19-week5-global-forecasting-eda-extratr\n\n# Thanks to https://www.kaggle.com/kashnitsky/topic-7-unsupervised-learning-pca-and-clustering\ninertia = []\npca = PCA(n_components=2)\n# fit X and apply the reduction to X \nx_3d = pca.fit_transform(data)\n#x_3d=data\nfor k in range(1, 8):\n    kmeans = KMeans(n_clusters=k, random_state=1).fit(x_3d)\n    inertia.append(np.sqrt(kmeans.inertia_))\nplt.plot(range(1, 8), inertia, marker='s');\nplt.xlabel('$k$')\nplt.ylabel('$J(C_k)$');", "processed": ["data cluster"]}, {"markdown": ["### Distribution of character length in question_title"], "code": "# Reference: https://www.kaggle.com/code/mobassir/jigsaw-google-q-a-eda\n\ntrain['question_title'].str.len()\n#Number of characters in the sentence\n\nlengths = train['question_title'].apply(len)\ntrain['lengths'] = lengths\nlengths = train.loc[train['lengths']<4000]['lengths']\nsns.distplot(lengths, color='b')\nplt.show()", "processed": ["distribut charact length question titl"]}, {"markdown": ["- Although the lengths seem to be skewed just a bit to the lower lengths.we see another clear peak around the 45-50 character mark.", "### Distribution of characters in question body & Answer body"], "code": "# Reference: https://www.kaggle.com/code/mobassir/jigsaw-google-q-a-eda\n\nquestion_body=train['question_body'].str.len()\nanswer_body=train['answer'].str.len()\nfig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,6))\nsns.distplot(question_body,ax=ax1,color='blue')\nsns.distplot(answer_body,ax=ax2,color='green')\nax2.set_title('Distribution for question body')\nax1.set_title('Distribution for answer')\nplt.show()\n", "processed": ["although length seem skew bit lower length see anoth clear peak around 45 50 charact mark", "distribut charact question bodi answer bodi"]}, {"markdown": ["- hmm,both the distributions are left skewed and almost identical.\n", "### Distribution of the number of words in the question_body"], "code": "# Reference: https://www.kaggle.com/code/mobassir/jigsaw-google-q-a-eda\n\nwords = train['question_body'].apply(lambda x: len(x) - len(''.join(x.split())) + 1)\ntrain['words'] = words\n#words = train.loc[train['words']<500]['words']\nsns.distplot(words, color='r')\nplt.show()", "processed": ["hmm distribut left skew almost ident", "distribut number word question bodi"]}, {"markdown": ["\n\nIt looks like we have a unimodal left-skewed distribution of the number of words in the question_body.\n", "### Distribution of the number of words in the Answer"], "code": "# Reference: https://www.kaggle.com/code/mobassir/jigsaw-google-q-a-eda\n\nanswer=train['answer'].apply(lambda x : len(x.split(' ')))\nsns.distplot(answer,color='red')\nplt.gca().set_title('Distribution of no: of words in answer')", "processed": ["look like unimod left skew distribut number word question bodi", "distribut number word answer"]}, {"markdown": ["### Average Word Length"], "code": "# Reference: https://www.kaggle.com/code/mobassir/jigsaw-google-q-a-eda\n\navg_word_len = train['answer'].apply(lambda x: 1.0*len(''.join(x.split()))/(len(x) - len(''.join(x.split())) + 1))\ntrain['avg_word_len'] = avg_word_len\navg_word_len = train.loc[train['avg_word_len']<10]['avg_word_len']\nsns.distplot(avg_word_len, color='g')\nplt.show()", "processed": ["averag word length"]}, {"markdown": ["### Distribution of stopwords in question & Answer"], "code": "# Reference: https://www.kaggle.com/code/mobassir/jigsaw-google-q-a-eda\n\nfig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,6))\nwords=train['que_stopwords'].apply(lambda x : len(x))\nsns.distplot(words,color='green',ax=ax1)\nax1.set_title('Distribution of stopwords in question ')\nwords=train['ans_stopwords'].apply(lambda x: len(x))\nsns.distplot(words,color='blue',ax=ax2)\nax2.set_title('Distribution of stopwords in  Answer')\n", "processed": ["distribut stopword question answer"]}, {"markdown": ["Reference : [JIGSAW EDA](https://www.kaggle.com/gpreda/jigsaw-eda) \n\n[Jigsaw Competition : EDA and Modeling](https://www.kaggle.com/tarunpaparaju/jigsaw-competition-eda-and-modeling)"], "code": "# Reference: https://www.kaggle.com/code/mobassir/jigsaw-google-q-a-eda\n\nplt.figure(figsize=(20,15))\nplt.title(\"Distribution of question_not_really_a_question\")\nsns.distplot(train['question_not_really_a_question'],kde=True,hist=False, bins=120, label='question_not_really_a_question')\nplt.legend(); plt.show()", "processed": ["refer jigsaw eda http www kaggl com gpreda jigsaw eda jigsaw competit eda model http www kaggl com tarunpaparaju jigsaw competit eda model"]}, {"markdown": ["*In cell above you can replace question_not_really_a_question with other keywords from targets variable to get exact distribution of that column*", "### Lets Plot Feature Distribution "], "code": "# Reference: https://www.kaggle.com/code/mobassir/jigsaw-google-q-a-eda\n\ndef plot_features_distribution(features, title):\n    plt.figure(figsize=(15,10))\n    plt.title(title)\n    for feature in features:\n        sns.distplot(train.loc[~train[feature].isnull(),feature],kde=True,hist=False, bins=120, label=feature)\n    plt.xlabel('')\n    plt.legend()\n    plt.show()\nplot_features_distribution(targets, \"Distribution of targets in train set\")", "processed": ["cell replac question realli question keyword target variabl get exact distribut column", "let plot featur distribut"]}, {"markdown": ["## Lets see More data distribution"], "code": "# Reference: https://www.kaggle.com/code/mobassir/jigsaw-google-q-a-eda\n\ndef plot_count(feature, title,size=1):\n    f, ax = plt.subplots(1,1, figsize=(10,10))\n    total = float(len(train))\n    g = sns.countplot(train[feature], order = train[feature].round(2).value_counts().index[:20], palette='Set3')\n    g.set_title(\"Number and percentage of {}\".format(title))\n    for p in ax.patches:\n        height = p.get_height()\n        ax.text(p.get_x()+p.get_width()/2,\n                height + 5,\n                '{:1.2f}%'.format(100*height/total),\n                ha=\"center\") \n    plt.show()   \nplot_count('question_well_written','question_well_written')", "processed": ["let see data distribut"]}, {"markdown": ["### Check for Class Imbalance"], "code": "# Reference: https://www.kaggle.com/code/jesucristo/santander-magic-lgb-0-901\n\nsns.set_style('whitegrid')\nsns.countplot(target)\nsns.set_style('whitegrid')", "processed": ["check class imbal"]}, {"markdown": ["<a id=1><pre><b>Run LGBM model</b></pre></a>"], "code": "# Reference: https://www.kaggle.com/code/jesucristo/santander-magic-lgb-0-901\n\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(train.values, target.values)):\n    \n    X_train, y_train = train.iloc[trn_idx][features], target.iloc[trn_idx]\n    X_valid, y_valid = train.iloc[val_idx][features], target.iloc[val_idx]\n    \n    X_tr, y_tr = augment(X_train.values, y_train.values)\n    X_tr = pd.DataFrame(X_tr)\n    \n    print(\"Fold idx:{}\".format(fold_ + 1))\n    trn_data = lgb.Dataset(X_tr, label=y_tr)\n    val_data = lgb.Dataset(X_valid, label=y_valid)\n    \n    clf = lgb.train(param, trn_data, 1000000, valid_sets = [trn_data, val_data], verbose_eval=5000, early_stopping_rounds = 4000)\n    oof[val_idx] = clf.predict(train.iloc[val_idx][features], num_iteration=clf.best_iteration)\n    getVal[val_idx]+= clf.predict(train.iloc[val_idx][features], num_iteration=clf.best_iteration) / folds.n_splits\n    \n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"feature\"] = features\n    fold_importance_df[\"importance\"] = clf.feature_importance()\n    fold_importance_df[\"fold\"] = fold_ + 1\n    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n    \n    predictions += clf.predict(test[features], num_iteration=clf.best_iteration) / folds.n_splits\nprint(\"\\n >> CV score: {:<8.5f}\".format(roc_auc_score(target, oof)))\ncols = (feature_importance_df[[\"feature\", \"importance\"]]\n        .groupby(\"feature\")\n        .mean()\n        .sort_values(by=\"importance\", ascending=False)[:1000].index)\nbest_features = feature_importance_df.loc[feature_importance_df.feature.isin(cols)]\n\nplt.figure(figsize=(14,26))\nsns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\",ascending=False))\nplt.title('LightGBM Features (averaged over folds)')\nplt.tight_layout()\nplt.savefig('lgbm_importances.png')", "processed": ["id 1 pre b run lgbm model b pre"]}, {"markdown": ["# Data Exploration\n\n\nLet's check distribution of data_provider, isup_grade and gleason_score in train data."], "code": "# Reference: https://www.kaggle.com/code/gpreda/panda-challenge-starting-eda\n\ndef plot_count(df, feature, title='', size=2):\n    f, ax = plt.subplots(1,1, figsize=(3*size,2*size))\n    total = float(len(df))\n    sns.countplot(df[feature],order = df[feature].value_counts().index, palette='Set3')\n    plt.title(title)\n    for p in ax.patches:\n        height = p.get_height()\n        ax.text(p.get_x()+p.get_width()/2.,\n                height + 3,\n                '{:1.2f}%'.format(100*height/total),\n                ha=\"center\") \n    plt.show()\n\nplot_count(train_df, 'data_provider', 'Data provider - data count and percent')", "processed": ["data explor let check distribut data provid isup grade gleason score train data"]}, {"markdown": ["The data is unbalanced with respect of Gleason score values distribution. \n\nLet's check now relative distribution of ISUP grade anf Gleason score values."], "code": "# Reference: https://www.kaggle.com/code/gpreda/panda-challenge-starting-eda\n\nfig, ax = plt.subplots(nrows=1,figsize=(12,6))\ntmp = train_df.groupby('isup_grade')['gleason_score'].value_counts()\ndf = pd.DataFrame(data={'Exams': tmp.values}, index=tmp.index).reset_index()\nsns.barplot(ax=ax,x = 'isup_grade', y='Exams',hue='gleason_score',data=df, palette='Set1')\nplt.title(\"Number of examinations grouped on ISUP grade and Gleason score\")\nplt.show()", "processed": ["data unbalanc respect gleason score valu distribut let check rel distribut isup grade anf gleason score valu"]}, {"markdown": ["The only misalignment is in the fact that for ISUP Grade 2, in the data we also have Gleason score 4+3 (which also appears for ISUP Grade 3).  \n\nLet's see how Gleason score is grouped by Data source."], "code": "# Reference: https://www.kaggle.com/code/gpreda/panda-challenge-starting-eda\n\nfig, ax = plt.subplots(nrows=1,figsize=(12,6)) \ntmp = train_df.groupby('data_provider')['gleason_score'].value_counts() \ndf = pd.DataFrame(data={'Exams': tmp.values}, index=tmp.index).reset_index() \nsns.barplot(ax=ax,x = 'data_provider', y='Exams',hue='gleason_score',data=df, palette='Set1') \nplt.title(\"Number of examinations grouped on Data provider and Gleason score\") \nplt.show()\n", "processed": ["misalign fact isup grade 2 data also gleason score 4 3 also appear isup grade 3 let see gleason score group data sourc"]}, {"markdown": ["We can observe that all of the 0+0 Gleason score data samples are from Karolinska while from Radboud we have most of negative data.\n\nFor Karolinska, next (in terms of frequency) are samples with Gleason score 3+3, 3+4, 4+4.\n\nFor Radboud, next (in terms of frequency) most frequent are samples with Gleason score 4+3, 3+3, 3+4, 4+4, 4+5.\n\n\nLet's see how ISUP grade is distributed with respect of Data provider."], "code": "# Reference: https://www.kaggle.com/code/gpreda/panda-challenge-starting-eda\n\nfig, ax = plt.subplots(nrows=1,figsize=(12,6)) \ntmp = train_df.groupby('data_provider')['isup_grade'].value_counts() \ndf = pd.DataFrame(data={'Exams': tmp.values}, index=tmp.index).reset_index() \nsns.barplot(ax=ax,x = 'data_provider', y='Exams',hue='isup_grade',data=df, palette='Set1') \nplt.title(\"Number of examinations grouped on Data provider and ISUP Grade\") \nplt.show()\n", "processed": ["observ 0 0 gleason score data sampl karolinska radboud neg data karolinska next term frequenc sampl gleason score 3 3 3 4 4 4 radboud next term frequenc frequent sampl gleason score 4 3 3 3 3 4 4 4 4 5 let see isup grade distribut respect data provid"]}, {"markdown": ["Let's now represent the distribution of image dimmensions (width, height), the spacing and level_count, also related to the other features, namely data_provide, isup_grade and gleason_score."], "code": "# Reference: https://www.kaggle.com/code/gpreda/panda-challenge-starting-eda\n\nprint(f\" level count: {train_df.level_count.nunique()}\")\nprint(f\" spacing: {train_df.spacing.nunique()}\")\nfig, ax = plt.subplots(nrows=1,figsize=(12,6)) \ntmp = train_df.groupby('data_provider')['spacing'].value_counts() \ndf = pd.DataFrame(data={'Exams': tmp.values}, index=tmp.index).reset_index() \nsns.barplot(ax=ax,x = 'data_provider', y='Exams',hue='spacing',data=df, palette='Set1') \nplt.title(\"Number of examinations grouped on Data provider and Spacing\") \nplt.show()\nfig, ax = plt.subplots(nrows=1,figsize=(12,6)) \ntmp = train_df.groupby('isup_grade')['spacing'].value_counts() \ndf = pd.DataFrame(data={'Exams': tmp.values}, index=tmp.index).reset_index() \nsns.barplot(ax=ax,x = 'isup_grade', y='Exams',hue='spacing',data=df, palette='Set1') \nplt.title(\"Number of examinations grouped on ISUP Grade and Spacing\") \nplt.show()\nfig, ax = plt.subplots(nrows=1,figsize=(12,6)) \ntmp = train_df.groupby('gleason_score')['spacing'].value_counts() \ndf = pd.DataFrame(data={'Exams': tmp.values}, index=tmp.index).reset_index() \nsns.barplot(ax=ax,x = 'gleason_score', y='Exams',hue='spacing',data=df, palette='Set1') \nplt.title(\"Number of examinations grouped on Gleason Score and Spacing\") \nplt.show()\nfig, ax = plt.subplots(nrows=1,figsize=(12,6)) \nsns.distplot(train_df['width'], kde=True, label='width')\nsns.distplot(train_df['height'], kde=True, label='height')\nplt.xlabel('dimension')\nplt.title('Images Width and Height distribution')\nplt.legend()\nplt.show()\ndef plot_distribution_grouped(feature, feature_group, hist_flag=True):\n    fig, ax = plt.subplots(nrows=1,figsize=(12,6)) \n    for f in train_df[feature_group].unique():\n        df = train_df.loc[train_df[feature_group] == f]\n        sns.distplot(df[feature], hist=hist_flag, label=f)\n    plt.title(f'Images {feature} distribution, grouped by {feature_group}')\n    plt.legend()\n    plt.show()\nplot_distribution_grouped('width', 'data_provider')\nplot_distribution_grouped('height', 'data_provider')\nplot_distribution_grouped('width', 'isup_grade', False)\nplot_distribution_grouped('height', 'isup_grade', False)\nplot_distribution_grouped('width', 'gleason_score', False)\nplot_distribution_grouped('height', 'gleason_score', False)", "processed": ["let repres distribut imag dimmens width height space level count also relat featur name data provid isup grade gleason score"]}, {"markdown": ["- 24.4% of TransactionIDs in **train** (144233 / 590540) have an associated train_identity.\n- 28.0% of TransactionIDs in **test** (144233 / 590540) have an associated train_identity.", "# Train vs Test are Time Series Split\n\nThe `TransactionDT` feature is a timedelta from a given reference datetime (not an actual timestamp). One early discovery about the data is that the train and test appear to be split by time. There is a slight gap inbetween, but otherwise the training set is from an earlier period of time and test is from a later period of time. This will impact which cross validation techniques should be used.\n\nWe will look into this more when reviewing differences in distribution of features between train and test."], "code": "# Reference: https://www.kaggle.com/code/robikscube/ieee-fraud-detection-first-look-and-eda\n\ntrain_transaction['TransactionDT'].plot(kind='hist',\n                                        figsize=(15, 5),\n                                        label='train',\n                                        bins=50,\n                                        title='Train vs Test TransactionDT distribution')\ntest_transaction['TransactionDT'].plot(kind='hist',\n                                       label='test',\n                                       bins=50)\nplt.legend()\nplt.show()\nax = train_transaction.plot(x='TransactionDT',\n                       y='TransactionAmt',\n                       kind='scatter',\n                       alpha=0.01,\n                       label='TransactionAmt-train',\n                       title='Train and test Transaction Ammounts by Time (TransactionDT)',\n                       ylim=(0, 5000),\n                       figsize=(15, 5))\ntest_transaction.plot(x='TransactionDT',\n                      y='TransactionAmt',\n                      kind='scatter',\n                      label='TransactionAmt-test',\n                      alpha=0.01,\n                      color=color_pal[1],\n                       ylim=(0, 5000),\n                      ax=ax)\n# Plot Fraud as Orange\ntrain_transaction.loc[train_transaction['isFraud'] == 1] \\\n    .plot(x='TransactionDT',\n         y='TransactionAmt',\n         kind='scatter',\n         alpha=0.01,\n         label='TransactionAmt-train',\n         title='Train and test Transaction Ammounts by Time (TransactionDT)',\n         ylim=(0, 5000),\n         color='orange',\n         figsize=(15, 5),\n         ax=ax)\nplt.show()", "processed": ["24 4 transactionid train 144233 590540 associ train ident 28 0 transactionid test 144233 590540 associ train ident", "train v test time seri split transactiondt featur timedelta given refer datetim actual timestamp one earli discoveri data train test appear split time slight gap inbetween otherwis train set earlier period time test later period time impact cross valid techniqu use look review differ distribut featur train test"]}, {"markdown": ["# Distribution of Target in Training Set\n- 3.5% of transacations are fraud"], "code": "# Reference: https://www.kaggle.com/code/robikscube/ieee-fraud-detection-first-look-and-eda\n\nprint('  {:.4f}% of Transactions that are fraud in train '.format(train_transaction['isFraud'].mean() * 100))\ntrain_transaction.groupby('isFraud') \\\n    .count()['TransactionID'] \\\n    .plot(kind='barh',\n          title='Distribution of Target in Train',\n          figsize=(15, 3))\nplt.show()", "processed": ["distribut target train set 3 5 transac fraud"]}, {"markdown": ["## TransactionAmt\nThe ammount of transaction. I've taken a log transform in some of these plots to better show the distribution- otherwise the few, very large transactions skew the distribution. Because of the log transfrom, any values between 0 and 1 will appear to be negative."], "code": "# Reference: https://www.kaggle.com/code/robikscube/ieee-fraud-detection-first-look-and-eda\n\ntrain_transaction['TransactionAmt'] \\\n    .apply(np.log) \\\n    .plot(kind='hist',\n          bins=100,\n          figsize=(15, 5),\n          title='Distribution of Log Transaction Amt')\nplt.show()\nfig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 6))\ntrain_transaction.loc[train_transaction['isFraud'] == 1] \\\n    ['TransactionAmt'].apply(np.log) \\\n    .plot(kind='hist',\n          bins=100,\n          title='Log Transaction Amt - Fraud',\n          color=color_pal[1],\n          xlim=(-3, 10),\n         ax= ax1)\ntrain_transaction.loc[train_transaction['isFraud'] == 0] \\\n    ['TransactionAmt'].apply(np.log) \\\n    .plot(kind='hist',\n          bins=100,\n          title='Log Transaction Amt - Not Fraud',\n          color=color_pal[2],\n          xlim=(-3, 10),\n         ax=ax2)\ntrain_transaction.loc[train_transaction['isFraud'] == 1] \\\n    ['TransactionAmt'] \\\n    .plot(kind='hist',\n          bins=100,\n          title='Transaction Amt - Fraud',\n          color=color_pal[1],\n         ax= ax3)\ntrain_transaction.loc[train_transaction['isFraud'] == 0] \\\n    ['TransactionAmt'] \\\n    .plot(kind='hist',\n          bins=100,\n          title='Transaction Amt - Not Fraud',\n          color=color_pal[2],\n         ax=ax4)\nplt.show()", "processed": ["transactionamt ammount transact taken log transform plot better show distribut otherwis larg transact skew distribut log transfrom valu 0 1 appear neg"]}, {"markdown": ["## ProductCD\n- For now we don't know exactly what these values represent.\n- `W` has the most number of observations, `C` the least.\n- ProductCD `C` has the most fraud with >11%\n- ProductCD `W` has the least with ~2%"], "code": "# Reference: https://www.kaggle.com/code/robikscube/ieee-fraud-detection-first-look-and-eda\n\ntrain_transaction.groupby('ProductCD') \\\n    ['TransactionID'].count() \\\n    .sort_index() \\\n    .plot(kind='barh',\n          figsize=(15, 3),\n         title='Count of Observations by ProductCD')\nplt.show()\ntrain_transaction.groupby('ProductCD')['isFraud'] \\\n    .mean() \\\n    .sort_index() \\\n    .plot(kind='barh',\n          figsize=(15, 3),\n         title='Percentage of Fraud by ProductCD')\nplt.show()", "processed": ["productcd know exactli valu repres w number observ c least productcd c fraud 11 productcd w least 2"]}, {"markdown": ["# Categorical Features - Transaction\nWe are told in the data description that the following transaction columns are categorical:\n- ProductCD\n- emaildomain\n- card1 - card6\n- addr1, addr2\n- P_emaildomain\n- R_emaildomain\n- M1 - M9", "# card1 - card6\n- We are told these are all categorical, even though some appear numeric."], "code": "# Reference: https://www.kaggle.com/code/robikscube/ieee-fraud-detection-first-look-and-eda\n\ncard_cols = [c for c in train_transaction.columns if 'card' in c]\ntrain_transaction[card_cols].head()\ncolor_idx = 0\nfor c in card_cols:\n    if train_transaction[c].dtype in ['float64','int64']:\n        train_transaction[c].plot(kind='hist',\n                                      title=c,\n                                      bins=50,\n                                      figsize=(15, 2),\n                                      color=color_pal[color_idx])\n    color_idx += 1\n    plt.show()\ntrain_transaction_fr = train_transaction.loc[train_transaction['isFraud'] == 1]\ntrain_transaction_nofr = train_transaction.loc[train_transaction['isFraud'] == 0]\nfig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 8))\ntrain_transaction_fr.groupby('card4')['card4'].count().plot(kind='barh', ax=ax1, title='Count of card4 fraud')\ntrain_transaction_nofr.groupby('card4')['card4'].count().plot(kind='barh', ax=ax2, title='Count of card4 non-fraud')\ntrain_transaction_fr.groupby('card6')['card6'].count().plot(kind='barh', ax=ax3, title='Count of card6 fraud')\ntrain_transaction_nofr.groupby('card6')['card6'].count().plot(kind='barh', ax=ax4, title='Count of card6 non-fraud')\nplt.show()", "processed": ["categor featur transact told data descript follow transact column categor productcd emaildomain card1 card6 addr1 addr2 p emaildomain r emaildomain m1 m9", "card1 card6 told categor even though appear numer"]}, {"markdown": ["# addr1 & addr2\nThe data description states that these are categorical even though they look numeric. Could they be the address value?"], "code": "# Reference: https://www.kaggle.com/code/robikscube/ieee-fraud-detection-first-look-and-eda\n\nprint(' addr1 - has {} NA values'.format(train_transaction['addr1'].isna().sum()))\nprint(' addr2 - has {} NA values'.format(train_transaction['addr2'].isna().sum()))\ntrain_transaction['addr1'].plot(kind='hist', bins=500, figsize=(15, 2), title='addr1 distribution')\nplt.show()\ntrain_transaction['addr2'].plot(kind='hist', bins=500, figsize=(15, 2), title='addr2 distribution')\nplt.show()", "processed": ["addr1 addr2 data descript state categor even though look numer could address valu"]}, {"markdown": ["# dist1 & dist2\nPlotting with logx to better show the distribution. Possibly this could be the distance of the transaction vs. the card owner's home/work address. This is just a guess."], "code": "# Reference: https://www.kaggle.com/code/robikscube/ieee-fraud-detection-first-look-and-eda\n\ntrain_transaction['dist1'].plot(kind='hist',\n                                bins=5000,\n                                figsize=(15, 2),\n                                title='dist1 distribution',\n                                color=color_pal[1],\n                                logx=True)\nplt.show()\ntrain_transaction['dist2'].plot(kind='hist',\n                                bins=5000,\n                                figsize=(15, 2),\n                                title='dist2 distribution',\n                                color=color_pal[1],\n                                logx=True)\nplt.show()", "processed": ["dist1 dist2 plot logx better show distribut possibl could distanc transact v card owner home work address guess"]}, {"markdown": ["# C1 - C14\nBecause we are provided many numerical columns, we can create a pairplot to plot feature interactions. I know these plots can be hard to read, but it is helpful for gaining intution about potential feature interactions and if certain features have more variance than others."], "code": "# Reference: https://www.kaggle.com/code/robikscube/ieee-fraud-detection-first-look-and-eda\n\nc_cols = [c for c in train_transaction if c[0] == 'C']\ntrain_transaction[c_cols].head()\n# Sample 500 fraud and 500 non-fraud examples to plot\nsampled_train = pd.concat([train_transaction.loc[train_transaction['isFraud'] == 0].sample(500),\n          train_transaction.loc[train_transaction['isFraud'] == 1].sample(500)])\n\nsns.pairplot(sampled_train, \n             hue='isFraud',\n            vars=c_cols)\nplt.show()", "processed": ["c1 c14 provid mani numer column creat pairplot plot featur interact know plot hard read help gain intut potenti featur interact certain featur varianc other"]}, {"markdown": ["# D1-D9\nSimilarly for features D1-D9. In these plots we can see some linear and non-linear interactions between features. We may want to create additional features using these interactions if we think it would help our model better find relationship between fraud and non-fraud observations."], "code": "# Reference: https://www.kaggle.com/code/robikscube/ieee-fraud-detection-first-look-and-eda\n\nd_cols = [c for c in train_transaction if c[0] == 'D']\ntrain_transaction[d_cols].head()\nsns.pairplot(sampled_train, \n             hue='isFraud',\n            vars=d_cols)\nplt.show()", "processed": ["d1 d9 similarli featur d1 d9 plot see linear non linear interact featur may want creat addit featur use interact think would help model better find relationship fraud non fraud observ"]}, {"markdown": ["# M1-M9\n- Values are `T` `F` or `NaN`\n- Column `M4` appears to be different with values like `M2` and `M0`"], "code": "# Reference: https://www.kaggle.com/code/robikscube/ieee-fraud-detection-first-look-and-eda\n\nm_cols = [c for c in train_transaction if c[0] == 'M']\ntrain_transaction[m_cols].head()\n(train_transaction[m_cols] == 'T').sum().plot(kind='bar',\n                                              title='Count of T by M column',\n                                              figsize=(15, 2),\n                                              color=color_pal[3])\nplt.show()\n(train_transaction[m_cols] == 'F').sum().plot(kind='bar',\n                                              title='Count of F by M column',\n                                              figsize=(15, 2),\n                                              color=color_pal[4])\nplt.show()\n(train_transaction[m_cols].isna()).sum().plot(kind='bar',\n                                              title='Count of NaN by M column',\n                                              figsize=(15, 2),\n                                              color=color_pal[0])\nplt.show()\n# Looking at M4 column since it is different than the others\ntrain_transaction.groupby('M4')['TransactionID'] \\\n    .count() \\\n    .plot(kind='bar',\n          title='Count of values for M4',\n          figsize=(15, 3))\nplt.show()", "processed": ["m1 m9 valu f nan column m4 appear differ valu like m2 m0"]}, {"markdown": ["# V1 - V339\nLots of 1s 0s and Nans, some larger values"], "code": "# Reference: https://www.kaggle.com/code/robikscube/ieee-fraud-detection-first-look-and-eda\n\nv_cols = [c for c in train_transaction if c[0] == 'V']\ntrain_transaction[v_cols].head()\ntrain_transaction[v_cols].describe()\ntrain_transaction['v_mean'] = train_transaction[v_cols].mean(axis=1)\nfig, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(15, 6))\ntrain_transaction.loc[train_transaction['isFraud'] == 1]['v_mean'] \\\n    .apply(np.log) \\\n    .plot(kind='hist',\n          bins=100,\n          title='log transformed mean of V columns - Fraud',\n          ax=ax1)\ntrain_transaction.loc[train_transaction['isFraud'] == 0]['v_mean'] \\\n    .apply(np.log) \\\n    .plot(kind='hist',\n          bins=100,\n          title='log transformed mean of V columns - Not Fraud',\n          color=color_pal[5],\n          ax=ax2)\nplt.show()", "processed": ["v1 v339 lot 1 0 nan larger valu"]}, {"markdown": ["## DeviceType"], "code": "# Reference: https://www.kaggle.com/code/robikscube/ieee-fraud-detection-first-look-and-eda\n\ntrain_identity_.groupby('DeviceType') \\\n    .mean()['isFraud'] \\\n    .sort_values() \\\n    .plot(kind='barh',\n          figsize=(15, 5),\n          title='Percentage of Fraud by Device Type')\nplt.show()\ntrain_identity_.groupby('DeviceInfo') \\\n    .count()['TransactionID'] \\\n    .sort_values(ascending=False) \\\n    .head(20) \\\n    .plot(kind='barh', figsize=(15, 5), title='Top 20 Devices in Train')\nplt.show()", "processed": ["devicetyp"]}, {"markdown": ["## Identity info as a function of time"], "code": "# Reference: https://www.kaggle.com/code/robikscube/ieee-fraud-detection-first-look-and-eda\n\nid_cols = [c for c in train_identity.columns if 'id' in c]\nfor i in id_cols:\n    try:\n        train_identity_.set_index('TransactionDT')[i].plot(style='.', title=i, figsize=(15, 3))\n        test_identity_.set_index('TransactionDT')[i].plot(style='.', title=i, figsize=(15, 3))\n        plt.show()\n    except TypeError:\n        pass", "processed": ["ident info function time"]}, {"markdown": ["## Data exploration"], "code": "# Reference: https://www.kaggle.com/code/artgor/santander-eda-fe-fs-and-models\n\ntrain.head()\ntrain[train.columns[2:]].std().plot('hist');\nplt.title('Distribution of stds of all columns');\ntrain[train.columns[2:]].mean().plot('hist');\nplt.title('Distribution of means of all columns');\ntrain.head()\n# we have no missing values\ntrain.isnull().any().any()\nprint('Distributions of first 28 columns')\nplt.figure(figsize=(26, 24))\nfor i, col in enumerate(list(train.columns)[2:30]):\n    plt.subplot(7, 4, i + 1)\n    plt.hist(train[col])\n    plt.title(col)\ntrain['target'].value_counts(normalize=True)", "processed": ["data explor"]}, {"markdown": ["We can see that all features have a low correlation with target. So we have no highly correlated features which we could drop, on the other hand we could drop some columns with have little correlation with the target.", "## Basic modelling"], "code": "# Reference: https://www.kaggle.com/code/artgor/santander-eda-fe-fs-and-models\n\nX = train.drop(['ID_code', 'target'], axis=1)\ny = train['target']\nX_test = test.drop(['ID_code'], axis=1)\nn_fold = 4\nfolds = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=42)\nrepeated_folds = RepeatedStratifiedKFold(n_splits=10, n_repeats=20, random_state=42)\n\n# scaler = StandardScaler()\n# X_train = scaler.fit_transform(X_train)\n# X_test = scaler.transform(X_test)\ndef train_model(X, X_test, y, params, folds, model_type='lgb', plot_feature_importance=False, averaging='usual', model=None):\n    oof = np.zeros(len(X))\n    prediction = np.zeros(len(X_test))\n    scores = []\n    feature_importance = pd.DataFrame()\n    for fold_n, (train_index, valid_index) in enumerate(folds.split(X, y)):\n        print('Fold', fold_n, 'started at', time.ctime())\n        X_train, X_valid = X.loc[train_index], X.loc[valid_index]\n        y_train, y_valid = y[train_index], y[valid_index]\n        \n        if model_type == 'lgb':\n            train_data = lgb.Dataset(X_train, label=y_train)\n            valid_data = lgb.Dataset(X_valid, label=y_valid)\n            \n            model = lgb.train(params,\n                    train_data,\n                    num_boost_round=20000,\n                    valid_sets = [train_data, valid_data],\n                    verbose_eval=1000,\n                    early_stopping_rounds = 200)\n            \n            y_pred_valid = model.predict(X_valid)\n            y_pred = model.predict(X_test, num_iteration=model.best_iteration)\n            \n        if model_type == 'xgb':\n            train_data = xgb.DMatrix(data=X_train, label=y_train, feature_names=X_train.columns)\n            valid_data = xgb.DMatrix(data=X_valid, label=y_valid, feature_names=X_train.columns)\n\n            watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]\n            model = xgb.train(dtrain=train_data, num_boost_round=20000, evals=watchlist, early_stopping_rounds=200, verbose_eval=500, params=params)\n            y_pred_valid = model.predict(xgb.DMatrix(X_valid, feature_names=X_train.columns), ntree_limit=model.best_ntree_limit)\n            y_pred = model.predict(xgb.DMatrix(X_test, feature_names=X_train.columns), ntree_limit=model.best_ntree_limit)\n        \n        if model_type == 'sklearn':\n            model = model\n            model.fit(X_train, y_train)\n            y_pred_valid = model.predict_proba(X_valid).reshape(-1,)\n            score = roc_auc_score(y_valid, y_pred_valid)\n            # print(f'Fold {fold_n}. AUC: {score:.4f}.')\n            # print('')\n            \n            y_pred = model.predict_proba(X_test)[:, 1]\n            \n        if model_type == 'glm':\n            model = sm.GLM(y_train, X_train, family=sm.families.Binomial())\n            model_results = model.fit()\n            model_results.predict(X_test)\n            y_pred_valid = model_results.predict(X_valid).reshape(-1,)\n            score = roc_auc_score(y_valid, y_pred_valid)\n            \n            y_pred = model_results.predict(X_test)\n            \n        if model_type == 'cat':\n            model = CatBoostClassifier(iterations=20000, learning_rate=0.05, loss_function='Logloss',  eval_metric='AUC', **params)\n            model.fit(X_train, y_train, eval_set=(X_valid, y_valid), cat_features=[], use_best_model=True, verbose=False)\n\n            y_pred_valid = model.predict_proba(X_valid)[:, 1]\n            y_pred = model.predict_proba(X_test)[:, 1]\n            \n        oof[valid_index] = y_pred_valid.reshape(-1,)\n        scores.append(roc_auc_score(y_valid, y_pred_valid))\n\n        if averaging == 'usual':\n            prediction += y_pred\n        elif averaging == 'rank':\n            prediction += pd.Series(y_pred).rank().values  \n        \n        if model_type == 'lgb':\n            # feature importance\n            fold_importance = pd.DataFrame()\n            fold_importance[\"feature\"] = X.columns\n            fold_importance[\"importance\"] = model.feature_importance()\n            fold_importance[\"fold\"] = fold_n + 1\n            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n\n    prediction /= n_fold\n    \n    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n    \n    if model_type == 'lgb':\n        feature_importance[\"importance\"] /= n_fold\n        if plot_feature_importance:\n            cols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n                by=\"importance\", ascending=False)[:50].index\n\n            best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n\n            plt.figure(figsize=(16, 12));\n            sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False));\n            plt.title('LGB Features (avg over folds)');\n        \n            return oof, prediction, feature_importance\n        return oof, prediction, scores\n    \n    else:\n        return oof, prediction, scores\n# %%time\n# model = linear_model.LogisticRegression(class_weight='balanced', penalty='l2', C=0.1)\n# oof_lr, prediction_lr, scores = train_model(X, X_test, y, params=None, folds=folds, model_type='sklearn', model=model)\nparams = {'num_leaves': 128,\n         'min_data_in_leaf': 42,\n         'objective': 'binary',\n         'max_depth': 16,\n         'learning_rate': 0.0123,\n         'boosting': 'gbdt',\n         'bagging_freq': 5,\n         'feature_fraction': 0.8201,\n         'bagging_seed': 11,\n         'reg_alpha': 1.728910519108444,\n         'reg_lambda': 4.9847051755586085,\n         'random_state': 42,\n         'metric': 'auc',\n         'verbosity': -1,\n         'subsample': 0.81,\n         'min_gain_to_split': 0.01077313523861969,\n         'min_child_weight': 19.428902804238373,\n         'num_threads': 4}\n# oof_lgb, prediction_lgb, scores = train_model(X, X_test, y, params=params, folds=folds, model_type='lgb', plot_feature_importance=True)\n# sub = pd.read_csv('../input/sample_submission.csv')\n# sub['target'] = prediction_lgb\n# sub.to_csv('lgb.csv', index=False)", "processed": ["see featur low correl target highli correl featur could drop hand could drop column littl correl target", "basic model"]}, {"markdown": ["For EDA and later modeling, it might be a good idea to create some metafeatures. This work is partly based on SRK's great EDAs, and [this one](http://www.kaggle.com/sudalairajkumar/simple-feature-engg-notebook-spooky-author) in particular. The metafeatures that we'll create are:\n\n\n* Number of words in the text\n* Number of unique words in the text\n* Number of characters in the text\n* Number of stopwords\n* Number of punctuations\n* Number of upper case words\n* Number of title case words\n* Average length of the words"], "code": "# Reference: https://www.kaggle.com/code/tunguz/just-some-simple-eda\n\n## Number of words in the text ##\ntrain[\"num_words\"] = train[\"question_text\"].apply(lambda x: len(str(x).split()))\ntest[\"num_words\"] = test[\"question_text\"].apply(lambda x: len(str(x).split()))\n\n## Number of unique words in the text ##\ntrain[\"num_unique_words\"] = train[\"question_text\"].apply(lambda x: len(set(str(x).split())))\ntest[\"num_unique_words\"] = test[\"question_text\"].apply(lambda x: len(set(str(x).split())))\n\n## Number of characters in the text ##\ntrain[\"num_chars\"] = train[\"question_text\"].apply(lambda x: len(str(x)))\ntest[\"num_chars\"] = test[\"question_text\"].apply(lambda x: len(str(x)))\n\n## Number of stopwords in the text ##\ntrain[\"num_stopwords\"] = train[\"question_text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\ntest[\"num_stopwords\"] = test[\"question_text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n\n## Number of punctuations in the text ##\ntrain[\"num_punctuations\"] =train['question_text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\ntest[\"num_punctuations\"] =test['question_text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n\n## Number of title case words in the text ##\ntrain[\"num_words_upper\"] = train[\"question_text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\ntest[\"num_words_upper\"] = test[\"question_text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n\n## Number of title case words in the text ##\ntrain[\"num_words_title\"] = train[\"question_text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\ntest[\"num_words_title\"] = test[\"question_text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n\n## Average length of the words in the text ##\ntrain[\"mean_word_len\"] = train[\"question_text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\ntest[\"mean_word_len\"] = test[\"question_text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\nplt.figure(figsize=(12,8))\nsns.violinplot(data=train['num_words'])\nplt.show()\n\nplt.figure(figsize=(12,8))\nsns.violinplot(data=train['num_unique_words'])\nplt.show()\nplt.figure(figsize=(12,8))\nsns.violinplot(data=train['num_chars'])\nplt.show()\nplt.figure(figsize=(12,8))\nsns.violinplot(data=train['num_stopwords'])\nplt.show()\nplt.figure(figsize=(12,8))\nsns.violinplot(data=train['num_punctuations'])\nplt.show()\nplt.figure(figsize=(12,8))\nsns.violinplot(data=train['num_words_upper'])\nplt.show()\nplt.figure(figsize=(12,8))\nsns.violinplot(data=train['num_words_title'])\nplt.show()\nplt.figure(figsize=(12,8))\nsns.violinplot(data=train['mean_word_len'])\nplt.show()\neng_features = ['num_words', 'num_unique_words', 'num_chars', \n                'num_stopwords', 'num_punctuations', 'num_words_upper', \n                'num_words_title', 'mean_word_len']\nkf = KFold(n_splits=5, shuffle=True, random_state=43)\ntest_pred = 0\noof_pred = np.zeros([train.shape[0],])\n\nx_test = test[eng_features].values\nfor i, (train_index, val_index) in tqdm(enumerate(kf.split(train))):\n    x_train, x_val = train.loc[train_index][eng_features].values, train.loc[val_index][eng_features].values\n    y_train, y_val = train_target[train_index], train_target[val_index]\n    classifier = LogisticRegression(C= 0.1)\n    classifier.fit(x_train, y_train)\n    val_preds = classifier.predict_proba(x_val)[:,1]\n    preds = classifier.predict_proba(x_test)[:,1]\n    test_pred += 0.2*preds\n    oof_pred[val_index] = val_preds\npred_train = (oof_pred > 0.5).astype(np.int)\nf1_score(train_target, pred_train)\nf1_score(train_target, pred_train)\npred_train = (oof_pred > 0.12).astype(np.int)\nf1_score(train_target, pred_train)", "processed": ["eda later model might good idea creat metafeatur work partli base srk great eda one http www kaggl com sudalairajkumar simpl featur engg notebook spooki author particular metafeatur creat number word text number uniqu word text number charact text number stopword number punctuat number upper case word number titl case word averag length word"]}, {"markdown": ["Numerical columns are either normalized or show a percentage, so no need to scale them."], "code": "# Reference: https://www.kaggle.com/code/artgor/eda-and-models-score-0-74291\n\ntrain.head()\nplt.subplot(1,4,1)\ntrain.groupby('type').mean()['rotting_flesh'].plot(kind='bar',figsize=(7,4), color='r')\nplt.subplot(1,4,2)\ntrain.groupby('type').mean()['bone_length'].plot(kind='bar',figsize=(7,4), color='g')\nplt.subplot(1,4,3)\ntrain.groupby('type').mean()['hair_length'].plot(kind='bar',figsize=(7,4), color='y')\nplt.subplot(1,4,4)\ntrain.groupby('type').mean()['has_soul'].plot(kind='bar',figsize=(7,4), color='teal')", "processed": ["numer column either normal show percentag need scale"]}, {"markdown": ["It seems that all numerical features may be useful."], "code": "# Reference: https://www.kaggle.com/code/artgor/eda-and-models-score-0-74291\n\nsns.factorplot(\"type\", col=\"color\", col_wrap=4, data=train, kind=\"count\", size=2.4, aspect=.8)", "processed": ["seem numer featur may use"]}, {"markdown": ["Funny, but many colors are evenly distributes among the monsters. So they maybe nor very useful for analysis."], "code": "# Reference: https://www.kaggle.com/code/artgor/eda-and-models-score-0-74291\n\n#The graphs look much better with higher figsize.\nfig, ax = plt.subplots(2, 2, figsize = (16, 12))\nsns.pointplot(x=\"color\", y=\"rotting_flesh\", hue=\"type\", data=train, ax = ax[0, 0])\nsns.pointplot(x=\"color\", y=\"bone_length\", hue=\"type\", data=train, ax = ax[0, 1])\nsns.pointplot(x=\"color\", y=\"hair_length\", hue=\"type\", data=train, ax = ax[1, 0])\nsns.pointplot(x=\"color\", y=\"has_soul\", hue=\"type\", data=train, ax = ax[1, 1])", "processed": ["funni mani color evenli distribut among monster mayb use analysi"]}, {"markdown": ["In most cases color won't \"help\" other variables to improve accuracy."], "code": "# Reference: https://www.kaggle.com/code/artgor/eda-and-models-score-0-74291\n\nsns.pairplot(train, hue='type')", "processed": ["case color help variabl improv accuraci"]}, {"markdown": ["The above graph clearly shows that all melanoma images are diagnosed as melanoma (as expected), and **most non-melanoma images are diagnosed as either \"unknown\" or \"nevus\".**", "## Approximate age vs. Target\n\nFinally let us see how the **approximate age** affects the **target**."], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/siim-isic-melanoma-eda-pytorch-baseline\n\nnums_1 = train_df.query(\"target == 1\")[\"age_approx\"]\nnums_2 = train_df.query(\"target == 0\")[\"age_approx\"]\n\nnums_1 = nums_1.fillna(nums_1.mean())\nnums_2 = nums_2.fillna(nums_2.mean())\n\nfig = ff.create_distplot(hist_data=[nums_1, nums_2],\n                         group_labels=[\"0\", \"1\"],\n                         colors=[\"darkorange\", \"dodgerblue\"], show_hist=False)\n\nfig.update_layout(title_text=\"Approximate age vs. Target\", xaxis_title=\"Approximate age\",\n                  template=\"plotly_white\", paper_bgcolor=\"#edebeb\")\nfig.show()", "processed": ["graph clearli show melanoma imag diagnos melanoma expect non melanoma imag diagnos either unknown nevu", "approxim age v target final let u see approxim age affect target"]}, {"markdown": ["Ok, Now that we have some idea of how the dataset is, we can start exploring and understanding the dataset", "## Ploting Target (Toxicity) distribuition"], "code": "# Reference: https://www.kaggle.com/code/kabure/simple-eda-hard-views-w-easy-code\n\ncount_target_zero = round(df_train[df_train['target'] == 0]['target'].count() / len(df_train['target']) * 100,2)\nprint(f'Total of zero values in Toxic rate: {count_target_zero}%')\n\nplt.figure(figsize=(13,6))\n\ng = sns.distplot(df_train[df_train['target'] > 0]['target'])\nplt.title('Toxic Distribuition', fontsize=22)\nplt.xlabel(\"Toxic Rate\", fontsize=18)\nplt.ylabel(\"Distribuition\", fontsize=18) \n\nplt.show()", "processed": ["ok idea dataset start explor understand dataset", "plote target toxic distribuit"]}, {"markdown": ["Interesting distribuition. <br>\nFor default, in the competition description we will consider toxic when the target has values above 0.5", "## Toxicity Subtype attributes Distribution"], "code": "# Reference: https://www.kaggle.com/code/kabure/simple-eda-hard-views-w-easy-code\n\ncomment_adj = ['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']\n\nplt.figure(figsize=(14,6))\n\nfor col in comment_adj[1:]:\n    g = sns.distplot(df_train[df_train[col] > 0][col], label=col, hist=False)\n    #plt.legend(f'{col} Distribuition', fontsize=22)\n    plt.xlabel(\"Rate\", fontsize=18)\n    plt.ylabel(\"Distribuition\", fontsize=18)\n    plt.legend(loc=1, prop={'size': 14})\n\nplt.show()", "processed": ["interest distribuit br default competit descript consid toxic target valu 0 5", "toxic subtyp attribut distribut"]}, {"markdown": ["## Ploting the Distribuition of Categorical Toxic and Non-Toxic"], "code": "# Reference: https://www.kaggle.com/code/kabure/simple-eda-hard-views-w-easy-code\n\ndf_train['toxic'].value_counts().iplot(kind='bar', xTitle='Toxic or Non-Toxic', yTitle=\"Count\", \n                                       title='Distribuition of Toxicity of comments')", "processed": ["plote distribuit categor toxic non toxic"]}, {"markdown": ["## Ploting the distribuitions of the data modelling"], "code": "# Reference: https://www.kaggle.com/code/kabure/simple-eda-hard-views-w-easy-code\n\nfor plot, text in zip(list_groupbys, list_names):\n    plot.T.iplot(kind='bar', xTitle='Demographic categories', yTitle='Count',\n                 title=text)", "processed": ["plote distribuit data model"]}, {"markdown": ["We can see and have insight about some of the toxicy comments. Further, I will explore the comments;", "## Rating distribution"], "code": "# Reference: https://www.kaggle.com/code/kabure/simple-eda-hard-views-w-easy-code\n\ndf_train['rating'].value_counts().iplot(kind='bar', title='Rating of Comment', \n                                        xTitle='Rating', yTitle='Count')", "processed": ["see insight toxici comment explor comment", "rate distribut"]}, {"markdown": ["## Rating by Toxic and Non-Toxic comments"], "code": "# Reference: https://www.kaggle.com/code/kabure/simple-eda-hard-views-w-easy-code\n\nround((pd.crosstab(df_train['rating'], df_train['toxic'], \n            normalize='index') * 100),2).iplot(kind='bar', barmode='stack', \n                                               title= \"Rating Ratio by Toxic and Non-Toxic\",\n                                               xTitle=\"Rating Status\", yTitle='% of Toxic and Non-Toxic')\n", "processed": ["rate toxic non toxic comment"]}, {"markdown": ["It's interesting. <br>\n5% of approved comments are Toxic; <br>\nAs we can see 66% of rejected comments aren't toxic.\n", "", "## Understanding Toxic and Non-Toxic Comments by Dates"], "code": "# Reference: https://www.kaggle.com/code/kabure/simple-eda-hard-views-w-easy-code\n\n# transforming to pandas \ndf_train['created_date'] = pd.to_datetime(df_train['created_date'], format='%Y-%m-%d %H:%M:%S')\n\ndf_train['month'] = df_train['created_date'].dt.month\ndf_train['weekday'] =  df_train['created_date'].dt.weekday_name\ndf_train['hour'] =  df_train['created_date'].dt.hour\n\n# df_train['created_date'] = pd.to_datetime(df_train['created_date'])\n# printing the first and last date\nprint(f'The first date is {df_train[\"created_date\"].dt.date.min()} and the last date is {df_train[\"created_date\"].dt.date.max()}')\n# I will filter by comments date higher than 2016-01-01\ntoxic_comment_dates = df_train[df_train['created_date'] >= '2016-01-01'].groupby([df_train['created_date'].dt.date,'toxic'])['id'].count().sort_index().unstack(\"toxic\").fillna(0)\n\ntoxic_comment_dates.iplot(kind='bar', barmode='stack', \n                   title='Toxic and Non-Toxic Comment by Date', \n                   xTitle='Dates', yTitle='Comment Counts'\n                )", "processed": ["interest br 5 approv comment toxic br see 66 reject comment toxic", "", "understand toxic non toxic comment date"]}, {"markdown": ["We can see that altough the number comments has increased a lot, the toxic comments are seemgly a constant value", "## Looking the distribution by Months;\nWe will consider the dates higher than 2016-01-01 so I have a small sample of months. <br>\nIt's just to explore and may find a interesting pattern."], "code": "# Reference: https://www.kaggle.com/code/kabure/simple-eda-hard-views-w-easy-code\n\n# dates higher than 2016-01-01\ntoxic_comment_months = df_train[df_train['created_date'] >= '2016-01-01'].groupby(['month','toxic'])['id'].count().sort_index().unstack(\"toxic\").fillna(0)\n\ntoxic_comment_months.iplot(kind='bar', barmode='group', \n                   title='Toxic and Non-Toxic Comment by Date', \n                   xTitle='Dates', yTitle='Comment Counts'\n                )", "processed": ["see altough number comment increas lot toxic comment seemgli constant valu", "look distribut month consid date higher 2016 01 01 small sampl month br explor may find interest pattern"]}, {"markdown": ["## Looking the distribution by week days;"], "code": "# Reference: https://www.kaggle.com/code/kabure/simple-eda-hard-views-w-easy-code\n\n# dates higher than 2016-01-01\ntoxic_comment_week = df_train[df_train['created_date'] >= '2016-01-01'].groupby(['weekday','toxic'])['id'].count().sort_index().unstack(\"toxic\").fillna(0).sort_index()\n\ntoxic_comment_week.iplot(kind='bar', barmode='group', \n                   title='Toxic and Non-Toxic Comment by Weekdays', \n                   xTitle='Weekday Names', yTitle='Comment Counts'\n                )\n# dates higher than 2016-01-01\ntoxic_comment_hour = df_train[df_train['created_date'] >= '2016-01-01'].groupby(['hour','toxic'])['id'].count().sort_index().unstack(\"toxic\").fillna(0)\n\ntoxic_comment_hour.iplot(kind='bar', barmode='group', \n                   title='Toxic and Non-Toxic Comment by Hours', \n                   xTitle='Weekday Names', yTitle='Comment Counts'\n                )", "processed": ["look distribut week day"]}, {"markdown": ["## Ploting distribuition of text metrics"], "code": "# Reference: https://www.kaggle.com/code/kabure/simple-eda-hard-views-w-easy-code\n\ntrace0 = go.Box(\n    x=df_train['toxic'].sample(20000),\n    y=df_train['count_word'].sample(20000),\n    name='Toxic', showlegend=False, jitter=0.2, marker = dict( color = 'rgb(0, 128, 128)')\n)\n\ntrace2 = go.Box(\n    x=df_train[(df_train['count_unique_word'] <= 128)]['toxic'].sample(20000),\n    y=df_train[(df_train['count_unique_word'] <= 128)]['count_unique_word'].sample(20000),\n    name='Toxic', showlegend=False, marker = dict( color = 'rgb(0, 128, 128)')\n)\n\ntrace4 = go.Box(\n    x=df_train[ (df_train['count_letters'] <= 999)]['toxic'].sample(20000),\n    y=df_train[  (df_train['count_letters'] <= 999)]['count_letters'].sample(20000),\n    name='Toxic',  showlegend=False, marker = dict( color = 'rgb(0, 128, 128)')\n)\n\n\ntrace6 = go.Box(\n    x=df_train[ (df_train['count_punctuations'] <= 45)]['toxic'].sample(20000),\n    y=df_train[  (df_train['count_punctuations'] <= 45)]['count_punctuations'].sample(20000),\n    name='Toxic',  showlegend=False, marker = dict( color = 'rgb(0, 128, 128)')\n)\n\n\ntrace8 = go.Box(\n    x=df_train[ (df_train['count_words_upper'] <= 9)]['toxic'].sample(20000),\n    y=df_train[ (df_train['count_words_upper'] <= 9)]['count_words_upper'].sample(20000),\n    name='Toxic', showlegend=False, marker = dict( color = 'rgb(0, 128, 128)')\n)\n\ntrace10 = go.Box(\n    x=df_train[ (df_train['count_words_title'] <= 9)]['toxic'].sample(20000),\n    y=df_train[ (df_train['count_words_title'] <= 9)]['count_words_title'].sample(20000),\n    name='Toxic', showlegend=False, marker = dict( color = 'rgb(0, 128, 128)')\n)\n\ntrace12 = go.Box(\n    x=df_train[ (df_train['count_stopwords'] <= 88)]['toxic'].sample(20000),\n    y=df_train[ (df_train['count_stopwords'] <= 88)]['count_stopwords'].sample(20000),\n    name='Toxic',  showlegend=False, marker = dict( color = 'rgb(0, 128, 128)')\n)\n\ntrace14 = go.Box(\n    x=df_train[  (df_train['mean_word_len'] <= 9.129411)]['toxic'].sample(20000),\n    y=df_train[ (df_train['mean_word_len'] <= 9.129411)]['mean_word_len'].sample(20000),\n    name='Toxic',  showlegend=False, marker = dict( color = 'rgb(0, 128, 128)')\n    \n)\n\n\ndata = [trace0, trace2, trace4, trace6,trace8, \n        trace10,trace12, trace14]\n\nfig = tls.make_subplots(rows=4, cols=2, specs=[[{}, {}], \n                                               [{}, {}], \n                                               [{}, {}], \n                                               [{}, {}]],\n                          subplot_titles=('Word Counts','Unique Words Count', 'Letters Count', 'Punctuation Count', \n                                          'Upper Case Count','Words Title Count', 'Stopwords Count', 'Mean Words Len'))\n\nfig.append_trace(trace0, 1, 1)\nfig.append_trace(trace2, 1, 2)\nfig.append_trace(trace4, 2, 1)\nfig.append_trace(trace6, 2, 2)\nfig.append_trace(trace8, 3, 1)\nfig.append_trace(trace10, 3, 2)\nfig.append_trace(trace12, 4, 1)\nfig.append_trace(trace14, 4, 2)\n\nfig['layout'].update(title='Comment Metrics by Toxic and Non-Toxic', autosize=True, boxmode='group')\n\niplot(fig)", "processed": ["plote distribuit text metric"]}, {"markdown": ["# Finding the highest values for each demographic group"], "code": "# Reference: https://www.kaggle.com/code/kabure/simple-eda-hard-views-w-easy-code\n\ndf_train['attacked_group'] = df_train[etnics+ sexual + sexual_orientation + religions + disabilities].replace(0, np.nan).idxmax(axis=1)\ndf_train['attacked_group'] = df_train['attacked_group'].fillna(\"No demo group detected\")\natt_count = df_train['attacked_group'].value_counts()\nprint(f\"Total of No Demographic Group Detected {att_count[0]}\")\n\ndf_train[df_train['attacked_group'] != 'No demo group detected']['attacked_group'].value_counts().iplot(kind='bar', title='Count of Highest values in Attacked Groups',\n                                                xTitle='Demographic Group Name', yTitle='Count of highest \"Citations\"')", "processed": ["find highest valu demograph group"]}, {"markdown": ["We have almost 1.6M comments that are not related to demographic groups that are in the categories", "## Looking the % ratio of Toxic and Non-Toxic for each demo group"], "code": "# Reference: https://www.kaggle.com/code/kabure/simple-eda-hard-views-w-easy-code\n\nattacked_group = pd.crosstab(df_train['attacked_group'],df_train['toxic'], aggfunc='count', values=df_train['target']).apply(lambda r: r/r.sum(), axis=1)\nattacked_group.iplot(kind='bar',barmode='stack',\n                     title='Percent of Toxic and Non-Toxic Comments for Attacked Groups',\n                     xTitle='Demographic Group Name', yTitle='Percent ratio of each Group')", "processed": ["almost 1 6m comment relat demograph group categori", "look ratio toxic non toxic demo group"]}, {"markdown": ["It's a very informative visualization. <br>We can see that some categories, as blacks, hindu (indians?), and hetero(??) orientation  has a highest number of toxic comments against this group. ", "## Understanding the \"Reaction\" metrics"], "code": "# Reference: https://www.kaggle.com/code/kabure/simple-eda-hard-views-w-easy-code\n\n## Again, calling the quantile function that we created before\nquantiles(reactions)\n\n\naggs = {\n    'sexual_explicit': ['sum', 'size'],\n    'likes': ['sum'],\n}\n\n# Previous applications categorical features\n\nprev_agg = df_train[df_train['attacked_group'] != 'No demo group detected'].groupby(['attacked_group','toxic']).agg({**aggs})\n\nprev_agg.columns = pd.Index(['Agg_' + e[0] + \"_\" + e[1].upper() for e in prev_agg.columns.tolist()])\n\nprev_agg.rename(columns={'Agg_sexual_explicit_SUM':'Sexual bias sum',\n                         'Agg_likes_SUM':' Likes sum',\n                         'Agg_sexual_explicit_SIZE':'Total Comments'}, inplace=True)\nprev_agg.T\nprev_agg.sort_index().unstack(\"toxic\").fillna(0).iplot(kind='bar', showlegend=False, \n                                                       title ='Demographic Groups by Sum of sexual explicit and Sum of Likes',\n                                                       xTitle='Demographic Groups', yTitle='Count')", "processed": ["inform visual br see categori black hindu indian hetero orient highest number toxic comment group", "understand reaction metric"]}, {"markdown": ["- I will try a way to best apresent this chart above\n\nIt's another interesting information about the data. Below, I will try to investigate it further;", "## Let's invetigate some \"stealth\" columns by many metrics\n- Some tests using PieChart"], "code": "# Reference: https://www.kaggle.com/code/kabure/simple-eda-hard-views-w-easy-code\n\ncolor_op = ['#5527A0', '#BB93D7', '#834CF7', '#6C941E', '#93EAEA', '#7425FF', '#F2098A', '#7E87AC', \n            '#EBE36F', '#7FD394', '#49C35D', '#3058EE', '#44FDCF', '#A38F85', '#C4CEE0', '#B63A05', \n            '#4856BF', '#F0DB1B', '#9FDBD9', '#B123AC']\n\ndef PieChart(df_cat, df_value, title, limit=15):\n    \"\"\"\n    This function helps to investigate the proportion of metrics of toxicity and other values\n    \"\"\"\n\n    # count_trace = df_train[df_cat].value_counts()[:limit].to_frame().reset_index()\n    rev_trace = df_train[df_train['toxic'] == \"Toxic\"].sample(50000).groupby(df_cat)[df_value].mean().nlargest(limit).to_frame().reset_index()\n    rev_trace_non = df_train[df_train['toxic'] != \"Toxic\"].sample(50000).groupby(df_cat)[df_value].mean().nlargest(limit).to_frame().reset_index()\n\n    trace1 = go.Pie(labels=rev_trace_non[df_cat], \n                    values=rev_trace_non[df_value], name= \"Non-Toxic\", hole= .5, \n                    hoverinfo=\"label+percent+name+value\", showlegend=True,\n                    domain= {'x': [0, .48]})\n\n    trace2 = go.Pie(labels=rev_trace[df_cat], \n                    values=rev_trace[df_value], name=\"Toxic\", hole= .5, \n                    hoverinfo=\"label+percent+name+value\", showlegend=False, \n                    domain= {'x': [.52, 1]})\n\n    layout = dict(title= title, height=450, font=dict(size=15),\n                  annotations = [\n                      dict(\n                          x=.20, y=.5,\n                          text='Non-Toxic', \n                          showarrow=False,\n                          font=dict(size=20)\n                      ),\n                      dict(\n                          x=.80, y=.5,\n                          text='Toxic', \n                          showarrow=False,\n                          font=dict(size=20)\n                      )\n        ])\n\n    fig = dict(data=[trace1, trace2], layout=layout)\n    iplot(fig)", "processed": ["tri way best apres chart anoth interest inform data tri investig", "let invetig stealth column mani metric test use piechart"]}, {"markdown": ["", "As we can see, the unique valid values are likes and sexual explicit. <br>\nLet's investigate it further", "## Knowing identity_annotator_count"], "code": "# Reference: https://www.kaggle.com/code/kabure/simple-eda-hard-views-w-easy-code\n\nname = df_train['identity_annotator_count'].value_counts()[:8]\n\nfig = pd.crosstab(df_train[df_train['identity_annotator_count'].isin(name.index)]['identity_annotator_count'], \n                  df_train[df_train['identity_annotator_count'].isin(name.index)]['toxic'], \n                  normalize='index').iplot(kind='bar', barmode='stack', bargap=.2, asFigure=True,\n                                           title= \"TOP 8 Identity Annotator by Toxic and Non-Toxic\",\n                                           xTitle=\"Identity Annotator Count\", yTitle='Count')\nfig.layout.xaxis.type = 'category'\niplot(fig)", "processed": ["", "see uniqu valid valu like sexual explicit br let investig", "know ident annot count"]}, {"markdown": ["### Meter type\n> Not every building has all meter types."], "code": "# Reference: https://www.kaggle.com/code/jesucristo/starter-great-energy-predictor\n\nsns.countplot(x='meter', data=train).set_title('{0: electricity, 1: chilledwater, 2: steam, hotwater: 3}\\n\\n')", "processed": ["meter type everi build meter type"]}, {"markdown": ["**Top 5 consuming buildings**"], "code": "# Reference: https://www.kaggle.com/code/jesucristo/starter-great-energy-predictor\n\ntop_buildings = train.groupby(\"building_id\")[\"meter_reading\"].mean().sort_values(ascending = False).iloc[:5]\nfor value in top_buildings.index:\n    train[train[\"building_id\"] == value][\"meter_reading\"].rolling(window = 24).mean().plot()\n    pyplot.title('Building {} at site: {}'.format(value,train[train[\"building_id\"] == value][\"site_id\"].unique()[0]))\n    pyplot.show()", "processed": ["top 5 consum build"]}, {"markdown": ["### primary_use"], "code": "# Reference: https://www.kaggle.com/code/jesucristo/starter-great-energy-predictor\n\nfig, ax = pyplot.subplots(figsize=(10, 8))\nsns.countplot(y='primary_use', data=train)\nfig, ax = pyplot.subplots(figsize=(10, 8))\nsns.countplot(y='primary_use', data=train, hue= 'month')", "processed": ["primari use"]}, {"markdown": ["**Click ```output``` to see the plots**"], "code": "# Reference: https://www.kaggle.com/code/jesucristo/starter-great-energy-predictor\n\nfor s in train.site_id.unique():\n    train[train[\"site_id\"] == s].plot(\"timestamp\", \"meter_reading\")", "processed": ["click output see plot"]}, {"markdown": ["### Plot importance"], "code": "# Reference: https://www.kaggle.com/code/jesucristo/starter-great-energy-predictor\n\nimport matplotlib.pyplot as plt\nfeature_imp = pd.DataFrame(sorted(zip(gbm.feature_importance(), gbm.feature_name()),reverse = True), columns=['Value','Feature'])\nplt.figure(figsize=(10, 5))\nsns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False))\nplt.title('LightGBM Features (avg over folds)')\nplt.tight_layout()\nplt.show()", "processed": ["plot import"]}, {"markdown": ["## 4.3 The change of mloss <a class=\"anchor\" id=\"4.3\"></a>\n\n[Back to Table of Contents](#0.1)"], "code": "# Reference: https://www.kaggle.com/code/vbmokin/higher-lb-score-by-tuning-mloss-upgrade-visual\n\nC1, C2 = tf.constant(70, dtype='float32'), tf.constant(1000, dtype=\"float32\")\n\ndef score(y_true, y_pred):\n    tf.dtypes.cast(y_true, tf.float32)\n    tf.dtypes.cast(y_pred, tf.float32)\n    sigma = y_pred[:, 2] - y_pred[:, 0]\n    fvc_pred = y_pred[:, 1]\n    \n    #sigma_clip = sigma + C1\n    sigma_clip = tf.maximum(sigma, C1)\n    delta = tf.abs(y_true[:, 0] - fvc_pred)\n    delta = tf.minimum(delta, C2)\n    sq2 = tf.sqrt( tf.dtypes.cast(2, dtype=tf.float32) )\n    metric = (delta / sigma_clip)*sq2 + tf.math.log(sigma_clip* sq2)\n    return K.mean(metric)\n\ndef qloss(y_true, y_pred):\n    # Pinball loss for multiple quantiles\n    qs = [0.2, 0.50, 0.8]\n    q = tf.constant(np.array([qs]), dtype=tf.float32)\n    e = y_true - y_pred\n    v = tf.maximum(q*e, (q-1)*e)\n    return K.mean(v)\n\ndef mloss(_lambda):\n    def loss(y_true, y_pred):\n        return _lambda * qloss(y_true, y_pred) + (1 - _lambda)*score(y_true, y_pred)\n    return loss\n\ndef make_model(nh):\n    z = L.Input((nh,), name=\"Patient\")\n    x = L.Dense(100, activation=\"relu\", name=\"d1\")(z)\n    x = L.Dense(100, activation=\"relu\", name=\"d2\")(x)\n    p1 = L.Dense(3, activation=\"linear\", name=\"p1\")(x)\n    p2 = L.Dense(3, activation=\"relu\", name=\"p2\")(x)\n    preds = L.Lambda(lambda x: x[0] + tf.cumsum(x[1], axis=1), \n                     name=\"preds\")([p1, p2])\n    \n    model = M.Model(z, preds, name=\"CNN\")\n    model.compile(loss=mloss(0.65), optimizer=tf.keras.optimizers.Adam(lr=0.1, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.01, amsgrad=False), metrics=[score])\n    return model\ny = tr['FVC'].values\nz = tr[FE].values\nze = sub[FE].values\nnh = z.shape[1]\npe = np.zeros((ze.shape[0], 3))\npred = np.zeros((z.shape[0], 3))\nnet = make_model(nh)\nprint(net.summary())\nprint(net.count_params())\nNFOLD = 5 # originally 5\nkf = KFold(n_splits=NFOLD)\n%%time\ncnt = 0\nEPOCHS = 800\nfor tr_idx, val_idx in kf.split(z):\n    cnt += 1\n    print(f\"FOLD {cnt}\")\n    net = make_model(nh)\n    net.fit(z[tr_idx], y[tr_idx], batch_size=BATCH_SIZE, epochs=EPOCHS, \n            validation_data=(z[val_idx], y[val_idx]), verbose=0) #\n    print(\"train\", net.evaluate(z[tr_idx], y[tr_idx], verbose=0, batch_size=BATCH_SIZE))\n    print(\"val\", net.evaluate(z[val_idx], y[val_idx], verbose=0, batch_size=BATCH_SIZE))\n    print(\"predict val...\")\n    pred[val_idx] = net.predict(z[val_idx], batch_size=BATCH_SIZE, verbose=0)\n    print(\"predict test...\")\n    pe += net.predict(ze, batch_size=BATCH_SIZE, verbose=0) / NFOLD\nsigma_opt = mean_absolute_error(y, pred[:, 1])\nunc = pred[:,2] - pred[:, 0]\nsigma_mean = np.mean(unc)\nprint(sigma_opt, sigma_mean)\nidxs = np.random.randint(0, y.shape[0], 100)\nplt.plot(y[idxs], label=\"ground truth\")\nplt.plot(pred[idxs, 0], label=\"q25\")\nplt.plot(pred[idxs, 1], label=\"q50\")\nplt.plot(pred[idxs, 2], label=\"q75\")\nplt.legend(loc=\"best\")\nplt.show()\nprint(unc.min(), unc.mean(), unc.max(), (unc>=0).mean())\nplt.hist(unc)\nplt.title(\"uncertainty in prediction\")\nplt.show()\nsub.head()\n# PREDICTION\nsub['FVC1'] = 1.*pe[:, 1]\nsub['Confidence1'] = pe[:, 2] - pe[:, 0]\nsubm = sub[['Patient_Week','FVC','Confidence','FVC1','Confidence1']].copy()\nsubm.loc[~subm.FVC1.isnull()].head(10)\nsubm.loc[~subm.FVC1.isnull(),'FVC'] = subm.loc[~subm.FVC1.isnull(),'FVC1']\nif sigma_mean<70:\n    subm['Confidence'] = sigma_opt\nelse:\n    subm.loc[~subm.FVC1.isnull(),'Confidence'] = subm.loc[~subm.FVC1.isnull(),'Confidence1']\nsubm.head()\nsubm.describe().T\notest = pd.read_csv('../input/osic-pulmonary-fibrosis-progression/test.csv')\nfor i in range(len(otest)):\n    subm.loc[subm['Patient_Week']==otest.Patient[i]+'_'+str(otest.Weeks[i]), 'FVC'] = otest.FVC[i]\n    subm.loc[subm['Patient_Week']==otest.Patient[i]+'_'+str(otest.Weeks[i]), 'Confidence'] = 0.1\nsubm[[\"Patient_Week\",\"FVC\",\"Confidence\"]].to_csv(\"submission_regression.csv\", index=False)\nreg_sub = subm[[\"Patient_Week\",\"FVC\",\"Confidence\"]].copy()", "processed": ["4 3 chang mloss class anchor id 4 3 back tabl content 0 1"]}, {"markdown": ["# Public LB Scores of Top Teams over time"], "code": "# Reference: https://www.kaggle.com/code/robikscube/the-race-for-lanl\n\n# Interative Plotly\ninit_notebook_mode(connected=True)\nTOP_TEAMS = df.min().loc[df.min() < 1.29].index.values\ndf_filtered = df[TOP_TEAMS].ffill()\ndf_filtered = df_filtered.loc[df_filtered.index > '2019-04-21']\n# Create a trace\ndata = []\nfor col in df_filtered.columns:\n    data.append(go.Scatter(\n                        x = df_filtered.index,\n                        y = df_filtered[col],\n                        name=col)\n               )\n    \niplot(data)", "processed": ["public lb score top team time"]}, {"markdown": ["# All competitors LB Position over Time"], "code": "# Reference: https://www.kaggle.com/code/robikscube/the-race-for-lanl\n\n# Scores of top teams over time\nALL_TEAMS = df.columns.values\ndf[ALL_TEAMS].ffill().plot(figsize=(20, 10),\n                           ylim=(1.0, 1.8),\n                           color=color_pal[0],\n                           legend=False,\n                           alpha=0.01,\n                           title='All LANL Teams Scores over Time')\nplt.show()", "processed": ["competitor lb posit time"]}, {"markdown": ["# Top LB Scores"], "code": "# Reference: https://www.kaggle.com/code/robikscube/the-race-for-lanl\n\n# Create Top Teams List\nTOP_TEAMS = df.min().loc[df.min() < 1.35].index.values\ndf[TOP_TEAMS].min().sort_values().plot(kind='barh',\n                                       xlim=(1.0, 1.36),\n                                       title='Teams with Scores less than 1.35',\n                                       figsize=(12, 15),\n                                       color=color_pal[3])\nplt.show()", "processed": ["top lb score"]}, {"markdown": ["# Count of LB Submissions that improved score\n## \"Slow and Steady wins the race\" ~or~ \"Keep overfitting until LB improves\"?\nThis is the count of times the person submitted and got the fun \"You're score improved\" notification. This is not the total submission count."], "code": "# Reference: https://www.kaggle.com/code/robikscube/the-race-for-lanl\n\ndf[TOP_TEAMS].nunique().sort_values().plot(kind='barh',\n                                           figsize=(12, 15),\n                                           color=color_pal[1],\n                                           title='Count of Submissions improving LB score by Team')\nplt.show()", "processed": ["count lb submiss improv score slow steadi win race keep overfit lb improv count time person submit got fun score improv notif total submiss count"]}, {"markdown": ["As we could see, orders.csv has all the information about the given order id like the user who has purchased the order, when was it purchased, days since prior order and so on.\n\nThe columns present in order_products_train and order_products_prior are same. Then what is the difference between these files.?\n\nAs mentioned earlier, in this dataset, 4 to 100 orders of a customer are given (we will look at this later) and we need to predict the products that will be re-ordered. So the last order of the user has been taken out and divided into train and test sets. All the prior order informations of the customer are present in order_products_prior file.  We can also note that there is a column in orders.csv file called eval_set which tells us as to which of the three datasets (prior, train or test) the given row goes to.\n\nOrder_products*csv file has more detailed information about the products that been bought in the given order along with the re-ordered status.\n\nLet us first get the count of rows in each of the three sets."], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-notebook-instacart\n\ncnt_srs = orders_df.eval_set.value_counts()\n\nplt.figure(figsize=(12,8))\nsns.barplot(cnt_srs.index, cnt_srs.values, alpha=0.8, color=color[1])\nplt.ylabel('Number of Occurrences', fontsize=12)\nplt.xlabel('Eval set type', fontsize=12)\nplt.title('Count of rows in each dataset', fontsize=15)\nplt.xticks(rotation='vertical')\nplt.show()\ndef get_unique_count(x):\n    return len(np.unique(x))\n\ncnt_srs = orders_df.groupby(\"eval_set\")[\"user_id\"].aggregate(get_unique_count)\ncnt_srs", "processed": ["could see order csv inform given order id like user purchas order purchas day sinc prior order column present order product train order product prior differ file mention earlier dataset 4 100 order custom given look later need predict product order last order user taken divid train test set prior order inform custom present order product prior file also note column order csv file call eval set tell u three dataset prior train test given row goe order product csv file detail inform product bought given order along order statu let u first get count row three set"]}, {"markdown": ["So there are 206,209 customers in total. Out of which, the last purchase of 131,209 customers are given as train set and we need to predict for the rest 75,000 customers. \n\nNow let us validate the claim that 4 to 100 orders of a customer are given. "], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-notebook-instacart\n\ncnt_srs = orders_df.groupby(\"user_id\")[\"order_number\"].aggregate(np.max).reset_index()\ncnt_srs = cnt_srs.order_number.value_counts()\n\nplt.figure(figsize=(12,8))\nsns.barplot(cnt_srs.index, cnt_srs.values, alpha=0.8, color=color[2])\nplt.ylabel('Number of Occurrences', fontsize=12)\nplt.xlabel('Maximum order number', fontsize=12)\nplt.xticks(rotation='vertical')\nplt.show()", "processed": ["206 209 custom total last purchas 131 209 custom given train set need predict rest 75 000 custom let u valid claim 4 100 order custom given"]}, {"markdown": ["So there are no orders less than 4 and is max capped at 100 as given in the data page. \n\nNow let us see how the ordering habit changes with day of week."], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-notebook-instacart\n\nplt.figure(figsize=(12,8))\nsns.countplot(x=\"order_dow\", data=orders_df, color=color[0])\nplt.ylabel('Count', fontsize=12)\nplt.xlabel('Day of week', fontsize=12)\nplt.xticks(rotation='vertical')\nplt.title(\"Frequency of order by week day\", fontsize=15)\nplt.show()", "processed": ["order le 4 max cap 100 given data page let u see order habit chang day week"]}, {"markdown": ["Seems like 0 and 1 is Saturday and Sunday when the orders are high and low during Wednesday.\n\nNow we shall see how the distribution is with respect to time of the day."], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-notebook-instacart\n\nplt.figure(figsize=(12,8))\nsns.countplot(x=\"order_hour_of_day\", data=orders_df, color=color[1])\nplt.ylabel('Count', fontsize=12)\nplt.xlabel('Hour of day', fontsize=12)\nplt.xticks(rotation='vertical')\nplt.title(\"Frequency of order by hour of day\", fontsize=15)\nplt.show()", "processed": ["seem like 0 1 saturday sunday order high low wednesday shall see distribut respect time day"]}, {"markdown": ["Seems Satuday evenings and Sunday mornings are the prime time for orders.\n\nNow let us check the time interval between the orders."], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-notebook-instacart\n\nplt.figure(figsize=(12,8))\nsns.countplot(x=\"days_since_prior_order\", data=orders_df, color=color[3])\nplt.ylabel('Count', fontsize=12)\nplt.xlabel('Days since prior order', fontsize=12)\nplt.xticks(rotation='vertical')\nplt.title(\"Frequency distribution by days since prior order\", fontsize=15)\nplt.show()", "processed": ["seem satuday even sunday morn prime time order let u check time interv order"]}, {"markdown": ["About 12% of the orders in prior set has no re-ordered items while in the train set it is 6.5%.\n\nNow let us see the number of products bought in each order."], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-notebook-instacart\n\ngrouped_df = order_products_train_df.groupby(\"order_id\")[\"add_to_cart_order\"].aggregate(\"max\").reset_index()\ncnt_srs = grouped_df.add_to_cart_order.value_counts()\n\nplt.figure(figsize=(12,8))\nsns.barplot(cnt_srs.index, cnt_srs.values, alpha=0.8)\nplt.ylabel('Number of Occurrences', fontsize=12)\nplt.xlabel('Number of products in the given order', fontsize=12)\nplt.xticks(rotation='vertical')\nplt.show()", "processed": ["12 order prior set order item train set 6 5 let u see number product bought order"]}, {"markdown": ["Wow. Most of them are organic products.! Also majority of them are fruits. \n\nNow let us look at the important aisles."], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-notebook-instacart\n\ncnt_srs = order_products_prior_df['aisle'].value_counts().head(20)\nplt.figure(figsize=(12,8))\nsns.barplot(cnt_srs.index, cnt_srs.values, alpha=0.8, color=color[5])\nplt.ylabel('Number of Occurrences', fontsize=12)\nplt.xlabel('Aisle', fontsize=12)\nplt.xticks(rotation='vertical')\nplt.show()", "processed": ["wow organ product also major fruit let u look import aisl"]}, {"markdown": ["Produce is the largest department. Now let us check the reordered percentage of each department. \n\n**Department wise reorder ratio:**"], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-notebook-instacart\n\ngrouped_df = order_products_prior_df.groupby([\"department\"])[\"reordered\"].aggregate(\"mean\").reset_index()\n\nplt.figure(figsize=(12,8))\nsns.pointplot(grouped_df['department'].values, grouped_df['reordered'].values, alpha=0.8, color=color[2])\nplt.ylabel('Reorder ratio', fontsize=12)\nplt.xlabel('Department', fontsize=12)\nplt.title(\"Department wise reorder ratio\", fontsize=15)\nplt.xticks(rotation='vertical')\nplt.show()", "processed": ["produc largest depart let u check reorder percentag depart depart wise reorder ratio"]}, {"markdown": ["**Add to Cart - Reorder ratio:**\n\nLet us now explore the relationship between how order of adding the product to the cart affects the reorder ratio."], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-notebook-instacart\n\norder_products_prior_df[\"add_to_cart_order_mod\"] = order_products_prior_df[\"add_to_cart_order\"].copy()\norder_products_prior_df[\"add_to_cart_order_mod\"].ix[order_products_prior_df[\"add_to_cart_order_mod\"]>70] = 70\ngrouped_df = order_products_prior_df.groupby([\"add_to_cart_order_mod\"])[\"reordered\"].aggregate(\"mean\").reset_index()\n\nplt.figure(figsize=(12,8))\nsns.pointplot(grouped_df['add_to_cart_order_mod'].values, grouped_df['reordered'].values, alpha=0.8, color=color[2])\nplt.ylabel('Reorder ratio', fontsize=12)\nplt.xlabel('Add to cart order', fontsize=12)\nplt.title(\"Add to cart order - Reorder ratio\", fontsize=15)\nplt.xticks(rotation='vertical')\nplt.show()", "processed": ["add cart reorder ratio let u explor relationship order ad product cart affect reorder ratio"]}, {"markdown": ["**Looks like the products that are added to the cart initially are more likely to be reordered again compared to the ones added later.** This makes sense to me as well since we tend to first order all the products we used to buy frequently and then look out for the new products available. \n\n**Reorder ratio by Time based variables:**"], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-notebook-instacart\n\norder_products_train_df = pd.merge(order_products_train_df, orders_df, on='order_id', how='left')\ngrouped_df = order_products_train_df.groupby([\"order_dow\"])[\"reordered\"].aggregate(\"mean\").reset_index()\n\nplt.figure(figsize=(12,8))\nsns.barplot(grouped_df['order_dow'].values, grouped_df['reordered'].values, alpha=0.8, color=color[3])\nplt.ylabel('Reorder ratio', fontsize=12)\nplt.xlabel('Day of week', fontsize=12)\nplt.title(\"Reorder ratio across day of week\", fontsize=15)\nplt.xticks(rotation='vertical')\nplt.ylim(0.5, 0.7)\nplt.show()\ngrouped_df = order_products_train_df.groupby([\"order_hour_of_day\"])[\"reordered\"].aggregate(\"mean\").reset_index()\n\nplt.figure(figsize=(12,8))\nsns.barplot(grouped_df['order_hour_of_day'].values, grouped_df['reordered'].values, alpha=0.8, color=color[4])\nplt.ylabel('Reorder ratio', fontsize=12)\nplt.xlabel('Hour of day', fontsize=12)\nplt.title(\"Reorder ratio across hour of day\", fontsize=15)\nplt.xticks(rotation='vertical')\nplt.ylim(0.5, 0.7)\nplt.show()\n\ngrouped_df = order_products_train_df.groupby([\"order_dow\", \"order_hour_of_day\"])[\"reordered\"].aggregate(\"mean\").reset_index()\ngrouped_df = grouped_df.pivot('order_dow', 'order_hour_of_day', 'reordered')\n\nplt.figure(figsize=(12,6))\nsns.heatmap(grouped_df)\nplt.title(\"Reorder ratio of Day of week Vs Hour of day\")\nplt.show()", "processed": ["look like product ad cart initi like reorder compar one ad later make sen well sinc tend first order product use buy frequent look new product avail reorder ratio time base variabl"]}, {"markdown": ["### Data augument with albumentations"], "code": "# Reference: https://www.kaggle.com/code/mobassir/mxnet-gluon-for-cloud-classification\n\nfrom albumentations import (\n    Compose, HorizontalFlip, ShiftScaleRotate, PadIfNeeded, RandomCrop,\n    RGBShift, RandomBrightness, RandomContrast, VerticalFlip\n)\ntrain_augmentator = Compose([\n        HorizontalFlip(p=0.5),\n        VerticalFlip(p=0.5),\n        ShiftScaleRotate(shift_limit=0.03, scale_limit=0,\n                         rotate_limit=(-3, 3), border_mode=0, p=0.75),\n        RandomBrightness(limit=(-0.25, 0.25), p=0.75),\n        RandomContrast(limit=(-0.15, 0.4), p=0.75),\n        RGBShift(r_shift_limit=10, g_shift_limit=10, b_shift_limit=10, p=0.75)\n    ], p=1.0)\nh = 400\nw = 400\nimport mxnet as mx\nfrom mxnet.gluon import data, HybridBlock, nn\nimport pandas as pd\nimport cv2\nimport os\nimport numpy as np\nfrom mxnet.gluon.data.vision import transforms\nfrom mxnet.gluon.model_zoo import vision\nfrom mxnet.lr_scheduler import CosineScheduler\nfrom mxnet.gluon import loss, Trainer\nfrom mxnet import autograd\nimport random\nfrom PIL import Image, ImageOps, ImageFilter\nfrom mxnet import nd as F, lr_scheduler as lrs\nfrom mxnet.gluon.contrib.estimator import Estimator\nimport gluoncv.model_zoo  as gm\n\ndef scale_func(image_shape):\n    return random.uniform(0.5, 1.2)\n\n\nclass cloudDataset(data.Dataset):\n    def __init__(self, df, img_dir, debug=False):\n        \n        self.train_df = df\n        self.root_dir = img_dir\n        self.transform = transforms.Compose(\n            [\n                transforms.ToTensor(),\n                transforms.Normalize(\n                    mean=(0.485, 0.456, 0.406),\n                   std=(0.229, 0.224, 0.225)\n                )\n            ]\n        )\n        \n        self.debug = debug\n        \n    def __getitem__(self, i):\n        if self.debug:\n            curr_df = self.train_df.head(20)\n        masks = np.zeros((h, w), np.uint8)\n        img_names = []\n        item = self.train_df.iloc[i, :]\n        img_name = item['ImageId']\n        defect_label = np.zeros((1, 4), dtype=np.float32)\n        for j in range(4):\n            curr_item = item[\"e{}\".format(j+1)]\n            if len(curr_item) > 0:\n\n                rle_pixels = curr_item\n                label = rle_pixels.split(\" \")\n                positions = list(map(int, label[0::2]))\n                length = list(map(int, label[1::2]))\n                mask = np.zeros(h * w, dtype=np.uint8)\n                for pos, le in zip(positions, length):\n                    mask[pos - 1:(pos + le - 1)] = j+1\n                count = np.sum(np.where(mask==(j+1), 1, 0))\n                if count < 8:\n                    mask = np.where(mask==(j+1), -1, 0)\n                defect_label[:, j] = 1\n                masks[ :, :] = masks[ :, :] + mask.reshape(h, w, order='F')\n                \n        oimg = cv2.imread(os.path.join(self.root_dir, img_name))[:, :, ::-1]\n        # oimg, masks = self.rescale_sample(oimg, masks)\n        aug_out = train_augmentator(image=oimg, mask=masks)\n        oimg = aug_out['image']\n        masks = aug_out['mask']\n      \n        img = F.array(oimg)\n        img = self.transform(img)\n        \n        if self.debug:\n            return img, F.array(masks[::4, ::4]), oimg, masks, curr_df\n        else:\n            return img, F.array(masks), F.array(defect_label)\n        \n    def __len__(self):\n        return len(self.train_df)\n\n\n    def rescale_sample(self, image, mask):\n\n        scale = scale_func(image.shape)\n        image = cv2.resize(image, (0, 0), fx=scale, fy=scale)\n        new_size = (image.shape[1], image.shape[0])\n\n        mask = cv2.resize(mask, new_size, interpolation=cv2.INTER_NEAREST)\n\n        return image, mask\n\n# for test\nimport matplotlib.pyplot as plt\ncsv_file = '../input/understanding_cloud_organization/train.csv'\nimg_dir = '../input/understanding_cloud_organization/train_images/'\ncloud_dataset = cloudDataset(train2, img_dir, debug=True)\nprint(len(cloud_dataset))\n_, mm, im, mask, curr_df = cloud_dataset[16]\nplt.figure(figsize=(20, 20))\nplt.subplot(2, 1, 1)\nplt.imshow(im)\nplt.subplot(2, 1, 2)\nplt.imshow(mask[::4, ::4])\nmm.flatten().shape\n", "processed": ["data augument albument"]}, {"markdown": ["## Flattening the features and doing basic EDA with seaborn", "Now, we flatten the 2D tensors associated with each segment into 1D arrays. Now, each data point (segment) is represented by a 1D array.\n\nHere are some flattened 1D arrays (with sparse selection) visualized with **matplotlib**."], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/lanl-earthquake-prediction-new-features\n\nshape = X.shape\nnew_signals = X.reshape((shape[0], shape[1]*shape[2]))\n\nsparse_signals = []\nfor i in range(3):\n    sparse_signal = []\n    for j in range(len(new_signals[i])):\n        if j % 3 == 0:\n            sparse_signal.append(new_signals[i][j])\n    sparse_signals.append(sparse_signal)\n\nplt.plot(sparse_signals[0], 'mediumseagreen')\nplt.show()\nplt.plot(sparse_signals[1], 'seagreen')\nplt.show()\nplt.plot(sparse_signals[2], 'green')\nplt.show()", "processed": ["flatten featur basic eda seaborn", "flatten 2d tensor associ segment 1d array data point segment repres 1d array flatten 1d array spar select visual matplotlib"]}, {"markdown": ["#### Bivariate KDE distribution plot"], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/lanl-earthquake-prediction-new-features\n\nplot = sns.jointplot(x=spectral_entropies, y=targets, kind='kde', color='blueviolet')\nplot.set_axis_labels('spectral_entropy', 'time_to_failure', fontsize=16)\nplt.show()", "processed": ["bivari kde distribut plot"]}, {"markdown": ["The KDE plot has highest density (darkness) along a line with negative slope.", "#### Bivariate hexplot"], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/lanl-earthquake-prediction-new-features\n\nplot = sns.jointplot(x=spectral_entropies, y=targets, kind='hex', color='blueviolet')\nplot.set_axis_labels('spectral_entropy', 'time_to_failure', fontsize=16)\nplt.show()", "processed": ["kde plot highest densiti dark along line neg slope", "bivari hexplot"]}, {"markdown": ["The hexplot is also darkest around a negatively-sloped line.", "#### Scatterplot with line of best fit"], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/lanl-earthquake-prediction-new-features\n\nplot = sns.jointplot(x=spectral_entropies, y=targets, kind='reg', color='blueviolet')\nplot.set_axis_labels('spectral_entropy', 'time_to_failure', fontsize=16)\nplt.show()", "processed": ["hexplot also darkest around neg slope line", "scatterplot line best fit"]}, {"markdown": ["#### Bivariate KDE distribution plot"], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/lanl-earthquake-prediction-new-features\n\nplot = sns.jointplot(x=sample_entropies, y=targets, kind='kde', color='mediumvioletred')\nplot.set_axis_labels('sample_entropy', 'time_to_failure', fontsize=16)\nplt.show()", "processed": ["bivari kde distribut plot"]}, {"markdown": ["The KDE plot has highest density (darkness) around a line with negative slope.", "#### Bivariate hexplot"], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/lanl-earthquake-prediction-new-features\n\nplot = sns.jointplot(x=sample_entropies, y=targets, kind='hex', color='mediumvioletred')\nplot.set_axis_labels('sample_entropy', 'time_to_failure', fontsize=16)\nplt.show()", "processed": ["kde plot highest densiti dark around line neg slope", "bivari hexplot"]}, {"markdown": ["The hexplot is also darkest around a negatively-sloped line.", "#### Scatterplot with line of best fit"], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/lanl-earthquake-prediction-new-features\n\nplot = sns.jointplot(x=sample_entropies, y=targets, kind='reg', color='mediumvioletred')\nplot.set_axis_labels('sample_entropy', 'time_to_failure', fontsize=16)\nplt.show()", "processed": ["hexplot also darkest around neg slope line", "scatterplot line best fit"]}, {"markdown": ["#### Bivariate KDE distribution plot"], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/lanl-earthquake-prediction-new-features\n\nplot = sns.jointplot(x=detrended_fluctuations, y=targets, kind='kde', color='mediumblue')\nplot.set_axis_labels('detrended_fluctuation', 'time_to_failure', fontsize=16)\nplt.show()", "processed": ["bivari kde distribut plot"]}, {"markdown": ["The KDE plot has highest density (darkness) around a line with negative slope.", "#### Bivariate hexplot"], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/lanl-earthquake-prediction-new-features\n\nplot = sns.jointplot(x=detrended_fluctuations, y=targets, kind='hex', color='mediumblue')\nplot.set_axis_labels('detrended_fluctuation', 'time_to_failure', fontsize=16)\nplt.show()", "processed": ["kde plot highest densiti dark around line neg slope", "bivari hexplot"]}, {"markdown": ["The hexplot is also darkest around a negatively-sloped line.", "#### Scatterplot with line of best fit"], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/lanl-earthquake-prediction-new-features\n\nplot = sns.jointplot(x=detrended_fluctuations, y=targets, kind='reg', color='mediumblue')\nplot.set_axis_labels('detrended_fluctuation', 'time_to_failure', fontsize=16)\nplt.show()", "processed": ["hexplot also darkest around neg slope line", "scatterplot line best fit"]}, {"markdown": ["Ok, most of the sentences in the train data are 8 to 18 tokens long. Let's have a look at the distribution:"], "code": "# Reference: https://www.kaggle.com/code/allunia/eda-en-text-normalization\n\nfig, ax = plt.subplots(1,1, figsize=(20,10))\nsns.set_style(\"whitegrid\")\ncount_length_fig = sns.countplot(train_sentences, ax=ax)\nfor item in count_length_fig.get_xticklabels():\n    item.set_rotation(90)", "processed": ["ok sentenc train data 8 18 token long let look distribut"]}, {"markdown": ["Amazing! There is a peak for sentences that are 7 tokens long. I will go back to them later on! ", "### And the test set?"], "code": "# Reference: https://www.kaggle.com/code/allunia/eda-en-text-normalization\n\ntest_sentences = test.groupby(\"sentence_id\")[\"sentence_id\"].count()\ntest_sentences.describe()\nfig, ax = plt.subplots(1,1, figsize=(20,10))\nsns.set_style(\"whitegrid\")\ncount_length_fig = sns.countplot(test_sentences, ax=ax)\nfor item in count_length_fig.get_xticklabels():\n    item.set_rotation(90)", "processed": ["amaz peak sentenc 7 token long go back later", "test set"]}, {"markdown": ["Ok, reading the explanations for the data in detail: Each token within a sentence has a token_id. Consequently the longest sentence has token ids ranging from 0 to 255 (inclusive), and one of the smallest from 0 to 1. ", "## Token classes", "#### How many token classes do we have? And how many counts per class?"], "code": "# Reference: https://www.kaggle.com/code/allunia/eda-en-text-normalization\n\nlen(train[\"class\"].unique())\nfig, ax = plt.subplots(1,1, figsize=(10,12))\n#sns.set_style(\"whitegrid\")\ncount_classes_fig = sns.countplot(y=\"class\", data=train, ax=ax)\nfor item in count_classes_fig.get_xticklabels():\n    item.set_rotation(45)\ntrain.groupby(\"class\")[\"class\"].count()", "processed": ["ok read explan data detail token within sentenc token id consequ longest sentenc token id rang 0 255 inclus one smallest 0 1", "token class", "mani token class mani count per class"]}, {"markdown": ["Ok, most are plain. But there are also some exotic classes.\n\n### What is meant by electronic or verbatim?", "#### ELECTRONIC class examples"], "code": "# Reference: https://www.kaggle.com/code/allunia/eda-en-text-normalization\n\nmost_electronic_cases = train[train[\"class\"]=='ELECTRONIC'].groupby(\"before\")[\"before\"].count(\n).sort_values(ascending=False).head(10)\nfig, ax = plt.subplots(1,1,figsize=(15,5))\nsns.barplot(x=most_electronic_cases.index, y=most_electronic_cases.values)", "processed": ["ok plain also exot class meant electron verbatim", "electron class exampl"]}, {"markdown": ["#### VERBATIM class examples"], "code": "# Reference: https://www.kaggle.com/code/allunia/eda-en-text-normalization\n\nmost_verbatim_cases = train[train[\"class\"]=='VERBATIM'].groupby(\"before\")[\"before\"].count(\n).sort_values(ascending=False).head(15)\nfig, ax = plt.subplots(1,1,figsize=(15,5))\nsns.barplot(x=most_verbatim_cases.index, y=most_verbatim_cases.values)", "processed": ["verbatim class exampl"]}, {"markdown": ["## Before words", "### How many unique before words do we have and how do the most common look like?"], "code": "# Reference: https://www.kaggle.com/code/allunia/eda-en-text-normalization\n\nlen(train.before.unique())\ntrain_word_counts = train.groupby(\"before\")[\"before\"].count().sort_values(ascending=False).head(15)\nfig, ax = plt.subplots(1,1,figsize=(15,5))\nsns.barplot(x=train_word_counts.index, y=train_word_counts.values)\nlen(test.before.unique())\ntest_word_counts = test.groupby(\"before\")[\"before\"].count().sort_values(ascending=False).head(15)\nfig, ax = plt.subplots(1,1,figsize=(15,5))\nsns.barplot(x=test_word_counts.index, y=test_word_counts.values)", "processed": ["word", "mani uniqu word common look like"]}, {"markdown": ["### Which words changed most often?"], "code": "# Reference: https://www.kaggle.com/code/allunia/eda-en-text-normalization\n\nmost_changed_words = train[train.change==1].groupby(\"before\")[\"before\"].count(\n).sort_values(ascending=False).head(20)\nfig, ax = plt.subplots(1,1,figsize=(15,5))\nsns.barplot(x=most_changed_words.index, y=most_changed_words.values)", "processed": ["word chang often"]}, {"markdown": ["### To which class do the changed words belong most often?"], "code": "# Reference: https://www.kaggle.com/code/allunia/eda-en-text-normalization\n\nfig, ax = plt.subplots(1,1,figsize=(15,5))\nchanges_classes_fig = sns.countplot(x=\"class\", data=train[train.change==1])\nfor item in changes_classes_fig.get_xticklabels():\n    item.set_rotation(45)", "processed": ["class chang word belong often"]}, {"markdown": ["### Do longer sentences have more changes than short ones?"], "code": "# Reference: https://www.kaggle.com/code/allunia/eda-en-text-normalization\n\nplt.figure(figsize=(15,5))\n#sns.jointplot(x=\"length\", y=\"num_changes\", data=train_sentences_info, kind=\"kde\")\nsns.jointplot(x=\"length\", y=\"num_changes\", data=train_sentences_info)", "processed": ["longer sentenc chang short one"]}, {"markdown": ["For those that changed more than 1, I would say yes. Let's have a closer look for them:"], "code": "# Reference: https://www.kaggle.com/code/allunia/eda-en-text-normalization\n\nplt.figure(figsize=(15,5))\nsns.jointplot(x=\"length\", y=\"num_changes\", data=train_sentences_info[train_sentences_info.num_changes > 1])", "processed": ["chang 1 would say ye let closer look"]}, {"markdown": ["We can find a weak correlation between sentence lengths and number of changes. But this looks still like a diffuse cloud. We can't see a nice pattern.", "### Which position in a sentence changed most often?\n\nThe token id tells us the position of a token in the sentence. I would guess that there are positions in a *specific kind of sentence* (whatever that means... must be specified) whose tokens changes more often. This would also relate to the language grammar structure. \n\n"], "code": "# Reference: https://www.kaggle.com/code/allunia/eda-en-text-normalization\n\nplt.figure(figsize=(10,6))\nsns.countplot(x=\"token_id\", data=train[(train.change==1) & (train.token_id <=30)])\nplt.xlabel(\"Token ID\")\nplt.ylabel(\"Number of changes\")", "processed": ["find weak correl sentenc length number chang look still like diffus cloud see nice pattern", "posit sentenc chang often token id tell u posit token sentenc would guess posit specif kind sentenc whatev mean must specifi whose token chang often would also relat languag grammar structur"]}, {"markdown": ["*The code is hidden for saving some space*"], "code": "# Reference: https://www.kaggle.com/code/pestipeti/visualize-your-model-s-augmented-predictions\n\nclass PredictionVisualizer:\n    \n    DARK_BLUE  = (0.1294, 0.5882, 0.9529) # True label (missclassified)\n    GREEN      = (0.2980, 0.6863, 0.3137) # Predicted: correct\n    RED        = (0.9869, 0.2627, 0.2118) # Predicted: incorrect\n    LIGHT_BLUE = (0.7333, 0.8706, 0.9843) # other\n    \n    def __init__(self):\n        \"\"\"Prediction visualizer.\n        \n        Usage:\n            See the example below in this notebook.\n        \"\"\"\n        self.__model = None\n        self.__labels = {}  # type: Dict[str, LabelConfig]\n        self.__sample = None  # type: Sample\n        self.__transform_fn = None  # type: Callable\n        self.__transform_fn_steps = None  # type: List[int]\n        self.__transofrm_data = []  # type: List[dict]\n    \n    @property\n    def model(self):\n        return self.__model\n    \n    @model.setter\n    def model(self, model):\n        self.__model = model\n\n    @property\n    def transform_fn(self):\n        return self.__transform_fn\n\n    def set_transform_fn(self, transform_fn, steps):\n        self.__transform_fn = transform_fn\n        self.__transform_fn_steps = steps\n    \n    @property\n    def sample(self) -> Sample:\n        return self.__sample\n\n    @sample.setter\n    def sample(self, sample: Sample):\n        self.__sample = sample\n    \n    def add_label(self, label_config: LabelConfig) -> None:\n        self.__labels[label_config['label_id']] = label_config\n    \n    @property\n    def labels(self) -> Dict[str, LabelConfig]:\n        return self.__labels\n    \n    def create(self):\n        \"\"\"Creates the animation.\"\"\"\n        fig, gs, subplots = self.__generate_figure()\n        frames = self.__generate_frames()\n        outputs = []\n        \n        for step_index, image in enumerate(frames['images']):\n            output = [\n                subplots['image'].imshow(image.astype(np.uint8), animated=True, cmap='Greys_r')\n            ]\n            \n            for label_idx, (label_id, label) in enumerate(self.__labels.items()):\n                colors = self.__get_colors(label['num_classes'],\n                                           self.sample['labels'][label_id],\n                                           np.argmax(frames[label_id], axis=1)[step_index]\n                                          )\n                output.append(\n                    subplots[label_id].vlines(np.array([x for x in range(0, label['num_classes'])]),\n                                              np.zeros(len(frames)),\n                                              frames[label_id][step_index],\n                                              colors\n                                             )\n                )\n                \n\n            outputs.append(output)\n\n        return animation.ArtistAnimation(fig, outputs, interval=50, blit=True, repeat=True, repeat_delay=2000)    \n    \n    def _before_forward(self, transformed_image):\n        \"\"\"Before forward adapter\n        \n        The default implementation converts the numpy array to torch tensor and adds the missing\n        `channel` and `batch` dimensions. You should update this method if your model expects a\n        different input format.\n\n        Args:\n            transformed_image (numpy.ndarray): Transformed image (rotated, etc), shape: H x W\n\n        Return:\n            Prepared images (batch of image 1) for your model. The returned image's shape should\n            be the shape of your model's input (For example, Pytorch: B x C x H x W)\n        \"\"\"        \n        # Convert to float tensor\n        transformed_image = torch.from_numpy(transformed_image).float()\n        \n        # Add 'channel' dim\n        transformed_image = transformed_image.unsqueeze(0)\n        \n        # Add 'batch' dimension\n        transformed_image = transformed_image.unsqueeze(0)\n        \n        return transformed_image\n    \n    def _forward(self, input_image):\n        \"\"\"You can make the forward call in here\n\n        Args: \n            input_image (torch.Tensor | any) Prepared input for your model. Shape: B x C x H x W\n        \n        Return:\n            You should return a dictionary of your model's predictions (logits or softmax)\n            for every registered labels.\n            \n            ```\n            with torch.no_grad():\n                out_graph, out_vowel, out_conso = self.model(input_image)\n            \n            return {\n                'grapheme_root': out_graph,\n                'vowel_diacritic': out_vowel,\n                'consonant_diacritic': out_conso\n            }\n            ```\n\n            out_x.shape => B x label.NUM_CLASS\n        \"\"\"\n        raise NotImplementedError\n\n    def _softmax(self, outputs):\n        \"\"\"Applies a softmax function and returns the result.\n\n        If your model has a final softmax layer, then you should override this to return\n        the `outputs` argument without changes.\n        \n        The visualizer will call this method for every label separately.\n        \n        Args:\n            outputs (torch.Tensor | any): Your model's output, shape: BATCH x NUM_CLASSES\n\n        Return:\n            Softmaxed values\n        \"\"\"\n        return F.softmax(outputs, dim=1)\n\n    def _after_forward(self, probabilities):\n        \"\"\"Convert the result to the required format.\n        \n        Args:\n            probabilities (torch.Tensor | any) Your model's output after the `self._softmax` call.\n            \n        Return: (numpy.ndarray)\n        \"\"\"\n        return probabilities.data.cpu().numpy()[0]\n    \n    def __generate_figure(self):\n        \"\"\"Generates the plot.\"\"\"\n        fig = plt.figure(constrained_layout=True, figsize=(14, 6))\n        gs = fig.add_gridspec(len(self.labels), 2)\n        \n        subplots = {}\n        subplots['image'] = fig.add_subplot(gs[:, 0], xticks=[], yticks=[])\n        subplots['image'].set_title('Image id: {}'.format(self.sample['image_id']), fontsize=10)\n\n        for label_idx, (label_id, label) in enumerate(self.__labels.items()):\n            subplots[label_id] = fig.add_subplot(gs[label_idx, 1], xlim=(-1, label['num_classes']))\n            subplots[label_id].set_title('{} (label: {})'.format(label['label_name'], self.sample['labels'][label_id]), fontsize=10)\n    \n        return fig, gs, subplots\n    \n    def __generate_frames(self):\n        \"\"\"Generates the frames.\"\"\"\n        \n        assert self.model is not None\n        assert self.sample is not None\n        assert self.transform_fn is not None\n        \n        h, w = self.sample['image'].shape\n        steps = len(self.__transform_fn_steps)\n        \n        frames = {}\n        \n        # Placeholder for the transformed images\n        frames['images'] = np.zeros((steps, h, w))\n        \n        # Create placeholders for the labels\n        for label_idx, (label_id, label) in enumerate(self.__labels.items()):\n            frames[label_id] = np.zeros((steps, label['num_classes']))\n            \n        for step, transform_step_value in enumerate(self.__transform_fn_steps):\n            \n            # Transform the original image\n            transformed_image = self.__transform_fn(self.sample['image'], transform_step_value)\n            \n            # Save the transformed image as a new frame\n            frames['images'][step, ...] = transformed_image\n            \n            # Prepare the image for the model\n            input_image = self._before_forward(transformed_image.copy())\n            \n            # Predict\n            model_output = self._forward(input_image)\n            \n            # Add the results to the frames\n            for label_id, output_logits in model_output.items():\n                frames[label_id][step, ...] = self._after_forward(self._softmax(output_logits))\n                \n        return frames\n\n    def __get_colors(self, size, target, pred):\n        \"\"\"Generates the colors of the vlines.\"\"\"\n        gra_color = [self.LIGHT_BLUE for _ in range(size)]\n\n        if pred == target:\n            gra_color[pred] = self.GREEN\n        else:\n            gra_color[pred] = self.RED\n            gra_color[target] = self.DARK_BLUE\n\n        return gra_color    \n", "processed": ["code hidden save space"]}, {"markdown": ["My experience (for example [Titanic (0.83253) - Comparison 20 popular models](https://www.kaggle.com/vbmokin/titanic-0-83253-comparison-20-popular-models)) has shown that simulation using LGBMClassifier is better if you set both parameter num_leaves and parameter max_depth"], "code": "# Reference: https://www.kaggle.com/code/vbmokin/lgbm-multiple-classifier-with-max-depth-5\n\nparams = {'num_leaves': 35, 'max_depth': 5, # 2**5 = 32 - Let's set num_leaves=35\n 'subsample': 0.4, 'min_child_samples': 10,\n 'learning_rate': 0.01,\n 'num_iterations': 500, 'random_state': 12}\nfrom sklearn.model_selection import train_test_split, KFold\nlosses = []\nmodels = []\nfor k in range(1):\n    kfold = KFold(5, random_state = 42 + k, shuffle = True)\n    for k_fold, (tr_inds, val_inds) in enumerate(kfold.split(train_y)):\n        print(\"-----------\")\n        print(\"-----------\")\n        model = MultiLGBMClassifier(resolution = 5, params = params)\n        model.fit(train_x[tr_inds], train_y_raw.values[tr_inds])\n        preds = model.predict(train_x[val_inds])\n        loss = np.mean((train_y[val_inds] - preds) ** 2)\n        models.append(model)\n        print(k_fold, loss)\n        losses.append(loss)\nprint(\"-------\")\nprint(losses)\nprint(np.mean(losses))\nprint(losses)\nprint(np.mean(losses))\nfeature_importances = 0\nnum_model = 0\nfor model in models:\n    for m in model.models:\n        feature_importances += m.booster_.feature_importance(\"gain\")\n        num_model += 1\n\nfeature_importances /= num_model\nfeature_names = dense_player_feature_names + dense_game_feature_names + cat_game_feature_names + cat_player_feature_names\nfeature_importance_df = pd.DataFrame(np.vstack([feature_importances, feature_names]).T, columns = [\"importance\", \"name\"])\nfeature_importance_df[\"importance\"] = feature_importance_df[\"importance\"].astype(np.float32)\nfeature_importance_df = feature_importance_df.groupby(\"name\").agg(\"mean\").reset_index()\nplt.figure(figsize = (8, 18))\nsns.barplot(data = feature_importance_df.sort_values(by = \"importance\", ascending = False).head(50), x = \"importance\", y = \"name\")\nplt.show()\nplt.figure(figsize = (8, 18))\nsns.barplot(data = feature_importance_df.sort_values(by = \"importance\", ascending = False).tail(50), x = \"importance\", y = \"name\")\nplt.show()\n## bad features\nlist(feature_importance_df[feature_importance_df[\"importance\"] < np.quantile(feature_importance_df[\"importance\"], 0.3)][\"name\"])", "processed": ["experi exampl titan 0 83253 comparison 20 popular model http www kaggl com vbmokin titan 0 83253 comparison 20 popular model shown simul use lgbmclassifi better set paramet num leav paramet max depth"]}, {"markdown": ["## Summary statistics of the training set\n\nHere we can visualize some basic statistics in the data, like the distribution of entries for each author. For this purpose, I will invoke the handy Plot.ly visualisation library and plot some simple bar plots. Unhide the cell below if you want to see the Plot.ly code."], "code": "# Reference: https://www.kaggle.com/code/arthurtok/spooky-nlp-and-topic-modelling-tutorial\n\nz = {'EAP': 'Edgar Allen Poe', 'MWS': 'Mary Shelley', 'HPL': 'HP Lovecraft'}\ndata = [go.Bar(\n            x = train.author.map(z).unique(),\n            y = train.author.value_counts().values,\n            marker= dict(colorscale='Jet',\n                         color = train.author.value_counts().values\n                        ),\n            text='Text entries attributed to Author'\n    )]\n\nlayout = go.Layout(\n    title='Target variable distribution'\n)\n\nfig = go.Figure(data=data, layout=layout)\n\npy.iplot(fig, filename='basic-bar')\nall_words = train['text'].str.split(expand=True).unstack().value_counts()\ndata = [go.Bar(\n            x = all_words.index.values[2:50],\n            y = all_words.values[2:50],\n            marker= dict(colorscale='Jet',\n                         color = all_words.values[2:100]\n                        ),\n            text='Word counts'\n    )]\n\nlayout = go.Layout(\n    title='Top 50 (Uncleaned) Word frequencies in the training dataset'\n)\n\nfig = go.Figure(data=data, layout=layout)\n\npy.iplot(fig, filename='basic-bar')", "processed": ["summari statist train set visual basic statist data like distribut entri author purpos invok handi plot ly visualis librari plot simpl bar plot unhid cell want see plot ly code"]}, {"markdown": ["Finally plotting the word clouds via the following few lines (unhide to see the code):"], "code": "# Reference: https://www.kaggle.com/code/arthurtok/spooky-nlp-and-topic-modelling-tutorial\n\n# The wordcloud of Cthulhu/squidy thing for HP Lovecraft\nplt.figure(figsize=(16,13))\nwc = WordCloud(background_color=\"black\", max_words=10000, \n               mask=hcmask3, stopwords=STOPWORDS, max_font_size= 40)\nwc.generate(\" \".join(hpl))\nplt.title(\"HP Lovecraft (Cthulhu-Squidy)\", fontsize=20)\n# plt.imshow(wc.recolor( colormap= 'Pastel1_r' , random_state=17), alpha=0.98)\nplt.imshow(wc.recolor( colormap= 'Pastel2' , random_state=17), alpha=0.98)\nplt.axis('off')\nplt.figure(figsize=(20,18))\n# The wordcloud of the raven for Edgar Allen Poe\nplt.subplot(211)\nwc = WordCloud(background_color=\"black\", \n               max_words=10000, \n               mask=hcmask, \n               stopwords=STOPWORDS, \n               max_font_size= 40)\nwc.generate(\" \".join(eap))\nplt.title(\"Edgar Allen Poe (The Raven)\")\nplt.imshow(wc.recolor( colormap= 'PuBu' , random_state=17), alpha=0.9)\nplt.axis('off')\nplt.figure(figsize=(20,18))\nwc = WordCloud(background_color=\"black\", \n               max_words=10000, \n               mask=hcmask2, \n               stopwords=STOPWORDS, \n               max_font_size= 40)\nwc.generate(\" \".join(mws))\nplt.title(\"Mary Shelley (Frankenstein's Monster)\", fontsize= 18)\nplt.imshow(wc.recolor( colormap= 'viridis' , random_state=17), alpha=0.9)\nplt.axis('off')", "processed": ["final plot word cloud via follow line unhid see code"]}, {"markdown": ["**Revisiting our Term frequencies**\n\nHaving implemented our lemmatized count vectorizer, let us revist the plots for the term frquencies of the top 50 words (by frequency). As you can see from the plot, all our prior preprocessing efforts have not gone to waste. With the removal of stopwords, the remaining words seem much more meaningful where you can see that all the stopwords in the earlier term frequency plot "], "code": "# Reference: https://www.kaggle.com/code/arthurtok/spooky-nlp-and-topic-modelling-tutorial\n\nfeature_names = tf_vectorizer.get_feature_names()\ncount_vec = np.asarray(tf.sum(axis=0)).ravel()\nzipped = list(zip(feature_names, count_vec))\nx, y = (list(x) for x in zip(*sorted(zipped, key=lambda x: x[1], reverse=True)))\n# Now I want to extract out on the top 15 and bottom 15 words\nY = np.concatenate([y[0:15], y[-16:-1]])\nX = np.concatenate([x[0:15], x[-16:-1]])\n\n# Plotting the Plot.ly plot for the Top 50 word frequencies\ndata = [go.Bar(\n            x = x[0:50],\n            y = y[0:50],\n            marker= dict(colorscale='Jet',\n                         color = y[0:50]\n                        ),\n            text='Word counts'\n    )]\n\nlayout = go.Layout(\n    title='Top 50 Word frequencies after Preprocessing'\n)\n\nfig = go.Figure(data=data, layout=layout)\n\npy.iplot(fig, filename='basic-bar')\n\n# Plotting the Plot.ly plot for the Top 50 word frequencies\ndata = [go.Bar(\n            x = x[-100:],\n            y = y[-100:],\n            marker= dict(colorscale='Portland',\n                         color = y[-100:]\n                        ),\n            text='Word counts'\n    )]\n\nlayout = go.Layout(\n    title='Bottom 100 Word frequencies after Preprocessing'\n)\n\nfig = go.Figure(data=data, layout=layout)\n\npy.iplot(fig, filename='basic-bar')", "processed": ["revisit term frequenc implement lemmat count vector let u revist plot term frquenci top 50 word frequenc see plot prior preprocess effort gone wast remov stopword remain word seem much meaning see stopword earlier term frequenc plot"]}, {"markdown": ["# Train EDA\nLet's confirm our Data Generator works and view some training images. We will only show examples with defects. Note that all mask contours are plotted with a little blank space around the defect to aid visualization. Below we show examples of each type but note that in the training set only 7.1%, 2.0%, 41.0%, 6.4% of images have defects 1, 2, 3, 4 respectively."], "code": "# Reference: https://www.kaggle.com/code/cdeotte/keras-unet-with-eda\n\nplt.figure(figsize=(13.5,2.5))\nbar = plt.bar( [1,2,3,4],100*np.mean( train2.iloc[:,1:5]!='',axis=0) )\nplt.title('Percent Training Images with Defect', fontsize=16)\nplt.ylabel('Percent of Images'); plt.xlabel('Defect Type')\nplt.xticks([1,2,3,4])\nfor rect in bar:\n    height = rect.get_height()\n    plt.text(rect.get_x() + rect.get_width()/2.0, height, '%.1f %%' % height,\n             ha='center', va='bottom',fontsize=16)\nplt.ylim((0,50)); plt.show()\n# DEFECTIVE IMAGE SAMPLES\nfilenames = {}\ndefects = list(train2[train2['e1']!=''].sample(3).index)\ndefects += list(train2[train2['e2']!=''].sample(3).index)\ndefects += list(train2[train2['e3']!=''].sample(7).index)\ndefects += list(train2[train2['e4']!=''].sample(3).index)\n\n# DATA GENERATOR\ntrain_batches = DataGenerator(train2[train2.index.isin(defects)],shuffle=True,info=filenames)\nprint('Images and masks from our Data Generator')\nprint('KEY: yellow=defect1, green=defect2, blue=defect3, magenta=defect4')\n\n# DISPLAY IMAGES WITH DEFECTS\nfor i,batch in enumerate(train_batches):\n    plt.figure(figsize=(14,50)) #20,18\n    for k in range(16):\n        plt.subplot(16,1,k+1)\n        img = batch[0][k,]\n        img = Image.fromarray(img.astype('uint8'))\n        img = np.array(img)\n        extra = '  has defect'\n        for j in range(4):\n            msk = batch[1][k,:,:,j]\n            msk = mask2pad(msk,pad=3)\n            msk = mask2contour(msk,width=2)\n            if np.sum(msk)!=0: extra += ' '+str(j+1)\n            if j==0: # yellow\n                img[msk==1,0] = 235 \n                img[msk==1,1] = 235\n            elif j==1: img[msk==1,1] = 210 # green\n            elif j==2: img[msk==1,2] = 255 # blue\n            elif j==3: # magenta\n                img[msk==1,0] = 255\n                img[msk==1,2] = 255\n        plt.title(filenames[16*i+k]+extra)\n        plt.axis('off') \n        plt.imshow(img)\n    plt.subplots_adjust(wspace=0.05)\n    plt.show()", "processed": ["train eda let confirm data gener work view train imag show exampl defect note mask contour plot littl blank space around defect aid visual show exampl type note train set 7 1 2 0 41 0 6 4 imag defect 1 2 3 4 respect"]}, {"markdown": ["# Download UNET\nWe will download a pretrained Keras UNET from GitHub [here][1] with docs [here][2]. In this repository, there are many different architectures and pretrained backbones. Although the bottom is pretrained, the top of the model needs to be trained to our data. We will train for 90 minutes. To use this model in Kaggle's Steel competition, we'll need to save this trained model to a Kaggle dataset and load it into another kernel (called an inference kernel). The second kernel will make predictions on the test set. We can submit this second kernel because it will execute under an hour and have internet turned off thus satifying the comp rules.\n\n[1]: https://github.com/qubvel/segmentation_models\n[2]: https://segmentation-models.readthedocs.io/en/latest/tutorial.html"], "code": "# Reference: https://www.kaggle.com/code/cdeotte/keras-unet-with-eda\n\n! pip install segmentation-models\nfrom keras import backend as K\n# https://www.kaggle.com/xhlulu/severstal-simple-keras-u-net-boilerplate\n\n# COMPETITION METRIC\ndef dice_coef(y_true, y_pred, smooth=1):\n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    intersection = K.sum(y_true_f * y_pred_f)\n    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\nfrom segmentation_models import Unet\nfrom segmentation_models.backbones import get_preprocessing\n\n# LOAD UNET WITH PRETRAINING FROM IMAGENET\npreprocess = get_preprocessing('resnet34') # for resnet, img = (img-110.0)/1.0\nmodel = Unet('resnet34', input_shape=(128, 800, 3), classes=4, activation='sigmoid')\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=[dice_coef])\n\n# TRAIN AND VALIDATE MODEL\nidx = int(0.8*len(train2)); print()\ntrain_batches = DataGenerator(train2.iloc[:idx],shuffle=True,preprocess=preprocess)\nvalid_batches = DataGenerator(train2.iloc[idx:],preprocess=preprocess)\nhistory = model.fit_generator(train_batches, validation_data = valid_batches, epochs = 30, verbose=2)\n# PLOT TRAINING\nplt.figure(figsize=(15,5))\nplt.plot(range(history.epoch[-1]+1),history.history['val_dice_coef'],label='val_dice_coef')\nplt.plot(range(history.epoch[-1]+1),history.history['dice_coef'],label='trn_dice_coef')\nplt.title('Training Accuracy'); plt.xlabel('Epoch'); plt.ylabel('Dice_coef');plt.legend(); \nplt.show()", "processed": ["download unet download pretrain kera unet github 1 doc 2 repositori mani differ architectur pretrain backbon although bottom pretrain top model need train data train 90 minut use model kaggl steel competit need save train model kaggl dataset load anoth kernel call infer kernel second kernel make predict test set submit second kernel execut hour internet turn thu satifi comp rule 1 http github com qubvel segment model 2 http segment model readthedoc io en latest tutori html"]}, {"markdown": ["# Prediction EDA\nIn the images below (which all contain defects), we will display one prediction mask for a defect that is present. Note that `matplotlib` scales the mask's largest pixel value to yellow and smallest to blue. Therefore the presense of yellow doesn't indicate that the mask achieved a pixel value 1.0. The maximum pixel value is indicated in the titles and is a measure of mask confidence."], "code": "# Reference: https://www.kaggle.com/code/cdeotte/keras-unet-with-eda\n\n# PREDICT FROM VALIDATION SET (ONLY IMAGES WITH DEFECTS)\nval_set = train2.iloc[idx:];\ndefects = list(val_set[val_set['e1']!=''].sample(6).index)\ndefects += list(val_set[val_set['e2']!=''].sample(6).index)\ndefects += list(val_set[val_set['e3']!=''].sample(14).index)\ndefects += list(val_set[val_set['e4']!=''].sample(6).index)\n\nvalid_batches = DataGenerator(val_set[val_set.index.isin(defects)],preprocess=preprocess)\npreds = model.predict_generator(valid_batches,verbose=1)\n# PLOT PREDICTIONS\nvalid_batches = DataGenerator(val_set[val_set.index.isin(defects)])\nprint('Plotting predictions...')\nprint('KEY: yellow=defect1, green=defect2, blue=defect3, magenta=defect4')\n\nfor i,batch in enumerate(valid_batches):\n    plt.figure(figsize=(20,36))\n    for k in range(16):\n        plt.subplot(16,2,2*k+1)\n        img = batch[0][k,]\n        img = Image.fromarray(img.astype('uint8'))\n        img = np.array(img)\n        dft = 0\n        extra = '  has defect '\n        for j in range(4):\n            msk = batch[1][k,:,:,j]\n            if np.sum(msk)!=0: \n                dft=j+1\n                extra += ' '+str(j+1)\n            msk = mask2pad(msk,pad=2)\n            msk = mask2contour(msk,width=3)\n            if j==0: # yellow\n                img[msk==1,0] = 235 \n                img[msk==1,1] = 235\n            elif j==1: img[msk==1,1] = 210 # green\n            elif j==2: img[msk==1,2] = 255 # blue\n            elif j==3: # magenta\n                img[msk==1,0] = 255\n                img[msk==1,2] = 255\n        if extra=='  has defect ': extra =''\n        plt.title('Train '+train2.iloc[16*i+k,0]+extra)\n        plt.axis('off') \n        plt.imshow(img)\n        plt.subplot(16,2,2*k+2) \n        if dft!=0:\n            msk = preds[16*i+k,:,:,dft-1]\n            plt.imshow(msk)\n        else:\n            plt.imshow(np.zeros((128,800)))\n        plt.axis('off')\n        mx = np.round(np.max(msk),3)\n        plt.title('Predict Defect '+str(dft)+'  (max pixel = '+str(mx)+')')\n    plt.subplots_adjust(wspace=0.05)\n    plt.show()", "processed": ["predict eda imag contain defect display one predict mask defect present note matplotlib scale mask largest pixel valu yellow smallest blue therefor presens yellow indic mask achiev pixel valu 1 0 maximum pixel valu indic titl measur mask confid"]}, {"markdown": ["# Error EDA 1\nThe masks above look pretty good. But note that we are only plotting masks corresponding to defects that are present. Below, regardless of what type of defect an image has, we will plot the defect 3 mask. Only the blue contour lines on the left are defect 3. So when we see contours on the left that do not include blue, we should not see defect 3 masks but we still do."], "code": "# Reference: https://www.kaggle.com/code/cdeotte/keras-unet-with-eda\n\n# PREDICT FROM VALIDATION SET (ONLY IMAGES WITH DEFECTS 1, 2, 4)\nval_set = train2.iloc[idx:]\nval_set2 = val_set[(val_set['count']!=0)&(val_set['e3']=='')].sample(16)\n\nvalid_batches = DataGenerator(val_set2,preprocess=preprocess)\npreds = model.predict_generator(valid_batches,verbose=1)\n# PLOT PREDICTIONS\nvalid_batches = DataGenerator(val_set2)\nprint('Plotting predictions...')\nprint('KEY: yellow=defect1, green=defect2, blue=defect3, magenta=defect4')\nfor i,batch in enumerate(valid_batches):\n    plt.figure(figsize=(20,36))\n    for k in range(16):\n        plt.subplot(16,2,2*k+1)\n        img = batch[0][k,]\n        img = Image.fromarray(img.astype('uint8'))\n        img = np.array(img)\n        dft = 0\n        three = False\n        for j in range(4):\n            msk = batch[1][k,:,:,j]\n            if (j==2)&(np.sum(msk)!=0): \n                three=np.sum(msk)\n            msk = mask2pad(msk,pad=2)\n            msk = mask2contour(msk,width=3)\n            if j==0: # yellow\n                img[msk==1,0] = 235 \n                img[msk==1,1] = 235\n            elif j==1: img[msk==1,1] = 210 # green\n            elif j==2: img[msk==1,2] = 255 # blue\n            elif j==3: # magenta\n                img[msk==1,0] = 255\n                img[msk==1,2] = 255\n        extra = ''; extra2 = ''\n        if not three: \n            extra = 'NO DEFECT 3'\n            extra2 = 'ERROR '\n        plt.title('Train '+train2.iloc[16*i+k,0]+'  '+extra)\n        plt.axis('off') \n        plt.imshow(img)\n        plt.subplot(16,2,2*k+2) \n        dft=3\n        if dft!=0:\n            msk = preds[16*i+k,:,:,dft-1]\n            plt.imshow(msk)\n        else:\n            plt.imshow(np.zeros((128,800)))\n        plt.axis('off')\n        mx = np.round(np.max(msk),3)\n        plt.title(extra2+'Predict Defect '+str(dft)+'  (max pixel = '+str(mx)+')')\n    plt.subplots_adjust(wspace=0.05)\n    plt.show()", "processed": ["error eda 1 mask look pretti good note plot mask correspond defect present regardless type defect imag plot defect 3 mask blue contour line left defect 3 see contour left includ blue see defect 3 mask still"]}, {"markdown": ["# Error EDA 2\nWe will plot histograms showing the predicted size of each defect mask. We would hope that if an image does not have a particular defect then UNET would not predict a mask (i.e. predict less than 250 pixel mask). This is not the case. When UNET predicts a mask when a defect isn't present, we call that an \"incorrect\" mask. When UNET predicts a mask when a defect is present, we call that a \"correct\" mask. If UNET predicts less than 250 pixels, we will treat that as no mask predicted. Let's compare the distribution of \"incorrect\" versus \"correct\" masks for each defect type.\n\nUNET outputs masks using all floating point values between 0 and 1 inclusive. When we submit to Kaggle, we need to use only integer 0 and 1. Therefore we must convert mask floating points into integers using a threshold. If `pixel>=THRESHOLD` then `pixel=1` else `pixel=0`. We will plot histograms for various thresholds below. We will consider all masks with less than 250 pixels as empty masks (where `pixel_count = 4 * pixel count` on 128x800).\n\nFrom the plots below, we see that UNET doesn't create more and/or larger masks for images with defects. UNET seems to equally create masks for all images whether there is a defect or not. If we submit the output from UNET to Kaggle, our LB score will be lower than submitting all empty masks (LB 0.85674) because there are more mistake masks than correct masks. Each mistake decreases our LB score by `1/7200` and each correct increases our score by `c*(1/7200)` where `0<=c<=1` is our average dice score."], "code": "# Reference: https://www.kaggle.com/code/cdeotte/keras-unet-with-eda\n\n# PREDICT FROM VALIDATION SET (USE ALL)\nvalid_batches = DataGenerator(train2.iloc[idx:],preprocess=preprocess)\npreds = model.predict_generator(valid_batches,verbose=1)\n# PLOT RESULTS\nimport seaborn as sns\npix_min = 250\nfor THRESHOLD in [0.1, 0.25, 0.50, 0.75, 0.9]:\n    print('######################################')\n    print('## Threshold =',THRESHOLD,'displayed below ##')\n    print('######################################')\n    correct=[[],[],[],[]]; incorrect=[[],[],[],[]]\n    for i,f in enumerate(train2.iloc[idx:idx+len(preds)]['ImageId']):\n        preds2 = preds[i].copy()\n        preds2[preds2>=THRESHOLD]=1\n        preds2[preds2<THRESHOLD]=0\n        sums = np.sum(preds2,axis=(0,1))\n        for j in range(4):\n            if 4*sums[j]<pix_min: continue\n            if train2.iloc[i,j+1]=='': incorrect[j].append(4*sums[j])\n            else: correct[j].append(4*sums[j])\n    plt.figure(figsize=(20,8))\n    for j in range(4):\n        limit = [10000,10000,100000,100000][j]\n        plt.subplot(2,2,j+1)\n        sns.distplot([x for x in correct[j] if x<limit], label = 'correct')\n        sns.distplot([x for x in incorrect[j] if x<limit], label = 'incorrect')\n        plt.title('Defect '+str(j+1)+' mask sizes with threshold = '+str(THRESHOLD)); plt.legend()\n    plt.show()\n    for j in range(4):\n        c1 = np.array(correct[j])\n        c2 = np.array(incorrect[j])\n        print('With threshold =',THRESHOLD,', defect',j+1,'has',len(c1[c1!=0]),'correct and',len(c2[c2!=0]),'incorrect masks')\n    print()", "processed": ["error eda 2 plot histogram show predict size defect mask would hope imag particular defect unet would predict mask e predict le 250 pixel mask case unet predict mask defect present call incorrect mask unet predict mask defect present call correct mask unet predict le 250 pixel treat mask predict let compar distribut incorrect versu correct mask defect type unet output mask use float point valu 0 1 inclus submit kaggl need use integ 0 1 therefor must convert mask float point integ use threshold pixel threshold pixel 1 el pixel 0 plot histogram variou threshold consid mask le 250 pixel empti mask pixel count 4 pixel count 128x800 plot see unet creat larger mask imag defect unet seem equal creat mask imag whether defect submit output unet kaggl lb score lower submit empti mask lb 0 85674 mistak mask correct mask mistak decreas lb score 1 7200 correct increas score c 1 7200 0 c 1 averag dice score"]}, {"markdown": ["And we have only 14 classes of 23 in train data. It seems that using external data is necessary. Let's see how far can we go without it."], "code": "# Reference: https://www.kaggle.com/code/artgor/iwildcam-basic-eda\n\nfig = plt.figure(figsize=(25, 60))\nimgs = [np.random.choice(train.loc[train['classes'] == i, 'file_name'], 4) for i in train.classes.unique()]\nimgs = [i for j in imgs for i in j]\nlabels = [[i] * 4 for i in train.classes.unique()]\nlabels = [i for j in labels for i in j]\nfor idx, img in enumerate(imgs):\n    ax = fig.add_subplot(14, 4, idx + 1, xticks=[], yticks=[])\n    im = Image.open(\"../input/train_images/\" + img)\n    plt.imshow(im)\n    ax.set_title(f'Label: {labels[idx]}')\ntarget_count = train['classes'].value_counts().reset_index().rename(columns={'index': 'target'})\nrender(alt.Chart(target_count).mark_bar().encode(\n    y=alt.Y(\"target:N\", axis=alt.Axis(title='Surface'), sort=list(target_count['target'])),\n    x=alt.X('classes:Q', axis=alt.Axis(title='Count')),\n    tooltip=['target', 'classes']\n).properties(title=\"Counts of target classes\", width=400).interactive())", "processed": ["14 class 23 train data seem use extern data necessari let see far go without"]}, {"markdown": ["# Public LB Scores of Top Teams over time"], "code": "# Reference: https://www.kaggle.com/code/robikscube/the-race-to-predict-molecular-properties\n\n# Interative Plotly\nmypal = cl.scales['9']['div']['Spectral']\ncolors = cl.interp( mypal, 15 )\nannotations = []\ninit_notebook_mode(connected=True)\nTOP_TEAMS = df.min().loc[df.min() < FIFTEENTH_SCORE].index.values\ndf_filtered = df[TOP_TEAMS].ffill()\nteam_ordered = df_filtered.loc[df_filtered.index.max()] \\\n    .sort_values(ascending=True).index.tolist()\n\ndata = []\ni = 0\nfor col in df_filtered[team_ordered].columns:\n    data.append(go.Scatter(\n                        x = df_filtered.index,\n                        y = df_filtered[col],\n                        name=col,\n                        line=dict(color=colors[i], width=2),)\n               )\n    i += 1\n\nannotations.append(dict(xref='paper', yref='paper', x=0.0, y=1.05,\n                              xanchor='left', yanchor='bottom',\n                              text='CHAMPS Leaderboard Tracking',\n                              font=dict(family='Arial',\n                                        size=30,\n                                        color='rgb(37,37,37)'),\n                              showarrow=False))\n\nlayout = go.Layout(yaxis=dict(range=[TOP_SCORE-0.1, 0]),\n                   hovermode='x',\n                   plot_bgcolor='white',\n                  annotations=annotations,\n                  )\nfig = go.Figure(data=data, layout=layout)\nfig.update_layout(\n    legend=go.layout.Legend(\n        traceorder=\"normal\",\n        font=dict(\n            family=\"sans-serif\",\n            size=12,\n            color=\"black\"\n        ),\n        bgcolor=\"LightSteelBlue\",\n        bordercolor=\"Black\",\n        borderwidth=2,\n    )\n)\n\nfig.update_layout(legend_orientation=\"h\")\nfig.update_layout(template=\"plotly_white\")\n#fig.update_yaxes(showgrid=True, gridwidth=0.5, gridcolor='LightGrey')\nfig.update_xaxes(showgrid=False)\n\niplot(fig)", "processed": ["public lb score top team time"]}, {"markdown": ["# All competitors LB Position over Time"], "code": "# Reference: https://www.kaggle.com/code/robikscube/the-race-to-predict-molecular-properties\n\n# Scores of top teams over time\nplt.rcParams[\"font.size\"] = \"12\"\nALL_TEAMS = df.columns.values\ndf[ALL_TEAMS].ffill().plot(figsize=(20, 10),\n                           color=color_pal[0],\n                           legend=False,\n                           alpha=0.05,\n                           ylim=(TOP_SCORE-0.5, 4),\n                           title='All Teams Public LB Scores over Time')\ndf.ffill().min(axis=1).plot(color=color_pal[1], label='1st Place Public LB', legend=True)\nplt.show()", "processed": ["competitor lb posit time"]}, {"markdown": ["# Number of teams by Date"], "code": "# Reference: https://www.kaggle.com/code/robikscube/the-race-to-predict-molecular-properties\n\nplt.rcParams[\"font.size\"] = \"12\"\nax =df.ffill() \\\n    .count(axis=1) \\\n    .plot(figsize=(20, 8),\n          title='Number of Teams in the Competition by Date',\n         color=color_pal[5], lw=5)\nax.set_ylabel('Number of Teams')\nplt.show()\nteam_over_time = df.ffill() \\\n    .count(axis=1)\n\nlr = LinearRegression()\n_ = lr.fit(np.array(pd.to_numeric(team_over_time.index).tolist()).reshape(-1, 1),\n           team_over_time.values)\n\nteamcount_df = pd.DataFrame(team_over_time)\n\nteamcount_pred_df = pd.DataFrame(index=pd.date_range('06-03-2019','08-29-2019'))\nteamcount_pred_df['teamcount_predict'] = lr.predict(np.array(pd.to_numeric(teamcount_pred_df.index).tolist()).reshape(-1, 1))\n\nlr = LinearRegression()\n_ = lr.fit(np.array(pd.to_numeric(team_over_time[-1000:].index).tolist()).reshape(-1, 1),\n           team_over_time[-1000:].values)\n\nteamcount_pred_df['teamcount_predict_recent'] = lr.predict(np.array(pd.to_numeric(teamcount_pred_df.index).tolist()).reshape(-1, 1))\n\nplt.rcParams[\"font.size\"] = \"12\"\nax =df.ffill() \\\n    .count(axis=1) \\\n    .plot(figsize=(20, 8),\n          title='Forecasting the Final Number of Teams',\n         color=color_pal[5], lw=5,\n         xlim=('05-29-2019','08-29-2019'),\n         label='Acutal Team Count by Date')\nax.set_ylabel('Number of Teams')\nteamcount_pred_df['teamcount_predict'].plot(ax=ax, style='.-.', alpha=0.5, label='Regression Using All Data')\nteamcount_pred_df['teamcount_predict_recent'].plot(ax=ax, style='.-.', alpha=0.5, label='Regression Using last 1000 observations')\nplt.legend()\nplt.axvline('08-21-2019', color='orange', linestyle='-.')\nplt.text('08-21-2019', 1000,'Merger Deadline',rotation=-90)\nplt.axvline('08-28-2019', color='orange', linestyle='-.')\nplt.text('08-28-2019', 1000,'Final Deadline',rotation=-90)\nplt.show()", "processed": ["number team date"]}, {"markdown": ["# Top LB Scores\n(Larger bar is better)"], "code": "# Reference: https://www.kaggle.com/code/robikscube/the-race-to-predict-molecular-properties\n\nplt.rcParams[\"font.size\"] = \"12\"\n# Create Top Teams List\nTOP_TEAMS = df.min().loc[df.min() < FIFTYTH_SCORE].index.values\ndf[TOP_TEAMS].min().sort_values(ascending=False).plot(kind='barh',\n                                       xlim=(FIFTYTH_SCORE+0.1,TOP_SCORE-0.1),\n                                       title='Top 50 Public LB Teams',\n                                       figsize=(12, 15),\n                                       color=color_pal[3])\nplt.show()", "processed": ["top lb score larger bar better"]}, {"markdown": ["# Count of LB Submissions that improved score\nThis is the count of times the person submitted and got the fun \"Your score improved\" notification. This is not the total submission count."], "code": "# Reference: https://www.kaggle.com/code/robikscube/the-race-to-predict-molecular-properties\n\nplt.rcParams[\"font.size\"] = \"12\"\ndf[TOP_TEAMS].nunique().sort_values().plot(kind='barh',\n                                           figsize=(12, 15),\n                                           color=color_pal[1],\n                                           title='Count of Submissions improving LB score by Team')\nplt.show()", "processed": ["count lb submiss improv score count time person submit got fun score improv notif total submiss count"]}, {"markdown": ["So there are 256 columns in the dataset having zero variance ie. they have constant values. \n\nLets plot the variance of the variables. "], "code": "# Reference: https://www.kaggle.com/code/shivamb/dataset-decomposition-techniques\n\nfeature_df = feature_df.sort_values('column_var', ascending = True)\nfeature_df['column_var'] = (feature_df['column_var'] - feature_df['column_var'].min()) / (feature_df['column_var'].max() - feature_df['column_var'].min())\ntrace1 = go.Scatter(x=feature_df['columns'], y=feature_df['column_var'], opacity=0.75, marker=dict(color=\"red\"))\nlayout = dict(height=400, title='Feature Variance', legend=dict(orientation=\"h\"));\nfig = go.Figure(data=[trace1], layout=layout);\niplot(fig);\ntrace1 = go.Histogram(x=feature_df[feature_df['column_var'] <= 0.01]['column_var'], opacity=0.45, marker=dict(color=\"red\"))\nlayout = dict(height=400, title='Distribution of Variable Variance <= 0.01', legend=dict(orientation=\"h\"));\nfig = go.Figure(data=[trace1], layout=layout);\niplot(fig);\n\ntrace1 = go.Histogram(x=feature_df[feature_df['column_var'] > 0.01]['column_var'], opacity=0.45, marker=dict(color=\"red\"))\nlayout = dict(height=400, title='Distribution of Variable Variance > 0.01', legend=dict(orientation=\"h\"));\nfig = go.Figure(data=[trace1], layout=layout);\niplot(fig);", "processed": ["256 column dataset zero varianc ie constant valu let plot varianc variabl"]}, {"markdown": ["So we can see that a large number of variables has variable less than 0.01 and fewer variables has variance greater than 0.01 .\n\n#### Correlation with Target Variable \n\nPearson\u2019s correlation coefficient is the test statistics that measures the statistical relationship, or association, between two continuous variables. \n\nAnother statistics test which which can be helpful about the columns is the correlation of the feature with the target variable. High correlated features are good for models for the reverse may not be true. Lets look at what is the distribution of correlations with the target variable in this dataset. "], "code": "# Reference: https://www.kaggle.com/code/shivamb/dataset-decomposition-techniques\n\ntrace1 = go.Histogram(x=feature_df['target_corr'], opacity=0.45, marker=dict(color=\"green\"))\nlayout = dict(height=400, title='Distribution of correlation with target', legend=dict(orientation=\"h\"));\nfig = go.Figure(data=[trace1], layout=layout);\niplot(fig);", "processed": ["see larg number variabl variabl le 0 01 fewer variabl varianc greater 0 01 correl target variabl pearson correl coeffici test statist measur statist relationship associ two continu variabl anoth statist test help column correl featur target variabl high correl featur good model revers may true let look distribut correl target variabl dataset"]}, {"markdown": ["As we can see that most of the variables are not very highly correlated with the target variable, and a majority of the variable have exteremely low correlation with the target. \n\nSo use of basic statistics is one of the method through which one can get idea about features having statistical significance. The same can be used to handpick important features and discard others.  \n\n### 3. Decomposition into EigenVectors and EigenValues  \n\nIn linear algebra, an eigenvector of a linear transformation is a non-zero vector that changes by only a scalar factor when that linear transformation is applied to it. If T is a linear transformation from a vector space V over a field F into itself and v is a vector in V that is not the zero vector, then v is an eigenvector of T if T(v) is a scalar multiple of v. \n\n> T(v)= \u03bbv\n\n\nwhere \u03bb is a scalar value known as the eigenvalue or characteristic root associated with the eigenvector v. In terms of decomposition, eigen vectors are the principal components for any dataset. Lets visualize the individual and cumulative variance explained by eigen vectors. "], "code": "# Reference: https://www.kaggle.com/code/shivamb/dataset-decomposition-techniques\n\n# Calculating Eigenvectors and eigenvalues of Cov matirx\nmean_vec = np.mean(standardized_train, axis=0)\ncov_matrix = np.cov(standardized_train.T)\neig_vals, eig_vecs = np.linalg.eig(cov_matrix)\n\n# Create a list of (eigenvalue, eigenvector) tuples\neig_pairs = [ (np.abs(eig_vals[i]),eig_vecs[:,i]) for i in range(len(eig_vals))]\n\n# Sort the eigenvalue, eigenvector pair from high to low\neig_pairs.sort(key = lambda x: x[0], reverse= True)\n\n# Calculation of Explained Variance from the eigenvalues\ntot = sum(eig_vals)\n\n# Individual explained variance\nvar_exp = [(i/tot)*100 for i in sorted(eig_vals, reverse=True)] \nvar_exp_real = [v.real for v in var_exp]\n\n# Cumulative explained variance\ncum_var_exp = np.cumsum(var_exp) \ncum_exp_real = [v.real for v in cum_var_exp]\n\n## plot the variance and cumulative variance \ntrace1 = go.Scatter(x=train.columns, y=var_exp_real, name=\"Individual Variance\", opacity=0.75, marker=dict(color=\"red\"))\ntrace2 = go.Scatter(x=train.columns, y=cum_exp_real, name=\"Cumulative Variance\", opacity=0.75, marker=dict(color=\"blue\"))\nlayout = dict(height=400, title='Variance Explained by Variables', legend=dict(orientation=\"h\", x=0, y=1.2));\nfig = go.Figure(data=[trace1, trace2], layout=layout);\niplot(fig);", "processed": ["see variabl highli correl target variabl major variabl exterem low correl target use basic statist one method one get idea featur statist signific use handpick import featur discard other 3 decomposit eigenvector eigenvalu linear algebra eigenvector linear transform non zero vector chang scalar factor linear transform appli linear transform vector space v field f v vector v zero vector v eigenvector v scalar multipl v v v scalar valu known eigenvalu characterist root associ eigenvector v term decomposit eigen vector princip compon dataset let visual individu cumul varianc explain eigen vector"]}, {"markdown": ["So, for a threshold value = 0.85, we can choose 993 components. These components will explain about 85% of the variance of the dataset"], "code": "# Reference: https://www.kaggle.com/code/shivamb/dataset-decomposition-techniques\n\ndef plot_3_components(x_trans, title):\n    trace = go.Scatter3d(x=x_trans[:,0], y=x_trans[:,1], z = x_trans[:,2],\n                          name = target, mode = 'markers', text = target, showlegend = False,\n                          marker = dict(size = 8, color=x_trans[:,1], \n                          line = dict(width = 1, color = '#f7f4f4'), opacity = 0.5))\n    layout = go.Layout(title = title, showlegend= True)\n    fig = dict(data=[trace], layout=layout)\n    iplot(fig)\n\ndef plot_2_components(x_trans, title):\n    trace = go.Scatter(x=x_trans[:,0], y=x_trans[:,1], name=target, mode='markers',\n        text = target, showlegend = False,\n        marker = dict(size = 8, color=x_trans[:,1], line = dict(width = 1, color = '#fefefe'), opacity = 0.7))\n    layout = go.Layout(title = title, hovermode= 'closest',\n        xaxis= dict(title= 'First Component',\n            ticklen = 5, zeroline= False, gridwidth= 2),\n        yaxis=dict(title= 'Second Component',\n            ticklen = 5, gridwidth = 2), showlegend= True)\n    fig = dict(data=[trace], layout=layout)\n    iplot(fig)", "processed": ["threshold valu 0 85 choos 993 compon compon explain 85 varianc dataset"]}, {"markdown": ["### 11. Lets further decompose the dataset into two components:  t- SNE \n\nt-SNE was introduced in 2008 as the method for dataset decomposition using non-linear relations.  (t-SNE) t-Distributed Stochastic Neighbor Embedding is a non-linear dimensionality reduction algorithm used for exploring high-dimensional data. It maps multi-dimensional data to two or more dimensions suitable for human observation.  t-SNE is based on probability distributions with random walk on neighborhood graphs to find the structure within the data. The goal is to take a set of points in a high-dimensional space and find a representation of those points in a lower-dimensional space, typically the 2D plane. Lets apply it on the truncated svd components and further decompose the data into two components.\n\n"], "code": "# Reference: https://www.kaggle.com/code/shivamb/dataset-decomposition-techniques\n\ntsne_model = TSNE(n_components=2, verbose=1, random_state=42, n_iter=500)\ntsne_results = tsne_model.fit_transform(X_svd)\n\ntraceTSNE = go.Scatter(\n    x = tsne_results[:,0],\n    y = tsne_results[:,1],\n    name = target,\n     hoveron = target,\n    mode = 'markers',\n    text = target,\n    showlegend = True,\n    marker = dict(\n        size = 8,\n        color = '#c94ff2',\n        showscale = False,\n        line = dict(\n            width = 2,\n            color = 'rgb(255, 255, 255)'\n        ),\n        opacity = 0.8\n    )\n)\ndata = [traceTSNE]\n\nlayout = dict(title = 'TSNE (T-Distributed Stochastic Neighbour Embedding)',\n              hovermode= 'closest',\n              yaxis = dict(zeroline = False),\n              xaxis = dict(zeroline = False),\n              showlegend= False,\n\n             )\n\nfig = dict(data=data, layout=layout)\niplot(fig)", "processed": ["11 let decompos dataset two compon sne sne introduc 2008 method dataset decomposit use non linear relat sne distribut stochast neighbor embed non linear dimension reduct algorithm use explor high dimension data map multi dimension data two dimens suitabl human observ sne base probabl distribut random walk neighborhood graph find structur within data goal take set point high dimension space find represent point lower dimension space typic 2d plane let appli truncat svd compon decompos data two compon"]}, {"markdown": ["### Now let's look at the evolution of f1_scores against scale_pos_weight\nWe can see that probabilities are more spread into [0, 1] space when scale_pos_weight increase"], "code": "# Reference: https://www.kaggle.com/code/ogrellier/scale-pos-weight-vs-duplication\n\nfig, ax = plt.subplots(figsize=(15, 10))\nplt.rc('legend', fontsize=18) \nplt.rc('axes', labelsize=18)\nplt.rc('axes', titlesize=24)\nfor i_w, scale_pos_weight in enumerate(scale_pos_weights):\n    # Get False positives, true positives and the list of thresholds used to compute them\n    fpr, tpr, thresholds = roc_curve(target, oof_proba[:, i_w])\n    # Compute recall, precision and f1_score\n    recall = tpr\n    precision = tpr / (fpr + tpr + 1e-5)\n    f1_scores = 2 * precision * recall / (precision + recall + 1e-5)\n    # Finally plot the f1_scores against thresholds\n    plt.plot(thresholds[-30000:], f1_scores[-30000:], \n             label=\"scale_pos_weight=%2d\" % scale_pos_weight)\nplt.title(\"F1 scores against threshold for different scale_pos_weight\")\nplt.ylabel(\"F1 score\")\nplt.xlabel(\"Probability thresholds\")\nplt.legend(loc=\"lower left\")\nax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n\n", "processed": ["let look evolut f1 score scale po weight see probabl spread 0 1 space scale po weight increas"]}, {"markdown": ["###\u00a0Confusion matrices\nTo see the way probabilities spread more when scale_pos_weight increase let's display the evolution of the confusion matrices "], "code": "# Reference: https://www.kaggle.com/code/ogrellier/scale-pos-weight-vs-duplication\n\nfig = plt.figure(figsize=(20, 15))\nplt.rc('legend', fontsize=10) \nplt.rc('axes', labelsize=10)\nplt.rc('axes', titlesize=10)\ngs = gridspec.GridSpec(int(len(scale_pos_weights) / 2), 2)\nfor i_w, weight in enumerate(scale_pos_weights):\n    ax = plt.subplot(gs[int(i_w / 2), i_w % 2])\n    class_names = [\"safe\", \"unsafe\"]\n    cnf_matrix = confusion_matrix(target, oof_label[:, i_w])\n    plot_confusion_matrix(cnf_matrix, classes=class_names, \n                          normalize=True,\n                          title='Matrix for scale_pos_weight = %2d, Gini %.6f' \n                          % (weight, eval_gini(target, oof_proba[:, i_w])))\nplt.tight_layout()", "processed": ["confus matric see way probabl spread scale po weight increas let display evolut confus matric"]}, {"markdown": ["### Let's check how confusion matrices are affected\nPlease remember labels are computed using a .5 threshold"], "code": "# Reference: https://www.kaggle.com/code/ogrellier/scale-pos-weight-vs-duplication\n\nfig = plt.figure(figsize=(20, 15))\nplt.rc('legend', fontsize=10) \nplt.rc('axes', labelsize=10)\nplt.rc('axes', titlesize=10)\ngs = gridspec.GridSpec(math.ceil(len(dupes) / 2), 2)\nfor i_w, weight in enumerate(dupes):\n    ax = plt.subplot(gs[int(i_w / 2), i_w % 2])\n    class_names = [\"safe\", \"unsafe\"]\n    cnf_matrix = confusion_matrix(target, oof_label[:, i_w])\n    plot_confusion_matrix(cnf_matrix, classes=class_names, \n                          normalize=False,\n                          title='Matrix for duplication = %.1f, Gini %.6f' \n                          % (weight, eval_gini(target, oof_proba[:, i_w])))\nplt.tight_layout()\nfig, ax = plt.subplots(figsize=(15, 10))\nplt.rc('legend', fontsize=18) \nplt.rc('axes', labelsize=18)\nplt.rc('axes', titlesize=24)\nfor i_w, dupe in enumerate(dupes):\n    # Get False positives, true positives and the list of thresholds used to compute them\n    fpr, tpr, thresholds = roc_curve(target, oof_proba[:, i_w])\n    # Compute recall, precision and f1_score\n    recall = tpr\n    precision = tpr / (fpr + tpr + 1e-5)\n    f1_scores = 2 * precision * recall / (precision + recall + 1e-5)\n    # Finally plot the f1_scores against thresholds\n    plt.plot(thresholds[-30000:], f1_scores[-30000:], \n             label=\"duplication rate x %.1f\" % dupe)\nplt.title(\"F1 scores against threshold for different duplication rate\")\nplt.ylabel(\"F1 score\")\nplt.xlabel(\"Probability thresholds\")\nplt.legend(loc=\"lower left\")\nax.legend(loc='center left', bbox_to_anchor=(1, 0.5))", "processed": ["let check confus matric affect plea rememb label comput use 5 threshold"]}, {"markdown": ["### Target Column Exploration:\n\nIn this section, let us explore the target column"], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-notebook-elo\n\ntarget_col = \"target\"\n\nplt.figure(figsize=(8,6))\nplt.scatter(range(train_df.shape[0]), np.sort(train_df[target_col].values))\nplt.xlabel('index', fontsize=12)\nplt.ylabel('Loyalty Score', fontsize=12)\nplt.show()\nplt.figure(figsize=(12,8))\nsns.distplot(train_df[target_col].values, bins=50, kde=False, color=\"red\")\nplt.title(\"Histogram of Loyalty score\")\nplt.xlabel('Loyalty score', fontsize=12)\nplt.show()", "processed": ["target column explor section let u explor target column"]}, {"markdown": ["We have about 2207 rows (almost 1% of the data), which has values different from the rest. Since the metric RMSE these rows might play an important role. So beware of them.", "### First Active Month\n\nIn this section, let us see if there are any distribution change between train and test sets with respect to first active month of the card."], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-notebook-elo\n\ncnt_srs = train_df['first_active_month'].dt.date.value_counts()\ncnt_srs = cnt_srs.sort_index()\nplt.figure(figsize=(14,6))\nsns.barplot(cnt_srs.index, cnt_srs.values, alpha=0.8, color='green')\nplt.xticks(rotation='vertical')\nplt.xlabel('First active month', fontsize=12)\nplt.ylabel('Number of cards', fontsize=12)\nplt.title(\"First active month count in train set\")\nplt.show()\n\ncnt_srs = test_df['first_active_month'].dt.date.value_counts()\ncnt_srs = cnt_srs.sort_index()\nplt.figure(figsize=(14,6))\nsns.barplot(cnt_srs.index, cnt_srs.values, alpha=0.8, color='green')\nplt.xticks(rotation='vertical')\nplt.xlabel('First active month', fontsize=12)\nplt.ylabel('Number of cards', fontsize=12)\nplt.title(\"First active month count in test set\")\nplt.show()", "processed": ["2207 row almost 1 data valu differ rest sinc metric rmse row might play import role bewar", "first activ month section let u see distribut chang train test set respect first activ month card"]}, {"markdown": ["Looks like the distribution is kind of similar between train and test set. So we need not really have to do time based split I think.\n\n### Feature 1,2 & 3:", "In this section, let us see if the other variables in the train dataset has good predictive power in finding the loyalty score."], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-notebook-elo\n\n# feature 1\nplt.figure(figsize=(8,4))\nsns.violinplot(x=\"feature_1\", y=target_col, data=train_df)\nplt.xticks(rotation='vertical')\nplt.xlabel('Feature 1', fontsize=12)\nplt.ylabel('Loyalty score', fontsize=12)\nplt.title(\"Feature 1 distribution\")\nplt.show()\n\n# feature 2\nplt.figure(figsize=(8,4))\nsns.violinplot(x=\"feature_2\", y=target_col, data=train_df)\nplt.xticks(rotation='vertical')\nplt.xlabel('Feature 2', fontsize=12)\nplt.ylabel('Loyalty score', fontsize=12)\nplt.title(\"Feature 2 distribution\")\nplt.show()\n\n# feature 3\nplt.figure(figsize=(8,4))\nsns.violinplot(x=\"feature_3\", y=target_col, data=train_df)\nplt.xticks(rotation='vertical')\nplt.xlabel('Feature 3', fontsize=12)\nplt.ylabel('Loyalty score', fontsize=12)\nplt.title(\"Feature 3 distribution\")\nplt.show()", "processed": ["look like distribut kind similar train test set need realli time base split think featur 1 2 3", "section let u see variabl train dataset good predict power find loyalti score"]}, {"markdown": ["The field descriptions are as follows:\n* card_id\t- Card identifier\n* month_lag\t- month lag to reference date\n* purchase_date\t- Purchase date\n* authorized_flag -\t'Y' if approved, 'N' if denied\n* category_3 - anonymized category\n* installments -\tnumber of installments of purchase\n* category_1 -\tanonymized category\n* merchant_category_id -\tMerchant category identifier (anonymized )\n* subsector_id -\tMerchant category group identifier (anonymized )\n* merchant_id -\tMerchant identifier (anonymized)\n* purchase_amount -\tNormalized purchase amount\n* city_id -\tCity identifier (anonymized )\n* state_id -\tState identifier (anonymized )\n* category_2 -\tanonymized category\n\nNow let us make some features based on the historical transactions and merge them with train and test set.\n\n#### Number of Historical Transactions for the card"], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-notebook-elo\n\ngdf = hist_df.groupby(\"card_id\")\ngdf = gdf[\"purchase_amount\"].size().reset_index()\ngdf.columns = [\"card_id\", \"num_hist_transactions\"]\ntrain_df = pd.merge(train_df, gdf, on=\"card_id\", how=\"left\")\ntest_df = pd.merge(test_df, gdf, on=\"card_id\", how=\"left\")\ncnt_srs = train_df.groupby(\"num_hist_transactions\")[target_col].mean()\ncnt_srs = cnt_srs.sort_index()\ncnt_srs = cnt_srs[:-50]\n\ndef scatter_plot(cnt_srs, color):\n    trace = go.Scatter(\n        x=cnt_srs.index[::-1],\n        y=cnt_srs.values[::-1],\n        showlegend=False,\n        marker=dict(\n            color=color,\n        ),\n    )\n    return trace\n\ntrace = scatter_plot(cnt_srs, \"orange\")\nlayout = dict(\n    title='Loyalty score by Number of historical transactions',\n    )\ndata = [trace]\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename=\"Histtranscnt\")", "processed": ["field descript follow card id card identifi month lag month lag refer date purchas date purchas date author flag approv n deni categori 3 anonym categori instal number instal purchas categori 1 anonym categori merchant categori id merchant categori identifi anonym subsector id merchant categori group identifi anonym merchant id merchant identifi anonym purchas amount normal purchas amount citi id citi identifi anonym state id state identifi anonym categori 2 anonym categori let u make featur base histor transact merg train test set number histor transact card"]}, {"markdown": ["Now let us bin the count of historical transactions and then do some box plots to see the plots better."], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-notebook-elo\n\nbins = [0, 10, 20, 30, 40, 50, 75, 100, 150, 200, 500, 10000]\ntrain_df['binned_num_hist_transactions'] = pd.cut(train_df['num_hist_transactions'], bins)\ncnt_srs = train_df.groupby(\"binned_num_hist_transactions\")[target_col].mean()\n\nplt.figure(figsize=(12,8))\nsns.boxplot(x=\"binned_num_hist_transactions\", y=target_col, data=train_df, showfliers=False)\nplt.xticks(rotation='vertical')\nplt.xlabel('binned_num_hist_transactions', fontsize=12)\nplt.ylabel('Loyalty score', fontsize=12)\nplt.title(\"binned_num_hist_transactions distribution\")\nplt.show()", "processed": ["let u bin count histor transact box plot see plot better"]}, {"markdown": ["#### Value of Historical Transactions\n\nNow let us check the value of the historical transactions for the cards and check the loyalty score distribution based on that."], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-notebook-elo\n\ngdf = hist_df.groupby(\"card_id\")\ngdf = gdf[\"purchase_amount\"].agg(['sum', 'mean', 'std', 'min', 'max']).reset_index()\ngdf.columns = [\"card_id\", \"sum_hist_trans\", \"mean_hist_trans\", \"std_hist_trans\", \"min_hist_trans\", \"max_hist_trans\"]\ntrain_df = pd.merge(train_df, gdf, on=\"card_id\", how=\"left\")\ntest_df = pd.merge(test_df, gdf, on=\"card_id\", how=\"left\")\nbins = np.percentile(train_df[\"sum_hist_trans\"], range(0,101,10))\ntrain_df['binned_sum_hist_trans'] = pd.cut(train_df['sum_hist_trans'], bins)\n#cnt_srs = train_df.groupby(\"binned_sum_hist_trans\")[target_col].mean()\n\nplt.figure(figsize=(12,8))\nsns.boxplot(x=\"binned_sum_hist_trans\", y=target_col, data=train_df, showfliers=False)\nplt.xticks(rotation='vertical')\nplt.xlabel('binned_sum_hist_trans', fontsize=12)\nplt.ylabel('Loyalty score', fontsize=12)\nplt.title(\"Sum of historical transaction value (Binned) distribution\")\nplt.show()", "processed": ["valu histor transact let u check valu histor transact card check loyalti score distribut base"]}, {"markdown": ["As we could see, the loyalty score seem to increase with the \"sum of historical transaction value\". This is expected. Now we can do the same plot with \"Mean value of historical transaction\"."], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-notebook-elo\n\nbins = np.percentile(train_df[\"mean_hist_trans\"], range(0,101,10))\ntrain_df['binned_mean_hist_trans'] = pd.cut(train_df['mean_hist_trans'], bins)\n#cnt_srs = train_df.groupby(\"binned_mean_hist_trans\")[target_col].mean()\n\nplt.figure(figsize=(12,8))\nsns.boxplot(x=\"binned_mean_hist_trans\", y=target_col, data=train_df, showfliers=False)\nplt.xticks(rotation='vertical')\nplt.xlabel('Binned Mean Historical Transactions', fontsize=12)\nplt.ylabel('Loyalty score', fontsize=12)\nplt.title(\"Mean of historical transaction value (Binned) distribution\")\nplt.show()", "processed": ["could see loyalti score seem increas sum histor transact valu expect plot mean valu histor transact"]}, {"markdown": ["### New Merchant Transactions\n\nIn this section, let us look at the new merchant transactions data and do some analysis"], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-notebook-elo\n\nnew_trans_df = pd.read_csv(\"../input/new_merchant_transactions.csv\")\nnew_trans_df.head()\ngdf = new_trans_df.groupby(\"card_id\")\ngdf = gdf[\"purchase_amount\"].size().reset_index()\ngdf.columns = [\"card_id\", \"num_merch_transactions\"]\ntrain_df = pd.merge(train_df, gdf, on=\"card_id\", how=\"left\")\ntest_df = pd.merge(test_df, gdf, on=\"card_id\", how=\"left\")\nbins = [0, 10, 20, 30, 40, 50, 75, 10000]\ntrain_df['binned_num_merch_transactions'] = pd.cut(train_df['num_merch_transactions'], bins)\ncnt_srs = train_df.groupby(\"binned_num_merch_transactions\")[target_col].mean()\n\nplt.figure(figsize=(12,8))\nsns.boxplot(x=\"binned_num_merch_transactions\", y=target_col, data=train_df, showfliers=False)\nplt.xticks(rotation='vertical')\nplt.xlabel('binned_num_merch_transactions', fontsize=12)\nplt.ylabel('Loyalty score', fontsize=12)\nplt.title(\"Number of new merchants transaction (Binned) distribution\")\nplt.show()", "processed": ["new merchant transact section let u look new merchant transact data analysi"]}, {"markdown": ["Loyalty score seem to decrease as the number of new merchant transactions increases except for the last bin. "], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-notebook-elo\n\ngdf = new_trans_df.groupby(\"card_id\")\ngdf = gdf[\"purchase_amount\"].agg(['sum', 'mean', 'std', 'min', 'max']).reset_index()\ngdf.columns = [\"card_id\", \"sum_merch_trans\", \"mean_merch_trans\", \"std_merch_trans\", \"min_merch_trans\", \"max_merch_trans\"]\ntrain_df = pd.merge(train_df, gdf, on=\"card_id\", how=\"left\")\ntest_df = pd.merge(test_df, gdf, on=\"card_id\", how=\"left\")\nbins = np.nanpercentile(train_df[\"sum_merch_trans\"], range(0,101,10))\ntrain_df['binned_sum_merch_trans'] = pd.cut(train_df['sum_merch_trans'], bins)\n#cnt_srs = train_df.groupby(\"binned_sum_hist_trans\")[target_col].mean()\n\nplt.figure(figsize=(12,8))\nsns.boxplot(x=\"binned_sum_merch_trans\", y=target_col, data=train_df, showfliers=False)\nplt.xticks(rotation='vertical')\nplt.xlabel('binned sum of new merchant transactions', fontsize=12)\nplt.ylabel('Loyalty score', fontsize=12)\nplt.title(\"Sum of New merchants transaction value (Binned) distribution\")\nplt.show()", "processed": ["loyalti score seem decreas number new merchant transact increas except last bin"]}, {"markdown": ["Loyalty scores seem to increase with the increase in the sum of new merchant transaction values but for the last bin."], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-notebook-elo\n\nbins = np.nanpercentile(train_df[\"mean_merch_trans\"], range(0,101,10))\ntrain_df['binned_mean_merch_trans'] = pd.cut(train_df['mean_merch_trans'], bins)\n#cnt_srs = train_df.groupby(\"binned_sum_hist_trans\")[target_col].mean()\n\nplt.figure(figsize=(12,8))\nsns.boxplot(x=\"binned_mean_merch_trans\", y=target_col, data=train_df, showfliers=False)\nplt.xticks(rotation='vertical')\nplt.xlabel('binned mean of new merchant transactions', fontsize=12)\nplt.ylabel('Loyalty score', fontsize=12)\nplt.title(\"Mean of New merchants transaction value (Binned) distribution\")\nplt.show()", "processed": ["loyalti score seem increas increas sum new merchant transact valu last bin"]}, {"markdown": ["We can make few observations here:   \n\n* standard deviation is relatively large for both train and test variable data;  \n* min, max, mean, sdt values for train and test data looks quite close;  \n* mean values are distributed over a large range.\n\nThe number of values in train and test set is the same. Let's plot the scatter plot for train and test set for few of the features.\n"], "code": "# Reference: https://www.kaggle.com/code/gpreda/santander-eda-and-prediction\n\ndef plot_feature_scatter(df1, df2, features):\n    i = 0\n    sns.set_style('whitegrid')\n    plt.figure()\n    fig, ax = plt.subplots(4,4,figsize=(14,14))\n\n    for feature in features:\n        i += 1\n        plt.subplot(4,4,i)\n        plt.scatter(df1[feature], df2[feature], marker='+')\n        plt.xlabel(feature, fontsize=9)\n    plt.show();", "processed": ["make observ standard deviat rel larg train test variabl data min max mean sdt valu train test data look quit close mean valu distribut larg rang number valu train test set let plot scatter plot train test set featur"]}, {"markdown": ["Let's check the distribution of **target** value in train dataset."], "code": "# Reference: https://www.kaggle.com/code/gpreda/santander-eda-and-prediction\n\nsns.countplot(train_df['target'], palette='Set3')\nprint(\"There are {}% target values with 1\".format(100 * train_df[\"target\"].value_counts()[1]/train_df.shape[0]))", "processed": ["let check distribut target valu train dataset"]}, {"markdown": ["The data is unbalanced with respect with **target** value.   ", "\n## <a id='32'>Density plots of features</a>  \n\nLet's show now the density plot of variables in train dataset. \n\nWe represent with different colors the distribution for values with **target** value **0** and **1**."], "code": "# Reference: https://www.kaggle.com/code/gpreda/santander-eda-and-prediction\n\ndef plot_feature_distribution(df1, df2, label1, label2, features):\n    i = 0\n    sns.set_style('whitegrid')\n    plt.figure()\n    fig, ax = plt.subplots(10,10,figsize=(18,22))\n\n    for feature in features:\n        i += 1\n        plt.subplot(10,10,i)\n        sns.distplot(df1[feature], hist=False,label=label1)\n        sns.distplot(df2[feature], hist=False,label=label2)\n        plt.xlabel(feature, fontsize=9)\n        locs, labels = plt.xticks()\n        plt.tick_params(axis='x', which='major', labelsize=6, pad=-6)\n        plt.tick_params(axis='y', which='major', labelsize=6)\n    plt.show();", "processed": ["data unbalanc respect target valu", "id 32 densiti plot featur let show densiti plot variabl train dataset repres differ color distribut valu target valu 0 1"]}, {"markdown": ["The train and test seems to be well ballanced with respect with distribution of the numeric variables.  \n\n## <a id='33'>Distribution of mean and std</a>  \n\nLet's check the distribution of the mean values per row in the train and test set."], "code": "# Reference: https://www.kaggle.com/code/gpreda/santander-eda-and-prediction\n\nplt.figure(figsize=(16,6))\nfeatures = train_df.columns.values[2:202]\nplt.title(\"Distribution of mean values per row in the train and test set\")\nsns.distplot(train_df[features].mean(axis=1),color=\"green\", kde=True,bins=120, label='train')\nsns.distplot(test_df[features].mean(axis=1),color=\"blue\", kde=True,bins=120, label='test')\nplt.legend()\nplt.show()", "processed": ["train test seem well ballanc respect distribut numer variabl id 33 distribut mean std let check distribut mean valu per row train test set"]}, {"markdown": ["Let's check the distribution of the mean values per columns in the train and test set."], "code": "# Reference: https://www.kaggle.com/code/gpreda/santander-eda-and-prediction\n\nplt.figure(figsize=(16,6))\nplt.title(\"Distribution of mean values per column in the train and test set\")\nsns.distplot(train_df[features].mean(axis=0),color=\"magenta\",kde=True,bins=120, label='train')\nsns.distplot(test_df[features].mean(axis=0),color=\"darkblue\", kde=True,bins=120, label='test')\nplt.legend()\nplt.show()", "processed": ["let check distribut mean valu per column train test set"]}, {"markdown": ["Let's show the distribution of standard deviation of values per row for train and test datasets."], "code": "# Reference: https://www.kaggle.com/code/gpreda/santander-eda-and-prediction\n\nplt.figure(figsize=(16,6))\nplt.title(\"Distribution of std values per row in the train and test set\")\nsns.distplot(train_df[features].std(axis=1),color=\"black\", kde=True,bins=120, label='train')\nsns.distplot(test_df[features].std(axis=1),color=\"red\", kde=True,bins=120, label='test')\nplt.legend();plt.show()", "processed": ["let show distribut standard deviat valu per row train test dataset"]}, {"markdown": ["Let's check the distribution of the standard deviation of values per columns in the train and test datasets."], "code": "# Reference: https://www.kaggle.com/code/gpreda/santander-eda-and-prediction\n\nplt.figure(figsize=(16,6))\nplt.title(\"Distribution of std values per column in the train and test set\")\nsns.distplot(train_df[features].std(axis=0),color=\"blue\",kde=True,bins=120, label='train')\nsns.distplot(test_df[features].std(axis=0),color=\"green\", kde=True,bins=120, label='test')\nplt.legend(); plt.show()", "processed": ["let check distribut standard deviat valu per column train test dataset"]}, {"markdown": ["Let's check now the distribution of the mean value per row in the train dataset, grouped by value of target."], "code": "# Reference: https://www.kaggle.com/code/gpreda/santander-eda-and-prediction\n\nt0 = train_df.loc[train_df['target'] == 0]\nt1 = train_df.loc[train_df['target'] == 1]\nplt.figure(figsize=(16,6))\nplt.title(\"Distribution of mean values per row in the train set\")\nsns.distplot(t0[features].mean(axis=1),color=\"red\", kde=True,bins=120, label='target = 0')\nsns.distplot(t1[features].mean(axis=1),color=\"blue\", kde=True,bins=120, label='target = 1')\nplt.legend(); plt.show()", "processed": ["let check distribut mean valu per row train dataset group valu target"]}, {"markdown": ["Let's check now the distribution of the mean value per column in the train dataset, grouped by value of target."], "code": "# Reference: https://www.kaggle.com/code/gpreda/santander-eda-and-prediction\n\nplt.figure(figsize=(16,6))\nplt.title(\"Distribution of mean values per column in the train set\")\nsns.distplot(t0[features].mean(axis=0),color=\"green\", kde=True,bins=120, label='target = 0')\nsns.distplot(t1[features].mean(axis=0),color=\"darkblue\", kde=True,bins=120, label='target = 1')\nplt.legend(); plt.show()", "processed": ["let check distribut mean valu per column train dataset group valu target"]}, {"markdown": ["## <a id='34'>Distribution of min and max</a>  \n\nLet's check the distribution of min per row in the train and test set."], "code": "# Reference: https://www.kaggle.com/code/gpreda/santander-eda-and-prediction\n\nplt.figure(figsize=(16,6))\nfeatures = train_df.columns.values[2:202]\nplt.title(\"Distribution of min values per row in the train and test set\")\nsns.distplot(train_df[features].min(axis=1),color=\"red\", kde=True,bins=120, label='train')\nsns.distplot(test_df[features].min(axis=1),color=\"orange\", kde=True,bins=120, label='test')\nplt.legend()\nplt.show()", "processed": ["id 34 distribut min max let check distribut min per row train test set"]}, {"markdown": ["A long queue to the lower values for both, extended as long as to -80 for test set, is observed.\n\nLet's now show the distribution of min per column in the train and test set."], "code": "# Reference: https://www.kaggle.com/code/gpreda/santander-eda-and-prediction\n\nplt.figure(figsize=(16,6))\nfeatures = train_df.columns.values[2:202]\nplt.title(\"Distribution of min values per column in the train and test set\")\nsns.distplot(train_df[features].min(axis=0),color=\"magenta\", kde=True,bins=120, label='train')\nsns.distplot(test_df[features].min(axis=0),color=\"darkblue\", kde=True,bins=120, label='test')\nplt.legend()\nplt.show()", "processed": ["long queue lower valu extend long 80 test set observ let show distribut min per column train test set"]}, {"markdown": ["Let's check now the distribution of max values per rows for train and test set."], "code": "# Reference: https://www.kaggle.com/code/gpreda/santander-eda-and-prediction\n\nplt.figure(figsize=(16,6))\nfeatures = train_df.columns.values[2:202]\nplt.title(\"Distribution of max values per row in the train and test set\")\nsns.distplot(train_df[features].max(axis=1),color=\"brown\", kde=True,bins=120, label='train')\nsns.distplot(test_df[features].max(axis=1),color=\"yellow\", kde=True,bins=120, label='test')\nplt.legend()\nplt.show()", "processed": ["let check distribut max valu per row train test set"]}, {"markdown": ["Let's show now the max distribution on columns for train and test set."], "code": "# Reference: https://www.kaggle.com/code/gpreda/santander-eda-and-prediction\n\nplt.figure(figsize=(16,6))\nfeatures = train_df.columns.values[2:202]\nplt.title(\"Distribution of max values per column in the train and test set\")\nsns.distplot(train_df[features].max(axis=0),color=\"blue\", kde=True,bins=120, label='train')\nsns.distplot(test_df[features].max(axis=0),color=\"red\", kde=True,bins=120, label='test')\nplt.legend()\nplt.show()", "processed": ["let show max distribut column train test set"]}, {"markdown": ["Let's show now the distributions of min values per row in train set, separated on the values of target (0 and 1)."], "code": "# Reference: https://www.kaggle.com/code/gpreda/santander-eda-and-prediction\n\nt0 = train_df.loc[train_df['target'] == 0]\nt1 = train_df.loc[train_df['target'] == 1]\nplt.figure(figsize=(16,6))\nplt.title(\"Distribution of min values per row in the train set\")\nsns.distplot(t0[features].min(axis=1),color=\"orange\", kde=True,bins=120, label='target = 0')\nsns.distplot(t1[features].min(axis=1),color=\"darkblue\", kde=True,bins=120, label='target = 1')\nplt.legend(); plt.show()", "processed": ["let show distribut min valu per row train set separ valu target 0 1"]}, {"markdown": ["We show here the distribution of min values per columns in train set."], "code": "# Reference: https://www.kaggle.com/code/gpreda/santander-eda-and-prediction\n\nplt.figure(figsize=(16,6))\nplt.title(\"Distribution of min values per column in the train set\")\nsns.distplot(t0[features].min(axis=0),color=\"red\", kde=True,bins=120, label='target = 0')\nsns.distplot(t1[features].min(axis=0),color=\"blue\", kde=True,bins=120, label='target = 1')\nplt.legend(); plt.show()", "processed": ["show distribut min valu per column train set"]}, {"markdown": ["Let's show now the distribution of max values per rown in the train set."], "code": "# Reference: https://www.kaggle.com/code/gpreda/santander-eda-and-prediction\n\nplt.figure(figsize=(16,6))\nplt.title(\"Distribution of max values per row in the train set\")\nsns.distplot(t0[features].max(axis=1),color=\"gold\", kde=True,bins=120, label='target = 0')\nsns.distplot(t1[features].max(axis=1),color=\"darkblue\", kde=True,bins=120, label='target = 1')\nplt.legend(); plt.show()", "processed": ["let show distribut max valu per rown train set"]}, {"markdown": ["Let's show also the distribution of max values per columns in the train set."], "code": "# Reference: https://www.kaggle.com/code/gpreda/santander-eda-and-prediction\n\nplt.figure(figsize=(16,6))\nplt.title(\"Distribution of max values per column in the train set\")\nsns.distplot(t0[features].max(axis=0),color=\"red\", kde=True,bins=120, label='target = 0')\nsns.distplot(t1[features].max(axis=0),color=\"blue\", kde=True,bins=120, label='target = 1')\nplt.legend(); plt.show()", "processed": ["let show also distribut max valu per column train set"]}, {"markdown": ["## <a id='35'>Distribution of skew and kurtosis</a>  \n\nLet's see now what is the distribution of skew values per rows and columns.\n\nLet's see first the distribution of skewness calculated per rows in train and test sets."], "code": "# Reference: https://www.kaggle.com/code/gpreda/santander-eda-and-prediction\n\nplt.figure(figsize=(16,6))\nplt.title(\"Distribution of skew per row in the train and test set\")\nsns.distplot(train_df[features].skew(axis=1),color=\"red\", kde=True,bins=120, label='train')\nsns.distplot(test_df[features].skew(axis=1),color=\"orange\", kde=True,bins=120, label='test')\nplt.legend()\nplt.show()", "processed": ["id 35 distribut skew kurtosi let see distribut skew valu per row column let see first distribut skew calcul per row train test set"]}, {"markdown": ["Let's see first the distribution of skewness calculated per columns in train and test set."], "code": "# Reference: https://www.kaggle.com/code/gpreda/santander-eda-and-prediction\n\nplt.figure(figsize=(16,6))\nplt.title(\"Distribution of skew per column in the train and test set\")\nsns.distplot(train_df[features].skew(axis=0),color=\"magenta\", kde=True,bins=120, label='train')\nsns.distplot(test_df[features].skew(axis=0),color=\"darkblue\", kde=True,bins=120, label='test')\nplt.legend()\nplt.show()", "processed": ["let see first distribut skew calcul per column train test set"]}, {"markdown": ["Let's see now what is the distribution of kurtosis values per rows and columns.\n\nLet's see first the distribution of kurtosis calculated per rows in train and test sets."], "code": "# Reference: https://www.kaggle.com/code/gpreda/santander-eda-and-prediction\n\nplt.figure(figsize=(16,6))\nplt.title(\"Distribution of kurtosis per row in the train and test set\")\nsns.distplot(train_df[features].kurtosis(axis=1),color=\"darkblue\", kde=True,bins=120, label='train')\nsns.distplot(test_df[features].kurtosis(axis=1),color=\"yellow\", kde=True,bins=120, label='test')\nplt.legend()\nplt.show()", "processed": ["let see distribut kurtosi valu per row column let see first distribut kurtosi calcul per row train test set"]}, {"markdown": ["Let's see first the distribution of kurtosis calculated per columns in train and test sets."], "code": "# Reference: https://www.kaggle.com/code/gpreda/santander-eda-and-prediction\n\nplt.figure(figsize=(16,6))\nplt.title(\"Distribution of kurtosis per column in the train and test set\")\nsns.distplot(train_df[features].kurtosis(axis=0),color=\"magenta\", kde=True,bins=120, label='train')\nsns.distplot(test_df[features].kurtosis(axis=0),color=\"green\", kde=True,bins=120, label='test')\nplt.legend()\nplt.show()", "processed": ["let see first distribut kurtosi calcul per column train test set"]}, {"markdown": ["Let's see now the distribution of skewness on rows in train separated for values of target 0 and 1."], "code": "# Reference: https://www.kaggle.com/code/gpreda/santander-eda-and-prediction\n\nt0 = train_df.loc[train_df['target'] == 0]\nt1 = train_df.loc[train_df['target'] == 1]\nplt.figure(figsize=(16,6))\nplt.title(\"Distribution of skew values per row in the train set\")\nsns.distplot(t0[features].skew(axis=1),color=\"red\", kde=True,bins=120, label='target = 0')\nsns.distplot(t1[features].skew(axis=1),color=\"blue\", kde=True,bins=120, label='target = 1')\nplt.legend(); plt.show()", "processed": ["let see distribut skew row train separ valu target 0 1"]}, {"markdown": ["Let's see now the distribution of skewness on columns in train separated for values of target 0 and 1."], "code": "# Reference: https://www.kaggle.com/code/gpreda/santander-eda-and-prediction\n\nt0 = train_df.loc[train_df['target'] == 0]\nt1 = train_df.loc[train_df['target'] == 1]\nplt.figure(figsize=(16,6))\nplt.title(\"Distribution of skew values per column in the train set\")\nsns.distplot(t0[features].skew(axis=0),color=\"red\", kde=True,bins=120, label='target = 0')\nsns.distplot(t1[features].skew(axis=0),color=\"blue\", kde=True,bins=120, label='target = 1')\nplt.legend(); plt.show()", "processed": ["let see distribut skew column train separ valu target 0 1"]}, {"markdown": ["Let's see now the distribution of kurtosis on rows in train separated for values of target 0 and 1."], "code": "# Reference: https://www.kaggle.com/code/gpreda/santander-eda-and-prediction\n\nt0 = train_df.loc[train_df['target'] == 0]\nt1 = train_df.loc[train_df['target'] == 1]\nplt.figure(figsize=(16,6))\nplt.title(\"Distribution of kurtosis values per row in the train set\")\nsns.distplot(t0[features].kurtosis(axis=1),color=\"red\", kde=True,bins=120, label='target = 0')\nsns.distplot(t1[features].kurtosis(axis=1),color=\"blue\", kde=True,bins=120, label='target = 1')\nplt.legend(); plt.show()", "processed": ["let see distribut kurtosi row train separ valu target 0 1"]}, {"markdown": ["Let's see now the distribution of kurtosis on columns in train separated for values of target 0 and 1."], "code": "# Reference: https://www.kaggle.com/code/gpreda/santander-eda-and-prediction\n\nt0 = train_df.loc[train_df['target'] == 0]\nt1 = train_df.loc[train_df['target'] == 1]\nplt.figure(figsize=(16,6))\nplt.title(\"Distribution of kurtosis values per column in the train set\")\nsns.distplot(t0[features].kurtosis(axis=0),color=\"red\", kde=True,bins=120, label='target = 0')\nsns.distplot(t1[features].kurtosis(axis=0),color=\"blue\", kde=True,bins=120, label='target = 1')\nplt.legend(); plt.show()\n", "processed": ["let see distribut kurtosi column train separ valu target 0 1"]}, {"markdown": ["Let's check the new created features."], "code": "# Reference: https://www.kaggle.com/code/gpreda/santander-eda-and-prediction\n\ntrain_df[train_df.columns[202:]].head()\ntest_df[test_df.columns[201:]].head()\ndef plot_new_feature_distribution(df1, df2, label1, label2, features):\n    i = 0\n    sns.set_style('whitegrid')\n    plt.figure()\n    fig, ax = plt.subplots(2,4,figsize=(18,8))\n\n    for feature in features:\n        i += 1\n        plt.subplot(2,4,i)\n        sns.kdeplot(df1[feature], bw=0.5,label=label1)\n        sns.kdeplot(df2[feature], bw=0.5,label=label2)\n        plt.xlabel(feature, fontsize=11)\n        locs, labels = plt.xticks()\n        plt.tick_params(axis='x', which='major', labelsize=8)\n        plt.tick_params(axis='y', which='major', labelsize=8)\n    plt.show();", "processed": ["let check new creat featur"]}, {"markdown": ["Let's check the feature importance."], "code": "# Reference: https://www.kaggle.com/code/gpreda/santander-eda-and-prediction\n\ncols = (feature_importance_df[[\"Feature\", \"importance\"]]\n        .groupby(\"Feature\")\n        .mean()\n        .sort_values(by=\"importance\", ascending=False)[:150].index)\nbest_features = feature_importance_df.loc[feature_importance_df.Feature.isin(cols)]\n\nplt.figure(figsize=(14,28))\nsns.barplot(x=\"importance\", y=\"Feature\", data=best_features.sort_values(by=\"importance\",ascending=False))\nplt.title('Features importance (averaged/folds)')\nplt.tight_layout()\nplt.savefig('FI.png')", "processed": ["let check featur import"]}, {"markdown": ["High interest goes for low prices", "## Boxplot gives even better insights "], "code": "# Reference: https://www.kaggle.com/code/ogrellier/median-rental-prices-matter\n\nplt.figure(figsize=(11,10))\nsns.boxplot(x=\"bedrooms\", y=\"price\", hue=\"interest_level\", data=usable_train, palette=palette)", "processed": ["high interest goe low price", "boxplot give even better insight"]}, {"markdown": ["## Plot a few training images from `stage_1_train_images.zip`"], "code": "# Reference: https://www.kaggle.com/code/robikscube/eda-lets-detect-pneumonia-explore-lung-images\n\nplt.style.use('default')\nfig=plt.figure(figsize=(20, 10))\ncolumns = 8; rows = 4\nfor i in range(1, columns*rows +1):\n    ds = pydicom.dcmread(train_images_dir + train_images[i])\n    fig.add_subplot(rows, columns, i)\n    plt.imshow(ds.pixel_array, cmap=plt.cm.bone)\n    fig.add_subplot", "processed": ["plot train imag stage 1 train imag zip"]}, {"markdown": ["## Distribution of Positive Labels"], "code": "# Reference: https://www.kaggle.com/code/robikscube/eda-lets-detect-pneumonia-explore-lung-images\n\n# Number of positive targets\nprint(round((8964 / (8964 + 20025)) * 100, 2), '% of the examples are positive')\npd.DataFrame(train_labels.groupby('Target')['patientId'].count())\n# Distribution of Target in Training Set\nplt.style.use('ggplot')\nplot = train_labels.groupby('Target') \\\n    .count()['patientId'] \\\n    .plot(kind='bar', figsize=(10,4), rot=0)", "processed": ["distribut posit label"]}, {"markdown": ["## Size of the impacted area\nWe can make a new feature called \"area\" to the train labels data to see what the distribution of areas label look like."], "code": "# Reference: https://www.kaggle.com/code/robikscube/eda-lets-detect-pneumonia-explore-lung-images\n\nplt.style.use('ggplot')\ntrain_labels['area'] = train_labels['width'] * train_labels['height']\nplot = train_labels['area'].plot(kind='hist',\n                          figsize=(10,4),\n                          bins=20,\n                          title='Distribution of Area within Image idenfitying a positive target')", "processed": ["size impact area make new featur call area train label data see distribut area label look like"]}, {"markdown": ["# Plotting Boxes around Images\nThanks for plotting functions from @peterchang77 `https://www.kaggle.com/peterchang77/exploratory-data-analysis` !!!"], "code": "# Reference: https://www.kaggle.com/code/robikscube/eda-lets-detect-pneumonia-explore-lung-images\n\n# Forked from `https://www.kaggle.com/peterchang77/exploratory-data-analysis`\ndef parse_data(df):\n    \"\"\"\n    Method to read a CSV file (Pandas dataframe) and parse the \n    data into the following nested dictionary:\n\n      parsed = {\n        \n        'patientId-00': {\n            'dicom': path/to/dicom/file,\n            'label': either 0 or 1 for normal or pnuemonia, \n            'boxes': list of box(es)\n        },\n        'patientId-01': {\n            'dicom': path/to/dicom/file,\n            'label': either 0 or 1 for normal or pnuemonia, \n            'boxes': list of box(es)\n        }, ...\n\n      }\n\n    \"\"\"\n    # --- Define lambda to extract coords in list [y, x, height, width]\n    extract_box = lambda row: [row['y'], row['x'], row['height'], row['width']]\n\n    parsed = {}\n    for n, row in df.iterrows():\n        # --- Initialize patient entry into parsed \n        pid = row['patientId']\n        if pid not in parsed:\n            parsed[pid] = {\n                'dicom': '../input/stage_1_train_images/%s.dcm' % pid,\n                'label': row['Target'],\n                'boxes': []}\n\n        # --- Add box if opacity is present\n        if parsed[pid]['label'] == 1:\n            parsed[pid]['boxes'].append(extract_box(row))\n\n    return parsed\n\nparsed = parse_data(train_labels)\n\ndef draw(data):\n    \"\"\"\n    Method to draw single patient with bounding box(es) if present \n\n    \"\"\"\n    # --- Open DICOM file\n    d = pydicom.read_file(data['dicom'])\n    im = d.pixel_array\n\n    # --- Convert from single-channel grayscale to 3-channel RGB\n    im = np.stack([im] * 3, axis=2)\n\n    # --- Add boxes with random color if present\n    for box in data['boxes']:\n        #rgb = np.floor(np.random.rand(3) * 256).astype('int')\n        rgb = [255, 251, 204] # Just use yellow\n        im = overlay_box(im=im, box=box, rgb=rgb, stroke=15)\n\n    plt.imshow(im, cmap=plt.cm.gist_gray)\n    plt.axis('off')\n\ndef overlay_box(im, box, rgb, stroke=2):\n    \"\"\"\n    Method to overlay single box on image\n\n    \"\"\"\n    # --- Convert coordinates to integers\n    box = [int(b) for b in box]\n    \n    # --- Extract coordinates\n    y1, x1, height, width = box\n    y2 = y1 + height\n    x2 = x1 + width\n\n    im[y1:y1 + stroke, x1:x2] = rgb\n    im[y2:y2 + stroke, x1:x2] = rgb\n    im[y1:y2, x1:x1 + stroke] = rgb\n    im[y1:y2, x2:x2 + stroke] = rgb\n\n    return im\nplt.style.use('default')\nfig=plt.figure(figsize=(20, 10))\ncolumns = 8; rows = 4\nfor i in range(1, columns*rows +1):\n    fig.add_subplot(rows, columns, i)\n    draw(parsed[train_labels['patientId'].unique()[i]])\n    fig.add_subplot", "processed": ["plot box around imag thank plot function peterchang77 http www kaggl com peterchang77 exploratori data analysi"]}, {"markdown": ["# EDA of Detailed Class Info"], "code": "# Reference: https://www.kaggle.com/code/robikscube/eda-lets-detect-pneumonia-explore-lung-images\n\ndetailed_class_info = pd.read_csv('../input/stage_1_detailed_class_info.csv')\ndetailed_class_info.groupby('class').count()\nplt.style.use('ggplot')\nplot = detailed_class_info.groupby('class').count().plot(kind='bar',\n                                                  rot=0,\n                                                  title='Count of Class Labels',\n                                                  figsize=(10,4))\ncount_labels_per_patient = detailed_class_info.groupby('patientId').count()", "processed": ["eda detail class info"]}, {"markdown": ["## ** Lung Opacity** Examples"], "code": "# Reference: https://www.kaggle.com/code/robikscube/eda-lets-detect-pneumonia-explore-lung-images\n\nplt.style.use('default')\nfig=plt.figure(figsize=(20, 10))\ncolumns = 8; rows = 4\nfor i in range(1, columns*rows +1):\n    fig.add_subplot(rows, columns, i)\n    draw(parsed[opacity['patientId'].unique()[i]])", "processed": ["lung opac exampl"]}, {"markdown": ["## ** No Lung Opacity / Not Normal** Examples"], "code": "# Reference: https://www.kaggle.com/code/robikscube/eda-lets-detect-pneumonia-explore-lung-images\n\nplt.style.use('default')\nfig=plt.figure(figsize=(20, 10))\ncolumns = 8; rows = 4\nfor i in range(1, columns*rows +1):\n    fig.add_subplot(rows, columns, i)\n    draw(parsed[not_normal['patientId'].loc[i]])", "processed": ["lung opac normal exampl"]}, {"markdown": ["## **Normal** Examples"], "code": "# Reference: https://www.kaggle.com/code/robikscube/eda-lets-detect-pneumonia-explore-lung-images\n\nplt.style.use('default')\nfig=plt.figure(figsize=(20, 10))\ncolumns = 8; rows = 4\nfor i in range(1, columns*rows +1):\n    fig.add_subplot(rows, columns, i)\n    draw(parsed[normal['patientId'].loc[i]])", "processed": ["normal exampl"]}, {"markdown": ["# Side By Side Compare of Opacity/Not Normal/Normal"], "code": "# Reference: https://www.kaggle.com/code/robikscube/eda-lets-detect-pneumonia-explore-lung-images\n\nfig=plt.figure(figsize=(20, 10))\ncolumns = 3; rows = 1\nfig.add_subplot(rows, columns, 1).set_title(\"Normal\", fontsize=30)\ndraw(parsed[normal['patientId'].unique()[0]])\nfig.add_subplot(rows, columns, 2).set_title(\"Not Normal\", fontsize=30)\n# ax2.set_title(\"Not Normal\", fontsize=30)\ndraw(parsed[not_normal['patientId'].unique()[0]])\nfig.add_subplot(rows, columns, 3).set_title(\"Opacity\", fontsize=30)\n# ax3.set_title(\"Opacity\", fontsize=30)\ndraw(parsed[opacity['patientId'].unique()[0]])\nfig=plt.figure(figsize=(20, 10))\ncolumns = 3; rows = 1\nfig.add_subplot(rows, columns, 1).set_title(\"Normal\", fontsize=30)\ndraw(parsed[normal['patientId'].unique()[1]])\nfig.add_subplot(rows, columns, 2).set_title(\"Not Normal\", fontsize=30)\n# ax2.set_title(\"Not Normal\", fontsize=30)\ndraw(parsed[not_normal['patientId'].unique()[1]])\nfig.add_subplot(rows, columns, 3).set_title(\"Opacity\", fontsize=30)\n# ax3.set_title(\"Opacity\", fontsize=30)\ndraw(parsed[opacity['patientId'].unique()[1]])\nfig=plt.figure(figsize=(20, 10))\ncolumns = 3; rows = 1\nfig.add_subplot(rows, columns, 1).set_title(\"Normal\", fontsize=30)\ndraw(parsed[normal['patientId'].unique()[2]])\nfig.add_subplot(rows, columns, 2).set_title(\"Not Normal\", fontsize=30)\n# ax2.set_title(\"Not Normal\", fontsize=30)\ndraw(parsed[not_normal['patientId'].unique()[2]])\nfig.add_subplot(rows, columns, 3).set_title(\"Opacity\", fontsize=30)\n# ax3.set_title(\"Opacity\", fontsize=30)\ndraw(parsed[opacity['patientId'].unique()[2]])", "processed": ["side side compar opac normal normal"]}, {"markdown": ["* We have almost 15 thousands dogs and cats in the dataset;\n* Main dataset contains all important information about pets: age, breed, color, some characteristics and other things;\n* Desctiptions were analyzed using Google's Natural Language API providing sentiments and entities. I suppose we could do a similar thing ourselves;\n* There are photos of some pets;\n* Some meta-information was extracted from images and we can use it;\n* There are separate files with labels for breeds, colors and states;\n\nLet's start with the main dataset.\n\nI have also created a full dataset by combining train and test data. This is done purely for more convenient visualization. Column \"dataset_type\" shows which dataset the data belongs to.", "## Main data exploration", "### Target: Adoption speed\n\n* 0 - Pet was adopted on the same day as it was listed.\n* 1 - Pet was adopted between 1 and 7 days (1st week) after being listed.\n* 2 - Pet was adopted between 8 and 30 days (1st month) after being listed.\n* 3 - Pet was adopted between 31 and 90 days (2nd & 3rd month) after being listed.\n* 4 - No adoption after 100 days of being listed. (There are no pets in this dataset that waited between 90 and 100 days). "], "code": "# Reference: https://www.kaggle.com/code/artgor/exploration-of-data-step-by-step\n\ntrain['AdoptionSpeed'].value_counts().sort_index().plot('barh', color='teal');\nplt.title('Adoption speed classes counts');", "processed": ["almost 15 thousand dog cat dataset main dataset contain import inform pet age breed color characterist thing desctipt analyz use googl natur languag api provid sentiment entiti suppos could similar thing photo pet meta inform extract imag use separ file label breed color state let start main dataset also creat full dataset combin train test data done pure conveni visual column dataset type show dataset data belong", "main data explor", "target adopt speed 0 pet adopt day list 1 pet adopt 1 7 day 1st week list 2 pet adopt 8 30 day 1st month list 3 pet adopt 31 90 day 2nd 3rd month list 4 adopt 100 day list pet dataset wait 90 100 day"]}, {"markdown": ["A small note on how annotating works:\n* When I use seaborn countplot, I assign the figure to a variable - this allows to change its attributes and go deeper into its parameters;\n* Figure has `Axes` - bars - which contain information about color, transparency and other parameters;\n* And `patches` in `Axes` contain this information;\n* So we can take information from 'patches`, for example width and height of each bar, and plot correct text in correct places\n\nhttps://matplotlib.org/users/artists.html"], "code": "# Reference: https://www.kaggle.com/code/artgor/exploration-of-data-step-by-step\n\nplt.figure(figsize=(14, 6));\ng = sns.countplot(x='AdoptionSpeed', data=all_data.loc[all_data['dataset_type'] == 'train']);\nplt.title('Adoption speed classes rates');\nax=g.axes\n#Axes\nax\n# patches\nax.patches\n# example of info in patches\nax.patches[0].get_x()\nplt.figure(figsize=(14, 6));\ng = sns.countplot(x='AdoptionSpeed', data=all_data.loc[all_data['dataset_type'] == 'train'])\nplt.title('Adoption speed classes rates');\nax=g.axes\nfor p in ax.patches:\n     ax.annotate(f\"{p.get_height() * 100 / train.shape[0]:.2f}%\", (p.get_x() + p.get_width() / 2., p.get_height()),\n         ha='center', va='center', fontsize=11, color='gray', rotation=0, xytext=(0, 10),\n         textcoords='offset points')  ", "processed": ["small note annot work use seaborn countplot assign figur variabl allow chang attribut go deeper paramet figur axe bar contain inform color transpar paramet patch axe contain inform take inform patch exampl width height bar plot correct text correct place http matplotlib org user artist html"]}, {"markdown": ["We can see that some pets were adopted immediately, but these are rare cases: maybe someone wanted to adopt any pet, or the pet was lucky to be seen by person, who wanted a similar pet.\nA lot of pets aren't adopted at all, which is quite sad :( I hope our models and analysis will help them to find their home!\n\nIt is nice that a lot of pets are adopted within a first week of being listed!\n\nOne more interesting thing is that the classes have a linear relationship - the higher the number, the worse situation is. So it could be possible to build not only multiclass classification, but also regression.", "### Type\n1 - Dog, 2 - Cat"], "code": "# Reference: https://www.kaggle.com/code/artgor/exploration-of-data-step-by-step\n\nall_data['Type'] = all_data['Type'].apply(lambda x: 'Dog' if x == 1 else 'Cat')\nplt.figure(figsize=(10, 6));\nsns.countplot(x='dataset_type', data=all_data, hue='Type');\nplt.title('Number of cats and dogs in train and test data');", "processed": ["see pet adopt immedi rare case mayb someon want adopt pet pet lucki seen person want similar pet lot pet adopt quit sad hope model analysi help find home nice lot pet adopt within first week list one interest thing class linear relationship higher number wors situat could possibl build multiclass classif also regress", "type 1 dog 2 cat"]}, {"markdown": ["We can see that the rate of dogs in train dataset is higher that in test set. But I don't think the difference is seriuos.", "#### Comparison of rates\n\nFrom here on I'll compare not only counts of pets in different categories, but also compate adoption speed rates with base ones.\n\nThis is how it works:\n* As we saw earlier the base rate of pets with Adoption speed 0 is 410 / 14993 = 0.027;\n* Now look at the next graph: there are 6861 cats in train dataset and 240 of them have Adoption Speed 0. So the rate is 240 / 6861 = 0.035;\n* 0.035/0.027 = 1.28, so by splitting out the data to cat vs dog, we can see that cats have a 28% increased chance of adoption speed class 0 over the base rate of adoption;"], "code": "# Reference: https://www.kaggle.com/code/artgor/exploration-of-data-step-by-step\n\nmain_count = train['AdoptionSpeed'].value_counts(normalize=True).sort_index()\ndef prepare_plot_dict(df, col, main_count):\n    \"\"\"\n    Preparing dictionary with data for plotting.\n    \n    I want to show how much higher/lower are the rates of Adoption speed for the current column comparing to base values (as described higher),\n    At first I calculate base rates, then for each category in the column I calculate rates of Adoption speed and find difference with the base rates.\n    \n    \"\"\"\n    main_count = dict(main_count)\n    plot_dict = {}\n    for i in df[col].unique():\n        val_count = dict(df.loc[df[col] == i, 'AdoptionSpeed'].value_counts().sort_index())\n\n        for k, v in main_count.items():\n            if k in val_count:\n                plot_dict[val_count[k]] = ((val_count[k] / sum(val_count.values())) / main_count[k]) * 100 - 100\n            else:\n                plot_dict[0] = 0\n\n    return plot_dict\n\ndef make_count_plot(df, x, hue='AdoptionSpeed', title='', main_count=main_count):\n    \"\"\"\n    Plotting countplot with correct annotations.\n    \"\"\"\n    g = sns.countplot(x=x, data=df, hue=hue);\n    plt.title(f'AdoptionSpeed {title}');\n    ax = g.axes\n\n    plot_dict = prepare_plot_dict(df, x, main_count)\n\n    for p in ax.patches:\n        h = p.get_height() if str(p.get_height()) != 'nan' else 0\n        text = f\"{plot_dict[h]:.0f}%\" if plot_dict[h] < 0 else f\"+{plot_dict[h]:.0f}%\"\n        ax.annotate(text, (p.get_x() + p.get_width() / 2., h),\n             ha='center', va='center', fontsize=11, color='green' if plot_dict[h] > 0 else 'red', rotation=0, xytext=(0, 10),\n             textcoords='offset points')  \nplt.figure(figsize=(18, 8));\nmake_count_plot(df=all_data.loc[all_data['dataset_type'] == 'train'], x='Type', title='by pet Type')", "processed": ["see rate dog train dataset higher test set think differ seriuo", "comparison rate compar count pet differ categori also compat adopt speed rate base one work saw earlier base rate pet adopt speed 0 410 14993 0 027 look next graph 6861 cat train dataset 240 adopt speed 0 rate 240 6861 0 035 0 035 0 027 1 28 split data cat v dog see cat 28 increas chanc adopt speed class 0 base rate adopt"]}, {"markdown": ["We can see that cats are more likely to be adopted early than dogs and overall the percentage of not adopted cats is lower. Does this mean people prefer cats? Or maybe this dataset is small and could contain bias.\nOn the other hand more dogs are adopted after several months.", "### Name\nI don't really think that names are important in adoption, but let's see.\n\nAt first let's look at most common names."], "code": "# Reference: https://www.kaggle.com/code/artgor/exploration-of-data-step-by-step\n\nfig, ax = plt.subplots(figsize = (16, 12))\nplt.subplot(1, 2, 1)\ntext_cat = ' '.join(all_data.loc[all_data['Type'] == 'Cat', 'Name'].fillna('').values)\nwordcloud = WordCloud(max_font_size=None, background_color='white',\n                      width=1200, height=1000).generate(text_cat)\nplt.imshow(wordcloud)\nplt.title('Top cat names')\nplt.axis(\"off\")\n\nplt.subplot(1, 2, 2)\ntext_dog = ' '.join(all_data.loc[all_data['Type'] == 'Dog', 'Name'].fillna('').values)\nwordcloud = WordCloud(max_font_size=None, background_color='white',\n                      width=1200, height=1000).generate(text_dog)\nplt.imshow(wordcloud)\nplt.title('Top dog names')\nplt.axis(\"off\")\n\nplt.show()", "processed": ["see cat like adopt earli dog overal percentag adopt cat lower mean peopl prefer cat mayb dataset small could contain bia hand dog adopt sever month", "name realli think name import adopt let see first let look common name"]}, {"markdown": ["Less than 10% of pets don't have names, but they have a higher possibility of not being adopted."], "code": "# Reference: https://www.kaggle.com/code/artgor/exploration-of-data-step-by-step\n\nplt.figure(figsize=(18, 8));\nmake_count_plot(df=all_data.loc[all_data['dataset_type'] == 'train'], x='No_name', title='and having a name')", "processed": ["le 10 pet name higher possibl adopt"]}, {"markdown": ["I think that we could create a new feature, showing that name is meaningless - pets with these names could have less success in adoption.", "### Age"], "code": "# Reference: https://www.kaggle.com/code/artgor/exploration-of-data-step-by-step\n\nfig, ax = plt.subplots(figsize = (16, 6))\nplt.subplot(1, 2, 1)\nplt.title('Distribution of pets age');\ntrain['Age'].plot('hist', label='train');\ntest['Age'].plot('hist', label='test');\nplt.legend();\n\nplt.subplot(1, 2, 2)\nplt.title('Distribution of pets age (log)');\nnp.log1p(train['Age']).plot('hist', label='train');\nnp.log1p(test['Age']).plot('hist', label='test');\nplt.legend();\ntrain['Age'].value_counts().head(10)", "processed": ["think could creat new featur show name meaningless pet name could le success adopt", "age"]}, {"markdown": ["We can see that most pets are young - maybe after the birth. Also there a lot of pets with an age equal to multiples of 12 - I think than owners didn't bother with the exact age."], "code": "# Reference: https://www.kaggle.com/code/artgor/exploration-of-data-step-by-step\n\nplt.figure(figsize=(10, 6));\nsns.violinplot(x=\"AdoptionSpeed\", y=\"Age\", hue=\"Type\", data=train);\nplt.title('AdoptionSpeed by Type and age');\ndata = []\nfor a in range(5):\n    df = train.loc[train['AdoptionSpeed'] == a]\n\n    data.append(go.Scatter(\n        x = df['Age'].value_counts().sort_index().index,\n        y = df['Age'].value_counts().sort_index().values,\n        name = str(a)\n    ))\n    \nlayout = go.Layout(dict(title = \"AdoptionSpeed trends by Age\",\n                  xaxis = dict(title = 'Age (months)'),\n                  yaxis = dict(title = 'Counts'),\n                  )\n                  )\npy.iplot(dict(data=data, layout=layout), filename='basic-line')", "processed": ["see pet young mayb birth also lot pet age equal multipl 12 think owner bother exact age"]}, {"markdown": ["* We can see that young pets are adopted quite fast and most of them are adopted;\n* Most pets are less than 4 months old with a huge spike at 2 months;\n* It seems that a lot of people don't input exact age and write age in years (or multiples of 12);\n* It could make sense to create some binary variables based on age;", "### Breeds\nThere is a main breed of the pet and secondary if relevant.\n\nAt first let's see whether having secondary breed influences adoption speed."], "code": "# Reference: https://www.kaggle.com/code/artgor/exploration-of-data-step-by-step\n\ntrain['Pure_breed'] = 0\ntrain.loc[train['Breed2'] == 0, 'Pure_breed'] = 1\ntest['Pure_breed'] = 0\ntest.loc[test['Breed2'] == 0, 'Pure_breed'] = 1\nall_data['Pure_breed'] = 0\nall_data.loc[all_data['Breed2'] == 0, 'Pure_breed'] = 1\n\nprint(f\"Rate of pure breed pets in train data: {train['Pure_breed'].sum() * 100 / train['Pure_breed'].shape[0]:.4f}%.\")\nprint(f\"Rate of pure breed pets in test data: {test['Pure_breed'].sum() * 100 / test['Pure_breed'].shape[0]:.4f}%.\")\ndef plot_four_graphs(col='', main_title='', dataset_title=''):\n    \"\"\"\n    Plotting four graphs:\n    - adoption speed by variable;\n    - counts of categories in the variable in train and test;\n    - adoption speed by variable for dogs;\n    - adoption speed by variable for cats;    \n    \"\"\"\n    plt.figure(figsize=(20, 12));\n    plt.subplot(2, 2, 1)\n    make_count_plot(df=train, x=col, title=f'and {main_title}')\n\n    plt.subplot(2, 2, 2)\n    sns.countplot(x='dataset_type', data=all_data, hue=col);\n    plt.title(dataset_title);\n\n    plt.subplot(2, 2, 3)\n    make_count_plot(df=train.loc[train['Type'] == 1], x=col, title=f'and {main_title} for dogs')\n\n    plt.subplot(2, 2, 4)\n    make_count_plot(df=train.loc[train['Type'] == 2], x=col, title=f'and {main_title} for cats')\n    \nplot_four_graphs(col='Pure_breed', main_title='having pure breed', dataset_title='Number of pets by pure/not-pure breed in train and test data')", "processed": ["see young pet adopt quit fast adopt pet le 4 month old huge spike 2 month seem lot peopl input exact age write age year multipl 12 could make sen creat binari variabl base age", "breed main breed pet secondari relev first let see whether secondari breed influenc adopt speed"]}, {"markdown": ["It seems that non-pure breed pets tend to be adopted more and faster, especially cats.\n\nLet's look at the breeds themselves"], "code": "# Reference: https://www.kaggle.com/code/artgor/exploration-of-data-step-by-step\n\nbreeds_dict = {k: v for k, v in zip(breeds['BreedID'], breeds['BreedName'])}\ntrain['Breed1_name'] = train['Breed1'].apply(lambda x: '_'.join(breeds_dict[x].split()) if x in breeds_dict else 'Unknown')\ntrain['Breed2_name'] = train['Breed2'].apply(lambda x: '_'.join(breeds_dict[x]) if x in breeds_dict else '-')\n\ntest['Breed1_name'] = test['Breed1'].apply(lambda x: '_'.join(breeds_dict[x].split()) if x in breeds_dict else 'Unknown')\ntest['Breed2_name'] = test['Breed2'].apply(lambda x: '_'.join(breeds_dict[x].split()) if x in breeds_dict else '-')\n\nall_data['Breed1_name'] = all_data['Breed1'].apply(lambda x: '_'.join(breeds_dict[x].split()) if x in breeds_dict else 'Unknown')\nall_data['Breed2_name'] = all_data['Breed2'].apply(lambda x: '_'.join(breeds_dict[x].split()) if x in breeds_dict else '-')\nfig, ax = plt.subplots(figsize = (20, 18))\nplt.subplot(2, 2, 1)\ntext_cat1 = ' '.join(all_data.loc[all_data['Type'] == 'Cat', 'Breed1_name'].fillna('').values)\nwordcloud = WordCloud(max_font_size=None, background_color='black', collocations=False,\n                      width=1200, height=1000).generate(text_cat1)\nplt.imshow(wordcloud)\nplt.title('Top cat breed1')\nplt.axis(\"off\")\n\nplt.subplot(2, 2, 2)\ntext_dog1 = ' '.join(all_data.loc[all_data['Type'] == 'Dog', 'Breed1_name'].fillna('').values)\nwordcloud = WordCloud(max_font_size=None, background_color='black', collocations=False,\n                      width=1200, height=1000).generate(text_dog1)\nplt.imshow(wordcloud)\nplt.title('Top dog breed1')\nplt.axis(\"off\")\n\nplt.subplot(2, 2, 3)\ntext_cat2 = ' '.join(all_data.loc[all_data['Type'] == 'Cat', 'Breed2_name'].fillna('').values)\nwordcloud = WordCloud(max_font_size=None, background_color='black', collocations=False,\n                      width=1200, height=1000).generate(text_cat2)\nplt.imshow(wordcloud)\nplt.title('Top cat breed1')\nplt.axis(\"off\")\n\nplt.subplot(2, 2, 4)\ntext_dog2 = ' '.join(all_data.loc[all_data['Type'] == 'Dog', 'Breed2_name'].fillna('').values)\nwordcloud = WordCloud(max_font_size=None, background_color='black', collocations=False,\n                      width=1200, height=1000).generate(text_dog2)\nplt.imshow(wordcloud)\nplt.title('Top dog breed2')\nplt.axis(\"off\")\nplt.show()", "processed": ["seem non pure breed pet tend adopt faster especi cat let look breed"]}, {"markdown": ["It seems that most dogs aren't pure breeds, but mixed breeds! My first assumption was wrong.\n\nSometimes people write \"mixed breed\" in the first fiels, sometimes in both, and sometimes main breed is in the first field and is marked as mixed breed in the second field.\n\nI think we can create new features based on this information. And later we can verify the hair length of pets.", "### Gender\n 1 = Male, 2 = Female, 3 = Mixed, if profile represents group of pets"], "code": "# Reference: https://www.kaggle.com/code/artgor/exploration-of-data-step-by-step\n\nplt.figure(figsize=(18, 6));\nplt.subplot(1, 2, 1)\nmake_count_plot(df=train, x='Gender', title='and gender')\n\nplt.subplot(1, 2, 2)\nsns.countplot(x='dataset_type', data=all_data, hue='Gender');\nplt.title('Number of pets by gender in train and test data');\nsns.factorplot('Type', col='Gender', data=all_data, kind='count', hue='dataset_type');\nplt.subplots_adjust(top=0.8)\nplt.suptitle('Count of cats and dogs in train and test set by gender');", "processed": ["seem dog pure breed mix breed first assumpt wrong sometim peopl write mix breed first fiel sometim sometim main breed first field mark mix breed second field think creat new featur base inform later verifi hair length pet", "gender 1 male 2 femal 3 mix profil repres group pet"]}, {"markdown": ["It seems that male pets are adopted faster than female. Having no information about the gender really decreases chances.", "### Colors"], "code": "# Reference: https://www.kaggle.com/code/artgor/exploration-of-data-step-by-step\n\ncolors_dict = {k: v for k, v in zip(colors['ColorID'], colors['ColorName'])}\ntrain['Color1_name'] = train['Color1'].apply(lambda x: colors_dict[x] if x in colors_dict else '')\ntrain['Color2_name'] = train['Color2'].apply(lambda x: colors_dict[x] if x in colors_dict else '')\ntrain['Color3_name'] = train['Color3'].apply(lambda x: colors_dict[x] if x in colors_dict else '')\n\ntest['Color1_name'] = test['Color1'].apply(lambda x: colors_dict[x] if x in colors_dict else '')\ntest['Color2_name'] = test['Color2'].apply(lambda x: colors_dict[x] if x in colors_dict else '')\ntest['Color3_name'] = test['Color3'].apply(lambda x: colors_dict[x] if x in colors_dict else '')\n\nall_data['Color1_name'] = all_data['Color1'].apply(lambda x: colors_dict[x] if x in colors_dict else '')\nall_data['Color2_name'] = all_data['Color2'].apply(lambda x: colors_dict[x] if x in colors_dict else '')\nall_data['Color3_name'] = all_data['Color3'].apply(lambda x: colors_dict[x] if x in colors_dict else '')\ndef make_factor_plot(df, x, col, title, main_count=main_count, hue=None, ann=True, col_wrap=4):\n    \"\"\"\n    Plotting countplot.\n    Making annotations is a bit more complicated, because we need to iterate over axes.\n    \"\"\"\n    if hue:\n        g = sns.factorplot(col, col=x, data=df, kind='count', col_wrap=col_wrap, hue=hue);\n    else:\n        g = sns.factorplot(col, col=x, data=df, kind='count', col_wrap=col_wrap);\n    plt.subplots_adjust(top=0.9);\n    plt.suptitle(title);\n    ax = g.axes\n    plot_dict = prepare_plot_dict(df, x, main_count)\n    if ann:\n        for a in ax:\n            for p in a.patches:\n                text = f\"{plot_dict[p.get_height()]:.0f}%\" if plot_dict[p.get_height()] < 0 else f\"+{plot_dict[p.get_height()]:.0f}%\"\n                a.annotate(text, (p.get_x() + p.get_width() / 2., p.get_height()),\n                     ha='center', va='center', fontsize=11, color='green' if plot_dict[p.get_height()] > 0 else 'red', rotation=0, xytext=(0, 10),\n                     textcoords='offset points')  \nsns.factorplot('dataset_type', col='Type', data=all_data, kind='count', hue='Color1_name', palette=['Black', 'Brown', '#FFFDD0', 'Gray', 'Gold', 'White', 'Yellow']);\nplt.subplots_adjust(top=0.8)\nplt.suptitle('Counts of pets in datasets by main color');", "processed": ["seem male pet adopt faster femal inform gender realli decreas chanc", "color"]}, {"markdown": ["We can see that most common colors are black and brown. Interesting to notice that there are almost no gray or yellow dogs :)\n\nNow let's see whether colors influence adoption speed"], "code": "# Reference: https://www.kaggle.com/code/artgor/exploration-of-data-step-by-step\n\nmake_factor_plot(df=train, x='Color1_name', col='AdoptionSpeed', title='Counts of pets by main color and Adoption Speed')\ntrain['full_color'] = (train['Color1_name'] + '__' + train['Color2_name'] + '__' + train['Color3_name']).str.replace('__', '')\ntest['full_color'] = (test['Color1_name'] + '__' + test['Color2_name'] + '__' + test['Color3_name']).str.replace('__', '')\nall_data['full_color'] = (all_data['Color1_name'] + '__' + all_data['Color2_name'] + '__' + all_data['Color3_name']).str.replace('__', '')\n\nmake_factor_plot(df=train.loc[train['full_color'].isin(list(train['full_color'].value_counts().index)[:12])], x='full_color', col='AdoptionSpeed', title='Counts of pets by color and Adoption Speed')", "processed": ["see common color black brown interest notic almost gray yellow dog let see whether color influenc adopt speed"]}, {"markdown": ["### MatiritySize\nSize at maturity (1 = Small, 2 = Medium, 3 = Large, 4 = Extra Large, 0 = Not Specified)"], "code": "# Reference: https://www.kaggle.com/code/artgor/exploration-of-data-step-by-step\n\nplot_four_graphs(col='MaturitySize', main_title='MaturitySize', dataset_title='Number of pets by MaturitySize in train and test data')\nmake_factor_plot(df=all_data, x='MaturitySize', col='Type', title='Count of cats and dogs in train and test set by MaturitySize', hue='dataset_type', ann=False)\nimages = [i.split('-')[0] for i in os.listdir('../input/train_images/')]\nsize_dict = {1: 'Small', 2: 'Medium', 3: 'Large', 4: 'Extra Large'}\nfor t in all_data['Type'].unique():\n    for m in all_data['MaturitySize'].unique():\n        df = all_data.loc[(all_data['Type'] == t) & (all_data['MaturitySize'] == m)]\n        top_breeds = list(df['Breed1_name'].value_counts().index)[:5]\n        m = size_dict[m]\n        print(f\"Most common Breeds of {m} {t}s:\")\n        \n        fig = plt.figure(figsize=(25, 4))\n        \n        for i, breed in enumerate(top_breeds):\n            # excluding pets without pictures\n            b_df = df.loc[(df['Breed1_name'] == breed) & (df['PetID'].isin(images)), 'PetID']\n            if len(b_df) > 1:\n                pet_id = b_df.values[1]\n            else:\n                pet_id = b_df.values[0]\n            ax = fig.add_subplot(1, 5, i+1, xticks=[], yticks=[])\n\n            im = Image.open(\"../input/train_images/\" + pet_id + '-1.jpg')\n            plt.imshow(im)\n            ax.set_title(f'Breed: {breed}')\n        plt.show();", "processed": ["matiritys size matur 1 small 2 medium 3 larg 4 extra larg 0 specifi"]}, {"markdown": ["* We can see that most of the pets have short fur and long fur is the least common;\n* Pets with long hair tend to have a higher chance of being adopted. Though it could be because of randomness due to low count;\n\nAs I said earlier, some breed have hair length in the text, let's check these values!"], "code": "# Reference: https://www.kaggle.com/code/artgor/exploration-of-data-step-by-step\n\nfig, ax = plt.subplots(figsize = (20, 18))\nplt.subplot(2, 2, 1)\ntext_cat1 = ' '.join(all_data.loc[(all_data['FurLength'] == 1) & (all_data['Type'] == 'Cat'), 'Breed1_name'].fillna('').values)\nwordcloud = WordCloud(max_font_size=None, background_color='white', collocations=False,\n                      width=1200, height=1000).generate(text_cat1)\nplt.imshow(wordcloud)\nplt.title('Top cat breed1 with short fur')\nplt.axis(\"off\")\n\nplt.subplot(2, 2, 2)\ntext_dog1 = ' '.join(all_data.loc[(all_data['FurLength'] == 1) & (all_data['Type'] == 'Dog'), 'Breed1_name'].fillna('').values)\nwordcloud = WordCloud(max_font_size=None, background_color='white', collocations=False,\n                      width=1200, height=1000).generate(text_dog1)\nplt.imshow(wordcloud)\nplt.title('Top dog breed1 with short fur')\nplt.axis(\"off\")\n\nplt.subplot(2, 2, 3)\ntext_cat2 = ' '.join(all_data.loc[(all_data['FurLength'] == 2) & (all_data['Type'] == 'Cat'), 'Breed1_name'].fillna('').values)\nwordcloud = WordCloud(max_font_size=None, background_color='white', collocations=False,\n                      width=1200, height=1000).generate(text_cat2)\nplt.imshow(wordcloud)\nplt.title('Top cat breed1 with medium fur')\nplt.axis(\"off\")\n\nplt.subplot(2, 2, 4)\ntext_dog2 = ' '.join(all_data.loc[(all_data['FurLength'] == 2) & (all_data['Type'] == 'Dog'), 'Breed1_name'].fillna('').values)\nwordcloud = WordCloud(max_font_size=None, background_color='white', collocations=False,\n                      width=1200, height=1000).generate(text_dog2)\nplt.imshow(wordcloud)\nplt.title('Top dog breed2 with medium fur')\nplt.axis(\"off\")\nplt.show()\nc = 0\nstrange_pets = []\nfor i, row in all_data[all_data['Breed1_name'].str.contains('air')].iterrows():\n    if 'Short' in row['Breed1_name'] and row['FurLength'] == 1:\n        pass\n    elif 'Medium' in row['Breed1_name'] and row['FurLength'] == 2:\n        pass\n    elif 'Long' in row['Breed1_name'] and row['FurLength'] == 3:\n        pass\n    else:\n        c += 1\n        strange_pets.append((row['PetID'], row['Breed1_name'], row['FurLength']))\n        \nprint(f\"There are {c} pets whose breed and fur length don't match\")", "processed": ["see pet short fur long fur least common pet long hair tend higher chanc adopt though could random due low count said earlier breed hair length text let check valu"]}, {"markdown": ["It seems that almost one thousand pets have mismatch in breeds and fur lengths. Let's see!"], "code": "# Reference: https://www.kaggle.com/code/artgor/exploration-of-data-step-by-step\n\nstrange_pets = [p for p in strange_pets if p[0] in images]\nfig = plt.figure(figsize=(25, 12))\nfur_dict = {1: 'Short', 2: 'Medium', 3: 'long'}\nfor i, s in enumerate(random.sample(strange_pets, 12)):\n    ax = fig.add_subplot(3, 4, i+1, xticks=[], yticks=[])\n\n    im = Image.open(\"../input/train_images/\" + s[0] + '-1.jpg')\n    plt.imshow(im)\n    ax.set_title(f'Breed: {s[1]} \\n Fur length: {fur_dict[s[2]]}')\nplt.show();", "processed": ["seem almost one thousand pet mismatch breed fur length let see"]}, {"markdown": ["Everybody lies!\n\nSometimes breed is more correct, sometimes fur length... I suppose we could create a feature showing whether breed and fur length match.", "### Health\n\nThere are four features showing health of the pets:\n\n* Vaccinated - Pet has been vaccinated (1 = Yes, 2 = No, 3 = Not Sure)\n* Dewormed - Pet has been dewormed (1 = Yes, 2 = No, 3 = Not Sure)\n* Sterilized - Pet has been spayed / neutered (1 = Yes, 2 = No, 3 = Not Sure)\n* Health - Health Condition (1 = Healthy, 2 = Minor Injury, 3 = Serious Injury, 0 = Not Specified)\n\nI think that these features are very important - most people would prefer a healthy pet. While sterilization isn't the main concern, having healty and dewormed pet should have a great importance. Let's see whether I'm right!"], "code": "# Reference: https://www.kaggle.com/code/artgor/exploration-of-data-step-by-step\n\nplt.figure(figsize=(20, 12));\nplt.subplot(2, 2, 1)\nmake_count_plot(df=train, x='Vaccinated', title='Vaccinated')\nplt.xticks([0, 1, 2], ['Yes', 'No', 'Not sure']);\nplt.title('AdoptionSpeed and Vaccinated');\n\nplt.subplot(2, 2, 2)\nmake_count_plot(df=train, x='Dewormed', title='Dewormed')\nplt.xticks([0, 1, 2], ['Yes', 'No', 'Not sure']);\nplt.title('AdoptionSpeed and Dewormed');\n\nplt.subplot(2, 2, 3)\nmake_count_plot(df=train, x='Sterilized', title='Sterilized')\nplt.xticks([0, 1, 2], ['Yes', 'No', 'Not sure']);\nplt.title('AdoptionSpeed and Sterilized');\n\nplt.subplot(2, 2, 4)\nmake_count_plot(df=train, x='Health', title='Health')\nplt.xticks([0, 1, 2], ['Healthy', 'Minor Injury', 'Serious Injury']);\nplt.title('AdoptionSpeed and Health');\n\nplt.suptitle('Adoption Speed and health conditions');", "processed": ["everybodi lie sometim breed correct sometim fur length suppos could creat featur show whether breed fur length match", "health four featur show health pet vaccin pet vaccin 1 ye 2 3 sure deworm pet deworm 1 ye 2 3 sure steril pet spay neuter 1 ye 2 3 sure health health condit 1 healthi 2 minor injuri 3 seriou injuri 0 specifi think featur import peopl would prefer healthi pet steril main concern healti deworm pet great import let see whether right"]}, {"markdown": ["* Almost all pets are healthy! Pets with minor injuries are rare and sadly they aren't adopted well. Number of pets with serious injuries is negligible.\n* It is interesting that people prefer non-vaccinated pets. Maybe they want to bring pets to vets themselves...\n* People also prefer non-sterilized pets! Maybe they want puppies/kittens :)\n* Quite important is the fact that when there is no information about health condition, the probability of not being adopted is much higher;\n\nLet's have a look at most popular health conditions."], "code": "# Reference: https://www.kaggle.com/code/artgor/exploration-of-data-step-by-step\n\ntrain['health'] = train['Vaccinated'].astype(str) + '_' + train['Dewormed'].astype(str) + '_' + train['Sterilized'].astype(str) + '_' + train['Health'].astype(str)\ntest['health'] = test['Vaccinated'].astype(str) + '_' + test['Dewormed'].astype(str) + '_' + test['Sterilized'].astype(str) + '_' + test['Health'].astype(str)\n\n\nmake_factor_plot(df=train.loc[train['health'].isin(list(train.health.value_counts().index[:5]))], x='health', col='AdoptionSpeed', title='Counts of pets by main health conditions and Adoption Speed')", "processed": ["almost pet healthi pet minor injuri rare sadli adopt well number pet seriou injuri neglig interest peopl prefer non vaccin pet mayb want bring pet vet peopl also prefer non steril pet mayb want puppi kitten quit import fact inform health condit probabl adopt much higher let look popular health condit"]}, {"markdown": ["* Healthy, dewormed and non-sterilized pets tend to be adopted faster!\n* Completely healthy pets are... more likely to be not adopted! I suppose that means that a lot of people pay attention to other characteristics;\n* And healthy pets with no information (not sure value) also tend to be adopted less frequently. Maybe people prefer having information, even if it is negative;"], "code": "# Reference: https://www.kaggle.com/code/artgor/exploration-of-data-step-by-step\n\nplt.figure(figsize=(20, 16))\nplt.subplot(3, 2, 1)\nsns.violinplot(x=\"AdoptionSpeed\", y=\"Age\", data=train);\nplt.title('Age distribution by Age');\nplt.subplot(3, 2, 3)\nsns.violinplot(x=\"AdoptionSpeed\", y=\"Age\", hue=\"Vaccinated\", data=train);\nplt.title('Age distribution by Age and Vaccinated');\nplt.subplot(3, 2, 4)\nsns.violinplot(x=\"AdoptionSpeed\", y=\"Age\", hue=\"Dewormed\", data=train);\nplt.title('Age distribution by Age and Dewormed');\nplt.subplot(3, 2, 5)\nsns.violinplot(x=\"AdoptionSpeed\", y=\"Age\", hue=\"Sterilized\", data=train);\nplt.title('Age distribution by Age and Sterilized');\nplt.subplot(3, 2, 6)\nsns.violinplot(x=\"AdoptionSpeed\", y=\"Age\", hue=\"Health\", data=train);\nplt.title('Age distribution by Age and Health');", "processed": ["healthi deworm non steril pet tend adopt faster complet healthi pet like adopt suppos mean lot peopl pay attent characterist healthi pet inform sure valu also tend adopt le frequent mayb peopl prefer inform even neg"]}, {"markdown": ["Most pets are free and it seems that asking for a fee slightly desreased the chance of adoption. Also free cats are adopted faster than free dogs"], "code": "# Reference: https://www.kaggle.com/code/artgor/exploration-of-data-step-by-step\n\nall_data.sort_values('Fee', ascending=False)[['Name', 'Description', 'Fee', 'AdoptionSpeed', 'dataset_type']].head(10)\nplt.figure(figsize=(16, 6));\nplt.subplot(1, 2, 1)\nplt.hist(train.loc[train['Fee'] < 400, 'Fee']);\nplt.title('Distribution of fees lower than 400');\n\nplt.subplot(1, 2, 2)\nsns.violinplot(x=\"AdoptionSpeed\", y=\"Fee\", hue=\"Type\", data=train);\nplt.title('AdoptionSpeed by Type and Fee');", "processed": ["pet free seem ask fee slightli desreas chanc adopt also free cat adopt faster free dog"]}, {"markdown": ["* It is interesting that pets with high fee tend to be adopted quite fast! Maybe people prefer to pay for \"better\" pets: healthy, trained and so on;\n* Most pets are given for free and fees are usually lower than 100 $;\n* Fees for dogs tend to be higher, though these are rare cases anyway."], "code": "# Reference: https://www.kaggle.com/code/artgor/exploration-of-data-step-by-step\n\nplt.figure(figsize=(16, 10));\nsns.scatterplot(x=\"Fee\", y=\"Quantity\", hue=\"Type\",data=all_data);\nplt.title('Quantity of pets and Fee');", "processed": ["interest pet high fee tend adopt quit fast mayb peopl prefer pay better pet healthi train pet given free fee usual lower 100 fee dog tend higher though rare case anyway"]}, {"markdown": ["Sadly I don't know anything about Malaysia\u2019s states, so I can only say that top three states account for ~90% of ads. Let's have a look at them."], "code": "# Reference: https://www.kaggle.com/code/artgor/exploration-of-data-step-by-step\n\nmake_factor_plot(df=train.loc[train['State_name'].isin(list(train.State_name.value_counts().index[:3]))], x='State_name', col='AdoptionSpeed', title='Counts of pets by states and Adoption Speed')", "processed": ["sadli know anyth malaysia state say top three state account 90 ad let look"]}, {"markdown": ["Top-5 resquers managed a lot of pets!\nI wonder whether these are individual people or organizations. Let's have a look at them."], "code": "# Reference: https://www.kaggle.com/code/artgor/exploration-of-data-step-by-step\n\nmake_factor_plot(df=train.loc[train['RescuerID'].isin(list(train.RescuerID.value_counts().index[:5]))], x='RescuerID', col='AdoptionSpeed', title='Counts of pets by rescuers and Adoption Speed', col_wrap=5)", "processed": ["top 5 resquer manag lot pet wonder whether individu peopl organ let look"]}, {"markdown": ["Hm. In most cases there are no videos at all. Sometimes there is one video, more than one video is quite rare. We don't have videos and considering a huge disbalance in values I'm not sure this variable will be useful.", "### PhotoAmt"], "code": "# Reference: https://www.kaggle.com/code/artgor/exploration-of-data-step-by-step\n\nprint(F'Maximum amount of photos in {train[\"PhotoAmt\"].max()}')\ntrain['PhotoAmt'].value_counts().head()\nmake_factor_plot(df=train.loc[train['PhotoAmt'].isin(list(train.PhotoAmt.value_counts().index[:5]))], x='PhotoAmt', col='AdoptionSpeed', title='Counts of pets by PhotoAmt and Adoption Speed', col_wrap=5)\nplt.figure(figsize=(16, 6));\nplt.subplot(1, 2, 1)\nplt.hist(train['PhotoAmt']);\nplt.title('Distribution of PhotoAmt');\n\nplt.subplot(1, 2, 2)\nsns.violinplot(x=\"AdoptionSpeed\", y=\"PhotoAmt\", hue=\"Type\", data=train);\nplt.title('AdoptionSpeed by Type and PhotoAmt');", "processed": ["hm case video sometim one video one video quit rare video consid huge disbal valu sure variabl use", "photoamt"]}, {"markdown": ["Some words/phrases seem to be useful, but it seems that different adoption speed classes could have similar important words..."], "code": "# Reference: https://www.kaggle.com/code/artgor/exploration-of-data-step-by-step\n\ntrain['Description'] = train['Description'].fillna('')\ntest['Description'] = test['Description'].fillna('')\nall_data['Description'] = all_data['Description'].fillna('')\n\ntrain['desc_length'] = train['Description'].apply(lambda x: len(x))\ntrain['desc_words'] = train['Description'].apply(lambda x: len(x.split()))\n\ntest['desc_length'] = test['Description'].apply(lambda x: len(x))\ntest['desc_words'] = test['Description'].apply(lambda x: len(x.split()))\n\nall_data['desc_length'] = all_data['Description'].apply(lambda x: len(x))\nall_data['desc_words'] = all_data['Description'].apply(lambda x: len(x.split()))\n\ntrain['averate_word_length'] = train['desc_length'] / train['desc_words']\ntest['averate_word_length'] = test['desc_length'] / test['desc_words']\nall_data['averate_word_length'] = all_data['desc_length'] / all_data['desc_words']\nplt.figure(figsize=(16, 6));\nplt.subplot(1, 2, 1)\nsns.violinplot(x=\"AdoptionSpeed\", y=\"desc_length\", hue=\"Type\", data=train);\nplt.title('AdoptionSpeed by Type and description length');\n\nplt.subplot(1, 2, 2)\nsns.violinplot(x=\"AdoptionSpeed\", y=\"desc_words\", hue=\"Type\", data=train);\nplt.title('AdoptionSpeed by Type and count of words in description');", "processed": ["word phrase seem use seem differ adopt speed class could similar import word"]}, {"markdown": ["Well, English is the most common language by far, so language feature will hardly help."], "code": "# Reference: https://www.kaggle.com/code/artgor/exploration-of-data-step-by-step\n\nplt.figure(figsize=(16, 6));\nplt.subplot(1, 2, 1)\nsns.violinplot(x=\"AdoptionSpeed\", y=\"score\", hue=\"Type\", data=train);\nplt.title('AdoptionSpeed by Type and score');\n\nplt.subplot(1, 2, 2)\nsns.violinplot(x=\"AdoptionSpeed\", y=\"magnitude\", hue=\"Type\", data=train);\nplt.title('AdoptionSpeed by Type and magnitude of sentiment');", "processed": ["well english common languag far languag featur hardli help"]}, {"markdown": ["**Visualize the actual signals and noises for non-faulty cases**"], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/vsb-signal-trend-analysis-with-seasonal\n\ns = time.time()\nfor i in range(50):\n    signal = neg_signals[i]\n    short_signal = signal[indices]\n    seasons, trend = fit_seasons(short_signal)\n    e = time.time()\n    print(\"SIGNAL SAMPLE {}\".format(i+1))\n    print(\"Total time : {}\".format(str(e - s) + \" s\"))\n    \n    color = 'g'\n    plt.plot(short_signal, color)\n    plt.show()\n    print(\"Trend\")\n    plt.plot(trend, color)\n    plt.show()\n    print(\"Noise\")\n    plt.plot(short_signal - trend, color)\n    plt.show()", "processed": ["visual actual signal nois non faulti case"]}, {"markdown": ["**Visualize the actual signals and noises for faulty cases**"], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/vsb-signal-trend-analysis-with-seasonal\n\ns = time.time()\nfor i in range(50):\n    signal = pos_signals[i]\n    short_signal = signal[indices]\n    seasons, trend = fit_seasons(short_signal)\n    e = time.time()\n    print(\"SIGNAL SAMPLE {}\".format(i+1))\n    print(\"Total time : {}\".format(str(e - s) + \" s\"))\n    \n    color = 'r'\n    plt.plot(short_signal, color)\n    plt.show()\n    print(\"Trend\")\n    plt.plot(trend, color)\n    plt.show()\n    print(\"Noise\")\n    plt.plot(short_signal - trend, color)\n    plt.show()", "processed": ["visual actual signal nois faulti case"]}, {"markdown": ["# Display Class Activation Maps\nEarlier we discussed how the layer preceeding the Global Average Pooling Layer has 2048 maps of size `11x16`. Each of these maps is like a segmentation mask that specializes in detecting a certain pattern. Let's imagine that detecting `Sugar` requires the use of maps 1, 45, 256, and 1039 where 1 detects small white shapes, 45 detects circular objects, 256 detects a lattice arrangement, and 1039 detects blurred edges. Then the segmentation map for `Sugar` is the addition of these 4 maps.\n\nBelow are 25 rows of images. The images on the left are the class activation maps (formed by summing all relevent pattern detection maps from among the 2048 possible maps). These CAMs result from feeding the associated image into our Xception CNN. For each row, we display the cloud type that activated the strongest. The images of the right are the true mask in yellow and the activation map converted to a mask in blue. Remember our network has never seen the annotators' training masks! But none-the-less, our model has found masks! Truly magical"], "code": "# Reference: https://www.kaggle.com/code/cdeotte/unsupervised-masks-cv-0-60\n\n# NEW MODEL FROM OLD TO EXTRACT ACTIVATION MAPS\nall_layer_weights = model.layers[-1].get_weights()[0]\ncam_model = Model(inputs=model.input, \n        outputs=(model.layers[-3].output, model.layers[-1].output)) \n\n# DISPLAY 25 RANDOM IMAGES\nPATH = '../input/understanding_cloud_organization/train_images/'\nIMGS = os.listdir(PATH)\nfor k in np.random.randint(0,5000,25):\n    \n    # LOAD IMAGE AND PREDICT CLASS ACTIVATION MAP\n    img = cv2.resize( cv2.imread(PATH+IMGS[k]), (512, 352))\n    x = np.expand_dims(img, axis=0)/128. -1.\n    last_conv_output, pred_vec = cam_model.predict(x) \n    last_conv_output = np.squeeze(last_conv_output) \n    pred = np.argmax(pred_vec)\n    layer_weights = all_layer_weights[:, pred] \n    final_output = np.dot(last_conv_output.reshape((16*11, 2048)), layer_weights).reshape(11,16) \n    final_output = scipy.ndimage.zoom(final_output, (32, 32), order=1) \n\n    # DISPLAY IMAGE WITH CLASS ACTIVATION MAPS\n    plt.figure(figsize=(12,6))\n    plt.subplot(1,2,1)\n    mx = np.round( np.max(final_output),1 )\n    mn = np.round( np.min(final_output),1 )\n    final_output = (final_output-mn)/(mx-mn)\n    mask0 = (final_output>0.3).astype(int)\n    contour0 = mask2contour(mask0,5)\n    plt.imshow(img, alpha=0.5)\n    plt.imshow(final_output, cmap='jet', alpha=0.5)\n    plt.title('Found '+types[pred]+'  -  Pr = '+str(np.round(pred_vec[0,pred],3)) )\n    \n    # DISPLAY IMAGE WITH MASKS\n    plt.subplot(1,2,2)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    rle = train2.loc[IMGS[k].split('.')[0],'e'+str(pred+1)]\n    mask = rle2mask2X(rle,shrink=(512,352))\n    contour = mask2contour(mask,5)\n    img[contour==1,:2] = 255\n    img[contour0==1,2] = 255\n    diff = np.ones((352,512,3),dtype=np.int)*255-img\n    img=img.astype(int); img[mask0==1,:] += diff[mask0==1,:]//4\n    plt.imshow( img )\n    dice = np.round( dice_coef8(mask,mask0),3 )\n    plt.title('Dice = '+str(dice)+'  -  '+IMGS[k]+'  -  '+types[pred])\n    \n    plt.show()", "processed": ["display class activ map earlier discus layer preceed global averag pool layer 2048 map size 11x16 map like segment mask special detect certain pattern let imagin detect sugar requir use map 1 45 256 1039 1 detect small white shape 45 detect circular object 256 detect lattic arrang 1039 detect blur edg segment map sugar addit 4 map 25 row imag imag left class activ map form sum relev pattern detect map among 2048 possibl map cam result feed associ imag xception cnn row display cloud type activ strongest imag right true mask yellow activ map convert mask blue rememb network never seen annot train mask none le model found mask truli magic"]}, {"markdown": ["We can see that Africa is not as well represented as the other continents in the dataset. The two most common African languages in the dataset are Afrikaans and Somali.", "## Comment words <a id=\"1.3\"></a>\n\nNow, I will look at the number of words present in the comments.", "### Distribution of comment words"], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/jigsaw-multilingual-toxicity-eda-models\n\ndef new_len(x):\n    if type(x) is str:\n        return len(x.split())\n    else:\n        return 0\n\ntrain_data[\"comment_words\"] = train_data[\"comment_text\"].apply(new_len)\nnums = train_data.query(\"comment_words != 0 and comment_words < 200\").sample(frac=0.1)[\"comment_words\"]\nfig = ff.create_distplot(hist_data=[nums],\n                         group_labels=[\"All comments\"],\n                         colors=[\"coral\"])\n\nfig.update_layout(title_text=\"Comment words\", xaxis_title=\"Comment words\", template=\"simple_white\", showlegend=False)\nfig.show()", "processed": ["see africa well repres contin dataset two common african languag dataset afrikaan somali", "comment word id 1 3 look number word present comment", "distribut comment word"]}, {"markdown": ["In the world plot, we can see that the language with highest average negativity is Afrikaans. Also, languages from western Europe and south-east Asia tend to have higher toxicity than Hindi and Russian.", "### Negativity vs. Toxicity"], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/jigsaw-multilingual-toxicity-eda-models\n\nnums_1 = train_data.sample(frac=0.1).query(\"toxic == 1\")[\"negativity\"]\nnums_2 = train_data.sample(frac=0.1).query(\"toxic == 0\")[\"negativity\"]\n\nfig = ff.create_distplot(hist_data=[nums_1, nums_2],\n                         group_labels=[\"Toxic\", \"Non-toxic\"],\n                         colors=[\"darkorange\", \"dodgerblue\"], show_hist=False)\n\nfig.update_layout(title_text=\"Negativity vs. Toxicity\", xaxis_title=\"Negativity\", template=\"simple_white\")\nfig.show()", "processed": ["world plot see languag highest averag neg afrikaan also languag western europ south east asia tend higher toxic hindi russian", "neg v toxic"]}, {"markdown": ["In the world plot, we can see that the languages with the highest average positivity are English, Spanish, Portuguese, and Danish.", "### Positivity vs. Toxicity"], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/jigsaw-multilingual-toxicity-eda-models\n\nnums_1 = train_data.sample(frac=0.1).query(\"toxic == 1\")[\"positivity\"]\nnums_2 = train_data.sample(frac=0.1).query(\"toxic == 0\")[\"positivity\"]\n\nfig = ff.create_distplot(hist_data=[nums_1, nums_2],\n                         group_labels=[\"Toxic\", \"Non-toxic\"],\n                         colors=[\"darkorange\", \"dodgerblue\"], show_hist=False)\n\nfig.update_layout(title_text=\"Positivity vs. Toxicity\", xaxis_title=\"Positivity\", template=\"simple_white\")\nfig.show()", "processed": ["world plot see languag highest averag posit english spanish portugues danish", "posit v toxic"]}, {"markdown": ["In the world plot, we can see that the languages with the highest neutrality are Persian, Hindi, and Russian. Few western European languages like German and English seem to have a lower average neutrality than most other languages.", "### Neutrality vs. Toxicity"], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/jigsaw-multilingual-toxicity-eda-models\n\nnums_1 = train_data.sample(frac=0.1).query(\"toxic == 1\")[\"neutrality\"]\nnums_2 = train_data.sample(frac=0.1).query(\"toxic == 0\")[\"neutrality\"]\n\nfig = ff.create_distplot(hist_data=[nums_1, nums_2],\n                         group_labels=[\"Toxic\", \"Non-toxic\"],\n                         colors=[\"darkorange\", \"dodgerblue\"], show_hist=False)\n\nfig.update_layout(title_text=\"Neutrality vs. Toxicity\", xaxis_title=\"Neutrality\", template=\"simple_white\")\nfig.show()", "processed": ["world plot see languag highest neutral persian hindi russian western european languag like german english seem lower averag neutral languag", "neutral v toxic"]}, {"markdown": ["In the world plot above, we can see that western European countries, south-east Asia, and Turkey have a lower average compound sentiment than most other countries. India, Russia, and Iran are among the countries with the maximum compound sentiment.", "### Compound sentiment vs. Toxicity"], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/jigsaw-multilingual-toxicity-eda-models\n\nnums_1 = train_data.sample(frac=0.1).query(\"toxic == 1\")[\"compound\"]\nnums_2 = train_data.sample(frac=0.1).query(\"toxic == 0\")[\"compound\"]\n\nfig = ff.create_distplot(hist_data=[nums_1, nums_2],\n                         group_labels=[\"Toxic\", \"Non-toxic\"],\n                         colors=[\"darkorange\", \"dodgerblue\"], show_hist=False)\n\nfig.update_layout(title_text=\"Compound vs. Toxicity\", xaxis_title=\"Compound\", template=\"simple_white\")\nfig.show()", "processed": ["world plot see western european countri south east asia turkey lower averag compound sentiment countri india russia iran among countri maximum compound sentiment", "compound sentiment v toxic"]}, {"markdown": ["In the world plot above, we can see that the Flesch readability ease is maximum in the Russian and Vietnamese languages. These languages have few words per sentence and few syllables per word, indicating that they are \"easier\" to read.", "### Flesch reading ease vs. Toxicity"], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/jigsaw-multilingual-toxicity-eda-models\n\nnums_1 = train_data.sample(frac=0.1).query(\"toxic == 1\")[\"flesch_reading_ease\"]\nnums_2 = train_data.sample(frac=0.1).query(\"toxic == 0\")[\"flesch_reading_ease\"]\n\nfig = ff.create_distplot(hist_data=[nums_1, nums_2],\n                         group_labels=[\"Toxic\", \"Non-toxic\"],\n                         colors=[\"darkorange\", \"dodgerblue\"], show_hist=False)\n\nfig.update_layout(title_text=\"Flesch reading ease vs. Toxicity\", xaxis_title=\"Flesch reading ease\", template=\"simple_white\")\nfig.show()", "processed": ["world plot see flesch readabl ea maximum russian vietnames languag languag word per sentenc syllabl per word indic easier read", "flesch read ea v toxic"]}, {"markdown": ["In the world plot above, we can see that automated readability is maximum in Hindi, Arabic, and Somali comments. Whereas, Turkish, English, and south-east Asian comments seem to have relatively lower automated readability value than most countries.", "### Automated readability vs. Toxicity"], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/jigsaw-multilingual-toxicity-eda-models\n\nnums_1 = train_data.sample(frac=0.1).query(\"toxic == 1\")[\"automated_readability\"]\nnums_2 = train_data.sample(frac=0.1).query(\"toxic == 0\")[\"automated_readability\"]\n\nfig = ff.create_distplot(hist_data=[nums_1, nums_2],\n                         group_labels=[\"Toxic\", \"Non-toxic\"],\n                         colors=[\"darkorange\", \"dodgerblue\"], show_hist=False)\n\nfig.update_layout(title_text=\"Automated readability vs. Toxicity\", xaxis_title=\"Automated readability\", template=\"simple_white\")\nfig.show()", "processed": ["world plot see autom readabl maximum hindi arab somali comment wherea turkish english south east asian comment seem rel lower autom readabl valu countri", "autom readabl v toxic"]}, {"markdown": ["The Dale-Chall readability score seems to be maximum in middle-eastern and south-east Asian languages. Russian and Arabic, on the other hand, have a lower Dale-Chall readability than most other languages in the dataset.", "### Dale-Chall readability"], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/jigsaw-multilingual-toxicity-eda-models\n\nnums_1 = train_data.sample(frac=0.1).query(\"toxic == 1\")[\"dale_chall_readability\"]\nnums_2 = train_data.sample(frac=0.1).query(\"toxic == 0\")[\"dale_chall_readability\"]\n\nfig = ff.create_distplot(hist_data=[nums_1, nums_2],\n                         group_labels=[\"Toxic\", \"Non-toxic\"],\n                         colors=[\"darkorange\", \"dodgerblue\"], show_hist=False)\n\nfig.update_layout(title_text=\"Dale-Chall readability vs. Toxicity\", xaxis_title=\"Dale-Chall readability\", template=\"simple_white\")\nfig.show()", "processed": ["dale chall readabl score seem maximum middl eastern south east asian languag russian arab hand lower dale chall readabl languag dataset", "dale chall readabl"]}, {"markdown": ["We can see from the above wordclouds, that toxic comments use more insluting or hateful words such as \"f**k\", while the non-toxic comments do not usually use such words.", "### Obscene vs. Severe Toxic vs. Threat vs. Insult"], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/jigsaw-multilingual-toxicity-eda-models\n\ntoxic_mask=np.array(Image.open(\"../input/imagesforkernal/toxic-sign.png\"))\ntoxic_mask=toxic_mask[:,:,1]\n#wordcloud for clean comments\nsubset=train_data.query(\"obscene == 1\")\ntext=subset.comment_text.values\nwc= WordCloud(background_color=\"black\",max_words=4000,mask=toxic_mask,stopwords=stopword)\nwc.generate(\" \".join(text))\nplt.figure(figsize=(20,20))\nplt.subplot(221)\nplt.axis(\"off\")\nplt.title(\"Words frequented in Obscene Comments\", fontsize=20)\nplt.imshow(wc.recolor(colormap= 'gist_earth' , random_state=244), alpha=0.98)\n\n#Severely toxic comments\nplt.subplot(222)\nsevere_toxic_mask=np.array(Image.open(\"../input/imagesforkernal/bomb.png\"))\nsevere_toxic_mask=severe_toxic_mask[:,:,1]\nsubset=train_data[train_data.severe_toxic==1]\ntext=subset.comment_text.values\nwc= WordCloud(background_color=\"black\",max_words=2000,mask=severe_toxic_mask,stopwords=stopword)\nwc.generate(\" \".join(text))\nplt.axis(\"off\")\nplt.title(\"Words frequented in Severe Toxic Comments\", fontsize=20)\nplt.imshow(wc.recolor(colormap= 'Reds' , random_state=244), alpha=0.98)\n\n#Threat comments\nplt.subplot(223)\nthreat_mask=np.array(Image.open(\"../input/imagesforkernal/anger.png\"))\nthreat_mask=threat_mask[:,:,1]\nsubset=train_data[train_data.threat==1]\ntext=subset.comment_text.values\nwc= WordCloud(background_color=\"black\",max_words=2000,mask=threat_mask,stopwords=stopword)\nwc.generate(\" \".join(text))\nplt.axis(\"off\")\nplt.title(\"Words frequented in Threatening Comments\", fontsize=20)\nplt.imshow(wc.recolor(colormap= 'summer' , random_state=2534), alpha=0.98)\n\n#insult\nplt.subplot(224)\ninsult_mask=np.array(Image.open(\"../input/imagesforkernal/swords.png\"))\ninsult_mask=insult_mask[:,:,1]\nsubset=train_data[train_data.insult==1]\ntext=subset.comment_text.values\nwc= WordCloud(background_color=\"black\",max_words=2000,mask=insult_mask,stopwords=stopword)\nwc.generate(\" \".join(text))\nplt.axis(\"off\")\nplt.title(\"Words frequented in insult Comments\", fontsize=20)\nplt.imshow(wc.recolor(colormap= 'Paired_r' , random_state=244), alpha=0.98)\n\nplt.show()", "processed": ["see wordcloud toxic comment use inslut hate word f k non toxic comment usual use word", "obscen v sever toxic v threat v insult"]}, {"markdown": ["The percent missing for x,y, height and width in train labels represents the percent of the target **0** (not **Lung opacity**).\n\nLet's check the class distribution from class detailed info."], "code": "# Reference: https://www.kaggle.com/code/gpreda/rsna-pneumonia-detection-eda\n\nf, ax = plt.subplots(1,1, figsize=(6,4))\ntotal = float(len(class_info_df))\nsns.countplot(class_info_df['class'],order = class_info_df['class'].value_counts().index, palette='Set3')\nfor p in ax.patches:\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()/2.,\n            height + 3,\n            '{:1.2f}%'.format(100*height/total),\n            ha=\"center\") \nplt.show()", "processed": ["percent miss x height width train label repres percent target 0 lung opac let check class distribut class detail info"]}, {"markdown": ["### Target and class  \n\nLet's plot the number of examinations for each class detected, grouped by Target value."], "code": "# Reference: https://www.kaggle.com/code/gpreda/rsna-pneumonia-detection-eda\n\nfig, ax = plt.subplots(nrows=1,figsize=(12,6))\ntmp = train_class_df.groupby('Target')['class'].value_counts()\ndf = pd.DataFrame(data={'Exams': tmp.values}, index=tmp.index).reset_index()\nsns.barplot(ax=ax,x = 'Target', y='Exams',hue='class',data=df, palette='Set3')\nplt.title(\"Chest exams class and Target\")\nplt.show()", "processed": ["target class let plot number examin class detect group target valu"]}, {"markdown": ["All chest examinations with`Target` = **1** (pathology detected) associated with `class`:  **Lung Opacity**.    \n\nThe chest examinations with `Target` = **0** (no pathology detected) are either of `class`: **Normal** or `class`: **No Lung Opacity / Not Normal**.", "### Detected Lung Opacity window   \n\nFor the class **Lung Opacity**, corresponding to values of **Target = 1**, we plot the density of **x**, **y**, **width** and **height**.\n\n"], "code": "# Reference: https://www.kaggle.com/code/gpreda/rsna-pneumonia-detection-eda\n\ntarget1 = train_class_df[train_class_df['Target']==1]\nsns.set_style('whitegrid')\nplt.figure()\nfig, ax = plt.subplots(2,2,figsize=(12,12))\nsns.distplot(target1['x'],kde=True,bins=50, color=\"red\", ax=ax[0,0])\nsns.distplot(target1['y'],kde=True,bins=50, color=\"blue\", ax=ax[0,1])\nsns.distplot(target1['width'],kde=True,bins=50, color=\"green\", ax=ax[1,0])\nsns.distplot(target1['height'],kde=True,bins=50, color=\"magenta\", ax=ax[1,1])\nlocs, labels = plt.xticks()\nplt.tick_params(axis='both', which='major', labelsize=12)\nplt.show()", "processed": ["chest examin target 1 patholog detect associ class lung opac chest examin target 0 patholog detect either class normal class lung opac normal", "detect lung opac window class lung opac correspond valu target 1 plot densiti x width height"]}, {"markdown": ["We confirmed that the number of *unique* **patientsId** are equal with the number of DICOM images in the train set.  \n\nLet's see what entries are duplicated. We want to check how are these distributed accross classes and Target value."], "code": "# Reference: https://www.kaggle.com/code/gpreda/rsna-pneumonia-detection-eda\n\ntmp = train_class_df.groupby(['patientId','Target', 'class'])['patientId'].count()\ndf = pd.DataFrame(data={'Exams': tmp.values}, index=tmp.index).reset_index()\ntmp = df.groupby(['Exams','Target','class']).count()\ndf2 = pd.DataFrame(data=tmp.values, index=tmp.index).reset_index()\ndf2.columns = ['Exams', 'Target','Class', 'Entries']\ndf2\nfig, ax = plt.subplots(nrows=1,figsize=(12,6))\nsns.barplot(ax=ax,x = 'Target', y='Entries', hue='Exams',data=df2, palette='Set2')\nplt.title(\"Chest exams class and Target\")\nplt.show()", "processed": ["confirm number uniqu patientsid equal number dicom imag train set let see entri duplic want check distribut accross class target valu"]}, {"markdown": ["Only {Rows:Columns} {1024:1024} are present in both train and test.  \n\n\n<a href=\"#0\"><font size=\"1\">Go to top</font></a>", "## <a id=\"310\">Patient Age</a>\n\nLet's examine now the data for the Patient Age for the train set.\n\n### Train dataset"], "code": "# Reference: https://www.kaggle.com/code/gpreda/rsna-pneumonia-detection-eda\n\ntmp = train_class_df.groupby(['Target', 'PatientAge'])['patientId'].count()\ndf = pd.DataFrame(data={'Exams': tmp.values}, index=tmp.index).reset_index()\ntmp = df.groupby(['Exams','Target', 'PatientAge']).count()\ndf2 = pd.DataFrame(data=tmp.values, index=tmp.index).reset_index()\ntmp = train_class_df.groupby(['class', 'PatientAge'])['patientId'].count()\ndf1 = pd.DataFrame(data={'Exams': tmp.values}, index=tmp.index).reset_index()\ntmp = df1.groupby(['Exams','class', 'PatientAge']).count()\ndf3 = pd.DataFrame(data=tmp.values, index=tmp.index).reset_index()\nfig, (ax) = plt.subplots(nrows=1,figsize=(16,6))\nsns.barplot(ax=ax, x = 'PatientAge', y='Exams', hue='Target',data=df2)\nplt.title(\"Train set: Chest exams Age and Target\")\nplt.xticks(rotation=90)\nplt.show()\nfig, (ax) = plt.subplots(nrows=1,figsize=(16,6))\nsns.barplot(ax=ax, x = 'PatientAge', y='Exams', hue='class',data=df3)\nplt.title(\"Train set: Chest exams Age and class\")\nplt.xticks(rotation=90)\nplt.show()", "processed": ["row column 1024 1024 present train test href 0 font size 1 go top font", "id 310 patient age let examin data patient age train set train dataset"]}, {"markdown": ["Let's check also the distribution of patient age for the test data set.\n\n### Test dataset"], "code": "# Reference: https://www.kaggle.com/code/gpreda/rsna-pneumonia-detection-eda\n\nfig, (ax) = plt.subplots(nrows=1,figsize=(16,6))\nsns.countplot(test_class_df['PatientAge'], ax=ax)\nplt.title(\"Test set: Patient Age\")\nplt.xticks(rotation=90)\nplt.show()", "processed": ["let check also distribut patient age test data set test dataset"]}, {"markdown": ["<a href=\"#0\"><font size=\"1\">Go to top</font></a>\n\n\n## <a id=\"311\">Patient Sex</a>\n\nLet's examine now the data for the Patient Sex.   \n\n### Train dataset\n\nWe represent the number of Exams for each Patient Sex, grouped by value of Target."], "code": "# Reference: https://www.kaggle.com/code/gpreda/rsna-pneumonia-detection-eda\n\ntmp = train_class_df.groupby(['Target', 'PatientSex'])['patientId'].count()\ndf = pd.DataFrame(data={'Exams': tmp.values}, index=tmp.index).reset_index()\ntmp = df.groupby(['Exams','Target', 'PatientSex']).count()\ndf2 = pd.DataFrame(data=tmp.values, index=tmp.index).reset_index()\nfig, ax = plt.subplots(nrows=1,figsize=(6,6))\nsns.barplot(ax=ax, x = 'PatientSex', y='Exams', hue='Target',data=df2)\nplt.title(\"Train set: Patient Sex and Target\")\nplt.show()", "processed": ["href 0 font size 1 go top font id 311 patient sex let examin data patient sex train dataset repres number exam patient sex group valu target"]}, {"markdown": ["We represent the number of Exams for each Patient Sex, grouped by value of  class."], "code": "# Reference: https://www.kaggle.com/code/gpreda/rsna-pneumonia-detection-eda\n\ntmp = train_class_df.groupby(['class', 'PatientSex'])['patientId'].count()\ndf1 = pd.DataFrame(data={'Exams': tmp.values}, index=tmp.index).reset_index()\ntmp = df1.groupby(['Exams','class', 'PatientSex']).count()\ndf3 = pd.DataFrame(data=tmp.values, index=tmp.index).reset_index()\nfig, (ax) = plt.subplots(nrows=1,figsize=(6,6))\nsns.barplot(ax=ax, x = 'PatientSex', y='Exams', hue='class',data=df3)\nplt.title(\"Train set: Patient Sex and class\")\nplt.show()", "processed": ["repres number exam patient sex group valu class"]}, {"markdown": ["Let's check as well the distribution of Patient Sex for the test data.   \n\n### Test dataset"], "code": "# Reference: https://www.kaggle.com/code/gpreda/rsna-pneumonia-detection-eda\n\nsns.countplot(test_class_df['PatientSex'])\nplt.title(\"Test set: Patient Sex\")\nplt.show()", "processed": ["let check well distribut patient sex test data test dataset"]}, {"markdown": ["# Public LB Scores of Top Teams over time"], "code": "# Reference: https://www.kaggle.com/code/robikscube/a-race-for-instant-gratification\n\n# Interative Plotly\ninit_notebook_mode(connected=True)\nTOP_TEAMS = df.min().loc[df.max() > FIFTEENTH_SCORE].index.values\ndf_filtered = df[TOP_TEAMS].ffill()\ndf_filtered = df_filtered.iloc[df_filtered.index > '05-22-2019']\n# Create a trace\ndata = []\nfor col in df_filtered.columns:\n    data.append(go.Scatter(\n                        x = df_filtered.index,\n                        y = df_filtered[col],\n                        name=col)\n               )\nlayout = go.Layout(yaxis=dict(range=[0.958, TOP_SCORE+0.0001]))\nfig = go.Figure(data=data, layout=layout)\niplot(fig)", "processed": ["public lb score top team time"]}, {"markdown": ["# Closer look at the top"], "code": "# Reference: https://www.kaggle.com/code/robikscube/a-race-for-instant-gratification\n\n# Interative Plotly\ninit_notebook_mode(connected=True)\nTOP_TEAMS = df.min().loc[df.max() > FIFTEENTH_SCORE].index.values\ndf_filtered = df[TOP_TEAMS].ffill()\ndf_filtered = df_filtered.iloc[df_filtered.index > '06-3-2019']\n# Create a trace\ndata = []\nfor col in df_filtered.columns:\n    data.append(go.Scatter(\n                        x = df_filtered.index,\n                        y = df_filtered[col],\n                        name=col)\n               )\nlayout = go.Layout(yaxis=dict(range=[0.9746, TOP_SCORE+0.0001]))\nfig = go.Figure(data=data, layout=layout)\niplot(fig)", "processed": ["closer look top"]}, {"markdown": ["# All competitors LB Position over Time"], "code": "# Reference: https://www.kaggle.com/code/robikscube/a-race-for-instant-gratification\n\n# Scores of top teams over time\nALL_TEAMS = [x for x in df.columns.values if x != 'nan']\ndf[ALL_TEAMS].ffill().plot(figsize=(20, 10),\n                           color=color_pal[0],\n                           legend=False,\n                           alpha=0.05,\n                           ylim=(0.94,TOP_SCORE+0.001),\n                           title='All Teams Public LB Scores over Time')\ndf.ffill().max(axis=1).plot(color=color_pal[1], label='1st Place Public LB', legend=True)\nplt.show()", "processed": ["competitor lb posit time"]}, {"markdown": ["# Top LB Scores"], "code": "# Reference: https://www.kaggle.com/code/robikscube/a-race-for-instant-gratification\n\n# Create Top Teams List\nTOP_TEAMS = df.max().loc[df.max() > FIFTYTH_SCORE].index.values\ndf[TOP_TEAMS].max().sort_values().plot(kind='barh',\n                                       xlim=(FIFTYTH_SCORE-0.0001,TOP_SCORE+0.0001),\n                                       title='Top 50 Public LB Teams',\n                                       figsize=(12, 15),\n                                       color=color_pal[3])\nplt.show()", "processed": ["top lb score"]}, {"markdown": ["# Count of LB Submissions that improved score\n\nThis is the count of times the person submitted and got the fun \"You're score improved\" notification. This is not the total submission count."], "code": "# Reference: https://www.kaggle.com/code/robikscube/a-race-for-instant-gratification\n\ndf[TOP_TEAMS].nunique().sort_values().plot(kind='barh',\n                                           figsize=(12, 15),\n                                           color=color_pal[1],\n                                           title='Count of Submissions improving LB score by Team')\nplt.show()", "processed": ["count lb submiss improv score count time person submit got fun score improv notif total submiss count"]}, {"markdown": ["## I am doing some tests, trying to found an interesting and meaningful graphic"], "code": "import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ndf_train = pd.read_csv(\"../input/train.csv\", parse_dates=['activation_date'], nrows=50000)\n\nprint(\"Shape train: \", df_train.shape)\nis_null = round((df_train.isnull().sum() / len(df_train) * 100),2)\nprint(\"NaN values in train Dataset\")\nprint(is_null[is_null > 0].sort_values(ascending=False))\nimport plotly\nfrom plotly.offline import init_notebook_mode, plot, iplot\nimport plotly.graph_objs as go\n\ninit_notebook_mode(connected=True)\ndf_train['price_log'] = np.log(df_train['price'] + 1)\n\ndf_2007 = df_train[(df_train.user_type == 'Private') & (df_train.deal_probability > 0)].copy()\n\ndf_2007['deal_probability_rounded'] = round(df_2007['deal_probability'],2)\ndf_2007['index'] = df_2007['deal_probability'] / df_2007['price'] \nslope = 3.2121e-05\nhover_text = []\nbubble_size = []\nimport math\n\nfor index, row in df_2007.iterrows():\n    hover_text.append(('Region: {region}<br>'+\n                      'Parent Category: {par_cat}<br>'+\n                      'Title: {title}<br>'+\n                      'Price: {price}<br>'+\n                      'Deal Probability: {deal_prob}').format(region=row['region'],\n                                            par_cat=row['parent_category_name'],\n                                            title=row['title'],\n                                            price=row['price'],\n                                            deal_prob=row['deal_probability']))\n    bubble_size.append(math.sqrt(row['deal_probability']*slope))\n\ndf_2007['text'] = hover_text\ndf_2007['size'] = bubble_size\nsizeref = 14*max(df_2007['size'])/(40**2)\n\ntrace0 = go.Scatter(\n    x=df_2007['price_log'][df_2007['parent_category_name'] == '\u041b\u0438\u0447\u043d\u044b\u0435 \u0432\u0435\u0449\u0438'],\n    y=df_2007['deal_probability'][df_2007['parent_category_name'] == '\u041b\u0438\u0447\u043d\u044b\u0435 \u0432\u0435\u0449\u0438'],\n    mode='markers', opacity=0.7,\n    name='\u041b\u0438\u0447\u043d\u044b\u0435 \u0432\u0435\u0449\u0438',\n    text=df_2007['text'][df_2007['parent_category_name'] == '\u041b\u0438\u0447\u043d\u044b\u0435 \u0432\u0435\u0449\u0438'],\n    marker=dict(\n        symbol='circle',\n        sizemode='area',\n        sizeref=sizeref,\n        size=df_2007['size'][df_2007['parent_category_name'] == '\u041b\u0438\u0447\u043d\u044b\u0435 \u0432\u0435\u0449\u0438'],\n        line=dict(\n            width=2\n        ),\n    )\n)\ntrace1 = go.Scatter(\n    x=df_2007['price_log'][df_2007['parent_category_name'] == '\u0414\u043b\u044f \u0434\u043e\u043c\u0430 \u0438 \u0434\u0430\u0447\u0438'],\n    y=df_2007['deal_probability'][df_2007['parent_category_name'] == '\u0414\u043b\u044f \u0434\u043e\u043c\u0430 \u0438 \u0434\u0430\u0447\u0438'],\n    mode='markers', opacity=0.7,\n    name='\u0414\u043b\u044f \u0434\u043e\u043c\u0430 \u0438 \u0434\u0430\u0447\u0438',\n    text=df_2007['text'][df_2007['parent_category_name'] == '\u0414\u043b\u044f \u0434\u043e\u043c\u0430 \u0438 \u0434\u0430\u0447\u0438'],\n    marker=dict(\n        sizemode='area',\n        sizeref=sizeref,\n        size=df_2007['size'][df_2007['parent_category_name'] == '\u0414\u043b\u044f \u0434\u043e\u043c\u0430 \u0438 \u0434\u0430\u0447\u0438'],\n        line=dict(\n            width=2\n        ),\n    )\n)\ntrace2 = go.Scatter(\n    x=df_2007['price_log'][df_2007['parent_category_name'] == '\u0411\u044b\u0442\u043e\u0432\u0430\u044f \u044d\u043b\u0435\u043a\u0442\u0440\u043e\u043d\u0438\u043a\u0430'],\n    y=df_2007['deal_probability'][df_2007['parent_category_name'] == '\u0411\u044b\u0442\u043e\u0432\u0430\u044f \u044d\u043b\u0435\u043a\u0442\u0440\u043e\u043d\u0438\u043a\u0430'],\n    mode='markers', opacity=0.7,\n    name='\u0411\u044b\u0442\u043e\u0432\u0430\u044f \u044d\u043b\u0435\u043a\u0442\u0440\u043e\u043d\u0438\u043a\u0430',\n    text=df_2007['text'][df_2007['parent_category_name'] == '\u0411\u044b\u0442\u043e\u0432\u0430\u044f \u044d\u043b\u0435\u043a\u0442\u0440\u043e\u043d\u0438\u043a\u0430'],\n    marker=dict(\n        sizemode='area',\n        sizeref=sizeref,\n        size=df_2007['size'][df_2007['parent_category_name'] == '\u0411\u044b\u0442\u043e\u0432\u0430\u044f \u044d\u043b\u0435\u043a\u0442\u0440\u043e\u043d\u0438\u043a\u0430'],\n        line=dict(\n            width=2\n        ),\n    )\n)\ntrace3 = go.Scatter(\n    x=df_2007['price_log'][df_2007['parent_category_name'] == '\u041d\u0435\u0434\u0432\u0438\u0436\u0438\u043c\u043e\u0441\u0442\u044c'],\n    y=df_2007['deal_probability'][df_2007['parent_category_name'] == '\u041d\u0435\u0434\u0432\u0438\u0436\u0438\u043c\u043e\u0441\u0442\u044c'],\n    mode='markers', opacity=0.7,\n    name='\u041d\u0435\u0434\u0432\u0438\u0436\u0438\u043c\u043e\u0441\u0442\u044c', \n    text=df_2007['text'][df_2007['parent_category_name'] == '\u041d\u0435\u0434\u0432\u0438\u0436\u0438\u043c\u043e\u0441\u0442\u044c'],\n    marker=dict(\n        sizemode='area',\n        sizeref=sizeref,\n        size=df_2007['size'][df_2007['parent_category_name'] == '\u041d\u0435\u0434\u0432\u0438\u0436\u0438\u043c\u043e\u0441\u0442\u044c'],\n        line=dict(\n            width=2\n        ),\n    )\n)\ntrace4 = go.Scatter(\n    x=df_2007['price_log'][df_2007['parent_category_name'] == '\u0425\u043e\u0431\u0431\u0438 \u0438 \u043e\u0442\u0434\u044b\u0445'],\n    y=df_2007['deal_probability'][df_2007['parent_category_name'] == '\u0425\u043e\u0431\u0431\u0438 \u0438 \u043e\u0442\u0434\u044b\u0445'],\n    mode='markers', opacity=0.7,\n    name='\u0425\u043e\u0431\u0431\u0438 \u0438 \u043e\u0442\u0434\u044b\u0445', \n    text=df_2007['text'][df_2007['parent_category_name'] == '\u0425\u043e\u0431\u0431\u0438 \u0438 \u043e\u0442\u0434\u044b\u0445'],\n    marker=dict(\n        sizemode='area',\n        sizeref=sizeref,\n        size=df_2007['size'][df_2007['parent_category_name'] == '\u0425\u043e\u0431\u0431\u0438 \u0438 \u043e\u0442\u0434\u044b\u0445'],\n        line=dict(\n            width=2\n        ),\n    )\n)\n\ndata = [trace0, trace1, trace2, trace3, trace4]\nlayout = go.Layout(\n    title='Price vs Deal Probability', showlegend=True,\n    xaxis=dict(\n        title=\"Price Logof Avito's Ads(US)\",\n        zerolinewidth=1,\n        ticklen=5,\n        gridwidth=2,\n    ),\n    yaxis=dict(\n        title='Deal Probability',\n        zerolinewidth=1,\n        ticklen=5,\n        gridwidth=2,\n    ), legend=dict(\n        orientation=\"v\")\n    )\n\nfig = go.Figure(data=data, layout=layout)\niplot(fig)", "processed": ["test tri found interest meaning graphic"]}, {"markdown": ["### 4.5 LGB <a class=\"anchor\" id=\"4.5\"></a>\n\n[Back to Table of Contents](#0.1)"], "code": "# Reference: https://www.kaggle.com/code/vbmokin/ion-switching-advfe-lgb-wavenet-confmatrix\n\n# Thanks to https://www.kaggle.com/siavrez/simple-eda-model\ndef MacroF1Metric(preds, dtrain):\n    labels = dtrain.get_label()\n    preds = np.round(np.clip(preds, 0, 10)).astype(int)\n    score = f1_score(labels, preds, average = 'macro')\n    return ('MacroF1Metric', score, True)\n%%time\n# Thanks to https://www.kaggle.com/jazivxt/physically-possible with tuning from https://www.kaggle.com/siavrez/simple-eda-model and my tuning\nX_train, X_valid, y_train, y_valid = train_test_split(train[col], y, test_size=0.3, random_state=seed_random)\nparams = {'learning_rate': lr_lgb, \n          'max_depth': -1, \n          'num_leaves': num_leaves,\n          'metric': 'logloss', \n          'random_state': seed_random, \n          'n_jobs':-1, \n          'sample_fraction':0.33}\nmodel = lgb.train(params, lgb.Dataset(X_train, y_train), num_iterations, lgb.Dataset(X_valid, y_valid), verbose_eval=100, early_stopping_rounds=200, feval=MacroF1Metric)\ngc.collect()\n%%time\ny_lgb_pred = model.predict(test[col], num_iteration=model.best_iteration)\ny_pred_train_lgb = model.predict(train[col], num_iteration=model.best_iteration)\ngc.collect()\nprint('LGB score {0:.4f}'.format(np.mean(f1_score(y, np.round(np.clip(y_pred_train_lgb,0,10)).astype(int), average=\"macro\"))))\nfig =  plt.figure(figsize = (15,15))\naxes = fig.add_subplot(111)\nlgb.plot_importance(model,ax = axes,height = 0.5)\nplt.show();plt.close()\ngc.collect()", "processed": ["4 5 lgb class anchor id 4 5 back tabl content 0 1"]}, {"markdown": ["# Missing Value Exploration"], "code": "# Reference: https://www.kaggle.com/code/frtgnn/elo-eda-lgbm\n\nfor df in [ht,new_merchant]:\n    df['category_2'].fillna(1.0,inplace=True)\n    df['category_3'].fillna('A',inplace=True)\n    df['merchant_id'].fillna('M_ID_00a6ca8a8a',inplace=True)\nfeature_1 = train.loc[train['feature_1'] == 1]\nfeature_2 = train.loc[train['feature_1'] == 2]\nfeature_3 = train.loc[train['feature_1'] == 3]\nfeature_4 = train.loc[train['feature_1'] == 4]\nfeature_5 = train.loc[train['feature_1'] == 5]\n\nplt.figure(figsize=(10, 6))\nplt.title('Feature_2 Distribution based on Feature_1 values')\nsns.despine()\nsns.set_context(\"notebook\", font_scale=1.5, rc={\"lines.linewidth\": 2.5})\n\nsns.distplot(feature_1['feature_2'], hist=False, rug=False,label='1')\nsns.distplot(feature_2['feature_2'], hist=False, rug=False,label='2')\nsns.distplot(feature_3['feature_2'], hist=False, rug=False,label='3')\nsns.distplot(feature_4['feature_2'], hist=False, rug=False,label='4')\nsns.distplot(feature_5['feature_2'], hist=False, rug=False,label='5')\nfeature_1 = train.loc[train['feature_2'] == 1]\nfeature_2 = train.loc[train['feature_2'] == 2]\nfeature_3 = train.loc[train['feature_2'] == 3]\n\nplt.figure(figsize=(10, 6))\nplt.title('Feature_1 Distribution based on Feature_2 values')\nsns.despine()\nsns.set_context(\"notebook\", font_scale=1.5, rc={\"lines.linewidth\": 2.5})\n\nsns.distplot(feature_1['feature_1'], hist=False, rug=False,label='1')\nsns.distplot(feature_2['feature_1'], hist=False, rug=False,label='2')\nsns.distplot(feature_3['feature_1'], hist=False, rug=False,label='3')\nfeature_1 = train.loc[train['feature_3'] == 0]\nfeature_2 = train.loc[train['feature_3'] == 1]\n\nplt.figure(figsize=(10, 6))\nplt.title('Feature_1 Distribution based on Feature_3 values')\nsns.despine()\nsns.set_context(\"notebook\", font_scale=1.5, rc={\"lines.linewidth\": 2.5})\n\nsns.distplot(feature_1['feature_1'], hist=False, rug=False,label='0')\nsns.distplot(feature_2['feature_1'], hist=False, rug=False,label='1')\nfeature_1 = train.loc[train['feature_3'] == 0]\nfeature_2 = train.loc[train['feature_3'] == 1]\n\nplt.figure(figsize=(10, 6))\nplt.title('Feature_2 Distribution based on Feature_3 values')\nsns.despine()\nsns.set_context(\"notebook\", font_scale=1.5, rc={\"lines.linewidth\": 2.5})\n\nsns.distplot(feature_1['feature_2'], hist=False, rug=False,label='0')\nsns.distplot(feature_2['feature_2'], hist=False, rug=False,label='1')\n# thanks to this kernel @ https://www.kaggle.com/artgor/elo-eda-and-models\nfig, ax = plt.subplots(3, 1, figsize = (12, 12))\ntrain['feature_1'].value_counts().sort_index().plot(kind='bar', ax=ax[0], color='teal')\ntrain['feature_2'].value_counts().sort_index().plot(kind='bar', ax=ax[1], color='brown')\ntrain['feature_3'].value_counts().sort_index().plot(kind='bar', ax=ax[2], color='gold');\nplt.figure(figsize=(10, 6))\nplt.title('Target Distribution')\nsns.despine()\nsns.set_context(\"notebook\", font_scale=1.5, rc={\"lines.linewidth\": 2.5})\n\nsns.distplot(train['target'], hist=True, rug=False,norm_hist=True)", "processed": ["miss valu explor"]}, {"markdown": ["The AUC of 0.63 is well beyond \"randon\", and could be pretty significant in terms of distinguishing between train and test sets. It's certainly well byond almost perfect 0.5 AUC of the fist Categorical Encoding competition. \n\nLet's take a look at what features are the most responsible for the discepancy."], "code": "# Reference: https://www.kaggle.com/code/tunguz/adversicat-ii\n\nfeature_imp = pd.DataFrame(sorted(zip(clf.feature_importance(),columns)), columns=['Value','Feature'])\n\nplt.figure(figsize=(20, 20))\nsns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False).head(100))\nplt.title('LightGBM Features')\nplt.tight_layout()\nplt.show()\nplt.savefig('lgbm_importances-01.png')", "processed": ["auc 0 63 well beyond randon could pretti signific term distinguish train test set certainli well byond almost perfect 0 5 auc fist categor encod competit let take look featur respons discep"]}, {"markdown": ["As we can see on average questions in train and test datasets are similar, but there are quite long questions in train dataset."], "code": "# Reference: https://www.kaggle.com/code/artgor/text-modelling-in-pytorch\n\npuncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '\u2022',  '~', '@', '\u00a3', \n '\u00b7', '_', '{', '}', '\u00a9', '^', '\u00ae', '`',  '<', '\u2192', '\u00b0', '\u20ac', '\u2122', '\u203a',  '\u2665', '\u2190', '\u00d7', '\u00a7', '\u2033', '\u2032', '\u00c2', '\u2588', '\u00bd', '\u00e0', '\u2026', \n '\u201c', '\u2605', '\u201d', '\u2013', '\u25cf', '\u00e2', '\u25ba', '\u2212', '\u00a2', '\u00b2', '\u00ac', '\u2591', '\u00b6', '\u2191', '\u00b1', '\u00bf', '\u25be', '\u2550', '\u00a6', '\u2551', '\u2015', '\u00a5', '\u2593', '\u2014', '\u2039', '\u2500', \n '\u2592', '\uff1a', '\u00bc', '\u2295', '\u25bc', '\u25aa', '\u2020', '\u25a0', '\u2019', '\u2580', '\u00a8', '\u2584', '\u266b', '\u2606', '\u00e9', '\u00af', '\u2666', '\u00a4', '\u25b2', '\u00e8', '\u00b8', '\u00be', '\u00c3', '\u22c5', '\u2018', '\u221e', \n '\u2219', '\uff09', '\u2193', '\u3001', '\u2502', '\uff08', '\u00bb', '\uff0c', '\u266a', '\u2569', '\u255a', '\u00b3', '\u30fb', '\u2566', '\u2563', '\u2554', '\u2557', '\u25ac', '\u2764', '\u00ef', '\u00d8', '\u00b9', '\u2264', '\u2021', '\u221a', ]\n\ndef clean_text(x):\n    x = str(x)\n    for punct in puncts:\n        x = x.replace(punct, f' {punct} ')\n    return x\n\ndef clean_numbers(x):\n    x = re.sub('[0-9]{5,}', '#####', x)\n    x = re.sub('[0-9]{4}', '####', x)\n    x = re.sub('[0-9]{3}', '###', x)\n    x = re.sub('[0-9]{2}', '##', x)\n    return x\n\nmispell_dict = {\"aren't\" : \"are not\",\n\"can't\" : \"cannot\",\n\"couldn't\" : \"could not\",\n\"didn't\" : \"did not\",\n\"doesn't\" : \"does not\",\n\"don't\" : \"do not\",\n\"hadn't\" : \"had not\",\n\"hasn't\" : \"has not\",\n\"haven't\" : \"have not\",\n\"he'd\" : \"he would\",\n\"he'll\" : \"he will\",\n\"he's\" : \"he is\",\n\"i'd\" : \"I would\",\n\"i'd\" : \"I had\",\n\"i'll\" : \"I will\",\n\"i'm\" : \"I am\",\n\"isn't\" : \"is not\",\n\"it's\" : \"it is\",\n\"it'll\":\"it will\",\n\"i've\" : \"I have\",\n\"let's\" : \"let us\",\n\"mightn't\" : \"might not\",\n\"mustn't\" : \"must not\",\n\"shan't\" : \"shall not\",\n\"she'd\" : \"she would\",\n\"she'll\" : \"she will\",\n\"she's\" : \"she is\",\n\"shouldn't\" : \"should not\",\n\"that's\" : \"that is\",\n\"there's\" : \"there is\",\n\"they'd\" : \"they would\",\n\"they'll\" : \"they will\",\n\"they're\" : \"they are\",\n\"they've\" : \"they have\",\n\"we'd\" : \"we would\",\n\"we're\" : \"we are\",\n\"weren't\" : \"were not\",\n\"we've\" : \"we have\",\n\"what'll\" : \"what will\",\n\"what're\" : \"what are\",\n\"what's\" : \"what is\",\n\"what've\" : \"what have\",\n\"where's\" : \"where is\",\n\"who'd\" : \"who would\",\n\"who'll\" : \"who will\",\n\"who're\" : \"who are\",\n\"who's\" : \"who is\",\n\"who've\" : \"who have\",\n\"won't\" : \"will not\",\n\"wouldn't\" : \"would not\",\n\"you'd\" : \"you would\",\n\"you'll\" : \"you will\",\n\"you're\" : \"you are\",\n\"you've\" : \"you have\",\n\"'re\": \" are\",\n\"wasn't\": \"was not\",\n\"we'll\":\" will\",\n\"didn't\": \"did not\",\n\"tryin'\":\"trying\"}\n\ndef _get_mispell(mispell_dict):\n    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n    return mispell_dict, mispell_re\n\nmispellings, mispellings_re = _get_mispell(mispell_dict)\ndef replace_typical_misspell(text):\n    def replace(match):\n        return mispellings[match.group(0)]\n    return mispellings_re.sub(replace, text)\n\n# Clean the text\ntrain[\"question_text\"] = train[\"question_text\"].apply(lambda x: clean_text(x.lower()))\ntest[\"question_text\"] = test[\"question_text\"].apply(lambda x: clean_text(x.lower()))\n\n# Clean numbers\ntrain[\"question_text\"] = train[\"question_text\"].apply(lambda x: clean_numbers(x))\ntest[\"question_text\"] = test[\"question_text\"].apply(lambda x: clean_numbers(x))\n\n# Clean speelings\ntrain[\"question_text\"] = train[\"question_text\"].apply(lambda x: replace_typical_misspell(x))\ntest[\"question_text\"] = test[\"question_text\"].apply(lambda x: replace_typical_misspell(x))\nmax_features = 120000\ntk = Tokenizer(lower = True, filters='', num_words=max_features)\nfull_text = list(train['question_text'].values) + list(test['question_text'].values)\ntk.fit_on_texts(full_text)\ntrain_tokenized = tk.texts_to_sequences(train['question_text'].fillna('missing'))\ntest_tokenized = tk.texts_to_sequences(test['question_text'].fillna('missing'))\ntrain['question_text'].apply(lambda x: len(x.split())).plot(kind='hist');\nplt.yscale('log');\nplt.title('Distribution of question text length in characters');", "processed": ["see averag question train test dataset similar quit long question train dataset"]}, {"markdown": ["For raw image data we were able to get an AUC of 0.65, while here we get 0.70. Seems that the image size and image metafeatrue data has more discrepancy between the train and test sets than the raw rescaled images.\n\nLet's look at the top features and theri relative importances."], "code": "# Reference: https://www.kaggle.com/code/tunguz/adversarial-melanoma-2\n\nfeature_imp = pd.DataFrame(sorted(zip(clf.feature_importance(),features)), columns=['Value','Feature'])\n\nplt.figure(figsize=(20, 20))\nsns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False).head(5))\nplt.title('LightGBM Features')\nplt.tight_layout()\nplt.show()\nplt.savefig('lgbm_importances-01.png')\nfeature_imp.sort_values(by=\"Value\", ascending=False).head(5)", "processed": ["raw imag data abl get auc 0 65 get 0 70 seem imag size imag metafeatru data discrep train test set raw rescal imag let look top featur theri rel import"]}, {"markdown": ["# Display Target Density and Target Probability\nBelow are two plots for each of the 200 variables. The first is the density of `target=1` versus `target=0`. The second gives the probability that `target=1` given different values for `var_k`."], "code": "# Reference: https://www.kaggle.com/code/cdeotte/modified-naive-bayes-santander-0-899\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# DRAW PLOTS, YES OR NO\nPicture = True\n# DATA HAS Z-SCORE RANGE OF -4.5 TO 4.5\nrmin=-5; rmax=5; \n# CALCULATE PROBABILITIES FOR 501 BINS\nres=501\n# STORE PROBABILITIES IN PR\npr = 0.1 * np.ones((200,res))\npr2 = pr.copy()\nxr = np.zeros((200,res))\nxr2 = xr.copy()\nct2 = 0\nfor j in range(50):\n    if Picture: plt.figure(figsize=(15,8))\n    for v in range(4):\n        ct = 0\n        # CALCULATE PROBABILITY FUNCTION FOR VAR\n        for i in np.linspace(rmin,rmax,res):\n            pr[v+4*j,ct] = getp(v+4*j,m[v+4*j]+i*s[v+4*j])\n            xr[v+4*j,ct] = m[v+4*j]+i*s[v+4*j]\n            xr2[v+4*j,ct] = i\n            ct += 1\n        if Picture:\n            # SMOOTH FUNCTION FOR PRETTIER DISPLAY\n            # BUT USE UNSMOOTHED FUNCTION FOR PREDICTION\n            pr2[v+4*j,:] = smooth(pr[v+4*j,:],res//10)\n            # DISPLAY PROBABILITY FUNCTION\n            plt.subplot(2, 4, ct2%4+5)\n            plt.plot(xr[v+4*j,:],pr2[v+4*j,:],'-')\n            plt.title('P( t=1 | var_'+str(v+4*j)+' )')\n            xx = plt.xlim()\n            # DISPLAY TARGET DENSITIES\n            plt.subplot(2, 4, ct2%4+1)            \n            sns.distplot(train0['var_'+str(v+4*j)], label = 't=0')\n            sns.distplot(train1['var_'+str(v+4*j)], label = 't=1')\n            plt.title('var_'+str(v+4*j))\n            plt.legend()\n            plt.xlim(xx)\n            plt.xlabel('')\n        if (ct2%8==0): print('Showing vars',ct2,'to',ct2+7,'...')\n        ct2 += 1\n    if Picture: plt.show()", "processed": ["display target densiti target probabl two plot 200 variabl first densiti target 1 versu target 0 second give probabl target 1 given differ valu var k"]}, {"markdown": ["# Validation\nWe will ignore the training data's target and make our own prediction for each training observation. Then using our predictions and the true value, we will calculate validation AUC. (There is a leak in this validation method but none-the-less it gives an approximation of CV score. If you wish to tune this model, you should use a proper validation set. Current actual 5-fold CV is 0.8995)"], "code": "# Reference: https://www.kaggle.com/code/cdeotte/modified-naive-bayes-santander-0-899\n\nfrom sklearn.metrics import roc_auc_score\nprint('Calculating 200000 predictions and displaying a few examples...')\npred = [0]*200000; ct = 0\nfor r in train.index:\n    p = 0.1\n    for i in range(200):\n        p *= 10*getp2(i,train.iloc[r,2+i])\n    if ct%25000==0: print('train',r,'has target =',train.iloc[r,1],'and prediction =',p)\n    pred[ct]=p; ct += 1\nprint('###############')\nprint('Validation AUC =',roc_auc_score(train['target'], pred))\n#https://stackoverflow.com/questions/25009284/how-to-plot-roc-curve-in-python\nfrom sklearn import metrics\nfpr, tpr, threshold = metrics.roc_curve(train['target'], pred)\nroc_auc = metrics.auc(fpr, tpr)\n\nplt.figure(figsize=(6,6))\nplt.title('Validation ROC')\nplt.plot(fpr, tpr, 'b', label = 'Val AUC = %0.3f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()", "processed": ["valid ignor train data target make predict train observ use predict true valu calcul valid auc leak valid method none le give approxim cv score wish tune model use proper valid set current actual 5 fold cv 0 8995"]}, {"markdown": ["The memory used is acceptable. We decide to not perform memory reduction for now.", "## Signal distribution\n\nWe will look now in detail to the signals in both train and test data.  \nFirst we create a function to plot the signal data."], "code": "# Reference: https://www.kaggle.com/code/gpreda/ion-switching-advanced-eda-and-prediction\n\ndef plot_time_data(data_df, title=\"Time variation data\", color='b'):\n    plt.figure(figsize=(18,8))\n    plt.plot(data_df[\"time\"], data_df[\"signal\"], color=color)\n    plt.title(title, fontsize=24)\n    plt.xlabel(\"Time [sec]\", fontsize=20)\n    plt.ylabel(\"Signal\", fontsize=20)\n    plt.show()\nplot_time_data(train_df,\"Train data\",'g')\nplot_time_data(test_df,\"Test data\",'m')", "processed": ["memori use accept decid perform memori reduct", "signal distribut look detail signal train test data first creat function plot signal data"]}, {"markdown": ["## Signal and open channel\n\nWe will show now both signal and channel signal on the same graph.  \n\nFor this we create a function to show the signal and open channel data.  \nWe will call this function with various segments of signal samples."], "code": "# Reference: https://www.kaggle.com/code/gpreda/ion-switching-advanced-eda-and-prediction\n\ndef plot_time_channel_data(data_df, title=\"Time variation data\"):\n    plt.figure(figsize=(18,8))\n    plt.plot(data_df[\"time\"], data_df[\"signal\"], color='b', label='Signal')\n    plt.plot(data_df[\"time\"], data_df[\"open_channels\"], color='r', label='Open channel')\n    plt.title(title, fontsize=24)\n    plt.xlabel(\"Time [sec]\", fontsize=20)\n    plt.ylabel(\"Signal & Open channel data\", fontsize=20)\n    plt.legend(loc='upper right')\n    plt.grid(True)\n    plt.show()\nplot_time_channel_data(train_df[0:500],'Train data: signal & open channel data')\nplot_time_channel_data(train_df[7000:7500],'Train data: signal & open channel data (0.7-0.75 sec.)')\nplot_time_channel_data(train_df[8000:8500],'Train data: signal & open channel data (0.8-0.85 sec.)')\nplot_time_channel_data(train_df[9000:9500],'Train data: signal & open channel data (0.9-0.95 sec.)')\nplot_time_channel_data(train_df[15000:15500],'Train data: signal & open channel data (1.5-1.55 sec.)')\nplot_time_channel_data(train_df[16000:16500],'Train data: signal & open channel data (1.6-1.65 sec.)')\nplot_time_channel_data(train_df[1200000:1200500],'Train data: signal & open channel data (120-120.05 sec.)')\nplot_time_channel_data(train_df[1300000:1300500],'Train data: signal & open channel data (130-130.05 sec.)')\nplot_time_channel_data(train_df[1400000:1400500],'Train data: signal & open channel data (140-140.05 sec.)')\nplot_time_channel_data(train_df[1500000:1500500],'Train data: signal & open channel data (150-150.05 sec.)')\nplot_time_channel_data(train_df[3500000:3500500],'Train data: signal & open channel data (350-350.05 sec.)')\nplot_time_channel_data(train_df[4100000:4100500],'Train data: signal & open channel data (410-410.05 sec.)')\nplot_time_channel_data(train_df[4500000:4500500],'Train data: signal & open channel data (450-450.05 sec.)')", "processed": ["signal open channel show signal channel signal graph creat function show signal open channel data call function variou segment signal sampl"]}, {"markdown": ["## Open channel distribution\n\nLet's look to the open channels distribution only."], "code": "# Reference: https://www.kaggle.com/code/gpreda/ion-switching-advanced-eda-and-prediction\n\ndef plot_open_channel_count(data_df, title):\n    plt.figure(figsize=(8,6))\n    sns.countplot(data_df['open_channels'])\n    plt.title(title)\n    plt.show()\nplot_open_channel_count(train_df,'Open channels distribution')\nplot_open_channel_count(train_df[0:1000000],'Open channels distribution (0-100 sec.)')\nplot_open_channel_count(train_df[1000000:2000000],'Open channels distribution (100-200 sec.)')\nplot_open_channel_count(train_df[2000000:3000000],'Open channels distribution (200-300 sec.)')\nplot_open_channel_count(train_df[3000000:4000000],'Open channels distribution (300-400 sec.)')\nplot_open_channel_count(train_df[4000000:5000000],'Open channels distribution (400-500 sec.)')", "processed": ["open channel distribut let look open channel distribut"]}, {"markdown": ["## Average denoising"], "code": "# Reference: https://www.kaggle.com/code/gpreda/ion-switching-advanced-eda-and-prediction\n\ndef average_signal_smoothing(signal, kernel_size=3, stride=1):\n    sample = []\n    start = 0\n    end = kernel_size\n    while end <= len(signal):\n        start = start + stride\n        end = end + stride\n        sample.extend(np.ones(end - start)*np.mean(signal[start:end]))\n    return np.array(sample[::kernel_size])\ndef plot_signal_signal_smoothed_open_channel(data_df, title):\n    sm_df = average_signal_smoothing(data_df[\"signal\"])\n    plt.figure(figsize=(18,8))\n    plt.plot(data_df[\"time\"], data_df[\"signal\"], color='b', label='Signal')\n    plt.plot(data_df[\"time\"][2:], sm_df, color='g', label='Smoothed signal')\n    plt.plot(data_df[\"time\"], data_df[\"open_channels\"], color='r', label='Open channel')\n    plt.title(title, fontsize=24)\n    plt.xlabel(\"Time [sec]\", fontsize=20)\n    plt.ylabel(\"Signal & Open channel data\", fontsize=20)\n    plt.legend(loc='upper right')\n    plt.grid(True)\n    plt.show()    \nplot_signal_signal_smoothed_open_channel(train_df[0:200], \"Train data: signal, smoothed signal & open channel data (0-0.02 sec.)\")\nplot_signal_signal_smoothed_open_channel(train_df[0:200], \"Train data: signal, smoothed signal & open channel data (0-0.02 sec.)\")\nplot_signal_signal_smoothed_open_channel(train_df[7200:7400], \"Train data: signal, smoothed signal & open channel data (0.72-0.74 sec.)\")\nplot_signal_signal_smoothed_open_channel(train_df[15300:15500], \"Train data: signal, smoothed signal & open channel data (1.53-1.55 sec.)\")\nplot_signal_signal_smoothed_open_channel(train_df[3500000:3500200],'Train data: signal, smoothed signal & open channel data (350-350.02 sec.)')", "processed": ["averag denois"]}, {"markdown": ["Let's plot the feature importance."], "code": "# Reference: https://www.kaggle.com/code/gpreda/ion-switching-advanced-eda-and-prediction\n\ncols = (feature_importance_df[[\"Feature\", \"importance\"]]\n        .groupby(\"Feature\")\n        .mean()\n        .sort_values(by=\"importance\", ascending=False)[:100].index)\nbest_features = feature_importance_df.loc[feature_importance_df.Feature.isin(cols)]\n\nplt.figure(figsize=(14,14))\nsns.barplot(x=\"importance\", y=\"Feature\", data=best_features.sort_values(by=\"importance\",ascending=False))\nplt.title('LightGBM Features (averaged over folds)')\nplt.tight_layout()\nplt.savefig('lgbm_importances.png')", "processed": ["let plot featur import"]}, {"markdown": ["## Target Variable Distribution"], "code": "# Reference: https://www.kaggle.com/code/robikscube/movie-db-box-office-prediction-intro-and-eda\n\ntrain['revenue'].plot(kind='hist',\n                      figsize=(15, 5),\n                      bins=50,\n                      title='Distribution of Revenue (Train Set)')\nplt.show()\nprint('Revenue has mean {:1.0f} and standard deviation {:1.0f}'.format(train['revenue'].mean(), train['revenue'].std())) ", "processed": ["target variabl distribut"]}, {"markdown": ["Log transforms are a common way to deal with features or targets that are heavily skewed. Log transforms are also easy to interperet. For every increase of 1 in the log transform, we can say the revenue increased 10x. With the log transform we can see we've reduced the skew."], "code": "# Reference: https://www.kaggle.com/code/robikscube/movie-db-box-office-prediction-intro-and-eda\n\ntrain['revenue_log'] = train['revenue'].apply(np.log)\ntrain['revenue_log'].plot(kind='hist',\n                      figsize=(15, 5),\n                      bins=50,\n                      title='Distribution of Log Revenue (Train Set)')\nplt.show()", "processed": ["log transform common way deal featur target heavili skew log transform also easi interperet everi increas 1 log transform say revenu increas 10x log transform see reduc skew"]}, {"markdown": ["## Overview of Features"], "code": "# Reference: https://www.kaggle.com/code/robikscube/movie-db-box-office-prediction-intro-and-eda\n\ntrain['budget'].plot(kind='hist',\n                      figsize=(15, 5),\n                      bins=50,\n                      title='Distribution of Budget (Train Set)',\n                      color='blue')\nplt.show()\n\n# Use the log1p transform since some values are zero\ntrain['budget_log'] = train['budget'].apply(np.log)\ntrain['budget_log'] = train['budget_log'].replace(-np.inf, 0)\ntrain['budget_log'].plot(kind='hist',\n                      figsize=(15, 5),\n                      bins=50,\n                      title='Distribution of Log+1 Budget (Train Set)',\n                      color='blue')\nplt.show()", "processed": ["overview featur"]}, {"markdown": ["## Budget vs. Revenue"], "code": "# Reference: https://www.kaggle.com/code/robikscube/movie-db-box-office-prediction-intro-and-eda\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\nsns.jointplot('budget_log', 'revenue_log', train.loc[train['budget_log'] > 1], kind='reg')\nplt.show()", "processed": ["budget v revenu"]}, {"markdown": ["## Helper functions and classes"], "code": "# Reference: https://www.kaggle.com/code/artgor/exploring-categorical-encodings\n\n# using ideas from this kernel: https://www.kaggle.com/notslush/altair-visualization-2018-stackoverflow-survey\ndef prepare_altair():\n    \"\"\"\n    Helper function to prepare altair for working.\n    \"\"\"\n\n    vega_url = 'https://cdn.jsdelivr.net/npm/vega@' + v5.SCHEMA_VERSION\n    vega_lib_url = 'https://cdn.jsdelivr.net/npm/vega-lib'\n    vega_lite_url = 'https://cdn.jsdelivr.net/npm/vega-lite@' + alt.SCHEMA_VERSION\n    vega_embed_url = 'https://cdn.jsdelivr.net/npm/vega-embed@3'\n    noext = \"?noext\"\n    \n    paths = {\n        'vega': vega_url + noext,\n        'vega-lib': vega_lib_url + noext,\n        'vega-lite': vega_lite_url + noext,\n        'vega-embed': vega_embed_url + noext\n    }\n    \n    workaround = f\"\"\"    requirejs.config({{\n        baseUrl: 'https://cdn.jsdelivr.net/npm/',\n        paths: {paths}\n    }});\n    \"\"\"\n    \n    return workaround\n    \n\ndef add_autoincrement(render_func):\n    # Keep track of unique <div/> IDs\n    cache = {}\n    def wrapped(chart, id=\"vega-chart\", autoincrement=True):\n        if autoincrement:\n            if id in cache:\n                counter = 1 + cache[id]\n                cache[id] = counter\n            else:\n                cache[id] = 0\n            actual_id = id if cache[id] == 0 else id + '-' + str(cache[id])\n        else:\n            if id not in cache:\n                cache[id] = 0\n            actual_id = id\n        return render_func(chart, id=actual_id)\n    # Cache will stay outside and \n    return wrapped\n           \n\n@add_autoincrement\ndef render(chart, id=\"vega-chart\"):\n    \"\"\"\n    Helper function to plot altair visualizations.\n    \"\"\"\n    chart_str = \"\"\"\n    <div id=\"{id}\"></div><script>\n    require([\"vega-embed\"], function(vg_embed) {{\n        const spec = {chart};     \n        vg_embed(\"#{id}\", spec, {{defaultStyle: true}}).catch(console.warn);\n        console.log(\"anything?\");\n    }});\n    console.log(\"really...anything?\");\n    </script>\n    \"\"\"\n    return HTML(\n        chart_str.format(\n            id=id,\n            chart=json.dumps(chart) if isinstance(chart, dict) else chart.to_json(indent=None)\n        )\n    )\n    \n\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage(deep=True).sum() / 1024**2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                c_prec = df[col].apply(lambda x: np.finfo(x).precision).max()\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max and c_prec == np.finfo(np.float32).precision:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df\n    \n\n\ndef prepare_plot_dict(df, col):\n    \"\"\"\n    Preparing dictionary with data for plotting.\n    \n    I want to show how much higher/lower are the target rates for the current column comparing to base values (as described higher),\n    At first I calculate base rates, then for each category in the column I calculate target rate and find difference with the base rates.\n    \n    \"\"\"\n    main_count = train['target'].value_counts(normalize=True).sort_index()\n    main_count = dict(main_count)\n    plot_dict = {}\n    for i in df[col].unique():\n        val_count = dict(df.loc[df[col] == i, 'target'].value_counts().sort_index())\n\n        for k, v in main_count.items():\n            if k in val_count:\n                plot_dict[val_count[k]] = ((val_count[k] / sum(val_count.values())) / main_count[k]) * 100 - 100\n            else:\n                plot_dict[0] = 0\n\n    return plot_dict\n\ndef make_count_plot(df, x, hue='target', title=''):\n    \"\"\"\n    Plotting countplot with correct annotations.\n    \"\"\"\n    g = sns.countplot(x=x, data=df, hue=hue);\n    plt.title(f'Target {title}');\n    ax = g.axes\n\n    plot_dict = prepare_plot_dict(df, x)\n\n    for p in ax.patches:\n        h = p.get_height() if str(p.get_height()) != 'nan' else 0\n        text = f\"{plot_dict[h]:.0f}%\" if plot_dict[h] < 0 else f\"+{plot_dict[h]:.0f}%\"\n        ax.annotate(text, (p.get_x() + p.get_width() / 2., h),\n             ha='center', va='center', fontsize=11, color='green' if plot_dict[h] > 0 else 'red', rotation=0, xytext=(0, 10),\n             textcoords='offset points')  \n\ndef plot_two_graphs(col: str = '', top_n: int = None):\n    \"\"\"\n    Plotting four graphs:\n    - target by variable;\n    - counts of categories in the variable in train and test;\n    \"\"\"\n    data = train.copy()\n    all_data1 = all_data.copy()\n    if top_n:\n        top_cats = list(train[col].value_counts()[:5].index)\n        data = data.loc[data[col].isin(top_cats)]\n        all_data1 = all_data1.loc[all_data1[col].isin(top_cats)]\n        \n    plt.figure(figsize=(20, 12));\n    plt.subplot(2, 2, 1)\n    make_count_plot(df=data, x=col, title=f'and {col}')\n\n    plt.subplot(2, 2, 2)\n    sns.countplot(x='dataset_type', data=all_data1, hue=col);\n    plt.title(f'Count of samples in {col} in train and test data');\n\n    \n@jit\ndef fast_auc(y_true, y_prob):\n    \"\"\"\n    fast roc_auc computation: https://www.kaggle.com/c/microsoft-malware-prediction/discussion/76013\n    \"\"\"\n    y_true = np.asarray(y_true)\n    y_true = y_true[np.argsort(y_prob)]\n    nfalse = 0\n    auc = 0\n    n = len(y_true)\n    for i in range(n):\n        y_i = y_true[i]\n        nfalse += (1 - y_i)\n        auc += y_i * nfalse\n    auc /= (nfalse * (n - nfalse))\n    return auc\n\n\ndef eval_auc(y_true, y_pred):\n    \"\"\"\n    Fast auc eval function for lgb.\n    \"\"\"\n    return 'auc', fast_auc(y_true, y_pred), True\n\n\nclass DoubleValidationEncoderNumerical:\n    \"\"\"\n    Encoder with validation within\n    \"\"\"\n    def __init__(self, cols: List, encoder, folds):\n        \"\"\"\n        :param cols: Categorical columns\n        :param encoder: Encoder class\n        :param folds: Folds to split the data\n        \"\"\"\n        self.cols = cols\n        self.encoder = encoder\n        self.encoders_dict = {}\n        self.folds = folds\n\n    def fit_transform(self, X: pd.DataFrame, y: np.array) -> pd.DataFrame:\n        X = X.reset_index(drop=True)\n        y = y.reset_index(drop=True)\n        for n_fold, (train_idx, val_idx) in enumerate(self.folds.split(X, y)):\n            X_train, X_val = X.loc[train_idx].reset_index(drop=True), X.loc[val_idx].reset_index(drop=True)\n            y_train, y_val = y[train_idx], y[val_idx]\n            _ = self.encoder.fit_transform(X_train, y_train)\n\n            # transform validation part and get all necessary cols\n            val_t = self.encoder.transform(X_val)\n\n            if n_fold == 0:\n                cols_representation = np.zeros((X.shape[0], val_t.shape[1]))\n            \n            self.encoders_dict[n_fold] = self.encoder\n\n            cols_representation[val_idx, :] += val_t.values\n\n        cols_representation = pd.DataFrame(cols_representation, columns=X.columns)\n\n        return cols_representation\n\n    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n        X = X.reset_index(drop=True)\n\n        cols_representation = None\n\n        for encoder in self.encoders_dict.values():\n            test_tr = encoder.transform(X)\n\n            if cols_representation is None:\n                cols_representation = np.zeros(test_tr.shape)\n\n            cols_representation = cols_representation + test_tr / self.folds.n_splits\n\n        cols_representation = pd.DataFrame(cols_representation, columns=X.columns)\n        \n        return cols_representation\n\n\nclass FrequencyEncoder:\n    def __init__(self, cols):\n        self.cols = cols\n        self.counts_dict = None\n\n    def fit(self, X: pd.DataFrame, y=None) -> pd.DataFrame:\n        counts_dict = {}\n        for col in self.cols:\n            values, counts = np.unique(X[col], return_counts=True)\n            counts_dict[col] = dict(zip(values, counts))\n        self.counts_dict = counts_dict\n\n    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n        counts_dict_test = {}\n        res = []\n        for col in self.cols:\n            values, counts = np.unique(X[col], return_counts=True)\n            counts_dict_test[col] = dict(zip(values, counts))\n\n            # if value is in \"train\" keys - replace \"test\" counts with \"train\" counts\n            for k in [key for key in counts_dict_test[col].keys() if key in self.counts_dict[col].keys()]:\n                counts_dict_test[col][k] = self.counts_dict[col][k]\n\n            res.append(X[col].map(counts_dict_test[col]).values.reshape(-1, 1))\n        res = np.hstack(res)\n\n        X[self.cols] = res\n        return X\n\n    def fit_transform(self, X: pd.DataFrame, y=None) -> pd.DataFrame:\n        self.fit(X, y)\n        X = self.transform(X)\n        return X\n    \n\ndef train_model_classification(X, X_test, y, params, folds, model_type='lgb', eval_metric='auc', columns=None, plot_feature_importance=False, model=None,\n                               verbose=10000, early_stopping_rounds=200, n_estimators=50000, splits=None, n_folds=3, averaging='usual', n_jobs=-1, encoder=None, enc_val='single'):\n    \"\"\"\n    A function to train a variety of classification models.\n    Returns dictionary with oof predictions, test predictions, scores and, if necessary, feature importances.\n    \n    :params: X - training data, can be pd.DataFrame or np.ndarray (after normalizing)\n    :params: X_test - test data, can be pd.DataFrame or np.ndarray (after normalizing)\n    :params: y - target\n    :params: folds - folds to split data\n    :params: model_type - type of model to use\n    :params: eval_metric - metric to use\n    :params: columns - columns to use. If None - use all columns\n    :params: plot_feature_importance - whether to plot feature importance of LGB\n    :params: model - sklearn model, works only for \"sklearn\" model type\n    \n    \"\"\"\n    columns = X.columns if columns is None else columns\n    n_splits = folds.n_splits if splits is None else n_folds\n    X_test = X_test[columns]\n    \n    # to set up scoring parameters\n    metrics_dict = {'auc': {'lgb_metric_name': eval_auc,\n                        'catboost_metric_name': 'AUC',\n                        'sklearn_scoring_function': metrics.roc_auc_score},\n                    }\n    \n    result_dict = {}\n    if averaging == 'usual':\n        # out-of-fold predictions on train data\n        oof = np.zeros((len(X), 1))\n\n        # averaged predictions on train data\n        prediction = np.zeros((len(X_test), 1))\n        \n    elif averaging == 'rank':\n        # out-of-fold predictions on train data\n        oof = np.zeros((len(X), 1))\n\n        # averaged predictions on train data\n        prediction = np.zeros((len(X_test), 1))\n\n    \n    # list of scores on folds\n    scores = []\n    feature_importance = pd.DataFrame()\n        \n    # split and train on folds\n    for fold_n, (train_index, valid_index) in enumerate(folds.split(X, y)):\n        if verbose:\n            print(f'Fold {fold_n + 1} started at {time.ctime()}')\n        if type(X) == np.ndarray:\n            X_train, X_valid = X[columns][train_index], X[columns][valid_index]\n            y_train, y_valid = y[train_index], y[valid_index]\n        else:\n            X_train, X_valid = X[columns].iloc[train_index], X[columns].iloc[valid_index]\n            y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n            \n        \n        X_t = X_test.copy()\n            \n        if encoder and enc_val == 'single':\n            X_train = encoder.fit_transform(X_train, y_train)\n            X_valid = encoder.transform(X_valid)\n            X_t = encoder.transform(X_t)\n        elif encoder and enc_val == 'double':\n            encoder_double = DoubleValidationEncoderNumerical(cols=columns, encoder=encoder, folds=folds)\n            X_train = encoder_double.fit_transform(X_train, y_train)\n            X_valid = encoder_double.transform(X_valid)\n            X_t = encoder_double.transform(X_t)\n            \n            \n        if model_type == 'lgb':\n            model = lgb.LGBMClassifier(**params, n_estimators=n_estimators, n_jobs = n_jobs)\n            model.fit(X_train, y_train, \n                    eval_set=[(X_valid, y_valid)], eval_metric=metrics_dict[eval_metric]['lgb_metric_name'],\n                    verbose=verbose, early_stopping_rounds=early_stopping_rounds)\n            \n            y_pred_valid = model.predict_proba(X_valid)[:, 1]\n            y_pred = model.predict_proba(X_t, num_iteration=model.best_iteration_)[:, 1]\n            \n        if model_type == 'xgb':\n            train_data = xgb.DMatrix(data=X_train, label=y_train, feature_names=X.columns)\n            valid_data = xgb.DMatrix(data=X_valid, label=y_valid, feature_names=X.columns)\n\n            watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]\n            model = xgb.train(dtrain=train_data, num_boost_round=n_estimators, evals=watchlist, early_stopping_rounds=early_stopping_rounds, verbose_eval=verbose, params=params)\n            y_pred_valid = model.predict(xgb.DMatrix(X_valid, feature_names=X.columns), ntree_limit=model.best_ntree_limit)\n            y_pred = model.predict(xgb.DMatrix(X_t, feature_names=X.columns), ntree_limit=model.best_ntree_limit)\n        \n        if model_type == 'sklearn':\n            model = model\n            model.fit(X_train, y_train)\n            \n            y_pred_valid = model.predict(X_valid).reshape(-1,)\n            score = metrics_dict[eval_metric]['sklearn_scoring_function'](y_valid, y_pred_valid)\n            print(f'Fold {fold_n}. {eval_metric}: {score:.4f}.')\n            print('')\n            \n            y_pred = model.predict_proba(X_t)[:, 1]\n        \n        if model_type == 'cat':\n            model = CatBoostClassifier(iterations=n_estimators, eval_metric=metrics_dict[eval_metric]['catboost_metric_name'], **params,\n                                      loss_function='Logloss')\n            model.fit(X_train, y_train, eval_set=(X_valid, y_valid), cat_features=[], use_best_model=True, verbose=False)\n\n            y_pred_valid = model.predict(X_valid)\n            y_pred = model.predict(X_t)\n        \n        if averaging == 'usual':\n            \n            oof[valid_index] = y_pred_valid.reshape(-1, 1)\n            scores.append(metrics_dict[eval_metric]['sklearn_scoring_function'](y_valid, y_pred_valid))\n            \n            prediction += y_pred.reshape(-1, 1)\n\n        elif averaging == 'rank':\n                                  \n            oof[valid_index] = y_pred_valid.reshape(-1, 1)\n            scores.append(metrics_dict[eval_metric]['sklearn_scoring_function'](y_valid, y_pred_valid))\n                                  \n            prediction += pd.Series(y_pred).rank().values.reshape(-1, 1)        \n        \n        if model_type == 'lgb':\n            # feature importance\n            fold_importance = pd.DataFrame()\n            fold_importance[\"feature\"] = columns\n            fold_importance[\"importance\"] = model.feature_importances_\n            fold_importance[\"fold\"] = fold_n + 1\n            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n\n    prediction /= n_splits\n    if verbose:\n        print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n    \n    result_dict['oof'] = oof\n    result_dict['prediction'] = prediction\n    result_dict['scores'] = scores\n    \n    if model_type == 'lgb':\n        feature_importance[\"importance\"] /= n_splits\n        cols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n            by=\"importance\", ascending=False)[:50].index\n\n        best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n\n        result_dict['feature_importance'] = feature_importance\n        result_dict['top_columns'] = list(cols)\n        if plot_feature_importance:\n\n            plt.figure(figsize=(16, 12));\n            sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False));\n            plt.title('LGB Features (avg over folds)');\n            \n        \n    return result_dict\n\n# setting up altair\nworkaround = prepare_altair()\nHTML(\"\".join((\n    \"<script>\",\n    workaround,\n    \"</script>\",\n)))", "processed": ["helper function class"]}, {"markdown": ["# <a id='3'>Data exploration</a>  \n\nThe comments are stored in `train` and `test` in `comment_text` column.  \nAdditionally, in `train` we have flags for the presence in the comments of a certain sensitive topic.\nThe topic is related to five categories: race or ethnicity, gender, sexual orientation, religion, disability, as following:\n* **race or ethnicity**: asian, black, jewish, latino, other_race_or_ethnicity, white  \n* **gender**: female, male, transgender, other_gender  \n* **sexual orientation**: bisexual, heterosexual, homosexual_gay_or_lesbian, other_sexual_orientation  \n* **religion**: atheist,buddhist,  christian, hindu, muslim, other_religion  \n* **disability**: intellectual_or_learning_disability, other_disability, physical_disability, psychiatric_or_mental_illness  \n\nWe also have few article/comment identification information:\n* created_date  \n* publication_id   \n* parent_id  \n* article_id \n\nSeveral user feedback information associated with the comments are provided:\n* rating  \n* funny  \n* wow  \n* sad  \n* likes  \n* disagree  \n* sexual_explicit  \n\nIn the datasets are also 2 fields relative to annotations:\n* identity_annotator_count  \n* toxicity_annotator_count\n\n\n", "##  <a id='31'>Target feature</a>\n\nLet's check the distribution of `target` value in the train set."], "code": "# Reference: https://www.kaggle.com/code/gpreda/jigsaw-eda\n\nplt.figure(figsize=(12,6))\nplt.title(\"Distribution of target in the train set\")\nsns.distplot(train['target'],kde=True,hist=False, bins=120, label='target')\nplt.legend(); plt.show()", "processed": ["id 3 data explor comment store train test comment text column addit train flag presenc comment certain sensit topic topic relat five categori race ethnic gender sexual orient religion disabl follow race ethnic asian black jewish latino race ethnic white gender femal male transgend gender sexual orient bisexu heterosexu homosexu gay lesbian sexual orient religion atheist buddhist christian hindu muslim religion disabl intellectu learn disabl disabl physic disabl psychiatr mental ill also articl comment identif inform creat date public id parent id articl id sever user feedback inform associ comment provid rate funni wow sad like disagre sexual explicit dataset also 2 field rel annot ident annot count toxic annot count", "id 31 target featur let check distribut target valu train set"]}, {"markdown": ["And let's represent similarly the distribution of the additional toxicity features."], "code": "# Reference: https://www.kaggle.com/code/gpreda/jigsaw-eda\n\ndef plot_features_distribution(features, title):\n    plt.figure(figsize=(12,6))\n    plt.title(title)\n    for feature in features:\n        sns.distplot(train.loc[~train[feature].isnull(),feature],kde=True,hist=False, bins=120, label=feature)\n    plt.xlabel('')\n    plt.legend()\n    plt.show()\nfeatures = ['severe_toxicity', 'obscene','identity_attack','insult','threat']\nplot_features_distribution(features, \"Distribution of additional toxicity features in the train set\")", "processed": ["let repres similarli distribut addit toxic featur"]}, {"markdown": ["## <a id='33'>Feedback information</a>\n\nLet's show the feedback values distribution."], "code": "# Reference: https://www.kaggle.com/code/gpreda/jigsaw-eda\n\ndef plot_count(feature, title,size=1):\n    f, ax = plt.subplots(1,1, figsize=(4*size,4))\n    total = float(len(train))\n    g = sns.countplot(train[feature], order = train[feature].value_counts().index[:20], palette='Set3')\n    g.set_title(\"Number and percentage of {}\".format(title))\n    for p in ax.patches:\n        height = p.get_height()\n        ax.text(p.get_x()+p.get_width()/2.,\n                height + 3,\n                '{:1.2f}%'.format(100*height/total),\n                ha=\"center\") \n    plt.show()   \nplot_count('rating','rating')\nplot_count('funny','funny votes given',3)\nplot_count('wow','wow votes given',3)\nplot_count('sad','sad votes given',3)\nplot_count('likes','likes given',3)\nplot_count('disagree','disagree given',3)\nfeatures = ['sexual_explicit']\nplot_features_distribution(features, \"Distribution of sexual explicit values in the train set\")", "processed": ["id 33 feedback inform let show feedback valu distribut"]}, {"markdown": ["## Helper functions and classes"], "code": "# Reference: https://www.kaggle.com/code/artgor/march-madness-2020-ncaam-eda-and-baseline\n\nclass LGBWrapper(object):\n    \"\"\"\n    A wrapper for lightgbm model so that we will have a single api for various models.\n    \"\"\"\n\n    def __init__(self):\n        self.model = lgb.LGBMClassifier()\n\n    def fit(self, X_train, y_train, X_valid=None, y_valid=None, X_holdout=None, y_holdout=None, params=None):\n\n        eval_set = [(X_train, y_train)]\n        eval_names = ['train']\n        self.model = self.model.set_params(**params)\n\n        if X_valid is not None:\n            eval_set.append((X_valid, y_valid))\n            eval_names.append('valid')\n\n        if X_holdout is not None:\n            eval_set.append((X_holdout, y_holdout))\n            eval_names.append('holdout')\n\n        if 'cat_cols' in params.keys():\n            cat_cols = [col for col in params['cat_cols'] if col in X_train.columns]\n            if len(cat_cols) > 0:\n                categorical_columns = params['cat_cols']\n            else:\n                categorical_columns = 'auto'\n        else:\n            categorical_columns = 'auto'\n\n        self.model.fit(X=X_train, y=y_train,\n                       eval_set=eval_set, eval_names=eval_names,\n                       verbose=params['verbose'], early_stopping_rounds=params['early_stopping_rounds'])\n\n        self.best_score_ = self.model.best_score_\n        self.feature_importances_ = self.model.feature_importances_\n\n    def predict_proba(self, X_test):\n        if self.model.objective == 'binary':\n            return self.model.predict_proba(X_test, num_iteration=self.model.best_iteration_)[:, 1]\n        else:\n            return self.model.predict_proba(X_test, num_iteration=self.model.best_iteration_)\nclass MainTransformer(BaseEstimator, TransformerMixin):\n\n    def __init__(self, convert_cyclical: bool = False, create_interactions: bool = False, n_interactions: int = 20):\n        \"\"\"\n        Main transformer for the data. Can be used for processing on the whole data.\n\n        :param convert_cyclical: convert cyclical features into continuous\n        :param create_interactions: create interactions between features\n        \"\"\"\n\n        self.convert_cyclical = convert_cyclical\n        self.create_interactions = create_interactions\n        self.feats_for_interaction = None\n        self.n_interactions = n_interactions\n\n    def fit(self, X, y=None):\n\n        if self.create_interactions:\n            pass\n        return self\n\n    def transform(self, X, y=None):\n        data = copy.deepcopy(X)\n\n        # data['installation_event_code_count_mean'] = data.groupby(['installation_id'])['sum_event_code_count'].transform('mean')\n\n        return data\n\n    def fit_transform(self, X, y=None, **fit_params):\n        data = copy.deepcopy(X)\n        self.fit(data)\n        return self.transform(data)\n\n\nclass FeatureTransformer(BaseEstimator, TransformerMixin):\n\n    def __init__(self, main_cat_features: list = None, num_cols: list = None):\n        \"\"\"\n\n        :param main_cat_features:\n        :param num_cols:\n        \"\"\"\n        self.main_cat_features = main_cat_features\n        self.num_cols = num_cols\n\n    def fit(self, X, y=None):\n\n        self.num_cols = [col for col in X.columns if 'sum' in col or 'mean' in col or 'max' in col or 'std' in col\n                         or 'attempt' in col]\n\n        return self\n\n    def transform(self, X, y=None):\n        data = copy.deepcopy(X)\n#         for col in self.num_cols:\n#             data[f'{col}_to_mean'] = data[col] / data.groupby('installation_id')[col].transform('mean')\n#             data[f'{col}_to_std'] = data[col] / data.groupby('installation_id')[col].transform('std')\n\n        return data\n\n    def fit_transform(self, X, y=None, **fit_params):\n        data = copy.deepcopy(X)\n        self.fit(data)\n        return self.transform(data)\nclass ClassifierModel(object):\n    \"\"\"\n    A wrapper class for classification models.\n    It can be used for training and prediction.\n    Can plot feature importance and training progress (if relevant for model).\n\n    \"\"\"\n\n    def __init__(self, columns: list = None, model_wrapper=None):\n        \"\"\"\n\n        :param original_columns:\n        :param model_wrapper:\n        \"\"\"\n        self.columns = columns\n        self.model_wrapper = model_wrapper\n        self.result_dict = {}\n        self.train_one_fold = False\n        self.preprocesser = None\n\n    def fit(self, X: pd.DataFrame, y,\n            X_holdout: pd.DataFrame = None, y_holdout=None,\n            folds=None,\n            params: dict = None,\n            eval_metric='auc',\n            cols_to_drop: list = None,\n            preprocesser=None,\n            transformers: dict = None,\n            adversarial: bool = False,\n            plot: bool = True):\n        \"\"\"\n        Training the model.\n\n        :param X: training data\n        :param y: training target\n        :param X_holdout: holdout data\n        :param y_holdout: holdout target\n        :param folds: folds to split the data. If not defined, then model will be trained on the whole X\n        :param params: training parameters\n        :param eval_metric: metric for validataion\n        :param cols_to_drop: list of columns to drop (for example ID)\n        :param preprocesser: preprocesser class\n        :param transformers: transformer to use on folds\n        :param adversarial\n        :return:\n        \"\"\"\n        self.cols_to_drop = cols_to_drop\n\n        if folds is None:\n            folds = KFold(n_splits=3, random_state=42)\n            self.train_one_fold = True\n\n        self.columns = X.columns if self.columns is None else self.columns\n        self.feature_importances = pd.DataFrame(columns=['feature', 'importance'])\n        self.trained_transformers = {k: [] for k in transformers}\n        self.transformers = transformers\n        self.models = []\n        self.folds_dict = {}\n        self.eval_metric = eval_metric\n        n_target = 1 if len(set(y.values)) == 2 else len(set(y.values))\n        self.oof = np.zeros((len(X), n_target))\n        self.n_target = n_target\n\n        X = X[self.columns]\n        if X_holdout is not None:\n            X_holdout = X_holdout[self.columns]\n\n        if preprocesser is not None:\n            self.preprocesser = preprocesser\n            self.preprocesser.fit(X, y)\n            X = self.preprocesser.transform(X, y)\n            self.columns = X.columns.tolist()\n            if X_holdout is not None:\n                X_holdout = self.preprocesser.transform(X_holdout)\n            # y = X['accuracy_group']\n\n        for fold_n, (train_index, valid_index) in enumerate(folds.split(X, y)):\n            if X_holdout is not None:\n                X_hold = X_holdout.copy()\n            else:\n                X_hold = None\n            self.folds_dict[fold_n] = {}\n            if params['verbose']:\n                print(f'Fold {fold_n + 1} started at {time.ctime()}')\n            self.folds_dict[fold_n] = {}\n\n            X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n            y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n            if self.train_one_fold:\n                X_train = X[self.original_columns]\n                y_train = y\n                X_valid = None\n                y_valid = None\n\n            datasets = {'X_train': X_train, 'X_valid': X_valid, 'X_holdout': X_hold, 'y_train': y_train}\n            X_train, X_valid, X_hold = self.transform_(datasets, cols_to_drop)\n\n            self.folds_dict[fold_n]['columns'] = X_train.columns.tolist()\n\n            model = copy.deepcopy(self.model_wrapper)\n\n            if adversarial:\n                X_new1 = X_train.copy()\n                if X_valid is not None:\n                    X_new2 = X_valid.copy()\n                elif X_holdout is not None:\n                    X_new2 = X_holdout.copy()\n                X_new = pd.concat([X_new1, X_new2], axis=0)\n                y_new = np.hstack((np.zeros((X_new1.shape[0])), np.ones((X_new2.shape[0]))))\n                X_train, X_valid, y_train, y_valid = train_test_split(X_new, y_new)\n\n            model.fit(X_train, y_train, X_valid, y_valid, X_hold, y_holdout, params=params)\n\n            self.folds_dict[fold_n]['scores'] = model.best_score_\n            if self.oof.shape[0] != len(X):\n                self.oof = np.zeros((X.shape[0], self.oof.shape[1]))\n            if not adversarial:\n                self.oof[valid_index] = model.predict_proba(X_valid).reshape(-1, n_target)\n\n            fold_importance = pd.DataFrame(list(zip(X_train.columns, model.feature_importances_)),\n                                           columns=['feature', 'importance'])\n            self.feature_importances = self.feature_importances.append(fold_importance)\n            self.models.append(model)\n\n        self.feature_importances['importance'] = self.feature_importances['importance'].astype(float)\n\n        # if params['verbose']:\n        self.calc_scores_()\n\n        if plot:\n#             print(classification_report(y, self.oof.argmax(1)))\n            print(classification_report(y, (self.oof > 0.5) * 1))\n            fig, ax = plt.subplots(figsize=(16, 12))\n            plt.subplot(2, 2, 1)\n            self.plot_feature_importance(top_n=25)\n            plt.subplot(2, 2, 2)\n            self.plot_metric()\n            plt.subplot(2, 2, 3)\n            g = sns.heatmap(confusion_matrix(y, (self.oof > 0.5) * 1), annot=True, cmap=plt.cm.Blues,fmt=\"d\")\n            g.set(ylim=(-0.5, 4), xlim=(-0.5, 4), title='Confusion matrix')\n\n            plt.subplot(2, 2, 4)\n            plt.hist(self.oof)\n            plt.xticks(range(self.n_target), range(self.n_target))\n            plt.title('Distribution of oof predictions');\n\n    def transform_(self, datasets, cols_to_drop):\n        for name, transformer in self.transformers.items():\n            transformer.fit(datasets['X_train'], datasets['y_train'])\n            datasets['X_train'] = transformer.transform(datasets['X_train'])\n            if datasets['X_valid'] is not None:\n                datasets['X_valid'] = transformer.transform(datasets['X_valid'])\n            if datasets['X_holdout'] is not None:\n                datasets['X_holdout'] = transformer.transform(datasets['X_holdout'])\n            self.trained_transformers[name].append(transformer)\n        if cols_to_drop is not None:\n            cols_to_drop = [col for col in cols_to_drop if col in datasets['X_train'].columns]\n            self.cols_to_drop = cols_to_drop\n            datasets['X_train'] = datasets['X_train'].drop(cols_to_drop, axis=1)\n            if datasets['X_valid'] is not None:\n                datasets['X_valid'] = datasets['X_valid'].drop(cols_to_drop, axis=1)\n            if datasets['X_holdout'] is not None:\n                datasets['X_holdout'] = datasets['X_holdout'].drop(cols_to_drop, axis=1)\n\n        return datasets['X_train'], datasets['X_valid'], datasets['X_holdout']\n\n    def calc_scores_(self):\n        print()\n        datasets = [k for k, v in [v['scores'] for k, v in self.folds_dict.items()][0].items() if len(v) > 0]\n        self.scores = {}\n        for d in datasets:\n            scores = [v['scores'][d][self.eval_metric] for k, v in self.folds_dict.items()]\n            print(f\"CV mean score on {d}: {np.mean(scores):.4f} +/- {np.std(scores):.4f} std.\")\n            self.scores[d] = np.mean(scores)\n\n    def predict(self, X_test, averaging: str = 'usual'):\n        \"\"\"\n        Make prediction\n\n        :param X_test:\n        :param averaging: method of averaging\n        :return:\n        \"\"\"\n        full_prediction = np.zeros((X_test.shape[0], self.oof.shape[1]))\n        if self.preprocesser is not None:\n            X_test = self.preprocesser.transform(X_test)\n        for i in range(len(self.models)):\n            X_t = X_test.copy()\n            for name, transformers in self.trained_transformers.items():\n                X_t = transformers[i].transform(X_t)\n            if self.cols_to_drop:\n                cols_to_drop = [col for col in self.cols_to_drop if col in X_t.columns]\n                X_t = X_t.drop(cols_to_drop, axis=1)\n            y_pred = self.models[i].predict_proba(X_t[self.folds_dict[i]['columns']]).reshape(-1, full_prediction.shape[1])\n\n            # if case transformation changes the number of the rows\n            if full_prediction.shape[0] != len(y_pred):\n                full_prediction = np.zeros((y_pred.shape[0], self.oof.shape[1]))\n\n            if averaging == 'usual':\n                full_prediction += y_pred\n            elif averaging == 'rank':\n                full_prediction += pd.Series(y_pred).rank().values\n\n        return full_prediction / len(self.models)\n\n    def plot_feature_importance(self, drop_null_importance: bool = True, top_n: int = 10):\n        \"\"\"\n        Plot default feature importance.\n\n        :param drop_null_importance: drop columns with null feature importance\n        :param top_n: show top n columns\n        :return:\n        \"\"\"\n\n        top_feats = self.get_top_features(drop_null_importance, top_n)\n        feature_importances = self.feature_importances.loc[self.feature_importances['feature'].isin(top_feats)]\n        feature_importances['feature'] = feature_importances['feature'].astype(str)\n        top_feats = [str(i) for i in top_feats]\n        sns.barplot(data=feature_importances, x='importance', y='feature', orient='h', order=top_feats)\n        plt.title('Feature importances')\n\n    def get_top_features(self, drop_null_importance: bool = True, top_n: int = 10):\n        \"\"\"\n        Get top features by importance.\n\n        :param drop_null_importance:\n        :param top_n:\n        :return:\n        \"\"\"\n        grouped_feats = self.feature_importances.groupby(['feature'])['importance'].mean()\n        if drop_null_importance:\n            grouped_feats = grouped_feats[grouped_feats != 0]\n        return list(grouped_feats.sort_values(ascending=False).index)[:top_n]\n\n    def plot_metric(self):\n        \"\"\"\n        Plot training progress.\n        Inspired by `plot_metric` from https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/plotting.html\n\n        :return:\n        \"\"\"\n        full_evals_results = pd.DataFrame()\n        for model in self.models:\n            evals_result = pd.DataFrame()\n            for k in model.model.evals_result_.keys():\n                evals_result[k] = model.model.evals_result_[k][self.eval_metric]\n            evals_result = evals_result.reset_index().rename(columns={'index': 'iteration'})\n            full_evals_results = full_evals_results.append(evals_result)\n\n        full_evals_results = full_evals_results.melt(id_vars=['iteration']).rename(columns={'value': self.eval_metric,\n                                                                                            'variable': 'dataset'})\n        full_evals_results[self.eval_metric] = np.abs(full_evals_results[self.eval_metric])\n        sns.lineplot(data=full_evals_results, x='iteration', y=self.eval_metric, hue='dataset')\n        plt.title('Training progress')", "processed": ["helper function class"]}, {"markdown": ["Here we can see pairs of teams which played in tournaments."], "code": "# Reference: https://www.kaggle.com/code/artgor/march-madness-2020-ncaam-eda-and-baseline\n\ndata_dict['MNCAATourneyCompactResults'].groupby(['Season'])['WScore'].mean().plot(kind='line');\nplt.title('Mean scores of winning teams by season in tourneys');", "processed": ["see pair team play tournament"]}, {"markdown": ["And here we can see the results of regular seasons."], "code": "# Reference: https://www.kaggle.com/code/artgor/march-madness-2020-ncaam-eda-and-baseline\n\ndata_dict['MRegularSeasonCompactResults'].groupby(['Season'])['WScore'].mean().plot();\nplt.title('Mean scores of winning teams by season in regular plays');", "processed": ["see result regular season"]}, {"markdown": ["## Model comparison\n\nIn this section I'll try variuos sklearn models and compair their score. Running GridSearchCV each time is too long, so I'll run it once for each model and use optimal parameters."], "code": "# Reference: https://www.kaggle.com/code/artgor/feature-selection-model-interpretation-and-more\n\n%%time\nrfr = RandomForestRegressor()\n\n# parameter_grid = {'n_estimators': [50, 60],\n#                   'max_depth': [5, 10]\n#                  }\n\n# grid_search = GridSearchCV(rfr, param_grid=parameter_grid, cv=folds, scoring='neg_mean_absolute_error', n_jobs=-1)\n# grid_search.fit(X, y)\n# print('Best score: {}'.format(grid_search.best_score_))\n# print('Best parameters: {}'.format(grid_search.best_params_))\n# rfr = RandomForestRegressor(**grid_search.best_params_)\nrfr = RandomForestRegressor(n_estimators=50, max_depth=5)\nresult_dict_rfr = artgor_utils.train_model_regression(X, X_test, y, params=params, folds=folds, model_type='sklearn', model=rfr)\n# print(scores_rfr)\n%%time\nlinreg = linear_model.LinearRegression(normalize=False, copy_X=True, n_jobs=-1)\n\nresult_dict_linreg = artgor_utils.train_model_regression(X, X_test, y, params=None, folds=folds, model_type='sklearn', model=linreg)\n# print(scores_linreg)\n%%time\nridge = linear_model.Ridge(normalize=True)\n\nparameter_grid = {'alpha': [0.1, 1.0, 10.0]}\n\ngrid_search = GridSearchCV(ridge, param_grid=parameter_grid, cv=folds, scoring='neg_mean_absolute_error', n_jobs=-1)\ngrid_search.fit(X, y)\nprint('Best score: {}'.format(grid_search.best_score_))\nprint('Best parameters: {}'.format(grid_search.best_params_))\nridge = linear_model.Ridge(**grid_search.best_params_, normalize=True)\nresult_dict_ridge = artgor_utils.train_model_regression(X, X_test, y, params=params, folds=folds, model_type='sklearn', model=ridge)\n# print(scores_ridge)\n%%time\nknn = neighbors.KNeighborsRegressor()\n\nparameter_grid = {'n_neighbors': [50, 100]}\n\ngrid_search = GridSearchCV(knn, param_grid=parameter_grid, cv=folds, scoring='neg_mean_absolute_error', n_jobs=-1)\ngrid_search.fit(X, y)\nprint('Best score: {}'.format(grid_search.best_score_))\nprint('Best parameters: {}'.format(grid_search.best_params_))\nknn = neighbors.KNeighborsRegressor(**grid_search.best_params_)\nresult_dict_knn = artgor_utils.train_model_regression(X, X_test, y, params=params, folds=folds, model_type='sklearn', model=knn)\n%%time\nlasso = linear_model.Lasso(normalize=True)\n\nparameter_grid = {'alpha': [0.1, 1.0, 10.0]}\n\ngrid_search = GridSearchCV(lasso, param_grid=parameter_grid, cv=folds, scoring='neg_mean_absolute_error', n_jobs=-1)\ngrid_search.fit(X, y)\nprint('Best score: {}'.format(grid_search.best_score_))\nprint('Best parameters: {}'.format(grid_search.best_params_))\nlasso = linear_model.Lasso(**grid_search.best_params_, normalize=True)\nresult_dict_lasso = artgor_utils.train_model_regression(X, X_test, y, params=params, folds=folds, model_type='sklearn', model=lasso)\n%%time\netr = ExtraTreesRegressor()\n\n# parameter_grid = {'n_estimators': [500, 1000],\n#                   'max_depth': [5, 10, 20]\n#                  }\n\n# grid_search = GridSearchCV(rfr, param_grid=parameter_grid, cv=folds, scoring='neg_mean_absolute_error', n_jobs=-1)\n# grid_search.fit(X, y)\n# print('Best score: {}'.format(grid_search.best_score_))\n# print('Best parameters: {}'.format(grid_search.best_params_))\n# etr = ExtraTreesRegressor(**grid_search.best_params_)\netr = ExtraTreesRegressor(n_estimators=1000, max_depth=10)\nresult_dict_etr = artgor_utils.train_model_regression(X, X_test, y, params=params, folds=folds, model_type='sklearn', model=etr)\n%%time\nadr = AdaBoostRegressor()\n\nparameter_grid = {'n_estimators': [10, 50],\n                 }\n\ngrid_search = GridSearchCV(adr, param_grid=parameter_grid, cv=folds, scoring='neg_mean_absolute_error', n_jobs=-1)\ngrid_search.fit(X, y)\nprint('Best score: {}'.format(grid_search.best_score_))\nprint('Best parameters: {}'.format(grid_search.best_params_))\nadr = AdaBoostRegressor(**grid_search.best_params_)\nresult_dict_adr = artgor_utils.train_model_regression(X, X_test, y, params=params, folds=folds, model_type='sklearn', model=adr)\nplt.figure(figsize=(12, 8));\nscores_df = pd.DataFrame({'RandomForestRegressor': result_dict_rfr['scores']})\nscores_df['ExtraTreesRegressor'] = result_dict_etr['scores']\nscores_df['AdaBoostRegressor'] = result_dict_adr['scores']\nscores_df['KNN'] = result_dict_knn['scores']\nscores_df['LinearRegression'] = result_dict_linreg['scores']\nscores_df['Ridge'] = result_dict_ridge['scores']\nscores_df['Lasso'] = result_dict_lasso['scores']\n\nsns.boxplot(data=scores_df);\nplt.xticks(rotation=45);", "processed": ["model comparison section tri variuo sklearn model compair score run gridsearchcv time long run model use optim paramet"]}, {"markdown": ["## Blending\n\nLet's try training and blending several models."], "code": "# Reference: https://www.kaggle.com/code/artgor/feature-selection-model-interpretation-and-more\n\nparams = {'num_leaves': 128,\n          'min_child_samples': 79,\n          'objective': 'gamma',\n          'max_depth': -1,\n          'learning_rate': 0.01,\n          \"boosting_type\": \"gbdt\",\n          \"subsample_freq\": 5,\n          \"subsample\": 0.9,\n          \"bagging_seed\": 11,\n          \"metric\": 'mae',\n          \"verbosity\": -1,\n          'reg_alpha': 0.1302650970728192,\n          'reg_lambda': 0.3603427518866501,\n          'colsample_bytree': 0.2\n         }\nresult_dict_lgb = artgor_utils.train_model_regression(X=X, X_test=X_test, y=y, params=params, folds=folds, model_type='lgb',\n                                                                                  eval_metric='mae', plot_feature_importance=True)\nxgb_params = {'eta': 0.03,\n              'max_depth': 9,\n              'subsample': 0.85,\n              'colsample_bytree': 0.3,\n              'objective': 'reg:linear',\n              'eval_metric': 'mae',\n              'silent': True,\n              'nthread': -1}\nresult_dict_xgb = artgor_utils.train_model_regression(X=X, X_test=X_test, y=y, params=xgb_params, folds=folds, model_type='xgb')\nsubmission['time_to_failure'] = (result_dict_lgb['prediction'] + result_dict_etr['prediction'] + result_dict_xgb['prediction']) / 3\nprint(submission.head())\nsubmission.to_csv('blending.csv')\nplt.figure(figsize=(18, 8))\nplt.subplot(2, 2, 1)\nplt.plot(y, color='g', label='y_train')\nplt.plot(result_dict_lgb['oof'], color='b', label='lgb')\nplt.legend(loc=(1, 0.5));\nplt.title('lgb');\nplt.subplot(2, 2, 2)\nplt.plot(y, color='g', label='y_train')\nplt.plot(result_dict_etr['oof'], color='teal', label='xgb')\nplt.legend(loc=(1, 0.5));\nplt.title('xgb');\nplt.subplot(2, 2, 3)\nplt.plot(y, color='g', label='y_train')\nplt.plot(result_dict_xgb['oof'], color='red', label='etr')\nplt.legend(loc=(1, 0.5));\nplt.title('Extratrees');\nplt.subplot(2, 2, 4)\nplt.plot(y, color='g', label='y_train')\nplt.plot((result_dict_lgb['oof'] + result_dict_etr['oof'] + result_dict_xgb['oof']) / 3, color='gold', label='blend')\nplt.legend(loc=(1, 0.5));\nplt.title('blend');", "processed": ["blend let tri train blend sever model"]}, {"markdown": ["Now, we are ready to start exploring the data to try get a good insight that can help us to build the Algorithm;<br>\nWe can see that we have many different metrics that was included only on training data... It will be useful to we better understand the categories.", "## UNIQUE QUESTIONS\n- Let's start by a more generalist understanding and how the questions are distributed between the different categories and hosts"], "code": "# Reference: https://www.kaggle.com/code/kabure/qa-eda-and-nlp-modelling-insights-vis-bert\n\n## I will use the host column, split and get the first string\nfor i in range(len(df_train)):\n    df_train.loc[i,'host_cat'] = df_train.host.str.split('.')[i][0]\ndf_train.drop('host', axis=1, inplace=True)\nhost = df_train.groupby(['host_cat'])['url'].nunique().sort_values(ascending=False)\ncategory = df_train.groupby(['category'])['url'].nunique().sort_values(ascending=False)\n\nplt.figure(figsize=(16,12))\nplt.suptitle('Unique URL by Host and Categories', size=22)\n\nplt.subplot(211)\ng0 = sns.barplot(x=category.index, y=category.values, color='blue')\ng0.set_title(\"Unique Answers by category\", fontsize=22)\ng0.set_xlabel(\"Category Name\", fontsize=19)\ng0.set_ylabel(\"Total Count\", fontsize=19)\n#g1.set_xticklabels(g1.get_xticklabels(),rotation=45)\nfor p in g0.patches:\n    height = p.get_height()\n    g0.text(p.get_x()+p.get_width()/2.,\n            height + 3,\n            '{:1.1f}%'.format(height/category.sum()*100),\n            ha=\"center\",fontsize=11) \n\nplt.subplot(212)\ng1 = sns.barplot(x=host[:20].index, y=host[:20].values, color='blue')\ng1.set_title(\"TOP 20 HOSTS with more UNIQUE questions\", fontsize=22)\ng1.set_xlabel(\"Host Name\", fontsize=19)\ng1.set_ylabel(\"Total Count\", fontsize=19)\ng1.set_xticklabels(g1.get_xticklabels(),rotation=45)\nfor p in g1.patches:\n    height = p.get_height()\n    g1.text(p.get_x()+p.get_width()/2.,\n            height + 3,\n            '{:1.1f}%'.format(height/host.sum()*100),\n            ha=\"center\",fontsize=11) \n    \nplt.subplots_adjust(hspace = 0.3, top = 0.90)\n\nplt.show()", "processed": ["readi start explor data tri get good insight help u build algorithm br see mani differ metric includ train data use better understand categori", "uniqu question let start generalist understand question distribut differ categori host"]}, {"markdown": ["It's very interesting. <br>\nWe can see that a small part of total users has done both actions on the platform. <br>\nI thought it had a more balanced ratio in users that do questions and who answer it;\n\n", "##  QA USERS Intersection by CATEGORY feature"], "code": "# Reference: https://www.kaggle.com/code/kabure/qa-eda-and-nlp-modelling-insights-vis-bert\n\nimport matplotlib.gridspec as gridspec # to do the grid of plots\n\ngrid = gridspec.GridSpec(3, 3)\nplt.figure(figsize=(16,3*4))\n\nplt.suptitle('Intersection QA USERS \\nQuestions and Answers by different CATEGORIES', size=20)\n\nfor n, col in enumerate(df_train['category'].value_counts().index):\n    ax = plt.subplot(grid[n])\n    venn2([set(df_train[df_train.category == col]['question_user_name'].value_counts(dropna=False).index), \n           set(df_train[df_train.category == col]['answer_user_name'].value_counts(dropna=False).index)],\n      set_labels=('Question Users', 'Answer Users'), )\n    ax.set_title(str(col), fontsize=15)\n    ax.set_xlabel('')\n    #plt.subplots_adjust(top = 0.98, wspace=.9, hspace=.9)\n    \nplt.subplots_adjust(top = 0.9, hspace=.1)\n\nplt.show()", "processed": ["interest br see small part total user done action platform br thought balanc ratio user question answer", "qa user intersect categori featur"]}, {"markdown": ["It's interesting, we can see some difference in the intersection ratio of the USERS in the different categories;<br>\nTake a look on Stackoverflow, it has a big difference between people who ask and people who answer questions; ", "## QA USERS Intersection by Host feature"], "code": "# Reference: https://www.kaggle.com/code/kabure/qa-eda-and-nlp-modelling-insights-vis-bert\n\ngrid = gridspec.GridSpec(5, 3)\nplt.figure(figsize=(16,4.5*4))\n\nplt.suptitle('Intersection QA USERS - TOP 15 \\nQuestions and Answers by different HOSTS', size=20)\ntop_host = df_train['host_cat'].value_counts()[:15].index\nfor n, col in enumerate(top_host):\n    ax = plt.subplot(grid[n])\n    venn2([set(df_train[df_train.host_cat == col]['question_user_name'].value_counts(dropna=False).index), \n           set(df_train[df_train.host_cat == col]['answer_user_name'].value_counts(dropna=False).index)],\n      set_labels=('Question Users', 'Answer Users'), )\n    ax.set_title(str(col), fontsize=15)\n    ax.set_xlabel('')\n    #plt.subplots_adjust(top = 0.98, wspace=.9, hspace=.9)\n    \nplt.subplots_adjust(top = 0.9, hspace=.1)\n\nplt.show()", "processed": ["interest see differ intersect ratio user differ categori br take look stackoverflow big differ peopl ask peopl answer question", "qa user intersect host featur"]}, {"markdown": [""], "code": "# Reference: https://www.kaggle.com/code/kabure/qa-eda-and-nlp-modelling-insights-vis-bert\n\ngrid = gridspec.GridSpec(5, 3)\nplt.figure(figsize=(16,6*4))\n\nplt.suptitle('Title and Question Lenghts by Different Categories \\nThe Mean in RED - Also 5% and 95% lines', size=20)\ncount=0\ntop_cats=df_train['category'].value_counts().index\nfor n, col in enumerate(top_cats):\n    for i, q_t in enumerate(['question_title', 'question_body', 'question_n_words']):\n        ax = plt.subplot(grid[count])\n        if q_t == 'question_n_words':\n            sns.distplot(df_train[df_train['category'] == col][q_t], bins = 50, \n                         color='g', label=\"RED - 50%\") \n            ax.set_title(f\"Distribution of {str(col)} \\nQuestion #Total Words Distribution\", fontsize=15)\n            ax.axvline(df_train[df_train['category'] == col][q_t].quantile(.95))\n            ax.axvline(df_train[df_train['category'] == col][q_t].quantile(.05))\n            mean_val = df_train[df_train['category'] == col][q_t].mean()\n            ax.axvline(mean_val, color='red' )\n            ax.set_xlabel('')            \n        else:\n            sns.distplot(df_train[df_train['category'] == col][q_t].str.len(), bins = 50, \n                         color='g', label=\"RED - 50%\") \n            ax.set_title(f\"Distribution of {str(col)} \\n{str(q_t)}\", fontsize=15)\n            ax.axvline(df_train[df_train['category'] == col][q_t].str.len().quantile(.95))\n            ax.axvline(df_train[df_train['category'] == col][q_t].str.len().quantile(.05))\n            mean_val = df_train[df_train['category'] == col][q_t].str.len().mean()\n            ax.axvline(mean_val, color='red' )\n            #ax.text(x=mean_val*1.1, y=.02, s='Holiday in US', alpha=0.7, color='#334f8d')\n            ax.set_xlabel('')\n        count+=1\n        \nplt.subplots_adjust(top = 0.90, hspace=.4, wspace=.15)\nplt.show()", "processed": [""]}, {"markdown": ["Cool! <br>\nThe quantiles of the distributions are very similar in all categories in both title and question lenghts.\n\n", "# Exploring Target Features\n- Let's explore the target features to find some clear pattern that can be useful.\n- I will apply some techniques of dimensionality reduction and verify the explained variability and f", "## Distribution of all Target Features\n"], "code": "# Reference: https://www.kaggle.com/code/kabure/qa-eda-and-nlp-modelling-insights-vis-bert\n\ngrid = gridspec.GridSpec(10, 3)\n\nplt.figure(figsize=(16,8*4))\ncount=0\nplt.suptitle('Distribution of QA metrics (Target Features)', size=20)\n# top_host = df_train['host_cat'].value_counts()[:15].index\nfor n, col in enumerate(target_cols):\n    #if df_train[target_cols].std()[col] > .15:\n    ax = plt.subplot(grid[count])\n    sns.boxplot(x='category', y=col, data=df_train)\n    ax.set_title(str(col), fontsize=13)\n    ax.set_xlabel('')\n    ax.set_ylabel(' ')\n    count+=1\n    ax.set_xticklabels(ax.get_xticklabels(),rotation=45)\n\nplt.subplots_adjust(top = 0.95, hspace=.9, wspace=.2)\n\nplt.show()\n\n", "processed": ["cool br quantil distribut similar categori titl question lenght", "explor target featur let explor target featur find clear pattern use appli techniqu dimension reduct verifi explain variabl f", "distribut target featur"]}, {"markdown": ["It's not so much, but it's a important value that means that we have some different patterns. \n- Let's create some charts to explain", "# Ploting PCA of Target\n- ploting the first and second principal components"], "code": "# Reference: https://www.kaggle.com/code/kabure/qa-eda-and-nlp-modelling-insights-vis-bert\n\nplt.figure(figsize=(15,6))\ng = sns.scatterplot(x='Target_PCA0', y='Target_PCA1', data=df_train, hue='category')\ng.set_title(\"PCA Components Distribution by Categories\", fontsize=22)\ng.set_xlabel(\"TARGET PCA 0\", fontsize=16)\ng.set_ylabel(\"TARGET PCA 1\", fontsize=16)\n\nplt.show()", "processed": ["much import valu mean differ pattern let creat chart explain", "plote pca target plote first second princip compon"]}, {"markdown": ["We can note that some features are high correlated.<br>\nFor instance: Critical questions and questions interestingness self seems more well written. ", "# Questions and Answers Texts\nLet's start ploting some WordClouds by Categories. \n"], "code": "# Reference: https://www.kaggle.com/code/kabure/qa-eda-and-nlp-modelling-insights-vis-bert\n\nfrom wordcloud import WordCloud, STOPWORDS\n\nstopwords = set(STOPWORDS)\nnewStopWords = ['amp', 'gt', 'lt', 'div', 'id',\n                'fi', 'will', 'use', 'one', 'nbsp', 'need']\nstopwords.update(newStopWords)\ngrid = gridspec.GridSpec(5, 2)\n\nplt.figure(figsize=(16,7*4))\n\nplt.suptitle('Word Cloud OF CATEGORY FEATURE', size=20)\n\nfor n, col in enumerate(df_train['category'].value_counts().index):\n    ax = plt.subplot(grid[n])  \n    \n    wordcloud = WordCloud(\n        background_color='black',\n        stopwords=stopwords,\n        max_words=250,\n        max_font_size=100, \n        width=400, height=280,\n        random_state=42,\n    ).generate(\" \".join(df_train[df_train['category'] == col]['answer'].astype(str)))\n\n    #print(wordcloud)\n\n    plt.imshow(wordcloud)\n    plt.title(f\"Category: {col}\",fontsize=18)\n    plt.axis('off')\nplt.subplots_adjust(top = 0.95, hspace=.2, wspace=.1 )\n\nplt.show()", "processed": ["note featur high correl br instanc critic question question interesting self seem well written", "question answer text let start plote wordcloud categori"]}, {"markdown": ["Nice I can see some interesting words in the different categories, but only stackoverflow seems more clear. Let's explore it in further;", "# WordCloud by HOSTS\n- TOP 10 hosts with more unique questions"], "code": "# Reference: https://www.kaggle.com/code/kabure/qa-eda-and-nlp-modelling-insights-vis-bert\n\n#newStopWords = ['fruit', \"Drink\", \"black\"]\n\n#stopwords.update(newStopWords)\n\nimport matplotlib.gridspec as gridspec # to do the grid of plots\ngrid = gridspec.GridSpec(5, 2)\n\nplt.figure(figsize=(16,7*4))\n\nplt.suptitle('Answers Word Cloud \\nTOP 10 hosts with more questions', size=20)\n\nfor n, col in enumerate(df_train['host_cat'].value_counts()[:10].index):\n    ax = plt.subplot(grid[n])   \n    wordcloud = WordCloud(\n        background_color='black',\n        stopwords=stopwords,\n        max_words=250,\n        max_font_size=100, \n        width=400, height=280,\n        random_state=42,\n    ).generate(\" \".join(df_train[df_train['host_cat'] == col]['answer'].astype(str)))\n\n    #print(wordcloud)\n\n    plt.imshow(wordcloud)\n    plt.title(f\"Host: {col}\",fontsize=18)\n    plt.axis('off')\n    \nplt.subplots_adjust(top = 0.95, hspace=.2, wspace=.1 )\n\nplt.show()", "processed": ["nice see interest word differ categori stackoverflow seem clear let explor", "wordcloud host top 10 host uniqu question"]}, {"markdown": ["Cool! Now that we got some metrics about the posible sentiment of the answers, let get some quantitative analysis of it.", "## Ploting the polarity x the subjetictivy"], "code": "# Reference: https://www.kaggle.com/code/kabure/qa-eda-and-nlp-modelling-insights-vis-bert\n\nplt.figure(figsize=(16,5))\n\ng = sns.scatterplot(x='ans_polarity', y='ans_subjectivity', \n                    data=df_train, hue='category')\ng.set_title(\"Sentiment Analyzis (Polarity x Subjectivity) by 'Category' Feature\", fontsize=21)\ng.set_xlabel(\"Polarity distribution\",fontsize=18)\ng.set_ylabel(\"Subjective \",fontsize=18)\n\nplt.show()", "processed": ["cool got metric posibl sentiment answer let get quantit analysi", "plote polar x subjetictivi"]}, {"markdown": ["## Random Example of Questions + Answers"], "code": "# Reference: https://www.kaggle.com/code/kabure/qa-eda-and-nlp-modelling-insights-vis-bert\n\n# Here, the order is important\ndf_train['answer'] = df_train['answer'].apply(replace_contractions)\ndf_train['answer'] = df_train['answer'].apply(clean_text)\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n\ngrid = gridspec.GridSpec(5, 3)\nplt.figure(figsize=(16,6*4))\n\nfor n, cat in enumerate(top_host[:9]):\n    \n    ax = plt.subplot(grid[n])   \n    # print(f'PRINCIPAL WORDS CATEGORY: {cat}')\n    # vectorizer = CountVectorizer(ngram_range = (3,3)) \n    # X1 = vectorizer.fit_transform(df_train[df_train['host_cat'] == cat]['answer'])  \n \n    min_df_val = round(len(df_train[df_train['host_cat'] == cat]) - len(df_train[df_train['host_cat'] == cat]) * .99)\n    max_df_val = round(len(df_train[df_train['host_cat'] == cat]) - len(df_train[df_train['host_cat'] == cat]) * .3)\n    \n    # Applying TFIDF \n    vectorizer = TfidfVectorizer(ngram_range = (2,2), min_df=5, stop_words='english',\n                                 max_df=.5) \n    X2 = vectorizer.fit_transform(df_train[df_train['host_cat'] == cat]['answer']) \n    features = (vectorizer.get_feature_names()) \n    scores = (X2.toarray()) \n\n    # Getting top ranking features \n    sums = X2.sum(axis = 0) \n    data1 = [] \n    \n    for col, term in enumerate(features): \n        data1.append( (term, sums[0,col] )) \n\n    ranking = pd.DataFrame(data1, columns = ['term','rank']) \n    words = (ranking.sort_values('rank', ascending = False))[:10]\n    \n    sns.barplot(x='term', y='rank', data=words, ax=ax, \n                color='blue', orient='v')\n    ax.set_title(f\"Top rank Trigram of: {cat}\")\n    ax.set_xticklabels(ax.get_xticklabels(),rotation=90)\n    ax.set_ylabel(' ')\n    ax.set_xlabel(\" \")\n\nplt.subplots_adjust(top = 0.95, hspace=.9, wspace=.1)\n\nplt.show()", "processed": ["random exampl question answer"]}, {"markdown": ["# Training Set, First Car Stats\n\n- Model type (You are not required to predict the model type of the vehicle in question.)"], "code": "# Reference: https://www.kaggle.com/code/robikscube/autonomous-driving-introduction-data-review\n\ntrain_expanded.groupby('1_model_type')['ImageId'] \\\n    .count() \\\n    .sort_values() \\\n    .plot(kind='barh',\n          figsize=(15, 8),\n          title='First Car, Count by Model Type',\n          color=my_pal[0])\nplt.show()\ntrain_expanded['1_yaw'] = pd.to_numeric(train_expanded['1_yaw'])\ntrain_expanded['1_yaw'] \\\n    .dropna() \\\n    .plot(kind='hist',\n          figsize=(15, 3),\n          bins=100,\n          title='Distribution of First car YAW',\n          color=my_pal[1])\nplt.show()\ntrain_expanded['1_pitch'] = pd.to_numeric(train_expanded['1_pitch'])\ntrain_expanded['1_pitch'] \\\n    .dropna() \\\n    .plot(kind='hist',\n          figsize=(15, 3),\n          bins=100,\n          title='Distribution of First car pitch',\n          color=my_pal[2])\nplt.show()\ntrain_expanded['1_roll'] = pd.to_numeric(train_expanded['1_roll'])\ntrain_expanded['1_roll'] \\\n    .dropna() \\\n    .plot(kind='hist',\n          figsize=(15, 3),\n          bins=100,\n          title='Distribution of First car roll',\n          color=my_pal[3])\nplt.show()", "processed": ["train set first car stat model type requir predict model type vehicl question"]}, {"markdown": ["## X, Y, and Z features"], "code": "# Reference: https://www.kaggle.com/code/robikscube/autonomous-driving-introduction-data-review\n\ntrain_expanded['1_x'] = pd.to_numeric(train_expanded['1_x'])\ntrain_expanded['1_y'] = pd.to_numeric(train_expanded['1_y'])\ntrain_expanded['1_z'] = pd.to_numeric(train_expanded['1_z'])\ntrain_expanded['1_x'] \\\n    .dropna() \\\n    .plot(kind='hist',\n          figsize=(15, 3),\n          bins=100,\n          title='Distribution of x',\n          color=my_pal[0])\nplt.show()\ntrain_expanded['1_y'] \\\n    .dropna() \\\n    .plot(kind='hist',\n          figsize=(15, 3),\n          bins=100,\n          title='Distribution of y',\n          color=my_pal[1])\nplt.show()\ntrain_expanded['1_z'] \\\n    .dropna() \\\n    .plot(kind='hist',\n          figsize=(15, 3),\n          bins=100,\n          title='Distribution of z',\n          color=my_pal[2])\nplt.show()", "processed": ["x z featur"]}, {"markdown": ["## Masks Next to Images"], "code": "# Reference: https://www.kaggle.com/code/robikscube/autonomous-driving-introduction-data-review\n\nids = train['ImageId'].values\nfig, axes = plt.subplots(4, 3, figsize=(18, 20))\nfor i in range(4):\n    img = load_img('../input/pku-autonomous-driving/train_images/' + ids[i] + '.jpg')\n    img_mask = load_img('../input/pku-autonomous-driving/train_masks/' + ids[i] + '.jpg')\n    #plt.subplot(1,2*(1+len(ids)),q*2-1)\n    ax=axes[i][0].imshow(img)\n    #plt.subplot(1,2*(1+len(ids)),q*2)\n    ax=axes[i][1].imshow(img_mask)\n    ax=axes[i][2].imshow(img)\n    ax=axes[i][2].imshow(img_mask, cmap=plt.cm.viridis, interpolation='none', alpha=0.4)\nplt.show()", "processed": ["mask next imag"]}, {"markdown": ["As we can see, distribution of values is almost normal.", "### Slope and aspect"], "code": "# Reference: https://www.kaggle.com/code/artgor/forest-exploration-and-trees\n\ntrain.groupby('Cover_Type').Aspect.mean()\nfor col in ['Aspect', 'Slope']:\n    sns.violinplot(data=train, x='Cover_Type', y=col)  \n    plt.show()", "processed": ["see distribut valu almost normal", "slope aspect"]}, {"markdown": ["Two columns have only zero values in train, so I drop them."], "code": "# Reference: https://www.kaggle.com/code/artgor/forest-exploration-and-trees\n\ntrain.drop(['Soil_Type7', 'Soil_Type15'], axis=1, inplace=True)\ntest.drop(['Soil_Type7', 'Soil_Type15'], axis=1, inplace=True)\nsns.pairplot(train[['Elevation', 'Aspect', 'Slope',\n       'Horizontal_Distance_To_Hydrology', 'Vertical_Distance_To_Hydrology',\n       'Horizontal_Distance_To_Roadways', 'Hillshade_9am', 'Hillshade_Noon',\n       'Hillshade_3pm', 'Horizontal_Distance_To_Fire_Points',\n       'Wilderness_Area1', 'Wilderness_Area2', 'Wilderness_Area3',\n       'Wilderness_Area4', 'Cover_Type']], hue='Cover_Type')\n# great features from this kernel: https://www.kaggle.com/rohandx1996/pca-fe-data-viz-top-10\n####################### Train data #############################################\ntrain['HF1'] = train['Horizontal_Distance_To_Hydrology']+train['Horizontal_Distance_To_Fire_Points']\ntrain['HF2'] = abs(train['Horizontal_Distance_To_Hydrology']-train['Horizontal_Distance_To_Fire_Points'])\ntrain['HR1'] = abs(train['Horizontal_Distance_To_Hydrology']+train['Horizontal_Distance_To_Roadways'])\ntrain['HR2'] = abs(train['Horizontal_Distance_To_Hydrology']-train['Horizontal_Distance_To_Roadways'])\ntrain['FR1'] = abs(train['Horizontal_Distance_To_Fire_Points']+train['Horizontal_Distance_To_Roadways'])\ntrain['FR2'] = abs(train['Horizontal_Distance_To_Fire_Points']-train['Horizontal_Distance_To_Roadways'])\ntrain['ele_vert'] = train.Elevation-train.Vertical_Distance_To_Hydrology\n\ntrain['slope_hyd'] = (train['Horizontal_Distance_To_Hydrology']**2+train['Vertical_Distance_To_Hydrology']**2)**0.5\ntrain.slope_hyd=train.slope_hyd.map(lambda x: 0 if np.isinf(x) else x) # remove infinite value if any\n\n#Mean distance to Amenities \ntrain['Mean_Amenities']=(train.Horizontal_Distance_To_Fire_Points + train.Horizontal_Distance_To_Hydrology + train.Horizontal_Distance_To_Roadways) / 3 \n#Mean Distance to Fire and Water \ntrain['Mean_Fire_Hyd']=(train.Horizontal_Distance_To_Fire_Points + train.Horizontal_Distance_To_Hydrology) / 2 \n####################### Test data #############################################\ntest['HF1'] = test['Horizontal_Distance_To_Hydrology']+test['Horizontal_Distance_To_Fire_Points']\ntest['HF2'] = abs(test['Horizontal_Distance_To_Hydrology']-test['Horizontal_Distance_To_Fire_Points'])\ntest['HR1'] = abs(test['Horizontal_Distance_To_Hydrology']+test['Horizontal_Distance_To_Roadways'])\ntest['HR2'] = abs(test['Horizontal_Distance_To_Hydrology']-test['Horizontal_Distance_To_Roadways'])\ntest['FR1'] = abs(test['Horizontal_Distance_To_Fire_Points']+test['Horizontal_Distance_To_Roadways'])\ntest['FR2'] = abs(test['Horizontal_Distance_To_Fire_Points']-test['Horizontal_Distance_To_Roadways'])\ntest['ele_vert'] = test.Elevation-test.Vertical_Distance_To_Hydrology\n\ntest['slope_hyd'] = (test['Horizontal_Distance_To_Hydrology']**2+test['Vertical_Distance_To_Hydrology']**2)**0.5\ntest.slope_hyd=test.slope_hyd.map(lambda x: 0 if np.isinf(x) else x) # remove infinite value if any\n\n#Mean distance to Amenities \ntest['Mean_Amenities']=(test.Horizontal_Distance_To_Fire_Points + test.Horizontal_Distance_To_Hydrology + test.Horizontal_Distance_To_Roadways) / 3 \n#Mean Distance to Fire and Water \ntest['Mean_Fire_Hyd']=(test.Horizontal_Distance_To_Fire_Points + test.Horizontal_Distance_To_Hydrology) / 2", "processed": ["two column zero valu train drop"]}, {"markdown": ["As we can see on average questions in train and test datasets are similar, but there are quite long questions in train dataset."], "code": "# Reference: https://www.kaggle.com/code/artgor/text-modelling-in-pytorch-v2\n\npuncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '\u2022',  '~', '@', '\u00a3', \n '\u00b7', '_', '{', '}', '\u00a9', '^', '\u00ae', '`',  '<', '\u2192', '\u00b0', '\u20ac', '\u2122', '\u203a',  '\u2665', '\u2190', '\u00d7', '\u00a7', '\u2033', '\u2032', '\u00c2', '\u2588', '\u00bd', '\u00e0', '\u2026', \n '\u201c', '\u2605', '\u201d', '\u2013', '\u25cf', '\u00e2', '\u25ba', '\u2212', '\u00a2', '\u00b2', '\u00ac', '\u2591', '\u00b6', '\u2191', '\u00b1', '\u00bf', '\u25be', '\u2550', '\u00a6', '\u2551', '\u2015', '\u00a5', '\u2593', '\u2014', '\u2039', '\u2500', \n '\u2592', '\uff1a', '\u00bc', '\u2295', '\u25bc', '\u25aa', '\u2020', '\u25a0', '\u2019', '\u2580', '\u00a8', '\u2584', '\u266b', '\u2606', '\u00e9', '\u00af', '\u2666', '\u00a4', '\u25b2', '\u00e8', '\u00b8', '\u00be', '\u00c3', '\u22c5', '\u2018', '\u221e', \n '\u2219', '\uff09', '\u2193', '\u3001', '\u2502', '\uff08', '\u00bb', '\uff0c', '\u266a', '\u2569', '\u255a', '\u00b3', '\u30fb', '\u2566', '\u2563', '\u2554', '\u2557', '\u25ac', '\u2764', '\u00ef', '\u00d8', '\u00b9', '\u2264', '\u2021', '\u221a', ]\n\ndef clean_text(x):\n    x = str(x)\n    for punct in puncts:\n        x = x.replace(punct, f' {punct} ')\n    return x\n\ndef clean_numbers(x):\n    if bool(re.search(r'\\d', x)):\n        x = re.sub('[0-9]{5,}', '#####', x)\n        x = re.sub('[0-9]{4}', '####', x)\n        x = re.sub('[0-9]{3}', '###', x)\n        x = re.sub('[0-9]{2}', '##', x)\n    return x\n\nmispell_dict = {\"aren't\" : \"are not\",\n                \"can't\" : \"cannot\",\n                \"couldn't\" : \"could not\",\n                \"didn't\" : \"did not\",\n                \"doesn't\" : \"does not\",\n                \"don't\" : \"do not\",\n                \"hadn't\" : \"had not\",\n                \"hasn't\" : \"has not\",\n                \"haven't\" : \"have not\",\n                \"he'd\" : \"he would\",\n                \"he'll\" : \"he will\",\n                \"he's\" : \"he is\",\n                \"i'd\" : \"I would\",\n                \"i'd\" : \"I had\",\n                \"i'll\" : \"I will\",\n                \"i'm\" : \"I am\",\n                \"isn't\" : \"is not\",\n                \"it's\" : \"it is\",\n                \"it'll\":\"it will\",\n                \"i've\" : \"I have\",\n                \"let's\" : \"let us\",\n                \"mightn't\" : \"might not\",\n                \"mustn't\" : \"must not\",\n                \"shan't\" : \"shall not\",\n                \"she'd\" : \"she would\",\n                \"she'll\" : \"she will\",\n                \"she's\" : \"she is\",\n                \"shouldn't\" : \"should not\",\n                \"that's\" : \"that is\",\n                \"there's\" : \"there is\",\n                \"they'd\" : \"they would\",\n                \"they'll\" : \"they will\",\n                \"they're\" : \"they are\",\n                \"they've\" : \"they have\",\n                \"we'd\" : \"we would\",\n                \"we're\" : \"we are\",\n                \"weren't\" : \"were not\",\n                \"we've\" : \"we have\",\n                \"what'll\" : \"what will\",\n                \"what're\" : \"what are\",\n                \"what's\" : \"what is\",\n                \"what've\" : \"what have\",\n                \"where's\" : \"where is\",\n                \"who'd\" : \"who would\",\n                \"who'll\" : \"who will\",\n                \"who're\" : \"who are\",\n                \"who's\" : \"who is\",\n                \"who've\" : \"who have\",\n                \"won't\" : \"will not\",\n                \"wouldn't\" : \"would not\",\n                \"you'd\" : \"you would\",\n                \"you'll\" : \"you will\",\n                \"you're\" : \"you are\",\n                \"you've\" : \"you have\",\n                \"'re\": \" are\",\n                \"wasn't\": \"was not\",\n                \"we'll\":\" will\",\n                \"didn't\": \"did not\",\n                \"tryin'\":\"trying\",\n               '\\u200b': '',\n                '\u2026': '',\n                '\\ufeff': '',\n                '\u0915\u0930\u0928\u093e': '',\n                '\u0939\u0948': ''}\n\nfor coin in ['Litecoin', 'altcoin', 'altcoins', 'coinbase', 'litecoin', 'Unocoin', 'Dogecoin', 'cryptocoin', 'Altcoins', 'filecoin', 'Altcoin', 'cryptocoins',\n             'Altacoin', 'Dentacoin', 'Bytecoin', 'Siacoin', 'Onecoin', 'dogecoin', 'unocoin', 'siacoin', 'litecoins', 'Filecoin', 'Buyucoin', 'Litecoins',\n             'Laxmicoin', 'shtcoins', 'Sweatcoin', 'Skycoin', 'vitrocoin', 'Monacoin', 'Litcoin', 'reddcoin', 'freebitcoin', 'Namecoin', 'plexcoin', 'Onecoins',\n             'daikicoin', 'Gainbitcoin', 'Gatecoin', 'Plexcoin', 'peercoin', 'coinsecure', 'dogecoins', 'cointries', 'Zcoin', 'genxcoin', 'Frazcoin', 'frazcoin',\n             'coinify', 'Nagricoin', 'OKcoin', 'Presscoins', 'Dagcoin', 'batcoin', 'Spectrocoin', 'Travelflexcoin', 'ecoin', 'Minexcoin', 'Kashhcoin', 'coinone',\n             'octacoin', 'coinsides', 'zabercoin', 'ADZcoin', 'cyptocoin', 'bitecoin', 'Bitecoin', 'Emercoin', 'tegcoin', 'flipcoin', 'Gridcoin', 'Facecoin',\n             'Ravencoins', 'digicoin', 'bitcoincash', 'Vitrocoin', 'Livecoin', 'dashcoin', 'Fedcoin', 'litcoins', 'Webcoin', 'coinspot', 'bitoxycoin', 'peercoins',\n             'Ucoin', 'ALTcoins', 'coincidece', 'dagcoin', 'Giracoin', 'coincheck', 'Swisscoin', 'butcoin', 'neocoin', 'mintcoin', 'Myriadcoin', 'Viacoin', 'jiocoin',\n             'Potcoin', 'bibitcoin', 'gainbitcoin', 'altercoins', 'coinburn', 'Kodakcoin', 'Bcoin', 'Kucoin', 'Operacoin', 'Lomocoin', 'dentacoin', 'Nyancoin',\n             'Jiocoin', 'Indicoin', 'coinsidered', 'Vertcoin', 'Maidsafecoin', 'coindelta', 'coinfirm', 'coinvest', 'bixcoin', 'litcoin', 'Dogecoins', 'Unicoin',\n             'Rothscoin', 'localbitcoins', 'groestlcoin', 'sibcoin', 'Travelercoin', 'Vericoin', 'bytecoin', 'Bananacoin', 'PACcoin']:\n    mispell_dict[coin] = 'bitcoin'\n\ndef _get_mispell(mispell_dict):\n    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n    return mispell_dict, mispell_re\n\nmispellings, mispellings_re = _get_mispell(mispell_dict)\ndef replace_typical_misspell(text):\n    def replace(match):\n        return mispellings[match.group(0)]\n    return mispellings_re.sub(replace, text)\n\n# Clean the text\ntrain[\"question_text\"] = train[\"question_text\"].apply(lambda x: clean_text(x.lower()))\ntest[\"question_text\"] = test[\"question_text\"].apply(lambda x: clean_text(x.lower()))\n\n# Clean numbers\ntrain[\"question_text\"] = train[\"question_text\"].apply(lambda x: clean_numbers(x))\ntest[\"question_text\"] = test[\"question_text\"].apply(lambda x: clean_numbers(x))\n\n# Clean speelings\ntrain[\"question_text\"] = train[\"question_text\"].apply(lambda x: replace_typical_misspell(x))\ntest[\"question_text\"] = test[\"question_text\"].apply(lambda x: replace_typical_misspell(x))\nmax_features = 120000\ntk = Tokenizer(lower = True, filters='', num_words=max_features)\nfull_text = list(train['question_text'].values) + list(test['question_text'].values)\ntk.fit_on_texts(full_text)\ntrain_tokenized = tk.texts_to_sequences(train['question_text'].fillna('missing'))\ntest_tokenized = tk.texts_to_sequences(test['question_text'].fillna('missing'))\ntrain['question_text'].apply(lambda x: len(x.split())).plot(kind='hist');\nplt.yscale('log');\nplt.title('Distribution of question text length in characters');", "processed": ["see averag question train test dataset similar quit long question train dataset"]}, {"markdown": ["## Data exploration"], "code": "# Reference: https://www.kaggle.com/code/artgor/is-this-malware-eda-fe-and-lgb-updated\n\ntrain.head()\n# function to plot data\ndef plot_categorical_feature(col, only_bars=False, top_n=10, by_touch=False):\n    top_n = top_n if train[col].nunique() > top_n else train[col].nunique()\n    print(f\"{col} has {train[col].nunique()} unique values and type: {train[col].dtype}.\")\n    print(train[col].value_counts(normalize=True, dropna=False).head())\n    if not by_touch:\n        if not only_bars:\n            df = train.groupby([col]).agg({'HasDetections': ['count', 'mean']})\n            df = df.sort_values(('HasDetections', 'count'), ascending=False).head(top_n).sort_index()\n            data = [go.Bar(x=df.index, y=df['HasDetections']['count'].values, name='counts'),\n                    go.Scatter(x=df.index, y=df['HasDetections']['mean'], name='Detections rate', yaxis='y2')]\n\n            layout = go.Layout(dict(title = f\"Counts of {col} by top-{top_n} categories and mean target value\",\n                                xaxis = dict(title = f'{col}',\n                                             showgrid=False,\n                                             zeroline=False,\n                                             showline=False,),\n                                yaxis = dict(title = 'Counts',\n                                             showgrid=False,\n                                             zeroline=False,\n                                             showline=False,),\n                                yaxis2=dict(title='Detections rate', overlaying='y', side='right')),\n                           legend=dict(orientation=\"v\"))\n\n        else:\n            top_cat = list(train[col].value_counts(dropna=False).index[:top_n])\n            df0 = train.loc[(train[col].isin(top_cat)) & (train['HasDetections'] == 1), col].value_counts().head(10).sort_index()\n            df1 = train.loc[(train[col].isin(top_cat)) & (train['HasDetections'] == 0), col].value_counts().head(10).sort_index()\n            data = [go.Bar(x=df0.index, y=df0.values, name='Has Detections'),\n                    go.Bar(x=df1.index, y=df1.values, name='No Detections')]\n\n            layout = go.Layout(dict(title = f\"Counts of {col} by top-{top_n} categories\",\n                                xaxis = dict(title = f'{col}',\n                                             showgrid=False,\n                                             zeroline=False,\n                                             showline=False,),\n                                yaxis = dict(title = 'Counts',\n                                             showgrid=False,\n                                             zeroline=False,\n                                             showline=False,),\n                                ),\n                           legend=dict(orientation=\"v\"), barmode='group')\n        \n        py.iplot(dict(data=data, layout=layout))\n        \n    else:\n        top_n = 10\n        top_cat = list(train[col].value_counts(dropna=False).index[:top_n])\n        df = train.loc[train[col].isin(top_cat)]\n\n        df1 = train.loc[train['Census_IsTouchEnabled'] == 1]\n        df0 = train.loc[train['Census_IsTouchEnabled'] == 0]\n\n        df0_ = df0.groupby([col]).agg({'HasDetections': ['count', 'mean']})\n        df0_ = df0_.sort_values(('HasDetections', 'count'), ascending=False).head(top_n).sort_index()\n        df1_ = df1.groupby([col]).agg({'HasDetections': ['count', 'mean']})\n        df1_ = df1_.sort_values(('HasDetections', 'count'), ascending=False).head(top_n).sort_index()\n        data1 = [go.Bar(x=df0_.index, y=df0_['HasDetections']['count'].values, name='Nontouch device counts'),\n                go.Scatter(x=df0_.index, y=df0_['HasDetections']['mean'], name='Detections rate for nontouch devices', yaxis='y2')]\n        data2 = [go.Bar(x=df1_.index, y=df1_['HasDetections']['count'].values, name='Touch device counts'),\n                go.Scatter(x=df1_.index, y=df1_['HasDetections']['mean'], name='Detections rate for touch devices', yaxis='y2')]\n\n        layout = go.Layout(dict(title = f\"Counts of {col} by top-{top_n} categories for nontouch devices\",\n                            xaxis = dict(title = f'{col}',\n                                         showgrid=False,\n                                         zeroline=False,\n                                         showline=False,\n                                         type='category'),\n                            yaxis = dict(title = 'Counts',\n                                         showgrid=False,\n                                         zeroline=False,\n                                         showline=False,),\n                                    yaxis2=dict(title='Detections rate', overlaying='y', side='right'),\n                            ),\n                       legend=dict(orientation=\"v\"), barmode='group')\n\n        py.iplot(dict(data=data1, layout=layout))\n        layout['title'] = f\"Counts of {col} by top-{top_n} categories for touch devices\"\n        py.iplot(dict(data=data2, layout=layout))", "processed": ["data explor"]}, {"markdown": ["As per description: If the value exists but is blank, the value \"ExistsNotSet\" is sent in telemetry.\nSo missing values and ExistsNotSet are in fact the same. This category + RequireAdmin + Off + Warn are 99.3% of all values. I'll combine all other values into Prompt."], "code": "# Reference: https://www.kaggle.com/code/artgor/is-this-malware-eda-fe-and-lgb-updated\n\n# train.loc[train['SmartScreen'].isnull(), 'SmartScreen'] = 'ExistsNotSet'\n# test.loc[test['SmartScreen'].isnull(), 'SmartScreen'] = 'ExistsNotSet'\n# train.loc[train['SmartScreen'].isin(['RequireAdmin', 'ExistsNotSet', 'Off', 'Warn']) == False, 'SmartScreen'] = 'Prompt'\n# test.loc[test['SmartScreen'].isin(['RequireAdmin', 'ExistsNotSet', 'Off', 'Warn']) == False, 'SmartScreen'] = 'Prompt'\n\n# train['SmartScreen'] = train['SmartScreen'].cat.remove_unused_categories()\n# test['SmartScreen'] = test['SmartScreen'].cat.remove_unused_categories()\nsns.set(rc={'figure.figsize':(15, 8)})\nsns.countplot(x=\"SmartScreen\", hue=\"HasDetections\",  palette=\"PRGn\", data=train)\nplt.title(\"SmartScreen counts\")\nplt.xticks(rotation='vertical')\nplt.show()", "processed": ["per descript valu exist blank valu existsnotset sent telemetri miss valu existsnotset fact categori requireadmin warn 99 3 valu combin valu prompt"]}, {"markdown": ["### Census_MDC2FormFactor"], "code": "# Reference: https://www.kaggle.com/code/artgor/is-this-malware-eda-fe-and-lgb-updated\n\ntrain['Census_MDC2FormFactor'].value_counts(dropna=False, normalize=True).cumsum()\n# top_cats = list(train['Census_MDC2FormFactor'].value_counts().index[:5])\n# train.loc[train['Census_MDC2FormFactor'].isin(top_cats) == False, 'Census_MDC2FormFactor'] = 'PCOther'\n# test.loc[test['Census_MDC2FormFactor'].isin(top_cats) == False, 'Census_MDC2FormFactor'] = 'PCOther'\n\n# train['Census_MDC2FormFactor'] = train['Census_MDC2FormFactor'].cat.remove_unused_categories()\n# test['Census_MDC2FormFactor'] = test['Census_MDC2FormFactor'].cat.remove_unused_categories()\nplot_categorical_feature('Census_MDC2FormFactor', True)\nsns.catplot(x=\"Census_PrimaryDiskTypeName\", hue=\"HasDetections\", col=\"Census_MDC2FormFactor\",\n                data=train, kind=\"count\",col_wrap=3);", "processed": ["censu mdc2formfactor"]}, {"markdown": ["We are ready to start! :-D", "# Why do proteins come together?"], "code": "# Reference: https://www.kaggle.com/code/allunia/uncover-target-correlations-with-bernoulli-mixture\n\ntarget_counts = train_labels.drop([\"Id\", \"Target\"],axis=1).sum(axis=0).sort_values(ascending=False)\nplt.figure(figsize=(15,15))\nsns.barplot(y=target_counts.index.values, x=target_counts.values, order=target_counts.index);", "processed": ["readi start", "protein come togeth"]}, {"markdown": ["This is already known! We have some very seldom targets like rods & rings and very common ones like nucleoplasmn, cytosol and plasma membrane. But wouldn't it be nice to know if some proteins are likely to come together? We have already seen that lysosomes, endosomes and endoplasmatic reticulum have target correlations:\n\n"], "code": "# Reference: https://www.kaggle.com/code/allunia/uncover-target-correlations-with-bernoulli-mixture\n\ntrain_labels[\"number_of_targets\"] = train_labels.drop([\"Id\", \"Target\"],axis=1).sum(axis=1)\n\ndef find_counts(special_target, labels):\n    counts = labels[labels[special_target] == 1].drop(\n        [\"Id\", \"Target\", \"number_of_targets\"],axis=1\n    ).sum(axis=0)\n    counts = counts[counts > 0]\n    counts = counts.sort_values()\n    return counts\n\nlyso_endo_counts = find_counts(\"Lysosomes\", train_labels)\n\nplt.figure(figsize=(15,5))\nsns.barplot(x=lyso_endo_counts.index.values, y=lyso_endo_counts.values, palette=\"Blues\");", "processed": ["alreadi known seldom target like rod ring common one like nucleoplasmn cytosol plasma membran nice know protein like come togeth alreadi seen lysosom endosom endoplasmat reticulum target correl"]}, {"markdown": ["**You can see that lysosomes and endosomes always come together and that it is likely that the endoplasmatic reticulum is present as well**. I have read about lysosomes that their membrane is produced by ribosomes that are located in the rough endoplasmatic reticulum and transferred to the golgi apparatus afterwards. Later they are shipped to endosomes, small bubbles full of stuff, and seem to fuse with them to digest all the stuff inside. What if staining is done with molecules that are found in all three participants: the rough ER, endosomes and lysosomes? Then we will always see that they often come together. \n\nBut isn't it nice to know? If our model is sure that lysosomes are present, we can automatically say endosomes is hot as well. Great! Hence instead of predicting both target classes we can reduce to both to one single lyso-endo class."], "code": "# Reference: https://www.kaggle.com/code/allunia/uncover-target-correlations-with-bernoulli-mixture\n\ncount_perc = np.round(100 * train_labels[\"number_of_targets\"].value_counts() / train_labels.shape[0], 2)\nplt.figure(figsize=(20,5))\nsns.barplot(x=count_perc.index.values, y=count_perc.values, palette=\"Reds\")\nplt.xlabel(\"Number of targets per image\")\nplt.ylabel(\"% of data\");", "processed": ["see lysosom endosom alway come togeth like endoplasmat reticulum present well read lysosom membran produc ribosom locat rough endoplasmat reticulum transfer golgi apparatu afterward later ship endosom small bubbl full stuff seem fuse digest stuff insid stain done molecul found three particip rough er endosom lysosom alway see often come togeth nice know model sure lysosom present automat say endosom hot well great henc instead predict target class reduc one singl lyso endo class"]}, {"markdown": ["Both maps already yielded some insights which targets are likely to come together and how seldom a target is given a cluster. We can go one step further. The model was trained with $\\mu$ and $\\pi$. The latter shows us the probability per cluster: ", "### How does the prior probability $\\pi_{k}$ per cluster look like?"], "code": "# Reference: https://www.kaggle.com/code/allunia/uncover-target-correlations-with-bernoulli-mixture\n\ncluster_ids = np.arange(0, estimated_components)\nnames = [cluster_names[l] for l in cluster_ids]\n\npi = pd.Series(data=model.pi, index=names).sort_values(ascending=False)\nplt.figure(figsize=(20,5))\nsns.barplot(x=pi.index, y=pi.values, palette=\"Reds_r\", order=pi.index)\nplt.xticks(rotation=90);", "processed": ["map alreadi yield insight target like come togeth seldom target given cluster go one step model train mu pi latter show u probabl per cluster", "prior probabl pi k per cluster look like"]}, {"markdown": ["### Take-Away\n\n* We can see that the probability of each target to be present yields a lot of insights. Let's consider the golgi apparatus for example. If we choose the corresponding cluster named the same we can observe a probability to occur of 100%. Hence if we now that a sample is located within that cluster, we can say it has a golgi apparatus!\n* There a lot of target proteins that have a very high probability to occur in their cluster!\n* But there are some strange probabilities as well. Think of cluster 1 again: It's nearly fully occupied by aggresomes but the mu-proability for this target given this cluster is very low with 18 %. That's strange! It's related to the seldomness of the target and could be caused by a shift due to weighted sum during mu calculation. As the sum is taken over all samples, nearby targets (of other components) that still have some higher responsibility for the aggresome cluster can cause a shift of the mu-center of the cluster. ", "## How many samples do the clusters hold?"], "code": "# Reference: https://www.kaggle.com/code/allunia/uncover-target-correlations-with-bernoulli-mixture\n\ncluster_counts = results.groupby(\"cluster\").cluster.count()\ncluster_counts = cluster_counts.sort_values()\nnames = [cluster_names[num] for num in cluster_counts.index]\n\nplt.figure(figsize=(20,5))\nsns.barplot(x=names, y=cluster_counts.values, order=names)\nplt.xticks(rotation=90);", "processed": ["take away see probabl target present yield lot insight let consid golgi apparatu exampl choos correspond cluster name observ probabl occur 100 henc sampl locat within cluster say golgi apparatu lot target protein high probabl occur cluster strang probabl well think cluster 1 nearli fulli occupi aggresom mu proabil target given cluster low 18 strang relat seldom target could caus shift due weight sum mu calcul sum taken sampl nearbi target compon still higher respons aggresom cluster caus shift mu center cluster", "mani sampl cluster hold"]}, {"markdown": ["To define which samples are anomal, I will setup a threshold of 5 %. If we compare this threshold with the distribution of the sample log-likelihoods, you can see..."], "code": "# Reference: https://www.kaggle.com/code/allunia/uncover-target-correlations-with-bernoulli-mixture\n\nplt.figure(figsize=(20,5))\nsns.distplot(sample_logLs)\nplt.axvline(my_threshold, color=\"Red\")\nplt.xlabel(\"Sample log likelihood of bernoulli mixture\")\nplt.title(\"Choosing a threshold to detect anomalies\")\nplt.ylabel(\"Density\")", "processed": ["defin sampl anom setup threshold 5 compar threshold distribut sampl log likelihood see"]}, {"markdown": ["### Take-Away\n\n* I expected the low-dense clusters to be more anomalistic than the others but this seems to be only true for low-dense 3 and a bit for low dense 1.\n* Interestingly some other clusters have anomalies as well. This is especially the case for clusters that tend to have only one target protein but have some with 2 target proteins as well. How anomal a cluster may depend on the total number of targets of the samples in that cluster.\n* In contrast the various Rods&Rings cluster that have uncertain cluster assignments (look at the chapter below) is not located in a low dense region. Hence there are other clusters and targets around that could suite as well. ", "## How certain are the cluster assignments?"], "code": "# Reference: https://www.kaggle.com/code/allunia/uncover-target-correlations-with-bernoulli-mixture\n\nresults[\"certainty\"] = np.sort(model.gamma, axis=1)[:,-1]\ncertainties = results.certainty.values\n\nplt.figure(figsize=(20,5))\nsns.distplot(certainties, color=\"Orange\")\nplt.xlabel(\"Certainty of cluster assignment\")\nplt.ylabel(\"Density\")\nplt.title(\"How sure was the model in predicting the winner?\");", "processed": ["take away expect low den cluster anomalist other seem true low den 3 bit low den 1 interestingli cluster anomali well especi case cluster tend one target protein 2 target protein well anom cluster may depend total number target sampl cluster contrast variou rod ring cluster uncertain cluster assign look chapter locat low den region henc cluster target around could suit well", "certain cluster assign"]}, {"markdown": ["### Take-Away\n\n* The certainties vary on a broad range. This suggests that we might have further cluster interactions or outlier target samples.\n* Let's have a look at the certainty distribution and statistics per cluster:"], "code": "# Reference: https://www.kaggle.com/code/allunia/uncover-target-correlations-with-bernoulli-mixture\n\nplt.figure(figsize=(20,5))\nsns.boxplot(x=\"cluster_names\", y=\"certainty\", data=results)\nplt.ylim([0,1])\nplt.xticks(rotation=90)\nplt.xlabel(\"\");", "processed": ["take away certainti vari broad rang suggest might cluster interact outlier target sampl let look certainti distribut statist per cluster"]}, {"markdown": ["**73 groups**\n**Each group_id is a unique recording session and has only one surface type **"], "code": "# Reference: https://www.kaggle.com/code/jesucristo/1-smart-robots-most-complete-notebook\n\nsns.set(style='darkgrid')\nsns.countplot(y = 'surface',\n              data = target,\n              order = target['surface'].value_counts().index)\nplt.show()", "processed": ["73 group group id uniqu record session one surfac type"]}, {"markdown": ["We need to classify on which surface our robot is standing.\n\nMulti-class Multi-output\n\n9 classes (suface)"], "code": "# Reference: https://www.kaggle.com/code/jesucristo/1-smart-robots-most-complete-notebook\n\nplt.figure(figsize=(23,5)) \nsns.set(style=\"darkgrid\")\ncountplot(x=\"group_id\", data=target, order = target['group_id'].value_counts().index)\nplt.show()", "processed": ["need classifi surfac robot stand multi class multi output 9 class sufac"]}, {"markdown": ["**So, we have 3810 train series, and 3816 test series.\nLet's engineer some features!**\n\n## Example: Series 1\n\nLet's have a look at the values of features in a single time-series, for example series 1  ```series_id=0```\n\nClick to see all measurements of the **first series** "], "code": "# Reference: https://www.kaggle.com/code/jesucristo/1-smart-robots-most-complete-notebook\n\nserie1 = tr.head(128)\nserie1.head()\nserie1.describe()\nplt.figure(figsize=(26, 16))\nfor i, col in enumerate(serie1.columns[3:]):\n    plt.subplot(3, 4, i + 1)\n    plt.plot(serie1[col])\n    plt.title(col)", "processed": ["3810 train seri 3816 test seri let engin featur exampl seri 1 let look valu featur singl time seri exampl seri 1 seri id 0 click see measur first seri"]}, {"markdown": ["## Visualizing Series\n\nBefore, I showed you as an example the series 1.\n\n**This code allows you to visualize any series.**\n\nFrom: *Code Snippet For Visualizing Series Id by @shaz13*"], "code": "# Reference: https://www.kaggle.com/code/jesucristo/1-smart-robots-most-complete-notebook\n\nseries_dict = {}\nfor series in (data['series_id'].unique()):\n    series_dict[series] = data[data['series_id'] == series]  \ndef plotSeries(series_id):\n    style.use('ggplot')\n    plt.figure(figsize=(28, 16))\n    print(target[target['series_id'] == series_id]['surface'].values[0].title())\n    for i, col in enumerate(series_dict[series_id].columns[3:]):\n        if col.startswith(\"o\"):\n            color = 'red'\n        elif col.startswith(\"a\"):\n            color = 'green'\n        else:\n            color = 'blue'\n        if i >= 7:\n            i+=1\n        plt.subplot(3, 4, i + 1)\n        plt.plot(series_dict[series_id][col], color=color, linewidth=3)\n        plt.title(col)", "processed": ["visual seri show exampl seri 1 code allow visual seri code snippet visual seri id shaz13"]}, {"markdown": ["Well, this is immportant, there is a **strong correlation** between:\n- angular_velocity_Z and angular_velocity_Y\n- orientation_X and orientation_Y\n- orientation_Y and orientation_Z\n\nMoreover, test has different correlations than training, for example:\n\n- angular_velocity_Z and orientation_X: -0.1(training) and 0.1(test). Anyway, is too small in both cases, it should not be a problem.", "## Fourier Analysis\n\nMy hope was, that different surface types yield (visible) differences in the frequency spectrum of the sensor measurements.\n\nMachine learning techniques might learn frequency filters on their own, but why don't give the machine a little head start? So I computed the the cyclic FFT for the angular velocity and linear acceleration sensors and plotted mean and standard deviation of the absolute values of the frequency components per training surface category (leaving out the frequency 0 (i.e. constants like sensor bias, earth gravity, ...).\n\nThe sensors show some different frequency characterists (see plots below), but unfortunately the surface categories have all similar (to the human eye) shapes, varying mostly in total power, and the standard deviations are high (compared to differences in the means). So there are no nice strong characteristic peaks for surface types. But that does not mean, that there is nothing detectable by more sophisticated statistical methods.\n\nThis article http://www.kaggle.com/christoffer/establishing-sampling-frequency makes a convincing case, that the sampling frequency is around 400Hz, so according to that you would see the frequency range to 3-200 Hz in the diagrams (and aliased higher frequencies).\n\nby [@trohwer64](https://www.kaggle.com/trohwer64)"], "code": "# Reference: https://www.kaggle.com/code/jesucristo/1-smart-robots-most-complete-notebook\n\n!ls ../input\ntrain_x = pd.read_csv('../input/career-con-2019/X_train.csv')\ntrain_y = pd.read_csv('../input/career-con-2019/y_train.csv')\nimport math\n\ndef prepare_data(t):\n    def f(d):\n        d=d.sort_values(by=['measurement_number'])\n        return pd.DataFrame({\n         'lx':[ d['linear_acceleration_X'].values ],\n         'ly':[ d['linear_acceleration_Y'].values ],\n         'lz':[ d['linear_acceleration_Z'].values ],\n         'ax':[ d['angular_velocity_X'].values ],\n         'ay':[ d['angular_velocity_Y'].values ],\n         'az':[ d['angular_velocity_Z'].values ],\n        })\n\n    t= t.groupby('series_id').apply(f)\n\n    def mfft(x):\n        return [ x/math.sqrt(128.0) for x in np.absolute(np.fft.fft(x)) ][1:65]\n\n    t['lx_f']=[ mfft(x) for x in t['lx'].values ]\n    t['ly_f']=[ mfft(x) for x in t['ly'].values ]\n    t['lz_f']=[ mfft(x) for x in t['lz'].values ]\n    t['ax_f']=[ mfft(x) for x in t['ax'].values ]\n    t['ay_f']=[ mfft(x) for x in t['ay'].values ]\n    t['az_f']=[ mfft(x) for x in t['az'].values ]\n    return t\nt=prepare_data(train_x)\nt=pd.merge(t,train_y[['series_id','surface','group_id']],on='series_id')\nt=t.rename(columns={\"surface\": \"y\"})\ndef aggf(d, feature):\n    va= np.array(d[feature].tolist())\n    mean= sum(va)/va.shape[0]\n    var= sum([ (va[i,:]-mean)**2 for i in range(va.shape[0]) ])/va.shape[0]\n    dev= [ math.sqrt(x) for x in var ]\n    return pd.DataFrame({\n        'mean': [ mean ],\n        'dev' : [ dev ],\n    })\n\ndisplay={\n'hard_tiles_large_space':'r-.',\n'concrete':'g-.',\n'tiled':'b-.',\n\n'fine_concrete':'r-',\n'wood':'g-',\n'carpet':'b-',\n'soft_pvc':'y-',\n\n'hard_tiles':'r--',\n'soft_tiles':'g--',\n}\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(14, 8*7))\n#plt.margins(x=0.0, y=0.0)\n#plt.tight_layout()\n# plt.figure()\n\nfeatures=['lx_f','ly_f','lz_f','ax_f','ay_f','az_f']\ncount=0\n\nfor feature in features:\n    stat= t.groupby('y').apply(aggf,feature)\n    stat.index= stat.index.droplevel(-1)\n    b=[*range(len(stat.at['carpet','mean']))]\n\n    count+=1\n    plt.subplot(len(features)+1,1,count)\n    for i,(k,v) in enumerate(display.items()):\n        plt.plot(b, stat.at[k,'mean'], v, label=k)\n        # plt.errorbar(b, stat.at[k,'mean'], yerr=stat.at[k,'dev'], fmt=v)\n   \n    leg = plt.legend(loc='best', ncol=3, mode=\"expand\", shadow=True, fancybox=True)\n    plt.title(\"sensor: \" + feature)\n    plt.xlabel(\"frequency component\")\n    plt.ylabel(\"amplitude\")\n\ncount+=1\nplt.subplot(len(features)+1,1,count)\nk='concrete'\nv=display[k]\nfeature='lz_f'\nstat= t.groupby('y').apply(aggf,feature)\nstat.index= stat.index.droplevel(-1)\nb=[*range(len(stat.at['carpet','mean']))]\n\nplt.errorbar(b, stat.at[k,'mean'], yerr=stat.at[k,'dev'], fmt=v)\nplt.title(\"sample for error bars (lz_f, surface concrete)\")\nplt.xlabel(\"frequency component\")\nplt.ylabel(\"amplitude\")\n\nplt.show()\ndel train_x, train_y\ngc.collect()", "processed": ["well immport strong correl angular veloc z angular veloc orient x orient orient orient z moreov test differ correl train exampl angular veloc z orient x 0 1 train 0 1 test anyway small case problem", "fourier analysi hope differ surfac type yield visibl differ frequenc spectrum sensor measur machin learn techniqu might learn frequenc filter give machin littl head start comput cyclic fft angular veloc linear acceler sensor plot mean standard deviat absolut valu frequenc compon per train surfac categori leav frequenc 0 e constant like sensor bia earth graviti sensor show differ frequenc characterist see plot unfortun surfac categori similar human eye shape vari mostli total power standard deviat high compar differ mean nice strong characterist peak surfac type mean noth detect sophist statist method articl http www kaggl com christoff establish sampl frequenc make convinc case sampl frequenc around 400hz accord would see frequenc rang 3 200 hz diagram alias higher frequenc trohwer64 http www kaggl com trohwer64"]}, {"markdown": ["## Is it an Humanoid Robot instead of a car?\n\n![](https://media1.giphy.com/media/on7ipUR0rFjRS/giphy.gif)\n\n**Acceleration**\n- X (mean at 0)\n- Y axis is centered at a value wich shows us the movement (straight ).\n- Z axis is centered at 10 (+- 9.8) wich is the gravity !! , you can see how the robot bounds.\n\nAngular velocity (X,Y,Z) has mean (0,0,0) so there is no lineal movement on those axis (measured with an encoder or potentiometer)\n\n**Fourier**\n\nWe can see: with a frequency 3 Hz we can see an acceleration, I think that acceleration represents one step.\nMaybe ee can suppose that every step is caused by many different movements, that's why there are different accelerations at different frequencies.\n\nAngular velocity represents spins. \nEvery time the engine/servo spins, the robot does an step - relation between acc y vel.", "---\n\n# Feature Engineering"], "code": "# Reference: https://www.kaggle.com/code/jesucristo/1-smart-robots-most-complete-notebook\n\ndef plot_feature_distribution(df1, df2, label1, label2, features,a=2,b=5):\n    i = 0\n    sns.set_style('whitegrid')\n    plt.figure()\n    fig, ax = plt.subplots(a,b,figsize=(17,9))\n\n    for feature in features:\n        i += 1\n        plt.subplot(a,b,i)\n        sns.kdeplot(df1[feature], bw=0.5,label=label1)\n        sns.kdeplot(df2[feature], bw=0.5,label=label2)\n        plt.xlabel(feature, fontsize=9)\n        locs, labels = plt.xticks()\n        plt.tick_params(axis='x', which='major', labelsize=8)\n        plt.tick_params(axis='y', which='major', labelsize=8)\n    plt.show();\nfeatures = data.columns.values[3:]\nplot_feature_distribution(data, test, 'train', 'test', features)", "processed": ["humanoid robot instead car http media1 giphi com medium on7ipur0rfjr giphi gif acceler x mean 0 axi center valu wich show u movement straight z axi center 10 9 8 wich graviti see robot bound angular veloc x z mean 0 0 0 lineal movement axi measur encod potentiomet fourier see frequenc 3 hz see acceler think acceler repres one step mayb ee suppos everi step caus mani differ movement differ acceler differ frequenc angular veloc repres spin everi time engin servo spin robot step relat acc vel", "featur engin"]}, {"markdown": ["Godd news, our basic features have the **same distribution (Normal) on test and training**. There are some differences between *orientation_X* , *orientation_Y* and *linear_acceleration_Y*.\n\nI willl try **StandardScaler** to fix this, and remember: orientation , angular velocity and linear acceleration are measured with different units, scaling might be a good choice."], "code": "# Reference: https://www.kaggle.com/code/jesucristo/1-smart-robots-most-complete-notebook\n\ndef plot_feature_class_distribution(classes,tt, features,a=5,b=2):\n    i = 0\n    sns.set_style('whitegrid')\n    plt.figure()\n    fig, ax = plt.subplots(a,b,figsize=(16,24))\n\n    for feature in features:\n        i += 1\n        plt.subplot(a,b,i)\n        for clas in classes:\n            ttc = tt[tt['surface']==clas]\n            sns.kdeplot(ttc[feature], bw=0.5,label=clas)\n        plt.xlabel(feature, fontsize=9)\n        locs, labels = plt.xticks()\n        plt.tick_params(axis='x', which='major', labelsize=8)\n        plt.tick_params(axis='y', which='major', labelsize=8)\n    plt.show();\nclasses = (target['surface'].value_counts()).index\naux = data.merge(target, on='series_id', how='inner')\nplot_feature_class_distribution(classes, aux, features)", "processed": ["godd news basic featur distribut normal test train differ orient x orient linear acceler willl tri standardscal fix rememb orient angular veloc linear acceler measur differ unit scale might good choic"]}, {"markdown": ["**Normal distribution**\n\nThere are obviously differences between *surfaces* and that's good, we will focus on that in order to classify them better.\n\nKnowing this differences and that variables follow a normal distribution (in most of the cases) we need to add new features like: ```mean, std, median, range ...``` (for each variable).\n\nHowever, I will try to fix *orientation_X* and *orientation_Y* as I explained before, scaling and normalizing data.\n\n---\n\n### Now with a new scale (more more precision)"], "code": "# Reference: https://www.kaggle.com/code/jesucristo/1-smart-robots-most-complete-notebook\n\nplt.figure(figsize=(26, 16))\nfor i,col in enumerate(aux.columns[3:13]):\n    ax = plt.subplot(3,4,i+1)\n    ax = plt.title(col)\n    for surface in classes:\n        surface_feature = aux[aux['surface'] == surface]\n        sns.kdeplot(surface_feature[col], label = surface)", "processed": ["normal distribut obvious differ surfac good focu order classifi better know differ variabl follow normal distribut case need add new featur like mean std median rang variabl howev tri fix orient x orient explain scale normal data new scale precis"]}, {"markdown": ["### Histogram for main features"], "code": "# Reference: https://www.kaggle.com/code/jesucristo/1-smart-robots-most-complete-notebook\n\nplt.figure(figsize=(26, 16))\nfor i, col in enumerate(data.columns[3:]):\n    ax = plt.subplot(3, 4, i + 1)\n    sns.distplot(data[col], bins=100, label='train')\n    sns.distplot(test[col], bins=100, label='test')\n    ax.legend()   ", "processed": ["histogram main featur"]}, {"markdown": ["\n> *Are there any reasons to not automatically normalize a quaternion? And if there are, what quaternion operations do result in non-normalized quaternions?*\n\nAny operation that produces a quaternion will need to be normalized because floating-point precession errors will cause it to not be unit length.\nI would advise against standard routines performing normalization automatically for performance reasons. \nAny competent programmer should be aware of the precision issues and be able to normalize the quantities when necessary - and it is not always necessary to have a unit length quaternion.\nThe same is true for vector operations.\n\nsource: https://stackoverflow.com/questions/11667783/quaternion-and-normalization"], "code": "# Reference: https://www.kaggle.com/code/jesucristo/1-smart-robots-most-complete-notebook\n\ndata = fe_step0(data)\ntest = fe_step0(test)\nprint(data.shape)\ndata.head()\nfig, (ax1, ax2, ax3, ax4) = plt.subplots(ncols=4, figsize=(18, 5))\n\nax1.set_title('quaternion X')\nsns.kdeplot(data['norm_X'], ax=ax1, label=\"train\")\nsns.kdeplot(test['norm_X'], ax=ax1, label=\"test\")\n\nax2.set_title('quaternion Y')\nsns.kdeplot(data['norm_Y'], ax=ax2, label=\"train\")\nsns.kdeplot(test['norm_Y'], ax=ax2, label=\"test\")\n\nax3.set_title('quaternion Z')\nsns.kdeplot(data['norm_Z'], ax=ax3, label=\"train\")\nsns.kdeplot(test['norm_Z'], ax=ax3, label=\"test\")\n\nax4.set_title('quaternion W')\nsns.kdeplot(data['norm_W'], ax=ax4, label=\"train\")\nsns.kdeplot(test['norm_W'], ax=ax4, label=\"test\")\n\nplt.show()", "processed": ["reason automat normal quaternion quaternion oper result non normal quaternion oper produc quaternion need normal float point precess error caus unit length would advis standard routin perform normal automat perform reason compet programm awar precis issu abl normal quantiti necessari alway necessari unit length quaternion true vector oper sourc http stackoverflow com question 11667783 quaternion normal"]}, {"markdown": ["![](https://d2gne97vdumgn3.cloudfront.net/api/file/UMYT4v0TyIgtyGm8ZXDQ)"], "code": "# Reference: https://www.kaggle.com/code/jesucristo/1-smart-robots-most-complete-notebook\n\nfig, (ax1, ax2, ax3) = plt.subplots(ncols=3, figsize=(15, 5))\n\nax1.set_title('Roll')\nsns.kdeplot(data['euler_x'], ax=ax1, label=\"train\")\nsns.kdeplot(test['euler_x'], ax=ax1, label=\"test\")\n\nax2.set_title('Pitch')\nsns.kdeplot(data['euler_y'], ax=ax2, label=\"train\")\nsns.kdeplot(test['euler_y'], ax=ax2, label=\"test\")\n\nax3.set_title('Yaw')\nsns.kdeplot(data['euler_z'], ax=ax3, label=\"train\")\nsns.kdeplot(test['euler_z'], ax=ax3, label=\"test\")\n\nplt.show()", "processed": ["http d2gne97vdumgn3 cloudfront net api file umyt4v0tyigtygm8zxdq"]}, {"markdown": ["Remember the order at the array is [0.73LB, 0.72LB, 0.71LB, 06LB].\nSeries with ```series_id```= 575, 1024, 911, 723, 148, 338 are really interesting because they show important differences between surfaces that often are being confused.", "## Next Step ??\n\n- I will create a test dataset with those special cases and then I will ad a new CV stage where I will try to classify those surfaces correctly.\n- I will look for surfaces distinctive features.", "## Generate a new train and test: Fast Fourier Transform Denoising"], "code": "# Reference: https://www.kaggle.com/code/jesucristo/1-smart-robots-most-complete-notebook\n\nfrom numpy.fft import *\nimport matplotlib.pyplot as plt\nimport matplotlib.style as style \nstyle.use('ggplot')\nX_train = pd.read_csv('../input/career-con-2019/X_train.csv')\nX_test = pd.read_csv('../input/career-con-2019/X_test.csv')\ntarget = pd.read_csv('../input/career-con-2019/y_train.csv')\nseries_dict = {}\nfor series in (X_train['series_id'].unique()):\n    series_dict[series] = X_train[X_train['series_id'] == series] \n# From: Code Snippet For Visualizing Series Id by @shaz13\ndef plotSeries(series_id):\n    style.use('ggplot')\n    plt.figure(figsize=(28, 16))\n    print(target[target['series_id'] == series_id]['surface'].values[0].title())\n    for i, col in enumerate(series_dict[series_id].columns[3:]):\n        if col.startswith(\"o\"):\n            color = 'red'\n        elif col.startswith(\"a\"):\n            color = 'green'\n        else:\n            color = 'blue'\n        if i >= 7:\n            i+=1\n        plt.subplot(3, 4, i + 1)\n        plt.plot(series_dict[series_id][col], color=color, linewidth=3)\n        plt.title(col)\nplotSeries(1)\n# from @theoviel at https://www.kaggle.com/theoviel/fast-fourier-transform-denoising\ndef filter_signal(signal, threshold=1e3):\n    fourier = rfft(signal)\n    frequencies = rfftfreq(signal.size, d=20e-3/signal.size)\n    fourier[frequencies > threshold] = 0\n    return irfft(fourier)", "processed": ["rememb order array 0 73lb 0 72lb 0 71lb 06lb seri seri id 575 1024 911 723 148 338 realli interest show import differ surfac often confus", "next step creat test dataset special case ad new cv stage tri classifi surfac correctli look surfac distinct featur", "gener new train test fast fourier transform denois"]}, {"markdown": ["Let's denoise train and test angular_velocity and linear_acceleration data"], "code": "# Reference: https://www.kaggle.com/code/jesucristo/1-smart-robots-most-complete-notebook\n\nX_train_denoised = X_train.copy()\nX_test_denoised = X_test.copy()\n\n# train\nfor col in X_train.columns:\n    if col[0:3] == 'ang' or col[0:3] == 'lin':\n        # Apply filter_signal function to the data in each series\n        denoised_data = X_train.groupby(['series_id'])[col].apply(lambda x: filter_signal(x))\n        \n        # Assign the denoised data back to X_train\n        list_denoised_data = []\n        for arr in denoised_data:\n            for val in arr:\n                list_denoised_data.append(val)\n                \n        X_train_denoised[col] = list_denoised_data\n        \n# test\nfor col in X_test.columns:\n    if col[0:3] == 'ang' or col[0:3] == 'lin':\n        # Apply filter_signal function to the data in each series\n        denoised_data = X_test.groupby(['series_id'])[col].apply(lambda x: filter_signal(x))\n        \n        # Assign the denoised data back to X_train\n        list_denoised_data = []\n        for arr in denoised_data:\n            for val in arr:\n                list_denoised_data.append(val)\n                \n        X_test_denoised[col] = list_denoised_data\nseries_dict = {}\nfor series in (X_train_denoised['series_id'].unique()):\n    series_dict[series] = X_train_denoised[X_train_denoised['series_id'] == series] \nplotSeries(1)\nplt.figure(figsize=(24, 8))\nplt.title('linear_acceleration_X')\nplt.plot(X_train.angular_velocity_Z[128:256], label=\"original\");\nplt.plot(X_train_denoised.angular_velocity_Z[128:256], label=\"denoised\");\nplt.legend()\nplt.show()", "processed": ["let denois train test angular veloc linear acceler data"]}, {"markdown": ["## Functions to epxlore the data"], "code": "# Reference: https://www.kaggle.com/code/kabure/baseline-fraud-detection-eda-interactive-views\n\ndef resumetable(df):\n    print(f\"Dataset Shape: {df.shape}\")\n    summary = pd.DataFrame(df.dtypes,columns=['dtypes'])\n    summary = summary.reset_index()\n    summary['Name'] = summary['index']\n    summary = summary[['Name','dtypes']]\n    summary['Missing'] = df.isnull().sum().values    \n    summary['Uniques'] = df.nunique().values\n    summary['First Value'] = df.loc[0].values\n    summary['Second Value'] = df.loc[1].values\n    summary['Third Value'] = df.loc[2].values\n\n    for name in summary['Name'].value_counts().index:\n        summary.loc[summary['Name'] == name, 'Entropy'] = round(stats.entropy(df[name].value_counts(normalize=True), base=2),2) \n\n    return summary\n\ndef plot_distribution(df, var_select=None, title=None, bins=1.0): \n    # Calculate the correlation coefficient between the new variable and the target\n    tmp_fraud = df[df['isFraud'] == 1]\n    tmp_no_fraud = df[df['isFraud'] == 0]    \n    corr = df['isFraud'].corr(df[var_select])\n    corr = np.round(corr,3)\n    tmp1 = tmp_fraud[var_select].dropna()\n    tmp2 = tmp_no_fraud[var_select].dropna()\n    hist_data = [tmp1, tmp2]\n    \n    group_labels = ['Fraud', 'No Fraud']\n    colors = ['seagreen','indianred', ]\n\n    fig = ff.create_distplot(hist_data,\n                             group_labels,\n                             colors = colors, \n                             show_hist = True,\n                             curve_type='kde', \n                             bin_size = bins\n                            )\n    \n    fig['layout'].update(title = title+' '+'(corr target ='+ str(corr)+')')\n\n    iplot(fig, filename = 'Density plot')\n    \ndef plot_dist_churn(df, col, binary=None):\n    tmp_churn = df[df[binary] == 1]\n    tmp_no_churn = df[df[binary] == 0]\n    tmp_attr = round(tmp_churn[col].value_counts().sort_index() / df[col].value_counts().sort_index(),2)*100\n    print(f'Distribution of {col}: ')\n    trace1 = go.Bar(\n        x=tmp_churn[col].value_counts().sort_index().index,\n        y=tmp_churn[col].value_counts().sort_index().values, \n        name='Fraud',opacity = 0.8, marker=dict(\n            color='seagreen',\n            line=dict(color='#000000',width=1)))\n\n    trace2 = go.Bar(\n        x=tmp_no_churn[col].value_counts().sort_index().index,\n        y=tmp_no_churn[col].value_counts().sort_index().values,\n        name='No Fraud', opacity = 0.8, \n        marker=dict(\n            color='indianred',\n            line=dict(color='#000000',\n                      width=1)\n        )\n    )\n\n    trace3 =  go.Scatter(   \n        x=tmp_attr.sort_index().index,\n        y=tmp_attr.sort_index().values,\n        yaxis = 'y2', \n        name='% Fraud', opacity = 0.6, \n        marker=dict(\n            color='black',\n            line=dict(color='#000000',\n                      width=2 )\n        )\n    )\n    \n    layout = dict(title =  f'Distribution of {str(col)} feature by %Fraud',\n              xaxis=dict(type='category'), \n              yaxis=dict(title= 'Count'), \n              yaxis2=dict(range= [0, 15], \n                          overlaying= 'y', \n                          anchor= 'x', \n                          side= 'right',\n                          zeroline=False,\n                          showgrid= False, \n                          title= 'Percentual Fraud Transactions'\n                         ))\n\n    fig = go.Figure(data=[trace1, trace2, trace3], layout=layout)\n    iplot(fig)\n    \n## Function to reduce the DF size\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df\n\n## REducing memory\ndf_train_trans = reduce_mem_usage(df_train_trans)\ndf_train_id = reduce_mem_usage(df_train_id)", "processed": ["function epxlor data"]}, {"markdown": ["Wow, We have a bizarre high dimension. The shape of Transactions is: 506691, 393<br>\nI will need some time to explore it further. The first aim is start simple. ", "## Understanding the Target Distribution"], "code": "# Reference: https://www.kaggle.com/code/kabure/baseline-fraud-detection-eda-interactive-views\n\nprint(\"Transactions % Fraud:\")\nprint(round(df_train_trans[['isFraud', 'TransactionID']]['isFraud'].value_counts(normalize=True) * 100,2))\n# df_train.groupby('Churn')['customerID'].count().iplot(kind='bar', title='Churn (Target) Distribution', \n#                                                      xTitle='Customer Churn?', yTitle='Count')\n\ntrace0 = go.Bar(\n    x=df_train_trans[['isFraud', 'TransactionID']].groupby('isFraud')['TransactionID'].count().index,\n    y=df_train_trans[['isFraud', 'TransactionID']].groupby('isFraud')['TransactionID'].count().values,\n    marker=dict(\n        color=['indianred', 'seagreen']),\n)\n\ndata = [trace0] \nlayout = go.Layout(\n    title='Fraud (Target) Distribution <br>## 0: No Fraud | 1: Is Fraud ##', \n    xaxis=dict(\n        title='Transaction is fraud', \n        type='category'),\n    yaxis=dict(\n        title='Count')\n)\n\nfig = go.Figure(data=data, layout=layout)\niplot(fig)", "processed": ["wow bizarr high dimens shape transact 506691 393 br need time explor first aim start simpl", "understand target distribut"]}, {"markdown": ["# Conclusion\nIn conclusion, the success of using 512 separate models suggests that the 262144 rows of \"Instant Gratification\" dataset may actually be 512 partial datasets that were combined. Is this competition actually 512 models (competitions) in one? The appendix below investigates this further.\n\nThree suggestions to improve this kernel's accuracy are (1) identify and model interactions between partial datasets (2) extract information from the approx 215 variables that are not used in each model (3) build better partial models than SVC polynomial degree=4 via NN or LGBM.\n\n# Appendix\n## Variables are not Gaussian\nEach variable appears to be a nice bell shaped curve. However the curves are too tall and narrow. This is seen below by comparing the distribution of some variables with a Gaussian of the same mean and standard deviation. One can also verify that the variables are not Gaussian by making normality plots (pictured below)."], "code": "# Reference: https://www.kaggle.com/code/cdeotte/support-vector-machine-0-925\n\n# LOAD LIBRARIES\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# PLOT FIRST 8 VARIABLES\nplt.figure(figsize=(15,15))\nfor i in range(8):\n    plt.subplot(3,3,i+1)\n    #plt.hist(train.iloc[:,i+1],bins=100)\n    sns.distplot(train.iloc[:,i+1],bins=100)\n    plt.title( train.columns[i+1] )\n    plt.xlabel('')\n    \n# PLOT GAUSSIAN FOR COMPARISON\nplt.subplot(3,3,9)\nstd = round(np.std(train.iloc[:,8]),2)\ndata = np.random.normal(0,std,len(train))\nsns.distplot(data,bins=100)\nplt.xlim((-17,17))\nplt.ylim((0,0.37))\nplt.title(\"Gaussian with m=0, std=\"+str(std))\n\nplt.subplots_adjust(hspace=0.3)\nplt.show()", "processed": ["conclus conclus success use 512 separ model suggest 262144 row instant gratif dataset may actual 512 partial dataset combin competit actual 512 model competit one appendix investig three suggest improv kernel accuraci 1 identifi model interact partial dataset 2 extract inform approx 215 variabl use model 3 build better partial model svc polynomi degre 4 via nn lgbm appendix variabl gaussian variabl appear nice bell shape curv howev curv tall narrow seen compar distribut variabl gaussian mean standard deviat one also verifi variabl gaussian make normal plot pictur"]}, {"markdown": ["## Normality Plots\nNormality plots indicate that the variables are not Gaussian. If they were Gaussian, then we would see straight lines below. Instead we see piecewise straight lines indicating that we may have Gaussian mixture models. (Each variable is the sum of multiple Gaussians)."], "code": "# Reference: https://www.kaggle.com/code/cdeotte/support-vector-machine-0-925\n\n# NORMALITY PLOTS FOR FIRST 8 VARIABLES\nplt.figure(figsize=(15,15))\nfor i in range(8):\n    plt.subplot(3,3,i+1)\n    stats.probplot(train.iloc[:,1], plot=plt)\n    plt.title( train.columns[i+1] )\n    \n# NORMALITY PLOT FOR GAUSSIAN\nplt.subplot(3,3,9)\nstats.probplot(data, plot=plt)   \nplt.title(\"Gaussian with m=0, std=\"+str(std))\n\nplt.subplots_adjust(hspace=0.4)\nplt.show()", "processed": ["normal plot normal plot indic variabl gaussian gaussian would see straight line instead see piecewis straight line indic may gaussian mixtur model variabl sum multipl gaussian"]}, {"markdown": ["## Variables within partial datasets are Gaussian\nIf we only look at the partial datasets where `wheezy-copper-turtle-magic = k` for `0 <= k <= 511`, then the variables are Gaussian. This can be seen by the plots below. This suggests how the full dataset was made. Perhaps Kaggle made 512 different datasets and then combined them for this competition."], "code": "# Reference: https://www.kaggle.com/code/cdeotte/support-vector-machine-0-925\n\ntrain0 = train[ train['wheezy-copper-turtle-magic']==0 ]\n# PLOT FIRST 8 VARIABLES\nplt.figure(figsize=(15,15))\nfor i in range(8):\n    plt.subplot(3,3,i+1)\n    #plt.hist(train0.iloc[:,i+1],bins=10)\n    sns.distplot(train0.iloc[:,i+1],bins=10)\n    plt.title( train.columns[i+1] )\n    plt.xlabel('')\n    \n# PLOT GAUSSIAN FOR COMPARISON\nplt.subplot(3,3,9)\nstd0 = round(np.std(train0.iloc[:,8]),2)\ndata0 = np.random.normal(0,std0,2*len(train0))\nsns.distplot(data0,bins=10)\nplt.xlim((-17,17))\nplt.ylim((0,0.1))\nplt.title(\"Gaussian with m=0, std=\"+str(std0))\n    \nplt.subplots_adjust(hspace=0.3)\nplt.show()", "processed": ["variabl within partial dataset gaussian look partial dataset wheezi copper turtl magic k 0 k 511 variabl gaussian seen plot suggest full dataset made perhap kaggl made 512 differ dataset combin competit"]}, {"markdown": ["## Evaluation"], "code": "# Reference: https://www.kaggle.com/code/xhlulu/severstal-simple-2-step-pipeline\n\nwith open('history.json', 'w') as f:\n    json.dump(history.history, f)\n\nhistory_df = pd.DataFrame(history.history)\nhistory_df[['loss', 'val_loss']].plot()\nhistory_df[['dice_coef', 'val_dice_coef']].plot()", "processed": ["evalu"]}, {"markdown": ["**Plot distribution plots of each feature for each class (0 and 1)**"], "code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/santander-data-visualization\n\ntrain_data = pd.DataFrame(train_features)\ntrain_data['target'] = train_target\ncolumns = train_data.columns\n\nfor column in columns:\n    if column != 'target':\n        sns.pairplot(x_vars=column, y_vars=column, hue='target', diag_kind='kde', data=train_data, palette=['green', 'red'])\nzero_samples = []\none_samples = []\n\nfor i, target in enumerate(train_target):\n    if target == 0:\n        zero_samples.append(train_features[i])\n    else:\n        one_samples.append(train_features[i])\n\nzero_samples = np.array(zero_samples)\none_samples = np.array(one_samples)", "processed": ["plot distribut plot featur class 0 1"]}, {"markdown": ["Looks like there are not any visible outliers in the data but the range is quite high.\n\nWe can now do a histogram plot of the target variable."], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-baseline-santander-value\n\nplt.figure(figsize=(12,8))\nsns.distplot(train_df[\"target\"].values, bins=50, kde=False)\nplt.xlabel('Target', fontsize=12)\nplt.title(\"Target Histogram\", fontsize=14)\nplt.show()", "processed": ["look like visibl outlier data rang quit high histogram plot target variabl"]}, {"markdown": ["This is a right (Thanks to Wesam for pointing out my mistake) skewed distribution with majority of the data points having low value. Our competition admins are aware of this one and so they have chosen the evaluation metric as RMSLE (Root Mean Squared Logarithmic Error.).  \n\nSo let us do a histogram plot on the log of target variables and recheck again."], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-baseline-santander-value\n\nplt.figure(figsize=(12,8))\nsns.distplot( np.log1p(train_df[\"target\"].values), bins=50, kde=False)\nplt.xlabel('Target', fontsize=12)\nplt.title(\"Log of Target Histogram\", fontsize=14)\nplt.show()", "processed": ["right thank wesam point mistak skew distribut major data point low valu competit admin awar one chosen evalu metric rmsle root mean squar logarithm error let u histogram plot log target variabl recheck"]}, {"markdown": ["We can now clearly see problems on severe_toxic, threat and identity_hate.\n\nAnother way to look at this uses the AUC curve directly."], "code": "# Reference: https://www.kaggle.com/code/ogrellier/things-you-need-to-be-aware-of-before-stacking\n\nfigures = []\nfor i_class, class_name in enumerate(class_names):\n    # create a new plot for current class\n    # Compute full score :\n    full = roc_auc_score(oof[class_names[i_class]], oof[class_preds[i_class]])\n    # Compute average score\n    avg = 0.0\n    for n_fold, (_, val_idx) in enumerate(folds.split(oof)):\n        avg += roc_auc_score(oof[class_names[i_class]].iloc[val_idx], oof[class_preds[i_class]].iloc[val_idx]) / folds.n_splits\n    \n    s = figure(plot_width=400, plot_height=400, \n               title=\"%s ROC curves OOF %.6f / Mean %.6f\" % (class_name, full, avg))\n    \n    for n_fold, (_, val_idx) in enumerate(folds.split(oof)):\n        # Get False positives, true positives and the list of thresholds used to compute them\n        fpr, tpr, thresholds = roc_curve(oof[class_names[i_class]].iloc[val_idx], \n                                         oof[class_preds[i_class]].iloc[val_idx])\n        s.line(fpr, tpr, name=\"Fold %d\" % n_fold, color=brewer[\"Set1\"][4][n_fold])\n        s.line([0, 1], [0, 1], color='navy', line_width=1, line_dash=\"dashed\")\n\n    figures.append(s)\n\n# put the results in a column and show\nshow(gridplot(np.array_split(figures, 3)))", "processed": ["clearli see problem sever toxic threat ident hate anoth way look use auc curv directli"]}, {"markdown": ["It could be interesting to see trends:"], "code": "# Reference: https://www.kaggle.com/code/artgor/eda-feature-engineering-and-xgb-lgb\n\nfig, ax1 = plt.subplots(figsize=(16, 8))\nplt.title(\"Trends of approval rates and number of projects\")\ncount_by_date.rolling(window=12,center=False).mean().plot(ax=ax1, legend=False)\nax1.set_ylabel('Projects count', color='b')\nplt.legend(['Projects count'])\nax2 = ax1.twinx()\nmean_by_date.rolling(window=12,center=False).mean().plot(ax=ax2, color='g', legend=False)\nax2.set_ylabel('Approval rate', color='g')\nplt.legend(['Approval rate'], loc=(0.875, 0.9))\nplt.grid(False)", "processed": ["could interest see trend"]}, {"markdown": ["As we can see, we don't have even one year of data, so it is diffucult to make good conclusions about long-term trends, but let's try to make some observations:\n- At the very beginning, when teachers had to write 4 essays, the number of projects was quite low, but the approval rate was the highest;\n- Then approval rate began rapidly declining. Maybe teachers weren't able to adapt to new rules fast enough, and as a result they weren't able to meet new criteria?\n- There are two huge spikes in the number of submitted projects: at the beginning of August and September. I suppose that teachers requested new supplies for the beginning of new school year;\n- And in 2017 we can see a downtrend both in the number of submitted projects and approval rates;"], "code": "# Reference: https://www.kaggle.com/code/artgor/eda-feature-engineering-and-xgb-lgb\n\nfig, ax1 = plt.subplots(figsize=(16, 8))\nplt.title(\"Project count and approval rate by day of week.\")\nsns.countplot(x='weekday', data=train, ax=ax1)\nax1.set_ylabel('Projects count', color='b')\nplt.legend(['Projects count'])\nax2 = ax1.twinx()\nsns.pointplot(x=\"weekday\", y=\"project_is_approved\", data=train, ci=99, ax=ax2, color='black')\nax2.set_ylabel('Approval rate', color='g')\nplt.legend(['Approval rate'], loc=(0.875, 0.9))\nplt.grid(False)", "processed": ["see even one year data diffucult make good conclus long term trend let tri make observ begin teacher write 4 essay number project quit low approv rate highest approv rate began rapidli declin mayb teacher abl adapt new rule fast enough result abl meet new criterion two huge spike number submit project begin august septemb suppos teacher request new suppli begin new school year 2017 see downtrend number submit project approv rate"]}, {"markdown": ["### teacher_number_of_previously_posted_projects"], "code": "# Reference: https://www.kaggle.com/code/artgor/eda-feature-engineering-and-xgb-lgb\n\ntrain.groupby('teacher_number_of_previously_posted_projects')['project_is_approved'].mean().plot()", "processed": ["teacher number previous post project"]}, {"markdown": ["We can see that the more projects were submitted by a techer, the higher are chances for approval.", "### Resources", "Let's transform the data:\n- create cost column which will show total cost of the item (quantity * price);\n- group data by project id and show aggregate values;"], "code": "# Reference: https://www.kaggle.com/code/artgor/eda-feature-engineering-and-xgb-lgb\n\nresources['cost'] = resources['quantity'] * resources['price']\nresources_aggregated = resources.groupby('id').agg({'description': ['nunique'], 'quantity': ['sum'], 'cost': ['mean', 'sum']})\nresources_aggregated.columns = ['unique_items', 'total_quantity', 'mean_cost', 'total_cost']\nresources_aggregated.reset_index(inplace=True)\nresources_aggregated.head()\nprint('99 percentile is {0}.'.format(np.percentile(resources_aggregated.mean_cost, 99)))\nplt.boxplot(resources_aggregated.mean_cost);", "processed": ["see project submit techer higher chanc approv", "resourc", "let transform data creat cost column show total cost item quantiti price group data project id show aggreg valu"]}, {"markdown": [":-)", "## Non-targeted Attack: Maximizing output-target discrepance <a class=\"anchor\" id=\"nontargeted\"></a>\n\nIn analogy to the maximum likelihood approach I define a discrepancy function as my objective. Usually the likelihood gives us the probability that our model output y matches the target t. Thus maximizing the likelihood yields us the best matches. Creating an attack one has to think the other way round: We want to maximize the probability that the outputs do not match the targets. Let's write down this descrepancy function by using the multinomial distribution again:\n\n$$ D(t|y(w,x)) = \\prod_{n=1}^{N} \\prod_{k=1}^{K} y_{n,k}^{1-t_{n,k}} $$", "Notice that in contrast to the likelihood function we now have 1-t instead of t. If our true target $t_{n,k}$ of class k for input $x_{n}$ is 1, but our model predicts $y_{n,k}=0$, then we have $0^{1-1}=1$ and vice versa $1^{1-0}=1$.  ", "To train a model we usually would maximize the likelihood with respect to the weight parameters whereas inputs are fixed. In our case we already have a trained model and fixed weights. But we can add tiny perturbations to our input images such that we maximize our discrepancy function. Let's do that and for making things simpler we use the log! :-)\n\n$$\\nabla_{x} \\hat{D} = \\nabla_{x} log D = \\nabla_{x} \\sum_{n=1}^{N} \\sum_{k=1}^{K} (1-t_{k}) \\log y_{k} $$\n\n$$\\hat{D} = \\sum_{n=1}^{N} \\sum_{k=1}^{K} (1-t_{n,k}) \\log y_{n,k}$$\n\n$$ \\partial_{x} \\hat{D} = \\sum_{n=1}^{N} \\sum_{k=1}^{K} \\frac {\\partial \\hat{D}}{\\partial y_{k}} \\frac {\\partial y_{k}}{\\partial x} $$", "The first part is easy:\n\n$$ \\frac {\\partial \\hat{D}}{\\partial y_{k}}  = \\frac {1-t_{k}} {y_{k}}$$", "For the second part we should keep in mind the gradient of activations yields the class weight vector:\n\n$$ a_{c} = \\vec{w_{c}}^{T} \\vec{x} = \\vec{x}^{T} \\vec{w_{c}} $$\n\n$$ \\nabla_{x} a_{c} = \\vec{w_{c}} $$\n\n$$ y_{k} = \\frac{\\exp(a_{k})} {\\sum_{c=1}^{K}\\exp(a_{c})}$$", "Now this second part is a bit cumbersome (... hopefully I didn't make a mistake ...):\n\n$$ \\frac {\\partial y_{k}}{\\partial x} = \\frac{\\Sigma \\cdot \\partial_{x} \\exp(a_{k}) - \\exp(a_{k}) \\partial_{x} \\Sigma}{\\Sigma^{2}} = \\frac {\\sum_{c=1}^{K} \\exp(a_{c}) \\cdot \\vec{w}_{k} \\exp(a_{k}) - \\exp(a_{k}) \\sum_{c=1}^{K} \\vec{w}_{c} \\exp(a_{c})} {\\sum_{c=1}^{K}\\exp(a_{c}) \\cdot \\sum_{c=1}^{K}\\exp(a_{c})} $$\n\n$$ \\frac {\\partial y_{k}}{\\partial x} =  y_{k} \\vec{w}_{k} - y_{k} \\cdot \\sum_{c=1}^{K} y_{c} \\vec{w}_{c}  $$", "Finally I end up with:\n\n$$ \\nabla_{x} \\log D = \\sum_{k=1}^{K} (1-t_{k}) \\cdot  (\\vec{w}_{k} - \\sum_{c=1}^{K} y_{c} \\vec{w}_{c}) $$", "Let's try to understand this: For the true label the summand is 0 whereas all other classes contribute to the gradient with their class weight vector $\\vec{w_{k}}$ reduced by an output \"weighted\" sum of all other class weights. What does that mean? ...", "To maximize the discrepance for playing around we can use gradient ascent. Though we have to find a sufficient rate $\\eta$. Given an input $x_{m}$ we will then add a perturbation with:", "$$ x_{p, m} = x_{m} + \\delta x_{m} = x_{m} + \\eta \\cdot \\nabla_{x_{m,c}} D_{m,c} $$\n\n$$ x_{p, m} = x_{m} + \\eta \\sum_{k=1}^{K} (1-t_{m,k}) \\cdot  (\\vec{w}_{k} - \\sum_{c=1}^{K} y_{m,c} \\vec{w}_{c}) $$", "Let's give it a try! :-)", "### Adding tiny perturbations", "First of all we need to calculate the perturbations for each image in the test set. To do this we have to transform our true targets to one-hot-targets and call attack :-). As I want to see, how much epsilon we need to create a good breakdown, I use the attack_to_max_epsilon method. "], "code": "# Reference: https://www.kaggle.com/code/allunia/how-to-attack-a-machine-learning-model\n\nattack.create_one_hot_targets(y_test)\nattack.attack_to_max_epsilon(non_targeted_gradient, 30)\nnon_targeted_scores = attack.scores\nsns.set()\nplt.figure(figsize=(10,5))\nplt.plot(attack.epsilons, attack.scores, 'g*')\nplt.ylabel('accuracy_score')\nplt.xlabel('epsilon')\nplt.title('Accuracy score breakdown - non-targeted attack');", "processed": ["", "non target attack maxim output target discrep class anchor id nontarget analog maximum likelihood approach defin discrep function object usual likelihood give u probabl model output match target thu maxim likelihood yield u best match creat attack one think way round want maxim probabl output match target let write descrep function use multinomi distribut w x prod n 1 n prod k 1 k n k 1 n k", "notic contrast likelihood function 1 instead true target n k class k input x n 1 model predict n k 0 0 1 1 1 vice versa 1 1 0 1", "train model usual would maxim likelihood respect weight paramet wherea input fix case alreadi train model fix weight add tini perturb input imag maxim discrep function let make thing simpler use log abla x hat abla x log abla x sum n 1 n sum k 1 k 1 k log k hat sum n 1 n sum k 1 k 1 n k log n k partial x hat sum n 1 n sum k 1 k frac partial hat partial k frac partial k partial x", "first part easi frac partial hat partial k frac 1 k k", "second part keep mind gradient activ yield class weight vector c vec w c vec x vec x vec w c abla x c vec w c k frac exp k sum c 1 k exp c", "second part bit cumbersom hope make mistak frac partial k partial x frac sigma cdot partial x exp k exp k partial x sigma sigma 2 frac sum c 1 k exp c cdot vec w k exp k exp k sum c 1 k vec w c exp c sum c 1 k exp c cdot sum c 1 k exp c frac partial k partial x k vec w k k cdot sum c 1 k c vec w c", "final end abla x log sum k 1 k 1 k cdot vec w k sum c 1 k c vec w c", "let tri understand true label summand 0 wherea class contribut gradient class weight vector vec w k reduc output weight sum class weight mean", "maxim discrep play around use gradient ascent though find suffici rate eta given input x add perturb", "x p x delta x x eta cdot abla x c c x p x eta sum k 1 k 1 k cdot vec w k sum c 1 k c vec w c", "let give tri", "ad tini perturb", "first need calcul perturb imag test set transform true target one hot target call attack want see much epsilon need creat good breakdown use attack max epsilon method"]}, {"markdown": ["### The gradient travel guide - natural fooling targets\n\nI'm happy that it was possible to fool our model but it's still diffuse and unclear where the one-step-gradient guides us through (remember we do not iterate with gradient ascent, we just take one step and size is given by strength of gradient times eta). I assume that some numbers are closer to each other in weight space than to others. As the model training draws decision boundaries dependent on the quality of the input data and flexibility of model architecture, there will be regions where a 3 is not predicted as 3 but as 8. Those regions where the model makes an incorrect prediction. And I think, that there are preffered numbers to be wrong predictions given a digit input image. Perhaps the fooling gradients drives us to those \"natural\" fooling target numbers? "], "code": "# Reference: https://www.kaggle.com/code/allunia/how-to-attack-a-machine-learning-model\n\nplt.figure(figsize=(10,5))\nsns.countplot(x='y_fooled', data=example_results[example_results.y_true != example_results.y_fooled])", "processed": ["gradient travel guid natur fool target happi possibl fool model still diffus unclear one step gradient guid u rememb iter gradient ascent take one step size given strength gradient time eta assum number closer weight space other model train draw decis boundari depend qualiti input data flexibl model architectur region 3 predict 3 8 region model make incorrect predict think preffer number wrong predict given digit input imag perhap fool gradient drive u natur fool target number"]}, {"markdown": ["Ok, so out of 16800 samples, the model failed to predict around 1600. That's why our intital accuracy score is close to 90 % (means 10 % failing). Now, which digit was selected as wrong prediction result most often?"], "code": "# Reference: https://www.kaggle.com/code/allunia/how-to-attack-a-machine-learning-model\n\nplt.figure(figsize=(10,5))\nsns.countplot(x='y_predicted', data=wrong_predictions)", "processed": ["ok 16800 sampl model fail predict around 1600 intit accuraci score close 90 mean 10 fail digit select wrong predict result often"]}, {"markdown": ["Yes, that's the same pattern as for the fooling targets. As this is caused by the difficulty of our model to draw good decision boundaries we should see this pattern as well for the true labels of those digits that were wrong predicted:"], "code": "# Reference: https://www.kaggle.com/code/allunia/how-to-attack-a-machine-learning-model\n\nplt.figure(figsize=(10,5))\nsns.countplot(x='y_true', data=wrong_predictions)", "processed": ["ye pattern fool target caus difficulti model draw good decis boundari see pattern well true label digit wrong predict"]}, {"markdown": ["Even though we can see high background noise the true label is not destroyed. I can still see the true label with my eyes whereas the model predicts the desired fooling target (0 to 9, except true label). :-) That's cool.", "## Natural vs. non-natural targeted attack\n\nNow, I like to see what happens with the accuracy score if we fool the model for each image in the test set. \nBy analyzing non-targeted attacks we found that some digits are more used as \"fooling\" target than others and that each digit has its fooling digit counterpart. I assume that fooling takes place in regions where the model fails to draw good decision boundaries. Using targeted attack we should see that we can breakdown the accuracy score easier with natural fooling targets than with the other digits. Let's try this! :-)", "### Prepare natural and non-natural fooling targets\n\nThe gradient travel guide showed us the occurences of fooling target digits for each true digit. The highest count stands for the natural fooling target whereas the lowest corresponds to the non-natural fooling target. Given the heatmap we could create the targets by argmin and argmax per row (y_true) as follows:"], "code": "# Reference: https://www.kaggle.com/code/allunia/how-to-attack-a-machine-learning-model\n\nf, ax = plt.subplots(figsize=(10, 10))\nsns.heatmap(attacktargets, annot=True, ax=ax, cbar=False, cmap=\"Purples\", fmt=\"g\");\nnatural_targets_dict = {}\nnon_natural_targets_dict = {}\nfor ix, series in attacktargets.iterrows():\n    natural_targets_dict[ix] = series.argmax()\n    non_natural_targets_dict[ix] = series.drop(ix).argmin()\nnatural_targets_dict\nnatural_foolingtargets = np.zeros((y_test.shape[0]))\nnon_natural_foolingtargets = np.zeros((y_test.shape[0]))\n\nfor n in range(len(natural_foolingtargets)):\n    target = y_test[n]\n    natural_foolingtargets[n] = natural_targets_dict[target]\n    non_natural_foolingtargets[n] = non_natural_targets_dict[target]\nattack.create_one_hot_targets(natural_foolingtargets.astype(np.int))\nattack.attack_to_max_epsilon(targeted_gradient, 30)\nnatural_scores = attack.scores\nattack.create_one_hot_targets(non_natural_foolingtargets.astype(np.int))\nattack.attack_to_max_epsilon(targeted_gradient, 30)\nnon_natural_scores = attack.scores\nplt.figure(figsize=(10,5))\nnf, = plt.plot(attack.epsilons, natural_scores, 'g*', label='natural fooling')\nnnf, = plt.plot(attack.epsilons, non_natural_scores, 'b*', label='non-natural fooling')\nplt.legend(handles=[nf, nnf])\nplt.ylabel('accuracy_score')\nplt.xlabel('epsilon')\nplt.title('Accuracy score breakdown: natural vs non-natural targeted attack');", "processed": ["even though see high background nois true label destroy still see true label eye wherea model predict desir fool target 0 9 except true label cool", "natur v non natur target attack like see happen accuraci score fool model imag test set analyz non target attack found digit use fool target other digit fool digit counterpart assum fool take place region model fail draw good decis boundari use target attack see breakdown accuraci score easier natur fool target digit let tri", "prepar natur non natur fool target gradient travel guid show u occur fool target digit true digit highest count stand natur fool target wherea lowest correspond non natur fool target given heatmap could creat target argmin argmax per row true follow"]}, {"markdown": ["Ahhh! :-) We can clearly see that it was easier to fool the model with natural fooling targets.", "## Comparison to Fast Gradient Method <a class=\"anchor\" id=\"fastgradient\"></a>\n\n\nSo far we have used gradient ascent to maximize the probability of no-matches (non-targeted & targeted) and we derived the gradient with respect to inputs for objectives that are only slightly changed in comparison to the model defining likelihood function. This way we are very close to the Fast Gradient Method given in the cleverhans library:\n", "* Maximizing probability of no-matches:\n\n$$ x_{p, m} = x_{m} + \\delta x_{m} = x_{m} + \\eta \\cdot \\nabla_{x_{m}} D(f_{m}|y_{m}(x_{m},w)) $$\n\n* Fast Gradient Method:\n\n$$ x_{p,m} = x_{m} + \\delta x_{m} = x_{m} + \\eta \\cdot sign (\\nabla_{x_{m}} J(w,x,y)) $$", "Whereas $J(w,x,y)$ stands for the cost function used to train the model.  In many cases this is $E=- \\log (L(t|y(x,w))$, the negative log likelihood function. One might think that this should also be the case for this example and that we could have also simply maximize the cost funtion. As this would be the same as to minimize the log-likelihood, one might think:", "** ... instead of defining a discrepance function one could have also just minimize the likelihood function to yield the lowest probability of matches... **", "Ohoh! Is this true? Sounds so nice, but... ", "### Maximizing discrepance = minimizing likelihood?\n\nNormally we use the likelihood to maximize the probability and we want to count those outputs $y_{n,c}$ related to target-values of $t_{n,c}=1$. Look at one example image $m$ that belongs to class 2 of 3, then for this one we would have:\n\n$$ l = y_{1}^{0} * y_{2}^{1} * y_{3}^{0} = y_{2} $$\n\nHence this image contribute to the overall product over $n$ by a factor of $y_{2}$. Consequently each image $n$ would give us a y-factor of its true class whereas all the others only give us a factor of 1 such that it does not disturb our nice probability. If we would now use the same likelihood to represent the probability of no-matches we encounter a problem: To minimize the likelihood the only thing that can be done now is to set $y_{2}=0$ for image m. We would do this for all outputs that enter the likelihood by themselves. But what is with all the others? What to do with $y_{1}$ and $y_{3}$ for image m? Looking for a minimum of $y_{2}$ we could select anything for $y_{1}$ and $y_{3}$ as they just stay 1. That's ill-defined in contrast to our discrepance probability. There the situation for image m would look like this:\n\n$$ l = y_{1}^{1} * y_{2}^{0} * y_{3}^{1} = y_{1} * y_{3} $$\n\nAnd also our targeted-attack looks well for image m, if we define class 1 as our fooling class:\n\n$$ l = y_{1}^{1} * y_{2}^{0} * y_{3}^{0} = y_{1} $$\n\nWe have made sure that only those output-values enter the likelihood that are not related to the true target and we made sure that they carry their values and not just a value of 1. ", "## Comparison to \"discrepance\" Gradient Sign Method", "Some small experiment: Let's only take the sign of the gradient of our attack. I think we will not be as goog as we could be with the full gradient:"], "code": "# Reference: https://www.kaggle.com/code/allunia/how-to-attack-a-machine-learning-model\n\nattack.create_one_hot_targets(y_test)\nattack.attack_to_max_epsilon(non_targeted_sign_gradient, 30)\nplt.figure(figsize=(10,5))\ngm, = plt.plot(attack.epsilons, non_targeted_scores, 'g*', label='gradient method')\ngsm, = plt.plot(attack.epsilons, attack.scores, 'r*', label='gradient sign method')\nplt.ylabel('accuracy_score')\nplt.xlabel('eta')\nplt.legend(handles=[gm, gsm])\nplt.title('Accuracy score breakdown')", "processed": ["ahhh clearli see easier fool model natur fool target", "comparison fast gradient method class anchor id fastgradi far use gradient ascent maxim probabl match non target target deriv gradient respect input object slightli chang comparison model defin likelihood function way close fast gradient method given cleverhan librari", "maxim probabl match x p x delta x x eta cdot abla x f x w fast gradient method x p x delta x x eta cdot sign abla x j w x", "wherea j w x stand cost function use train model mani case e log l x w neg log likelihood function one might think also case exampl could also simpli maxim cost funtion would minim log likelihood one might think", "instead defin discrep function one could also minim likelihood function yield lowest probabl match", "ohoh true sound nice", "maxim discrep minim likelihood normal use likelihood maxim probabl want count output n c relat target valu n c 1 look one exampl imag belong class 2 3 one would l 1 0 2 1 3 0 2 henc imag contribut overal product n factor 2 consequ imag n would give u factor true class wherea other give u factor 1 disturb nice probabl would use likelihood repres probabl match encount problem minim likelihood thing done set 2 0 imag would output enter likelihood other 1 3 imag look minimum 2 could select anyth 1 3 stay 1 ill defin contrast discrep probabl situat imag would look like l 1 1 2 0 3 1 1 3 also target attack look well imag defin class 1 fool class l 1 1 2 0 3 0 1 made sure output valu enter likelihood relat true target made sure carri valu valu 1", "comparison discrep gradient sign method", "small experi let take sign gradient attack think goog could full gradient"]}, {"markdown": ["Did it, let's start ploting some graphs to try understand the distribuitions. '", "<h2>Starting by % of downloaded over the rest of data </h2>"], "code": "# Reference: https://www.kaggle.com/code/kabure/eda-feat-engineering-with-xgboosting\n\nprint(\"The proportion of downloaded over just click: \")\nprint(round((df_talk.is_attributed.value_counts() / len(df_talk.is_attributed) * 100),2))\nprint(\" \")\nprint(\"Downloaded over just clicks description: \")\nprint(df_talk.is_attributed.value_counts())\n\nplt.figure(figsize=(8, 5))\nsns.set(font_scale=1.2)\nmean = (df_talk.is_attributed.values == 1).mean()\n\nax = sns.barplot(['Fraudulent (1)', 'Not Fradulent (0)'], [mean, 1-mean])\nax.set_xlabel('Target Value', fontsize=15) \nax.set_ylabel('Probability', fontsize=15)\nax.set_title('Target value distribution', fontsize=20)\n\nfor p, uniq in zip(ax.patches, [mean, 1-mean]):\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()/2.,\n            height+0.01,\n            '{}%'.format(round(uniq * 100, 2)),\n            ha=\"center\") ", "processed": ["let start plote graph tri understand distribuit", "h2 start download rest data h2"]}, {"markdown": ["We can see a very unbalanced dataset. Sample of 1 million and we have just 1693 or .17% of target to train the model. But, it's very normal when we are working with fraud datasets. <br>\n\nLet's try take a best understand of this distribuition using another features. ", "<h2>Most Frequent IPs on dataset</h2>"], "code": "# Reference: https://www.kaggle.com/code/kabure/eda-feat-engineering-with-xgboosting\n\nip_frequency_downloaded = df_talk[df_talk['is_attributed'] == 1]['ip'].value_counts()[:20]\nip_frequency_click = df_talk[df_talk['is_attributed'] == 0]['ip'].value_counts()[:20]\n\nplt.figure(figsize=(16,10))\nplt.subplot(2,1,1)\ng = sns.barplot(ip_frequency_downloaded.index, ip_frequency_downloaded.values, color='blue')\ng.set_title(\"TOP 20 IP's where the click come from was downloaded\",fontsize=20)\ng.set_xlabel('Most frequents IPs',fontsize=16)\ng.set_ylabel('Count',fontsize=16)\n\nplt.subplot(2,1,2)\ng1 = sns.barplot(ip_frequency_click.index, ip_frequency_click.values, color='blue')\ng1.set_title(\"TOP 20 IP's where the click come from was NOT downloaded\",fontsize=20)\ng1.set_xlabel('Most frequents IPs',fontsize=16)\ng1.set_ylabel('Count',fontsize=16)\n\nplt.subplots_adjust(wspace = 0.1, hspace = 0.4,top = 0.9)\n\nplt.show()", "processed": ["see unbalanc dataset sampl 1 million 1693 17 target train model normal work fraud dataset br let tri take best understand distribuit use anoth featur", "h2 frequent ip dataset h2"]}, {"markdown": ["We can see that 2 ip's have a almost 2 times the others ip's clicks, but it isn't very significant in the download rate, with just 9 downloads in total", "<h2>Taking a look on App feature</h2>"], "code": "# Reference: https://www.kaggle.com/code/kabure/eda-feat-engineering-with-xgboosting\n\napp_frequency_downloaded = df_talk[df_talk['is_attributed'] == 1]['app'].value_counts()[:20]\napp_frequency_click = df_talk[df_talk['is_attributed'] == 0]['app'].value_counts()[:20]\n\nplt.figure(figsize=(16,10))\nplt.subplot(2,1,1)\ng = sns.barplot(app_frequency_downloaded.index, app_frequency_downloaded.values,\n                palette='husl')\ng.set_title(\"TOP 20 APP where the click come from and downloaded\",fontsize=20)\ng.set_xlabel('Most frequents APP ID',fontsize=16)\ng.set_ylabel('Count',fontsize=16)\n\nplt.subplot(2,1,2)\ng1 = sns.barplot(app_frequency_click.index, app_frequency_click.values,\n                palette='husl')\ng1.set_title(\"TOP 20 APP where the click come from NOT downloaded\",fontsize=20)\ng1.set_xlabel('Most frequents APP ID',fontsize=16)\ng1.set_ylabel('Count',fontsize=16)\n\nplt.subplots_adjust(wspace = 0.1, hspace = 0.4,top = 0.9)\n\nplt.show()", "processed": ["see 2 ip almost 2 time other ip click signific download rate 9 download total", "h2 take look app featur h2"]}, {"markdown": ["\n\nWith first 5 highest values we have 64% of downloads total. ", "<h2>Channel feature </h2>\n- <i> channel is of  id of mobile ad publisher"], "code": "# Reference: https://www.kaggle.com/code/kabure/eda-feat-engineering-with-xgboosting\n\nchannel_frequency_downloaded = df_talk[df_talk['is_attributed'] == 1]['channel'].value_counts()[:20]\nchannel_frequency_click = df_talk[df_talk['is_attributed'] == 0]['channel'].value_counts()[:20]\n\nplt.figure(figsize=(16,10))\n\nplt.subplot(2,1,1)\ng = sns.barplot(channel_frequency_downloaded.index, channel_frequency_downloaded.values, \\\n                palette='husl')\ng.set_title(\"TOP 20 channels with download Count\",fontsize=20)\ng.set_xlabel('Most frequents Channels ID',fontsize=16)\ng.set_ylabel('Count',fontsize=16)\n\nplt.subplot(2,1,2)\ng1 = sns.barplot(channel_frequency_click.index, channel_frequency_click.values,\\\n                 palette='husl')\ng1.set_title(\"TOP 20 channels clicks Count\",fontsize=20)\ng1.set_xlabel('Most frequents Channels ID',fontsize=16)\ng1.set_ylabel('Count',fontsize=16)\n\nplt.subplots_adjust(wspace = 0.1, hspace = 0.4,top = 0.9)\n\nplt.show()", "processed": ["first 5 highest valu 64 download total", "h2 channel featur h2 channel id mobil ad publish"]}, {"markdown": ["The top five highest channels corresponds to 59% of total downloads registereds in this sample. ", "<h2>Device Feature  </h2>"], "code": "# Reference: https://www.kaggle.com/code/kabure/eda-feat-engineering-with-xgboosting\n\ndevice_frequency_downloaded = df_talk[df_talk['is_attributed'] == 1]['device'].value_counts()[:20]\ndevice_frequency_click = df_talk[df_talk['is_attributed'] == 0]['device'].value_counts()[:20]\n\nplt.figure(figsize=(16,10))\nplt.subplot(2,1,1)\ng = sns.barplot(device_frequency_downloaded.index, device_frequency_downloaded.values,\n                palette='husl')\ng.set_title(\"TOP 20 devices with download - Count\",fontsize=20)\ng.set_xlabel('Most frequents Devices ID',fontsize=16)\ng.set_ylabel('Count',fontsize=16)\n\nplt.subplot(2,1,2)\ng1 = sns.barplot(device_frequency_click.index, device_frequency_click.values,\n                palette='husl')\ng1.set_title(\"TOP 20 devices with download - Count\",fontsize=20)\ng1.set_xlabel('Most frequents Devices ID',fontsize=16)\ng1.set_ylabel('Count',fontsize=16)\n\nplt.subplots_adjust(wspace = 0.1, hspace = 0.4,top = 0.9)\n\nplt.show()", "processed": ["top five highest channel correspond 59 total download regist sampl", "h2 devic featur h2"]}, {"markdown": ["The top 5 corresponds to 89% of our sample, but with significant values in just two variable... What corresponds to this values? I'm am very interested to understand. Why just two? ", "<h2>Operational System version (os) Feature</h2>"], "code": "# Reference: https://www.kaggle.com/code/kabure/eda-feat-engineering-with-xgboosting\n\nos_frequency_downloaded = df_talk[df_talk['is_attributed'] == 1]['os'].value_counts()[:20]\nos_frequency_click = df_talk[df_talk['is_attributed'] == 0]['os'].value_counts()[:20]\n\nplt.figure(figsize=(16,10))\nplt.subplot(2,1,1)\ng = sns.barplot(os_frequency_downloaded.index, os_frequency_downloaded.values,\n                palette='husl')\ng.set_title(\"TOP 20 OS with download - Count\",fontsize=20)\ng.set_xlabel(\"Most frequents OS's ID\",fontsize=16)\ng.set_ylabel('Count',fontsize=16)\n\nplt.subplot(2,1,2)\ng1 = sns.barplot(os_frequency_downloaded.index, os_frequency_downloaded.values,\n                palette='husl')\ng1.set_title(\"TOP 20 OS with download - Count\",fontsize=20)\ng1.set_xlabel(\"Most frequents OS's ID\",fontsize=16)\ng1.set_ylabel('Count',fontsize=16)\n\nplt.subplots_adjust(wspace = 0.1, hspace = 0.4,top = 0.9)\n\nplt.show()\nprint(\"Device percentual distribuition: \")\nprint(round(df_talk[df_talk['is_attributed'] == 1]['os'].value_counts()[:5] \\\n            / len(df_talk[df_talk['is_attributed'] == 1]) * 100),2)", "processed": ["top 5 correspond 89 sampl signific valu two variabl correspond valu interest understand two", "h2 oper system version o featur h2"]}, {"markdown": ["The first 5 highest values in this sample represents 55% of total downloads", "<h2>Let's take a look at our new features extracteds by time</h2>", "Visualizing the value's in hour column"], "code": "# Reference: https://www.kaggle.com/code/kabure/eda-feat-engineering-with-xgboosting\n\nhour_frequency_downloaded = df_talk[df_talk['is_attributed'] == 1]['hour'].value_counts()\nhour_frequency_click = df_talk[df_talk['is_attributed'] == 0]['hour'].value_counts()\n\nplt.figure(figsize=(16,10))\nplt.subplot(2,1,1)\ng = sns.barplot(hour_frequency_downloaded.index, hour_frequency_downloaded.values,\n                palette='husl')\ng.set_title(\"Downloads Count by Hour\",fontsize=20)\ng.set_xlabel(\"Hour Download distribuition\",fontsize=16)\ng.set_ylabel('Count',fontsize=16)\n\nplt.subplot(2,1,2)\ng1 = sns.barplot(hour_frequency_click.index, hour_frequency_click.values,\n                palette='husl')\ng1.set_title(\"Clicks Count by Hour\",fontsize=20)\ng1.set_xlabel(\"Hour Click distribuition\",fontsize=16)\ng1.set_ylabel('Count',fontsize=16)\n\nplt.subplots_adjust(wspace = 0.1, hspace = 0.4,top = 0.9)\n\nplt.show()", "processed": ["first 5 highest valu sampl repres 55 total download", "h2 let take look new featur extract time h2", "visual valu hour column"]}, {"markdown": ["<h2> Visualizing the minute column</h2> "], "code": "# Reference: https://www.kaggle.com/code/kabure/eda-feat-engineering-with-xgboosting\n\nminute_frequency_downloaded = df_talk[df_talk['is_attributed'] == 1]['minute'].value_counts()\nminute_frequency_click = df_talk[df_talk['is_attributed'] == 0]['minute'].value_counts()\n\nplt.figure(figsize=(16,10))\nplt.subplot(2,1,1)\ng = sns.barplot(minute_frequency_downloaded.index, minute_frequency_downloaded.values,\n                palette='husl')\ng.set_title(\"Downloads Count by Minute\",fontsize=20)\ng.set_xlabel(\"Minute Download distribuition\",fontsize=16)\ng.set_ylabel('Count',fontsize=16)\n\nplt.subplot(2,1,2)\ng1 = sns.barplot(minute_frequency_click.index, minute_frequency_click.values,\n                palette='husl')\ng1.set_title(\"Clicks Count by Minute\",fontsize=20)\ng1.set_xlabel(\"Minute Click distribuition\",fontsize=16)\ng1.set_ylabel('Count',fontsize=16)\n\nplt.subplots_adjust(wspace = 0.1, hspace = 0.4,top = 0.9)\n\nplt.show()", "processed": ["h2 visual minut column h2"]}, {"markdown": ["Interesting that in the first minutes of an hour the rate of downloads is higher. We can see this in the both filters\n", "<h2>Visualizing the Second distribuition.</h2>"], "code": "# Reference: https://www.kaggle.com/code/kabure/eda-feat-engineering-with-xgboosting\n\nsecond_frequency_downloaded = df_talk[df_talk['is_attributed'] == 1]['second'].value_counts()\nsecond_frequency_click = df_talk[df_talk['is_attributed'] == 0]['second'].value_counts()\n\nplt.figure(figsize=(16,10))\nplt.subplot(2,1,1)\ng = sns.barplot(second_frequency_downloaded.index, second_frequency_downloaded.values,\n                palette='husl')\ng.set_title(\"Downloads Count by Hour\",fontsize=20)\ng.set_xlabel(\"Second Download distribuition\",fontsize=16)\ng.set_ylabel('Count',fontsize=16)\n\nplt.subplot(2,1,2)\ng1 = sns.barplot(second_frequency_click.index, second_frequency_click.values,\n                palette='husl')\ng1.set_title(\"Clicks Count by Hour\",fontsize=20)\ng1.set_xlabel(\"Second Click distribuition\",fontsize=16)\ng1.set_ylabel('Count',fontsize=16)\n\nplt.subplots_adjust(wspace = 0.1, hspace = 0.4,top = 0.9)\n\nplt.show()", "processed": ["interest first minut hour rate download higher see filter", "h2 visual second distribuit h2"]}, {"markdown": ["## Verifying feature importances"], "code": "# Reference: https://www.kaggle.com/code/kabure/eda-feat-engineering-with-xgboosting\n\nfrom sklearn import preprocessing\n\n# Get xgBoost importances\nfeature_importance = {}\nfor import_type in ['weight', 'gain', 'cover']:\n    feature_importance['xgBoost-'+import_type] = clf_xgBoost.get_booster().get_score(importance_type=import_type)\n    \n# MinMax scale all importances\nfeatures = pd.DataFrame(feature_importance).fillna(0)\nfeatures = pd.DataFrame(\n    preprocessing.MinMaxScaler().fit_transform(features),\n    columns=features.columns,\n    index=features.index\n)\n\n# Create mean column\nfeatures['mean'] = features.mean(axis=1)\n\n# Plot the feature importances\nfeatures.sort_values('mean').plot(kind='bar', figsize=(16, 6))\nplt.show()", "processed": ["verifi featur import"]}, {"markdown": ["# Target Distribution"], "code": "# Reference: https://www.kaggle.com/code/kabure/extensive-eda-and-modeling-xgb-hyperopt\n\ndf_trans['TransactionAmt'] = df_trans['TransactionAmt'].astype(float)\ntotal = len(df_trans)\ntotal_amt = df_trans.groupby(['isFraud'])['TransactionAmt'].sum().sum()\nplt.figure(figsize=(16,6))\n\nplt.subplot(121)\ng = sns.countplot(x='isFraud', data=df_trans, )\ng.set_title(\"Fraud Transactions Distribution \\n# 0: No Fraud | 1: Fraud #\", fontsize=22)\ng.set_xlabel(\"Is fraud?\", fontsize=18)\ng.set_ylabel('Count', fontsize=18)\nfor p in g.patches:\n    height = p.get_height()\n    g.text(p.get_x()+p.get_width()/2.,\n            height + 3,\n            '{:1.2f}%'.format(height/total*100),\n            ha=\"center\", fontsize=15) \n\nperc_amt = (df_trans.groupby(['isFraud'])['TransactionAmt'].sum())\nperc_amt = perc_amt.reset_index()\nplt.subplot(122)\ng1 = sns.barplot(x='isFraud', y='TransactionAmt',  dodge=True, data=perc_amt)\ng1.set_title(\"% Total Amount in Transaction Amt \\n# 0: No Fraud | 1: Fraud #\", fontsize=22)\ng1.set_xlabel(\"Is fraud?\", fontsize=18)\ng1.set_ylabel('Total Transaction Amount Scalar', fontsize=18)\nfor p in g1.patches:\n    height = p.get_height()\n    g1.text(p.get_x()+p.get_width()/2.,\n            height + 3,\n            '{:1.2f}%'.format(height/total_amt * 100),\n            ha=\"center\", fontsize=15) \n    \nplt.show()", "processed": ["target distribut"]}, {"markdown": ["# Ploting Transaction Amount Values Distribution"], "code": "# Reference: https://www.kaggle.com/code/kabure/extensive-eda-and-modeling-xgb-hyperopt\n\nplt.figure(figsize=(16,12))\nplt.suptitle('Transaction Values Distribution', fontsize=22)\nplt.subplot(221)\ng = sns.distplot(df_trans[df_trans['TransactionAmt'] <= 1000]['TransactionAmt'])\ng.set_title(\"Transaction Amount Distribuition <= 1000\", fontsize=18)\ng.set_xlabel(\"\")\ng.set_ylabel(\"Probability\", fontsize=15)\n\nplt.subplot(222)\ng1 = sns.distplot(np.log(df_trans['TransactionAmt']))\ng1.set_title(\"Transaction Amount (Log) Distribuition\", fontsize=18)\ng1.set_xlabel(\"\")\ng1.set_ylabel(\"Probability\", fontsize=15)\n\nplt.figure(figsize=(16,12))\n\n\nplt.subplot(212)\ng4 = plt.scatter(range(df_trans[df_trans['isFraud'] == 0].shape[0]),\n                 np.sort(df_trans[df_trans['isFraud'] == 0]['TransactionAmt'].values), \n                 label='NoFraud', alpha=.2)\ng4 = plt.scatter(range(df_trans[df_trans['isFraud'] == 1].shape[0]),\n                 np.sort(df_trans[df_trans['isFraud'] == 1]['TransactionAmt'].values), \n                 label='Fraud', alpha=.2)\ng4= plt.title(\"ECDF \\nFRAUD and NO FRAUD Transaction Amount Distribution\", fontsize=18)\ng4 = plt.xlabel(\"Index\")\ng4 = plt.ylabel(\"Amount Distribution\", fontsize=15)\ng4 = plt.legend()\n\nplt.figure(figsize=(16,12))\n\nplt.subplot(321)\ng = plt.scatter(range(df_trans[df_trans['isFraud'] == 1].shape[0]), \n                 np.sort(df_trans[df_trans['isFraud'] == 1]['TransactionAmt'].values), \n                label='isFraud', alpha=.4)\nplt.title(\"FRAUD - Transaction Amount ECDF\", fontsize=18)\nplt.xlabel(\"Index\")\nplt.ylabel(\"Amount Distribution\", fontsize=12)\n\nplt.subplot(322)\ng1 = plt.scatter(range(df_trans[df_trans['isFraud'] == 0].shape[0]),\n                 np.sort(df_trans[df_trans['isFraud'] == 0]['TransactionAmt'].values), \n                 label='NoFraud', alpha=.2)\ng1= plt.title(\"NO FRAUD - Transaction Amount ECDF\", fontsize=18)\ng1 = plt.xlabel(\"Index\")\ng1 = plt.ylabel(\"Amount Distribution\", fontsize=15)\n\nplt.suptitle('Individual ECDF Distribution', fontsize=22)\n\nplt.show()", "processed": ["plote transact amount valu distribut"]}, {"markdown": ["If we consider only values between >= 0 to 800 we will avoid the outliers and has more confidence in our distribution. <br>\nWe have 10k rows with outliers that represents 1.74% of total rows.", "# Now, let's known the Product Feature\n- Distribution Products\n- Distribution of Frauds by Product\n- Has Difference between Transaction Amounts in Products? "], "code": "# Reference: https://www.kaggle.com/code/kabure/extensive-eda-and-modeling-xgb-hyperopt\n\ntmp = pd.crosstab(df_trans['ProductCD'], df_trans['isFraud'], normalize='index') * 100\ntmp = tmp.reset_index()\ntmp.rename(columns={0:'NoFraud', 1:'Fraud'}, inplace=True)\n\nplt.figure(figsize=(14,10))\nplt.suptitle('ProductCD Distributions', fontsize=22)\n\nplt.subplot(221)\ng = sns.countplot(x='ProductCD', data=df_trans)\n# plt.legend(title='Fraud', loc='upper center', labels=['No', 'Yes'])\n\ng.set_title(\"ProductCD Distribution\", fontsize=19)\ng.set_xlabel(\"ProductCD Name\", fontsize=17)\ng.set_ylabel(\"Count\", fontsize=17)\ng.set_ylim(0,500000)\nfor p in g.patches:\n    height = p.get_height()\n    g.text(p.get_x()+p.get_width()/2.,\n            height + 3,\n            '{:1.2f}%'.format(height/total*100),\n            ha=\"center\", fontsize=14) \n\nplt.subplot(222)\ng1 = sns.countplot(x='ProductCD', hue='isFraud', data=df_trans)\nplt.legend(title='Fraud', loc='best', labels=['No', 'Yes'])\ngt = g1.twinx()\ngt = sns.pointplot(x='ProductCD', y='Fraud', data=tmp, color='black', order=['W', 'H',\"C\", \"S\", \"R\"], legend=False)\ngt.set_ylabel(\"% of Fraud Transactions\", fontsize=16)\n\ng1.set_title(\"Product CD by Target(isFraud)\", fontsize=19)\ng1.set_xlabel(\"ProductCD Name\", fontsize=17)\ng1.set_ylabel(\"Count\", fontsize=17)\n\nplt.subplot(212)\ng3 = sns.boxenplot(x='ProductCD', y='TransactionAmt', hue='isFraud', \n              data=df_trans[df_trans['TransactionAmt'] <= 2000] )\ng3.set_title(\"Transaction Amount Distribuition by ProductCD and Target\", fontsize=20)\ng3.set_xlabel(\"ProductCD Name\", fontsize=17)\ng3.set_ylabel(\"Transaction Values\", fontsize=17)\n\nplt.subplots_adjust(hspace = 0.6, top = 0.85)\n\nplt.show()", "processed": ["consid valu 0 800 avoid outlier confid distribut br 10k row outlier repres 1 74 total row", "let known product featur distribut product distribut fraud product differ transact amount product"]}, {"markdown": ["# Visualizing Card 1, Card 2 and Card 3 Distributions\n- As the Card 1 and 2 are numericals, I will plot the distribution of them\n- in Card 3, as we have many values with low frequencies, I decided to set value to \"Others\" \n- Also, in Card 3 I set the % of Fraud ratio in yaxis2"], "code": "# Reference: https://www.kaggle.com/code/kabure/extensive-eda-and-modeling-xgb-hyperopt\n\ntmp = pd.crosstab(df_trans['card3'], df_trans['isFraud'], normalize='index') * 100\ntmp = tmp.reset_index()\ntmp.rename(columns={0:'NoFraud', 1:'Fraud'}, inplace=True)\n\ntmp2 = pd.crosstab(df_trans['card5'], df_trans['isFraud'], normalize='index') * 100\ntmp2 = tmp2.reset_index()\ntmp2.rename(columns={0:'NoFraud', 1:'Fraud'}, inplace=True)\n\nplt.figure(figsize=(14,22))\n\nplt.subplot(411)\ng = sns.distplot(df_trans[df_trans['isFraud'] == 1]['card1'], label='Fraud')\ng = sns.distplot(df_trans[df_trans['isFraud'] == 0]['card1'], label='NoFraud')\ng.legend()\ng.set_title(\"Card 1 Values Distribution by Target\", fontsize=20)\ng.set_xlabel(\"Card 1 Values\", fontsize=18)\ng.set_ylabel(\"Probability\", fontsize=18)\n\nplt.subplot(412)\ng1 = sns.distplot(df_trans[df_trans['isFraud'] == 1]['card2'].dropna(), label='Fraud')\ng1 = sns.distplot(df_trans[df_trans['isFraud'] == 0]['card2'].dropna(), label='NoFraud')\ng1.legend()\ng1.set_title(\"Card 2 Values Distribution by Target\", fontsize=20)\ng1.set_xlabel(\"Card 2 Values\", fontsize=18)\ng1.set_ylabel(\"Probability\", fontsize=18)\n\nplt.subplot(413)\ng2 = sns.countplot(x='card3', data=df_trans, order=list(tmp.card3.values))\ng22 = g2.twinx()\ngg2 = sns.pointplot(x='card3', y='Fraud', data=tmp, \n                    color='black', order=list(tmp.card3.values))\ngg2.set_ylabel(\"% of Fraud Transactions\", fontsize=16)\ng2.set_title(\"Card 3 Values Distribution and % of Transaction Frauds\", fontsize=20)\ng2.set_xlabel(\"Card 3 Values\", fontsize=18)\ng2.set_ylabel(\"Count\", fontsize=18)\nfor p in g2.patches:\n    height = p.get_height()\n    g2.text(p.get_x()+p.get_width()/2.,\n            height + 25,\n            '{:1.2f}%'.format(height/total*100),\n            ha=\"center\") \n\nplt.subplot(414)\ng3 = sns.countplot(x='card5', data=df_trans, order=list(tmp2.card5.values))\ng3t = g3.twinx()\ng3t = sns.pointplot(x='card5', y='Fraud', data=tmp2, \n                    color='black', order=list(tmp2.card5.values))\ng3t.set_ylabel(\"% of Fraud Transactions\", fontsize=16)\ng3.set_title(\"Card 5 Values Distribution and % of Transaction Frauds\", fontsize=20)\ng3.set_xticklabels(g3.get_xticklabels(),rotation=90)\ng3.set_xlabel(\"Card 5 Values\", fontsize=18)\ng3.set_ylabel(\"Count\", fontsize=18)\nfor p in g3.patches:\n    height = p.get_height()\n    g3.text(p.get_x()+p.get_width()/2.,\n            height + 3,\n            '{:1.2f}%'.format(height/total*100),\n            ha=\"center\",fontsize=11) \n    \nplt.subplots_adjust(hspace = 0.6, top = 0.85)\n\n\nplt.show()", "processed": ["visual card 1 card 2 card 3 distribut card 1 2 numer plot distribut card 3 mani valu low frequenc decid set valu other also card 3 set fraud ratio yaxis2"]}, {"markdown": ["Cool and Very Meaningful information. <br>\nIn Card3 we can see that 100 and 106 are the most common values in the column. <br>\nWe have 4.95% of Frauds in 100 and 1.52% in 106; The values with highest Fraud Transactions are 185, 119 and 119; <br>\n\nIn card5 the most frequent values are 226, 224, 166 that represents 73% of data. Also is posible to see high % of frauds in 137, 147, 141 that has few entries for values.", "# Card 4 - Categorical"], "code": "# Reference: https://www.kaggle.com/code/kabure/extensive-eda-and-modeling-xgb-hyperopt\n\ntmp = pd.crosstab(df_trans['card4'], df_trans['isFraud'], normalize='index') * 100\ntmp = tmp.reset_index()\ntmp.rename(columns={0:'NoFraud', 1:'Fraud'}, inplace=True)\n\nplt.figure(figsize=(14,10))\nplt.suptitle('Card 4 Distributions', fontsize=22)\n\nplt.subplot(221)\ng = sns.countplot(x='card4', data=df_trans)\n# plt.legend(title='Fraud', loc='upper center', labels=['No', 'Yes'])\ng.set_title(\"Card4 Distribution\", fontsize=19)\ng.set_ylim(0,420000)\ng.set_xlabel(\"Card4 Category Names\", fontsize=17)\ng.set_ylabel(\"Count\", fontsize=17)\nfor p in g.patches:\n    height = p.get_height()\n    g.text(p.get_x()+p.get_width()/2.,\n            height + 3,\n            '{:1.2f}%'.format(height/total*100),\n            ha=\"center\",fontsize=14) \n\n\nplt.subplot(222)\ng1 = sns.countplot(x='card4', hue='isFraud', data=df_trans)\nplt.legend(title='Fraud', loc='best', labels=['No', 'Yes'])\ngt = g1.twinx()\ngt = sns.pointplot(x='card4', y='Fraud', data=tmp, \n                   color='black', legend=False, \n                   order=['discover', 'mastercard', 'visa', 'american express'])\ngt.set_ylabel(\"% of Fraud Transactions\", fontsize=16)\ng1.set_title(\"Card4 by Target(isFraud)\", fontsize=19)\ng1.set_xlabel(\"Card4 Category Names\", fontsize=17)\ng1.set_ylabel(\"Count\", fontsize=17)\n\nplt.subplot(212)\ng3 = sns.boxenplot(x='card4', y='TransactionAmt', hue='isFraud', \n              data=df_trans[df_trans['TransactionAmt'] <= 2000] )\ng3.set_title(\"Card 4 Distribuition by ProductCD and Target\", fontsize=20)\ng3.set_xlabel(\"Card4 Category Names\", fontsize=17)\ng3.set_ylabel(\"Transaction Values\", fontsize=17)\n\nplt.subplots_adjust(hspace = 0.6, top = 0.85)\n\nplt.show()", "processed": ["cool meaning inform br card3 see 100 106 common valu column br 4 95 fraud 100 1 52 106 valu highest fraud transact 185 119 119 br card5 frequent valu 226 224 166 repres 73 data also posibl see high fraud 137 147 141 entri valu", "card 4 categor"]}, {"markdown": ["We can see that 97% of our data are in Mastercard(32%) and Visa(65%);  <br>\nwe have a highest value in discover(~8%) against ~3.5% of Mastercard and Visa and 2.87% in American Express", "# Card 6 - Categorical"], "code": "# Reference: https://www.kaggle.com/code/kabure/extensive-eda-and-modeling-xgb-hyperopt\n\ntmp = pd.crosstab(df_trans['card6'], df_trans['isFraud'], normalize='index') * 100\ntmp = tmp.reset_index()\ntmp.rename(columns={0:'NoFraud', 1:'Fraud'}, inplace=True)\n\nplt.figure(figsize=(14,10))\nplt.suptitle('Card 6 Distributions', fontsize=22)\n\nplt.subplot(221)\ng = sns.countplot(x='card6', data=df_trans, order=list(tmp.card6.values))\n# plt.legend(title='Fraud', loc='upper center', labels=['No', 'Yes'])\ng.set_title(\"Card6 Distribution\", fontsize=19)\ng.set_ylim(0,480000)\ng.set_xlabel(\"Card6 Category Names\", fontsize=17)\ng.set_ylabel(\"Count\", fontsize=17)\nfor p in g.patches:\n    height = p.get_height()\n    g.text(p.get_x()+p.get_width()/2.,\n            height + 3,\n            '{:1.2f}%'.format(height/total*100),\n            ha=\"center\",fontsize=14) \n\nplt.subplot(222)\ng1 = sns.countplot(x='card6', hue='isFraud', data=df_trans, order=list(tmp.card6.values))\nplt.legend(title='Fraud', loc='best', labels=['No', 'Yes'])\ngt = g1.twinx()\ngt = sns.pointplot(x='card6', y='Fraud', data=tmp, order=list(tmp.card6.values),\n                   color='black', legend=False, )\ngt.set_ylim(0,20)\ngt.set_ylabel(\"% of Fraud Transactions\", fontsize=16)\ng1.set_title(\"Card6 by Target(isFraud)\", fontsize=19)\ng1.set_xlabel(\"Card6 Category Names\", fontsize=17)\ng1.set_ylabel(\"Count\", fontsize=17)\n\nplt.subplot(212)\ng3 = sns.boxenplot(x='card6', y='TransactionAmt', hue='isFraud', order=list(tmp.card6.values),\n              data=df_trans[df_trans['TransactionAmt'] <= 2000] )\ng3.set_title(\"Card 6 Distribuition by ProductCD and Target\", fontsize=20)\ng3.set_xlabel(\"Card6 Category Names\", fontsize=17)\ng3.set_ylabel(\"Transaction Values\", fontsize=17)\n\nplt.subplots_adjust(hspace = 0.6, top = 0.85)\n\nplt.show()", "processed": ["see 97 data mastercard 32 visa 65 br highest valu discov 8 3 5 mastercard visa 2 87 american express", "card 6 categor"]}, {"markdown": ["All data is on Credit and Debit. We can see a high percentual of Frauds in Credit than Debit transactions. <br>\nThe Distribution of Transaction Amount don't shows clear differences.", "# Exploring M1-M9 Features "], "code": "# Reference: https://www.kaggle.com/code/kabure/extensive-eda-and-modeling-xgb-hyperopt\n\nfor col in ['M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9']:\n    df_trans[col] = df_trans[col].fillna(\"Miss\")\n    \ndef ploting_dist_ratio(df, col, lim=2000):\n    tmp = pd.crosstab(df[col], df['isFraud'], normalize='index') * 100\n    tmp = tmp.reset_index()\n    tmp.rename(columns={0:'NoFraud', 1:'Fraud'}, inplace=True)\n\n    plt.figure(figsize=(20,5))\n    plt.suptitle(f'{col} Distributions ', fontsize=22)\n\n    plt.subplot(121)\n    g = sns.countplot(x=col, data=df, order=list(tmp[col].values))\n    # plt.legend(title='Fraud', loc='upper center', labels=['No', 'Yes'])\n    g.set_title(f\"{col} Distribution\\nCound and %Fraud by each category\", fontsize=18)\n    g.set_ylim(0,400000)\n    gt = g.twinx()\n    gt = sns.pointplot(x=col, y='Fraud', data=tmp, order=list(tmp[col].values),\n                       color='black', legend=False, )\n    gt.set_ylim(0,20)\n    gt.set_ylabel(\"% of Fraud Transactions\", fontsize=16)\n    g.set_xlabel(f\"{col} Category Names\", fontsize=16)\n    g.set_ylabel(\"Count\", fontsize=17)\n    for p in gt.patches:\n        height = p.get_height()\n        gt.text(p.get_x()+p.get_width()/2.,\n                height + 3,\n                '{:1.2f}%'.format(height/total*100),\n                ha=\"center\",fontsize=14) \n        \n    perc_amt = (df_trans.groupby(['isFraud',col])['TransactionAmt'].sum() / total_amt * 100).unstack('isFraud')\n    perc_amt = perc_amt.reset_index()\n    perc_amt.rename(columns={0:'NoFraud', 1:'Fraud'}, inplace=True)\n\n    plt.subplot(122)\n    g1 = sns.boxplot(x=col, y='TransactionAmt', hue='isFraud', \n                     data=df[df['TransactionAmt'] <= lim], order=list(tmp[col].values))\n    g1t = g1.twinx()\n    g1t = sns.pointplot(x=col, y='Fraud', data=perc_amt, order=list(tmp[col].values),\n                       color='black', legend=False, )\n    g1t.set_ylim(0,5)\n    g1t.set_ylabel(\"%Fraud Total Amount\", fontsize=16)\n    g1.set_title(f\"{col} by Transactions dist\", fontsize=18)\n    g1.set_xlabel(f\"{col} Category Names\", fontsize=16)\n    g1.set_ylabel(\"Transaction Amount(U$)\", fontsize=16)\n        \n    plt.subplots_adjust(hspace=.4, wspace = 0.35, top = 0.80)\n    \n    plt.show()\n", "processed": ["data credit debit see high percentu fraud credit debit transact br distribut transact amount show clear differ", "explor m1 m9 featur"]}, {"markdown": ["## Addr1 Distributions"], "code": "# Reference: https://www.kaggle.com/code/kabure/extensive-eda-and-modeling-xgb-hyperopt\n\n def ploting_cnt_amt(df, col, lim=2000):\n    tmp = pd.crosstab(df[col], df['isFraud'], normalize='index') * 100\n    tmp = tmp.reset_index()\n    tmp.rename(columns={0:'NoFraud', 1:'Fraud'}, inplace=True)\n    \n    plt.figure(figsize=(16,14))    \n    plt.suptitle(f'{col} Distributions ', fontsize=24)\n    \n    plt.subplot(211)\n    g = sns.countplot( x=col,  data=df, order=list(tmp[col].values))\n    gt = g.twinx()\n    gt = sns.pointplot(x=col, y='Fraud', data=tmp, order=list(tmp[col].values),\n                       color='black', legend=False, )\n    gt.set_ylim(0,tmp['Fraud'].max()*1.1)\n    gt.set_ylabel(\"%Fraud Transactions\", fontsize=16)\n    g.set_title(f\"Most Frequent {col} values and % Fraud Transactions\", fontsize=20)\n    g.set_xlabel(f\"{col} Category Names\", fontsize=16)\n    g.set_ylabel(\"Count\", fontsize=17)\n    g.set_xticklabels(g.get_xticklabels(),rotation=45)\n    sizes = []\n    for p in g.patches:\n        height = p.get_height()\n        sizes.append(height)\n        g.text(p.get_x()+p.get_width()/2.,\n                height + 3,\n                '{:1.2f}%'.format(height/total*100),\n                ha=\"center\",fontsize=12) \n        \n    g.set_ylim(0,max(sizes)*1.15)\n    \n    #########################################################################\n    perc_amt = (df.groupby(['isFraud',col])['TransactionAmt'].sum() \\\n                / df.groupby([col])['TransactionAmt'].sum() * 100).unstack('isFraud')\n    perc_amt = perc_amt.reset_index()\n    perc_amt.rename(columns={0:'NoFraud', 1:'Fraud'}, inplace=True)\n    amt = df.groupby([col])['TransactionAmt'].sum().reset_index()\n    perc_amt = perc_amt.fillna(0)\n    plt.subplot(212)\n    g1 = sns.barplot(x=col, y='TransactionAmt', \n                       data=amt, \n                       order=list(tmp[col].values))\n    g1t = g1.twinx()\n    g1t = sns.pointplot(x=col, y='Fraud', data=perc_amt, \n                        order=list(tmp[col].values),\n                       color='black', legend=False, )\n    g1t.set_ylim(0,perc_amt['Fraud'].max()*1.1)\n    g1t.set_ylabel(\"%Fraud Total Amount\", fontsize=16)\n    g.set_xticklabels(g.get_xticklabels(),rotation=45)\n    g1.set_title(f\"{col} by Transactions Total + %of total and %Fraud Transactions\", fontsize=20)\n    g1.set_xlabel(f\"{col} Category Names\", fontsize=16)\n    g1.set_ylabel(\"Transaction Total Amount(U$)\", fontsize=16)\n    g1.set_xticklabels(g.get_xticklabels(),rotation=45)    \n    \n    for p in g1.patches:\n        height = p.get_height()\n        g1.text(p.get_x()+p.get_width()/2.,\n                height + 3,\n                '{:1.2f}%'.format(height/total_amt*100),\n                ha=\"center\",fontsize=12) \n        \n    plt.subplots_adjust(hspace=.4, top = 0.9)\n    plt.show()\n    \nploting_cnt_amt(df_trans, 'addr1')", "processed": ["addr1 distribut"]}, {"markdown": ["", "## Transactions and Total Amount by each day"], "code": "# Reference: https://www.kaggle.com/code/kabure/extensive-eda-and-modeling-xgb-hyperopt\n\n# Calling the function to transform the date column in datetime pandas object\n\n#seting some static color options\ncolor_op = ['#5527A0', '#BB93D7', '#834CF7', '#6C941E', '#93EAEA', '#7425FF', '#F2098A', '#7E87AC', \n            '#EBE36F', '#7FD394', '#49C35D', '#3058EE', '#44FDCF', '#A38F85', '#C4CEE0', '#B63A05', \n            '#4856BF', '#F0DB1B', '#9FDBD9', '#B123AC']\n\n\ndates_temp = df_trans.groupby(df_trans.Date.dt.date)['TransactionAmt'].count().reset_index()\n# renaming the columns to apropriate names\n\n# creating the first trace with the necessary parameters\ntrace = go.Scatter(x=dates_temp['Date'], y=dates_temp.TransactionAmt,\n                    opacity = 0.8, line = dict(color = color_op[7]), name= 'Total Transactions')\n\n# Below we will get the total amount sold\ndates_temp_sum = df_trans.groupby(df_trans.Date.dt.date)['TransactionAmt'].sum().reset_index()\n\n# using the new dates_temp_sum we will create the second trace\ntrace1 = go.Scatter(x=dates_temp_sum.Date, line = dict(color = color_op[1]), name=\"Total Amount\",\n                        y=dates_temp_sum['TransactionAmt'], opacity = 0.8, yaxis='y2')\n\n#creating the layout the will allow us to give an title and \n# give us some interesting options to handle with the outputs of graphs\nlayout = dict(\n    title= \"Total Transactions and Fraud Informations by Date\",\n    xaxis=dict(\n        rangeselector=dict(\n            buttons=list([\n                dict(count=1, label='1m', step='month', stepmode='backward'),\n                dict(count=3, label='3m', step='month', stepmode='backward'),\n                dict(count=6, label='6m', step='month', stepmode='backward'),\n                dict(step='all')\n            ])\n        ),\n        rangeslider=dict(visible = True),\n        type='date' ),\n    yaxis=dict(title='Total Transactions'),\n    yaxis2=dict(overlaying='y',\n                anchor='x', side='right',\n                zeroline=False, showgrid=False,\n                title='Total Transaction Amount')\n)\n\n# creating figure with the both traces and layout\nfig = dict(data= [trace, trace1,], layout=layout)\n\n#rendering the graphs\niplot(fig) #it's an equivalent to plt.show()", "processed": ["", "transact total amount day"]}, {"markdown": ["", "## FRAUD TRANSACTIONS BY DATE\n- Visualizing only Fraud Transactions by Date"], "code": "# Reference: https://www.kaggle.com/code/kabure/extensive-eda-and-modeling-xgb-hyperopt\n\n# Calling the function to transform the date column in datetime pandas object\n\n#seting some static color options\ncolor_op = ['#5527A0', '#BB93D7', '#834CF7', '#6C941E', '#93EAEA', '#7425FF', '#F2098A', '#7E87AC', \n            '#EBE36F', '#7FD394', '#49C35D', '#3058EE', '#44FDCF', '#A38F85', '#C4CEE0', '#B63A05', \n            '#4856BF', '#F0DB1B', '#9FDBD9', '#B123AC']\n\ntmp_amt = df_trans.groupby([df_trans.Date.dt.date, 'isFraud'])['TransactionAmt'].sum().reset_index()\ntmp_trans = df_trans.groupby([df_trans.Date.dt.date, 'isFraud'])['TransactionAmt'].count().reset_index()\n\ntmp_trans_fraud = tmp_trans[tmp_trans['isFraud'] == 1]\ntmp_amt_fraud = tmp_amt[tmp_amt['isFraud'] == 1]\n\ndates_temp = df_trans.groupby(df_trans.Date.dt.date)['TransactionAmt'].count().reset_index()\n# renaming the columns to apropriate names\n\n# creating the first trace with the necessary parameters\ntrace = go.Scatter(x=tmp_trans_fraud['Date'], y=tmp_trans_fraud.TransactionAmt,\n                    opacity = 0.8, line = dict(color = color_op[1]), name= 'Fraud Transactions')\n\n# using the new dates_temp_sum we will create the second trace\ntrace1 = go.Scatter(x=tmp_amt_fraud.Date, line = dict(color = color_op[7]), name=\"Fraud Amount\",\n                    y=tmp_amt_fraud['TransactionAmt'], opacity = 0.8, yaxis='y2')\n\n#creating the layout the will allow us to give an title and \n# give us some interesting options to handle with the outputs of graphs\nlayout = dict(\n    title= \"FRAUD TRANSACTIONS - Total Transactions and Fraud Informations by Date\",\n    xaxis=dict(\n        rangeselector=dict(\n            buttons=list([\n                dict(count=1, label='1m', step='month', stepmode='backward'),\n                dict(count=3, label='3m', step='month', stepmode='backward'),\n                dict(count=6, label='6m', step='month', stepmode='backward'),\n                dict(step='all')\n            ])\n        ),\n        rangeslider=dict(visible = True),\n        type='date' ),\n    yaxis=dict(title='Total Transactions'),\n    yaxis2=dict(overlaying='y',\n                anchor='x', side='right',\n                zeroline=False, showgrid=False,\n                title='Total Transaction Amount')\n)\n\n# creating figure with the both traces and layout\nfig = dict(data= [trace, trace1], layout=layout)\n\n#rendering the graphs\niplot(fig) #it's an equivalent to plt.show()", "processed": ["", "fraud transact date visual fraud transact date"]}, {"markdown": ["", "# Features [id_12 to id_38]\n- categorical features in training identity dataset"], "code": "# Reference: https://www.kaggle.com/code/kabure/extensive-eda-and-modeling-xgb-hyperopt\n\ndf_id[['id_12', 'id_13', 'id_14', 'id_15', 'id_16', 'id_17', 'id_18',\n       'id_19', 'id_20', 'id_21', 'id_22', 'id_23', 'id_24', 'id_25',\n       'id_26', 'id_27', 'id_28', 'id_29', 'id_30', 'id_31', 'id_32',\n       'id_33', 'id_34', 'id_35', 'id_36', 'id_37', 'id_38']].describe(include='all')\ndf_train = df_trans.merge(df_id, how='left', left_index=True, right_index=True)\ndef cat_feat_ploting(df, col):\n    tmp = pd.crosstab(df[col], df['isFraud'], normalize='index') * 100\n    tmp = tmp.reset_index()\n    tmp.rename(columns={0:'NoFraud', 1:'Fraud'}, inplace=True)\n\n    plt.figure(figsize=(14,10))\n    plt.suptitle(f'{col} Distributions', fontsize=22)\n\n    plt.subplot(221)\n    g = sns.countplot(x=col, data=df, order=tmp[col].values)\n    # plt.legend(title='Fraud', loc='upper center', labels=['No', 'Yes'])\n\n    g.set_title(f\"{col} Distribution\", fontsize=19)\n    g.set_xlabel(f\"{col} Name\", fontsize=17)\n    g.set_ylabel(\"Count\", fontsize=17)\n    # g.set_ylim(0,500000)\n    for p in g.patches:\n        height = p.get_height()\n        g.text(p.get_x()+p.get_width()/2.,\n                height + 3,\n                '{:1.2f}%'.format(height/total*100),\n                ha=\"center\", fontsize=14) \n\n    plt.subplot(222)\n    g1 = sns.countplot(x=col, hue='isFraud', data=df, order=tmp[col].values)\n    plt.legend(title='Fraud', loc='best', labels=['No', 'Yes'])\n    gt = g1.twinx()\n    gt = sns.pointplot(x=col, y='Fraud', data=tmp, color='black', order=tmp[col].values, legend=False)\n    gt.set_ylabel(\"% of Fraud Transactions\", fontsize=16)\n\n    g1.set_title(f\"{col} by Target(isFraud)\", fontsize=19)\n    g1.set_xlabel(f\"{col} Name\", fontsize=17)\n    g1.set_ylabel(\"Count\", fontsize=17)\n\n    plt.subplot(212)\n    g3 = sns.boxenplot(x=col, y='TransactionAmt', hue='isFraud', \n                       data=df[df['TransactionAmt'] <= 2000], order=tmp[col].values )\n    g3.set_title(\"Transaction Amount Distribuition by ProductCD and Target\", fontsize=20)\n    g3.set_xlabel(\"ProductCD Name\", fontsize=17)\n    g3.set_ylabel(\"Transaction Values\", fontsize=17)\n\n    plt.subplots_adjust(hspace = 0.4, top = 0.85)\n\n    plt.show()", "processed": ["", "featur id 12 id 38 categor featur train ident dataset"]}, {"markdown": ["## The target.\nFirst we will look at the target we intend to predict.\n\nWe are told: *The intent of the competition is to use the gameplay data to forecast how many attempts a child will take to pass a given assessment (an incorrect answer is counted as an attempt).*\nThe outcomes in this competition are grouped into 4 groups (labeled accuracy_group in the data):\n- 3: the assessment was solved on the first attempt\n- 2: the assessment was solved on the second attempt\n- 1: the assessment was solved after 3 or more attempts\n- 0: the assessment was never solved\n"], "code": "# Reference: https://www.kaggle.com/code/robikscube/2019-data-science-bowl-an-introduction\n\ntrain_labels.head()\ntrain_labels.groupby('accuracy_group')['game_session'].count() \\\n    .plot(kind='barh', figsize=(15, 5), title='Target (accuracy group)')\nplt.show()", "processed": ["target first look target intend predict told intent competit use gameplay data forecast mani attempt child take pas given ass incorrect answer count attempt outcom competit group 4 group label accuraci group data 3 ass solv first attempt 2 ass solv second attempt 1 ass solv 3 attempt 0 ass never solv"]}, {"markdown": ["Thngs to note about the taget:\n- Accuracy of 100% goes to group 3\n- Accuracy of ~50% goes to group 2\n- Not finishing goes to group 0\n- Group 1 looks to have the most variation"], "code": "# Reference: https://www.kaggle.com/code/robikscube/2019-data-science-bowl-an-introduction\n\nsns.pairplot(train_labels, hue='accuracy_group')\nplt.show()", "processed": ["thng note taget accuraci 100 goe group 3 accuraci 50 goe group 2 finish goe group 0 group 1 look variat"]}, {"markdown": ["## timestamp\nLets see how many observations we have over time. Are they all in the same/similar time zone?\n- Looks like number of observations rises over time. Steep pickup and dropoff at the start/end\n- Much less use during the middle of the night hours. Use increases during the day with a slow reduction in use around midnight. We don't know how the timestamp relates to time zones for different users.\n- More users on Thursday and Friday. "], "code": "# Reference: https://www.kaggle.com/code/robikscube/2019-data-science-bowl-an-introduction\n\n# Format and make date / hour features\ntrain['timestamp'] = pd.to_datetime(train['timestamp'])\ntrain['date'] = train['timestamp'].dt.date\ntrain['hour'] = train['timestamp'].dt.hour\ntrain['weekday_name'] = train['timestamp'].dt.weekday_name\n# Same for test\ntest['timestamp'] = pd.to_datetime(test['timestamp'])\ntest['date'] = test['timestamp'].dt.date\ntest['hour'] = test['timestamp'].dt.hour\ntest['weekday_name'] = test['timestamp'].dt.weekday_name\nprint(f'Train data has shape: {train.shape}')\nprint(f'Test data has shape: {test.shape}')\ntrain.groupby('date')['event_id'] \\\n    .agg('count') \\\n    .plot(figsize=(15, 3),\n         title='Numer of Event Observations by Date',\n         color=my_pal[2])\nplt.show()\ntrain.groupby('hour')['event_id'] \\\n    .agg('count') \\\n    .plot(figsize=(15, 3),\n         title='Numer of Event Observations by Hour',\n         color=my_pal[1])\nplt.show()\ntrain.groupby('weekday_name')['event_id'] \\\n    .agg('count').T[['Monday','Tuesday','Wednesday',\n                     'Thursday','Friday','Saturday',\n                     'Sunday']].T.plot(figsize=(15, 3),\n                                       title='Numer of Event Observations by Day of Week',\n                                       color=my_pal[3])\nplt.show()", "processed": ["timestamp let see mani observ time similar time zone look like number observ rise time steep pickup dropoff start end much le use middl night hour use increas day slow reduct use around midnight know timestamp relat time zone differ user user thursday friday"]}, {"markdown": ["## installation_id *important - predictions are grouped by these*\n- Randomly generated unique identifier grouping game sessions within a single installed application instance.\n- We will be predicting based off of these IDs\n- The training set has exactly 17000 unique `installation_ids`"], "code": "# Reference: https://www.kaggle.com/code/robikscube/2019-data-science-bowl-an-introduction\n\ntrain['installation_id'].nunique()\ntrain.groupby('installation_id') \\\n    .count()['event_id'] \\\n    .plot(kind='hist',\n          bins=40,\n          color=my_pal[4],\n          figsize=(15, 5),\n         title='Count of Observations by installation_id')\nplt.show()", "processed": ["instal id import predict group randomli gener uniqu identifi group game session within singl instal applic instanc predict base id train set exactli 17000 uniqu instal id"]}, {"markdown": ["Lets take a log transform of this count to we can more easily see what the distribution of counts by `insallation_id` looks like"], "code": "# Reference: https://www.kaggle.com/code/robikscube/2019-data-science-bowl-an-introduction\n\ntrain.groupby('installation_id') \\\n    .count()['event_id'] \\\n    .apply(np.log1p) \\\n    .plot(kind='hist',\n          bins=40,\n          color=my_pal[6],\n         figsize=(15, 5),\n         title='Log(Count) of Observations by installation_id')\nplt.show()", "processed": ["let take log transform count easili see distribut count insal id look like"]}, {"markdown": ["Wow, 50000+ events for a single `installation_id`. Lets take a closer look at the id with the most observations. Not exactly sure what I'm looking at here. But it looks like this `installation_id` spans a long duration (over one month). Could this be installed by a bot? The use history does not look natural."], "code": "# Reference: https://www.kaggle.com/code/robikscube/2019-data-science-bowl-an-introduction\n\ntrain.query('installation_id == \"f1c21eda\"') \\\n    .set_index('timestamp')['event_code'] \\\n    .plot(figsize=(15, 5),\n          title='installation_id #f1c21eda event Id - event code vs time',\n         style='.',\n         color=my_pal[8])\nplt.show()", "processed": ["wow 50000 event singl instal id let take closer look id observ exactli sure look look like instal id span long durat one month could instal bot use histori look natur"]}, {"markdown": ["## event_code\n- Identifier of the event 'class'. Unique per game, but may be duplicated across games. E.g. event code '2000' always identifies the 'Start Game' event for all games. Extracted from event_data."], "code": "# Reference: https://www.kaggle.com/code/robikscube/2019-data-science-bowl-an-introduction\n\ntrain.groupby('event_code') \\\n    .count()['event_id'] \\\n    .sort_values() \\\n    .plot(kind='bar',\n         figsize=(15, 5),\n         title='Count of different event codes.')\nplt.show()", "processed": ["event code identifi event class uniqu per game may duplic across game e g event code 2000 alway identifi start game event game extract event data"]}, {"markdown": ["lets take a closer look at the event codes `4070` and `4030`\n- We notice that event 4070 and 4030 always comes with coordinates (x, y) and stage_width.\n- Possibly they could be marking acheivements or something related to position on the screen.\nThese events look like this:\n```\n{\"size\":0,\"coordinates\":{\"x\":782,\"y\":207,\"stage_width\":1015,\"stage_height\":762},\"event_count\":55,\"game_time\":34324,\"event_code\":4030}\n```", "## game_time\n- Time in milliseconds since the start of the game session. Extracted from event_data.\n- The `log1p` transform shows a somewhat normal distribution with a peak at zero."], "code": "# Reference: https://www.kaggle.com/code/robikscube/2019-data-science-bowl-an-introduction\n\ntrain['game_time'].apply(np.log1p) \\\n    .plot(kind='hist',\n          figsize=(15, 5),\n          bins=100,\n          title='Log Transform of game_time',\n          color=my_pal[1])\nplt.show()", "processed": ["let take closer look event code 4070 4030 notic event 4070 4030 alway come coordin x stage width possibl could mark acheiv someth relat posit screen event look like size 0 coordin x 782 207 stage width 1015 stage height 762 event count 55 game time 34324 event code 4030", "game time time millisecond sinc start game session extract event data log1p transform show somewhat normal distribut peak zero"]}, {"markdown": ["## Game/Video titles\n- Chow Time is very popular, along with Sandcastle Builder, Scrub-A-Dub, and Bottle Filler\n- After that there is a steep dropoff\n- Assessment's are in the 200000 count range.\n- Games with levels are less frequent\n- Some games or titles (maybe videos?) at the bottom are very infrequently seen.\n\nSome examples of the top games:\nChow Time:\nhttps://www.youtube.com/watch?v=tvRtFqOqa-Y\n\n"], "code": "# Reference: https://www.kaggle.com/code/robikscube/2019-data-science-bowl-an-introduction\n\ntrain.groupby('title')['event_id'] \\\n    .count() \\\n    .sort_values() \\\n    .plot(kind='barh',\n          title='Count of Observation by Game/Video title',\n         figsize=(15, 15))\nplt.show()", "processed": ["game video titl chow time popular along sandcastl builder scrub dub bottl filler steep dropoff ass 200000 count rang game level le frequent game titl mayb video bottom infrequ seen exampl top game chow time http www youtub com watch v tvrtfqoqa"]}, {"markdown": ["## Game/Video type\n- Media type of the game or video. Possible values are: 'Game', 'Assessment', 'Activity', 'Clip'.\n- Most are games, next are activities\n- Clips are the least common"], "code": "# Reference: https://www.kaggle.com/code/robikscube/2019-data-science-bowl-an-introduction\n\ntrain.groupby('type')['event_id'] \\\n    .count() \\\n    .sort_values() \\\n    .plot(kind='bar',\n          figsize=(15, 4),\n          title='Count by Type',\n          color=my_pal[2])\nplt.show()", "processed": ["game video type medium type game video possibl valu game ass activ clip game next activ clip least common"]}, {"markdown": ["## World\n- The section of the application the game or video belongs to. Helpful to identify the educational curriculum goals of the media.\n- Possible values are: 'NONE' (at the app's start screen), TREETOPCITY' (Length/Height), 'MAGMAPEAK' (Capacity/Displacement), 'CRYSTALCAVES' (Weight)."], "code": "# Reference: https://www.kaggle.com/code/robikscube/2019-data-science-bowl-an-introduction\n\ntrain.groupby('world')['event_id'] \\\n    .count() \\\n    .sort_values() \\\n    .plot(kind='bar',\n          figsize=(15, 4),\n          title='Count by World',\n          color=my_pal[3])\nplt.show()", "processed": ["world section applic game video belong help identifi educ curriculum goal medium possibl valu none app start screen treetopc length height magmapeak capac displac crystalcav weight"]}, {"markdown": ["## log(game_time) vs game/video categories "], "code": "# Reference: https://www.kaggle.com/code/robikscube/2019-data-science-bowl-an-introduction\n\ntrain['log1p_game_time'] = train['game_time'].apply(np.log1p)\nfig, ax = plt.subplots(figsize=(15, 5))\nsns.catplot(x=\"type\", y=\"log1p_game_time\",\n            data=train.sample(10000), alpha=0.5, ax=ax);\nax.set_title('Distribution of log1p(game_time) by Type')\nplt.close()\nplt.show()\nfig, ax = plt.subplots(figsize=(15, 5))\nsns.catplot(x=\"world\", y=\"log1p_game_time\",\n            data=train.sample(10000), alpha=0.5, ax=ax);\nax.set_title('Distribution of log1p(game_time) by World')\nplt.close()\nplt.show()", "processed": ["log game time v game video categori"]}, {"markdown": ["<a id=\"5\"></a>\n## 5. Understanding the variables distributions\n<a id=\"5-1\"></a>\n### 5.1 What is the distribution of Deal Probability"], "code": "# Reference: https://www.kaggle.com/code/shivamb/in-depth-analysis-visualisations-avito\n\n## 1. Deal Probability Distribution\ndef generate_histogram_plot(df, col, title):\n    trace = go.Histogram(x = df[col]) \n    layout = go.Layout(title=title, legend=dict(orientation=\"h\"), height=400)\n    fig = go.Figure(data=[trace], layout=layout)\n    iplot(fig)\n\n\n# generate_histogram_plot(train_df, 'deal_probability', 'Distribution of Deal Probability')\nplt.figure(figsize=(15,5))\nsns.distplot(train_df[\"deal_probability\"].values, bins=120, color=\"#ff002e\")\nplt.xlabel('Deal Probility', fontsize=14);\nplt.title(\"Distribution of Deal Probability\", fontsize=14);\nplt.show();", "processed": ["id 5 5 understand variabl distribut id 5 1 5 1 distribut deal probabl"]}, {"markdown": ["**Inference**  \n> - From the deal probability distribution graph, it is clear that majority of the items have exteremely low deal probability, ie. about 78%, while very few values have the deal probability of 0.7 or larger.  \n> - A very small tower is observed near the deal probability of 1.0, indicating that there are some items in the dataset having very high value of deal probability.  \n\n**Top 10 Items having deal probability  = 1.0 are**     \n> - 92013ca1fe79 | Installation of doors, openings, slopes, arches  \n> - c6239fc67a6f | Nail extension, correction  \n> - 44aa121e4559 | Cargo transportation (long distance), onboard, open  \n> - b16d1b27c975 | Rise of houses  \n> - fe03dbc60ccf | Transportation across the southern Federal district North Caucasian Federal district Crimea  \n\n\n<a id=\"5-2\"></a>\n### 5.2 Deal Probability Bins !\n"], "code": "# Reference: https://www.kaggle.com/code/shivamb/in-depth-analysis-visualisations-avito\n\ndef _generate_bar_plot_hor(df, col, title, color, w=None, h=None, lm=0, limit=100):\n    cnt_srs = df[col].value_counts()[:limit]\n    trace = go.Bar(y=cnt_srs.index[::-1], x=cnt_srs.values[::-1], orientation = 'h',\n        marker=dict(color=color))\n\n    layout = dict(title=title, margin=dict(l=lm), width=w, height=h)\n    data = [trace]\n    fig = go.Figure(data=data, layout=layout)\n    iplot(fig)\n\ndef _generate_bar_plot_ver(df, col, title, color, w=None, h=None, lm=0, limit=100, need_trace = False):\n    cnt_srs = df[col].value_counts()[:limit]\n    trace = go.Bar(x=list(cnt_srs.index), y=list(cnt_srs.values),\n        marker=dict(color = color))\n    if need_trace:\n        return trace\n    if w != None and h != None:\n        layout = dict(title=title, margin=dict(l=lm), width=w, height=h)\n    else:\n        layout = dict(title=title, margin=dict(l=lm))\n    data = [trace]\n    fig = go.Figure(data=data, layout=layout)\n    iplot(fig)\n\ntrace1 = _generate_bar_plot_ver(train_df, 'deal_class_2', \"Deal Probability Bins\", '#7a8aa3', lm=0, limit=30, need_trace = True)\ntrace2 = _generate_bar_plot_ver(train_df, 'deal_class', \"Deal Proabibilities (>0.5 or <= 0.5)\", ['#f25771','#93ef51'], 200, limit=30, need_trace = True)\n\nfig = tools.make_subplots(rows=1, cols=3, specs=[[{'colspan': 2}, {},{}]], print_grid=False, subplot_titles = ['Deal Probability Bins','','Deal Proabibilities (>0.5 or <= 0.5)'])\nfig.append_trace(trace1, 1, 1);\nfig.append_trace(trace2, 1, 3);\n\nfig['layout'].update(height=400, title='',showlegend=False)\niplot(fig); ", "processed": ["infer deal probabl distribut graph clear major item exterem low deal probabl ie 78 valu deal probabl 0 7 larger small tower observ near deal probabl 1 0 indic item dataset high valu deal probabl top 10 item deal probabl 1 0 92013ca1fe79 instal door open slope arch c6239fc67a6f nail extens correct 44aa121e4559 cargo transport long distanc onboard open b16d1b27c975 rise hous fe03dbc60ccf transport across southern feder district north caucasian feder district crimea id 5 2 5 2 deal probabl bin"]}, {"markdown": ["> - Top cities are: Krasnodar (63638 items), Yekaterinburg, (63602 items), Novosibirsk (56929 items), Rostov-on-don (52323 items)\n\n<a id=\"5-9\"></a>\n### 5.9 Understanding distributions of Param_1 and Param_2 in the dataset"], "code": "# Reference: https://www.kaggle.com/code/shivamb/in-depth-analysis-visualisations-avito\n\ntrace1 = _generate_bar_plot_ver(train_df, cols[4], \"Param 1 Values\", '#1bf7bc', 700, 400, 200, limit=30, need_trace = True)\ntrace2 = _generate_bar_plot_ver(train_df, cols[5], \"Param 2 Values\", '#1bf7bc', 700, 400, 200, limit=30, need_trace = True)\ntrace3 = _generate_bar_plot_ver(train_df, cols[6], \"Param 3 Values\", '#1bf7bc', 700, 400, 200, limit=30, need_trace = True)\n\nfig = tools.make_subplots(rows=1, cols=2, print_grid=False, subplot_titles = ['Param 1 Values','Param 2 Values'])\nfig.append_trace(trace1, 1, 1);\nfig.append_trace(trace2, 1, 2);\n# fig.append_trace(trace3, 1, 3);\n\nfig['layout'].update(height=400,title='Top Values in Param 1,2 columns', showlegend=False);\niplot(fig); ", "processed": ["top citi krasnodar 63638 item yekaterinburg 63602 item novosibirsk 56929 item rostov 52323 item id 5 9 5 9 understand distribut param 1 param 2 dataset"]}, {"markdown": ["> - There are 371 Unique values for Param_1, 271 for Param_2, and about 1200 for Param_3.\n> - Param_3 contains very low level type of categorization about the items such as the centimeter lengths, quantites etc.\n\n\n<a id=\"5-10\"></a>\n### 5.10 On which Month Day and Week Day, the items are activated "], "code": "# Reference: https://www.kaggle.com/code/shivamb/in-depth-analysis-visualisations-avito\n\ntrace1 = _generate_bar_plot_ver(train_df, cols[7], \"WeekDays\", '#d38bed', 700, 400, 200, limit=30, need_trace = True)\ntrace2 = _generate_bar_plot_ver(train_df, cols[8], \"MonthDays\", '#d38bed', 700, 400, 200, limit=30, need_trace = True)\n\n\nfig = tools.make_subplots(rows=1, cols=2, print_grid=False, subplot_titles = ['Week Days','Month Days'])\nfig.append_trace(trace1, 1, 1);\nfig.append_trace(trace2, 1, 2);\n\nfig['layout'].update(height=300,title='Ads Posted in different Week/Month Days', showlegend=False);\niplot(fig); \n", "processed": ["371 uniqu valu param 1 271 param 2 1200 param 3 param 3 contain low level type categor item centimet length quantit etc id 5 10 5 10 month day week day item activ"]}, {"markdown": ["> - More number of items are published on Weekends (about 460K in total), while Fridays appear to have lowest number of items\n> - In the given dataset, maximun number of items were observed on 19th and 20th of the month, while lowest on 17th and 18th. The exact dates from the database are March 19th and 20th of 2017, and lowest - March 17th and March 18th\n\n<a id=\"5-11\"></a>\n### 5.11 Length of Item Title and Description"], "code": "# Reference: https://www.kaggle.com/code/shivamb/in-depth-analysis-visualisations-avito\n\ntrace1 = _generate_bar_plot_ver(train_df, cols[9], \"Title Length\", '#f7a92c', 700, 400, 200, limit=30, need_trace = True)\ntrace2 = _generate_bar_plot_ver(train_df, cols[10], \"Description Length\", '#f7a92c', 700, 400, 200, limit=30, need_trace = True)\n\nfig = tools.make_subplots(rows=1, cols=2, print_grid=False, subplot_titles = ['Title Word Count','Description Word Count'])\nfig.append_trace(trace1, 1, 1);\nfig.append_trace(trace2, 1, 2);\n\nfig['layout'].update(height=400, title='', showlegend=False);\niplot(fig); \n", "processed": ["number item publish weekend 460k total friday appear lowest number item given dataset maximun number item observ 19th 20th month lowest 17th 18th exact date databas march 19th 20th 2017 lowest march 17th march 18th id 5 11 5 11 length item titl descript"]}, {"markdown": ["> - Items present in the dataset have a range of 1 to 10 words in the title, with title having 2 words in them have the highest number equal to 360K items  \n> - Also, the descriptions of the items takes different values: as low as zero to as high as 25 words. A large number of items with empty descriptions are present in the dataset.  \n> - Items having description containing 6 words are highest while items having description containing 1 word are the lowest equal to 66K and 17K respectively  \n\n<a id=\"5-12\"></a>\n### 5.12 Image Top1 and UserId"], "code": "# Reference: https://www.kaggle.com/code/shivamb/in-depth-analysis-visualisations-avito\n\ntrace1 = _generate_bar_plot_ver(train_df, cols[11], \"Image Top 1\", '#e2dfd9', 700, 400, 200, limit=30, need_trace = True)\ntrace2 = _generate_bar_plot_ver(train_df, cols[12], \"User Id\", '#e2dfd9', 700, 400, 200, limit=30, need_trace = True)\n\n\nfig = tools.make_subplots(rows=1, cols=2, print_grid=False, subplot_titles = ['Image Top 1','User Id'])\nfig.append_trace(trace1, 1, 1);\nfig.append_trace(trace2, 1, 2);\n\nfig['layout'].update(height=400,title='Ads having different ImageTop1 and User Id', showlegend=False);\niplot(fig); ", "processed": ["item present dataset rang 1 10 word titl titl 2 word highest number equal 360k item also descript item take differ valu low zero high 25 word larg number item empti descript present dataset item descript contain 6 word highest item descript contain 1 word lowest equal 66k 17k respect id 5 12 5 12 imag top1 userid"]}, {"markdown": ["> - User having user_id = 45ba3f23bf25 have posted maximum number of items (108) followed by user_id = ee74bccca74f (980 items) and user_id = 60dfed1efb6e (907 items)\n> - There are 771769 distincit users who have posted these items while there are 3062 unique image top 1 codes\n> - Top Image Top 1 codes are 2219 (18739 items),  1002 (18646 items), 2918 (15407 items)\n \n\n<a id=\"5-13\"></a>\n### 5.13 Distribution of User-Type in the dataset"], "code": "# Reference: https://www.kaggle.com/code/shivamb/in-depth-analysis-visualisations-avito\n\ndef _create_pie_chart(df, col):\n    tm = df[col].value_counts()\n    labels = list(tm.index)\n    values = list(tm.values)\n    trace = go.Pie(labels=labels, values=values, marker=dict(colors=['#f9c968', '#75e575', '#d693b3']))\n    return trace\ntrace1 = _create_pie_chart(train_df, 'user_type')\nlayout = go.Layout(title='Distribution of User Type', width=600, height=400, margin=dict(l=100))\ndata = [trace1]\nfig = go.Figure(data=data, layout=layout)\niplot(fig)", "processed": ["user user id 45ba3f23bf25 post maximum number item 108 follow user id ee74bccca74f 980 item user id 60dfed1efb6 907 item 771769 distincit user post item 3062 uniqu imag top 1 code top imag top 1 code 2219 18739 item 1002 18646 item 2918 15407 item id 5 13 5 13 distribut user type dataset"]}, {"markdown": [">- Majority of the items are posted by users which belong to Private User type while minority of the items are posted by users which belong to \"Shop\" user type. \n> - About 72% of the users are of Private Type while about 5% of the users are of Shop user type\n\n<a id=\"5-14\"></a>\n### 5.14 For what number of Days Ads are Run"], "code": "# Reference: https://www.kaggle.com/code/shivamb/in-depth-analysis-visualisations-avito\n\nt = pr_train['total_period'].value_counts()\n\nlabels = list(t.index)\nlabels = [str(x).replace(\"00:00:00\",\"\").strip() for x in labels]\nvalues = list(t.values)\n\nlayout = go.Layout(title='For How Much Days Ads are Run', width=600, height=400, margin=dict(l=100), xaxis=dict(tickangle=-65))\ntrace1 = go.Bar(x=labels, y=values, marker=dict(color=\"#FF7441\"))\n\ndata = [trace1]\nfig = go.Figure(data=data, layout=layout)\niplot(fig)", "processed": ["major item post user belong privat user type minor item post user belong shop user type 72 user privat type 5 user shop user type id 5 14 5 14 number day ad run"]}, {"markdown": ["> - From the correlation graph, Deal probability shows good correlation with description len and title len. These may by important features while modelling. \n> - Item Seq Number is also correlated mildly with title len, description len, and Image Top 1  \n> - Image Top 1 code shows a good correlation with deal probability and length of title and description", "<a id=\"6-2\"></a>\n### 6.2 Deal Probability by Parent Category and User Type"], "code": "# Reference: https://www.kaggle.com/code/shivamb/in-depth-analysis-visualisations-avito\n\nsns.set(rc={'figure.figsize':(15, 8)})\nsns.boxplot(x=\"parent_category_name_en\", y=\"deal_probability\", hue=\"user_type\",  palette=\"PRGn\", data=train_df)\nplt.title(\"Deal probability by parent category and User Type\")\nplt.xticks(rotation='vertical')\nplt.show()", "processed": ["correl graph deal probabl show good correl descript len titl len may import featur model item seq number also correl mildli titl len descript len imag top 1 imag top 1 code show good correl deal probabl length titl descript", "id 6 2 6 2 deal probabl parent categori user type"]}, {"markdown": ["> - From the above boxplot graph, Users having \"shop\" user types are only related with items of Real Estate, Transport, and Services categories. Service Category is the one having largest deal probability, while Real estate have lowest. \n> - Users having \"Company\" as user type also have highest deal probability in the Services category followed by Transport and Animals. Unlike Shop user types, Company user types are related with almost all parent categoies\n>- Users having \"Private\" as user type are scattered in the range of 0.1 to 0.6 in the Services category. \n\n<a id=\"6-3\"></a>\n### 6.3 Deal Probability by Region and User Type"], "code": "# Reference: https://www.kaggle.com/code/shivamb/in-depth-analysis-visualisations-avito\n\nsns.set(rc={'figure.figsize':(15,8)})\nsns.boxplot(x=\"region_en\", y=\"deal_probability\", hue=\"user_type\",  palette=\"coolwarm\", data=train_df)\nplt.title(\"Deal probability by Region and User Type\")\nplt.xticks(rotation='vertical')\nplt.show()", "processed": ["boxplot graph user shop user type relat item real estat transport servic categori servic categori one largest deal probabl real estat lowest user compani user type also highest deal probabl servic categori follow transport anim unlik shop user type compani user type relat almost parent categoi user privat user type scatter rang 0 1 0 6 servic categori id 6 3 6 3 deal probabl region user type"]}, {"markdown": ["> - Above box plot shows that Items which are posted in Orenburg Oblast and Saratov Oblast and which belongs to Shop user type has higest range of deal probabilities.  \n> - For the items from Company User type users, maximum variation is shown in the Stavropol Krai and Orenburg Oblast. \n\n<a id=\"6-4\"></a>\n### 6.4 Understanding Log of Price with respect to deal probability Class "], "code": "# Reference: https://www.kaggle.com/code/shivamb/in-depth-analysis-visualisations-avito\n\ntrain_df['price_log'] = np.log(train_df['price'] + 1)\nsns.set_style(\"whitegrid\", {'axes.grid' : False})\ng = sns.boxplot(x='deal_class_2', y='price_log', data=train_df, palette=\"RdBu\")\ng.set_xlabel('The Deal Probability Class',fontsize=12)\ng.set_ylabel('Log of Price',fontsize=12)\nplt.show()", "processed": ["box plot show item post orenburg oblast saratov oblast belong shop user type higest rang deal probabl item compani user type user maximum variat shown stavropol krai orenburg oblast id 6 4 6 4 understand log price respect deal probabl class"]}, {"markdown": ["\n<a id=\"6-7\"></a>\n### 6.7 Different Regions and the mean price of items having different deal probability"], "code": "# Reference: https://www.kaggle.com/code/shivamb/in-depth-analysis-visualisations-avito\n\ndef _create_bubble_plots(col1, col2, aggcol, func, title, cs):\n    tempdf = train_df.groupby([col1, col2]).agg({aggcol : func}).reset_index()\n    tempdf[aggcol] = tempdf[aggcol].apply(lambda x : int(x))\n    tempdf = tempdf.sort_values(aggcol, ascending=False)\n\n    sizes = list(reversed([i for i in range(10,31)]))\n    intervals = int(len(tempdf) / len(sizes))\n    size_array = [9]*len(tempdf)\n    \n    st = 0\n    for i, size in enumerate(sizes):\n        for j in range(st, st+intervals):\n            size_array[j] = size \n        st = st+intervals\n    tempdf['size_n'] = size_array\n    # tempdf = tempdf.sample(frac=1).reset_index(drop=True)\n\n    cols = list(tempdf['size_n'])\n\n    trace1 = go.Scatter( x=tempdf[col1], y=tempdf[col2], mode='markers', text=tempdf[aggcol],\n        marker=dict( size=tempdf.size_n, color=cols, colorscale=cs ))\n    data = [trace1]\n    layout = go.Layout(title=title)\n    fig = go.Figure(data=data, layout=layout)\n    iplot(fig)\n\n_create_bubble_plots('region_en', 'deal_class_2', 'price', 'mean', '', 'Picnic')", "processed": ["id 6 7 6 7 differ region mean price item differ deal probabl"]}, {"markdown": ["<a id=\"7\"></a>\n## 7. What are the charateristics of Items having High or Low Mean Price\n\n<a id=\"7-1\"></a>\n### 7.1 Mean Price of Items by Week Day and Month Day"], "code": "# Reference: https://www.kaggle.com/code/shivamb/in-depth-analysis-visualisations-avito\n\ndef _aggregate(base_col, agg_col, agg_func, return_trace = False):\n    tempdf = train_df.groupby(base_col).agg({agg_col : agg_func})\n    \n    trace = go.Bar(x=tempdf.index, y=tempdf.price, name='', marker=dict(color='#8a7aff'))\n    if return_trace:\n        return trace\n    \n    data = [trace]\n    layout = go.Layout(title='', width=500, legend=dict(orientation=\"h\"), margin=dict(b=200))\n    fig = go.Figure(data=data, layout=layout)\n    iplot(fig)\n\ntrace1 = _aggregate('weekday', 'price', 'mean', return_trace=True)\ntrace2 = _aggregate('day', 'price', 'mean', return_trace=True)\n\nfig = tools.make_subplots(rows=1, cols=2, print_grid=False, subplot_titles = ['Mean Price on WeekDays','Mean Price of Month Days'])\nfig.append_trace(trace1, 1, 1);\nfig.append_trace(trace2, 1, 2);\n\nfig['layout'].update(height=400, paper_bgcolor='#f0eff7', plot_bgcolor='#f0eff7', showlegend=False);\niplot(fig); ", "processed": ["id 7 7 charaterist item high low mean price id 7 1 7 1 mean price item week day month day"]}, {"markdown": ["> - Items with maximum price were promoted on 22nd date of the month ie. 22 March 2017, while the cheapest items was promoted on 3rd of March 2017\n\n<a id=\"7-2\"></a>\n### 7.2 Mean price of items by Regions and Cities"], "code": "# Reference: https://www.kaggle.com/code/shivamb/in-depth-analysis-visualisations-avito\n\ntrace1 = _aggregate('region_en', 'price', 'mean', return_trace=True)\ntrace2 = _aggregate('city', 'price', 'mean', return_trace=True)\n\nfig = tools.make_subplots(rows=1, cols=2, print_grid=False, subplot_titles = ['Mean Price of Regions','Mean Price of Cities'])\nfig.append_trace(trace1, 1, 1);\nfig.append_trace(trace2, 1, 2);\n\nfig['layout'].update(height=400, paper_bgcolor='#f0eff7', plot_bgcolor='#f0eff7', showlegend=False);\niplot(fig); ", "processed": ["item maximum price promot 22nd date month ie 22 march 2017 cheapest item promot 3rd march 2017 id 7 2 7 2 mean price item region citi"]}, {"markdown": ["> - Items having maximum mean price (2.14M) were promoted in Irutsk Oblast Region and Krasnoyarask Krai\n\n<a id=\"7-3\"></a>\n### 7.3 Mean Price of Items by Categories and Parent Categories"], "code": "# Reference: https://www.kaggle.com/code/shivamb/in-depth-analysis-visualisations-avito\n\ntrace1 = _aggregate('category_name_en', 'price', 'mean', return_trace=True)\ntrace2 = _aggregate('parent_category_name_en', 'price', 'mean', return_trace=True)\n\nfig = tools.make_subplots(rows=1, cols=2, print_grid=False, subplot_titles = ['Mean Price on Categories','Mean Price of Parent Categories'])\nfig.append_trace(trace1, 1, 1);\nfig.append_trace(trace2, 1, 2);\n\nfig['layout'].update(height=400, paper_bgcolor='#f0eff7', plot_bgcolor='#f0eff7', showlegend=False);\niplot(fig); ", "processed": ["item maximum mean price 2 14m promot irutsk oblast region krasnoyarask krai id 7 3 7 3 mean price item categori parent categori"]}, {"markdown": ["> - As expected, Real Estate items have highest mean items followed by Business and Transport\n> - Property Abroa, Land, and Commerical Property have highest mean price\n\n<a id=\"7-4\"></a>\n### 7.4 Mean Price by Count of Words present in Title and Description"], "code": "# Reference: https://www.kaggle.com/code/shivamb/in-depth-analysis-visualisations-avito\n\ntrace1 = _aggregate('title_len', 'price', 'mean', return_trace=True)\ntrace2 = _aggregate('description_len', 'price', 'mean', return_trace=True)\n\nfig = tools.make_subplots(rows=1, cols=2, print_grid=False, subplot_titles = ['Mean Price by Title Length','Mean Price by Description Length'])\nfig.append_trace(trace1, 1, 1);\nfig.append_trace(trace2, 1, 2);\n\nfig['layout'].update(height=400, paper_bgcolor='#f0eff7', plot_bgcolor='#f0eff7', showlegend=False);\niplot(fig); ", "processed": ["expect real estat item highest mean item follow busi transport properti abroa land commer properti highest mean price id 7 4 7 4 mean price count word present titl descript"]}, {"markdown": ["![](https://preview.ibb.co/iBsnDc/wc1.png)\n\n>- Interesting to see that there are items such as Iphone, Hyuanday, JCB, Toyota which have high deal percentage and they are not present in titles of items having low deal percentage\n>- Brands such as Samsung, Zara, and Adiddas are from the item sets having low deal probabilities.\n\n<a id=\"8-2\"></a>\n### 8.2 Comparing all other features of Items with Low and High deal percentages\n"], "code": "# Reference: https://www.kaggle.com/code/shivamb/in-depth-analysis-visualisations-avito\n\ndef get_trace(col, df, color):\n    temp = df[col].value_counts().nlargest(3)\n    x = list(reversed(list(temp.index)))\n    y = list(reversed(list(temp.values)))\n    trace = go.Bar(x = y, y = x, width = [0.9, 0.9, 0.9], orientation='h', marker=dict(color=color))\n    return trace\n\n\ncompare_cols = ['parent_category_name_en', 'category_name', 'city', 'param_1', 'param_2', 'weekday', 'title_len', 'description_len']\n\n\ngoodtraces = []\ngoodtitles = []\nfor i,col in enumerate(compare_cols):\n    goodtitles.append(col)\n    goodtraces.append(get_trace(col, good_performing_ads, '#a3dd56'))\n\n    \nbadtraces = []\nbadtitles = []\nfor i,col in enumerate(compare_cols):\n    badtitles.append(col)\n    badtraces.append(get_trace(col, bad_performing_ads, '#ef7067'))\n\ntitles = []\nfor each in compare_cols:\n    titles.append(each)\n    titles.append(each)\nfig = tools.make_subplots(rows=len(compare_cols), cols=2, print_grid=False, horizontal_spacing = 0.15, subplot_titles=titles)\ni = 0\nfor g,b in zip(goodtraces, badtraces):\n    i += 1\n    fig.append_trace(g, i, 1);\n    fig.append_trace(b, i, 2);\n\nfig['layout'].update(height=1000, margin=dict(l=100), showlegend=False, title=\"Comapring Features of Ads with High and Low Deal Percentages\");\niplot(fig, filename='simple-subplot');    ", "processed": ["http preview ibb co ibsndc wc1 png interest see item iphon hyuanday jcb toyota high deal percentag present titl item low deal percentag brand samsung zara adidda item set low deal probabl id 8 2 8 2 compar featur item low high deal percentag"]}, {"markdown": ["<a id=8-3></a>\n### 8.3 Top N-grams used in descriptions"], "code": "# Reference: https://www.kaggle.com/code/shivamb/in-depth-analysis-visualisations-avito\n\n# stopwords = ['\u0430', '\u0435', '\u0438', '\u0436', '\u043c', '\u043e', '\u043d\u0430', '\u043d\u0435', '\u043d\u0438', '\u043e\u0431', '\u043d\u043e', '\u043e\u043d', '\u043c\u043d\u0435', '\u043c\u043e\u0438', '\u043c\u043e\u0436', '\u043e\u043d\u0430', '\u043e\u043d\u0438', '\u043e\u043d\u043e', '\u043c\u043d\u043e\u0439', '\u043c\u043d\u043e\u0433\u043e', '\u043c\u043d\u043e\u0433\u043e\u0447\u0438\u0441\u043b\u0435\u043d\u043d\u043e\u0435', '\u043c\u043d\u043e\u0433\u043e\u0447\u0438\u0441\u043b\u0435\u043d\u043d\u0430\u044f', '\u043c\u043d\u043e\u0433\u043e\u0447\u0438\u0441\u043b\u0435\u043d\u043d\u044b\u0435', '\u043c\u043d\u043e\u0433\u043e\u0447\u0438\u0441\u043b\u0435\u043d\u043d\u044b\u0439', '\u043c\u043d\u043e\u044e', '\u043c\u043e\u0439', '\u043c\u043e\u0433', '\u043c\u043e\u0433\u0443\u0442', '\u043c\u043e\u0436\u043d\u043e', '\u043c\u043e\u0436\u0435\u0442', '\u043c\u043e\u0436\u0445\u043e', '\u043c\u043e\u0440', '\u043c\u043e\u044f', '\u043c\u043e\u0451', '\u043c\u043e\u0447\u044c', '\u043d\u0430\u0434', '\u043d\u0435\u0435', '\u043e\u0431\u0430', '\u043d\u0430\u043c', '\u043d\u0435\u043c', '\u043d\u0430\u043c\u0438', '\u043d\u0438\u043c\u0438', '\u043c\u0438\u043c\u043e', '\u043d\u0435\u043c\u043d\u043e\u0433\u043e', '\u043e\u0434\u043d\u043e\u0439', '\u043e\u0434\u043d\u043e\u0433\u043e', '\u043c\u0435\u043d\u0435\u0435', '\u043e\u0434\u043d\u0430\u0436\u0434\u044b', '\u043e\u0434\u043d\u0430\u043a\u043e', '\u043c\u0435\u043d\u044f', '\u043d\u0435\u043c\u0443', '\u043c\u0435\u043d\u044c\u0448\u0435', '\u043d\u0435\u0439', '\u043d\u0430\u0432\u0435\u0440\u0445\u0443', '\u043d\u0435\u0433\u043e', '\u043d\u0438\u0436\u0435', '\u043c\u0430\u043b\u043e', '\u043d\u0430\u0434\u043e', '\u043e\u0434\u0438\u043d', '\u043e\u0434\u0438\u043d\u043d\u0430\u0434\u0446\u0430\u0442\u044c', '\u043e\u0434\u0438\u043d\u043d\u0430\u0434\u0446\u0430\u0442\u044b\u0439', '\u043d\u0430\u0437\u0430\u0434', '\u043d\u0430\u0438\u0431\u043e\u043b\u0435\u0435', '\u043d\u0435\u0434\u0430\u0432\u043d\u043e', '\u043c\u0438\u043b\u043b\u0438\u043e\u043d\u043e\u0432', '\u043d\u0435\u0434\u0430\u043b\u0435\u043a\u043e', '\u043c\u0435\u0436\u0434\u0443', '\u043d\u0438\u0437\u043a\u043e', '\u043c\u0435\u043b\u044f', '\u043d\u0435\u043b\u044c\u0437\u044f', '\u043d\u0438\u0431\u0443\u0434\u044c', '\u043d\u0435\u043f\u0440\u0435\u0440\u044b\u0432\u043d\u043e', '\u043d\u0430\u043a\u043e\u043d\u0435\u0446', '\u043d\u0438\u043a\u043e\u0433\u0434\u0430', '\u043d\u0438\u043a\u0443\u0434\u0430', '\u043d\u0430\u0441', '\u043d\u0430\u0448', '\u043d\u0435\u0442', '\u043d\u0435\u044e', '\u043d\u0435\u0451', '\u043d\u0438\u0445', '\u043c\u0438\u0440\u0430', '\u043d\u0430\u0448\u0430', '\u043d\u0430\u0448\u0435', '\u043d\u0430\u0448\u0438', '\u043d\u0438\u0447\u0435\u0433\u043e', '\u043d\u0430\u0447\u0430\u043b\u0430', '\u043d\u0435\u0440\u0435\u0434\u043a\u043e', '\u043d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u043e', '\u043e\u0431\u044b\u0447\u043d\u043e', '\u043e\u043f\u044f\u0442\u044c', '\u043e\u043a\u043e\u043b\u043e', '\u043c\u044b', '\u043d\u0443', '\u043d\u0445', '\u043e\u0442', '\u043e\u0442\u043e\u0432\u0441\u044e\u0434\u0443', '\u043e\u0441\u043e\u0431\u0435\u043d\u043d\u043e', '\u043d\u0443\u0436\u043d\u043e', '\u043e\u0447\u0435\u043d\u044c', '\u043e\u0442\u0441\u044e\u0434\u0430', '\u0432', '\u0432\u043e', '\u0432\u043e\u043d', '\u0432\u043d\u0438\u0437', '\u0432\u043d\u0438\u0437\u0443', '\u0432\u043e\u043a\u0440\u0443\u0433', '\u0432\u043e\u0442', '\u0432\u043e\u0441\u0435\u043c\u043d\u0430\u0434\u0446\u0430\u0442\u044c', '\u0432\u043e\u0441\u0435\u043c\u043d\u0430\u0434\u0446\u0430\u0442\u044b\u0439', '\u0432\u043e\u0441\u0435\u043c\u044c', '\u0432\u043e\u0441\u044c\u043c\u043e\u0439', '\u0432\u0432\u0435\u0440\u0445', '\u0432\u0430\u043c', '\u0432\u0430\u043c\u0438', '\u0432\u0430\u0436\u043d\u043e\u0435', '\u0432\u0430\u0436\u043d\u0430\u044f', '\u0432\u0430\u0436\u043d\u044b\u0435', '\u0432\u0430\u0436\u043d\u044b\u0439', '\u0432\u0434\u0430\u043b\u0438', '\u0432\u0435\u0437\u0434\u0435', '\u0432\u0435\u0434\u044c', '\u0432\u0430\u0441', '\u0432\u0430\u0448', '\u0432\u0430\u0448\u0430', '\u0432\u0430\u0448\u0435', '\u0432\u0430\u0448\u0438', '\u0432\u043f\u0440\u043e\u0447\u0435\u043c', '\u0432\u0435\u0441\u044c', '\u0432\u0434\u0440\u0443\u0433', '\u0432\u044b', '\u0432\u0441\u0435', '\u0432\u0442\u043e\u0440\u043e\u0439', '\u0432\u0441\u0435\u043c', '\u0432\u0441\u0435\u043c\u0438', '\u0432\u0440\u0435\u043c\u0435\u043d\u0438', '\u0432\u0440\u0435\u043c\u044f', '\u0432\u0441\u0435\u043c\u0443', '\u0432\u0441\u0435\u0433\u043e', '\u0432\u0441\u0435\u0433\u0434\u0430', '\u0432\u0441\u0435\u0445', '\u0432\u0441\u0435\u044e', '\u0432\u0441\u044e', '\u0432\u0441\u044f', '\u0432\u0441\u0451', '\u0432\u0441\u044e\u0434\u0443', '\u0433', '\u0433\u043e\u0434', '\u0433\u043e\u0432\u043e\u0440\u0438\u043b', '\u0433\u043e\u0432\u043e\u0440\u0438\u0442', '\u0433\u043e\u0434\u0430', '\u0433\u043e\u0434\u0443', '\u0433\u0434\u0435', '\u0434\u0430', '\u0435\u0435', '\u0437\u0430', '\u0438\u0437', '\u043b\u0438', '\u0436\u0435', '\u0438\u043c', '\u0434\u043e', '\u043f\u043e', '\u0438\u043c\u0438', '\u043f\u043e\u0434', '\u0438\u043d\u043e\u0433\u0434\u0430', '\u0434\u043e\u0432\u043e\u043b\u044c\u043d\u043e', '\u0438\u043c\u0435\u043d\u043d\u043e', '\u0434\u043e\u043b\u0433\u043e', '\u043f\u043e\u0437\u0436\u0435', '\u0431\u043e\u043b\u0435\u0435', '\u0434\u043e\u043b\u0436\u043d\u043e', '\u043f\u043e\u0436\u0430\u043b\u0443\u0439\u0441\u0442\u0430', '\u0437\u043d\u0430\u0447\u0438\u0442', '\u0438\u043c\u0435\u0442\u044c', '\u0431\u043e\u043b\u044c\u0448\u0435', '\u043f\u043e\u043a\u0430', '\u0435\u043c\u0443', '\u0438\u043c\u044f', '\u043f\u043e\u0440', '\u043f\u043e\u0440\u0430', '\u043f\u043e\u0442\u043e\u043c', '\u043f\u043e\u0442\u043e\u043c\u0443', '\u043f\u043e\u0441\u043b\u0435', '\u043f\u043e\u0447\u0435\u043c\u0443', '\u043f\u043e\u0447\u0442\u0438', '\u043f\u043e\u0441\u0440\u0435\u0434\u0438', '\u0435\u0439', '\u0434\u0432\u0430', '\u0434\u0432\u0435', '\u0434\u0432\u0435\u043d\u0430\u0434\u0446\u0430\u0442\u044c', '\u0434\u0432\u0435\u043d\u0430\u0434\u0446\u0430\u0442\u044b\u0439', '\u0434\u0432\u0430\u0434\u0446\u0430\u0442\u044c', '\u0434\u0432\u0430\u0434\u0446\u0430\u0442\u044b\u0439', '\u0434\u0432\u0443\u0445', '\u0435\u0433\u043e', '\u0434\u0435\u043b', '\u0438\u043b\u0438', '\u0431\u0435\u0437', '\u0434\u0435\u043d\u044c', '\u0437\u0430\u043d\u044f\u0442', '\u0437\u0430\u043d\u044f\u0442\u0430', '\u0437\u0430\u043d\u044f\u0442\u043e', '\u0437\u0430\u043d\u044f\u0442\u044b', '\u0434\u0435\u0439\u0441\u0442\u0432\u0438\u0442\u0435\u043b\u044c\u043d\u043e', '\u0434\u0430\u0432\u043d\u043e', '\u0434\u0435\u0432\u044f\u0442\u043d\u0430\u0434\u0446\u0430\u0442\u044c', '\u0434\u0435\u0432\u044f\u0442\u043d\u0430\u0434\u0446\u0430\u0442\u044b\u0439', '\u0434\u0435\u0432\u044f\u0442\u044c', '\u0434\u0435\u0432\u044f\u0442\u044b\u0439', '\u0434\u0430\u0436\u0435', '\u0430\u043b\u043b\u043e', '\u0436\u0438\u0437\u043d\u044c', '\u0434\u0430\u043b\u0435\u043a\u043e', '\u0431\u043b\u0438\u0437\u043a\u043e', '\u0437\u0434\u0435\u0441\u044c', '\u0434\u0430\u043b\u044c\u0448\u0435', '\u0434\u043b\u044f', '\u043b\u0435\u0442', '\u0437\u0430\u0442\u043e', '\u0434\u0430\u0440\u043e\u043c', '\u043f\u0435\u0440\u0432\u044b\u0439', '\u043f\u0435\u0440\u0435\u0434', '\u0437\u0430\u0442\u0435\u043c', '\u0437\u0430\u0447\u0435\u043c', '\u043b\u0438\u0448\u044c', '\u0434\u0435\u0441\u044f\u0442\u044c', '\u0434\u0435\u0441\u044f\u0442\u044b\u0439', '\u0435\u044e', '\u0435\u0451', '\u0438\u0445', '\u0431\u044b', '\u0435\u0449\u0435', '\u043f\u0440\u0438', '\u0431\u044b\u043b', '\u043f\u0440\u043e', '\u043f\u0440\u043e\u0446\u0435\u043d\u0442\u043e\u0432', '\u043f\u0440\u043e\u0442\u0438\u0432', '\u043f\u0440\u043e\u0441\u0442\u043e', '\u0431\u044b\u0432\u0430\u0435\u0442', '\u0431\u044b\u0432\u044c', '\u0435\u0441\u043b\u0438', '\u043b\u044e\u0434\u0438', '\u0431\u044b\u043b\u0430', '\u0431\u044b\u043b\u0438', '\u0431\u044b\u043b\u043e', '\u0431\u0443\u0434\u0435\u043c', '\u0431\u0443\u0434\u0435\u0442', '\u0431\u0443\u0434\u0435\u0442\u0435', '\u0431\u0443\u0434\u0435\u0448\u044c', '\u043f\u0440\u0435\u043a\u0440\u0430\u0441\u043d\u043e', '\u0431\u0443\u0434\u0443', '\u0431\u0443\u0434\u044c', '\u0431\u0443\u0434\u0442\u043e', '\u0431\u0443\u0434\u0443\u0442', '\u0435\u0449\u0451', '\u043f\u044f\u0442\u043d\u0430\u0434\u0446\u0430\u0442\u044c', '\u043f\u044f\u0442\u043d\u0430\u0434\u0446\u0430\u0442\u044b\u0439', '\u0434\u0440\u0443\u0433\u043e', '\u0434\u0440\u0443\u0433\u043e\u0435', '\u0434\u0440\u0443\u0433\u043e\u0439', '\u0434\u0440\u0443\u0433\u0438\u0435', '\u0434\u0440\u0443\u0433\u0430\u044f', '\u0434\u0440\u0443\u0433\u0438\u0445', '\u0435\u0441\u0442\u044c', '\u043f\u044f\u0442\u044c', '\u0431\u044b\u0442\u044c', '\u043b\u0443\u0447\u0448\u0435', '\u043f\u044f\u0442\u044b\u0439', '\u043a', '\u043a\u043e\u043c', '\u043a\u043e\u043d\u0435\u0447\u043d\u043e', '\u043a\u043e\u043c\u0443', '\u043a\u043e\u0433\u043e', '\u043a\u043e\u0433\u0434\u0430', '\u043a\u043e\u0442\u043e\u0440\u043e\u0439', '\u043a\u043e\u0442\u043e\u0440\u043e\u0433\u043e', '\u043a\u043e\u0442\u043e\u0440\u0430\u044f', '\u043a\u043e\u0442\u043e\u0440\u044b\u0435', '\u043a\u043e\u0442\u043e\u0440\u044b\u0439', '\u043a\u043e\u0442\u043e\u0440\u044b\u0445', '\u043a\u0435\u043c', '\u043a\u0430\u0436\u0434\u043e\u0435', '\u043a\u0430\u0436\u0434\u0430\u044f', '\u043a\u0430\u0436\u0434\u044b\u0435', '\u043a\u0430\u0436\u0434\u044b\u0439', '\u043a\u0430\u0436\u0435\u0442\u0441\u044f', '\u043a\u0430\u043a', '\u043a\u0430\u043a\u043e\u0439', '\u043a\u0430\u043a\u0430\u044f', '\u043a\u0442\u043e', '\u043a\u0440\u043e\u043c\u0435', '\u043a\u0443\u0434\u0430', '\u043a\u0440\u0443\u0433\u043e\u043c', '\u0441', '\u0442', '\u0443', '\u044f', '\u0442\u0430', '\u0442\u0435', '\u0443\u0436', '\u0441\u043e', '\u0442\u043e', '\u0442\u043e\u043c', '\u0441\u043d\u043e\u0432\u0430', '\u0442\u043e\u043c\u0443', '\u0441\u043e\u0432\u0441\u0435\u043c', '\u0442\u043e\u0433\u043e', '\u0442\u043e\u0433\u0434\u0430', '\u0442\u043e\u0436\u0435', '\u0441\u043e\u0431\u043e\u0439', '\u0442\u043e\u0431\u043e\u0439', '\u0441\u043e\u0431\u043e\u044e', '\u0442\u043e\u0431\u043e\u044e', '\u0441\u043d\u0430\u0447\u0430\u043b\u0430', '\u0442\u043e\u043b\u044c\u043a\u043e', '\u0443\u043c\u0435\u0442\u044c', '\u0442\u043e\u0442', '\u0442\u043e\u044e', '\u0445\u043e\u0440\u043e\u0448\u043e', '\u0445\u043e\u0442\u0435\u0442\u044c', '\u0445\u043e\u0447\u0435\u0448\u044c', '\u0445\u043e\u0442\u044c', '\u0445\u043e\u0442\u044f', '\u0441\u0432\u043e\u0435', '\u0441\u0432\u043e\u0438', '\u0442\u0432\u043e\u0439', '\u0441\u0432\u043e\u0435\u0439', '\u0441\u0432\u043e\u0435\u0433\u043e', '\u0441\u0432\u043e\u0438\u0445', '\u0441\u0432\u043e\u044e', '\u0442\u0432\u043e\u044f', '\u0442\u0432\u043e\u0451', '\u0440\u0430\u0437', '\u0443\u0436\u0435', '\u0441\u0430\u043c', '\u0442\u0430\u043c', '\u0442\u0435\u043c', '\u0447\u0435\u043c', '\u0441\u0430\u043c\u0430', '\u0441\u0430\u043c\u0438', '\u0442\u0435\u043c\u0438', '\u0441\u0430\u043c\u043e', '\u0440\u0430\u043d\u043e', '\u0441\u0430\u043c\u043e\u043c', '\u0441\u0430\u043c\u043e\u043c\u0443', '\u0441\u0430\u043c\u043e\u0439', '\u0441\u0430\u043c\u043e\u0433\u043e', '\u0441\u0435\u043c\u043d\u0430\u0434\u0446\u0430\u0442\u044c', '\u0441\u0435\u043c\u043d\u0430\u0434\u0446\u0430\u0442\u044b\u0439', '\u0441\u0430\u043c\u0438\u043c', '\u0441\u0430\u043c\u0438\u043c\u0438', '\u0441\u0430\u043c\u0438\u0445', '\u0441\u0430\u043c\u0443', '\u0441\u0435\u043c\u044c', '\u0447\u0435\u043c\u0443', '\u0440\u0430\u043d\u044c\u0448\u0435', '\u0441\u0435\u0439\u0447\u0430\u0441', '\u0447\u0435\u0433\u043e', '\u0441\u0435\u0433\u043e\u0434\u043d\u044f', '\u0441\u0435\u0431\u0435', '\u0442\u0435\u0431\u0435', '\u0441\u0435\u0430\u043e\u0439', '\u0447\u0435\u043b\u043e\u0432\u0435\u043a', '\u0440\u0430\u0437\u0432\u0435', '\u0442\u0435\u043f\u0435\u0440\u044c', '\u0441\u0435\u0431\u044f', '\u0442\u0435\u0431\u044f', '\u0441\u0435\u0434\u044c\u043c\u043e\u0439', '\u0441\u043f\u0430\u0441\u0438\u0431\u043e', '\u0441\u043b\u0438\u0448\u043a\u043e\u043c', '\u0442\u0430\u043a', '\u0442\u0430\u043a\u043e\u0435', '\u0442\u0430\u043a\u043e\u0439', '\u0442\u0430\u043a\u0438\u0435', '\u0442\u0430\u043a\u0436\u0435', '\u0442\u0430\u043a\u0430\u044f', '\u0441\u0438\u0445', '\u0442\u0435\u0445', '\u0447\u0430\u0449\u0435', '\u0447\u0435\u0442\u0432\u0435\u0440\u0442\u044b\u0439', '\u0447\u0435\u0440\u0435\u0437', '\u0447\u0430\u0441\u0442\u043e', '\u0448\u0435\u0441\u0442\u043e\u0439', '\u0448\u0435\u0441\u0442\u043d\u0430\u0434\u0446\u0430\u0442\u044c', '\u0448\u0435\u0441\u0442\u043d\u0430\u0434\u0446\u0430\u0442\u044b\u0439', '\u0448\u0435\u0441\u0442\u044c', '\u0447\u0435\u0442\u044b\u0440\u0435', '\u0447\u0435\u0442\u044b\u0440\u043d\u0430\u0434\u0446\u0430\u0442\u044c', '\u0447\u0435\u0442\u044b\u0440\u043d\u0430\u0434\u0446\u0430\u0442\u044b\u0439', '\u0441\u043a\u043e\u043b\u044c\u043a\u043e', '\u0441\u043a\u0430\u0437\u0430\u043b', '\u0441\u043a\u0430\u0437\u0430\u043b\u0430', '\u0441\u043a\u0430\u0437\u0430\u0442\u044c', '\u0442\u0443', '\u0442\u044b', '\u0442\u0440\u0438', '\u044d\u0442\u0430', '\u044d\u0442\u0438', '\u0447\u0442\u043e', '\u044d\u0442\u043e', '\u0447\u0442\u043e\u0431', '\u044d\u0442\u043e\u043c', '\u044d\u0442\u043e\u043c\u0443', '\u044d\u0442\u043e\u0439', '\u044d\u0442\u043e\u0433\u043e', '\u0447\u0442\u043e\u0431\u044b', '\u044d\u0442\u043e\u0442', '\u0441\u0442\u0430\u043b', '\u0442\u0443\u0434\u0430', '\u044d\u0442\u0438\u043c', '\u044d\u0442\u0438\u043c\u0438', '\u0440\u044f\u0434\u043e\u043c', '\u0442\u0440\u0438\u043d\u0430\u0434\u0446\u0430\u0442\u044c', '\u0442\u0440\u0438\u043d\u0430\u0434\u0446\u0430\u0442\u044b\u0439', '\u044d\u0442\u0438\u0445', '\u0442\u0440\u0435\u0442\u0438\u0439', '\u0442\u0443\u0442', '\u044d\u0442\u0443', '\u0441\u0443\u0442\u044c', '\u0447\u0443\u0442\u044c', '\u0442\u044b\u0441\u044f\u0447']\n\n# from collections import Counter \n# import string \n# punc = string.punctuation \n\n# def clean_text(txt):    \n#     txt = txt.lower()\n#     txt = \"\".join(x for x in txt if x not in punc)\n#     words = txt.split()\n#     words = [wrd for wrd in words if wrd not in stopwords]\n#     words = [wrd for wrd in words if len(wrd) > 1]\n#     txt = \" \".join(words)\n#     return txt\n\n# def ngrams(txt, n):\n#     txt = txt.split()\n#     output = []\n#     for i in range(len(txt)-n+1):\n#         output.append(\" \".join(txt[i:i+n]))\n#     return output\n\n# good_translated = ['any complexity', 'experience', 'type of work', 'Experience', 'All types', 'Offered service', 'The price is negotiable', 'quickly high quality', 'work of any', 'home appliances', 'questions telephone', 'approach to each', 'Rent rent', 'Quickly high quality', 'Extensive experience', 'experience', 'Individual approach', 'washing machine', 'construction waste', 'each client', 'the price is negotiable', 'city area', 'clearing settlement', 'Call any', 'short time', 'garbage disposal', 'long term', 'paving slab', '20 tons', 'finishing work', 'finishing work', 'Call agree', 'check out the house', 'All issues', 'welding work', '500 rubles', 'switch sockets', 'extensive experience', 'building work', 'high quality low price', 'Offered service', 'any convenient', 'bad habit', 'convenient for You', 'Departure is possible', 'ceiling walls', 'Our company', 'individual approach', 'Nail extension', '300 RUB', 'Work weekends', 'Garbage disposal', 'You can', 'interior door', 'types of construction', 'types of finishing', 'extensive experience', 'low price', 'any kind', '500 rubles', 'shortest time', 'dishwasher', '15 tons', 'range of services', 'cash non-cash', 'offered service', 'work 10', 'Without days', 'rubles per hour', 'qualitatively expensive', 'executed work', 'washing machine', 'Cash non-cash', 'construction work', '300 rubles', 'building material', 'installation of doors', 'eyelash extension', '10 tons', 'All work', 'the gel Polish', 'system of discounts', 'Provide services', '100 rubles', '1000 rubles', 'shower cabin', 'office moving', 'We work', 'wallpapering', 'Departure of the specialist', 'removal of construction', 'Cargo transportation to the city', 'Quality assurance', '200 rubles', 'apartment houses', 'the alignment of the walls', 'any area', 'Departure of the master', 'roofing work', 'quality repair']\n# bad_translated = ['As a unit', 'fit to size', 'Android smartphone', 'state of new', '400 rubles', 'land plot', 'room apartment', 'Will sell a new', 'GB slot', 'On growth', 'Sell dress', 'view of the move', 'In the photo', 'Operating time', 'battery charger', 'State of new', 'see In', 'genuine leather', 'small bargain', 'Very convenient', 'public transport', 'On a plot', 'Complete set', 'GB memory', '300 RUB', 'Non-negotiable', 'cm Weight', 'see Sell', 'appearance', '150 RUB', 'rest of russia', 'room apartment', 'availability order', 'trades Sell', 'At purchase', 'card slot', 'Will sell a jacket -', 'Whole range', 'able Sell', '200 rubles', 'We will be glad', 'Our address', 'The price is negotiable', 'bargaining is appropriate', 'condition size', 'Within walking distance', 'plastic Windows', 'state In', 'operational volume', 'cm length', 'condition Size', 'Boo good', 'Reason for sale', 'Good condition', 'wifi bluetooth', 'our shop', 'Sell new', 'On issues', 'On insole', 'storage card', 'Excellent condition', 'In the apartment', '100 cotton', 'All issues', 'Sell new', 'our shop', 'perfect condition', 'photo in sight', 'kindergarten', 'wide choice', '100 rubles', '500 rubles', 'Genuine leather', 'Delivery is possible', 'Perfect condition', 'The price shown', 'As a gift', 'our site', 'RAM', 'It is possible to bargain', 'genuine leather', 'You can', 'questions telephone', 'Bargaining is appropriate', 'working condition', 'able will Sell', 'walking distance', 'good condition', 'excellent condition', 'In a perfect', 'As a unit', 'In stock', 'Good condition', 'Excellent condition', 'In good', 'conditin is', 'In excellent', 'perfect condition', 'good condition', 'excellent condition']\n# bad_translated = list(reversed(bad_translated))\n\n# def get_bigrams_data(txt, tag, col, translated_list):\n#     txt = clean_text(txt)\n#     all_bigrams = ngrams(txt, 2)\n#     topbigrams = Counter(all_bigrams).most_common(100)\n    \n#     xvals = [translated_list[i] for i in range(len(topbigrams))]    \n#     # xvals = list(reversed([_[0] for _ in topbigrams]))\n#     yvals = list(reversed([_[1] for _ in topbigrams]))\n#     trace = go.Bar(x=yvals, y=xvals, name=tag, marker=dict(color=col), xaxis=dict(linecolor='#fff',), opacity=0.7, orientation='h')\n#     return trace\n\n# txt = \" \".join(good_performing_ads.description)\n# good_tr1 = get_bigrams_data(txt, 'Top Ngrams used in Items with High Deal Percentage', '#bbf783', good_translated)\n\n# txt = \" \".join(bad_performing_ads.description[:10000])\n# bad_tr1 = get_bigrams_data(txt, 'Top Ngrams used in Items with Low Deal Percentage', '#f78396', bad_translated)\n\n# fig = tools.make_subplots(rows=1, cols=2, print_grid=False);\n# fig.append_trace(good_tr1, 1, 1);\n# fig.append_trace(bad_tr1, 1, 2);\n\n# fig['layout'].update(height=1000, margin=dict(l=200), title='', \n#                      legend=dict(x=0.1, y=1.2));\n# iplot(fig, filename='simple-subplot');", "processed": ["id 8 3 8 3 top n gram use descript"]}, {"markdown": ["## Data Description\n- Each row represents a player at a given moment in time.\n- Each 22 players participating in a given play have a row.\n\nFrom the official description:\n```\nEach row in the file corresponds to a single player's involvement in a single play.\nThe dataset was intentionally joined (i.e. denormalized) to make the API simple.\nAll the columns are contained in one large dataframe which is grouped and provided by PlayId.\n```", "## Yards *The target we are trying to predict*\nIt's always smart to take a close look at the variable we are trying to predict."], "code": "# Reference: https://www.kaggle.com/code/robikscube/big-data-bowl-comprehensive-eda-with-pandas\n\ntrain.groupby('PlayId').first()['Yards'] \\\n    .plot(kind='hist',\n          figsize=(15, 5),\n          bins=50,\n          title='Distribution of Yards Gained (Target)')\nplt.show()", "processed": ["data descript row repres player given moment time 22 player particip given play row offici descript row file correspond singl player involv singl play dataset intent join e denorm make api simpl column contain one larg datafram group provid playid", "yard target tri predict alway smart take close look variabl tri predict"]}, {"markdown": ["## Yards gained by Down"], "code": "# Reference: https://www.kaggle.com/code/robikscube/big-data-bowl-comprehensive-eda-with-pandas\n\nfig, axes = plt.subplots(4, 1, figsize=(15, 8), sharex=True)\nn = 0\nfor i, d in train.groupby('Down'):\n    d['Yards'].plot(kind='hist',\n                    bins=30,\n                   color=color_pal[n],\n                   ax=axes[n],\n                   title=f'Yards Gained on down {i}')\n    n+=1", "processed": ["yard gain"]}, {"markdown": ["## Yards gained by Distance-to-Gain\nWe can see that there appears to be a increase in the average yards gained as the distance to gain increases. We also can see that as the distances increase the distribution of `Yards` moves from a normal distribution to bimodal. This could be because of sparsity of data for the extremely large distance-to-gain values."], "code": "# Reference: https://www.kaggle.com/code/robikscube/big-data-bowl-comprehensive-eda-with-pandas\n\nfig, ax = plt.subplots(figsize=(20, 5))\nsns.violinplot(x='Distance-to-Gain',\n               y='Yards',\n               data=train.rename(columns={'Distance':'Distance-to-Gain'}),\n               ax=ax)\nplt.ylim(-10, 20)\nplt.title('Yards vs Distance-to-Gain')\nplt.show()", "processed": ["yard gain distanc gain see appear increas averag yard gain distanc gain increas also see distanc increas distribut yard move normal distribut bimod could sparsiti data extrem larg distanc gain valu"]}, {"markdown": ["(Thanks @arnabbiswas1 for pointing out an error in this plot that I've now fixed.)"], "code": "# Reference: https://www.kaggle.com/code/robikscube/big-data-bowl-comprehensive-eda-with-pandas\n\ntrain.groupby('GameId')['PlayId'] \\\n    .nunique() \\\n    .plot(kind='hist', figsize=(15, 5),\n         title='Distribution of Plays per GameId',\n         bins=50)\nplt.show()", "processed": ["thank arnabbiswas1 point error plot fix"]}, {"markdown": ["## Down and Distance\n- We can see the majority of running plays occur on first down. This is not unexpected as running plays are much more common in earlier downs."], "code": "# Reference: https://www.kaggle.com/code/robikscube/big-data-bowl-comprehensive-eda-with-pandas\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\nsns.boxplot(data=train.groupby('PlayId').first()[['Distance','Down']],\n            x='Down', y='Distance', ax=ax1)\nax1.set_title('Distance-to-Gain by Down')\nsns.boxplot(data=train.groupby('PlayId').first()[['Yards','Down']],\n            x='Down', y='Yards', ax=ax2)\nax2.set_title('Yards Gained by Down')\nplt.show()", "processed": ["distanc see major run play occur first unexpect run play much common earlier down"]}, {"markdown": ["## Distance to gain is commonly 10 yards"], "code": "# Reference: https://www.kaggle.com/code/robikscube/big-data-bowl-comprehensive-eda-with-pandas\n\ntrain['Distance'].plot(kind='hist',\n                       title='Distribution of Distance to Go',\n                       figsize=(15, 5),\n                       bins=30,\n                       color=color_pal[2])\nplt.show()", "processed": ["distanc gain commonli 10 yard"]}, {"markdown": ["## Speed, Acceleration, and Distance\nWe are provided with the speed, acceleration, and distance each player has traveled since the previous point."], "code": "# Reference: https://www.kaggle.com/code/robikscube/big-data-bowl-comprehensive-eda-with-pandas\n\nfig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(16, 4))\ntrain['S'].plot(kind='hist', ax=ax1,\n                title='Distribution of Speed',\n                bins=20,\n                color=color_pal[0])\ntrain['A'].plot(kind='hist',\n                ax=ax2,\n                title='Distribution of Acceleration',\n                bins=20,\n                color=color_pal[1])\ntrain['Dis'].plot(kind='hist',\n                  ax=ax3,\n                  title='Distribution of Distance',\n                  bins=20,\n                  color=color_pal[2])\nplt.show()\nfig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(16, 4))\ntrain.query(\"NflIdRusher == NflId\")['S'] \\\n    .plot(kind='hist',\n          ax=ax1,\n          title='Distribution of Speed (Ball Carrier Only)',\n          bins=20,\n          color=color_pal[0])\ntrain.query(\"NflIdRusher == NflId\")['A'] \\\n    .plot(kind='hist',\n          ax=ax2,\n          title='Distribution of Acceleration (Ball Carrier Only)',\n          bins=20,\n          color=color_pal[1])\ntrain.query(\"NflIdRusher == NflId\")['Dis'] \\\n    .plot(kind='hist',\n          ax=ax3,\n          title='Distribution of Distance (Ball Carrier Only)',\n          bins=20,\n          color=color_pal[2])\nplt.show()", "processed": ["speed acceler distanc provid speed acceler distanc player travel sinc previou point"]}, {"markdown": ["## Does Speed, Acceleration, and Distance of the runningback have a relationship with yards gained?\nLets look and see if the speed of the runningback correlates with the yardage gained. The color shows the different defensive personnels in each run.\n\nIt's not immediately clear if these features have a meaningful relationship with the yards gained."], "code": "# Reference: https://www.kaggle.com/code/robikscube/big-data-bowl-comprehensive-eda-with-pandas\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n\nsns.pairplot(train.query(\"NflIdRusher == NflId\").sample(1000)[['S','Dis','A','Yards','DefensePersonnel']],\n            hue='DefensePersonnel')\nplt.suptitle('Speed, Acceleration, Distance traveled, and Yards Gained for Rushers')\nplt.show()", "processed": ["speed acceler distanc runningback relationship yard gain let look see speed runningback correl yardag gain color show differ defens personnel run immedi clear featur meaning relationship yard gain"]}, {"markdown": ["# OffensePersonnel / DefensePersonnel\nLets see what the top personnel groupings are for the offense and defense"], "code": "# Reference: https://www.kaggle.com/code/robikscube/big-data-bowl-comprehensive-eda-with-pandas\n\nfig, ax = plt.subplots(1, 2, figsize=(20, 8))\ntrain.groupby('PlayId') \\\n    .first() \\\n    .groupby('OffensePersonnel') \\\n    .count()['GameId'] \\\n    .sort_values(ascending=False) \\\n    .head(15) \\\n    .sort_values() \\\n    .plot(kind='barh',\n         title='Offense Personnel # of Plays',\n         ax=ax[0])\ntrain.groupby('PlayId') \\\n    .first() \\\n    .groupby('DefensePersonnel') \\\n    .count()['GameId'] \\\n    .sort_values(ascending=False) \\\n    .head(15) \\\n    .sort_values() \\\n    .plot(kind='barh',\n         title='Defense Personnel # of Plays',\n         ax=ax[1])\nplt.show()", "processed": ["offensepersonnel defensepersonnel let see top personnel group offens defens"]}, {"markdown": ["## Defensive Personnel's impact on yard gained\nWe can see that there are about 5 common defensive packages that are used. How does the way the defense is aligned correlate with the offensive production (yards gained)?\n\nWhat stands out at first glance is that the `4DL - 4LB - 3DB` Defense shows a different distribution in yards gained.\n\nPer wikipedia: https://en.wikipedia.org/wiki/4%E2%80%934_defense\n\n*Originally seen as a passing defense against the spread, modern versions of the 4-4 are attacking defenses stocked with multiple blitz packages that can easily be concealed and altered.*"], "code": "# Reference: https://www.kaggle.com/code/robikscube/big-data-bowl-comprehensive-eda-with-pandas\n\ntop_10_defenses = train.groupby('DefensePersonnel')['GameId'] \\\n    .count() \\\n    .sort_values(ascending=False).index[:10] \\\n    .tolist()\ntrain_play = train.groupby('PlayId').first()\ntrain_top10_def = train_play.loc[train_play['DefensePersonnel'].isin(top_10_defenses)]\n\nfig, ax = plt.subplots(figsize=(15, 5))\nsns.violinplot(x='DefensePersonnel',\n               y='Yards',\n               data=train_top10_def,\n               ax=ax)\nplt.ylim(-10, 20)\nplt.title('Yards vs Defensive Personnel')\nplt.show()", "processed": ["defens personnel impact yard gain see 5 common defens packag use way defens align correl offens product yard gain stand first glanc 4dl 4lb 3db defens show differ distribut yard gain per wikipedia http en wikipedia org wiki 4 e2 80 934 defens origin seen pas defens spread modern version 4 4 attack defens stock multipl blitz packag easili conceal alter"]}, {"markdown": ["## Running strategies change as the game goes on...\n\nHow are the yards gained impacted by the time in the game? Many times teams run the ball at the end of the game when they are ahead, in order to run out the gameclock and win. In these situations the run is expected more and defenses can scheme against it.\n\nIt doesn't look like the quarter has a huge impact on the running yards gained."], "code": "# Reference: https://www.kaggle.com/code/robikscube/big-data-bowl-comprehensive-eda-with-pandas\n\nfig, ax = plt.subplots(figsize=(15, 5))\nax.set_ylim(-10, 60)\nax.set_title('Yards vs Quarter')\nsns.boxenplot(x='Quarter',\n            y='Yards',\n            data=train.sample(5000),\n            ax=ax)\nplt.show()", "processed": ["run strategi chang game goe yard gain impact time game mani time team run ball end game ahead order run gameclock win situat run expect defens scheme look like quarter huge impact run yard gain"]}, {"markdown": ["# Defenders In The \"Box\"\n\nThe number of defenders in the box is an important part of stopping the running game. Typically defenses will add more players to this area of the field when they really want to stop a run, this comes at a cost leaving wide recievers less covered.\n\n![](https://i0.wp.com/www.footballzebras.com/wp-content/uploads/2019/02/Slide1.jpg?resize=596%2C317)", "Wow! This plot shows a big difference in yards gained when looking at the number of defenders in the box. If you've got 8+ defenders in the box you're looking to stop the run big time! And you can see the average rush yardage is lower. Conversely having 3 men in the box (maybe because they are in prevent defense for a long yard to gain) allows for a average return of about 10 yards!"], "code": "# Reference: https://www.kaggle.com/code/robikscube/big-data-bowl-comprehensive-eda-with-pandas\n\nfig, ax = plt.subplots(figsize=(15, 5))\nax.set_ylim(-10, 60)\nsns.boxenplot(x='DefendersInTheBox',\n               y='Yards',\n               data=train.query('DefendersInTheBox > 2'),\n               ax=ax)\nplt.title('Yards vs Defenders in the Box')\nplt.show()", "processed": ["defend box number defend box import part stop run game typic defens add player area field realli want stop run come cost leav wide reciev le cover http i0 wp com www footballzebra com wp content upload 2019 02 slide1 jpg resiz 596 2c317", "wow plot show big differ yard gain look number defend box got 8 defend box look stop run big time see averag rush yardag lower convers 3 men box mayb prevent defens long yard gain allow averag return 10 yard"]}, {"markdown": ["# Distribution of Yards gained vs Defenders in the Box\nWe can clearly see some variation in yards gained depending on the number of defenders in the box. "], "code": "# Reference: https://www.kaggle.com/code/robikscube/big-data-bowl-comprehensive-eda-with-pandas\n\nfig, axes = plt.subplots(3, 2, constrained_layout=True, figsize=(15 , 10))\n#fig.tight_layout()\nax_idx = 0\nax_idx2 = 0\nfor i in range(4, 10):\n    this_ax = axes[ax_idx2][ax_idx]\n    #print(ax_idx, ax_idx2)\n    sns.distplot(train.query('DefendersInTheBox == @i')['Yards'],\n                ax=this_ax,\n                color=color_pal[ax_idx2])\n    this_ax.set_title(f'{i} Defenders in the box')\n    this_ax.set_xlim(-10, 20)\n    ax_idx += 1\n    if ax_idx == 2:\n        ax_idx = 0\n        ax_idx2 += 1\nplt.show()", "processed": ["distribut yard gain v defend box clearli see variat yard gain depend number defend box"]}, {"markdown": ["# What Ball Carriers stand out?\n> Lets now look at ball carriers (the players who typically are handed off the ball) and see if any individual players stand out. We will only look at players with more than 100 plays. Then we can plot the top and bottom 10 players."], "code": "# Reference: https://www.kaggle.com/code/robikscube/big-data-bowl-comprehensive-eda-with-pandas\n\ntrain.query(\"NflIdRusher == NflId\") \\\n    .groupby('DisplayName')['Yards'] \\\n    .agg(['count','mean']) \\\n    .query('count > 100') \\\n    .sort_values('mean', ascending=True) \\\n    .tail(10)['mean'] \\\n    .plot(kind='barh',\n          figsize=(15, 5),\n          color=color_pal[5],\n          xlim=(0,6),\n          title='Top 10 Players by Average Yards')\nplt.show()\ntrain.query(\"NflIdRusher == NflId\") \\\n    .groupby('DisplayName')['Yards'] \\\n    .agg(['count','mean']) \\\n    .query('count > 100') \\\n    .sort_values('mean', ascending=True) \\\n    .head(10)['mean'] \\\n    .plot(kind='barh',\n          figsize=(15, 5),\n          color=color_pal[0],\n          xlim=(0,6),\n          title='Bottom 10 Players by Average Yards')\nplt.show()", "processed": ["ball carrier stand let look ball carrier player typic hand ball see individu player stand look player 100 play plot top bottom 10 player"]}, {"markdown": ["# 3-4 vs 4-3 vs 4-2 (nickel) vs 2-4 Defense - Impact on Yards gained?\n\nNext we will use the defensive scheme data to see how the difference in down linemen impacts the yards gained.\nWhile there are many defensive packages, commonly defenses will run a 4-3 or 3-4 defense.\n\nYou can read more about them here: https://bleacherreport.com/articles/1289011-showcasing-the-biggest-differences-in-the-4-3-and-3-4-pass-rush\n\nBasically you have \"Down linemen\" and \"Linebackers\" the number of players you put in each of these positions determines the defensive scheme.\n\n_ | _\n- | - \n![](https://usercontent2.hubstatic.com/14167861_f496.jpg) | ![](https://img.bleacherreport.net/img/images/photos/002/822/262/07d6727c91db0f152422689a0c095c4c_crop_north.jpg?h=533&w=800&q=70&crop_x=center&crop_y=top)\n![](https://usercontent2.hubstatic.com/14148753_f496.jpg) | ![](https://img.bleacherreport.net/img/images/photos/002/811/505/1f3630fb49f2e47dd3d62f01f6956940_crop_north.png?1395418910&w=630&h=420)"], "code": "# Reference: https://www.kaggle.com/code/robikscube/big-data-bowl-comprehensive-eda-with-pandas\n\n# Create the DL-LB combos\ntrain['DL_LB'] = train['DefensePersonnel'] \\\n    .str[:10] \\\n    .str.replace(' DL, ','-') \\\n    .str.replace(' LB','') # Clean up and convert to DL-LB combo\ntop_5_dl_lb_combos = train.groupby('DL_LB').count()['GameId'] \\\n    .sort_values() \\\n    .tail(10).index.tolist()\nax = train.loc[train['DL_LB'].isin(top_5_dl_lb_combos)] \\\n    .groupby('DL_LB').mean()['Yards'] \\\n    .sort_values(ascending=True) \\\n    .plot(kind='bar',\n          title='Average Yards Top 5 Defensive DL-LB combos',\n          figsize=(15, 5),\n          color=color_pal[4])\n# for p in ax.patches:\n#     ax.annotate(str(round(p.get_height(), 2)),\n#                 (p.get_x() * 1.005, p.get_height() * 1.015))\n\n#bars = ax.bar(0.5, 5, width=0.5, align=\"center\")\nbars = [p for p in ax.patches]\nvalue_format = \"{:0.2f}\"\nlabel_bars(ax, bars, value_format, fontweight='bold')\nplt.show()", "processed": ["3 4 v 4 3 v 4 2 nickel v 2 4 defens impact yard gain next use defens scheme data see differ linemen impact yard gain mani defens packag commonli defens run 4 3 3 4 defens read http bleacherreport com articl 1289011 showcas biggest differ 4 3 3 4 pas rush basic linemen lineback number player put posit determin defens scheme http usercontent2 hubstat com 14167861 f496 jpg http img bleacherreport net img imag photo 002 822 262 07d6727c91db0f152422689a0c095c4c crop north jpg h 533 w 800 q 70 crop x center crop top http usercontent2 hubstat com 14148753 f496 jpg http img bleacherreport net img imag photo 002 811 505 1f3630fb49f2e47dd3d62f01f6956940 crop north png 1395418910 w 630 h 420"]}, {"markdown": ["## Lets Plot some defensive schemes\nUsing some of the additional code created by the great SRK (@sudalairajkumar) in this kernel: https://www.kaggle.com/sudalairajkumar/simple-exploration-notebook-nfl\n\nNote that we are given the player positions at the time the ball is handed off, so the player formation isn't as clean as in the diagrams above."], "code": "# Reference: https://www.kaggle.com/code/robikscube/big-data-bowl-comprehensive-eda-with-pandas\n\ndef create_football_field(linenumbers=True,\n                          endzones=True,\n                          highlight_line=False,\n                          highlight_line_number=50,\n                          highlighted_name='Line of Scrimmage',\n                          fifty_is_los=False,\n                          figsize=(12*2, 6.33*2)):\n    \"\"\"\n    Function that plots the football field for viewing plays.\n    Allows for showing or hiding endzones.\n    \"\"\"\n    rect = patches.Rectangle((0, 0), 120, 53.3, linewidth=0.1,\n                             edgecolor='r', facecolor='darkgreen', zorder=0)\n\n    fig, ax = plt.subplots(1, figsize=figsize)\n    ax.add_patch(rect)\n\n    plt.plot([10, 10, 10, 20, 20, 30, 30, 40, 40, 50, 50, 60, 60, 70, 70, 80,\n              80, 90, 90, 100, 100, 110, 110, 120, 0, 0, 120, 120],\n             [0, 0, 53.3, 53.3, 0, 0, 53.3, 53.3, 0, 0, 53.3, 53.3, 0, 0, 53.3,\n              53.3, 0, 0, 53.3, 53.3, 0, 0, 53.3, 53.3, 53.3, 0, 0, 53.3],\n             color='white')\n    if fifty_is_los:\n        plt.plot([60, 60], [0, 53.3], color='gold')\n        plt.text(62, 50, '<- Player Yardline at Snap', color='gold')\n    # Endzones\n    if endzones:\n        ez1 = patches.Rectangle((0, 0), 10, 53.3,\n                                linewidth=0.1,\n                                edgecolor='r',\n                                facecolor='blue',\n                                alpha=0.2,\n                                zorder=0)\n        ez2 = patches.Rectangle((110, 0), 120, 53.3,\n                                linewidth=0.1,\n                                edgecolor='r',\n                                facecolor='blue',\n                                alpha=0.2,\n                                zorder=0)\n        ax.add_patch(ez1)\n        ax.add_patch(ez2)\n    plt.xlim(0, 120)\n    plt.ylim(-5, 58.3)\n    plt.axis('off')\n    if linenumbers:\n        for x in range(20, 110, 10):\n            numb = x\n            if x > 50:\n                numb = 120 - x\n            plt.text(x, 5, str(numb - 10),\n                     horizontalalignment='center',\n                     fontsize=20,  # fontname='Arial',\n                     color='white')\n            plt.text(x - 0.95, 53.3 - 5, str(numb - 10),\n                     horizontalalignment='center',\n                     fontsize=20,  # fontname='Arial',\n                     color='white', rotation=180)\n    if endzones:\n        hash_range = range(11, 110)\n    else:\n        hash_range = range(1, 120)\n\n    for x in hash_range:\n        ax.plot([x, x], [0.4, 0.7], color='white')\n        ax.plot([x, x], [53.0, 52.5], color='white')\n        ax.plot([x, x], [22.91, 23.57], color='white')\n        ax.plot([x, x], [29.73, 30.39], color='white')\n\n    if highlight_line:\n        hl = highlight_line_number + 10\n        plt.plot([hl, hl], [0, 53.3], color='yellow')\n        plt.text(hl + 2, 50, '<- {}'.format(highlighted_name),\n                 color='yellow')\n    return fig, ax\n\nimport math\ndef get_dx_dy(angle, dist):\n    cartesianAngleRadians = (450-angle)*math.pi/180.0\n    dx = dist * math.cos(cartesianAngleRadians)\n    dy = dist * math.sin(cartesianAngleRadians)\n    return dx, dy\nplay_id = train.query(\"DL_LB == '3-4'\")['PlayId'].reset_index(drop=True)[500]\nfig, ax = create_football_field()\ntrain.query(\"PlayId == @play_id and Team == 'away'\") \\\n    .plot(x='X', y='Y', kind='scatter', ax=ax, color='orange', s=200, legend='Away')\ntrain.query(\"PlayId == @play_id and Team == 'home'\") \\\n    .plot(x='X', y='Y', kind='scatter', ax=ax, color='blue', s=200, legend='Home')\ntrain.query(\"PlayId == @play_id and NflIdRusher == NflId\") \\\n    .plot(x='X', y='Y', kind='scatter', ax=ax, color='red', s=200, legend='Rusher')\nrusher_row = train.query(\"PlayId == @play_id and NflIdRusher == NflId\")\nyards_covered = rusher_row[\"Yards\"].values[0]\n\nx = rusher_row[\"X\"].values[0]\ny = rusher_row[\"Y\"].values[0]\nrusher_dir = rusher_row[\"Dir\"].values[0]\nrusher_speed = rusher_row[\"S\"].values[0]\ndx, dy = get_dx_dy(rusher_dir, rusher_speed)\nyards_gained = train.query(\"PlayId == @play_id\")['Yards'].tolist()[0]\nax.arrow(x, y, dx, dy, length_includes_head=True, width=0.3)\nplt.title(f'Example of a 3-4 Defense - run resulted in {yards_gained} yards gained', fontsize=20)\nplt.legend()\nplt.show()\nplay_id = train.query(\"DL_LB == '4-3'\")['PlayId'].reset_index(drop=True)[500]\nfig, ax = create_football_field()\ntrain.query(\"PlayId == @play_id and Team == 'away'\") \\\n    .plot(x='X', y='Y', kind='scatter', ax=ax, color='orange', s=200, legend='Away')\ntrain.query(\"PlayId == @play_id and Team == 'home'\") \\\n    .plot(x='X', y='Y', kind='scatter', ax=ax, color='blue', s=200, legend='Home')\ntrain.query(\"PlayId == @play_id and NflIdRusher == NflId\") \\\n    .plot(x='X', y='Y', kind='scatter', ax=ax, color='red', s=200, legend='Rusher')\nrusher_row = train.query(\"PlayId == @play_id and NflIdRusher == NflId\")\nyards_covered = rusher_row[\"Yards\"].values[0]\n\nx = rusher_row[\"X\"].values[0]\ny = rusher_row[\"Y\"].values[0]\nrusher_dir = rusher_row[\"Dir\"].values[0]\nrusher_speed = rusher_row[\"S\"].values[0]\ndx, dy = get_dx_dy(rusher_dir, rusher_speed)\nyards_gained = train.query(\"PlayId == @play_id\")['Yards'].tolist()[0]\nax.arrow(x, y, dx, dy, length_includes_head=True, width=0.3)\nplt.title(f'Example of a 4-3 Defense - run resulted in {yards_gained} yard gained', fontsize=20)\nplt.legend()\nplt.show()\nplay_id = train.query(\"DL_LB == '4-2'\")['PlayId'].reset_index(drop=True)[500]\nfig, ax = create_football_field()\ntrain.query(\"PlayId == @play_id and Team == 'away'\") \\\n    .plot(x='X', y='Y', kind='scatter', ax=ax, color='orange', s=200, legend='Away')\ntrain.query(\"PlayId == @play_id and Team == 'home'\") \\\n    .plot(x='X', y='Y', kind='scatter', ax=ax, color='blue', s=200, legend='Home')\ntrain.query(\"PlayId == @play_id and NflIdRusher == NflId\") \\\n    .plot(x='X', y='Y', kind='scatter', ax=ax, color='red', s=200, legend='Rusher')\nrusher_row = train.query(\"PlayId == @play_id and NflIdRusher == NflId\")\nyards_covered = rusher_row[\"Yards\"].values[0]\n\nx = rusher_row[\"X\"].values[0]\ny = rusher_row[\"Y\"].values[0]\nrusher_dir = rusher_row[\"Dir\"].values[0]\nrusher_speed = rusher_row[\"S\"].values[0]\ndx, dy = get_dx_dy(rusher_dir, rusher_speed)\nyards_gained = train.query(\"PlayId == @play_id\")['Yards'].tolist()[0]\nax.arrow(x, y, dx, dy, length_includes_head=True, width=0.3)\nplt.title(f'Example of a 4-2 Defense - run resulted in {yards_gained} yards gained', fontsize=20)\nplt.legend()\nplt.show()", "processed": ["let plot defens scheme use addit code creat great srk sudalairajkumar kernel http www kaggl com sudalairajkumar simpl explor notebook nfl note given player posit time ball hand player format clean diagram"]}, {"markdown": ["# Snap to Handoff Time\nDifferent types of designed runs develop differently, one way to understand the play design is by looking at the time it takes the quarterback to hand the ball off to the rusher. Lets take a look at the distribution of seconds taken."], "code": "# Reference: https://www.kaggle.com/code/robikscube/big-data-bowl-comprehensive-eda-with-pandas\n\ntrain['SnapHandoffSeconds'] = (pd.to_datetime(train['TimeHandoff']) - \\\n                               pd.to_datetime(train['TimeSnap'])).dt.total_seconds()\n\n(train.groupby('SnapHandoffSeconds').count() / 22 )['GameId'].plot(kind='bar',\n                                                                   figsize=(15, 5))\nbars = [p for p in ax.patches]\nvalue_format = \"{}\"\nlabel_bars(ax, bars, value_format, fontweight='bold')\nplt.show()", "processed": ["snap handoff time differ type design run develop differ one way understand play design look time take quarterback hand ball rusher let take look distribut second taken"]}, {"markdown": ["It looks like this feature might cause some issues. Due to lack of percision we don't have much detail about the snap time. Additionally it looks like the sparcity of data for seconds that are not 1 or 2 - cause the average Yards to have large variance."], "code": "# Reference: https://www.kaggle.com/code/robikscube/big-data-bowl-comprehensive-eda-with-pandas\n\ntrain.groupby('SnapHandoffSeconds')['Yards'].mean().plot(kind='barh',\n                                                         color=color_pal[1],\n                                                         figsize=(15, 5),\n                                                         title='Average Yards Gained by SnapHandoff Seconds')\nplt.show()", "processed": ["look like featur might caus issu due lack percis much detail snap time addit look like sparciti data second 1 2 caus averag yard larg varianc"]}, {"markdown": ["## Acoustic data animation"], "code": "# Reference: https://www.kaggle.com/code/pestipeti/signal-and-frequency-data-animation\n\n%matplotlib inline\n\nad = train_df.acoustic_data.values#[::5]\nttf = train_df.ttf.values#[::5]\n\ndef animate(i):\n    line.set_ydata(ad[i*STEP_SIZE:i*STEP_SIZE + WINDOW_SIZE])\n    ax.set_xlabel('TTF {0:.8f}'.format(ttf[i*STEP_SIZE + WINDOW_SIZE]))\n\nfig, ax = plt.subplots(figsize=(9,4))\nax.set(xlim=(0,WINDOW_SIZE), ylim=(-175, 175))\n\nline = ax.plot(train_df.iloc[0:WINDOW_SIZE], lw=1)[0]\nanim = matplotlib.animation.FuncAnimation(fig, animate, frames=(len(ad) - WINDOW_SIZE) // STEP_SIZE,\n                                          interval=REFRESH_INTERVAL, repeat=True)\n\n# You can save the animation\n# anim.save('acoustic_data.gif', writer='imagemagick')\n\n# Show the animation (does not work in kernel)\n# plt.show()\n\n# Remove this plt.close() in your experiment.\nplt.close()", "processed": ["acoust data anim"]}, {"markdown": ["![SignalAnim](https://i.imgur.com/5ndR48p.gif)", "## Frequency band animation"], "code": "# Reference: https://www.kaggle.com/code/pestipeti/signal-and-frequency-data-animation\n\n# Low, High \"bandpass\" frequencies\nFREQUENCY_BAND = (45000, 55000)\n%matplotlib inline\n\ndef animate_freqs(i):\n    x = ad[i*STEP_SIZE:i*STEP_SIZE + WINDOW_SIZE]\n    frequencies, power_spectrum = signal.periodogram(x, 4000000, scaling='spectrum')\n    idx = (frequencies >= FREQUENCY_BAND[0]) & (frequencies <= FREQUENCY_BAND[1])\n    line.set_data(frequencies[idx].astype(np.int32), power_spectrum[idx])\n    ax.set_xlabel('TTF {0:.8f}'.format(ttf[i*STEP_SIZE + WINDOW_SIZE]))\n\nfig, ax = plt.subplots(figsize=(9,4))\n\n# !!!! If you don't see anything on the plot, try to adjust the `ylim` argument !!!!\nax.set(xlim=FREQUENCY_BAND, ylim=(0, .005))\nax.set_title(\"{}Hz - {}Hz\".format(FREQUENCY_BAND[0], FREQUENCY_BAND[1]))\n\nfrequencies, power_spectrum = signal.periodogram(ad[0:WINDOW_SIZE], 4000000, scaling='spectrum')      \nidx = (frequencies >= FREQUENCY_BAND[0]) & (frequencies <= FREQUENCY_BAND[1])\nline = ax.plot(frequencies[idx].astype(np.int32), power_spectrum[idx], lw=1)[0]\n\nanim = matplotlib.animation.FuncAnimation(fig, animate_freqs, frames=(len(ad) - WINDOW_SIZE) // STEP_SIZE,\n                                          interval=REFRESH_INTERVAL, repeat=True)\n\n# You can save the animation\n# anim.save('frequency_{}_{}.gif'.format(FREQUENCY_BAND[0], FREQUENCY_BAND[1]), writer='imagemagick')\n\n\n# Show the animation (does not work in kernel)\n# plt.show()\n\n# Remove this plt.close() in your experiment.\nplt.close()", "processed": ["signalanim http imgur com 5ndr48p gif", "frequenc band anim"]}, {"markdown": ["* 2 Yards has the maximum count followed by 1 and 3 yards\n* The distribution looks normal with a right skewness. There are some extreme right values which means the player has covered more than 80 yards.", "### Football Field Plot\n\nThanks to Rob Mulla for this [wonderful code](https://www.kaggle.com/robikscube/nfl-big-data-bowl-plotting-player-position) to create the plot of football field"], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-notebook-nfl\n\ndef create_football_field(linenumbers=True,\n                          endzones=True,\n                          highlight_line=False,\n                          highlight_line_number=50,\n                          highlighted_name='Line of Scrimmage',\n                          fifty_is_los=False,\n                          figsize=(12*2, 6.33*2)):\n    \"\"\"\n    Function that plots the football field for viewing plays.\n    Allows for showing or hiding endzones.\n    \"\"\"\n    rect = patches.Rectangle((0, 0), 120, 53.3, linewidth=0.1,\n                             edgecolor='r', facecolor='darkgreen', zorder=0)\n\n    fig, ax = plt.subplots(1, figsize=figsize)\n    ax.add_patch(rect)\n\n    plt.plot([10, 10, 10, 20, 20, 30, 30, 40, 40, 50, 50, 60, 60, 70, 70, 80,\n              80, 90, 90, 100, 100, 110, 110, 120, 0, 0, 120, 120],\n             [0, 0, 53.3, 53.3, 0, 0, 53.3, 53.3, 0, 0, 53.3, 53.3, 0, 0, 53.3,\n              53.3, 0, 0, 53.3, 53.3, 0, 0, 53.3, 53.3, 53.3, 0, 0, 53.3],\n             color='white')\n    if fifty_is_los:\n        plt.plot([60, 60], [0, 53.3], color='gold')\n        plt.text(62, 50, '<- Player Yardline at Snap', color='gold')\n    # Endzones\n    if endzones:\n        ez1 = patches.Rectangle((0, 0), 10, 53.3,\n                                linewidth=0.1,\n                                edgecolor='r',\n                                facecolor='blue',\n                                alpha=0.2,\n                                zorder=0)\n        ez2 = patches.Rectangle((110, 0), 120, 53.3,\n                                linewidth=0.1,\n                                edgecolor='r',\n                                facecolor='blue',\n                                alpha=0.2,\n                                zorder=0)\n        ax.add_patch(ez1)\n        ax.add_patch(ez2)\n    plt.xlim(0, 120)\n    plt.ylim(-5, 58.3)\n    plt.axis('off')\n    if linenumbers:\n        for x in range(20, 110, 10):\n            numb = x\n            if x > 50:\n                numb = 120 - x\n            plt.text(x, 5, str(numb - 10),\n                     horizontalalignment='center',\n                     fontsize=20,  # fontname='Arial',\n                     color='white')\n            plt.text(x - 0.95, 53.3 - 5, str(numb - 10),\n                     horizontalalignment='center',\n                     fontsize=20,  # fontname='Arial',\n                     color='white', rotation=180)\n    if endzones:\n        hash_range = range(11, 110)\n    else:\n        hash_range = range(1, 120)\n\n    for x in hash_range:\n        ax.plot([x, x], [0.4, 0.7], color='white')\n        ax.plot([x, x], [53.0, 52.5], color='white')\n        ax.plot([x, x], [22.91, 23.57], color='white')\n        ax.plot([x, x], [29.73, 30.39], color='white')\n\n    if highlight_line:\n        hl = highlight_line_number + 10\n        plt.plot([hl, hl], [0, 53.3], color='yellow')\n        plt.text(hl + 2, 50, '<- {}'.format(highlighted_name),\n                 color='yellow')\n    return fig, ax\n\ncreate_football_field()\nplt.show()", "processed": ["2 yard maximum count follow 1 3 yard distribut look normal right skew extrem right valu mean player cover 80 yard", "footbal field plot thank rob mulla wonder code http www kaggl com robikscub nfl big data bowl plot player posit creat plot footbal field"]}, {"markdown": ["## Ball Carrier Direction Analysis\n\nCompetition organizer in [this thread](https://www.kaggle.com/c/nfl-big-data-bowl-2020/discussion/111918#latest-645404) has mentioned that *\"One of the most crucial features to analyzing run play success is likely the amount of space that the ball carrier had available when he received the ball\"*\n\nSo let us explore that first in our EDA. Let us take the playid '20181007011551' to start with. \n* We will plot the home team using blue color and away team using orange color.\n* Ball carrier is plotted using red color\n* Direction of movement of the ball carrier is shown using arrow\n* Yards covered (target) variable is available in the title"], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-notebook-nfl\n\nimport math\ndef get_dx_dy(angle, dist):\n    cartesianAngleRadians = (450-angle)*math.pi/180.0\n    dx = dist * math.cos(cartesianAngleRadians)\n    dy = dist * math.sin(cartesianAngleRadians)\n    return dx, dy\n\nplay_id = 20181007011551\nfig, ax = create_football_field()\ntrain_df.query(\"PlayId == @play_id and Team == 'away'\") \\\n    .plot(x='X', y='Y', kind='scatter', ax=ax, color='orange', s=50, legend='Away')\ntrain_df.query(\"PlayId == @play_id and Team == 'home'\") \\\n    .plot(x='X', y='Y', kind='scatter', ax=ax, color='blue', s=50, legend='Home')\ntrain_df.query(\"PlayId == @play_id and NflIdRusher == NflId\") \\\n    .plot(x='X', y='Y', kind='scatter', ax=ax, color='red', s=100, legend='Rusher')\nrusher_row = train_df.query(\"PlayId == @play_id and NflIdRusher == NflId\")\nyards_covered = rusher_row[\"Yards\"].values[0]\n\nx = rusher_row[\"X\"].values[0]\ny = rusher_row[\"Y\"].values[0]\nrusher_dir = rusher_row[\"Dir\"].values[0]\nrusher_speed = rusher_row[\"S\"].values[0]\ndx, dy = get_dx_dy(rusher_dir, rusher_speed)\n\nax.arrow(x, y, dx, dy, length_includes_head=True, width=0.3)\nplt.title(f'Play # {play_id} and yard distance is {yards_covered}', fontsize=20)\nplt.legend()\nplt.show()\nrusher_dir", "processed": ["ball carrier direct analysi competit organ thread http www kaggl com c nfl big data bowl 2020 discus 111918 latest 645404 mention one crucial featur analyz run play success like amount space ball carrier avail receiv ball let u explor first eda let u take playid 20181007011551 start plot home team use blue color away team use orang color ball carrier plot use red color direct movement ball carrier shown use arrow yard cover target variabl avail titl"]}, {"markdown": ["### Direction of Ball Carrier during Negative Yards \n\nLet us take some plays where the yard distance is negative and see if there are any patterns that we can see."], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-notebook-nfl\n\ndef get_plot(play_id):\n    fig, ax = create_football_field()\n    train_df.query(\"PlayId == @play_id and Team == 'away'\") \\\n        .plot(x='X', y='Y', kind='scatter', ax=ax, color='orange', s=50, legend='Away')\n    train_df.query(\"PlayId == @play_id and Team == 'home'\") \\\n        .plot(x='X', y='Y', kind='scatter', ax=ax, color='blue', s=50, legend='Home')\n    train_df.query(\"PlayId == @play_id and NflIdRusher == NflId\") \\\n        .plot(x='X', y='Y', kind='scatter', ax=ax, color='red', s=100, legend='Rusher')\n    rusher_row = train_df.query(\"PlayId == @play_id and NflIdRusher == NflId\")\n    yards_covered = rusher_row[\"Yards\"].values[0]\n\n    x = rusher_row[\"X\"].values[0]\n    y = rusher_row[\"Y\"].values[0]\n    rusher_dir = rusher_row[\"Dir\"].values[0]\n    rusher_speed = rusher_row[\"S\"].values[0]\n    dx, dy = get_dx_dy(rusher_dir, rusher_speed)\n\n    ax.arrow(x, y, dx, dy, length_includes_head=True, width=0.3)\n    plt.title(f'Play # {play_id} and yard distance is {yards_covered}', fontsize=26)\n    plt.legend()\n    return plt\n\ntemp_df = train_df.groupby(\"PlayId\").first()\ntemp_df = temp_df.sort_values(by=\"Yards\").reset_index().head(5)\n\nfor play_id in temp_df[\"PlayId\"].values:\n    plt = get_plot(play_id)\n    plt.show()", "processed": ["direct ball carrier neg yard let u take play yard distanc neg see pattern see"]}, {"markdown": ["We could clearly see that most of the times the ball carrier moved in the opposite direction of the game and so the yards are negative.\n\n### Direction of Ball Carrier during Zero Yards\n\nNow let us take some plays where the distance covered is zero yards and plot them."], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-notebook-nfl\n\ntemp_df = train_df.groupby(\"PlayId\").first()\ntemp_df = temp_df[temp_df[\"Yards\"]==0].reset_index().head(5)\n\nfor play_id in temp_df[\"PlayId\"].values:\n    plt = get_plot(play_id)\n    plt.show()", "processed": ["could clearli see time ball carrier move opposit direct game yard neg direct ball carrier zero yard let u take play distanc cover zero yard plot"]}, {"markdown": ["Hmmm. Most of the times, the ball carrier is moving towards the center of the opposition. \n\n### Direction of Ball Carrier during High Positive Yards Coverage\n"], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-notebook-nfl\n\ntemp_df = train_df.groupby(\"PlayId\").first()\ntemp_df = temp_df[temp_df[\"Yards\"]>10].reset_index().head(5)\n\nfor play_id in temp_df[\"PlayId\"].values:\n    plt = get_plot(play_id)\n    plt.show()", "processed": ["hmmm time ball carrier move toward center opposit direct ball carrier high posit yard coverag"]}, {"markdown": ["It looks like the ball carrier tries to move away from the opposition team towards a gap so as to move forward gaining more yards.\n\n## Distance Covered by the Rusher at TimeHandoff "], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-notebook-nfl\n\nplt.figure(figsize=(16,12))\ntemp_df = train_df.query(\"NflIdRusher == NflId\")\nsns.scatterplot(temp_df[\"Dis\"], temp_df[\"Yards\"])\nplt.xlabel('Distance covered', fontsize=12)\nplt.ylabel('Yards (Target)', fontsize=12)\nplt.title(\"Distance covered by Rusher Vs Yards (target)\", fontsize=20)\nplt.show()", "processed": ["look like ball carrier tri move away opposit team toward gap move forward gain yard distanc cover rusher timehandoff"]}, {"markdown": ["We can see a nice increasing trend in the yards gained by the rusher as the distance travelled by the rusher increases from zero. Then it starts decreasing which indicates that too much distance covered before the handoff of the ball also affects the yards gained.\n\n## Speed of the Rusher at TimeHandoff "], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-notebook-nfl\n\nplt.figure(figsize=(16,12))\ntemp_df = train_df.query(\"NflIdRusher == NflId\")\nsns.scatterplot(temp_df[\"S\"], temp_df[\"Yards\"])\nplt.xlabel('Rusher Speed', fontsize=12)\nplt.ylabel('Yards (Target)', fontsize=12)\nplt.title(\"Rusher Speed Vs Yards (target)\", fontsize=20)\nplt.show()", "processed": ["see nice increas trend yard gain rusher distanc travel rusher increas zero start decreas indic much distanc cover handoff ball also affect yard gain speed rusher timehandoff"]}, {"markdown": ["Speed also shows a trend similar to distance covered. 2 to 6 yards per second seem to be a better speed for yards covered.\n\n## Rusher Acceleration at TimeHandOff"], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-notebook-nfl\n\nplt.figure(figsize=(16,12))\ntemp_df = train_df.query(\"NflIdRusher == NflId\")\nsns.scatterplot(temp_df[\"A\"], temp_df[\"Yards\"])\nplt.xlabel('Rusher Acceleration', fontsize=12)\nplt.ylabel('Yards (Target)', fontsize=12)\nplt.title(\"Rusher Acceleration Vs Yards (target)\", fontsize=20)\nplt.show()", "processed": ["speed also show trend similar distanc cover 2 6 yard per second seem better speed yard cover rusher acceler timehandoff"]}, {"markdown": ["Acceleration also shows a similar trend as that of distance and speed. \n\n## Position of the Rusher / Ball Carrier\n\nLet us now see how the position of the rusher affects the yards gained"], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-notebook-nfl\n\nplt.figure(figsize=(16,12))\ntemp_df = train_df.query(\"NflIdRusher == NflId\")\nsns.boxplot(data=temp_df, x=\"Position\", y=\"Yards\", showfliers=False, whis=3.0)\nplt.xlabel('Rusher position', fontsize=12)\nplt.ylabel('Yards (Target)', fontsize=12)\nplt.title(\"Rusher Position Vs Yards (target)\", fontsize=20)\nplt.show()", "processed": ["acceler also show similar trend distanc speed posit rusher ball carrier let u see posit rusher affect yard gain"]}, {"markdown": ["* Position CB has the highest median of yards gained followed by Wide Receiver\n\n## Number of Defenders in the Box\n\nNow let us see how the number of defenders in the box affects the target"], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-notebook-nfl\n\nplt.figure(figsize=(16,12))\ntemp_df = train_df.query(\"NflIdRusher == NflId\")\nsns.boxplot(data=temp_df, x=\"DefendersInTheBox\", y=\"Yards\", showfliers=False, whis=3.0)\nplt.xlabel('Number of defenders in the box', fontsize=12)\nplt.ylabel('Yards (Target)', fontsize=12)\nplt.title(\"Count of defenders in the box Vs Yards (target)\", fontsize=20)\nplt.show()", "processed": ["posit cb highest median yard gain follow wide receiv number defend box let u see number defend box affect target"]}, {"markdown": ["We can see a nice decrease in the median value of the yards gained by the rusher with the increase in the number of defenders in the box.\n\n## Down Number Vs Yards\n\nLet us now check how the yards gained vary with respect to down number."], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-notebook-nfl\n\nplt.figure(figsize=(16,12))\ntemp_df = train_df.query(\"NflIdRusher == NflId\")\nsns.boxplot(data=temp_df, x=\"Down\", y=\"Yards\", showfliers=False)\nplt.xlabel('Down Number', fontsize=12)\nplt.ylabel('Yards (Target)', fontsize=12)\nplt.title(\"Down Number Vs Yards (target)\", fontsize=20)\nplt.show()", "processed": ["see nice decreas median valu yard gain rusher increas number defend box number v yard let u check yard gain vari respect number"]}, {"markdown": ["We can see a decrease in the median value of the target with an increase in the down number from 1 to 4.\n\n## Possession Team Vs Yards\n\nNow let us see the distribution of yards based on the team"], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-notebook-nfl\n\nplt.figure(figsize=(12,10))\ntemp_df = train_df.query(\"NflIdRusher == NflId\")\nsns.boxplot(data=temp_df, y=\"PossessionTeam\", x=\"Yards\", showfliers=False, whis=3.0)\nplt.ylabel('PossessionTeam', fontsize=12)\nplt.xlabel('Yards (Target)', fontsize=12)\nplt.title(\"Possession team Vs Yards (target)\", fontsize=20)\nplt.show()", "processed": ["see decreas median valu target increas number 1 4 posse team v yard let u see distribut yard base team"]}, {"markdown": ["* LA team seems to have higher median yards value than the rest.\n* NYJ team seems to have lower median yards value than the rest.", "## Quarter Vs Yards"], "code": "# Reference: https://www.kaggle.com/code/sudalairajkumar/simple-exploration-notebook-nfl\n\nplt.figure(figsize=(16,12))\ntemp_df = train_df.query(\"NflIdRusher == NflId\")\nsns.catplot(data=temp_df, x=\"Quarter\", y=\"Yards\", kind=\"boxen\")\nplt.xlabel('Quarter', fontsize=12)\nplt.ylabel('Yards (Target)', fontsize=12)\nplt.title(\"Quarter Vs Yards (target)\", fontsize=20)\nplt.show()", "processed": ["la team seem higher median yard valu rest nyj team seem lower median yard valu rest", "quarter v yard"]}, {"markdown": ["Those who are casted heavily impact the quality of the film. We have not only the name of the actor, but also the gender and character name/type.\n\nAt first let's have a look at the popular names."], "code": "# Reference: https://www.kaggle.com/code/artgor/eda-feature-engineering-and-model-interpretation\n\nlist_of_cast_names = list(train['cast'].apply(lambda x: [i['name'] for i in x] if x != {} else []).values)\nCounter([i for j in list_of_cast_names for i in j]).most_common(15)\nlist_of_cast_names_url = list(train['cast'].apply(lambda x: [(i['name'], i['profile_path']) for i in x] if x != {} else []).values)\nd = Counter([i for j in list_of_cast_names_url for i in j]).most_common(16)\nfig = plt.figure(figsize=(20, 12))\nfor i, p in enumerate([j[0] for j in d]):\n    ax = fig.add_subplot(4, 4, i+1, xticks=[], yticks=[])\n    im = Image.open(urlopen(f\"https://image.tmdb.org/t/p/w600_and_h900_bestv2{p[1]}\"))\n    plt.imshow(im)\n    ax.set_title(f'{p[0]}')\nlist_of_cast_genders = list(train['cast'].apply(lambda x: [i['gender'] for i in x] if x != {} else []).values)\nCounter([i for j in list_of_cast_genders for i in j]).most_common()", "processed": ["cast heavili impact qualiti film name actor also gender charact name type first let look popular name"]}, {"markdown": ["The great crew is very important in creating the film. We have not only the names of the crew members, but also the genders, jobs and departments.\n\nAt first let's have a look at the popular names."], "code": "# Reference: https://www.kaggle.com/code/artgor/eda-feature-engineering-and-model-interpretation\n\nlist_of_crew_names = list(train['crew'].apply(lambda x: [i['name'] for i in x] if x != {} else []).values)\nCounter([i for j in list_of_crew_names for i in j]).most_common(15)\nlist_of_crew_names_url = list(train['crew'].apply(lambda x: [(i['name'], i['profile_path'], i['job']) for i in x] if x != {} else []).values)\nd = Counter([i for j in list_of_crew_names_url for i in j]).most_common(16)\nfig = plt.figure(figsize=(20, 16))\nfor i, p in enumerate([j[0] for j in d]):\n    ax = fig.add_subplot(4, 4, i+1, xticks=[], yticks=[])\n    if p[1]:\n        im = Image.open(urlopen(f\"https://image.tmdb.org/t/p/w600_and_h900_bestv2{p[1]}\"))\n    else:\n        im = Image.new('RGB', (5, 5))\n    plt.imshow(im)\n    ax.set_title(f'Name: {p[0]} \\n Job: {p[2]}')\nlist_of_crew_jobs = list(train['crew'].apply(lambda x: [i['job'] for i in x] if x != {} else []).values)\nCounter([i for j in list_of_crew_jobs for i in j]).most_common(15)\nlist_of_crew_genders = list(train['crew'].apply(lambda x: [i['gender'] for i in x] if x != {} else []).values)\nCounter([i for j in list_of_crew_genders for i in j]).most_common(15)\nlist_of_crew_departments = list(train['crew'].apply(lambda x: [i['department'] for i in x] if x != {} else []).values)\nCounter([i for j in list_of_crew_departments for i in j]).most_common(14)\nlist_of_crew_names = train['crew'].apply(lambda x: [i['name'] for i in x] if x != {} else []).values\nCounter([i for j in list_of_crew_names for i in j]).most_common(15)\ntrain['num_crew'] = train['crew'].apply(lambda x: len(x) if x != {} else 0)\ntop_crew_names = [m[0] for m in Counter([i for j in list_of_crew_names for i in j]).most_common(15)]\nfor g in top_crew_names:\n    train['crew_name_' + g] = train['crew'].apply(lambda x: 1 if g in str(x) else 0)\ntrain['genders_0_crew'] = train['crew'].apply(lambda x: sum([1 for i in x if i['gender'] == 0]))\ntrain['genders_1_crew'] = train['crew'].apply(lambda x: sum([1 for i in x if i['gender'] == 1]))\ntrain['genders_2_crew'] = train['crew'].apply(lambda x: sum([1 for i in x if i['gender'] == 2]))\ntop_crew_jobs = [m[0] for m in Counter([i for j in list_of_crew_jobs for i in j]).most_common(15)]\nfor j in top_crew_jobs:\n    train['jobs_' + j] = train['crew'].apply(lambda x: sum([1 for i in x if i['job'] == j]))\ntop_crew_departments = [m[0] for m in Counter([i for j in list_of_crew_departments for i in j]).most_common(15)]\nfor j in top_crew_departments:\n    train['departments_' + j] = train['crew'].apply(lambda x: sum([1 for i in x if i['department'] == j])) \n    \ntest['num_crew'] = test['crew'].apply(lambda x: len(x) if x != {} else 0)\nfor g in top_crew_names:\n    test['crew_name_' + g] = test['crew'].apply(lambda x: 1 if g in str(x) else 0)\ntest['genders_0_crew'] = test['crew'].apply(lambda x: sum([1 for i in x if i['gender'] == 0]))\ntest['genders_1_crew'] = test['crew'].apply(lambda x: sum([1 for i in x if i['gender'] == 1]))\ntest['genders_2_crew'] = test['crew'].apply(lambda x: sum([1 for i in x if i['gender'] == 2]))\nfor j in top_crew_jobs:\n    test['jobs_' + j] = test['crew'].apply(lambda x: sum([1 for i in x if i['job'] == j]))\nfor j in top_crew_departments:\n    test['departments_' + j] = test['crew'].apply(lambda x: sum([1 for i in x if i['department'] == j])) \n\ntrain = train.drop(['crew'], axis=1)\ntest = test.drop(['crew'], axis=1)", "processed": ["great crew import creat film name crew member also gender job depart first let look popular name"]}, {"markdown": ["<a id=\"target\"></a>\n### Target"], "code": "# Reference: https://www.kaggle.com/code/artgor/eda-feature-engineering-and-model-interpretation\n\nfig, ax = plt.subplots(figsize = (16, 6))\nplt.subplot(1, 2, 1)\nplt.hist(train['revenue']);\nplt.title('Distribution of revenue');\nplt.subplot(1, 2, 2)\nplt.hist(np.log1p(train['revenue']));\nplt.title('Distribution of log of revenue');\ntrain['log_revenue'] = np.log1p(train['revenue'])", "processed": ["id target target"]}, {"markdown": ["As we can see revenue distribution has a high skewness! It is better to use `np.log1p` of revenue.", "<a id=\"budget\"></a>\n### Budget"], "code": "# Reference: https://www.kaggle.com/code/artgor/eda-feature-engineering-and-model-interpretation\n\nfig, ax = plt.subplots(figsize = (16, 6))\nplt.subplot(1, 2, 1)\nplt.hist(train['budget']);\nplt.title('Distribution of budget');\nplt.subplot(1, 2, 2)\nplt.hist(np.log1p(train['budget']));\nplt.title('Distribution of log of budget');\nplt.figure(figsize=(16, 8))\nplt.subplot(1, 2, 1)\nplt.scatter(train['budget'], train['revenue'])\nplt.title('Revenue vs budget');\nplt.subplot(1, 2, 2)\nplt.scatter(np.log1p(train['budget']), train['log_revenue'])\nplt.title('Log Revenue vs log budget');", "processed": ["see revenu distribut high skew better use np log1p revenu", "id budget budget"]}, {"markdown": ["Most of homepages are unique, so this feature may be useless."], "code": "# Reference: https://www.kaggle.com/code/artgor/eda-feature-engineering-and-model-interpretation\n\ntrain['has_homepage'] = 0\ntrain.loc[train['homepage'].isnull() == False, 'has_homepage'] = 1\ntest['has_homepage'] = 0\ntest.loc[test['homepage'].isnull() == False, 'has_homepage'] = 1\nsns.catplot(x='has_homepage', y='revenue', data=train);\nplt.title('Revenue for film with and without homepage');", "processed": ["homepag uniqu featur may useless"]}, {"markdown": ["Films with homepage tend to generate more revenue! I suppose people can know more about the film thanks to homepage.", "<a id=\"or_lang\"></a>\n### original_language"], "code": "# Reference: https://www.kaggle.com/code/artgor/eda-feature-engineering-and-model-interpretation\n\nplt.figure(figsize=(16, 8))\nplt.subplot(1, 2, 1)\nsns.boxplot(x='original_language', y='revenue', data=train.loc[train['original_language'].isin(train['original_language'].value_counts().head(10).index)]);\nplt.title('Mean revenue per language');\nplt.subplot(1, 2, 2)\nsns.boxplot(x='original_language', y='log_revenue', data=train.loc[train['original_language'].isin(train['original_language'].value_counts().head(10).index)]);\nplt.title('Mean log revenue per language');", "processed": ["film homepag tend gener revenu suppos peopl know film thank homepag", "id lang origin languag"]}, {"markdown": ["We can see that some words can be used to predict revenue, but we will need more that overview text to build a good model.", "<a id=\"popularity\"></a>\n### popularity\n\nI'm not exactly sure what does popularity represents. Maybe it is some king of weighted rating, maybe something else. It seems it has low correlation with the target."], "code": "# Reference: https://www.kaggle.com/code/artgor/eda-feature-engineering-and-model-interpretation\n\nplt.figure(figsize=(16, 8))\nplt.subplot(1, 2, 1)\nplt.scatter(train['popularity'], train['revenue'])\nplt.title('Revenue vs popularity');\nplt.subplot(1, 2, 2)\nplt.scatter(train['popularity'], train['log_revenue'])\nplt.title('Log Revenue vs popularity');", "processed": ["see word use predict revenu need overview text build good model", "id popular popular exactli sure popular repres mayb king weight rate mayb someth el seem low correl target"]}, {"markdown": ["<a id=\"release_data\"></a>\n### release_date"], "code": "# Reference: https://www.kaggle.com/code/artgor/eda-feature-engineering-and-model-interpretation\n\ntest.loc[test['release_date'].isnull() == True, 'release_date'] = '01/01/98'\ndef fix_date(x):\n    \"\"\"\n    Fixes dates which are in 20xx\n    \"\"\"\n    year = x.split('/')[2]\n    if int(year) <= 19:\n        return x[:-2] + '20' + year\n    else:\n        return x[:-2] + '19' + year\ntrain['release_date'] = train['release_date'].apply(lambda x: fix_date(x))\ntest['release_date'] = test['release_date'].apply(lambda x: fix_date(x))\ntrain['release_date'] = pd.to_datetime(train['release_date'])\ntest['release_date'] = pd.to_datetime(test['release_date'])\n# creating features based on dates\ndef process_date(df):\n    date_parts = [\"year\", \"weekday\", \"month\", 'weekofyear', 'day', 'quarter']\n    for part in date_parts:\n        part_col = 'release_date' + \"_\" + part\n        df[part_col] = getattr(df['release_date'].dt, part).astype(int)\n    \n    return df\n\ntrain = process_date(train)\ntest = process_date(test)\nd1 = train['release_date_year'].value_counts().sort_index()\nd2 = test['release_date_year'].value_counts().sort_index()\ndata = [go.Scatter(x=d1.index, y=d1.values, name='train'), go.Scatter(x=d2.index, y=d2.values, name='test')]\nlayout = go.Layout(dict(title = \"Number of films per year\",\n                  xaxis = dict(title = 'Year'),\n                  yaxis = dict(title = 'Count'),\n                  ),legend=dict(\n                orientation=\"v\"))\npy.iplot(dict(data=data, layout=layout))\nd1 = train['release_date_year'].value_counts().sort_index()\nd2 = train.groupby(['release_date_year'])['revenue'].sum()\ndata = [go.Scatter(x=d1.index, y=d1.values, name='film count'), go.Scatter(x=d2.index, y=d2.values, name='total revenue', yaxis='y2')]\nlayout = go.Layout(dict(title = \"Number of films and total revenue per year\",\n                  xaxis = dict(title = 'Year'),\n                  yaxis = dict(title = 'Count'),\n                  yaxis2=dict(title='Total revenue', overlaying='y', side='right')\n                  ),legend=dict(\n                orientation=\"v\"))\npy.iplot(dict(data=data, layout=layout))\nd1 = train['release_date_year'].value_counts().sort_index()\nd2 = train.groupby(['release_date_year'])['revenue'].mean()\ndata = [go.Scatter(x=d1.index, y=d1.values, name='film count'), go.Scatter(x=d2.index, y=d2.values, name='mean revenue', yaxis='y2')]\nlayout = go.Layout(dict(title = \"Number of films and average revenue per year\",\n                  xaxis = dict(title = 'Year'),\n                  yaxis = dict(title = 'Count'),\n                  yaxis2=dict(title='Average revenue', overlaying='y', side='right')\n                  ),legend=dict(\n                orientation=\"v\"))\npy.iplot(dict(data=data, layout=layout))", "processed": ["id releas data releas date"]}, {"markdown": ["We can see that number of films and total revenue are growing, which is to be expected. But there were some years in the past with a high number of successful films, which brought high revenue."], "code": "# Reference: https://www.kaggle.com/code/artgor/eda-feature-engineering-and-model-interpretation\n\nsns.catplot(x='release_date_weekday', y='revenue', data=train);\nplt.title('Revenue on different days of week of release');", "processed": ["see number film total revenu grow expect year past high number success film brought high revenu"]}, {"markdown": ["Surprisingly films releases on Wednesdays and on Thursdays tend to have a higher revenue.", "<a id=\"runtime\"></a>\n### runtime\n\nThe length of the film in minutes"], "code": "# Reference: https://www.kaggle.com/code/artgor/eda-feature-engineering-and-model-interpretation\n\nplt.figure(figsize=(20, 6))\nplt.subplot(1, 3, 1)\nplt.hist(train['runtime'].fillna(0) / 60, bins=40);\nplt.title('Distribution of length of film in hours');\nplt.subplot(1, 3, 2)\nplt.scatter(train['runtime'].fillna(0), train['revenue'])\nplt.title('runtime vs revenue');\nplt.subplot(1, 3, 3)\nplt.scatter(train['runtime'].fillna(0), train['popularity'])\nplt.title('runtime vs popularity');", "processed": ["surprisingli film releas wednesday thursday tend higher revenu", "id runtim runtim length film minut"]}, {"markdown": ["<a id=\"collections\"></a>\n### Collections"], "code": "# Reference: https://www.kaggle.com/code/artgor/eda-feature-engineering-and-model-interpretation\n\nsns.boxplot(x='has_collection', y='revenue', data=train);", "processed": ["id collect collect"]}, {"markdown": ["Films, which are part of a collection usually have higher revenues. I suppose such films have a bigger fan base thanks to previous films.", "<a id=\"genres_\"></a>\n### Genres"], "code": "# Reference: https://www.kaggle.com/code/artgor/eda-feature-engineering-and-model-interpretation\n\nsns.catplot(x='num_genres', y='revenue', data=train);\nplt.title('Revenue for different number of genres in the film');\nsns.violinplot(x='genre_Drama', y='revenue', data=train[:100]);\nf, axes = plt.subplots(3, 5, figsize=(24, 12))\nplt.suptitle('Violinplot of revenue vs genres')\nfor i, e in enumerate([col for col in train.columns if 'genre_' in col]):\n    sns.violinplot(x=e, y='revenue', data=train, ax=axes[i // 5][i % 5]);", "processed": ["film part collect usual higher revenu suppos film bigger fan base thank previou film", "id genr genr"]}, {"markdown": ["Some genres tend to have less revenue, some tend to have higher.", "<a id=\"prod_comp\"></a>\n### Production companies"], "code": "# Reference: https://www.kaggle.com/code/artgor/eda-feature-engineering-and-model-interpretation\n\nf, axes = plt.subplots(6, 5, figsize=(24, 32))\nplt.suptitle('Violinplot of revenue vs production company')\nfor i, e in enumerate([col for col in train.columns if 'production_company' in col]):\n    sns.violinplot(x=e, y='revenue', data=train, ax=axes[i // 5][i % 5]);", "processed": ["genr tend le revenu tend higher", "id prod comp product compani"]}, {"markdown": ["There are only a couple of companies, which have distinctly higher revenues compared to others.", "<a id=\"prod_count\"></a>\n### Production countries"], "code": "# Reference: https://www.kaggle.com/code/artgor/eda-feature-engineering-and-model-interpretation\n\nsns.catplot(x='num_countries', y='revenue', data=train);\nplt.title('Revenue for different number of countries producing the film');", "processed": ["coupl compani distinctli higher revenu compar other", "id prod count product countri"]}, {"markdown": ["In fact I think that number of production countries hardly matters. Most films are produced by 1-2 companies, so films with 1-2 companies have the highest revenue."], "code": "# Reference: https://www.kaggle.com/code/artgor/eda-feature-engineering-and-model-interpretation\n\nf, axes = plt.subplots(5, 5, figsize=(24, 32))\nplt.suptitle('Violinplot of revenue vs production country')\nfor i, e in enumerate([col for col in train.columns if 'production_country' in col]):\n    sns.violinplot(x=e, y='revenue', data=train, ax=axes[i // 5][i % 5]);", "processed": ["fact think number product countri hardli matter film produc 1 2 compani film 1 2 compani highest revenu"]}, {"markdown": ["There are only a couple of countries, which have distinctly higher revenues compared to others.", "<a id=\"cast_viz\"></a>\n### Cast"], "code": "# Reference: https://www.kaggle.com/code/artgor/eda-feature-engineering-and-model-interpretation\n\nplt.figure(figsize=(16, 8))\nplt.subplot(1, 2, 1)\nplt.scatter(train['num_cast'], train['revenue'])\nplt.title('Number of cast members vs revenue');\nplt.subplot(1, 2, 2)\nplt.scatter(train['num_cast'], train['log_revenue'])\nplt.title('Log Revenue vs number of cast members');\nf, axes = plt.subplots(3, 5, figsize=(24, 18))\nplt.suptitle('Violinplot of revenue vs cast')\nfor i, e in enumerate([col for col in train.columns if 'cast_name' in col]):\n    sns.violinplot(x=e, y='revenue', data=train, ax=axes[i // 5][i % 5]);\nf, axes = plt.subplots(3, 5, figsize=(24, 18))\nplt.suptitle('Violinplot of revenue vs cast')\nfor i, e in enumerate([col for col in train.columns if 'cast_character_' in col]):\n    sns.violinplot(x=e, y='revenue', data=train, ax=axes[i // 5][i % 5]);", "processed": ["coupl countri distinctli higher revenu compar other", "id cast viz cast"]}, {"markdown": ["<a id=\"key_viz\"></a>\n### Keywords"], "code": "# Reference: https://www.kaggle.com/code/artgor/eda-feature-engineering-and-model-interpretation\n\nf, axes = plt.subplots(6, 5, figsize=(24, 32))\nplt.suptitle('Violinplot of revenue vs keyword')\nfor i, e in enumerate([col for col in train.columns if 'keyword_' in col]):\n    sns.violinplot(x=e, y='revenue', data=train, ax=axes[i // 5][i % 5]);", "processed": ["id key viz keyword"]}, {"markdown": ["<a id=\"crew_viz\"></a>\n### Crew"], "code": "# Reference: https://www.kaggle.com/code/artgor/eda-feature-engineering-and-model-interpretation\n\nplt.figure(figsize=(16, 8))\nplt.subplot(1, 2, 1)\nplt.scatter(train['num_crew'], train['revenue'])\nplt.title('Number of crew members vs revenue');\nplt.subplot(1, 2, 2)\nplt.scatter(train['num_crew'], train['log_revenue'])\nplt.title('Log Revenue vs number of crew members');\nf, axes = plt.subplots(3, 5, figsize=(24, 18))\nplt.suptitle('Violinplot of revenue vs crew_character')\nfor i, e in enumerate([col for col in train.columns if 'crew_character_' in col]):\n    sns.violinplot(x=e, y='revenue', data=train, ax=axes[i // 5][i % 5]);\nf, axes = plt.subplots(3, 5, figsize=(24, 18))\nplt.suptitle('Violinplot of revenue vs jobs')\nfor i, e in enumerate([col for col in train.columns if 'jobs_' in col]):\n    sns.violinplot(x=e, y='revenue', data=train, ax=axes[i // 5][i % 5]);", "processed": ["id crew viz crew"]}, {"markdown": ["<a id=\"basic_model\"></a>\n## Modelling and feature generation"], "code": "# Reference: https://www.kaggle.com/code/artgor/eda-feature-engineering-and-model-interpretation\n\ntrain = train.drop(['homepage', 'imdb_id', 'poster_path', 'release_date', 'status', 'log_revenue'], axis=1)\ntest = test.drop(['homepage', 'imdb_id', 'poster_path', 'release_date', 'status'], axis=1)\nfor col in train.columns:\n    if train[col].nunique() == 1:\n        print(col)\n        train = train.drop([col], axis=1)\n        test = test.drop([col], axis=1)\nfor col in ['original_language', 'collection_name', 'all_genres']:\n    le = LabelEncoder()\n    le.fit(list(train[col].fillna('')) + list(test[col].fillna('')))\n    train[col] = le.transform(train[col].fillna('').astype(str))\n    test[col] = le.transform(test[col].fillna('').astype(str))\ntrain_texts = train[['title', 'tagline', 'overview', 'original_title']]\ntest_texts = test[['title', 'tagline', 'overview', 'original_title']]\nfor col in ['title', 'tagline', 'overview', 'original_title']:\n    train['len_' + col] = train[col].fillna('').apply(lambda x: len(str(x)))\n    train['words_' + col] = train[col].fillna('').apply(lambda x: len(str(x.split(' '))))\n    train = train.drop(col, axis=1)\n    test['len_' + col] = test[col].fillna('').apply(lambda x: len(str(x)))\n    test['words_' + col] = test[col].fillna('').apply(lambda x: len(str(x.split(' '))))\n    test = test.drop(col, axis=1)\n# data fixes from https://www.kaggle.com/somang1418/happy-valentines-day-and-keep-kaggling-3\ntrain.loc[train['id'] == 16,'revenue'] = 192864          # Skinning\ntrain.loc[train['id'] == 90,'budget'] = 30000000         # Sommersby          \ntrain.loc[train['id'] == 118,'budget'] = 60000000        # Wild Hogs\ntrain.loc[train['id'] == 149,'budget'] = 18000000        # Beethoven\ntrain.loc[train['id'] == 313,'revenue'] = 12000000       # The Cookout \ntrain.loc[train['id'] == 451,'revenue'] = 12000000       # Chasing Liberty\ntrain.loc[train['id'] == 464,'budget'] = 20000000        # Parenthood\ntrain.loc[train['id'] == 470,'budget'] = 13000000        # The Karate Kid, Part II\ntrain.loc[train['id'] == 513,'budget'] = 930000          # From Prada to Nada\ntrain.loc[train['id'] == 797,'budget'] = 8000000         # Welcome to Dongmakgol\ntrain.loc[train['id'] == 819,'budget'] = 90000000        # Alvin and the Chipmunks: The Road Chip\ntrain.loc[train['id'] == 850,'budget'] = 90000000        # Modern Times\ntrain.loc[train['id'] == 1112,'budget'] = 7500000        # An Officer and a Gentleman\ntrain.loc[train['id'] == 1131,'budget'] = 4300000        # Smokey and the Bandit   \ntrain.loc[train['id'] == 1359,'budget'] = 10000000       # Stir Crazy \ntrain.loc[train['id'] == 1542,'budget'] = 1              # All at Once\ntrain.loc[train['id'] == 1570,'budget'] = 15800000       # Crocodile Dundee II\ntrain.loc[train['id'] == 1571,'budget'] = 4000000        # Lady and the Tramp\ntrain.loc[train['id'] == 1714,'budget'] = 46000000       # The Recruit\ntrain.loc[train['id'] == 1721,'budget'] = 17500000       # Cocoon\ntrain.loc[train['id'] == 1865,'revenue'] = 25000000      # Scooby-Doo 2: Monsters Unleashed\ntrain.loc[train['id'] == 2268,'budget'] = 17500000       # Madea Goes to Jail budget\ntrain.loc[train['id'] == 2491,'revenue'] = 6800000       # Never Talk to Strangers\ntrain.loc[train['id'] == 2602,'budget'] = 31000000       # Mr. Holland's Opus\ntrain.loc[train['id'] == 2612,'budget'] = 15000000       # Field of Dreams\ntrain.loc[train['id'] == 2696,'budget'] = 10000000       # Nurse 3-D\ntrain.loc[train['id'] == 2801,'budget'] = 10000000       # Fracture\ntest.loc[test['id'] == 3889,'budget'] = 15000000       # Colossal\ntest.loc[test['id'] == 6733,'budget'] = 5000000        # The Big Sick\ntest.loc[test['id'] == 3197,'budget'] = 8000000        # High-Rise\ntest.loc[test['id'] == 6683,'budget'] = 50000000       # The Pink Panther 2\ntest.loc[test['id'] == 5704,'budget'] = 4300000        # French Connection II\ntest.loc[test['id'] == 6109,'budget'] = 281756         # Dogtooth\ntest.loc[test['id'] == 7242,'budget'] = 10000000       # Addams Family Values\ntest.loc[test['id'] == 7021,'budget'] = 17540562       #  Two Is a Family\ntest.loc[test['id'] == 5591,'budget'] = 4000000        # The Orphanage\ntest.loc[test['id'] == 4282,'budget'] = 20000000       # Big Top Pee-wee\n\npower_six = train.id[train.budget > 1000][train.revenue < 100]\n\nfor k in power_six :\n    train.loc[train['id'] == k,'revenue'] =  train.loc[train['id'] == k,'revenue'] * 1000000\nX = train.drop(['id', 'revenue'], axis=1)\ny = np.log1p(train['revenue'])\nX_test = test.drop(['id'], axis=1)\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.1)\nparams = {'num_leaves': 30,\n         'min_data_in_leaf': 20,\n         'objective': 'regression',\n         'max_depth': 5,\n         'learning_rate': 0.01,\n         \"boosting\": \"gbdt\",\n         \"feature_fraction\": 0.9,\n         \"bagging_freq\": 1,\n         \"bagging_fraction\": 0.9,\n         \"bagging_seed\": 11,\n         \"metric\": 'rmse',\n         \"lambda_l1\": 0.2,\n         \"verbosity\": -1}\nmodel1 = lgb.LGBMRegressor(**params, n_estimators = 20000, nthread = 4, n_jobs = -1)\nmodel1.fit(X_train, y_train, \n        eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric='rmse',\n        verbose=1000, early_stopping_rounds=200)\neli5.show_weights(model1, feature_filter=lambda x: x != '<BIAS>')\nn_fold = 10\nfolds = KFold(n_splits=n_fold, shuffle=True, random_state=42)\ndef train_model(X, X_test, y, params=None, folds=folds, model_type='lgb', plot_feature_importance=False, model=None):\n\n    oof = np.zeros(X.shape[0])\n    prediction = np.zeros(X_test.shape[0])\n    scores = []\n    feature_importance = pd.DataFrame()\n    for fold_n, (train_index, valid_index) in enumerate(folds.split(X)):\n        print('Fold', fold_n, 'started at', time.ctime())\n        if model_type == 'sklearn':\n            X_train, X_valid = X[train_index], X[valid_index]\n        else:\n            X_train, X_valid = X.values[train_index], X.values[valid_index]\n        y_train, y_valid = y[train_index], y[valid_index]\n        \n        if model_type == 'lgb':\n            model = lgb.LGBMRegressor(**params, n_estimators = 20000, nthread = 4, n_jobs = -1)\n            model.fit(X_train, y_train, \n                    eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric='rmse',\n                    verbose=1000, early_stopping_rounds=200)\n            \n            y_pred_valid = model.predict(X_valid)\n            y_pred = model.predict(X_test, num_iteration=model.best_iteration_)\n            \n        if model_type == 'xgb':\n            train_data = xgb.DMatrix(data=X_train, label=y_train)\n            valid_data = xgb.DMatrix(data=X_valid, label=y_valid)\n\n            watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]\n            model = xgb.train(dtrain=train_data, num_boost_round=20000, evals=watchlist, early_stopping_rounds=200, verbose_eval=500, params=params)\n            y_pred_valid = model.predict(xgb.DMatrix(X_valid), ntree_limit=model.best_ntree_limit)\n            y_pred = model.predict(xgb.DMatrix(X_test.values), ntree_limit=model.best_ntree_limit)\n\n        if model_type == 'sklearn':\n            model = model\n            model.fit(X_train, y_train)\n            y_pred_valid = model.predict(X_valid).reshape(-1,)\n            score = mean_squared_error(y_valid, y_pred_valid)\n            \n            y_pred = model.predict(X_test)\n            \n        if model_type == 'cat':\n            model = CatBoostRegressor(iterations=20000,  eval_metric='RMSE', **params)\n            model.fit(X_train, y_train, eval_set=(X_valid, y_valid), cat_features=[], use_best_model=True, verbose=False)\n\n            y_pred_valid = model.predict(X_valid)\n            y_pred = model.predict(X_test)\n        \n        oof[valid_index] = y_pred_valid.reshape(-1,)\n        scores.append(mean_squared_error(y_valid, y_pred_valid) ** 0.5)\n        \n        prediction += y_pred    \n        \n        if model_type == 'lgb':\n            # feature importance\n            fold_importance = pd.DataFrame()\n            fold_importance[\"feature\"] = X.columns\n            fold_importance[\"importance\"] = model.feature_importances_\n            fold_importance[\"fold\"] = fold_n + 1\n            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n\n    prediction /= n_fold\n    \n    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n    \n    if model_type == 'lgb':\n        feature_importance[\"importance\"] /= n_fold\n        if plot_feature_importance:\n            cols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n                by=\"importance\", ascending=False)[:50].index\n\n            best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n\n            plt.figure(figsize=(16, 12));\n            sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False));\n            plt.title('LGB Features (avg over folds)');\n        \n            return oof, prediction, feature_importance\n        return oof, prediction\n    \n    else:\n        return oof, prediction\nparams = {'num_leaves': 30,\n         'min_data_in_leaf': 10,\n         'objective': 'regression',\n         'max_depth': 5,\n         'learning_rate': 0.01,\n         \"boosting\": \"gbdt\",\n         \"feature_fraction\": 0.9,\n         \"bagging_freq\": 1,\n         \"bagging_fraction\": 0.9,\n         \"bagging_seed\": 11,\n         \"metric\": 'rmse',\n         \"lambda_l1\": 0.2,\n         \"verbosity\": -1}\noof_lgb, prediction_lgb, _ = train_model(X, X_test, y, params=params, model_type='lgb', plot_feature_importance=True)", "processed": ["id basic model model featur gener"]}, {"markdown": ["We can see that important features native to LGB and top features in ELI5 are mostly similar. This means that our model is quite good at working with these features."], "code": "# Reference: https://www.kaggle.com/code/artgor/eda-feature-engineering-and-model-interpretation\n\nexplainer = shap.TreeExplainer(model1, X_train)\nshap_values = explainer.shap_values(X_train)\n\nshap.summary_plot(shap_values, X_train)", "processed": ["see import featur nativ lgb top featur eli5 mostli similar mean model quit good work featur"]}, {"markdown": ["SHAP provides more detailed information even if it may be more difficult to understand.\n\nFor example low budget has negavite impact on revenue, while high values usually tend to have higher revenue."], "code": "# Reference: https://www.kaggle.com/code/artgor/eda-feature-engineering-and-model-interpretation\n\ntop_cols = X_train.columns[np.argsort(shap_values.std(0))[::-1]][:10]\nfor col in top_cols:\n    shap.dependence_plot(col, shap_values, X_train)", "processed": ["shap provid detail inform even may difficult understand exampl low budget negavit impact revenu high valu usual tend higher revenu"]}]