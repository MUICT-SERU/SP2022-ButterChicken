{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get BM25 recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(filename='error.log', level=logging.ERROR)\n",
    "from datetime import datetime\n",
    "FILES = [\n",
    "    \"grandmaster_nl_pl_only_plot.json\",\n",
    "    \"master_nl_pl_only_plot.json\",\n",
    "    \"expert_nl_pl_only_plot.json\",\n",
    "]\n",
    "\n",
    "CLASS_NAME = {\n",
    "    \"grandmaster\": \"GrandMasterCode\",\n",
    "    \"master\": \"MasterCode\",\n",
    "    \"expert\": \"ExpertCode\",\n",
    "}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weaviate Instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "{\"deprecations\": null, \"objects\": [{\"class\": \"ExpertCode\", \"creationTimeUnix\": 1680809768520, \"id\": \"00004e69-955d-4519-92db-581b8222e7c7\", \"lastUpdateTimeUnix\": 1680809768520, \"properties\": {\"code\": \"# Reference: https://www.kaggle.com/code/yamsam/ashrae-highway-kernel-route4\\n\\n# i'm now using my leak data station kernel to shortcut.\\nleak_df = pd.read_feather('../input/ashrae-leak-data-station/leak.feather')\\n\\nleak_df.fillna(0, inplace=True)\\nleak_df = leak_df[leak_df.timestamp.dt.year > 2016]\\nleak_df.loc[leak_df.meter_reading < 0, 'meter_reading'] = 0 # remove large negative values\\nleak_df = leak_df[leak_df.building_id!=245]\\n\\nsample_submission.loc[sample_submission.meter_reading < 0, 'meter_reading'] = 0\\n\\ntest_df['pred'] = sample_submission.meter_reading\\n\\nleak_df = leak_df.merge(test_df[['building_id', 'meter', 'timestamp', 'pred', 'row_id']], left_on = ['building_id', 'meter', 'timestamp'], right_on = ['building_id', 'meter', 'timestamp'], how = \\\"left\\\")\\nleak_df = leak_df.merge(building_meta_df[['building_id', 'site_id']], on='building_id', how='left')\\nleak_df.head()\\nleak_df['pred_l1p'] = np.log1p(leak_df.pred)\\nleak_df['meter_reading_l1p'] = np.log1p(leak_df.meter_reading)\\n\\nsns.distplot(leak_df.pred_l1p)\\nsns.distplot(leak_df.meter_reading_l1p)\\n\\nleak_score = np.sqrt(mean_squared_error(leak_df.pred_l1p, leak_df.meter_reading_l1p))\\nleak_df = leak_df[['meter_reading', 'row_id']].set_index('row_id').dropna()\\nsample_submission.loc[leak_df.index, 'meter_reading'] = leak_df['meter_reading']\\nif not debug:\\n    sample_submission.to_csv('submission_ucf_replaced.csv', index=False, float_format='%.4f')\\nsample_submission.head()\\nnp.log1p(sample_submission['meter_reading']).hist(bins=100)\"}, \"vectorWeights\": null}, {\"class\": \"ExpertCode\", \"creationTimeUnix\": 1680807837522, \"id\": \"000c73fd-60ad-49fb-b506-1255086bad36\", \"lastUpdateTimeUnix\": 1680807837522, \"properties\": {\"code\": \"# Reference: https://www.kaggle.com/code/gowrishankarin/interactive-eda-using-plotly\\n\\nrevenue_by_country_new = revenue_by_country_new[revenue_by_country_new[\\\"geoNetwork.country\\\"] != \\\"United States\\\"]\\n\\ndata = [dict(\\n    type=\\\"choropleth\\\",\\n    locations=revenue_by_country_new[\\\"CODE\\\"],\\n    z=revenue_by_country_new[\\\"totals.transactionRevenue\\\"],\\n    text=revenue_by_country_new[\\\"geoNetwork.country\\\"],\\n    colorscale=\\\"Viridis\\\",\\n    autocolorscale=False,\\n    reversescale=True,\\n    marker=dict(\\n        line=dict(\\n            color=\\\"rgb(180, 180, 180)\\\",\\n            width=0.5\\n        )\\n    ),\\n    colorbar=dict(\\n        autotick=False,\\n        title=\\\"Transaction Revenue\\\"\\n    )\\n)]\\nlayout = dict(\\n    title=\\\"Countrywise Transaction Revenue\\\",\\n    geo=dict(\\n        showframe=False,\\n        showcoastlines=True,\\n        projection=dict(type=\\\"Mercator\\\")\\n    )\\n)\\nfig = dict(data=data, layout=layout)\\n# py.iplot(fig, validate=False, filename=\\\"d3-world-map\\\")\"}, \"vectorWeights\": null}, {\"class\": \"ExpertCode\", \"creationTimeUnix\": 1680800039422, \"id\": \"000dcf7b-a921-4844-aa68-5411e050cfdd\", \"lastUpdateTimeUnix\": 1680800039422, \"properties\": {\"code\": \"# Reference: https://www.kaggle.com/code/shams1/audio-data-analysis-using-librosa\\n\\nsr = 22050 # sample rate\\nT = 5.0    # seconds\\nt = np.linspace(0, T, int(T*sr), endpoint=False) # time variable\\nx = 0.5*np.sin(2*np.pi*220*t)# pure sine wave at 220 Hz\\n#Playing the audio\\nipd.Audio(x, rate=sr) # load a NumPy array\\n#Saving the audio\\nlibrosa.output.write_wav('tone_220.wav', x, sr)\\n\\nimport sklearn\\nspectral_centroids = librosa.feature.spectral_centroid(x, sr=sr)[0]\\nspectral_centroids.shape\\n# Computing the time variable for visualization\\nplt.figure(figsize=(12, 4))\\nframes = range(len(spectral_centroids))\\nt = librosa.frames_to_time(frames)\\n# Normalising the spectral centroid for visualisation\\ndef normalize(x, axis=0):\\n    return sklearn.preprocessing.minmax_scale(x, axis=axis)\\n#Plotting the Spectral Centroid along the waveform\\nlibrosa.display.waveplot(x, sr=sr, alpha=0.4)\\nplt.plot(t, normalize(spectral_centroids), color='b')\"}, \"vectorWeights\": null}, {\"class\": \"ExpertCode\", \"creationTimeUnix\": 1680801171013, \"id\": \"0012c67b-316a-45ef-8d13-4ac8f6edf0c1\", \"lastUpdateTimeUnix\": 1680801171013, \"properties\": {\"code\": \"import numpy as np\\nimport pandas as pd \\nimport scipy as sci\\nimport seaborn as sns\\nimport matplotlib.pyplot as plt\\nimport os\\nimport warnings\\nwarnings.filterwarnings(\\\"ignore\\\")\\nprint(os.listdir(\\\"../input\\\"))\\n#Changing the working directory\\nos.chdir(\\\"../input/train_1\\\")\\nlen(os.listdir())\\nos.listdir()[:5]\\ntypes_of_csvs = [x.split('-')[1].split('.')[0] for x in os.listdir()]\\nset(types_of_csvs)\"}, \"vectorWeights\": null}, {\"class\": \"ExpertCode\", \"creationTimeUnix\": 1680810657760, \"id\": \"00178397-1944-426c-8695-1d42e4b0cd11\", \"lastUpdateTimeUnix\": 1680810657760, \"properties\": {\"code\": \"# Reference: https://www.kaggle.com/code/safavieh/filler-starter-notebook\\n\\ndef plot_task(task):\\n    \\\"\\\"\\\"\\n    Plots the first train and test pairs of a specified task,\\n    using same color scheme as the ARC app\\n    \\\"\\\"\\\"\\n    cmap = colors.ListedColormap(\\n        ['#000000', '#0074D9','#FF4136','#2ECC40','#FFDC00',\\n         '#AAAAAA', '#F012BE', '#FF851B', '#7FDBFF', '#870C25'])\\n    norm = colors.Normalize(vmin=0, vmax=9)\\n    fig, axs = plt.subplots(1, 4, figsize=(15,15))\\n    axs[0].imshow(task['train'][0]['input'], cmap=cmap, norm=norm)\\n    axs[0].axis('off')\\n    axs[0].set_title('Train Input')\\n    axs[1].imshow(task['train'][0]['output'], cmap=cmap, norm=norm)\\n    axs[1].axis('off')\\n    axs[1].set_title('Train Output')\\n    axs[2].imshow(task['test'][0]['input'], cmap=cmap, norm=norm)\\n    axs[2].axis('off')\\n    axs[2].set_title('Test Input')\\n    axs[3].imshow(task['test'][0]['output'], cmap=cmap, norm=norm)\\n    axs[3].axis('off')\\n    axs[3].set_title('Test Output')\\n    plt.tight_layout()\\n    plt.show()\\nplot_task(task)\"}, \"vectorWeights\": null}, {\"class\": \"ExpertCode\", \"creationTimeUnix\": 1680804859551, \"id\": \"00227ea6-4ace-4538-93ea-a45a78f5722b\", \"lastUpdateTimeUnix\": 1680804859551, \"properties\": {\"code\": \"# Reference: https://www.kaggle.com/code/justjun0321/products-eda-how-to-make-price-prediction-clear\\n\\nax = sns.boxplot(x=\\\"general_category\\\", y=\\\"item_condition_id\\\", data=train_df)\\n\\nax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha=\\\"right\\\")\\nax.set_title('Condition distribution by general category')\\n\\nplt.tight_layout()\\nplt.show()\"}, \"vectorWeights\": null}, {\"class\": \"ExpertCode\", \"creationTimeUnix\": 1680800913423, \"id\": \"0024c048-a4d0-467a-868a-5caf27931eb7\", \"lastUpdateTimeUnix\": 1680800913423, \"properties\": {\"code\": \"# Reference: https://www.kaggle.com/code/rishabhiitbhu/ashrae-simple-eda\\n\\n# Compute the correlation matrix\\ncorr = train_df.corr()\\n\\n# Generate a mask for the upper triangle\\nmask = np.zeros_like(corr, dtype=np.bool)\\nmask[np.triu_indices_from(mask)] = True\\n\\n# Set up the matplotlib figure\\nf, ax = plt.subplots(figsize=(11, 9))\\n\\n# Generate a custom diverging colormap\\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\\n\\n# Draw the heatmap with the mask and correct aspect ratio\\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\\n            square=True, linewidths=.5, cbar_kws={\\\"shrink\\\": .5})\"}, \"vectorWeights\": null}, {\"class\": \"ExpertCode\", \"creationTimeUnix\": 1680803284697, \"id\": \"00258fe7-73e8-477b-8cff-2a89f7755a30\", \"lastUpdateTimeUnix\": 1680803284697, \"properties\": {\"code\": \"import numpy as np # linear algebra\\nimport pandas as pd \\n\\nimport matplotlib\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\n%matplotlib inline \\n\\nimport lightgbm as lgb\\nfrom sklearn.metrics import f1_score\\nfrom sklearn.externals.joblib import Parallel, delayed\\nfrom sklearn.base import clone\\nfrom sklearn.ensemble import VotingClassifier\\n\\nimport warnings\\nwarnings.filterwarnings(\\\"ignore\\\")\"}, \"vectorWeights\": null}, {\"class\": \"ExpertCode\", \"creationTimeUnix\": 1680805309117, \"id\": \"002950c4-6eb9-46a0-bd9e-cab08c9cd441\", \"lastUpdateTimeUnix\": 1680805309117, \"properties\": {\"code\": \"# Reference: https://www.kaggle.com/code/gautham11/lung-segmentation-methods-comparison\\n\\npatient_id = 'ID00007637202177411956430'\\ntrain = pd.read_csv(DATA_DIR/'train.csv')\\ntrain[train['Patient'] == patient_id]\\nslices = load_scan(str(DATA_DIR/f'train/{patient_id}'))\\nhu_slices = get_pixels_hu(slices)\\ndef plot_hist(arr):\\n    plt.hist(arr.flatten(), bins=80, color='c')\\n    plt.xlabel(\\\"Hounsfield Units (HU)\\\")\\n    plt.ylabel(\\\"Frequency\\\")\\n    plt.show()\\n\\n# Show some slice in the middle\\ndef plot_slice(arr, n=80):\\n    plt.imshow(arr[n], cmap=plt.cm.gray)\\n    plt.show()\\n    \\n#watershed plot markers\\ndef plot_watershed_markers(hu_slice):\\n    test_patient_internal, test_patient_external, test_patient_watershed = generate_markers(hu_slice)\\n\\n    f, (ax1, ax2, ax3) = plt.subplots(1, 3, sharey=True, figsize=(15,15))\\n\\n    ax1.imshow(test_patient_internal, cmap='gray')\\n    ax1.set_title(\\\"Internal Marker\\\")\\n    ax1.axis('off')\\n\\n    ax2.imshow(test_patient_external, cmap='gray')\\n    ax2.set_title(\\\"External Marker\\\")\\n    ax2.axis('off')\\n\\n    ax3.imshow(test_patient_watershed, cmap='gray')\\n    ax3.set_title(\\\"Watershed Marker\\\")\\n    ax3.axis('off')\\n\\n    plt.show()\\n    \\ndef plot_watershed_results(hu_slice, itrs=1):\\n    test_segmented, test_lungfilter, test_outline, test_watershed, test_sobel_gradient = seperate_lungs(hu_slice, itrs)\\n    f, ax = plt.subplots(3, 2, sharey=True, figsize = (12, 12))\\n    ax[0][0].imshow(test_sobel_gradient, cmap='gray')\\n    ax[0][0].set_title(\\\"Sobel Gradient\\\")\\n    ax[0][0].axis('off')\\n\\n    ax[0][1].imshow(test_watershed, cmap='gray')\\n    ax[0][1].set_title(\\\"Watershed\\\")\\n    ax[0][1].axis('off')\\n    \\n    ax[1][0].imshow(test_segmented, cmap='gray')\\n    ax[1][0].set_title('Segmented Lung')\\n    ax[1][0].axis('off')\\n    \\n    ax[1][1].imshow(test_lungfilter, cmap='gray')\\n    ax[1][1].set_title('Lungfilter')\\n    ax[1][1].axis('off')\\n    \\n    ax[2][0].imshow(test_outline, cmap='gray')\\n    ax[2][0].set_title('Outline')\\n    ax[2][0].axis('off')\\n\\n    plt.show()\\nplot_hist(hu_slices)\\nplot_slice(hu_slices, 20)\\n# the images rearraged by instance numbers\\nplt.imshow(pydicom.read_file(str(DATA_DIR/f'train/{patient_id}/20.dcm')).pixel_array, cmap='gray')\\nst = time.time()\\nresampled_slices, new_spacing = resample(hu_slices, slices)\\nprint(new_spacing)\\nprint(resampled_slices.shape)\\nprint(time.time() - st)\\nplot_slice(resampled_slices,200)\"}, \"vectorWeights\": null}, {\"class\": \"ExpertCode\", \"creationTimeUnix\": 1680800720056, \"id\": \"00298e57-024f-4ac6-b80f-c9516c68f0e6\", \"lastUpdateTimeUnix\": 1680800720056, \"properties\": {\"code\": \"# Reference: https://www.kaggle.com/code/osciiart/homecreditrisk-extensive-eda-baseline-model-jp\\n\\nt = previous_application['CHANNEL_TYPE'].value_counts()\\nlabels = t.index\\nvalues = t.values\\n\\ncolors = ['#FEBFB3', '#E1396C', '#96D38C', '#D0F9B1']\\n\\ntrace = go.Pie(labels=labels, values=values,\\n               hoverinfo='', textinfo='',\\n               textfont=dict(size=12),\\n               marker=dict(colors=colors,\\n                           line=dict(color='#fff', width=2)))\\n\\nlayout = go.Layout(title='Channel Type in Previous Applications', height=400)\\nfig = go.Figure(data=[trace], layout=layout)\\niplot(fig)\"}, \"vectorWeights\": null}, {\"class\": \"ExpertCode\", \"creationTimeUnix\": 1680806313677, \"id\": \"0033f4bb-1596-426a-8b23-c882623c74d6\", \"lastUpdateTimeUnix\": 1680806313677, \"properties\": {\"code\": \"# Reference: https://www.kaggle.com/code/hengzheng/simple-autoencoder-keras-and-kmean-cluster\\n\\nmodel = Sequential()\\n\\nmodel.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape, padding='same'))\\nmodel.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\\nmodel.add(Conv2D(16, kernel_size=(3, 3), activation='relu', padding='same'))\\nmodel.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\\nmodel.add(Conv2D(8, kernel_size=(3, 3), activation='relu', padding='same'))\\nmodel.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\\n\\nmodel.add(Conv2D(8, kernel_size=(3, 3), activation='relu', padding='same'))\\nmodel.add(UpSampling2D((2, 2)))\\nmodel.add(Conv2D(16, kernel_size=(3, 3), activation='relu', padding='same'))\\nmodel.add(UpSampling2D((2, 2)))\\nmodel.add(Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same'))\\nmodel.add(UpSampling2D((2, 2)))\\nmodel.add(Conv2D(3, kernel_size=(3, 3), activation='sigmoid', padding='same'))\\n\\nmodel.compile(loss='binary_crossentropy', optimizer='adadelta', metrics=['accuracy'])\\n\\nmodel.summary()\\nmodel.fit(X, X, epochs=20, batch_size=128, shuffle=True)\\ndef plot_some(im_list):\\n    plt.figure(figsize=(15,4))\\n    for i, array in enumerate(im_list):\\n        plt.subplot(1, len(im_list), i+1)\\n        plt.imshow(array)\\n        plt.axis('off')\\n    plt.show()\\n\\nimg_decoded = model.predict(X[:5])\\n\\nprint('Before autoencoding:')\\nplot_some(X[:5])\\nprint('After decoding:')\\nplot_some(img_decoded)\\nX_sample = X[:100]\\nprint(X_sample.shape)\\n\\nget_encoded = K.function([model.layers[0].input], [model.layers[5].output])\\nencoded_sample = get_encoded([X_sample])[0]\\nprint(encoded_sample.shape)\\nfor n_image in range(0, 5):\\n    \\n    plt.figure(figsize=(12,4))\\n\\n    plt.subplot(1,4,1)\\n    plt.imshow(X_sample[n_image][:,:,::-1])\\n    plt.axis('off')\\n    plt.title('Original Image')\\n\\n    plt.subplot(1,4,2)\\n    plt.imshow(encoded_sample[n_image].mean(axis=-1))\\n    plt.axis('off')\\n    plt.title('Encoded Mean')\\n\\n    plt.subplot(1,4,3)\\n    plt.imshow(encoded_sample[n_image].max(axis=-1))\\n    plt.axis('off')\\n    plt.title('Encoded Max')\\n\\n    plt.subplot(1,4,4)\\n    plt.imshow(encoded_sample[n_image].std(axis=-1))\\n    plt.axis('off')\\n    plt.title('Encoded Std')\\n\\n    plt.show()\"}, \"vectorWeights\": null}, {\"class\": \"ExpertCode\", \"creationTimeUnix\": 1680807296994, \"id\": \"00354a03-95ba-4657-9e83-4ea5a62f0fa2\", \"lastUpdateTimeUnix\": 1680807296994, \"properties\": {\"code\": \"# Reference: https://www.kaggle.com/code/philschmidt/sea-lion-correlations-cv2-template-matching\\n\\nfrom scipy.stats.kde import gaussian_kde\\n\\nk = gaussian_kde(np.vstack([x, y]), bw_method=0.5)\\nxi, yi = np.mgrid[x.min():x.max():x.size**0.5*1j,y.min():y.max():y.size**0.5*1j]\\nzi = k(np.vstack([xi.flatten(), yi.flatten()]))\\nfig = plt.figure(figsize=(12,12))\\nax1 = fig.add_subplot(211)\\nax2 = fig.add_subplot(212)\\n\\n# alpha=0.5 will make the plots semitransparent\\nax1.pcolormesh(xi, yi, zi.reshape(xi.shape), alpha=0.5)\\nax2.contourf(xi, yi, zi.reshape(xi.shape), alpha=0.5)\\n\\nax1.set_xlim(x.min(), x.max())\\nax1.set_ylim(y.min(), y.max())\\nax2.set_xlim(x.min(), x.max())\\nax2.set_ylim(y.min(), y.max())\\n\\nax1.imshow(cropped_raw, extent=[x.min(), x.max(), y.min(), y.max()], aspect='auto')\\nax2.imshow(cropped_raw, extent=[x.min(), x.max(), y.min(), y.max()], aspect='auto')\"}, \"vectorWeights\": null}, {\"class\": \"ExpertCode\", \"creationTimeUnix\": 1680800273604, \"id\": \"0041c3f1-b198-4f5a-9819-5ff76dd521c7\", \"lastUpdateTimeUnix\": 1680800273604, \"properties\": {\"code\": \"# Reference: https://www.kaggle.com/code/fchmiel/low-variance-features-useless\\n\\nmi = mutual_info_classif(train_wheezy.values, target_wheezy, discrete_features=False)\\n\\nfeature_stds = train_wheezy.std().values\\n\\nplt.plot(mi[feature_stds>1.5], label='high variance features')\\nplt.plot(mi[feature_stds<1.5], label='low variance features')\\nplt.legend()\"}, \"vectorWeights\": null}, {\"class\": \"ExpertCode\", \"creationTimeUnix\": 1680807511775, \"id\": \"004342a9-d30c-413b-9a5a-2e6857896897\", \"lastUpdateTimeUnix\": 1680807511775, \"properties\": {\"code\": \"# Reference: https://www.kaggle.com/code/anirbansen3027/google-landmark-recognition-2020-eda\\n\\ndef plot_images(image_list,rows,cols,title):\\n    fig,ax = plt.subplots(rows,cols,figsize = (25,5*rows))\\n    ax = ax.flatten()\\n    for i, image_id in enumerate(image_list):\\n        image = cv2.imread(TRAIN_PATH+'/{}/{}/{}/{}.jpg'.format(image_id[0],image_id[1],image_id[2],image_id))\\n        image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\\n        ax[i].imshow(image)\\n        ax[i].set_axis_off()\\n        ax[i].set_title(image_id)\\n    plt.suptitle(title)\\nplot_images(df_train.loc[df_train.landmark_id==df_image_counts[df_image_counts.images == 6272][\\\"landmark_id\\\"].values[0],\\\"id\\\"].values[:10],2,5,\\\"Images of Landmark - 138982 (10 out of 6k images of this landmark)\\\")\"}, \"vectorWeights\": null}, {\"class\": \"ExpertCode\", \"creationTimeUnix\": 1680801791724, \"id\": \"0050c798-b799-4adc-890d-a53beee4d706\", \"lastUpdateTimeUnix\": 1680801791724, \"properties\": {\"code\": \"# Reference: https://www.kaggle.com/code/rajeshcv/understanding-v-columns\\n\\ncolfreq=column_value_freq(Vcols,cum_per)\\ncolfreqbool = colfreq[colfreq.unique_values==2]\\nif len(colfreqbool)%3 == 0:\\n    nrow = len(colfreqbool)/3\\nelse:\\n    nrow = len(colfreqbool) // 3 + 1 \\nsns.set(rc={'figure.figsize':(14,16)})\\nfor num, alpha in enumerate(colfreqbool.col_name):\\n    plt.subplot(nrow, 3, num+1)\\n    plot1= sns.countplot(data=train_transaction,x=alpha,hue='isFraud')\\n    for p in plot1.patches[1:]:\\n        h = p.get_height()\\n        x = p.get_x()+p.get_width()/2.\\n        if h != 0:\\n            plot1.annotate(\\\"%g\\\" % p.get_height(), xy=(x,h), xytext=(0,4), rotation=90, \\n                   textcoords=\\\"offset points\\\", ha=\\\"center\\\", va=\\\"bottom\\\")\\n    plt.legend(title='isFraud',loc='upper right')\\nplt.tight_layout(pad=0.4, w_pad=0.5, h_pad=1.0)\"}, \"vectorWeights\": null}, {\"class\": \"ExpertCode\", \"creationTimeUnix\": 1680810511219, \"id\": \"00589b17-3b3b-461a-81f3-b0a498f5d7b8\", \"lastUpdateTimeUnix\": 1680810511219, \"properties\": {\"code\": \"# Reference: https://www.kaggle.com/code/jhoward/some-dicom-gotchas-to-be-aware-of-fastai\\n\\ndcm.scaled_px.type()\"}, \"vectorWeights\": null}, {\"class\": \"ExpertCode\", \"creationTimeUnix\": 1680802878431, \"id\": \"00658c7b-6766-4de1-b828-1548e47311d1\", \"lastUpdateTimeUnix\": 1680802878431, \"properties\": {\"code\": \"import math, re, os\\nimport tensorflow as tf\\nimport numpy as np\\nfrom matplotlib import pyplot as plt\\nfrom kaggle_datasets import KaggleDatasets\\nfrom sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\\nprint(\\\"Tensorflow version \\\" + tf.__version__)\\nAUTO = tf.data.experimental.AUTOTUNE\"}, \"vectorWeights\": null}, {\"class\": \"ExpertCode\", \"creationTimeUnix\": 1680809108224, \"id\": \"006644e1-fc77-422c-b9e0-3344121a315d\", \"lastUpdateTimeUnix\": 1680809108224, \"properties\": {\"code\": \"# Reference: https://www.kaggle.com/code/sajikim/baseline-vaccine-degradation-prediction\\n\\n# read npy data file\\nbpps_list = os.listdir('../input/stanford-covid-vaccine/bpps/')\\nbpps_npy = np.load(f'../input/stanford-covid-vaccine/bpps/{bpps_list[0]}')\\nprint('Count of npy files: ', len(bpps_list))\\nprint('Size of image: ', bpps_npy.shape)\\n# show the images of npy data.\\nimport matplotlib.pyplot as plt\\nfrom skimage import color\\nfrom skimage import io\\n\\nfig = plt.figure(figsize=(15, 15))\\nfor i, f in enumerate(bpps_list):\\n    if i == 25:\\n        break\\n    sub = fig.add_subplot(5,5, i + 1)\\n    example_bpps = np.load(f'../input/stanford-covid-vaccine/bpps/{f}')\\n    sub.imshow(example_bpps,interpolation='nearest')\\n    sub.set_title(f)\\nplt.tight_layout()\\nplt.show()\\n\\na = []\\nb = []\\nfor i, f in enumerate(bpps_list):\\n    test_bpps = np.load(f'../input/stanford-covid-vaccine/bpps/{f}')\\n    for j, test_bpp in enumerate(test_bpps):\\n        a += [os.path.splitext(f)[0] + \\\"_\\\" + str(j)]\\n        b += [test_bpp.mean()]\\n        \\ndf_npy=pd.DataFrame(data={'id_seqpos': a, 'mean_npy': b})\\ndf_npy['mean_npy2']=df_npy['mean_npy']/df_npy['mean_npy'].mean()\\ndf_train['mean_reactivity'] = df_train['reactivity'].apply(lambda x: np.mean(x))\\ndf_train['mean_deg_Mg_pH10'] = df_train['deg_Mg_pH10'].apply(lambda x: np.mean(x))\\ndf_train['mean_deg_pH10'] = df_train['deg_pH10'].apply(lambda x: np.mean(x))\\ndf_train['mean_deg_Mg_50C'] = df_train['deg_Mg_50C'].apply(lambda x: np.mean(x))\\ndf_train['mean_deg_50C'] = df_train['deg_50C'].apply(lambda x: np.mean(x))\\n\\nmean_react = df_train['mean_reactivity'].mean()\\nmean_deg_Mg_pH10 = df_train['mean_deg_Mg_pH10'].mean()\\nmean_deg_pH10 = df_train['mean_deg_pH10'].mean()\\nmean_deg_Mg_50C = df_train['mean_deg_Mg_50C'].mean()\\nmean_deg_50C = df_train['mean_deg_50C'].mean()\\n\\ndf_sample = pd.merge(df_sample, df_npy, on='id_seqpos', how='left')\\n\\ndf_sample['reactivity'] = mean_react\\ndf_sample['deg_Mg_pH10'] = mean_deg_Mg_pH10\\ndf_sample['deg_pH10'] = mean_deg_pH10\\ndf_sample['deg_Mg_50C'] = mean_deg_Mg_50C\\ndf_sample['deg_50C'] = mean_deg_50C\\n\\ndf_sample['reactivity'] = df_sample['reactivity'] * df_sample['mean_npy2'] \\ndf_sample['deg_Mg_pH10'] = df_sample['deg_Mg_pH10'] * df_sample['mean_npy2'] \\ndf_sample['deg_pH10'] = df_sample['deg_pH10'] * df_sample['mean_npy2'] \\ndf_sample['deg_Mg_50C'] = df_sample['deg_Mg_50C'] * df_sample['mean_npy2'] \\ndf_sample['deg_50C'] = df_sample['deg_50C'] * df_sample['mean_npy2'] \\n\\ndf_sample.drop(columns=['mean_npy', 'mean_npy2'], inplace=True)\"}, \"vectorWeights\": null}, {\"class\": \"ExpertCode\", \"creationTimeUnix\": 1680807254511, \"id\": \"006ef51b-a90d-4bb3-8ed8-19d08e6450c9\", \"lastUpdateTimeUnix\": 1680807254511, \"properties\": {\"code\": \"# Reference: https://www.kaggle.com/code/soumya9977/eda-w-plotly-and-stacking-on-tabular-data-0-685\\n\\nimport gc\\ngc.collect()\\ntrain.head()\\ntrain.isnull().sum()\\nnan_features = [i for i in train.columns if train[i].isnull().sum()>=1]\\nnan_features\\n# pd.pandas.set_option('display.max_columns',None)\\n# pd.pandas.set_option('display.max_rows',None)\\n\\ntrain['age_approx'] = train['age_approx'].fillna(train['age_approx'].median())\\ntrain['age_approx'].isnull().sum()\\ntrain['anatom_site_general_challenge'].mode()\\ntrain['anatom_site_general_challenge']=train['anatom_site_general_challenge'].fillna('torso') \\ntrain['anatom_site_general_challenge'].isnull().sum()\\ntrain['sex'] = train['sex'].fillna(str(train['sex'].mode()))\\ntrain['diagnosis'] = train['diagnosis'].fillna(str(train['diagnosis'].mode()))\\ntrain['sex'].isnull().sum()\\ntrain['diagnosis'].isnull().sum()\\ntrain.isnull().sum()\"}, \"vectorWeights\": null}, {\"class\": \"ExpertCode\", \"creationTimeUnix\": 1680809536814, \"id\": \"0073ea0b-6a71-4680-8f53-c7add0dda2ab\", \"lastUpdateTimeUnix\": 1680809536814, \"properties\": {\"code\": \"# Reference: https://www.kaggle.com/code/alijs1/explaining-model-s-predictions\\n\\nprint('Truth:', train_y[2])\\nshap.force_plot(shap_values[2,:], train_df.iloc[2,:])\"}, \"vectorWeights\": null}, {\"class\": \"ExpertCode\", \"creationTimeUnix\": 1680803304815, \"id\": \"00784458-455e-4e23-a964-2fa1e49201c7\", \"lastUpdateTimeUnix\": 1680803304815, \"properties\": {\"code\": \"# Reference: https://www.kaggle.com/code/asindico/kkbox-music-the-essential-kickstart\\n\\nfdf = df[np.abs(df['days']-df['days'].mean())<=(3*df['days'].std())]\\ndayshist = df.groupby(['days'],as_index=False).count()\\ndayshist = dayshist.drop(0,axis=0)\\nsns.distplot(dayshist['days'], hist=True, color=\\\"g\\\", kde_kws={\\\"shade\\\": True})\\ncities = fdf['city'].unique()\\ncduration = []\\nfor c in cities:\\n    duration = []\\n    tmp = fdf[fdf['city']==c]['days']\\n    cduration.append(tmp)\"}, \"vectorWeights\": null}, {\"class\": \"ExpertCode\", \"creationTimeUnix\": 1680809598759, \"id\": \"007c7118-d191-4985-a8c4-10801690818e\", \"lastUpdateTimeUnix\": 1680809598759, \"properties\": {\"code\": \"# Reference: https://www.kaggle.com/code/siavrez/simple-eda-with-croston-method\\n\\nplt.figure()\\nax = autocorrelation_plot(items.T.iloc[1])\\nax.grid(color='grey', linestyle='-', linewidth=0.25, alpha=0.5)\\nplt.show()\"}, \"vectorWeights\": null}, {\"class\": \"ExpertCode\", \"creationTimeUnix\": 1680803899414, \"id\": \"007d576f-8831-40e1-90fc-f4dfc5e864c5\", \"lastUpdateTimeUnix\": 1680803899414, \"properties\": {\"code\": \"# Reference: https://www.kaggle.com/code/raoulma/plants-xception-90-06-test-accuracy\\n\\n## take a fixed number of samples for testing purpose\\n\\nif take_only_samples_of_train_data:\\n    train_df = pd.concat([train_df[train_df['species'] == sp][:num_samples_of_train_data_per_species] for sp in species])\\n    train_df.index = np.arange(len(train_df))\\n\\nif take_only_samples_of_test_data:\\n    test_df = test_df[:num_samples_of_test_data]\\n## detect and segment plants in the image \\n\\ndef create_mask_for_plant(image):\\n    image_hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\\n    \\n    sensitivity = 35\\n    lower_hsv = np.array([60 - sensitivity, 100, 50])\\n    upper_hsv = np.array([60 + sensitivity, 255, 255])\\n\\n    mask = cv2.inRange(image_hsv, lower_hsv, upper_hsv)\\n    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (11,11))\\n    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\\n    \\n    return mask\\n\\ndef segment_plant(image):\\n    mask = create_mask_for_plant(image)\\n    output = cv2.bitwise_and(image, image, mask = mask)\\n    return output\\n\\ndef sharpen_image(image):\\n    image_blurred = cv2.GaussianBlur(image, (0, 0), 3)\\n    image_sharp = cv2.addWeighted(image, 1.5, image_blurred, -0.5, 0)\\n    return image_sharp\\n\\ndef read_segmented_image(filepath, img_size):\\n    img = cv2.imread(os.path.join(data_dir, filepath), cv2.IMREAD_COLOR)\\n    img = cv2.resize(img.copy(), img_size, interpolation = cv2.INTER_AREA)\\n\\n    image_mask = create_mask_for_plant(img)\\n    image_segmented = segment_plant(img)\\n    image_sharpen = sharpen_image(image_segmented)\\n    return img, image_mask, image_segmented, image_sharpen\\n     \\n\\n# show some images\\nif show_plots:\\n    for i in range(4):\\n \\n        img, image_mask, image_segmented, image_sharpen = read_segmented_image(\\n            train_df.loc[i,'filepath'],(224,224))\\n        \\n        fig, axs = plt.subplots(1, 4, figsize=(20, 20))\\n        axs[0].imshow(img.astype(np.uint8))\\n        axs[1].imshow(image_mask.astype(np.uint8))\\n        axs[2].imshow(image_segmented.astype(np.uint8))\\n        axs[3].imshow(image_sharpen.astype(np.uint8))\\n        \\n## read and preprocess all training/validation/test images and labels\\n\\ndef preprocess_image(img):\\n    img /= 255.\\n    img -= 0.5\\n    img *= 2\\n    return img\\n\\ntarget_image_size = 299\\n\\nprint('read and preprocess training and validation images')\\n\\n# read, preprocess training and validation images  \\nx_train_valid = np.zeros((len(train_df), target_image_size, target_image_size, 3),\\n                         dtype='float32')\\ny_train_valid = train_df.loc[:, 'species_id'].values \\nfor i, filepath in tqdm(enumerate(train_df['filepath'])):\\n    \\n    # read original images\\n    #img = read_image(filepath, (target_image_size, target_image_size))\\n    \\n    # read segmented image\\n    _,_,_,img = read_segmented_image(filepath, (target_image_size, target_image_size))\\n    \\n    # all pixel values are now between -1 and 1\\n    x_train_valid[i] = preprocess_image(np.expand_dims(img.copy().astype(np.float), axis=0)) \\n\\nprint('read and preprocess test images')\\n\\n# read, preprocess test images  \\nx_test = np.zeros((len(test_df), target_image_size, target_image_size, 3), dtype='float32')\\nfor i, filepath in tqdm(enumerate(test_df['filepath'])):\\n    \\n    # read original image\\n    #img = read_image(filepath, (target_image_size, target_image_size))\\n    \\n    # read segmented image\\n    _,_,_,img = read_segmented_image(filepath, (target_image_size, target_image_size))\\n    \\n    # all pixel values are now between -1 and 1\\n    x_test[i] = preprocess_image(np.expand_dims(img.copy().astype(np.float), axis=0)) \\n    \\nprint('x_train_valid.shape = ', x_train_valid.shape)\\nprint('x_test.shape = ', x_test.shape)\\n\\n## data augmentation\\n\\n# generate new images via rotations, translations, flippings\\ndef generate_images(imgs):\\n    imgs_len = len(imgs)\\n    image_generator = keras.preprocessing.image.ImageDataGenerator(\\n        rotation_range = 10, width_shift_range = 0.1 , height_shift_range = 0.1,\\n        horizontal_flip = False, vertical_flip = False, zoom_range = 0.1)\\n\\n    imgs = image_generator.flow(imgs.copy(), np.zeros(imgs_len), batch_size=imgs_len, shuffle = False).next()    \\n  \\n    # return transformed images in the same order as the original ones\\n    return imgs[0]\\n\\n\\n# show some examples\\nif show_plots:\\n    imgs = (((x_train_valid[0:4]+1.)/2.)*255.) # transform pixels into range [0,255]\\n    imgs_generated = imgs\\n\\n    fig, axs = plt.subplots(4, 8, figsize=(20, 10))\\n    for i in range(8):\\n        axs[0,i].imshow(imgs_generated[0].astype(np.uint8))\\n        axs[1,i].imshow(imgs_generated[1].astype(np.uint8))\\n        axs[2,i].imshow(imgs_generated[2].astype(np.uint8))\\n        axs[3,i].imshow(imgs_generated[3].astype(np.uint8))   \\n        imgs_generated = generate_images(imgs)\\n\\n\"}, \"vectorWeights\": null}, {\"class\": \"ExpertCode\", \"creationTimeUnix\": 1680805479183, \"id\": \"007fc465-544e-4b6a-892a-2b1a64c21de0\", \"lastUpdateTimeUnix\": 1680805479183, \"properties\": {\"code\": \"# Reference: https://www.kaggle.com/code/havinath/eda-observations-visualizations-pytorch\\n\\ntrain_patient_data.boxplot(column='Age', by='Sex')\\n#plt.title(\\\"ddddd\\\")\\nplt.title(\\\"Boxplot of Age by Sex\\\")\\nplt.ylabel(\\\"Age\\\")\\n# get rid of the automatic 'Boxplot grouped by group_by_column_name' title\\nplt.suptitle(\\\"\\\")\\nplt.show()\"}, \"vectorWeights\": null}, {\"class\": \"ExpertCode\", \"creationTimeUnix\": 1680804923723, \"id\": \"00881d37-5267-4c52-883a-ee8c9dd070c1\", \"lastUpdateTimeUnix\": 1680804923723, \"properties\": {\"code\": \"# Reference: https://www.kaggle.com/code/noelmat/training-efficientnet-with-pytorch\\n\\nimport pandas as pd\\nimport numpy as np\\nimport torch\\nfrom torchvision import models\\nfrom pathlib import Path\\nPath.ls = lambda x: list(x.iterdir())\\n\\nimport cv2 \\nimport pydicom\\nfrom tqdm import tqdm\\nfrom matplotlib import pyplot as plt\\nfrom torchvision import transforms\\n\\nfrom torch import nn\\nfrom efficientnet_pytorch import EfficientNet\\nfrom efficientnet_pytorch.utils import MemoryEfficientSwish\\n\\nfrom torch.optim import Adam\\nfrom torch.optim.lr_scheduler import OneCycleLR, ReduceLROnPlateau\"}, \"vectorWeights\": null}], \"totalResults\": 25}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from json import loads\n",
    "import weaviate\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "\n",
    "weaviate_client = weaviate.Client(\"http://202.151.177.149:81\")  # Replace with your endpoint\n",
    "some_objects = weaviate_client.data_object.get()\n",
    "if (json.dumps(some_objects)):\n",
    "    print(True)\n",
    "else:\n",
    "    print(False)\n",
    "\n",
    "print(json.dumps(some_objects))\n",
    "\n",
    "\n",
    "elastic_client = Elasticsearch(\"http://202.151.177.154:9200\")\n",
    "\n",
    "response = str(elastic_client.info())\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMLRecommendation(text: str, target_class: str) -> dict:\n",
    "    # md_text = obj['markdown']\n",
    "    # cur_class = \"grandmaster\"\n",
    "    near_text = {\"concepts\": [text]}\n",
    "    fetched = (weaviate_client.query\n",
    "                      .get(CLASS_NAME[target_class], [\"code\"])\n",
    "                      .with_near_text(near_text)\n",
    "                      .with_limit(1)\n",
    "                      .do()\n",
    "                      )\n",
    "    data = fetched['data']['Get'][CLASS_NAME[target_class]]\n",
    "    return data[0]\n",
    "\n",
    "def getElasticQuery(query, type=[\"base\", \"processed\"]):\n",
    "    if (type == \"processed\"):\n",
    "        result = {\n",
    "        \"query\": {\n",
    "            \"match\": {\n",
    "                \"processed\": {\n",
    "                    \"query\": query\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    else:\n",
    "        result = {\n",
    "        \"query\": {\n",
    "            \"match\": {\n",
    "                \"markdown\": {\n",
    "                    \"query\": query\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    return result \n",
    "\n",
    "def getElasticRecommendation(index, queryBody):\n",
    "    response = elastic_client.search(index=index, body=queryBody)\n",
    "\n",
    "    if response and response[\"hits\"][\"hits\"]:\n",
    "        result = response[\"hits\"][\"hits\"][0][\"_source\"]\n",
    "        return result\n",
    "    return []"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load recommendation result into JSON files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Empty Markdown in rank grandmaster = 22\n",
      "Total Empty Markdown in rank master = 116\n",
      "Total Empty Markdown in rank expert = 528\n"
     ]
    }
   ],
   "source": [
    "for file_name in FILES:\n",
    "    with open(f'data/{file_name}', 'r') as file:\n",
    "        data = json.load(file)\n",
    "        data_rank = file_name[:-21]\n",
    "\n",
    "        data_length = len(data)\n",
    "        testing_accumulate = []\n",
    "        count = 0\n",
    "        for row in data:\n",
    "            markdown = \"\".join(row['markdown'])\n",
    "            \n",
    "            if (not markdown):\n",
    "                count =  count + 1\n",
    "        print(f'Total Empty Markdown in rank {data_rank} = {count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Veerakit_Admin\\AppData\\Local\\Temp\\ipykernel_4200\\4069455083.py:38: DeprecationWarning: The 'body' parameter is deprecated and will be removed in a future version. Instead use individual parameters.\n",
      "  response = elastic_client.search(index=index, body=queryBody)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"markdown\": [\n",
      "        \"# Acknowledgements\\n\\n1. [M5 Forecasting - Starter Data Exploration](https://www.kaggle.com/robikscube/m5-forecasting-starter-data-exploration) ~ by Rob Mulla\\n2. [EDA and Baseline Model](https://www.kaggle.com/rdizzl3/eda-and-baseline-model) ~ by RDizzl3\\n3. [How to Create an ARIMA Model for Time Series Forecasting in Python](https://machinelearningmastery.com/arima-for-time-series-forecasting-with-python/) ~ by Machine Learning Mastery\\n4. [7 methods to perform Time Series forecasting (with Python codes)](https://www.analyticsvidhya.com/blog/2018/02/time-series-forecasting-methods/) ~ by Analytics Vidhya\\n5. [Economics for the IB Diploma](https://www.cambridge.org/core/books/economics-for-the-ib-diploma/1918CF16A8FC979AAB19951A487DCB1C) ~ by Ellie Tragakes\\n6. [Prophet](https://facebook.github.io/prophet/) ~ by Facebook\",\n",
      "        \"# Contents\\n\\n* [<font size=4>The dataset</font>](#1)\\n\\n\\n* [<font size=4>EDA</font>](#2)\\n    * [Preparing the ground](#2.1)\\n    * [Sales data](#2.2)\\n    * [Denoising](#2.3)\\n    * [Stores and sales](#2.4)\\n\\n    \\n* [<font size=4>Modeling</font>](#3)\\n    * [Train/Val split](#3.1)\\n    * [Naive approach](#3.2)\\n    * [Moving average](#3.3)\\n    * [Holt linear](#3.4)\\n    * [Exponential smoothing](#3.5)\\n    * [ARIMA](#3.6)\\n    * [Prophet](#3.7)\\n    * [Loss for each model](#3.8)\\n\\n\\n* [<font size=4>Takeaways</font>](#4)\\n\\n\\n* [<font size=4>Ending Note</font>](#5)\",\n",
      "        \"# The dataset <a id=\\\"1\\\"></a>\\n\\nThe dataset consists of five .csv files.\\n\\n* <code>calendar.csv</code> - Contains the dates on which products are sold. The dates are in a <code>yyyy/dd/mm</code> format.\\n\\n* <code>sales_train_validation.csv</code> - Contains the historical daily unit sales data per product and store <code>[d_1 - d_1913]</code>.\\n\\n* <code>submission.csv</code> - Demonstrates the correct format for submission to the competition.\\n\\n* <code>sell_prices.csv</code> - Contains information about the price of the products sold per store and date.\\n\\n* <code>sales_train_evaluation.csv</code> - Available one month before the competition deadline. It will include sales for <code>[d_1 - d_1941]</code>.\\n\\nIn this competition, we need to forecast the sales for <code>[d_1942 - d_1969]</code>. These rows form the evaluation set. The rows <code>[d_1914 - d_1941]</code> form the validation set, and the remaining rows form the training set. Now, since we understand the dataset and know what to predict, let us visualize the dataset.\",\n",
      "        \"# EDA <a id=\\\"2\\\"></a>\\n\\nNow, I will try to visualize the sales data and gain some insights from it.\",\n",
      "        \"## Preparing the ground <a id=\\\"2.1\\\"></a>\",\n",
      "        \"### Import libraries\"\n",
      "    ],\n",
      "    \"code\": \"# Reference: https://www.kaggle.com/code/tarunpaparaju/m5-competition-eda-models\\n\\nimport os\\nimport gc\\nimport time\\nimport math\\nimport datetime\\nfrom math import log, floor\\nfrom sklearn.neighbors import KDTree\\n\\nimport numpy as np\\nimport pandas as pd\\nfrom pathlib import Path\\nfrom sklearn.utils import shuffle\\nfrom tqdm.notebook import tqdm as tqdm\\n\\nimport seaborn as sns\\nfrom matplotlib import colors\\nimport matplotlib.pyplot as plt\\nfrom matplotlib.colors import Normalize\\n\\nimport plotly.express as px\\nimport plotly.graph_objects as go\\nimport plotly.figure_factory as ff\\nfrom plotly.subplots import make_subplots\\n\\nimport pywt\\nfrom statsmodels.robust import mad\\n\\nimport scipy\\nimport statsmodels\\nfrom scipy import signal\\nimport statsmodels.api as sm\\nfrom fbprophet import Prophet\\nfrom scipy.signal import butter, deconvolve\\nfrom statsmodels.tsa.arima_model import ARIMA\\nfrom statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing, Holt\\n\\nimport warnings\\nwarnings.filterwarnings(\\\"ignore\\\")\",\n",
      "    \"processed\": [\n",
      "        \"acknowledg 1 m5 forecast starter data explor http www kaggl com robikscub m5 forecast starter data explor rob mulla 2 eda baselin model http www kaggl com rdizzl3 eda baselin model rdizzl3 3 creat arima model time seri forecast python http machinelearningmasteri com arima time seri forecast python machin learn masteri 4 7 method perform time seri forecast python code http www analyticsvidhya com blog 2018 02 time seri forecast method analyt vidhya 5 econom ib diploma http www cambridg org core book econom ib diploma 1918cf16a8fc979aab19951a487dcb1c elli tragak 6 prophet http facebook github io prophet facebook\",\n",
      "        \"content font size 4 dataset font 1 font size 4 eda font 2 prepar ground 2 1 sale data 2 2 denois 2 3 store sale 2 4 font size 4 model font 3 train val split 3 1 naiv approach 3 2 move averag 3 3 holt linear 3 4 exponenti smooth 3 5 arima 3 6 prophet 3 7 loss model 3 8 font size 4 takeaway font 4 font size 4 end note font 5\",\n",
      "        \"dataset id 1 dataset consist five csv file code calendar csv code contain date product sold date code yyyi dd mm code format code sale train valid csv code contain histor daili unit sale data per product store code 1 1913 code code submiss csv code demonstr correct format submiss competit code sell price csv code contain inform price product sold per store date code sale train evalu csv code avail one month competit deadlin includ sale code 1 1941 code competit need forecast sale code 1942 1969 code row form evalu set row code 1914 1941 code form valid set remain row form train set sinc understand dataset know predict let u visual dataset\",\n",
      "        \"eda id 2 tri visual sale data gain insight\",\n",
      "        \"prepar ground id 2 1\",\n",
      "        \"import librari\"\n",
      "    ]\n",
      "}\n",
      "{\n",
      "    \"markdown\": [\n",
      "        \"## Algorithm to compute \\\"optimal\\\" coordinates for patches/tiles\\n\\nSorry if the code is messy and/or unreadable! But I tried to document/comment here and there to make it a bit clearer. Also, the algorithm is not optimized in terms of run-time (it's rather slow actually), but aims to optimize the coordinates of the patches/tiles.\\n\\n\\nMain sections:\\n\\n* [Computing patch coordinates with visualization](#Precompute-patch-coordinates-for-later-use)\\n* [Computing patches and stitching them together with visualization](#TensorFlow:-Stitch-patches-together,-using-tf-operations-and-tf.data.Dataset)\\n\"\n",
      "    ],\n",
      "    \"code\": \"!pip install imagecodecs\\nimport numpy as np\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\nimport cv2 \\nfrom tqdm.notebook import tqdm\\nimport skimage.io\\nimport tensorflow as tf\\nimport math\\nimport glob\\ndata = pd.read_csv('../input/prostate-cancer-grade-assessment/train.csv')\\ninput_path = '../input/prostate-cancer-grade-assessment/train_images/'\\ndata.head(3)\",\n",
      "    \"processed\": [\n",
      "        \"algorithm comput optim coordin patch tile sorri code messi unread tri document comment make bit clearer also algorithm optim term run time rather slow actual aim optim coordin patch tile main section comput patch coordin visual precomput patch coordin later use comput patch stitch togeth visual tensorflow stitch patch togeth use tf oper tf data dataset\"\n",
      "    ]\n",
      "}\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "for file_name in FILES:\n",
    "    with open(f'data/{file_name}', 'r') as file:\n",
    "        data = json.load(file)\n",
    "        data_rank = file_name[:-21]\n",
    "\n",
    "        data_length = len(data)\n",
    "        testing_accumulate = []\n",
    "        count = 0\n",
    "        \n",
    "\n",
    "        for row in data:\n",
    "            if (count>=1):\n",
    "                break\n",
    "            \n",
    "            markdown = \"\".join(row['markdown'])\n",
    "            \n",
    "            try:\n",
    "                temp_result = {\n",
    "                    \"original_md\": markdown,\n",
    "                    \"original_code\": row['code'],\n",
    "                    \"count\": count,\n",
    "                    \"data_rank\": data_rank\n",
    "                }\n",
    "\n",
    "                es_query = getElasticQuery(markdown)\n",
    "                es_result = getElasticRecommendation(data_rank, es_query)\n",
    "                es_result = json.dumps(es_result, indent=4)\n",
    "                print(es_result)\n",
    "\n",
    "                # # Get ML Recommendation\n",
    "                # recommended_code_ml = getMLRecommendation(markdown, data_rank)\n",
    "                # temp_result[\"recommended_code_ml\"] = recommended_code_ml['code']\n",
    "            \n",
    "                # # Get Elastic Recommendation\n",
    "                # query_base = getElasticQuery(markdown)\n",
    "                # query_processed = getElasticQuery(markdown, \"processed\")\n",
    "\n",
    "                # recommended_code_es_base = getElasticRecommendation(data_rank, query_base)\n",
    "                # recommended_code_es_processed = getElasticRecommendation(data_rank, query_processed)\n",
    "\n",
    "                # temp_result['recommended_code_es_base'] = recommended_code_es_base\n",
    "                # temp_result['recommended_code_es_processed'] = recommended_code_es_processed\n",
    "\n",
    "                # # Append to the collection\n",
    "                # testing_accumulate.append(temp_result)\n",
    "\n",
    "                # print(f'Count: {count}/{data_length}')\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f'[{datetime.now().strftime(\"%d/%m/%y %H:%M:%S\")}] Skipping item #{count} due to an error.')\n",
    "                logging.error(f\"Error occurred for item: {markdown}\\nError message: {str(e)}\\n>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\\n\")\n",
    "                continue\n",
    "            count =  count + 1\n",
    "            \n",
    "        \n",
    "        # with open(f\"recommendation result/{data_rank}_result.json\", \"w\") as file:\n",
    "        #     json.dump(testing_accumulate, file)\n",
    "        #     print(f\"successfully write to: {data_rank}_result.json \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_name in FILES:\n",
    "    with open(f'data/{file_name}', 'r') as file:\n",
    "        data = json.load(file)\n",
    "        data_rank = file_name[:-21]\n",
    "\n",
    "        data_length = len(data)\n",
    "        testing_accumulate = []\n",
    "        count = 0\n",
    "        for row in data:\n",
    "            markdown = \"\".join(row['markdown'])\n",
    "            \n",
    "            try:\n",
    "                # temp_result = {\n",
    "                #     \"original_md\": markdown,\n",
    "                #     \"original_code\": row['code'],\n",
    "                #     \"count\": count,\n",
    "                #     \"data_rank\": data_rank\n",
    "                # }\n",
    "\n",
    "                temp_result = {\n",
    "                    \"running_count_label\": count,\n",
    "                    \"data_rank\": data_rank,\n",
    "                    # Template for insert\n",
    "                    \"original\": {\n",
    "                        \"markdown\": \"\",\n",
    "                        \"code\": \"\",\n",
    "                    },\n",
    "                    \"mlRecommendation\": {\n",
    "                        \"code\": \"\"\n",
    "                    },\n",
    "                    \"bm25Recommendation\": {\n",
    "                        \"markdown\": \"\",\n",
    "                        \"processed_markdown\": \"\",\n",
    "                        \"code\": \"\",\n",
    "                    },\n",
    "                    \"bm25ProcessedRecommendation\": {\n",
    "                        \"markdown\": \"\",\n",
    "                        \"processed_markdown\": \"\",\n",
    "                        \"code\": \"\"\n",
    "                    }\n",
    "                }\n",
    "\n",
    "                # Get ML Recommendation\n",
    "                recommended_code_ml = getMLRecommendation(markdown, data_rank)\n",
    "                temp_result[\"mlRecommendation\"][\"code\"] = recommended_code_ml['code']\n",
    "            \n",
    "                # Get Elastic Recommendation\n",
    "                query_base = getElasticQuery(markdown)\n",
    "                query_processed = getElasticQuery(markdown, \"processed\")\n",
    "\n",
    "                recommended_code_es_base = getElasticRecommendation(data_rank, query_base)\n",
    "                recommended_code_es_processed = getElasticRecommendation(data_rank, query_processed)\n",
    "\n",
    "                temp_result[\"bm25Recommendation\"][\"markdown\"] = recommended_code_es_base['markdown']\n",
    "                temp_result[\"bm25Recommendation\"][\"processed_markdown\"] = recommended_code_es_base['processed']\n",
    "                temp_result[\"bm25Recommendation\"][\"code\"] = recommended_code_es_base['code']\n",
    "\n",
    "                temp_result[\"bm25ProcessedRecommendation\"][\"markdown\"] = recommended_code_es_base['markdown']\n",
    "                temp_result[\"bm25ProcessedRecommendation\"][\"processed_markdown\"] = recommended_code_es_base['processed']\n",
    "                temp_result[\"bm25ProcessedRecommendation\"][\"code\"] = recommended_code_es_base['code']\n",
    "\n",
    "                # temp_result['recommended_code_es_base'] = recommended_code_es_base['code']\n",
    "                # temp_result['recommended_code_es_processed'] = recommended_code_es_processed['code']\n",
    "\n",
    "                # Append to the collection\n",
    "                testing_accumulate.append(temp_result)\n",
    "\n",
    "                print(f'Count: {count}/{data_length}')\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f'[{datetime.now().strftime(\"%d/%m/%y %H:%M:%S\")}] Skipping item #{count} due to an error.')\n",
    "                logging.error(f\"Error occurred for item: {markdown}\\nError message: {str(e)}\\n>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\\n\")\n",
    "                continue\n",
    "            count =  count + 1\n",
    "        \n",
    "        with open(f\"recommendation result/{data_rank}_result.json\", \"w\") as file:\n",
    "            json.dump(testing_accumulate, file)\n",
    "            print(f\"successfully write to: {data_rank}_result.json \")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing original with recommended ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Veerakit_Admin\\AppData\\Local\\Temp\\ipykernel_12900\\2423600966.py:12: ResourceWarning: unclosed file <_io.TextIOWrapper name='data/expert_nl_pl_only_plot.json' mode='r' encoding='cp874'>\n",
      "  original_file_path = open(f'data/{data_rank}_nl_pl_only_plot.json')\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "C:\\Users\\Veerakit_Admin\\AppData\\Local\\Temp\\ipykernel_12900\\2423600966.py:12: ResourceWarning: unclosed file <_io.TextIOWrapper name='data/grandmaster_nl_pl_only_plot.json' mode='r' encoding='cp874'>\n",
      "  original_file_path = open(f'data/{data_rank}_nl_pl_only_plot.json')\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rank grandmaster: 2670 items || original : 2670 items\n",
      "rank master: 4098 items || original : 4098 items\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Veerakit_Admin\\AppData\\Local\\Temp\\ipykernel_12900\\2423600966.py:12: ResourceWarning: unclosed file <_io.TextIOWrapper name='data/master_nl_pl_only_plot.json' mode='r' encoding='cp874'>\n",
      "  original_file_path = open(f'data/{data_rank}_nl_pl_only_plot.json')\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rank expert: 10851 items || original : 10855 items\n"
     ]
    }
   ],
   "source": [
    "FILE_NAMES = [\n",
    "    \"grandmaster_result.json\",\n",
    "    \"master_result.json\",\n",
    "    \"expert_result.json\",\n",
    "]\n",
    "\n",
    "for name in FILE_NAMES:\n",
    "    path = f\"recommendation result/{name}\"\n",
    "    data_rank = name[:-12]\n",
    "\n",
    "    with open(path, 'r') as file:\n",
    "        original_file_path = open(f'data/{data_rank}_nl_pl_only_plot.json')\n",
    "        original_data = json.load(original_file_path)\n",
    "\n",
    "        data = json.load(file)\n",
    "\n",
    "        print(f\"rank {data_rank}: {len(data)} items || original : {len(original_data)} items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Veerakit_Admin\\AppData\\Local\\Temp\\ipykernel_12900\\3344088031.py:10: ResourceWarning: unclosed file <_io.TextIOWrapper name='data/expert_nl_pl_only_plot.json' mode='r' encoding='cp874'>\n",
      "  original_file_path = open(f'data/{data_rank}_nl_pl_only_plot.json')\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rank grandmaster: 2670 items || original : 2670 items\n",
      "Skipped 22 empty Markdown items in rank grandmaster\n",
      "rank master: 4098 items || original : 4098 items\n",
      "Skipped 116 empty Markdown items in rank master\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Veerakit_Admin\\AppData\\Local\\Temp\\ipykernel_12900\\3344088031.py:10: ResourceWarning: unclosed file <_io.TextIOWrapper name='data/grandmaster_nl_pl_only_plot.json' mode='r' encoding='cp874'>\n",
      "  original_file_path = open(f'data/{data_rank}_nl_pl_only_plot.json')\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "C:\\Users\\Veerakit_Admin\\AppData\\Local\\Temp\\ipykernel_12900\\3344088031.py:10: ResourceWarning: unclosed file <_io.TextIOWrapper name='data/master_nl_pl_only_plot.json' mode='r' encoding='cp874'>\n",
      "  original_file_path = open(f'data/{data_rank}_nl_pl_only_plot.json')\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rank expert: 10851 items || original : 10855 items\n",
      "Skipped 528 empty Markdown items in rank expert\n",
      "successfully write to /result/comparison_result.json\n",
      "Done!!\n",
      "\n",
      "Summarize:\n",
      "\n",
      "Rank Grandmaster\n",
      "- Total items: 2670\n",
      "- Total skipped empty Markdown: 22\n",
      "# Below is only for Elasticsearch base approach #\n",
      "- Total correct original-recommended pairs: 2512\n",
      "- Total incorrect original-recommended pairs: 136\n",
      "\n",
      "Rank Master\n",
      "- Total items: 4098\n",
      "- Total skipped empty Markdown: 116\n",
      "# Below is only for Elasticsearch base approach #\n",
      "- Total correct original-recommended pairs: 3572\n",
      "- Total incorrect original-recommended pairs: 410\n",
      "\n",
      "Rank Expert\n",
      "- Total items: 10851\n",
      "- Total skipped empty Markdown: 528\n",
      "# Below is only for Elasticsearch base approach #\n",
      "- Total correct original-recommended pairs: 9187\n",
      "- Total incorrect original-recommended pairs: 1136\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output = {}\n",
    "metadata = {\n",
    "    \n",
    "}\n",
    "for name in FILE_NAMES:\n",
    "    path = f\"recommendation result/{name}\"\n",
    "    data_rank = name[:-12]\n",
    "\n",
    "    with open(path, 'r') as file:\n",
    "        original_file_path = open(f'data/{data_rank}_nl_pl_only_plot.json')\n",
    "        original_data = json.load(original_file_path)\n",
    "        data = json.load(file)\n",
    "        print(f\"rank {data_rank}: {len(data)} items || original : {len(original_data)} items\")\n",
    "\n",
    "        comparison_collection = {\n",
    "            \"ml\": 0,\n",
    "            \"es\": 0,\n",
    "            \"es-processed\": 0,\n",
    "        }\n",
    "        incorrect_summarize = {\n",
    "            # \"ml\": 0,\n",
    "            \"es\": 0,\n",
    "            # \"es-processed\": 0,\n",
    "        }\n",
    "\n",
    "        incorrect_pairs = []\n",
    "        incorrect = {\n",
    "\n",
    "        }\n",
    "        i = 0\n",
    "        count_empty = 0\n",
    "        for item in data:\n",
    "            if (not item['original_md']):\n",
    "                count_empty = count_empty + 1\n",
    "                continue\n",
    "\n",
    "            original_code = item['original_code']\n",
    "            ml_code = item['recommended_code_ml']\n",
    "            es_base_code = item['recommended_code_es_base']\n",
    "            es_processed_code = item['recommended_code_es_processed']\n",
    "\n",
    "            if (item['original_code'] == item['recommended_code_ml']):\n",
    "                comparison_collection[\"ml\"] = comparison_collection[\"ml\"] + 1\n",
    "\n",
    "            if (item['original_code'] == item['recommended_code_es_processed']):\n",
    "                comparison_collection[\"es-processed\"] = comparison_collection[\"es-processed\"] + 1\n",
    "                \n",
    "            if (item['original_code'] == item['recommended_code_es_base']):\n",
    "                comparison_collection[\"es\"] = comparison_collection[\"es\"] + 1\n",
    "                \n",
    "            if (item['original_code'] != item['recommended_code_es_base']):\n",
    "                incorrect_summarize[\"es\"] = incorrect_summarize[\"es\"] + 1\n",
    "                incorrect_pairs.append({\n",
    "                    \"originalMarkdown\": item['original_md'],\n",
    "                    \"originalCode\": item['original_code'],\n",
    "                    \"bm25Code\": item['recommended_code_es_base']\n",
    "                })\n",
    "        \n",
    "        print(f\"Skipped {count_empty} empty Markdown items in rank {data_rank}\")\n",
    "        \n",
    "        output[data_rank] = {\n",
    "            \"correct_pairs_summarize\": comparison_collection,\n",
    "            \"incorrect_summarize\": {\n",
    "                \"es\": len(data)-count_empty-comparison_collection[\"es\"]\n",
    "            },\n",
    "            \"incorrect_pairs_items\": incorrect_pairs,\n",
    "        }\n",
    "\n",
    "        metadata[data_rank] = {\n",
    "            \"total_items\": len(data),\n",
    "            \"total_skipped_empty_markdown\": count_empty,\n",
    "            \"total_es_correct_pairs_\": comparison_collection[\"es\"],\n",
    "            \"total_es_incorrect_pairs_\": incorrect_summarize[\"es\"]\n",
    "        }\n",
    "        \n",
    "with open(f\"comparison_result.json\", \"w\") as file:\n",
    "    output = json.dump(output, file)\n",
    "    print(f'successfully write to /result/comparison_result.json')\n",
    "print(\"Done!!\")\n",
    "\n",
    "print(f\"\"\"\n",
    "Summarize:\n",
    "\n",
    "Rank Grandmaster\n",
    "- Total items: {metadata['grandmaster'][\"total_items\"]}\n",
    "- Total skipped empty Markdown: {metadata['grandmaster'][\"total_skipped_empty_markdown\"]}\n",
    "# Below is only for Elasticsearch base approach #\n",
    "- Total correct original-recommended pairs: {metadata['grandmaster'][\"total_es_correct_pairs_\"]}\n",
    "- Total incorrect original-recommended pairs: {metadata['grandmaster'][\"total_es_incorrect_pairs_\"]}\n",
    "\n",
    "Rank Master\n",
    "- Total items: {metadata['master'][\"total_items\"]}\n",
    "- Total skipped empty Markdown: {metadata['master'][\"total_skipped_empty_markdown\"]}\n",
    "# Below is only for Elasticsearch base approach #\n",
    "- Total correct original-recommended pairs: {metadata['master'][\"total_es_correct_pairs_\"]}\n",
    "- Total incorrect original-recommended pairs: {metadata['master'][\"total_es_incorrect_pairs_\"]}\n",
    "\n",
    "Rank Expert\n",
    "- Total items: {metadata['expert'][\"total_items\"]}\n",
    "- Total skipped empty Markdown: {metadata['expert'][\"total_skipped_empty_markdown\"]}\n",
    "# Below is only for Elasticsearch base approach #\n",
    "- Total correct original-recommended pairs: {metadata['expert'][\"total_es_correct_pairs_\"]}\n",
    "- Total incorrect original-recommended pairs: {metadata['expert'][\"total_es_incorrect_pairs_\"]}\n",
    "\"\"\")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter and take some sample of data for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "import json\n",
    "RANKS = ['grandmaster', 'master', 'expert']\n",
    "MAX_SAMPLE = 30\n",
    "\n",
    "with open(\"comparison_result.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "    for rank in RANKS:\n",
    "        incorrect_pairs = data[rank]['incorrect_pairs_items']\n",
    "        with open(f'./analysis-result/{rank}_analysis.txt', 'w', encoding=\"utf-8\") as file:\n",
    "            random_range = list(range(0, len(incorrect_pairs)))\n",
    "            random.shuffle(random_range)\n",
    "\n",
    "            i = 0\n",
    "            while i<MAX_SAMPLE:\n",
    "                if (i>=MAX_SAMPLE):\n",
    "                    break\n",
    "                file.write(f\"ITEM #{i+1}\\n\\n\")\n",
    "                file.write(f\"ORIGINAL MARKDOWN:\\n{'-'*99}\\n\")\n",
    "                file.write(str(incorrect_pairs[i]['originalMarkdown']))\n",
    "                file.write(\"\\n================================================================\\n\")\n",
    "                file.write(\"RECOMMENDED CODE:\\n----------------------------------------------------\\n\")\n",
    "                file.write(str(incorrect_pairs[i]['bm25Code']))\n",
    "                file.write(\"\\n================================================================\\n\")\n",
    "                file.write(\"ORIGINAL CODE:\\n----------------------------------------------------\\n\")\n",
    "                file.write(str(incorrect_pairs[i]['originalCode']))\n",
    "                file.write(\"\\n\" + \"++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\\n\"*3 + \"\\n\")\n",
    "\n",
    "                i = i+1\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtualenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
