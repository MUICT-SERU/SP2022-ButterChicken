{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get BM25 recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(filename='error.log', level=logging.ERROR)\n",
    "from datetime import datetime\n",
    "FILES = [\n",
    "    \"grandmaster_nl_pl_only_plot.json\",\n",
    "    \"master_nl_pl_only_plot.json\",\n",
    "    \"expert_nl_pl_only_plot.json\",\n",
    "]\n",
    "\n",
    "CLASS_NAME = {\n",
    "    \"grandmaster\": \"GrandMasterCode\",\n",
    "    \"master\": \"MasterCode\",\n",
    "    \"expert\": \"ExpertCode\",\n",
    "}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weaviate Instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from json import loads\n",
    "import weaviate\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "\n",
    "weaviate_client = weaviate.Client(\"http://202.151.177.149:81\")  # Replace with your endpoint\n",
    "some_objects = weaviate_client.data_object.get()\n",
    "if (json.dumps(some_objects)):\n",
    "    print(True)\n",
    "else:\n",
    "    print(False)\n",
    "\n",
    "print(json.dumps(some_objects))\n",
    "\n",
    "\n",
    "elastic_client = Elasticsearch(\"http://202.151.177.154:9200\")\n",
    "\n",
    "response = str(elastic_client.info())\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMLRecommendation(text: str, target_class: str) -> dict:\n",
    "    # md_text = obj['markdown']\n",
    "    # cur_class = \"grandmaster\"\n",
    "    near_text = {\"concepts\": [text]}\n",
    "    fetched = (weaviate_client.query\n",
    "                      .get(CLASS_NAME[target_class], [\"code\"])\n",
    "                      .with_near_text(near_text)\n",
    "                      .with_limit(1)\n",
    "                      .do()\n",
    "                      )\n",
    "    data = fetched['data']['Get'][CLASS_NAME[target_class]]\n",
    "    return data[0]\n",
    "\n",
    "def getElasticQuery(query, type=[\"base\", \"processed\"]):\n",
    "    if (type == \"processed\"):\n",
    "        result = {\n",
    "        \"query\": {\n",
    "            \"match\": {\n",
    "                \"processed\": {\n",
    "                    \"query\": query\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    else:\n",
    "        result = {\n",
    "        \"query\": {\n",
    "            \"match\": {\n",
    "                \"markdown\": {\n",
    "                    \"query\": query\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    return result \n",
    "\n",
    "def getElasticRecommendation(index, queryBody):\n",
    "    response = elastic_client.search(index=index, body=queryBody)\n",
    "\n",
    "    if response and response[\"hits\"][\"hits\"]:\n",
    "        result = response[\"hits\"][\"hits\"][0][\"_source\"]\n",
    "        return result['code']\n",
    "    return []"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load recommendation result into JSON files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Empty Markdown in rank grandmaster = 22\n",
      "Total Empty Markdown in rank master = 116\n",
      "Total Empty Markdown in rank expert = 528\n"
     ]
    }
   ],
   "source": [
    "for file_name in FILES:\n",
    "    with open(f'data/{file_name}', 'r') as file:\n",
    "        data = json.load(file)\n",
    "        data_rank = file_name[:-21]\n",
    "\n",
    "        data_length = len(data)\n",
    "        testing_accumulate = []\n",
    "        count = 0\n",
    "        for row in data:\n",
    "            markdown = \"\".join(row['markdown'])\n",
    "            \n",
    "            if (not markdown):\n",
    "                count =  count + 1\n",
    "        print(f'Total Empty Markdown in rank {data_rank} = {count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_name in FILES:\n",
    "    with open(f'data/{file_name}', 'r') as file:\n",
    "        data = json.load(file)\n",
    "        data_rank = file_name[:-21]\n",
    "\n",
    "        data_length = len(data)\n",
    "        testing_accumulate = []\n",
    "        count = 0\n",
    "        for row in data:\n",
    "            markdown = \"\".join(row['markdown'])\n",
    "            \n",
    "            try:\n",
    "                temp_result = {\n",
    "                    \"original_md\": markdown,\n",
    "                    \"original_code\": row['code'],\n",
    "                    \"count\": count,\n",
    "                    \"data_rank\": data_rank\n",
    "                }\n",
    "\n",
    "                # Get ML Recommendation\n",
    "                recommended_code_ml = getMLRecommendation(markdown, data_rank)\n",
    "                temp_result[\"recommended_code_ml\"] = recommended_code_ml['code']\n",
    "            \n",
    "                # Get Elastic Recommendation\n",
    "                query_base = getElasticQuery(markdown)\n",
    "                query_processed = getElasticQuery(markdown, \"processed\")\n",
    "\n",
    "                recommended_code_es_base = getElasticRecommendation(data_rank, query_base)\n",
    "                recommended_code_es_processed = getElasticRecommendation(data_rank, query_processed)\n",
    "\n",
    "                temp_result['recommended_code_es_base'] = recommended_code_es_base\n",
    "                temp_result['recommended_code_es_processed'] = recommended_code_es_processed\n",
    "\n",
    "                # Append to the collection\n",
    "                testing_accumulate.append(temp_result)\n",
    "\n",
    "                print(f'Count: {count}/{data_length}')\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f'[{datetime.now().strftime(\"%d/%m/%y %H:%M:%S\")}] Skipping item #{count} due to an error.')\n",
    "                logging.error(f\"Error occurred for item: {markdown}\\nError message: {str(e)}\\n>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\\n\")\n",
    "                continue\n",
    "            count =  count + 1\n",
    "        \n",
    "        with open(f\"recommendation result/{data_rank}_result.json\", \"w\") as file:\n",
    "            json.dump(testing_accumulate, file)\n",
    "            print(f\"successfully write to: {data_rank}_result.json \")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing original with recommended ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Veerakit_Admin\\AppData\\Local\\Temp\\ipykernel_12900\\2423600966.py:12: ResourceWarning: unclosed file <_io.TextIOWrapper name='data/expert_nl_pl_only_plot.json' mode='r' encoding='cp874'>\n",
      "  original_file_path = open(f'data/{data_rank}_nl_pl_only_plot.json')\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "C:\\Users\\Veerakit_Admin\\AppData\\Local\\Temp\\ipykernel_12900\\2423600966.py:12: ResourceWarning: unclosed file <_io.TextIOWrapper name='data/grandmaster_nl_pl_only_plot.json' mode='r' encoding='cp874'>\n",
      "  original_file_path = open(f'data/{data_rank}_nl_pl_only_plot.json')\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rank grandmaster: 2670 items || original : 2670 items\n",
      "rank master: 4098 items || original : 4098 items\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Veerakit_Admin\\AppData\\Local\\Temp\\ipykernel_12900\\2423600966.py:12: ResourceWarning: unclosed file <_io.TextIOWrapper name='data/master_nl_pl_only_plot.json' mode='r' encoding='cp874'>\n",
      "  original_file_path = open(f'data/{data_rank}_nl_pl_only_plot.json')\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rank expert: 10851 items || original : 10855 items\n"
     ]
    }
   ],
   "source": [
    "FILE_NAMES = [\n",
    "    \"grandmaster_result.json\",\n",
    "    \"master_result.json\",\n",
    "    \"expert_result.json\",\n",
    "]\n",
    "\n",
    "for name in FILE_NAMES:\n",
    "    path = f\"recommendation result/{name}\"\n",
    "    data_rank = name[:-12]\n",
    "\n",
    "    with open(path, 'r') as file:\n",
    "        original_file_path = open(f'data/{data_rank}_nl_pl_only_plot.json')\n",
    "        original_data = json.load(original_file_path)\n",
    "\n",
    "        data = json.load(file)\n",
    "\n",
    "        print(f\"rank {data_rank}: {len(data)} items || original : {len(original_data)} items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Veerakit_Admin\\AppData\\Local\\Temp\\ipykernel_12900\\3344088031.py:10: ResourceWarning: unclosed file <_io.TextIOWrapper name='data/expert_nl_pl_only_plot.json' mode='r' encoding='cp874'>\n",
      "  original_file_path = open(f'data/{data_rank}_nl_pl_only_plot.json')\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rank grandmaster: 2670 items || original : 2670 items\n",
      "Skipped 22 empty Markdown items in rank grandmaster\n",
      "rank master: 4098 items || original : 4098 items\n",
      "Skipped 116 empty Markdown items in rank master\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Veerakit_Admin\\AppData\\Local\\Temp\\ipykernel_12900\\3344088031.py:10: ResourceWarning: unclosed file <_io.TextIOWrapper name='data/grandmaster_nl_pl_only_plot.json' mode='r' encoding='cp874'>\n",
      "  original_file_path = open(f'data/{data_rank}_nl_pl_only_plot.json')\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "C:\\Users\\Veerakit_Admin\\AppData\\Local\\Temp\\ipykernel_12900\\3344088031.py:10: ResourceWarning: unclosed file <_io.TextIOWrapper name='data/master_nl_pl_only_plot.json' mode='r' encoding='cp874'>\n",
      "  original_file_path = open(f'data/{data_rank}_nl_pl_only_plot.json')\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rank expert: 10851 items || original : 10855 items\n",
      "Skipped 528 empty Markdown items in rank expert\n",
      "successfully write to /result/comparison_result.json\n",
      "Done!!\n",
      "\n",
      "Summarize:\n",
      "\n",
      "Rank Grandmaster\n",
      "- Total items: 2670\n",
      "- Total skipped empty Markdown: 22\n",
      "# Below is only for Elasticsearch base approach #\n",
      "- Total correct original-recommended pairs: 2512\n",
      "- Total incorrect original-recommended pairs: 136\n",
      "\n",
      "Rank Master\n",
      "- Total items: 4098\n",
      "- Total skipped empty Markdown: 116\n",
      "# Below is only for Elasticsearch base approach #\n",
      "- Total correct original-recommended pairs: 3572\n",
      "- Total incorrect original-recommended pairs: 410\n",
      "\n",
      "Rank Expert\n",
      "- Total items: 10851\n",
      "- Total skipped empty Markdown: 528\n",
      "# Below is only for Elasticsearch base approach #\n",
      "- Total correct original-recommended pairs: 9187\n",
      "- Total incorrect original-recommended pairs: 1136\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output = {}\n",
    "metadata = {\n",
    "    \n",
    "}\n",
    "for name in FILE_NAMES:\n",
    "    path = f\"recommendation result/{name}\"\n",
    "    data_rank = name[:-12]\n",
    "\n",
    "    with open(path, 'r') as file:\n",
    "        original_file_path = open(f'data/{data_rank}_nl_pl_only_plot.json')\n",
    "        original_data = json.load(original_file_path)\n",
    "        data = json.load(file)\n",
    "        print(f\"rank {data_rank}: {len(data)} items || original : {len(original_data)} items\")\n",
    "\n",
    "        comparison_collection = {\n",
    "            \"ml\": 0,\n",
    "            \"es\": 0,\n",
    "            \"es-processed\": 0,\n",
    "        }\n",
    "        incorrect_summarize = {\n",
    "            # \"ml\": 0,\n",
    "            \"es\": 0,\n",
    "            # \"es-processed\": 0,\n",
    "        }\n",
    "\n",
    "        incorrect_pairs = []\n",
    "        incorrect = {\n",
    "\n",
    "        }\n",
    "        i = 0\n",
    "        count_empty = 0\n",
    "        for item in data:\n",
    "            if (not item['original_md']):\n",
    "                count_empty = count_empty + 1\n",
    "                continue\n",
    "\n",
    "            original_code = item['original_code']\n",
    "            ml_code = item['recommended_code_ml']\n",
    "            es_base_code = item['recommended_code_es_base']\n",
    "            es_processed_code = item['recommended_code_es_processed']\n",
    "\n",
    "            if (item['original_code'] == item['recommended_code_ml']):\n",
    "                comparison_collection[\"ml\"] = comparison_collection[\"ml\"] + 1\n",
    "\n",
    "            if (item['original_code'] == item['recommended_code_es_processed']):\n",
    "                comparison_collection[\"es-processed\"] = comparison_collection[\"es-processed\"] + 1\n",
    "                \n",
    "            if (item['original_code'] == item['recommended_code_es_base']):\n",
    "                comparison_collection[\"es\"] = comparison_collection[\"es\"] + 1\n",
    "                \n",
    "            if (item['original_code'] != item['recommended_code_es_base']):\n",
    "                incorrect_summarize[\"es\"] = incorrect_summarize[\"es\"] + 1\n",
    "                incorrect_pairs.append({\n",
    "                    \"originalMarkdown\": item['original_md'],\n",
    "                    \"originalCode\": item['original_code'],\n",
    "                    \"bm25Code\": item['recommended_code_es_base']\n",
    "                })\n",
    "        \n",
    "        print(f\"Skipped {count_empty} empty Markdown items in rank {data_rank}\")\n",
    "        \n",
    "        output[data_rank] = {\n",
    "            \"correct_pairs_summarize\": comparison_collection,\n",
    "            \"incorrect_summarize\": {\n",
    "                \"es\": len(data)-count_empty-comparison_collection[\"es\"]\n",
    "            },\n",
    "            \"incorrect_pairs_items\": incorrect_pairs,\n",
    "        }\n",
    "\n",
    "        metadata[data_rank] = {\n",
    "            \"total_items\": len(data),\n",
    "            \"total_skipped_empty_markdown\": count_empty,\n",
    "            \"total_es_correct_pairs_\": comparison_collection[\"es\"],\n",
    "            \"total_es_incorrect_pairs_\": incorrect_summarize[\"es\"]\n",
    "        }\n",
    "        \n",
    "with open(f\"comparison_result.json\", \"w\") as file:\n",
    "    output = json.dump(output, file)\n",
    "    print(f'successfully write to /result/comparison_result.json')\n",
    "print(\"Done!!\")\n",
    "\n",
    "print(f\"\"\"\n",
    "Summarize:\n",
    "\n",
    "Rank Grandmaster\n",
    "- Total items: {metadata['grandmaster'][\"total_items\"]}\n",
    "- Total skipped empty Markdown: {metadata['grandmaster'][\"total_skipped_empty_markdown\"]}\n",
    "# Below is only for Elasticsearch base approach #\n",
    "- Total correct original-recommended pairs: {metadata['grandmaster'][\"total_es_correct_pairs_\"]}\n",
    "- Total incorrect original-recommended pairs: {metadata['grandmaster'][\"total_es_incorrect_pairs_\"]}\n",
    "\n",
    "Rank Master\n",
    "- Total items: {metadata['master'][\"total_items\"]}\n",
    "- Total skipped empty Markdown: {metadata['master'][\"total_skipped_empty_markdown\"]}\n",
    "# Below is only for Elasticsearch base approach #\n",
    "- Total correct original-recommended pairs: {metadata['master'][\"total_es_correct_pairs_\"]}\n",
    "- Total incorrect original-recommended pairs: {metadata['master'][\"total_es_incorrect_pairs_\"]}\n",
    "\n",
    "Rank Expert\n",
    "- Total items: {metadata['expert'][\"total_items\"]}\n",
    "- Total skipped empty Markdown: {metadata['expert'][\"total_skipped_empty_markdown\"]}\n",
    "# Below is only for Elasticsearch base approach #\n",
    "- Total correct original-recommended pairs: {metadata['expert'][\"total_es_correct_pairs_\"]}\n",
    "- Total incorrect original-recommended pairs: {metadata['expert'][\"total_es_incorrect_pairs_\"]}\n",
    "\"\"\")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter and take some sample of data for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "import json\n",
    "RANKS = ['grandmaster', 'master', 'expert']\n",
    "MAX_SAMPLE = 30\n",
    "\n",
    "with open(\"comparison_result.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "    for rank in RANKS:\n",
    "        incorrect_pairs = data[rank]['incorrect_pairs_items']\n",
    "        with open(f'./analysis-result/{rank}_analysis.txt', 'w', encoding=\"utf-8\") as file:\n",
    "            random_range = list(range(0, len(incorrect_pairs)))\n",
    "            random.shuffle(random_range)\n",
    "\n",
    "            i = 0\n",
    "            while i<MAX_SAMPLE:\n",
    "                if (i>=MAX_SAMPLE):\n",
    "                    break\n",
    "                file.write(f\"ITEM #{i+1}\\n\\n\")\n",
    "                file.write(f\"ORIGINAL MARKDOWN:\\n{'-'*99}\\n\")\n",
    "                file.write(str(incorrect_pairs[i]['originalMarkdown']))\n",
    "                file.write(\"\\n================================================================\\n\")\n",
    "                file.write(\"RECOMMENDED CODE:\\n----------------------------------------------------\\n\")\n",
    "                file.write(str(incorrect_pairs[i]['bm25Code']))\n",
    "                file.write(\"\\n================================================================\\n\")\n",
    "                file.write(\"ORIGINAL CODE:\\n----------------------------------------------------\\n\")\n",
    "                file.write(str(incorrect_pairs[i]['originalCode']))\n",
    "                file.write(\"\\n\" + \"++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\\n\"*3 + \"\\n\")\n",
    "\n",
    "                i = i+1\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtualenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
