markdown_content,index_in_notebook,author_name,notebook_title,markdown_key
"['## Our goal\n', '\n', 'In this competition we are asked to predict if a customer will make a transaction or not regardless of the amount of money transacted. Hence our goal is to solve a binary classification problem. In the data description you can see that the features given are numeric and anonymized. Furthermore the data seems to be artificial as they state that ""the data has the same structure as our real data"". \n', '\n', '### Table of contents\n', '\n', '1. [Loading packages](#load) (complete)\n', '2. [Sneak a peek at the data](#data) (complete)\n', '2. [What can we say about the target?](#target) (complete)\n', '3. [Can we find relationships between features?](#correlation) (complete)\n', '4. [Baseline submission](#baselines) (complete)\n', '5. [Basic feature engineering](#engineering) (complete)\n', '6. [Gaussian Mixture Clustering](#clustering) (complete)']",0,allunia,santander-customer-transaction-eda,allunia_santander-customer-transaction-eda_m0
['## Kernel settings'],1,allunia,santander-customer-transaction-eda,allunia_santander-customer-transaction-eda_m1
"['## Loading packages <a class=""anchor"" id=""load""></a>']",2,allunia,santander-customer-transaction-eda,allunia_santander-customer-transaction-eda_m2
"['## Sneak a peek at the data <a class=""anchor"" id=""data""></a>']",3,allunia,santander-customer-transaction-eda,allunia_santander-customer-transaction-eda_m3
['### Train'],4,allunia,santander-customer-transaction-eda,allunia_santander-customer-transaction-eda_m4
"['Ok, 200.000 rows and 202 features. ']",5,allunia,santander-customer-transaction-eda,allunia_santander-customer-transaction-eda_m5
"[""The target as well as the ID-Code of a sample are 2 special variables. Consequently 200 features are left. Browsing through the columns we can see that they look really numeric. It seems that there are no counter or integer variables. In addition it looks like if there are no missing values. Let's check it out:""]",6,allunia,santander-customer-transaction-eda,allunia_santander-customer-transaction-eda_m6
['### Test'],7,allunia,santander-customer-transaction-eda,allunia_santander-customer-transaction-eda_m7
['At a first glance this looks similar to train except from the missing target.'],8,allunia,santander-customer-transaction-eda,allunia_santander-customer-transaction-eda_m8
"['### Submission\n', '\n', ""Before we start, let's look at the sample submission as well:""]",9,allunia,santander-customer-transaction-eda,allunia_santander-customer-transaction-eda_m9
"['Ok, not much to say about it.']",10,allunia,santander-customer-transaction-eda,allunia_santander-customer-transaction-eda_m10
"['## What can we say about the target? <a class=""anchor"" id=""target""></a>']",11,allunia,santander-customer-transaction-eda,allunia_santander-customer-transaction-eda_m11
"['### Take Away\n', '\n', '* We have to solve an imbalanced class problem. The number of customers that will not make a transaction is much higher than those that will. \n', '* It seem that there is no relationship of the target with the index of the train dataframe. This is more empressend by the zero targets than for the ones. \n', '* Take a look at the jitter plots within the violinplots. We can see that the targets look uniformly distributed over the indexes. It seems that the competitors were careful during the process of ordering the data. Once more this indicates that the data is simulated.']",12,allunia,santander-customer-transaction-eda,allunia_santander-customer-transaction-eda_m12
"['## Can we find relationships between features? <a class=""anchor"" id=""correlation""></a>']",13,allunia,santander-customer-transaction-eda,allunia_santander-customer-transaction-eda_m13
"['### Linear correlations\n', '\n', ""I have already seen some correlation heatmaps in public kernels and it seems as if there is almost no correlation between features. Let's check this out by computing all correlation values and plotting the overall distribution:""]",14,allunia,santander-customer-transaction-eda,allunia_santander-customer-transaction-eda_m14
['Woooow! :-O All features seem to have no linear correlation!!! Neither in train nor in test. Very strange. We know that they are anonymized and perhaps they are decorrelated by some transformation as well. '],15,allunia,santander-customer-transaction-eda,allunia_santander-customer-transaction-eda_m15
"['### Random Forest Top Features\n', '\n', ""To start easy, let's use a random forest to select top 10 features. They can serve as a starting point to discover their nature and for trying to understand the data. In addition they may yield some ideas on how to generate new features. I am going to use stratified KFold as a cross validation strategy. It's somehow arbitrary to use KFold as we don't know if we have time series data given, but it may serve as a good starting point. \n"", '\n', ""To start simple I like to use a random forest that helps us to select important features. As there are no linear correlations it's a good idea to start with a nonlinear model that allows us to discover features, their importances as well as interactions. Let's start! :-)""]",16,allunia,santander-customer-transaction-eda,allunia_santander-customer-transaction-eda_m16
['You can see that the score is not as good as some other scores of public kernels but nontheless my attempt is to understand the data by improving this score. We can use more powerful models later on.'],17,allunia,santander-customer-transaction-eda,allunia_santander-customer-transaction-eda_m17
"[""Let's take a look at n_top features of your choice:""]",18,allunia,santander-customer-transaction-eda,allunia_santander-customer-transaction-eda_m18
"['Ok, that\'s enough to start with the ""data-understanding-journey"".']",19,allunia,santander-customer-transaction-eda,allunia_santander-customer-transaction-eda_m19
"['### Exploring top features\n', '\n', 'First of all: How do the distributions of the variables look like with respect to the targets in train? Can we observe discrepancies between train and test features for selected top features?']",20,allunia,santander-customer-transaction-eda,allunia_santander-customer-transaction-eda_m20
"['### Take Away\n', '\n', '* Interestingly there are some peeks inside the distributions, especially for variables 81, 12 and 53. Why do these data points accumulate on these values?\n', '* We can observe that the accumulations are less dense in the test data. \n', '* Variable 174 seem to miss the bulb on the right hand side of the distribution in the test data. ']",21,allunia,santander-customer-transaction-eda,allunia_santander-customer-transaction-eda_m21
['### How do the scatter plots look like?'],22,allunia,santander-customer-transaction-eda,allunia_santander-customer-transaction-eda_m22
['Crazy! Can you see the sharp limits of several variables where the samples with target 1 suddenly accumulate and seldomly pass over. Look at var 81 and 12 for example. You can see that there are limits close to 10 (var 81) and 13.5 (var 12). This finding could be a nice entry point for further feature engineering.'],23,allunia,santander-customer-transaction-eda,allunia_santander-customer-transaction-eda_m23
"['## Baseline submissions \n', '\n', '### What score does the forest yield on public LB?']",24,allunia,santander-customer-transaction-eda,allunia_santander-customer-transaction-eda_m24
['Yields 0.662 on public LB.'],25,allunia,santander-customer-transaction-eda,allunia_santander-customer-transaction-eda_m25
"['## Feature engineering \n', '\n', ""Let's do some basic feature engineering. Perhaps it helps to improve:""]",26,allunia,santander-customer-transaction-eda,allunia_santander-customer-transaction-eda_m26
['### Rounding & quantile based binning'],27,allunia,santander-customer-transaction-eda,allunia_santander-customer-transaction-eda_m27
['### New feature importances'],28,allunia,santander-customer-transaction-eda,allunia_santander-customer-transaction-eda_m28
"['## Gaussian Mixture Clustering  <a class=""anchor"" id=""clustering""></a>\n', '\n', ""The majority of the data looks like a big gaussian distribution. Besides that there seems to be at least one or two more gaussians that could explain the second and third mode that we can find for important features. Let's motivate this even further by looking at scatter and kde-plots of some top-features:""]",29,allunia,santander-customer-transaction-eda,allunia_santander-customer-transaction-eda_m29
"['* At least one big gaussians with one or two small, very thin but long gaussians.\n', ""* It's very interesting that we can still find outliers beside sharp lines. \n"", '\n', ""Let's assume now that the data was generated using a mixture of gaussians and let's try to cluster them. Perhaps we can see that some clusters occupy more hot targets than others.""]",30,allunia,santander-customer-transaction-eda,allunia_santander-customer-transaction-eda_m30
"['### Take-Away\n', '\n', ""* By fitting the gaussian mixture model we are maximizing the log likelihood. The higher, the better the gaussians suite to our data. As it's difficult to choose the right number of components (gaussians) I decided to use a stratified k fold of the train data. This way we can fit gaussians to a train subset, and test how big the log likelihood is on the test subset. By doing so three times for each selected component, we gain some more information about the stability of our solution. **We can see that 3 gaussians seem to be sufficient as the log likelihood values decrease with more components**. \n"", '* This need not be true as the **solution depends on the initialization of the gaussians (the seeds I used) and with more data, the result may be different**. \n', '* But we can say: There are **at least 3 gaussians**. This is what we have already found by visual exploration of the data. \n', ""* The individual score per data spot can be understood as a measure of density. If it's low, the data spot lives in a region with other data points far away. If it's high, it should have a lot of neighbors. Consequently the individual logL-score can tell us something about outliers in the data.""]",31,allunia,santander-customer-transaction-eda,allunia_santander-customer-transaction-eda_m31
"[""* As we have much more cold-targets (zero) that hot (ones), I'm not surprised that hot targets occupy only a small part of the data per cluster. Nonetheless we can see that cluster 1 has significantly more hot targets than the others.\n"", '* The second plot shows that most hot targets are located in cluster 1 followed by cluster 2. This confirms our assumption that the big gaussian in the middle (cluster 0) has the smallest amount of hot targets and that the small, thin side distributions are more likely to have hot targets. ']",32,allunia,santander-customer-transaction-eda,allunia_santander-customer-transaction-eda_m32
['Only some features are important to separate the structure of the data. '],33,allunia,santander-customer-transaction-eda,allunia_santander-customer-transaction-eda_m33
['Good luck for the last days! :-)'],34,allunia,santander-customer-transaction-eda,allunia_santander-customer-transaction-eda_m34
['![Imgur](https://i.imgur.com/L2Jaqm8.png)'],0,amrmahmoud123,1-guide-to-ensembling-methods,amrmahmoud123_1-guide-to-ensembling-methods_m0
"['What is in this tutorial?\n', 'in this tutorial I am trying to illustrate how ensembling techniques work manually and by python code to make a good intuition about why is it  useful and why do we use it.\n']",1,amrmahmoud123,1-guide-to-ensembling-methods,amrmahmoud123_1-guide-to-ensembling-methods_m1
"['**Introduction to ensembling**\n', '\n', '**Types of ensembling :**\n', '\n', '**Basic Ensemble Techniques**\n', '\n', '*     Max Voting\n', '*     Averaging\n', '*     Weighted Average\n', '\n', '**Advanced Ensemble Techniques**\n', '\n', '* Stacking\n', '* Blending\n', '* Bagging\n', '* Boosting\n', '\n', '**Algorithms based on Bagging and Boosting**\n', '\n', '> * Bagging meta-estimator\n', '* Random Forest\n', '* AdaBoost\n', '* GBM\n', '* XGB\n', '* Light GBM\n', '* CatBoost\n']",2,amrmahmoud123,1-guide-to-ensembling-methods,amrmahmoud123_1-guide-to-ensembling-methods_m2
"['What is in this tutorial?\n', 'in thi tutorial I am trying to illustrate how ensembling techniques work manually and by python code to make a good intuition about why it is useful and why we use it.\n', '\n']",3,amrmahmoud123,1-guide-to-ensembling-methods,amrmahmoud123_1-guide-to-ensembling-methods_m3
['At first there is a rational we must stabilize that : combination between models increase accuracy and in machine learning combination is **Ensembling** \n'],4,amrmahmoud123,1-guide-to-ensembling-methods,amrmahmoud123_1-guide-to-ensembling-methods_m4
['**Introduction to ensembling :**'],5,amrmahmoud123,1-guide-to-ensembling-methods,amrmahmoud123_1-guide-to-ensembling-methods_m5
['**Errors**'],6,amrmahmoud123,1-guide-to-ensembling-methods,amrmahmoud123_1-guide-to-ensembling-methods_m6
['The error emerging from any model can be broken down into three components mathematically. Following are these component :'],7,amrmahmoud123,1-guide-to-ensembling-methods,amrmahmoud123_1-guide-to-ensembling-methods_m7
['![Imgur](https://i.imgur.com/LmeI08b.png)'],8,amrmahmoud123,1-guide-to-ensembling-methods,amrmahmoud123_1-guide-to-ensembling-methods_m8
"['**Why is this important in the current context?**\n', 'To understand what really goes behind an ensemble model, we need to first understand what causes error in the model. We will briefly introduce you to these errors and give an insight to each ensemble learner in this regards.']",9,amrmahmoud123,1-guide-to-ensembling-methods,amrmahmoud123_1-guide-to-ensembling-methods_m9
"['**Bias error **\n', '\n', 'is useful to quantify how much on an average are the predicted values different from the actual value. A high bias error means we have a under-performing model which keeps on missing important trends.\n', '\n', '**Variance**\n', '\n', 'on the other side quantifies how are the prediction made on same observation different from each other. A high variance model will over-fit on your training population and perform badly on any observation beyond training. Following diagram will give you more clarity (Assume that red spot is the real value and blue dots are predictions) :\n']",10,amrmahmoud123,1-guide-to-ensembling-methods,amrmahmoud123_1-guide-to-ensembling-methods_m10
['![Imgur](https://i.imgur.com/jFfarvo.png)'],11,amrmahmoud123,1-guide-to-ensembling-methods,amrmahmoud123_1-guide-to-ensembling-methods_m11
['model should maintain a balance between these two types of errors. This is known as the trade-off management of bias-variance errors. **Ensemble learning is one way to execute this trade off analysis.**'],12,amrmahmoud123,1-guide-to-ensembling-methods,amrmahmoud123_1-guide-to-ensembling-methods_m12
['![Imgur](https://i.imgur.com/ZDZsSr1.png)'],13,amrmahmoud123,1-guide-to-ensembling-methods,amrmahmoud123_1-guide-to-ensembling-methods_m13
['![Imgur](https://i.imgur.com/Xm5sKxD.png)'],14,amrmahmoud123,1-guide-to-ensembling-methods,amrmahmoud123_1-guide-to-ensembling-methods_m14
['![Imgur](https://i.imgur.com/GEG80ni.png)'],15,amrmahmoud123,1-guide-to-ensembling-methods,amrmahmoud123_1-guide-to-ensembling-methods_m15
"['**A group of predictors is called an ensemble**; thus, this technique is called Ensemble Learning, and an Ensemble Learning algorithm is called an** Ensemble method.**\n', '\n', ""Suppose you ask a complex question to thousands of random people, then aggregate their answers. In many cases you will find that this aggregated answer is better than an expert's answer. This is called **the wisdom of the crowd**\n"", '\n', 'Likewise, if you aggregate the predictions of a group of predictors (e.g. decision tree classifer, SVM, logistic regression), you will often get better predictions than with the best individual predictor.']",16,amrmahmoud123,1-guide-to-ensembling-methods,amrmahmoud123_1-guide-to-ensembling-methods_m16
['**Types of ensembling :****'],17,amrmahmoud123,1-guide-to-ensembling-methods,amrmahmoud123_1-guide-to-ensembling-methods_m17
"[' **Basic Ensemble Techniques**\n', '\n', '*     Max Voting\n', '*     Averaging\n', '*     Weighted Average\n', '\n', '**Advanced Ensemble Techniques**\n', '\n', '* Stacking\n', '* Blending\n', '* Bagging\n', '* Boosting\n', '\n', '**Algorithms based on Bagging and Boosting**\n', '\n', '> * Bagging meta-estimator\n', '* Random Forest\n', '* AdaBoost\n', '* GBM\n', '* XGB\n', '* Light GBM\n', '* CatBoost\n', '\n']",18,amrmahmoud123,1-guide-to-ensembling-methods,amrmahmoud123_1-guide-to-ensembling-methods_m18
['lets talk first about Max voting'],19,amrmahmoud123,1-guide-to-ensembling-methods,amrmahmoud123_1-guide-to-ensembling-methods_m19
"['**Max Voting\n', '**\n']",20,amrmahmoud123,1-guide-to-ensembling-methods,amrmahmoud123_1-guide-to-ensembling-methods_m20
"['The max voting method is generally used for classification problems. In this technique, multiple models are used to make predictions for each data point. The predictions by each model are considered as a ‘vote’. The predictions which we get from the majority of the models are used as the final prediction.\n', '\n', 'For example, when you asked 5 of your colleagues to rate your movie (out of 5); we’ll assume three of them rated it as 4 while two of them gave it a 5. Since the majority gave a rating of 4, the final rating will be taken as 4. You can consider this as taking the mode of all the predictions.\n', '\n', 'The result of max voting would be something like this:\n', '\n', 'Colleague 1-5\n', '\n', 'Colleague 2-4\n', '\n', 'Colleague 3-5\n', '\n', 'Colleague 4-4\n', '\n', 'Colleague 5-4\n', '\n', 'Finalrating-4']",21,amrmahmoud123,1-guide-to-ensembling-methods,amrmahmoud123_1-guide-to-ensembling-methods_m21
['**Code in python**'],22,amrmahmoud123,1-guide-to-ensembling-methods,amrmahmoud123_1-guide-to-ensembling-methods_m22
"['there are 2 methods :\n', '\n', '1-Mode\n', '\n', '2-Voting classifier']",23,amrmahmoud123,1-guide-to-ensembling-methods,amrmahmoud123_1-guide-to-ensembling-methods_m23
"['**Majority Voting / Hard Voting\n', '**\n']",24,amrmahmoud123,1-guide-to-ensembling-methods,amrmahmoud123_1-guide-to-ensembling-methods_m24
"['**Hard voting **\n', '\n', 'is the simplest case of majority voting. Here, we predict the class label y^ via majority (plurality) voting of each classifier\n', '\n', 'y^=mode{C1(x),C2(x),...,Cm(x)}\n', '\n', 'Assuming that we combine three classifiers that classify a training sample as follows:\n', '\n', 'classifier 1 -> class 0 classifier 2 -> class 0 classifier 3 -> class 1\n', '\n', 'y^=mode{0,0,1}=0\n', '\n', 'Via majority vote, we would we would classify the sample as ""class 0.""']",25,amrmahmoud123,1-guide-to-ensembling-methods,amrmahmoud123_1-guide-to-ensembling-methods_m25
"['**Soft Voting\n', '**\n']",26,amrmahmoud123,1-guide-to-ensembling-methods,amrmahmoud123_1-guide-to-ensembling-methods_m26
"['If all classifiers are able to estimate class probabilities (i.e. they have a predict_proba() method), then you can tell Scikit-Learn to predict the class with the highest class probability, averaged over all the individual classifiers.\n', '\n', 'This is called **soft voting** and it often achieves higher performance than hard voting because *it gives more weight to highly confident votes*.\n', '\n', ""To perform soft voting, all you need to do is **replace** voting='hard' with voting='soft' **and ensure that all classifiers can estimate class probabilities.\n"", '\n', ""**The SVC class can't estimate class probabilities by default**, so you'll need to set its probability hyperparameter to **True**, as this will make the SVC class use cross-validation to estimate class probabilities (which slows training down), and it will add a predict_proba() method.\n"", '\n', '**In soft voting**, we predict the class labels based on the predicted probabilities p for classifier -- this approach is only recommended if the classifiers are **well-calibrated**.\n', '\n', '*y^=argmaxi∑j=1mwjpij,* where **wj** is the weight that can be assigned to the **jth** classifier.\n', '\n', 'Assuming the example in the previous section was a *binary classification* task with class labels i∈{0,1}, our ensemble could make the following prediction:\n', '\n', 'C1(x)→[0.9,0.1]\n', '\n', 'C2(x)→[0.8,0.2]\n', '\n', 'C3(x)→[0.4,0.6]\n', '\n', 'Using uniform weights, we compute the average probabilities:\n', '\n', 'p(i0∣x)=0.9+0.8+0.43=0.7p(i1∣x)=0.1+0.2+0.63=0.3\n', '\n', 'y^=argmaxi[p(i0∣x),p(i1∣x)]=0\n', '\n', 'However, assigning the weights {0.1, 0.1, 0.8} would yield a prediction y^=1:\n', '\n', 'p(i0∣x)=0.1×0.9+0.1×0.8+0.8×0.4=0.49p(i1∣x)=0.1×0.1+0.2×0.1+0.8×0.6=0.51\n', '\n', 'y^=argmaxi[p(i0∣x),p(i1∣x)]=1']",27,amrmahmoud123,1-guide-to-ensembling-methods,amrmahmoud123_1-guide-to-ensembling-methods_m27
"['**Averaging**\n', '\n', 'Similar to the max voting technique, multiple predictions are made for each data point in averaging. In this method, we take an **average** of predictions from all the models and use it to make the final prediction.Averaging can be used for making predictions in regression problems or while calculating probabilities for classification problems.For example, in the below case, the averaging method would take the average of all the values.\n', '\n', '*i.e. (5+4+5+4+4)/5 = 4.4*']",28,amrmahmoud123,1-guide-to-ensembling-methods,amrmahmoud123_1-guide-to-ensembling-methods_m28
['**Code in python**\n'],29,amrmahmoud123,1-guide-to-ensembling-methods,amrmahmoud123_1-guide-to-ensembling-methods_m29
"['**Weighted Average\n', '**']",30,amrmahmoud123,1-guide-to-ensembling-methods,amrmahmoud123_1-guide-to-ensembling-methods_m30
"['**This is an extension of the averaging method.** All models are assigned different weights defining the importance of each model for prediction. For instance, if two of your colleagues are critics, while others have no prior experience in this field, then the answers by these two friends are given more importance as compared to the other people.\n', '\n', 'The result is calculated as\n', '\n', '*[(50.23) + (40.23) + (50.18) + (40.18) + (4*0.18)] = 4.41.*']",31,amrmahmoud123,1-guide-to-ensembling-methods,amrmahmoud123_1-guide-to-ensembling-methods_m31
['![Imgur](https://i.imgur.com/J9drqs1.png)'],32,amrmahmoud123,1-guide-to-ensembling-methods,amrmahmoud123_1-guide-to-ensembling-methods_m32
['**Code in python**'],33,amrmahmoud123,1-guide-to-ensembling-methods,amrmahmoud123_1-guide-to-ensembling-methods_m33
['**Advanced Ensemble techniques**\n'],34,amrmahmoud123,1-guide-to-ensembling-methods,amrmahmoud123_1-guide-to-ensembling-methods_m34
['**Bagging**'],35,amrmahmoud123,1-guide-to-ensembling-methods,amrmahmoud123_1-guide-to-ensembling-methods_m35
"['**Bagging** is very common in competitions. I don’t think I have ever seen anybody win without using it. But, in order for this to work, your data must have *variance*, otherwise you’re just adding levels after levels of additional iterations with **little benefit** to your score and a big headache for those maintaining your modeling pipeline in production. Even when it does improve things, you have to asked yourself if its worth all that extra work…']",36,amrmahmoud123,1-guide-to-ensembling-methods,amrmahmoud123_1-guide-to-ensembling-methods_m36
"['In simple terms, **bagging irons out variance from a data set** . If, after splitting your data into multiple chunks and training them, you find that your predictions are *different*, then your data has *variance*. Bagging can turn a bad thing into a competitive advantage. For more theory behind the magic, check out *Bootstrap Aggregating on Wikipedia.* Bagging was invented by *Leo Breiman* at the University of California. He is also one of the grandfathers of Boosting and Random Forests.']",37,amrmahmoud123,1-guide-to-ensembling-methods,amrmahmoud123_1-guide-to-ensembling-methods_m37
['**Stability and Accuracy**\n'],38,amrmahmoud123,1-guide-to-ensembling-methods,amrmahmoud123_1-guide-to-ensembling-methods_m38
"['By saving each prediction set and averaging them together, you not only lower variance without affecting bias, but your accuracy may be **improved**! In essence, you are creating many slightly different models and ensembling them together; **this avoids over-fitting**, **stabilizes your predictions and increases your accuracy**. Mind you, this assumes your data has variance, if it doesn’t,**bagging won’t help.**']",39,amrmahmoud123,1-guide-to-ensembling-methods,amrmahmoud123_1-guide-to-ensembling-methods_m39
"['Bagging is based on the *statistical method of bootstrapping*, Bagging actually refers to (Bootstrap Aggregators). Most any paper or post that references using bagging algorithms will also reference Leo Breiman who wrote a paper in 1996 called “*Bagging Predictors*”.']",40,amrmahmoud123,1-guide-to-ensembling-methods,amrmahmoud123_1-guide-to-ensembling-methods_m40
"['1-we make subsets with replacement: that means every item may appears in different subsets.\n', '\n', '2-apply model for every subset of the sample.\n', '\n', '3-The models run in parallel and are independent of each other.\n', '\n', '4-predict x-text by using each model\n', '\n', '5-then aggregate their predictions (either by voting or by averaging) to form a final prediction.']",41,amrmahmoud123,1-guide-to-ensembling-methods,amrmahmoud123_1-guide-to-ensembling-methods_m41
['![Imgur](https://i.imgur.com/eu95V9N.png)'],42,amrmahmoud123,1-guide-to-ensembling-methods,amrmahmoud123_1-guide-to-ensembling-methods_m42
['**Bagging algorithms:**\n'],43,amrmahmoud123,1-guide-to-ensembling-methods,amrmahmoud123_1-guide-to-ensembling-methods_m43
"['\n', '* Bagging meta-estimator\n', '* Random forest']",44,amrmahmoud123,1-guide-to-ensembling-methods,amrmahmoud123_1-guide-to-ensembling-methods_m44
['**Bagging meta-estimator**'],45,amrmahmoud123,1-guide-to-ensembling-methods,amrmahmoud123_1-guide-to-ensembling-methods_m45
"['**Bagging meta-estimator** is an ensembling algorithm that can be used for **both** classification (BaggingClassifier) and regression (BaggingRegressor) problems. It follows the typical bagging technique to make predictions. Following are the steps for the bagging meta-estimator algorithm:\n', '\n', '1-Random subsets are created from the original dataset (Bootstrapping).\n', '\n', '2-The subset of the dataset includes all features.\n', '\n', '3-A user-specified base estimator is fitted on each of these smaller sets.\n', '\n', '4-Predictions from each model are combined to get the final result.']",46,amrmahmoud123,1-guide-to-ensembling-methods,amrmahmoud123_1-guide-to-ensembling-methods_m46
['**Code in python**'],47,amrmahmoud123,1-guide-to-ensembling-methods,amrmahmoud123_1-guide-to-ensembling-methods_m47
"['where :\n', '\n', '**base_estimator:**\n', 'It defines the base estimator to fit on random subsets of the dataset. When nothing is specified, the base estimator is a decision tree.\n', '\n', '**n_estimators:**\n', 'It is the number of base estimators to be created. The number of estimators should be carefully tuned as a large number would take a very long time to run, while a very small number might not provide the best results.\n', '\n', '**max_samples:**\n', 'This parameter controls the size of the subsets. It is the maximum number of samples to train each base estimator.\n', '\n', '**max_features:**\n', 'Controls the number of features to draw from the whole dataset. It defines the maximum number of features required to train each base estimator.\n', '\n', '**n_jobs:**\n', 'The number of jobs to run in parallel. Set this value equal to the cores in your system. If -1, the number of jobs is set to the number of cores.\n', '\n', '**random_state:**\n', 'It specifies the method of random split. When random state value is same for two models, the random selection is same for both models. This parameter is useful when you want to compare different models.']",48,amrmahmoud123,1-guide-to-ensembling-methods,amrmahmoud123_1-guide-to-ensembling-methods_m48
['**Random Forest**\n'],49,amrmahmoud123,1-guide-to-ensembling-methods,amrmahmoud123_1-guide-to-ensembling-methods_m49
"['**Random Forest** is another ensemble machine learning algorithm that follows the bagging technique. It is an extension of the bagging estimator algorithm. The base estimators in random forest are decision trees. Unlike bagging meta estimator, random forest **randomly** selects a set of features which are used to decide the best split at each node of the decision tree.']",50,amrmahmoud123,1-guide-to-ensembling-methods,amrmahmoud123_1-guide-to-ensembling-methods_m50
"['step-by-step, this is what a random forest model does:\n', '\n', '1-Random subsets are created from the original dataset (bootstrapping).\n', '\n', '2-At each node in the decision tree, only a random set of features are considered to decide the best split.\n', '\n', '3-A decision tree model is fitted on each of the subsets. The final prediction is calculated by averaging the predictions from all decision trees.\n', '\n', '**Note:** The decision trees in random forest can be built on a subset of data and features. Particularly, the sklearn model of random forest uses all features for decision tree and a subset of features are randomly selected for splitting at each node.\n', '\n', '**To sum up, Random forest randomly selects data points and features, and builds multiple trees (Forest) .**']",51,amrmahmoud123,1-guide-to-ensembling-methods,amrmahmoud123_1-guide-to-ensembling-methods_m51
['**Code in python**'],52,amrmahmoud123,1-guide-to-ensembling-methods,amrmahmoud123_1-guide-to-ensembling-methods_m52
"['You can see feature importance by using **model.featureimportances** in random forest.\n', '\n']",53,amrmahmoud123,1-guide-to-ensembling-methods,amrmahmoud123_1-guide-to-ensembling-methods_m53
['**Sample code for regression problem:**\n'],54,amrmahmoud123,1-guide-to-ensembling-methods,amrmahmoud123_1-guide-to-ensembling-methods_m54
"['Parameters :\n', '\n', '**n_estimators:**\n', 'It defines the number of decision trees to be created in a random forest. Generally, a higher number makes the predictions stronger and more stable, but a very large number can result in higher training time.\n', '\n', '**criterion:**\n', 'It defines the function that is to be used for splitting. The function measures the quality of a split for each feature and chooses the best split.\n', '\n', '**max_features :**\n', 'It defines the maximum number of features allowed for the split in each decision tree. Increasing max features usually improve performance but a very high number can decrease the diversity of each tree.\n', '\n', '**max_depth:**\n', 'Random forest has multiple decision trees. This parameter defines the maximum depth of the trees. min_samples_split: Used to define the minimum number of samples required in a leaf node before a split is attempted. If the number of samples is less than the required number, the node is not split.\n', '\n', '**min_samples_leaf:**\n', 'This defines the minimum number of samples required to be at a leaf node. Smaller leaf size makes the model more prone to capturing noise in train data.\n', '\n', '**max_leaf_nodes:**\n', 'This parameter specifies the maximum number of leaf nodes for each tree. The tree stops splitting when the number of leaf nodes becomes equal to the max leaf node.\n', '\n', '**n_jobs:**\n', 'This indicates the number of jobs to run in parallel. S']",55,amrmahmoud123,1-guide-to-ensembling-methods,amrmahmoud123_1-guide-to-ensembling-methods_m55
['**Boosting**\n'],56,amrmahmoud123,1-guide-to-ensembling-methods,amrmahmoud123_1-guide-to-ensembling-methods_m56
"['The term ‘Boosting’ refers to a family of algorithms which **converts weak learner to strong learners**. Boosting is an ensemble method for improving the model predictions of any given learning algorithm. The idea of boosting **is to train weak learners sequentially, each trying to correct its predecessor**.\n', '\n', 'Boosting is all about “*teamwork*”. Each model that runs, dictates what features the next model will focus on.']",57,amrmahmoud123,1-guide-to-ensembling-methods,amrmahmoud123_1-guide-to-ensembling-methods_m57
['**AdaBoost**\n'],58,amrmahmoud123,1-guide-to-ensembling-methods,amrmahmoud123_1-guide-to-ensembling-methods_m58
"['**Adaptive boosting or AdaBoost** is one of the simplest boosting algorithms. Usually, decision trees are used for modelling. Multiple sequential models are created, each correcting the errors from the last model. AdaBoost assigns weights to the observations which are incorrectly predicted and the subsequent model works to predict these values correctly.\n', '\n', '**steps:**\n', '\n', '1-all observations in the dataset are given equal weights.\n', '\n', '2-A model is built on a subset of data.\n', '\n', '3-Using this model, predictions are made on the whole dataset.\n', '\n', '4-Errors are calculated by comparing the predictions and actual values.\n', '\n', '5-While creating the next model, higher weights are given to the data points which were predicted incorrectly.\n', '\n', '6-Weights can be determined using the error value. For instance, higher the error more is the weight assigned to the observation.\n', '\n', '7-This process is repeated until the error function does not change, or the maximum limit of the number of estimators is reached.']",59,amrmahmoud123,1-guide-to-ensembling-methods,amrmahmoud123_1-guide-to-ensembling-methods_m59
['**Code in python**'],60,amrmahmoud123,1-guide-to-ensembling-methods,amrmahmoud123_1-guide-to-ensembling-methods_m60
"['**Parameters**\n', '\n', '**base_estimators:**\n', 'It helps to specify the type of base estimator, that is, the machine learning algorithm to be used as base learner.\n', '\n', '**n_estimators:**\n', 'It defines the number of base estimators.\n', 'The default value is 10, but you should keep a higher value to get better performance.\n', '\n', '**learning_rate:**\n', 'This parameter controls the contribution of the estimators in the final combination.\n', 'There is a trade-off between learning_rate and n_estimators.\n', '\n', '**max_depth:**\n', 'Defines the maximum depth of the individual estimator.\n', 'Tune this parameter for best performance.\n', '\n', '**n_jobs:**\n', 'Specifies the number of processors it is allowed to use.\n', 'Set value to -1 for maximum processors allowed.\n', '\n', '**random_state :**\n', 'An integer value to specify the random data split.\n', 'A definite value of random_state will always produce same results if given with same parameters and training data.']",61,amrmahmoud123,1-guide-to-ensembling-methods,amrmahmoud123_1-guide-to-ensembling-methods_m61
"['**stacking\n', '**']",62,amrmahmoud123,1-guide-to-ensembling-methods,amrmahmoud123_1-guide-to-ensembling-methods_m62
"['\n', 'Stacking is a similar to boosting:\n', '\n', ""you also apply several models to your original data. The difference here is, however, that you don't have just an empirical formula for your weight function, rather you introduce a meta-level and use another model/approach to estimate the input together with outputs of every model to estimate the weights or, in other words, to determine what models perform well and what badly given these input data. and finally I get its true illustration.""]",63,amrmahmoud123,1-guide-to-ensembling-methods,amrmahmoud123_1-guide-to-ensembling-methods_m63
"['consider we have a dataset we splite our data set into 3 parts : training, validation , test']",64,amrmahmoud123,1-guide-to-ensembling-methods,amrmahmoud123_1-guide-to-ensembling-methods_m64
['![Imgur](https://i.imgur.com/EQYa8C8.png)'],65,amrmahmoud123,1-guide-to-ensembling-methods,amrmahmoud123_1-guide-to-ensembling-methods_m65
['then make this step'],66,amrmahmoud123,1-guide-to-ensembling-methods,amrmahmoud123_1-guide-to-ensembling-methods_m66
['![Imgur](https://i.imgur.com/fVazCYe.png)'],67,amrmahmoud123,1-guide-to-ensembling-methods,amrmahmoud123_1-guide-to-ensembling-methods_m67
['![Imgur](https://i.imgur.com/2zjxVBC.png)'],68,amrmahmoud123,1-guide-to-ensembling-methods,amrmahmoud123_1-guide-to-ensembling-methods_m68
['![Imgur](https://i.imgur.com/lpDhGd1.png)'],69,amrmahmoud123,1-guide-to-ensembling-methods,amrmahmoud123_1-guide-to-ensembling-methods_m69
"['train algorythm 0 on **A** and make predictions for **B** and **C** and save to **B1,C1**\n', '\n', 'train algorythm 1 on **A** and make predictions for **B** and **C** and save to **B1,C1**']",70,amrmahmoud123,1-guide-to-ensembling-methods,amrmahmoud123_1-guide-to-ensembling-methods_m70
['![Imgur](https://i.imgur.com/10slay8.png)'],71,amrmahmoud123,1-guide-to-ensembling-methods,amrmahmoud123_1-guide-to-ensembling-methods_m71
['**At this moment we stacked predictions to each others thats where stacking name comes from** and then'],72,amrmahmoud123,1-guide-to-ensembling-methods,amrmahmoud123_1-guide-to-ensembling-methods_m72
"['train algorythm 2 on **A** and make predictions for **B** and **C** and save to **B1,C1**']",73,amrmahmoud123,1-guide-to-ensembling-methods,amrmahmoud123_1-guide-to-ensembling-methods_m73
['![Imgur](https://i.imgur.com/hfp6JGP.png)'],74,amrmahmoud123,1-guide-to-ensembling-methods,amrmahmoud123_1-guide-to-ensembling-methods_m74
"['then we take the data from the validation set which we already knew and we are going to feed a new model .\n', '\n', 'train algorythm 3 on **B1** and make predictions for **C1**']",75,amrmahmoud123,1-guide-to-ensembling-methods,amrmahmoud123_1-guide-to-ensembling-methods_m75
['![Imgur](https://i.imgur.com/md3L8yB.png)'],76,amrmahmoud123,1-guide-to-ensembling-methods,amrmahmoud123_1-guide-to-ensembling-methods_m76
['**Code in python**'],77,amrmahmoud123,1-guide-to-ensembling-methods,amrmahmoud123_1-guide-to-ensembling-methods_m77
['![Imgur](https://i.imgur.com/bfb2qlr.png)'],78,amrmahmoud123,1-guide-to-ensembling-methods,amrmahmoud123_1-guide-to-ensembling-methods_m78
['![Imgur](https://i.imgur.com/Bo4KItc.png)'],79,amrmahmoud123,1-guide-to-ensembling-methods,amrmahmoud123_1-guide-to-ensembling-methods_m79
['![Imgur](https://i.imgur.com/hGkZd9T.png)'],80,amrmahmoud123,1-guide-to-ensembling-methods,amrmahmoud123_1-guide-to-ensembling-methods_m80
['I hope that I give you a piece of introduction of ensembling methods and this is not the end of my tutorial but this is only the first episode and I will continue soon illustrating the remaining methods of ensemlbing techniques.\n'],81,amrmahmoud123,1-guide-to-ensembling-methods,amrmahmoud123_1-guide-to-ensembling-methods_m81
"['resources :\n', '\n', '\n', '[Google](https://www.google.com/webhp?hl=en&sa=X&ved=0ahUKEwiU0c_cgOLhAhUjQxUIHfetDCwQPAgH)\n', '    \n', '\n', '[Analytics videa](https://www.analyticsvidhya.com/)\n', '    \n', '\n', '[youtube](https://www.youtube.com/)\n', '    \n', '\n', '[wikipedia](https://www.wikipedia.org/)\n', '    \n', '  \n', '  \n', '  and a lot of other resources .\n', '    thanks a lot.']",82,amrmahmoud123,1-guide-to-ensembling-methods,amrmahmoud123_1-guide-to-ensembling-methods_m82
"[""The data looks like it has had Principal Component Analysis applied to it, since each feature looks near Gaussian. Let's generate some data to see what PCA looks like on generated continuous and categorical data, and compare some plots to the Santander data.\n"", '\n', ""**Update**: CPMP suggested Truncated SVD, so we'll explore that too (https://www.kaggle.com/c/santander-customer-transaction-prediction/discussion/87301#503701)""]",0,anjum48,has-pca-or-svd-been-performed-on-the-dataset,anjum48_has-pca-or-svd-been-performed-on-the-dataset_m0
"['# Create random data\n', ""Let's create 1000 random variables which are correlated""]",1,anjum48,has-pca-or-svd-been-performed-on-the-dataset,anjum48_has-pca-or-svd-been-performed-on-the-dataset_m1
['# PCA on continuous variables'],2,anjum48,has-pca-or-svd-been-performed-on-the-dataset,anjum48_has-pca-or-svd-been-performed-on-the-dataset_m2
"[""Let's have a look at the distributions and cross plots""]",3,anjum48,has-pca-or-svd-been-performed-on-the-dataset,anjum48_has-pca-or-svd-been-performed-on-the-dataset_m3
['# Truncated SVD on continuous variables'],4,anjum48,has-pca-or-svd-been-performed-on-the-dataset,anjum48_has-pca-or-svd-been-performed-on-the-dataset_m4
"['# PCA on categorical variables\n', ""Let's convert our continuous dataset to a discrete/categorical dataset by rounding to the nearest 100. Then see what the transformed data looks like""]",5,anjum48,has-pca-or-svd-been-performed-on-the-dataset,anjum48_has-pca-or-svd-been-performed-on-the-dataset_m5
['So now there should only be a handful of categories per feature'],6,anjum48,has-pca-or-svd-been-performed-on-the-dataset,anjum48_has-pca-or-svd-been-performed-on-the-dataset_m6
"['Even with a handful of categories per feature, after performing PCA, the cross plots still look like circular/spherical data clouds\n', '\n', '# Truncated SVD on categorical data']",7,anjum48,has-pca-or-svd-been-performed-on-the-dataset,anjum48_has-pca-or-svd-been-performed-on-the-dataset_m7
['# Comparing to the Santander dataset'],8,anjum48,has-pca-or-svd-been-performed-on-the-dataset,anjum48_has-pca-or-svd-been-performed-on-the-dataset_m8
"['# Discussion\n', ""* By visually comparing the pair plots above with our data for both continuous and categorical features, they look pretty similar. Although we can't prove that PCA or SVD has been applied, we can't rule it out either\n"", '* Applying PCA or SVD on categorical data makes it hard to see that the data was originally categorical after the transform. For example, there are no bands/stripes in the cross plots to suggest discretisation. It looks pretty difficult to recover any categories from the transformed data\n', ""* PCA features typically have a mean close to zero. The Santander features do not. This might be a clue hinting that PCA wasn't applied to the data, or deliberate obfuscation by the organisers. SVD on the otherhand creates features with non-zero mean""]",9,anjum48,has-pca-or-svd-been-performed-on-the-dataset,anjum48_has-pca-or-svd-been-performed-on-the-dataset_m9
"['# General information\n', '\n', 'In Santander Customer Transaction Prediction competition we have a binary classification task. Train and test data have 200k samples each and we have 200 anonimyzed numerical columns. It would be interesting to try good models without overfitting and knowing the meaning of the features.\n', ""In fact this competition seems to be similar to another current competition: don't overfit II, so I'll use a lot of ideas from my [kernel](https://www.kaggle.com/artgor/how-to-not-overfit).\n"", '\n', ""In this kernel I'll write the following things:\n"", '\n', '* EDA on the features and trying to get some insights;\n', '* Using permutation importance to select most impactful features;\n', '* Comparing various models: linear models, tree based models and others;\n', '* Trying various approaches to feature selection including taking top features from eli5;\n', '* Hyperparameter optimization for models;\n', '* Feature generation;\n', '* Other things;\n', '\n', '![](https://i.imgur.com/e5vPHpJ.png)\n', '\n', '*Work still in progress*']",0,artgor,santander-eda-fe-fs-and-models,artgor_santander-eda-fe-fs-and-models_m0
['## Data exploration'],1,artgor,santander-eda-fe-fs-and-models,artgor_santander-eda-fe-fs-and-models_m1
"['From this overview we can see the following things:\n', '* target is binary and has disbalance: 10% of samples belong to 1 class;\n', '* values in columns are more or less similar;\n', '* columns have high std (up to 20)\n', '* columns have a high range of means;']",2,artgor,santander-eda-fe-fs-and-models,artgor_santander-eda-fe-fs-and-models_m2
"[""Let's have a look at correlations now!""]",3,artgor,santander-eda-fe-fs-and-models,artgor_santander-eda-fe-fs-and-models_m3
"['We can see that all features have a low correlation with target. So we have no highly correlated features which we could drop, on the other hand we could drop some columns with have little correlation with the target.']",4,artgor,santander-eda-fe-fs-and-models,artgor_santander-eda-fe-fs-and-models_m4
['## Basic modelling'],5,artgor,santander-eda-fe-fs-and-models,artgor_santander-eda-fe-fs-and-models_m5
['## ELI5'],6,artgor,santander-eda-fe-fs-and-models,artgor_santander-eda-fe-fs-and-models_m6
"[""ELI5 didn't help up to eliminate features, but let's at least try to take top-100 and see how it helps.""]",7,artgor,santander-eda-fe-fs-and-models,artgor_santander-eda-fe-fs-and-models_m7
['## Feature generation'],8,artgor,santander-eda-fe-fs-and-models,artgor_santander-eda-fe-fs-and-models_m8
"['### Feature interaction\n', '\n', ""Didn't improve score""]",9,artgor,santander-eda-fe-fs-and-models,artgor_santander-eda-fe-fs-and-models_m9
"['### Scaling\n', '\n', '! **Notice** scaling severely decreases score']",10,artgor,santander-eda-fe-fs-and-models,artgor_santander-eda-fe-fs-and-models_m10
['### Statistics'],11,artgor,santander-eda-fe-fs-and-models,artgor_santander-eda-fe-fs-and-models_m11
['Training with these features gives the same score on LB: 0.899'],12,artgor,santander-eda-fe-fs-and-models,artgor_santander-eda-fe-fs-and-models_m12
"['### NN features\n', '\n', 'Takes several hours.']",13,artgor,santander-eda-fe-fs-and-models,artgor_santander-eda-fe-fs-and-models_m13
['## Blend'],14,artgor,santander-eda-fe-fs-and-models,artgor_santander-eda-fe-fs-and-models_m14
['### Rounding data'],15,artgor,santander-eda-fe-fs-and-models,artgor_santander-eda-fe-fs-and-models_m15
"['----\n', '\n', '![](data:image/png;base64,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)\n', '\n', '-----\n', '\n', '# ***OUTLINE OF THE NOTEBOOK***\n', '\n', '------\n', '\n', '* **Step:** [**1.Load Libraries**](#1.Load-Libraries)\n', '* **Step:** [**2.Read Dataset**](#2.Read-Dataset)\n', '* **Step:** [**3. Display Dataset**](#3.-Display-Dataset*)\n', '* **Step:** [**4.Remove unwanted columns**](#4.Remove-unwanted-columns)\n', '* **Step:** [**5.Create Instance of Feature Selector**](#5.Create-Instance-of-Feature-Selector)\n', '* **Step:** [**6.Missing Value**](#6.Missing-Value)\n', '* **Step:** [**7.Single Unique Value**](#7.Single-Unique-Value)\n', '* **Step:** [**8.Plot Feature Importances**](#8.Plot-Feature-Importances)\n', '* **Step:** [**9.Low Importance Features**](#9.Low-Importance-Features)\n', '* **Step:** [**10.Removing Features**](#10.Removing-Features)\n', '* **Step:** [**11.Handling One-Hot Features**](#11.Handling-One-Hot-Features)\n', '* **Step:** [**12.Model Training**](#12.Model-Training)\n', '\n', '------\n', '\n', '## **1.Load Libraries**']",0,ashishpatel26,feature-selection-pca-catboost,ashishpatel26_feature-selection-pca-catboost_m0
"['## **2.Read Dataset**\n', '\n', '* We can see that our dataset contain **200000 Rows** and **202 Columns** with **target columns**\n', '* Here I tried to take **150000 Rows** for testing purpose.']",1,ashishpatel26,feature-selection-pca-catboost,ashishpatel26_feature-selection-pca-catboost_m1
['## **3. Display Dataset**'],2,ashishpatel26,feature-selection-pca-catboost,ashishpatel26_feature-selection-pca-catboost_m2
['## **4.Remove unwanted columns**'],3,ashishpatel26,feature-selection-pca-catboost,ashishpatel26_feature-selection-pca-catboost_m3
['## **5.Create Instance of Feature Selector**'],4,ashishpatel26,feature-selection-pca-catboost,ashishpatel26_feature-selection-pca-catboost_m4
"['## **6.Missing Value**\n', '\n', 'The first feature selection method is straightforward: find any columns with a missing fraction greater than a specified threshold. For this example we will use a threhold of 0.6 which corresponds to finding features with more than 60% missing values. (This method does not one-hot encode the features first).']",5,ashishpatel26,feature-selection-pca-catboost,ashishpatel26_feature-selection-pca-catboost_m5
['The features identified for removal can be accessed through the ops dictionary of the FeatureSelector object.'],6,ashishpatel26,feature-selection-pca-catboost,ashishpatel26_feature-selection-pca-catboost_m6
['## **7.Single Unique Value**'],7,ashishpatel26,feature-selection-pca-catboost,ashishpatel26_feature-selection-pca-catboost_m7
['Running the gradient boosting model requires one hot encoding the features. These features are saved in the one_hot_features attribute of the FeatureSelector. The original features are saved in the base_features.'],8,ashishpatel26,feature-selection-pca-catboost,ashishpatel26_feature-selection-pca-catboost_m8
"['## **8.Plot Feature Importances**\n', '\n', 'The feature importance plot using plot_feature_importances will show us the plot_n most important features (on a normalized scale where the features sum to 1). It also shows us the cumulative feature importance versus the number of features.\n', '\n', 'When we plot the feature importances, we can pass in a threshold which identifies the number of features required to reach a specified cumulative feature importance. For example, threshold = 0.99 will tell us the number of features needed to account for 99% of the total importance.\n', '\n']",9,ashishpatel26,feature-selection-pca-catboost,ashishpatel26_feature-selection-pca-catboost_m9
"[""We could use these results to select only the 'n' most important features. For example, if we want the top 100 most importance, we could do the following.""]",10,ashishpatel26,feature-selection-pca-catboost,ashishpatel26_feature-selection-pca-catboost_m10
"['## **9.Low Importance Features**\n', '\n', 'This method builds off the feature importances from the gradient boosting machine (identify_zero_importance must be run first) by finding the lowest importance features not needed to reach a specified cumulative total feature importance. For example, if we pass in 0.99, this will find the lowest important features that are not needed to reach 99% of the total feature importance.\n', '\n', 'When using this method, we must have already run identify_zero_importance and need to pass in a cumulative_importance that accounts for that fraction of total feature importance.\n', '\n', '**Note of caution:** this method builds on the gradient boosting model features importances and again is non-deterministic. I advise running these two methods several times with varying parameters and testing each resulting set of features rather than picking one number and sticking to it.']",11,ashishpatel26,feature-selection-pca-catboost,ashishpatel26_feature-selection-pca-catboost_m11
['The low importance features to remove are those that do not contribute to the specified cumulative importance. These are also available in the ops dictionary.'],12,ashishpatel26,feature-selection-pca-catboost,ashishpatel26_feature-selection-pca-catboost_m12
"['## **10.Removing Features**\n', '\n', 'Once we have identified the features to remove, we have a number of ways to drop the features. We can access any of the feature lists in the removal_ops dictionary and remove the columns manually. We also can use the remove method, passing in the methods that identified the features we want to remove.\n', '\n', 'This method returns the resulting data which we can then use for machine learning. The original data will still be accessible in the data attribute of the Feature Selector.\n', '\n', ""**Be careful of the methods** used for removing features! It's a good idea to inspect the features that will be removed before using the remove function.""]",13,ashishpatel26,feature-selection-pca-catboost,ashishpatel26_feature-selection-pca-catboost_m13
"[""To remove the features from all of the methods, pass in method='all'. Before we do this, we can check how many features will be removed using check_removal. This returns a list of all the features that have been idenfitied for removal.""]",14,ashishpatel26,feature-selection-pca-catboost,ashishpatel26_feature-selection-pca-catboost_m14
['Now we can remove all of the features idenfitied.'],15,ashishpatel26,feature-selection-pca-catboost,ashishpatel26_feature-selection-pca-catboost_m15
"['## **11.Handling One-Hot Features**\n', '\n', 'If we look at the dataframe that is returned, we may notice several new columns that were not in the original data. These are created when the data is one-hot encoded for machine learning. To remove all the one-hot features, we can pass in keep_one_hot = False to the remove method.']",16,ashishpatel26,feature-selection-pca-catboost,ashishpatel26_feature-selection-pca-catboost_m16
"['## **12.Model Training**\n', '\n', '* Model is Binary Classification so used Catboost because **we have dataset size is more than 100000 Samples..**']",17,ashishpatel26,feature-selection-pca-catboost,ashishpatel26_feature-selection-pca-catboost_m17
"['Hi everyone! \n', '\n', ""Here is another take on trying to get the most out of the data when the features are assumed to be independent. The idea is to fit 200 classifiers, each using only one of the features, and then combining the predictions using Bayes' rule as in Naive Bayes methods. I've previously experimented by using kernel density estimates to obtain the individual classifiers as in Chris Deotte's kernel [here](http://www.kaggle.com/cdeotte/modified-naive-bayes-santander-0-899), but achieved better results when the individual classifiers where gradient boosted trees. Enjoy!""]",0,b5strbal,lightgbm-naive-bayes-santander-0-900,b5strbal_lightgbm-naive-bayes-santander-0-900_m0
"[""One could of course try to tune the LGBM parameters for each feature individually, but that's just too tedious. So I prayed that the same parameters work for every feature as long as I allow the number of number of trees to vary for each feature. The following function estimates the optimal number of rounds for each feature.""]",1,b5strbal,lightgbm-naive-bayes-santander-0-900,b5strbal_lightgbm-naive-bayes-santander-0-900_m1
"['It is important that the metric used for the early stopping is the binary logloss (or something similar) and not the AUC score! For many features, the probability function to predict seem to be monotonic. That is, the larger the value of that feature the more likely that the class value is 1 (or 0). In these cases, **any** monotonic predictor will have the same (and best possible) AUC score, since all induce the same ranking of the observations. But a random monotonic function is not good for anything, since it is crucial for Naive Bayes is to estimate the individual conditional probabilities as accurately as possible.']",2,b5strbal,lightgbm-naive-bayes-santander-0-900,b5strbal_lightgbm-naive-bayes-santander-0-900_m2
"['As we can see, the optimal number of rounds varies greatly. \n', '\n', ""There are features that require quite a long training. For example, it seems that the distibution of var_108 has a sharp spike somewhere at the middle and that takes a long time to take into account. (See the end of this notebook for a figure.) The kernel density estimate in Chris's [kernel](http://www.kaggle.com/cdeotte/modified-naive-bayes-santander-0-899) does seem to see this spike very well, so this might illustrate why this version of Naive Bayes performs slightly better.\n"", '\n', ""Interestingly, there are also features where the optimal number of trees is less then 10, indicating that those features might be very weak predictors (or perhaps that the chosen LGBM parameters just don't work well for those features).""]",3,b5strbal,lightgbm-naive-bayes-santander-0-900,b5strbal_lightgbm-naive-bayes-santander-0-900_m3
"['Next, we implement our LGB Naive Bayes classifier.']",4,b5strbal,lightgbm-naive-bayes-santander-0-900,b5strbal_lightgbm-naive-bayes-santander-0-900_m4
['Performing 5-fold CV.'],5,b5strbal,lightgbm-naive-bayes-santander-0-900,b5strbal_lightgbm-naive-bayes-santander-0-900_m5
['Creating submission:'],6,b5strbal,lightgbm-naive-bayes-santander-0-900,b5strbal_lightgbm-naive-bayes-santander-0-900_m6
"['Finally, here is the prediction plot for var_108. The spike in the middle is nicely recognized.']",7,b5strbal,lightgbm-naive-bayes-santander-0-900,b5strbal_lightgbm-naive-bayes-santander-0-900_m7
"['## Abstract\n', '\n', ""The aim of this notebook is to check whether train and test sets are significantly different. Can we trust our local validation schemas and public LB? I'll use adversarial validation and Kolmogorov-Smirnov Test for these purposes.""]",0,bearstrikesback,adversarial-validation-plus-ks-test,bearstrikesback_adversarial-validation-plus-ks-test_m0
['### Adversarial Validation'],1,bearstrikesback,adversarial-validation-plus-ks-test,bearstrikesback_adversarial-validation-plus-ks-test_m1
"['Average AUC across folds is stable and concentrates around 0.5. It means that we can hardly distinguish train set from test set using adversarial validation.\n', '\n', ""Now let's expand our investigation of dataset and look at distribution of features in train and test sets with respect to [Kolmogorov-Smirnov test](https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test).""]",2,bearstrikesback,adversarial-validation-plus-ks-test,bearstrikesback_adversarial-validation-plus-ks-test_m2
['### Kolmogorov-Smirnov Test'],3,bearstrikesback,adversarial-validation-plus-ks-test,bearstrikesback_adversarial-validation-plus-ks-test_m3
"[""As we can see, 185 features successfully passed Kolmogorov-Smirnov test. We cannot reject null hypothesis that those features in train and test sets came from the same distribution. 15 features haven't passed this test and probably require our attention.""]",4,bearstrikesback,adversarial-validation-plus-ks-test,bearstrikesback_adversarial-validation-plus-ks-test_m4
"['## Conslusion:\n', '\n', 'From adversarial validation we have no evidence that train and test sets come from different distributions. AUC around 0.50 states that LGBM can hardly distinguish train observations from test. These datasets are quite similar. Local validation schemas and public LB track should correctly reflect your efforts in this competition.\n', '\n', 'From Kolmogorov-Smirnov Test we can also state that both sets are quite similar. Hypothesis that samples are drawn from the same distribution can be rejected only for 15 out of 200 features based on KS-Test. Probably, we should pay more attention to those 15 features.']",5,bearstrikesback,adversarial-validation-plus-ks-test,bearstrikesback_adversarial-validation-plus-ks-test_m5
"['## Abstract\n', 'The aim of this notebook is to look at target distribution versus all variables. Many people have already done similar job, so, first of all, I kindly invite you to check their efforts:\n', '* [Modified Naive Bayes - Santander - [0.899]](https://www.kaggle.com/cdeotte/modified-naive-bayes-santander-0-899)\n', '* [Gaussian Naive Bayes](https://www.kaggle.com/blackblitz/gaussian-naive-bayes)\n', '* [Fast PDF calculation with correlation matrix](https://www.kaggle.com/jiweiliu/fast-pdf-calculation-with-correlation-matrix)\n', '* [Are vars mixed up time intervals?](https://www.kaggle.com/sibmike/are-vars-mixed-up-time-intervals)\n', '* [Boosting creativity towards feature engineering](https://www.kaggle.com/felipemello/boosting-creativity-towards-feature-engineering)\n', '\n', 'Yet many results have already been shown, there is always place for exploration. In this kernel I will try to look at variables from machines perspective.']",0,bearstrikesback,yet-another-target-vs-vars,bearstrikesback_yet-another-target-vs-vars_m0
"['## Charts and cool stuff\n', 'It is not surprise that current **.900** score can be achieved by using LGBM with shallow (only 2 or 3 leaves) trees. More investigation into this area shows that standard **max_bins=255** parameter can be reduced down to **max_bins=25** without any loss in quality. It means that 25 possible points for split is more than enough for this data. (i.e. algorithm sees no point in complicated splits like top 0.5% values vs 99.5% rest). However, the tails of the variables is the most interesting part in this task. Usually they have increased number of occurrences of the positive class. \n', '\n', ""Let's look how positive class is distributed across variables.""]",1,bearstrikesback,yet-another-target-vs-vars,bearstrikesback_yet-another-target-vs-vars_m1
"['### Three types of distribution\n', 'As we can see target seems to have 3 types of distributions across variables.\n', '\n', '### 1st type. Nice and cosy exponential-like monotonous-like distribution.\n', ""These type of distribution seems to growth (or decrease) exponentially as variable increases in value. Probably, that's where models took most information from. It is easy to build good separation rules with these varaibles. Good examples are - **var_34** and **var_40**.""]",2,bearstrikesback,yet-another-target-vs-vars,bearstrikesback_yet-another-target-vs-vars_m2
"['### 2nd type. Saw-like distribution.\n', 'This type of distribution represents contradictory information. It is concentrated around average target value. Consecutive bin values tend to vary a lot. Unlikely this type of variables presents any usefull information to the model. Moreover, I did local experiment where I dropped 30 saw-like variables and still got **.900** score on 5-folds CV. Good examples are - **var_29** and **var_38**.']",3,bearstrikesback,yet-another-target-vs-vars,bearstrikesback_yet-another-target-vs-vars_m3
"['### 3rd type. Nice and cosy with an outlier.\n', ""Probably this is the most interesting type of distribution. It behaves like 1st type, but has a spike somewhere near median. This can heavily affect LGBM ability to identify optimal split. I don't know why this pattern cannot be seen in Chris's kernel. Some variables has enourmous outliers in the center. Good examples are - **var_80** and **var_108**.""]",4,bearstrikesback,yet-another-target-vs-vars,bearstrikesback_yet-another-target-vs-vars_m4
"['### 4th type. (Arbitrary)\n', 'Whereas there are three clear patterns, someone might observe other patterns. For example, some features are stable for the first 50% of values and then start behave like 1st type. (or it might be binning effect).']",5,bearstrikesback,yet-another-target-vs-vars,bearstrikesback_yet-another-target-vs-vars_m5
"['## Closing points\n', '* Score is already high. Decision trees are capable of capturing higher target distribution in tails on their own. No point in trying to generate any features like quantile, quintile, rounding etc.\n', ""* Despite the fact that dropping saw-like features didn't affect the score, it is still question whether these features might be usefull for feature engineering.\n"", '* Discussions [here](https://www.kaggle.com/c/santander-customer-transaction-prediction/discussion/80996) and [here](https://www.kaggle.com/c/santander-customer-transaction-prediction/discussion/86736) indicates that one way to increase score (at least up to **.910** level) is to create additional 200 features. (i.e. apply some kind of transformation to original 200 features).\n', '* People who broke out of **.901** club also state that they observed weird patterns of [local CV increase whereas public LB stayed on the same level](https://www.kaggle.com/c/santander-customer-transaction-prediction/discussion/80996#502449). What kind of transformation can lead to such results?\n', '* No target encoding reportedly been used. \n', '* What if vars are time series and FE might be related to order and aggregate features based on their target distribution plot?\n']",6,bearstrikesback,yet-another-target-vs-vars,bearstrikesback_yet-another-target-vs-vars_m6
"[""This is based on the [Porto Seguro winner's solution](https://www.google.com/search?q=porto+seguro+winn+kaggle&oq=porto+seguro+winn+kaggle&aqs=chrome..69i57j69i60l3.10900j0j4&sourceid=chrome&ie=UTF-8).\n"", '\n', '[Autoencoder](https://alanbertl.com/autoencoder-with-fast-ai/)\n', '\n', '[Hook](https://nbviewer.jupyter.org/github/fastai/course-v3/blob/master/nbs/dl1/lesson6-pets-more.ipynb)\n', '\n', '[Fast.ai](https://nbviewer.jupyter.org/github/fastai/course-v3/blob/master/nbs/dl1/lesson5-sgd-mnist.ipynb)']",0,benjibb,denoised-autoencoder-for-feature-engineering,benjibb_denoised-autoencoder-for-feature-engineering_m0
['# Import modules'],1,benjibb,denoised-autoencoder-for-feature-engineering,benjibb_denoised-autoencoder-for-feature-engineering_m1
['# Import dataset and concatenate it'],2,benjibb,denoised-autoencoder-for-feature-engineering,benjibb_denoised-autoencoder-for-feature-engineering_m2
['# Rank Guass'],3,benjibb,denoised-autoencoder-for-feature-engineering,benjibb_denoised-autoencoder-for-feature-engineering_m3
['# Preprocess for Neural Net'],4,benjibb,denoised-autoencoder-for-feature-engineering,benjibb_denoised-autoencoder-for-feature-engineering_m4
['to see each batch size'],5,benjibb,denoised-autoencoder-for-feature-engineering,benjibb_denoised-autoencoder-for-feature-engineering_m5
['# DAE'],6,benjibb,denoised-autoencoder-for-feature-engineering,benjibb_denoised-autoencoder-for-feature-engineering_m6
['# Hook to extract activations'],7,benjibb,denoised-autoencoder-for-feature-engineering,benjibb_denoised-autoencoder-for-feature-engineering_m7
['# First batch '],8,benjibb,denoised-autoencoder-for-feature-engineering,benjibb_denoised-autoencoder-for-feature-engineering_m8
['# Second batch'],9,benjibb,denoised-autoencoder-for-feature-engineering,benjibb_denoised-autoencoder-for-feature-engineering_m9
['# Third batch'],10,benjibb,denoised-autoencoder-for-feature-engineering,benjibb_denoised-autoencoder-for-feature-engineering_m10
['# Fourth batch'],11,benjibb,denoised-autoencoder-for-feature-engineering,benjibb_denoised-autoencoder-for-feature-engineering_m11
"['# Introduction\n', '\n', 'In this kernel, we attempt to improve the [Gaussian naive Bayes classifier](https://www.kaggle.com/blackblitz/gaussian-naive-bayes) by replacing the Gaussian model with the more flexible Gaussian mixture model.\n', '\n', 'We implement the Gaussian mixture naive Bayes model to predict Santander Customer Transaction Prediction data. The problem has a binary target and 200 continuous features, and we assume that these features are conditionally independent given the class. We model the target $Y$ as Bernoulli, taking values $0$ (negative) and $1$ (positive). The features $X_0,X_1,\\ldots,X_{199}$ are modelled as continuous random variables. Recall the Bayes rule:\n', '\n', ""$$p_{Y|X_0,X_1,\\ldots,X_{199}}(y|x_0,x_1,\\ldots,x_{199})=\\frac{p_Y(y)\\prod_{i=0}^{199}f_{X_i|Y}(x_i|y)}{\\sum_{y'=0}^1p_Y(y')\\prod_{i=0}^{199}f_{X_i|Y}(x_i|y')}$$\n"", '\n', 'The prior $p_Y(y)$ will be taken as the proportion of the two classes, and the likelihood $f_{X_i|Y}(x_i|y)$ will be obtained by fitting the data with the Gaussian mixture model.']",0,blackblitz,gaussian-mixture-naive-bayes,blackblitz_gaussian-mixture-naive-bayes_m0
"['# Getting Acquainted with the Gaussian Mixture Model\n', 'The Gaussian mixture model gives a mixture of normal distrubutions. We can use [sklearn.mixture.GaussianMixture](https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html) to fit the data and compare it with the histogram to get a feel of its behaviour. We also need to standardize the features because too narrow data can impair the fitting ability of the Gaussian mixture model. There are two important hyperparameters: `n_components` is the number of normal distributions to mix in and `reg_covar` is a regularization parameter that controls the spread of the bumps. Note that `score_samples` method gives the log density, so we need to exponentiate to get the density.']",1,blackblitz,gaussian-mixture-naive-bayes,blackblitz_gaussian-mixture-naive-bayes_m1
"['# Implementing the Model\n', '\n', 'We are going to use the Gaussian mixture model to estimate the likelihood probability density functions $f_{X_i|Y}(x_i|y)$. Since multiplying a lot of small numbers will lead to underflow, we take the logarithm and turn products into sums:\n', ""$$\\ln p_{Y|X_0,X_1,\\ldots,X_{199}}(y|x_0,x_1,\\ldots,x_{199})=\\underbrace{\\overbrace{\\ln p_Y(y)}^\\text{log prior}+\\overbrace{\\sum_{i=0}^{199}\\ln f_{X_i|Y}(x_i|y)}^\\text{log likelihood}}_\\text{log joint}-\\overbrace{\\ln\\sum_{y'=0}^1e^{\\ln p_Y(y')+\\sum_{i=0}^{199}\\ln f_{X_i|Y}(x_i|y')}}^\\text{log marginal}$$\n"", '\n', 'Key points in the implementation are:\n', '* The log prior is set as the logarithm of the proportion of different classes.\n', ""* The log likelihood is computed by using [sklearn.mixture.GaussianMixture](https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html)'s `score_samples` method.\n"", '* Computing the log marginal is prone to overflow/underflow, so we use [scipy.special.logsumexp](https://docs.scipy.org/doc/scipy/reference/generated/scipy.special.logsumexp.html) to avoid that.\n', '* In the end, we convert back to probability by exponentiation.\n', '\n', 'All the heavy lifting is done by the Gaussian mixture model. The rest of the computation is very simple and fast.']",2,blackblitz,gaussian-mixture-naive-bayes,blackblitz_gaussian-mixture-naive-bayes_m2
"['# Training and Evaluating the Model\n', '\n', 'We train and evaluate the model by using the training AUC and validation AUC. The process can take time because we train the Gaussian mixture model 400 times, and the training time will increase with higher `n_components`. In order to speed up the hyperparameter search, we use validation, which is k times faster than k-fold cross-validation. Feel free to use cross-validation if you have the time (and tell me if you find better hyperparameters).']",3,blackblitz,gaussian-mixture-naive-bayes,blackblitz_gaussian-mixture-naive-bayes_m3
"['# Submitting the Test Predictions\n', '\n', 'We retrain using all the data and submit the test predictions for the competition.']",4,blackblitz,gaussian-mixture-naive-bayes,blackblitz_gaussian-mixture-naive-bayes_m4
"['# Conclusion\n', '\n', 'The Gaussian mixture naive Bayes performs very well and is an improvement over the Gaussian naive Bayes, although it takes a little more time to train. It has the advantage that it is more flexible and does not require that the data come from a normal distribution. The only assumption is that the features are conditionally independent given the class. An alternative approach is to use kernel density estimation. Whichever method we use, the goal is to have a model that is simple (easily understood), tractable (easily computed) and accurate (represents reality very well).']",5,blackblitz,gaussian-mixture-naive-bayes,blackblitz_gaussian-mixture-naive-bayes_m5
"['# Introduction\n', '\n', 'In this kernel, we will apply Bayesian inference on Santander Customer Transaction data, which has a binary target and 200 continuous features. We model the target as unknown $Y$ and the features as observation $X$. The prior $p_Y(y)$ reflects our knowledge about the unknown before observation. In this problem, $Y$ is Bernoulli (only two classes) so it can be specified by setting the positive probability, which is usually set as the proportion of the positive class in the data. The likelihood $f_{X|Y}(x|y)$ models the distribution of the observation given that we know the class. The posterior $p_{Y|X}(y|x)$ is our updated knowledge about the unknown after observation.\n', '\n', 'The MAP (Maximum A Posteriori) estimator picks the class with the highest posterior probability. For binary classification, it has the same effect as setting a threshold of $0.5$ for the positive posterior probability. The LMS (Least Mean Squares) estimator $\\mathbf E[Y|X]$ picks the mean of the posterior distribution. For binary classification, this is just the positive posterior probability $p_{Y|X}(1|x)$, which is what we need to submit for the competition.\n', '\n', 'The Bayes rule for this problem is of the form\n', ""$$p_{Y|X}(y|x)=\\frac{p_Y(y)f_{X|Y}(x|y)}{\\sum_{y'}p_Y(y')f_{X|Y}(x|y')}$$\n"", '\n', 'Here $X$ represents a sequence of 200 observations $X_0,X_1,\\ldots,X_{199}$. We assume that the likelihood distributions are normal and independent. This gives us the Gaussian naive Bayes classifier (Gaussian means normal and naive means independent):\n', ""$$p_{Y|X_0,X_1,\\ldots,X_{199}}(y|x_0,x_1,\\ldots,x_{199})=\\frac{p_Y(y)\\prod_{i=0}^{199}f_{X_i|Y}(x_i|y)}{\\sum_{y'=0}^1p_Y(y')\\prod_{i=0}^{199}f_{X_i|Y}(x_i|y')}$$\n"", '\n', 'Note that we only require 1 number for the prior and 800 numbers for the likelihood (200 sample means and variances for each of the two classes). ""Fitting"" is just computing those numbers, and ""predicting"" is carried out according to the above formula (although we need to operate on the log scale because multiplying many small numbers poses a problem when our machine has limited precision). It is a very simple and efficient model.']",0,blackblitz,gaussian-naive-bayes,blackblitz_gaussian-naive-bayes_m0
"['# Checking Assumptions\n', '\n', 'The classifier has already been implemented by scikit-learn, so we can use it right away. But we have to make sure that our assumptions hold, i.e., the likelihood distributions are normal and independent.']",1,blackblitz,gaussian-naive-bayes,blackblitz_gaussian-naive-bayes_m1
"['We will look at the likelihood distributions by plotting the KDE (Kernel Density Estimates) using the [pandas.DataFrame.plot.kde](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.plot.kde.html). Note that KDE is a similarity-based method so it gets slower with more data. We can speed up by reducing the number of evaluation points (`ind` parameter), but this also decreases the resolution of the plot.']",2,blackblitz,gaussian-naive-bayes,blackblitz_gaussian-naive-bayes_m2
['The KDE plots above suggest that the likelihood distributions have different centers and spread. We will standardize them (subtract mean and divide by standard deviation) so that they have zero mean and unit variance. We can use [sklearn.preprocessing.StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) for standardization.'],3,blackblitz,gaussian-naive-bayes,blackblitz_gaussian-naive-bayes_m3
"['Now the KDE plots above look approximately normal, but some have small bumps on the left or right. We can proceed without doing anything, or we can use quantile transformation to remove the small bumps. It turns out that the transformation provides only marginal improvement in performance (0.001 in cross-validation AUC) despite requiring significantly more computation. In practice, we might choose to skip the transformation. In this competition, however, we will do the transformation for that tiny improvement.\n', '\n', 'Ideally, we need to apply the transformation to the features separately for the positive and negative classes. However, we cannot because it becomes a trouble when we are predicting the test data (we do not know the target value). We will instead apply it to the features as a whole so what we really get are normal unconditional distributions $f_{X_i}$ instead of normal conditional distributions $f_{X_i|Y}$, but we hope that the conditional distributions will become more normal as well. We can use [sklearn.preprocessing.QuantileTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.QuantileTransformer.html) for quantile transformation.']",4,blackblitz,gaussian-naive-bayes,blackblitz_gaussian-naive-bayes_m4
"['In the KDE plots above, the likelihood distributions have become normal as we desire.\n', '\n', 'Independence is difficult to check, but we can check the sample correlation coefficients. Small correlation coefficients mean that there is a weak linear pattern. We visualize the correlation matrix by using [matplotlib.pyplot.imshow](https://matplotlib.org/api/_as_gen/matplotlib.pyplot.imshow.html).']",5,blackblitz,gaussian-naive-bayes,blackblitz_gaussian-naive-bayes_m5
"['The correlation matrix plot above shows very small correlation coefficients between the features.\n', '\n', 'Finally, it is important that $Y$ is dependent on $X$. If $X$ and $Y$ were independent, then the posterior would be equal to the prior  $p_{Y|X}(y|x)=p_Y(y)$, and we would not need to do any calculation! We have already seen above that the positive and negative likelihood distributions are slightly different. Let us look at how the sample means and sample variances differ.']",6,blackblitz,gaussian-naive-bayes,blackblitz_gaussian-naive-bayes_m6
"['While the sample mean differences are more or less balanced around zero, the sample variance differences are almost entirely on the negative side. This means that the negative likelihood distributions are more concentrated around their means than the positive ones. These differences add to the discriminative power of the model. The further away the centers of the distributions or the greater the difference in the spread of the distributions, the more it can tell about which class the point is coming from.\n', '\n', 'If there are features $X_i$ such that the likelihood distributions are equal — $f_{X_i|Y}(x_i|0)=f_{X_i|Y}(x_i|1)$, their densities will cancel in the numerator and the denominator. These features do not help in classification. So, in some sense, the Bayes classifier performs automatic feature selection.\n', '\n', 'Now I have the following puzzle. The plot below shows two features with the least sample variance difference (greatest absolute difference where the variance of the positive class is higher). Surprisingly, the negative class looks more spread out despite having lower sample variance than the positive class.']",7,blackblitz,gaussian-naive-bayes,blackblitz_gaussian-naive-bayes_m7
['Why? Let us look at the sample mean differences.'],8,blackblitz,gaussian-naive-bayes,blackblitz_gaussian-naive-bayes_m8
"['The center of the negative class is above and to the right of that of the positive class, but in the above plot, we see straight lines on the lower and left edges. The bounds have remained even after quantile transformation. It looks like these bounds have prevented the positive class from expanding to the lower and left sides. The bounds are more obvious when you look at the original data.']",9,blackblitz,gaussian-naive-bayes,blackblitz_gaussian-naive-bayes_m9
"['Despite the presence of bounds, we are going to assume that the transformed data is normal and proceed anyway. We can sample data from normal distributions using [np.random.normal](https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.normal.html) and plot them for comparison.']",10,blackblitz,gaussian-naive-bayes,blackblitz_gaussian-naive-bayes_m10
['We see above that the positive class spreads more to the lower and left sides than the negative class. Another reason for the illusion is that we have far fewer positive points than negative points.'],11,blackblitz,gaussian-naive-bayes,blackblitz_gaussian-naive-bayes_m11
"['# Training and Evaluating the Model\n', '\n', 'Now we are ready to train our model. We combine the quantile transformer and Gaussian naive Bayes classifer, [sklearn.naive_bayes.GaussianNB](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html), into a pipeline using [sklearn.pipeline.make_pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html).']",12,blackblitz,gaussian-naive-bayes,blackblitz_gaussian-naive-bayes_m12
"['After training the model, we plot the ROC curve on training data and evaluate the model by computing the training AUC and cross-validation AUC. We can use [sklearn.metrics.roc_curve](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html) to obtain the values for plotting the curve and [sklearn.metrics.auc](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html) for computing the AUC.']",13,blackblitz,gaussian-naive-bayes,blackblitz_gaussian-naive-bayes_m13
['We compute the 10-fold cross-validation score by using [sklearn.model_selection.cross_val_score](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html).'],14,blackblitz,gaussian-naive-bayes,blackblitz_gaussian-naive-bayes_m14
['We achieved good AUC on both training and cross-validation. But is this the best that this model can achieve? Let us use simulation to get an estimate of the optimal AUC that this model can achieve. We will draw samples from the normal distribution with the 800 parameters of the likelihood. The amount of samples to draw from each class will be determined by the prior so that the classes have the same proportions as the training data.'],15,blackblitz,gaussian-naive-bayes,blackblitz_gaussian-naive-bayes_m15
['We see that the optimal AUC under the model is not much different from the cross-validation AUC.'],16,blackblitz,gaussian-naive-bayes,blackblitz_gaussian-naive-bayes_m16
"['# Submitting the Test Predictions\n', '\n', 'Let us use this model to predict the test data.']",17,blackblitz,gaussian-naive-bayes,blackblitz_gaussian-naive-bayes_m17
"['# Conclusion\n', '\n', 'The Gaussian naive Bayes classifier performs quite well on Santander Customer Trasaction data. This is because the normality and independence assumptions are closely followed by the data. We have seen that even if the data have been generated by independent normal distributions (according to the model trained on transformed data), we cannot get a better AUC. However, there may still be some other transformation that can improve our model. In my opinion, the normality assumption is not very realistic since some features seem to have lower and upper bounds.\n', '\n', 'One can also remove the assumptions and try to use density estimation techniques to model the likelihood distributions. KDE is not very tractable on data of this size. We can also use a Gaussian mixture model or a multivariate normal distribution with the sample covariance matrix from the data. In my experience, they give better training AUC but worse cross-validation AUC. The Gaussian naive Bayes classifier (improved a little bit by quantile transformation) is currently the best Bayesian model for the data. Please tell me if you have found a better one!']",18,blackblitz,gaussian-naive-bayes,blackblitz_gaussian-naive-bayes_m18
"['# The ""Magic"" of Santander\n', 'In this kernel, we will display pictures of the Santander magic! Previously [here][1], in ""Modified Naive Bayes"", we saw that we can model each variable separately and then combine the 200 models to score LB 0.899. We will do the same here after adding a ""magic"" feature to each variable. We will then ensemble the 200 models with logistic regression and score LB 0.920  \n', '  \n', '![image](http://playagricola.com/Kaggle/magic41019.jpg)\n', '  \n', '[1]: https://www.kaggle.com/cdeotte/modified-naive-bayes-santander-0-899']",0,cdeotte,200-magical-models-santander-0-920,cdeotte_200-magical-models-santander-0-920_m0
"['# The ""Magic"" Feature\n', 'When LGBM ""looks"" at the histogram for `Var_198`, it ""sees"" that when `var_198<13` the probability of having `target=1` is high. And when `var_198>13` the probability is low. This can be displayed by showing the predictions made by LGBM (when building a model from only the variable `var_198`). LGBM basically predicts `target=0.18` for `var_198<13` and `target=0.10` otherwise. \n', '  \n', '![image](http://playagricola.com/Kaggle/198without.png)  \n', '  \n', 'LGBM ""divides"" the histogram with **vertical lines** because LGBM does not see **horizontal** differences. A histogram places multiple values into a single bin and produces a smooth picture. If you place every value in its own bin, you will have a jagged picture, where bars change heights from value to value. Some values are unique, some values occur dozens of times (and in the case of `var_108`, some values occur over 300 times!!) Below is a histogram with one value per bin and we zoom in on `11.0000<x<11.1000`. We see that value `11.0712` occurs 5 times and its close neighbor `11.0720` occurs only once.  \n', '    \n', '![image](http://playagricola.com/Kaggle/198zoom3.png)   \n', '  \n', 'These counts are the ""magic"" feature. For each variable, we make a new feature (column) whose value is the number of counts of the corresponding variable. An example of this new column is displayed above next to the histogram. When LGBM has this new feature, it can now ""divide"" the histogram with **horizontal lines** in addition to vertical.  \n', '  \n', '![image](http://playagricola.com/Kaggle/198with.png)  \n', '  \n', 'Notice now that LGBM predicts `target=0.1` when `var_198<13` AND `count=1`. When  `var_198<13` AND `count>1`, it predicts `target=0.36`. This improvement (using the magic) causes validation AUC to become 0.551 as opposed to 0.547 when using `var_198` alone.']",1,cdeotte,200-magical-models-santander-0-920,cdeotte_200-magical-models-santander-0-920_m1
"['# Why is the Magic difficult to find?\n', 'The ""magic"" is difficult to find because the new feature `Var_198_FE` interacts with `Var_198`. Therefore if you add the new feature to an LGBM with `feature_fraction=0.05`, you will not increase your CV or LB. You must set `feature_fraction=1.0`. Then you will gain the benefit from the new feature but you also have the determental effect of modeling spurious original variable interactions. None-the-less, adding the new feature and using `feature_fraction=1.0` achieves CV 0.910. To reach 0.920, we must remove the spurious effects from the original variable interactions.  \n', '  \n', ""UPDATE: I just discovered another reason why magic is hidden. When calculating frequency counts for the test data, you must remove the **fake** test data before counting. (Fake test described [here][1]) If you don't, then your test predictions will score LB 0.900 instead of LB 0.910 and you may disregard frequency counts as useless.\n"", '\n', '# Maximizing Magic Feature\n', 'To maximize the gain of the ""magic"" feature (and climb from LB 0.910 to 0.920), we must allow the new feature to interact with the original variables while preventing the original variables from interacting with each other. Here are 3 ways to do that:\n', ""* Use Data Augmentation (as shown in Jiwei's awesome kernel [here][2]). You must keep original and new feature in same row.\n"", '* Use 200 separate models as shown in this kernel below.\n', ""* Merge new feature and original feature into one feature. In original data, simply add 200 to each unique value. (And don't add new columns)\n"", '\n', ""# Let's Begin\n"", ""When counting the occurence of each value, we will merge the training data and **real** test data first, and count everything together. In YaG320's brilliant kernel [here][1], we learned that half the test data is fake.  \n"", '  \n', '[1]: https://www.kaggle.com/yag320/list-of-fake-samples-and-public-private-lb-split\n', '[2]: https://www.kaggle.com/jiweiliu/lgb-2-leaves-augment']",2,cdeotte,200-magical-models-santander-0-920,cdeotte_200-magical-models-santander-0-920_m2
"['# Ensemble 200 Models with LR\n', ""We now have a model for each variable and its predictions on test and its out-of-fold predictions on train. If we just add (or multiply) the predictions together, the AUC is low. Instead we will use logistic regression to ensemble them. Each set of predictions is a vector of length 200000. We have 200 vectors of out-of-fold predictions, call them `x1, x2, x3, ..., x200`. We know the true train target, call it `y`. We will now use logistic regression to find 200 coefficients (model y from x's). Then we will use those coefficients to combine our 200 test predictions to create a submission.""]",3,cdeotte,200-magical-models-santander-0-920,cdeotte_200-magical-models-santander-0-920_m3
"['# Modified Naive Bayes scores 0.899 LB - Santander\n', 'In this kernel we demonstrate that unconstrained Naive Bayes can score 0.899 LB. I call it ""unconstrained"" because it doesn\'t assume that each variable has a Gaussian distribution like typical Naive Bayes. Instead we allow for arbitrary distributions and we plot these distributions below. I called it ""modified"" because we don\'t reverse the conditional probabilities.\n', '\n', 'This kernel is useful because (1) it shows that an accurate score can be achieved using a simple model that assumes the variables are independent. And (2) this kernel displays interesting EDA which provides insights about the data.\n', '  \n', '# Load Data']",0,cdeotte,modified-naive-bayes-santander-0-899,cdeotte_modified-naive-bayes-santander-0-899_m0
"['# Statistical Functions\n', 'Below are functions to calcuate various statistical things.']",1,cdeotte,modified-naive-bayes-santander-0-899,cdeotte_modified-naive-bayes-santander-0-899_m1
"['# Display Target Density and Target Probability\n', 'Below are two plots for each of the 200 variables. The first is the density of `target=1` versus `target=0`. The second gives the probability that `target=1` given different values for `var_k`.']",2,cdeotte,modified-naive-bayes-santander-0-899,cdeotte_modified-naive-bayes-santander-0-899_m2
"['# Target Probability Function\n', 'Above, the target probability function was calculated for each variable with resolution equal to `standard deviation / 50` from -5 to 5. For example, we know the `Probability ( target=1 | var=x )` for `z-score = -5.00, -4.98, ..., -0.02, 0, 0.02, ..., 4.98, 5.00` where `z-score = (x - var_mean) / (var_standard_deviation)`. The python function below accesses these pre-calculated values from their numpy array.']",3,cdeotte,modified-naive-bayes-santander-0-899,cdeotte_modified-naive-bayes-santander-0-899_m3
"['# Validation\n', ""We will ignore the training data's target and make our own prediction for each training observation. Then using our predictions and the true value, we will calculate validation AUC. (There is a leak in this validation method but none-the-less it gives an approximation of CV score. If you wish to tune this model, you should use a proper validation set. Current actual 5-fold CV is 0.8995)""]",4,cdeotte,modified-naive-bayes-santander-0-899,cdeotte_modified-naive-bayes-santander-0-899_m4
"['# Predict Test and Submit\n', 'Naive Bayes is a simple model. Given observation with `var_0 = 15`, `var_1 = 5`, `var_2 = 10`, etc. We compute the probability that `target=1` by calculating `P(t=1) * P(t=1 | var_0=15)/P(t=1) * P(t=1 | var_1=5)/P(t=1) * P(t=1 | var_2=10)/P(t=1) * ...` where `P(t=1)=0.1` and the other probabilities are computed above by counting occurences in the training data. So each observation has 200 variables and we simply multiply together the 200 target probabilities given by each variable. (In typical Naive Bayes, you use Bayes formula, reverse the probabilities, and find `P(var_0=15 | t=1)`. This is modified Naive Bayes and more intuitive.)']",5,cdeotte,modified-naive-bayes-santander-0-899,cdeotte_modified-naive-bayes-santander-0-899_m5
['# Plot Predictions'],6,cdeotte,modified-naive-bayes-santander-0-899,cdeotte_modified-naive-bayes-santander-0-899_m6
"['# Conclusion\n', ""In conclusion we used modified Naive Bayes to predict Santander Customer transactions. Since we achieved an accurate score of 0.899 LB (which rivals other methods that capture interactions), this demonstrates that there is little or no interaction between the 200 variables. Additionally in this kernel we observed some fascinating EDA which provide insights about the variables. Can this method be improved? Perhaps by tuning this model better (adjust smoothing, resolution, etc) we can increase validation AUC and increase LB AUC but I don't think we can score over 0.902 with this method. There are other secrets hiding in the Santander data.\n"", '![image](http://playagricola.com/Kaggle/score32319.png)']",7,cdeotte,modified-naive-bayes-santander-0-899,cdeotte_modified-naive-bayes-santander-0-899_m7
"['![](https://storage.googleapis.com/kaggle-organizations/141/thumbnail.jpg?r=890)\n', '# Santander Customer Transaction Prediction\n', 'Can you identify who will make a transaction?\n', '\n', 'Version6\n', '- Ensemble : LB 0.899\n', '- LightGBM : LB 0.898\n', '- Catboost : LB 0.898 ']",0,chocozzz,santander-lightgbm-baseline-lb-0-899,chocozzz_santander-lightgbm-baseline-lb-0-899_m0
"['There are some check point. \n', '- 1. The train and test row are similar.  \n', '- 2. The column size so many.  ']",1,chocozzz,santander-lightgbm-baseline-lb-0-899,chocozzz_santander-lightgbm-baseline-lb-0-899_m1
['Wow. All variable name is var_. it means that the variable is identifier !!!. https://www.kaggle.com/c/porto-seguro-safe-driver-prediction porto competition also has identifier variable. This link will help.'],2,chocozzz,santander-lightgbm-baseline-lb-0-899,chocozzz_santander-lightgbm-baseline-lb-0-899_m2
"[""Target is unbalanced. i'll try upsampling...!""]",3,chocozzz,santander-lightgbm-baseline-lb-0-899,chocozzz_santander-lightgbm-baseline-lb-0-899_m3
['Train and Test has no missing value. Very Nice !!!. '],4,chocozzz,santander-lightgbm-baseline-lb-0-899,chocozzz_santander-lightgbm-baseline-lb-0-899_m4
['## LightGBM BaseLine'],5,chocozzz,santander-lightgbm-baseline-lb-0-899,chocozzz_santander-lightgbm-baseline-lb-0-899_m5
"[""Only 10% of the data is positive, so we'll reduce the train size to have an equal numbers of positive and negative samples.""]",0,danielgrimshaw,sklearn-model-exploration,danielgrimshaw_sklearn-model-exploration_m0
"['<div style=""background: linear-gradient(to bottom, #200122, #6f0000); border: 2px; box-radius: 20px""><h1 style=""color: white; text-align: center""><br> <center>Santander Customer Transaction Prediction<center><br></h1></div>']",0,deepak525,sctp-lightgbm-lb-0-899,deepak525_sctp-lightgbm-lb-0-899_m0
['## **Load the Data**'],1,deepak525,sctp-lightgbm-lb-0-899,deepak525_sctp-lightgbm-lb-0-899_m1
['- The Dataset containing 200 numeric feature variables from var_0 to var_199 and a target value.'],2,deepak525,sctp-lightgbm-lb-0-899,deepak525_sctp-lightgbm-lb-0-899_m2
['## Train the model'],3,deepak525,sctp-lightgbm-lb-0-899,deepak525_sctp-lightgbm-lb-0-899_m3
['## **LGBM**'],4,deepak525,sctp-lightgbm-lb-0-899,deepak525_sctp-lightgbm-lb-0-899_m4
['## **CatBoost Classifier**'],5,deepak525,sctp-lightgbm-lb-0-899,deepak525_sctp-lightgbm-lb-0-899_m5
['## **XGBoost**'],6,deepak525,sctp-lightgbm-lb-0-899,deepak525_sctp-lightgbm-lb-0-899_m6
['## **Submission**'],7,deepak525,sctp-lightgbm-lb-0-899,deepak525_sctp-lightgbm-lb-0-899_m7
"['# Bayesian global optimization with gaussian processes for finding (sub-)optimal parameters of LightGBM\n', '\n', 'As many of fellow kaggler asking how did I get LightGBM parameters for the kernel [Customer Transaction Prediction](https://www.kaggle.com/fayzur/customer-transaction-prediction) I published. So, I decided to publish a kernel to optimize parameters. \n', '\n', '\n', '\n', 'In this kernel I use Bayesian global optimization with gaussian processes for finding optimal parameters. This optimization attempts to find the maximum value of an black box function in as few iterations as possible. In our case the black box function will be a function that I will write to optimize (maximize) the evaluation function (AUC) so that parameters get maximize AUC in training and validation, and expect to do good in the private. The final prediction will be **rank average on 5 fold cross validation predictions**.\n', '\n', 'Continue to the end of this kernel and **upvote it if you find it is interesting**.\n', '\n', '![image.jpg](https://i.imgur.com/XKS1oqU.jpg)\n', '\n', 'Image taken from : https://github.com/fmfn/BayesianOptimization']",0,fayzur,lgb-bayesian-parameters-finding-rank-average,fayzur_lgb-bayesian-parameters-finding-rank-average_m0
"['## Notebook  Content\n', '0. [Installing Bayesian global optimization library](#0) <br>    \n', '1. [Loading the data](#1)\n', '2. [Black box function to be optimized (LightGBM)](#2)\n', '3. [Training LightGBM model](#3)\n', '4. [Rank averaging](#4)\n', '5. [Submission](#5)']",1,fayzur,lgb-bayesian-parameters-finding-rank-average,fayzur_lgb-bayesian-parameters-finding-rank-average_m1
"['<a id=""0""></a> <br>\n', '## 0. Installing Bayesian global optimization library\n', '\n', ""Let's install the latest release from pip""]",2,fayzur,lgb-bayesian-parameters-finding-rank-average,fayzur_lgb-bayesian-parameters-finding-rank-average_m2
"['<a id=""1""></a> <br>\n', '## 1. Loading the data']",3,fayzur,lgb-bayesian-parameters-finding-rank-average,fayzur_lgb-bayesian-parameters-finding-rank-average_m3
"[""We are given anonymized dataset containing 200 numeric feature variables from var_0 to var_199. Let's have a look train dataset:""]",4,fayzur,lgb-bayesian-parameters-finding-rank-average,fayzur_lgb-bayesian-parameters-finding-rank-average_m4
['Test dataset:'],5,fayzur,lgb-bayesian-parameters-finding-rank-average,fayzur_lgb-bayesian-parameters-finding-rank-average_m5
['Distribution of target variable'],6,fayzur,lgb-bayesian-parameters-finding-rank-average,fayzur_lgb-bayesian-parameters-finding-rank-average_m6
['The problem is unbalanced! '],7,fayzur,lgb-bayesian-parameters-finding-rank-average,fayzur_lgb-bayesian-parameters-finding-rank-average_m7
['In this kernel I will be using **50% Stratified rows** as holdout rows for the validation-set to get optimal parameters. Later I will use 5 fold cross validation in the final model fit.'],8,fayzur,lgb-bayesian-parameters-finding-rank-average,fayzur_lgb-bayesian-parameters-finding-rank-average_m8
['These `bayesian_tr_index` and `bayesian_val_index` indexes will be used for the bayesian optimization as training and validation index of training dataset.'],9,fayzur,lgb-bayesian-parameters-finding-rank-average,fayzur_lgb-bayesian-parameters-finding-rank-average_m9
"['<a id=""2""></a> <br>\n', '## 2. Black box function to be optimized (LightGBM)']",10,fayzur,lgb-bayesian-parameters-finding-rank-average,fayzur_lgb-bayesian-parameters-finding-rank-average_m10
"[""As data is loaded, let's create the black box function for LightGBM to find parameters.""]",11,fayzur,lgb-bayesian-parameters-finding-rank-average,fayzur_lgb-bayesian-parameters-finding-rank-average_m11
"['The above `LGB_bayesian` function will act as black box function for Bayesian optimization. I already defined the the trainng and validation dataset for LightGBM inside the `LGB_bayesian` function. \n', '\n', 'The `LGB_bayesian` function takes values for `num_leaves`, `min_data_in_leaf`, `learning_rate`, `min_sum_hessian_in_leaf`, `feature_fraction`, `lambda_l1`, `lambda_l2`, `min_gain_to_split`, `max_depth` from Bayesian optimization framework. Keep in mind that `num_leaves`, `min_data_in_leaf`, and `max_depth` should be integer for LightGBM. But Bayesian Optimization sends continous vales to function. So I force them to be integer. I am only going to find optimal parameter values of them. The reader may increase or decrease number of parameters to optimize.']",12,fayzur,lgb-bayesian-parameters-finding-rank-average,fayzur_lgb-bayesian-parameters-finding-rank-average_m12
"['Now I need to give bounds for these parameters, so that Bayesian optimization only search inside the bounds.']",13,fayzur,lgb-bayesian-parameters-finding-rank-average,fayzur_lgb-bayesian-parameters-finding-rank-average_m13
"[""Let's put all of them in BayesianOptimization object""]",14,fayzur,lgb-bayesian-parameters-finding-rank-average,fayzur_lgb-bayesian-parameters-finding-rank-average_m14
"[""Now, let's the the key space (parameters) we are going to optimize:""]",15,fayzur,lgb-bayesian-parameters-finding-rank-average,fayzur_lgb-bayesian-parameters-finding-rank-average_m15
"['I have created the BayesianOptimization object (`LGB_BO`), it will not work until I call maximize. Before calling it, I want to explain two parameters of BayesianOptimization object (`LGB_BO`) which we can pass to maximize:\n', '- `init_points`: How many initial random runs of **random** exploration we want to perform. In our case `LGB_bayesian` will be called `n_iter` times.\n', '- `n_iter`: How many runs of bayesian optimization we want to perform after number of `init_points` runs. ']",16,fayzur,lgb-bayesian-parameters-finding-rank-average,fayzur_lgb-bayesian-parameters-finding-rank-average_m16
"[""Now, it's time to call the function from Bayesian optimization framework to maximize. I allow `LGB_BO` object to run for 5 `init_points` (exploration) and 5 `n_iter` (exploitation).""]",17,fayzur,lgb-bayesian-parameters-finding-rank-average,fayzur_lgb-bayesian-parameters-finding-rank-average_m17
"[""As the optimization is done, let's see what is the maximum value we have got.""]",18,fayzur,lgb-bayesian-parameters-finding-rank-average,fayzur_lgb-bayesian-parameters-finding-rank-average_m18
"[""The validation AUC for parameters is 0.89 ! Let's see parameters is responsible for this score :)""]",19,fayzur,lgb-bayesian-parameters-finding-rank-average,fayzur_lgb-bayesian-parameters-finding-rank-average_m19
['Now we can use these parameters to our final model!'],20,fayzur,lgb-bayesian-parameters-finding-rank-average,fayzur_lgb-bayesian-parameters-finding-rank-average_m20
"['Wait, I want to show one more cool option from BayesianOptimization library. You can probe the `LGB_bayesian` function, if you have an idea of the optimal parameters or it you get **parameters from other kernel** like mine [mine](https://www.kaggle.com/fayzur/customer-transaction-prediction). I will copy and paste parameters from my other kernel here. You can probe as folowing:']",21,fayzur,lgb-bayesian-parameters-finding-rank-average,fayzur_lgb-bayesian-parameters-finding-rank-average_m21
"[""OK, by default these will be explored lazily (lazy=True), meaning these points will be evaluated only the next time you call maximize. Let's do a maximize call of `LGB_BO` object.""]",22,fayzur,lgb-bayesian-parameters-finding-rank-average,fayzur_lgb-bayesian-parameters-finding-rank-average_m22
"['Finally, the list of all parameters probed and their corresponding target values is available via the property LGB_BO.res.']",23,fayzur,lgb-bayesian-parameters-finding-rank-average,fayzur_lgb-bayesian-parameters-finding-rank-average_m23
['We have got a better validation score in the probe! As previously I ran `LGB_BO` only for 10 runs. In practice I increase it to arround 100.'],24,fayzur,lgb-bayesian-parameters-finding-rank-average,fayzur_lgb-bayesian-parameters-finding-rank-average_m24
"[""Let's build a model together use therse parameters ;)""]",25,fayzur,lgb-bayesian-parameters-finding-rank-average,fayzur_lgb-bayesian-parameters-finding-rank-average_m25
"['<a id=""3""></a> <br>\n', '## 3. Training LightGBM model']",26,fayzur,lgb-bayesian-parameters-finding-rank-average,fayzur_lgb-bayesian-parameters-finding-rank-average_m26
"[""As you see, I assined `LGB_BO`'s optimal parameters to the `param_lgb` dictionary and they will be used to train a model with 5 fold.""]",27,fayzur,lgb-bayesian-parameters-finding-rank-average,fayzur_lgb-bayesian-parameters-finding-rank-average_m27
['Number of Kfolds:'],28,fayzur,lgb-bayesian-parameters-finding-rank-average,fayzur_lgb-bayesian-parameters-finding-rank-average_m28
['So we got 0.90 AUC in 5 fold cross validation. And 5 fold prediction look like:'],29,fayzur,lgb-bayesian-parameters-finding-rank-average,fayzur_lgb-bayesian-parameters-finding-rank-average_m29
"[""If you are still reading, bare with me. I will not take much of your time. :D We are almost done. Let's do a rank averaging on 5 fold predictions.""]",30,fayzur,lgb-bayesian-parameters-finding-rank-average,fayzur_lgb-bayesian-parameters-finding-rank-average_m30
"['<a id=""4""></a> <br>\n', '## 4. Rank averaging']",31,fayzur,lgb-bayesian-parameters-finding-rank-average,fayzur_lgb-bayesian-parameters-finding-rank-average_m31
"[""Let's submit prediction to Kaggle.""]",32,fayzur,lgb-bayesian-parameters-finding-rank-average,fayzur_lgb-bayesian-parameters-finding-rank-average_m32
"['<a id=""5""></a> <br>\n', '## 5. Submission']",33,fayzur,lgb-bayesian-parameters-finding-rank-average,fayzur_lgb-bayesian-parameters-finding-rank-average_m33
['Do not forget to upvote :) Also fork and modify for your own use. ;)'],34,fayzur,lgb-bayesian-parameters-finding-rank-average,fayzur_lgb-bayesian-parameters-finding-rank-average_m34
"['# Santander Customer Transaction Prediction\n', '\n', 'This kernel uses LGBM model to predict Customer Transaction.\n', '\n', '**For LightGBM parameters optimization, please find my other kernel below, where I show how to take advantage of Bayesian Optimization to find optimal paramer ofr LightGBM:**\n', '\n', 'https://www.kaggle.com/fayzur/lgb-bayesian-parameters-finding-rank-average\n', '\n', '\n', '## Notebook  Content\n', '1. [Loading the data](#0) <br>    \n', '1. [Training the model](#1)\n', '1. [Submission](#2)']",0,fayzur,lightgbm-customer-transaction-prediction,fayzur_lightgbm-customer-transaction-prediction_m0
"['<a id=""0""></a> <br>\n', '## 1. Loading the data']",1,fayzur,lightgbm-customer-transaction-prediction,fayzur_lightgbm-customer-transaction-prediction_m1
"[""We are given anonymized dataset containing 200 numeric feature variables from var_0 to var_199. Let's have a look train dataset:""]",2,fayzur,lightgbm-customer-transaction-prediction,fayzur_lightgbm-customer-transaction-prediction_m2
['Test dataset:'],3,fayzur,lightgbm-customer-transaction-prediction,fayzur_lightgbm-customer-transaction-prediction_m3
['Distribution of target variable'],4,fayzur,lightgbm-customer-transaction-prediction,fayzur_lightgbm-customer-transaction-prediction_m4
"['The problem is unbalance!\n', 'We can build a quick model on this dataset considering unbalance to see how far we can go without Feature engineering! ']",5,fayzur,lightgbm-customer-transaction-prediction,fayzur_lightgbm-customer-transaction-prediction_m5
"['<a id=""1""></a> <br>\n', '## 2. Training the model']",6,fayzur,lightgbm-customer-transaction-prediction,fayzur_lightgbm-customer-transaction-prediction_m6
"['The above parameters were obtained using the same structure presented in the following kernel:\n', '\n', 'https://www.kaggle.com/fayzur/lgb-bayesian-parameters-finding-rank-average']",7,fayzur,lightgbm-customer-transaction-prediction,fayzur_lightgbm-customer-transaction-prediction_m7
['Number of Kfolds:'],8,fayzur,lightgbm-customer-transaction-prediction,fayzur_lightgbm-customer-transaction-prediction_m8
"['<a id=""2""></a> <br>\n', '## 2. Submission']",9,fayzur,lightgbm-customer-transaction-prediction,fayzur_lightgbm-customer-transaction-prediction_m9
['Upvote if it is useful :)'],10,fayzur,lightgbm-customer-transaction-prediction,fayzur_lightgbm-customer-transaction-prediction_m10
"[""For 2 weeks I worked hard and didn't improve my score. For 4 days I worked smart, and got LB 0.920 (74th out of 9000+). \n"", '\n', 'The main purpose of this kernel is to share the thinking process that led me to this LB score, at the same time that you can have fun visualizing the data.\n']",0,felipemello,step-by-step-guide-to-the-magic-lb-0-922,felipemello_step-by-step-guide-to-the-magic-lb-0-922_m0
"['### **1. INTRODUCTION**\n', '\n', 'In this competition, the objective was to forecast which clients would make a specific transfer in the future. The data given was composed of 200.000 rows and 200 anonymized features.\n', '\n', 'The evaluation metric was ROC-AUC. In case you are not familliar with it, check it [here](https://medium.com/greyatom/lets-learn-about-auc-roc-curve-4a94b4d88152).\n', '\n', '**Context**:\n', 'This competition broke Kaggle\'s previous competitors record (around 6000), and became the most disputed competition ever in this platform, with over 9000 competitors. For many weeks, almost no team could surpass the 0.901 score barrier. So everyone started to look for the ""magic"".\n', '\n', ""### **2. WHAT DIDN'T WORK**\n"", '\n', 'Many teams tried to explore the data in every possible way: Features operations (+,-,*,/,log,tanh,etc), distribution, probability, time series, target encoding, data rotation, genetic programming, oversampling, undersampling, different models....you get it. Absolutely nothing worked\n', '\n', 'After 2 weeks trying everything, I got tired of working hard and decided to work smart, so this happened: I broke the 0.901 barrier.\n', '\n', '![](https://raw.githubusercontent.com/fmellomascarenhas/Kaggle/master/Santander%20-%202019%20-%2074th%20place/pictures/0.902_score.png)\n', '\n', '### **3. WHAT WORKED**\n', '\n', ""**WARNING:** Before you proceed, you should know that **the scores you will see won't match the ones described**. That's because I used a simple train_test_split to speed up this kernel, instead of kfolds.\n"", '\n', '### **3.1. CV 0.902 - MAGIC IN 4 LINES**\n', '\n', 'When I decided to work smart, I re-read relevant topics and kernels in this competition and made a list about the dos and donts expressed by other competitors (as you can see below). [Making a kernel](https://www.kaggle.com/felipemello/why-your-model-is-overfitting-not-making-progress) about my own failed experiments helped me realize why they were not working.\n', '\n', '**At this point, what did I know?**\n', '\n', ""1) Regular transformations wouldn't work (many topics shared failed experiments). \n"", '\n', '2) [The test dataset had fake data](https://www.kaggle.com/yag320/list-of-fake-samples-and-public-private-lb-split), and that was relevant.\n', '\n', '![](https://raw.githubusercontent.com/fmellomascarenhas/Kaggle/master/Santander%20-%202019%20-%2074th%20place/pictures/kernel_comment_1.png)\n', '\n', '3) That [unique values](https://www.kaggle.com/triplex/more-unique-values-in-train-set-than-test-set) were, somehow, important.\n', '\n', '![](https://raw.githubusercontent.com/fmellomascarenhas/Kaggle/master/Santander%20-%202019%20-%2074th%20place/pictures/kernel_comment_2.png)\n', '\n', 'So I decided to experiment with both of these ideas: I merged the train and test datataset without fake values, created new columns for each variable with the number of unique values in it, and trained my model. That was enough to break the 0.901. You can see this simples procedure below.\n']",1,felipemello,step-by-step-guide-to-the-magic-lb-0-922,felipemello_step-by-step-guide-to-the-magic-lb-0-922_m1
"['### **3.2. CV 0.909 - SETTING FEATURE FRACTION TO 1**\n', '\n', 'Feature fraction is a [parameter of the LGB model](https://lightgbm.readthedocs.io/en/latest/Parameters.html). It goes from 0 to 1 and represents the percentage of the data that you will use on each iteration of the traning. My feature fraction was 0.3. When I set it to 1, the model was able to look at all the variables at once, and voilá: CV 0.909! \n', '\n', '**TIP**: Setting feature fraction to 1 is a great way to understand the impact of a new feature in your model.\n', '\n', '![](https://raw.githubusercontent.com/fmellomascarenhas/Kaggle/master/Santander%20-%202019%20-%2074th%20place/pictures/909_score.png)\n', '\n', '\n', '### **3.3. CV 0.913 - MAKING MY JOB EASIER**\n', '\n', 'At this point, I had a problem to solve: Each new feature about values frequency only mattered for one other specific feature. My model, however, was checking all possible interactions between my 400 features and taking a **long time to run**. \n', '\n', 'This [kernel](https://www.kaggle.com/ymatioun/santander-model-one-feature-at-a-time) was the perfect solution, when it proved that it was possible to train the model on each one of the variables separately and still achieve the same LB score. \n', '\n', 'So I decided train my model with 2 features at a time: The original one and an extra column with the unique values count.\n', '\n', '**RESULT: Less than 2 minutes to train my whole model (as opposed to over an hour) and CV of 0.913. Working smart was the way to go.**']",2,felipemello,step-by-step-guide-to-the-magic-lb-0-922,felipemello_step-by-step-guide-to-the-magic-lb-0-922_m2
"['#### **3.4. OPTIMIZING PARAMETERS - CV 0.920**\n', '\n', 'Since my model now only took a couple of minutes to train, it became really easy to test hundreds of parameters. You can check a great kernel about Bayesian Optimization [here](https://www.kaggle.com/fayzur/lgb-bayesian-parameters-finding-rank-average).\n', '\n', '![](https://raw.githubusercontent.com/fmfn/BayesianOptimization/master/examples/func.png)\n', '\n', 'Just leaving the algorithm working there for 3 hours was enough to score 0.920LB on the last day of competition. If I had more hours to work with it, probably my score would have improved substantially.\n', '\n', ""**Tips:** The number of iterations on the LGB was found to be really important. You can't just leave a high number of iterations and set your model to do an early stop if the score doesn't improve.\n"", '\n', '![](https://raw.githubusercontent.com/fmellomascarenhas/Kaggle/master/Santander%20-%202019%20-%2074th%20place/pictures/920_score.png)\n', '\n', 'Those were my final parameters (click on the **code** button ->):']",3,felipemello,step-by-step-guide-to-the-magic-lb-0-922,felipemello_step-by-step-guide-to-the-magic-lb-0-922_m3
"[""#### **3.5. GIVING WEIGHTS TO EACH FEATURE'S RESULT - CV 0.922**\n"", '\n', 'Since we were training the model on each feature, maybe one matered more than other, but how could our model find it? To solve that, before we calculated the AUC score, we decided to give to each column a weight, based on their individual AUC score. That raised my score in 0.002.']",4,felipemello,step-by-step-guide-to-the-magic-lb-0-922,felipemello_step-by-step-guide-to-the-magic-lb-0-922_m4
"['You could try to optimize those weights in some other way. A good option, for example, would be to use the bayesian optimizer.\n', '\n', 'If you want to use the weights, make sure that they are not overfitting your results. The best way is to use stratified k folds, and then apply the weights. Below is an example on how to test them before submiting.']",5,felipemello,step-by-step-guide-to-the-magic-lb-0-922,felipemello_step-by-step-guide-to-the-magic-lb-0-922_m5
"['### **4. LOOKING AT THE DATA: WHAT IS ODD ABOUT THE UNIQUE VALUES?**\n', '\n', ""**EDIT: I didn't know that  bimodal distribution is a natural phenomena of normal distributions. Thanks @cdeotte and @triplex for pointing that out. I will still leave the plots there, because even though it is a natural phenomena, it is a cool one.**\n"", '\n', 'When I looked at the Probability Density Functions (PDFs) of our variables, I discovered a clear bimodal distribution that exists among values that do not repeat. As those values start repeating more, the distribution changes, and it is really interesting to see.\n', '\n', 'To show that to you, first I will need to group some values.\n', '\n', '### **4.1. GROUPING VALUES**\n', '\n', 'Some variables, like var_12 and var_68, have many frequency groups. Some of these groups are really small, and can have less than 100 samples. Take a look and compare the PDFs of frequency groups in var_12 and var_81:']",6,felipemello,step-by-step-guide-to-the-magic-lb-0-922,felipemello_step-by-step-guide-to-the-magic-lb-0-922_m6
"['To avoid working with such small groups and risk overfitting, we will say that each group has to have at least 2000 samples in the test + train datasets in total.']",7,felipemello,step-by-step-guide-to-the-magic-lb-0-922,felipemello_step-by-step-guide-to-the-magic-lb-0-922_m7
"['### **4.2. BIMODAL DISTRIBUTIONS**.\n', '\n', 'Now that we are done with grouping, check the bimodals distributions below. They show the PDFs of the 6 first frequency groups of each variable. Example: Graph 1: All values that are unique / Graph 2: All values that repeat only twitce / Graph 3: .... / etc.']",8,felipemello,step-by-step-guide-to-the-magic-lb-0-922,felipemello_step-by-step-guide-to-the-magic-lb-0-922_m8
['Not all features are like that though. Take a look at the feature 117 and 120.'],9,felipemello,step-by-step-guide-to-the-magic-lb-0-922,felipemello_step-by-step-guide-to-the-magic-lb-0-922_m9
"[""How cool is that? Couldn't figure out why this happens. If you have any hypothesis, let me know.""]",10,felipemello,step-by-step-guide-to-the-magic-lb-0-922,felipemello_step-by-step-guide-to-the-magic-lb-0-922_m10
"['### **5. WHAT I WOULD LIKE TO HAVE DONE, IF I HAD MORE TIME**\n', '\n', '#### **5.1. EXPLORE THE BIMODAL DISTRIBUTIONS**\n', '\n', 'Probabily trying to show to LGB that there is a bimodal distribution would be useless, because it already sees it. Take a look at the plot below of the variable 81.']",11,felipemello,step-by-step-guide-to-the-magic-lb-0-922,felipemello_step-by-step-guide-to-the-magic-lb-0-922_m11
"[""For each one of the frequencys, the LGB is already going to make splits, figure out that there is a bimodal distribution and check the probabilities of being target = 1 or target = 0. If you are confused about it or if your don't believe me, check my kernel [Why your model is overfitting/not making progress](https://www.kaggle.com/felipemello/why-your-model-is-overfitting-not-making-progress)\n"", '\n', '**Besides that**, with a simple linear transformation, we could supperpose two distributions, making them more readable to our human eyes. Take a look at var_81 and var_126 before and after we transform var_126.']",12,felipemello,step-by-step-guide-to-the-magic-lb-0-922,felipemello_step-by-step-guide-to-the-magic-lb-0-922_m12
"[""Isn't this so interesting? But, again, scalling values won't affect the way that LGB sees the model. This transformation just makes it more pleasent to our eyes. \n"", '\n', '**Note:** If you wanna do it yourself, just get the ratio of the distances between the peaks of the distributions, multiply one of the distributions by this ratio and subtract the differente between peaks of distribution 1 and 2.\n', '\n', '#### **5.2. OPTIMIZE THE LGB PARAMETERS FOR EACH VARIABLE**\n', '\n', 'Since we have 200 different models, instead of having one set of parameters for all variables, we could pick  a specific set of parameters that works best for them.\n', '\n', ""I tried doing it, and saw my AUC falling back to 0.900. I am not sure why. Maybe its overfitting? I don't know if it is possible to do that, but if it is, I bet it could substantially increase the score.\n"", '\n', '#### **5.3. OTHER STUFF**\n', '\n', '- Create NN to do ensembling;\n', '- Try to use pure probability, instead of the LGB, as it was done in [this](https://www.kaggle.com/cdeotte/modified-naive-bayes-santander-0-899) kernel. Another competitor reported a 0.922 LB score using this technic and a gold medal;\n', '- Applied augumentation, as described [here](https://www.kaggle.com/jiweiliu/lgb-2-leaves-augment).\n', '\n', 'However, there is only so much you can do in 4 days.\n', '\n', '\n', '### 6. SUMMARY\n', '\n', 'Thank you for your attention. I hope you had fun. In this kernel we saw that:\n', '- Smart work is better than hard work. It is faster and produces better results;\n', '- If you think rationally about things, understand your model and your data, you can quickly improve your results;\n', '- There is a lot more to explore in the data;\n', '\n', '**If you liked the kernel, please, consider upvoting. It is the way to best reward the hours I put making this kernel. Thanks :)**\n', '\n', '### MY FINAL RUN WITH 4 FOLDS']",13,felipemello,step-by-step-guide-to-the-magic-lb-0-922,felipemello_step-by-step-guide-to-the-magic-lb-0-922_m13
"['# Santander Customer Transaction Prediction\n', '\n', '![](https://storage.googleapis.com/kaggle-media/competitions/santander/atm_image.png)']",0,frtgnn,santander-eda-prediction-augmentation,frtgnn_santander-eda-prediction-augmentation_m0
"['# Weighted Kernel Naive Bayes with Lasso Feature Elimination by TF - Santander  \n', ""> *More things should not be used than are necessary. - Occam's razor*\n"", '\n', 'Is **EVERY** variable IMPORTANT for prediction? ** The answer is not even close to yes by our finding.**\n', '\n', 'In this work, we will explore the santander using two upgraded naive bayes inference -- *Kernel NB and Weighted Kernel NB.*  \n', '\n', 'Firstly, we are going to introduce an updated version of Gaussian Naive Bayes method called **Kernel Naive Bayes** which release the assumption of every features follow normal distribution.   \n', '\n', 'Secondly, we are going to use back-prop & gradient decent to learn an updated model called **Weighted Kernel Naive Bayes** with lasso feature elimination. This is implemented by log transform the naive bayes formular from product terms into additive terms, and use TensorFlow to learn the loss function with L1-norm and ReLU constriants on weights.   \n', '\n', 'Our final experiment show that we can achieve **same level of AUC** using **only 75% or even 50% of features**,meaning that nearly half of the features are not informative for prediction.\n']",0,fuhang,weighted-kernel-naive-bayes-with-lasso-elimination,fuhang_weighted-kernel-naive-bayes-with-lasso-elimination_m0
['-------------------------'],1,fuhang,weighted-kernel-naive-bayes-with-lasso-elimination,fuhang_weighted-kernel-naive-bayes-with-lasso-elimination_m1
"['Recall naive bayes follows the **feature independence law**, and use logit probability as prediction score. Conclusively, it should have the following fomular:  \n', '<img src=""https://latex.codecogs.com/gif.latex?logit\\_prob=\\frac{p(y=1|X)}{p(y=0|X)}=\\frac{p(y=1)}{p(y=0)}*\\frac{p(X|y=1)}{p(X|y=0)}=\\frac{p(y=1)}{p(y=0)}*\\prod_{i=1}^{200}\\frac{p(x_{i}|y=1)}{p(x_{i}|y=0)}"" title=""logit_p=\\frac{p(y=1|X)}{p(y=0|X)}=\\frac{p(y=1)}{p(y=0)}*\\frac{p(X|y=1)}{p(X|y=0)}=\\frac{p(y=1)}{p(y=0)}*\\prod_{i=1}^{200}\\frac{p(x_{i}|y=1)}{p(x_{i}|y=0)}"" />\n', '\n', 'where logit_prob is the final prediction. \n']",2,fuhang,weighted-kernel-naive-bayes-with-lasso-elimination,fuhang_weighted-kernel-naive-bayes-with-lasso-elimination_m2
"['  \n', 'As we know already, in the first term <img src=""https://latex.codecogs.com/gif.latex?{p(y=1)}"" title=""{p(y=1)}"" /> is prior probability of positive class. And P(y=0) can also be calculated easily by 1-P(y=1).  ']",3,fuhang,weighted-kernel-naive-bayes-with-lasso-elimination,fuhang_weighted-kernel-naive-bayes-with-lasso-elimination_m3
"['The problem lies on how to calculate the 200 other terms, that is to say, how to calculate the following:\n', '<img src=""https://latex.codecogs.com/gif.latex?{p(x_{i}|y=1)}"" title=""{p(x_{i}|y=1)}"" /> as well as <img src=""https://latex.codecogs.com/gif.latex?{p(x_{i}|y=0)}"" title=""{p(x_{i}|y=0)}"" />']",4,fuhang,weighted-kernel-naive-bayes-with-lasso-elimination,fuhang_weighted-kernel-naive-bayes-with-lasso-elimination_m4
"['There are **two** basic way of calculating this.   \n', 'First way to do is assume that ith feature (**xi**) follows a gaussian distribution a.k.a. normal distribution, and calculate <img src=""https://latex.codecogs.com/gif.latex?{p(x_{i}|y=1)}"" title=""{p(x_{i}|y=1)}"" /> by the probability density function (PDF) of normal distribution.  \n', '\n', 'However,from other kernel we can see clearly that NOT all of these features follow gaussian distribution, for e.g.https://www.kaggle.com/cdeotte/modified-naive-bayes-santander-0-899\n', '\n', 'So a Gaussian assumution might **not be the best choice for estimating p(xi|y)**.  \n', 'For the second way we introduce** Kernel Density Estimation** to calculate the pdf of an arbitary distribution of features.']",5,fuhang,weighted-kernel-naive-bayes-with-lasso-elimination,fuhang_weighted-kernel-naive-bayes-with-lasso-elimination_m5
"[""What's more we use gaussian kernel KDE using scipy.stats.kde.gaussian_kde, and binize the features to reduce the complexity from O(#data) to O(#bins).  ""]",6,fuhang,weighted-kernel-naive-bayes-with-lasso-elimination,fuhang_weighted-kernel-naive-bayes-with-lasso-elimination_m6
"[""## For accelation, instead of calculating **p(xi|y)**, we now calculate **p(bin\\_of\\_xi|y)** for every bins. To achieve that, we cut every continues value in xi into bins, and map continues xi to its bins' probability:** p(bin\\_of\\_xi|y)** . This is bining Kernel Naive Bayes. ""]",7,fuhang,weighted-kernel-naive-bayes-with-lasso-elimination,fuhang_weighted-kernel-naive-bayes-with-lasso-elimination_m7
"['Here we have already get the Kernel Naive Bayes transformation of original data. It have 201 columns. Be aware of what we have done, these 201 features are not orignal features, but 201 terms in naive bayes formular:\n', '<img src=""https://latex.codecogs.com/gif.latex?logit\\_prob=\\frac{p(y=1)}{p(y=0)}*\\prod_{i=1}^{200}\\frac{p(x_{i}|y=1)}{p(x_{i}|y=0)}"" title=""\\frac{p(y=1)}{p(y=0)}*\\prod_{i=1}^{200}\\frac{p(x_{i}|y=1)}{p(x_{i}|y=0)}"" />\n', 'where the first term is logit of prior probability, and last 200 terms are logit of likelyhood.']",8,fuhang,weighted-kernel-naive-bayes-with-lasso-elimination,fuhang_weighted-kernel-naive-bayes-with-lasso-elimination_m8
"['We are pretty **close to the final prediction**. If we now apply Naive Bayes, we can get the final prediction by multiplying these 201 terms as following:']",9,fuhang,weighted-kernel-naive-bayes-with-lasso-elimination,fuhang_weighted-kernel-naive-bayes-with-lasso-elimination_m9
"['**The AUC is over 0.908.** That is a very impressive score in training set, althought may be a little overfit on the training data.   \n', 'According to the original kernel https://www.kaggle.com/jiazhuang/demonstrate-naive-bayes/notebook, it reported **AUC 0.894** in public leaderboard.']",10,fuhang,weighted-kernel-naive-bayes-with-lasso-elimination,fuhang_weighted-kernel-naive-bayes-with-lasso-elimination_m10
['**Above are explanation & code for Kernel Naive Bayes. **  \n'],11,fuhang,weighted-kernel-naive-bayes-with-lasso-elimination,fuhang_weighted-kernel-naive-bayes-with-lasso-elimination_m11
"['------------------------\n', '# Now introduce Weighted Kernel Naive Bayes with Lasso Feature Elimination by gradient decent.']",12,fuhang,weighted-kernel-naive-bayes-with-lasso-elimination,fuhang_weighted-kernel-naive-bayes-with-lasso-elimination_m12
"['## First, in order to simplfy our naive bayes problem, we log the ""logit\\_prob"" term in order to transform series of product into series of sum.\n', '<img src=""https://latex.codecogs.com/gif.latex?log[\\frac{p(y=1)}{p(y=0)}*\\prod_{i=1}^{200}\\frac{p(x_{i}|y=1)}{p(x_{i}|y=0)}]=log\\frac{p(y=1)}{p(y=0)}&plus;\\sum_{i=1}^{200}log\\frac{p(x_{i}|y=1)}{p(x_{i}|y=0)}"" title=""log[\\frac{p(y=1)}{p(y=0)}*\\prod_{i=1}^{200}\\frac{p(x_{i}|y=1)}{p(x_{i}|y=0)}]=w_{0}\\cdot log\\frac{p(y=1)}{p(y=0)}+\\sum_{i=1}^{200}w_{i}\\cdot log\\frac{p(x_{i}|y=1)}{p(x_{i}|y=0)}"" />']",13,fuhang,weighted-kernel-naive-bayes-with-lasso-elimination,fuhang_weighted-kernel-naive-bayes-with-lasso-elimination_m13
"['## Second, we try to **weighted every term above** -- that is why it is called weighted naive bayes.\n', '<img src=""https://latex.codecogs.com/gif.latex?log\\_logit\\_prob=w_{0}\\cdot&space;log\\frac{p(y=1)}{p(y=0)}&plus;\\sum_{i=1}^{200}w_{i}\\cdot&space;log\\frac{p(x_{i}|y=1)}{p(x_{i}|y=0)}"" title=""w_{0}\\cdot log\\frac{p(y=1)}{p(y=0)}+\\sum_{i=1}^{200}w_{i}\\cdot log\\frac{p(x_{i}|y=1)}{p(x_{i}|y=0)}"" />']",14,fuhang,weighted-kernel-naive-bayes-with-lasso-elimination,fuhang_weighted-kernel-naive-bayes-with-lasso-elimination_m14
['And the constrant is that **every weights wi (201>=i>=0) should be greater or equal to 0.**   \n'],15,fuhang,weighted-kernel-naive-bayes-with-lasso-elimination,fuhang_weighted-kernel-naive-bayes-with-lasso-elimination_m15
"['Actually, the ""naive bayes"" is just a special case of ""weighted naive bayes"" **when every weights wi are initlized to 1**. --That insire me to initlize every weights wi to 1 (see the code tf.ones), and let the model update the weights. And experiments show that **initilizations are so vital** that it stablize the model training -- the model would become very hard to learn if we init the weights in random way.']",16,fuhang,weighted-kernel-naive-bayes-with-lasso-elimination,fuhang_weighted-kernel-naive-bayes-with-lasso-elimination_m16
"['## Finally, after we get the sum of weighted log logit probability, we now exp the sum and revocer the raw score!<img src=""https://latex.codecogs.com/gif.latex?logit\\_prob=exp(log\\_logit\\_prob)=exp(w_{0}\\cdot&space;log\\frac{p(y=1)}{p(y=0)}&plus;\\sum_{i=1}^{200}w_{0}\\cdot&space;log\\frac{p(x_{i}|y=1)}{p(x_{i}|y=0)})"" title=""logit\\_prob=exp(w_{0}\\cdot log\\frac{p(y=1)}{p(y=0)}+\\sum_{i=1}^{200}w_{0}\\cdot log\\frac{p(x_{i}|y=1)}{p(x_{i}|y=0)})"" />']",17,fuhang,weighted-kernel-naive-bayes-with-lasso-elimination,fuhang_weighted-kernel-naive-bayes-with-lasso-elimination_m17
"['The implement is as follows: We choose **TensorFlow** to do the gradient decent&back-prop to learn the weights wi. Also, in order to elimintate the useless features, we use **Lasso shrikage** method -- also known as L1 normalization. In order to make sure the all weights are greater or equal to zero, **a Rectified Linear Unit(ReLU) are applied to the weights** before its dot product with log of logit.']",18,fuhang,weighted-kernel-naive-bayes-with-lasso-elimination,fuhang_weighted-kernel-naive-bayes-with-lasso-elimination_m18
['**You might wonder why L1-norm produce sparsity? ** you cuold check [this kaggle discussion](https://www.kaggle.com/residentmario/l1-norms-versus-l2-norms) for further machine learning knowledge.'],19,fuhang,weighted-kernel-naive-bayes-with-lasso-elimination,fuhang_weighted-kernel-naive-bayes-with-lasso-elimination_m19
['## So the main formula are below:'],20,fuhang,weighted-kernel-naive-bayes-with-lasso-elimination,fuhang_weighted-kernel-naive-bayes-with-lasso-elimination_m20
"['<img src=""https://latex.codecogs.com/gif.latex?logit\\_prob=exp(ReLU(w_{0})\\cdot&space;log\\frac{p(y=1)}{p(y=0)}&plus;\\sum_{i=1}^{200}ReLU(w_{i})\\cdot&space;log\\frac{p(x_{i}|y=1)}{p(x_{i}|y=0)})"" title=""logit\\_prob=exp(ReLU(w_{0})\\cdot log\\frac{p(y=1)}{p(y=0)}+\\sum_{i=1}^{200}ReLU(w_{i})\\cdot log\\frac{p(x_{i}|y=1)}{p(x_{i}|y=0)})"" />']",21,fuhang,weighted-kernel-naive-bayes-with-lasso-elimination,fuhang_weighted-kernel-naive-bayes-with-lasso-elimination_m21
"['<img src=""https://latex.codecogs.com/gif.latex?L1\\_norm=\\lambda&space;\\cdot&space;\\sum_{i=0}^{200}\\left&space;|&space;w_{i}&space;\\right&space;|"" title=""L1\\_norm=\\lambda \\cdot \\sum_{i=0}^{200}\\left | w_{i} \\right |"" />']",22,fuhang,weighted-kernel-naive-bayes-with-lasso-elimination,fuhang_weighted-kernel-naive-bayes-with-lasso-elimination_m22
"['**We choose cross_entropy as target function, and add l1 norm to prevent overfitting as well as doing lasso shrinkage.**']",23,fuhang,weighted-kernel-naive-bayes-with-lasso-elimination,fuhang_weighted-kernel-naive-bayes-with-lasso-elimination_m23
"['<img src=""https://latex.codecogs.com/gif.latex?loss=cross\\_entropy(y\\_true,logit\\_prob)&space;&plus;L1\\_norm"" title=""loss=cross\\_entropy(y\\_true,logit\\_prob) +L1\\_norm"" />']",24,fuhang,weighted-kernel-naive-bayes-with-lasso-elimination,fuhang_weighted-kernel-naive-bayes-with-lasso-elimination_m24
"[""Let's start, Frist we log the train_KernelNB as input data.""]",25,fuhang,weighted-kernel-naive-bayes-with-lasso-elimination,fuhang_weighted-kernel-naive-bayes-with-lasso-elimination_m25
"['Just a notice, we can get the same AUC score of 0.908 by **doing sum** on the logified data before doing exp on it. We should have a clear understanding of what we have done: first do a log transformation, and **a sum in log transform means product in original form**.   \n', '**And the exp of sum actually reverse the transform, making it equal to product of origanl data**. That is to say:']",26,fuhang,weighted-kernel-naive-bayes-with-lasso-elimination,fuhang_weighted-kernel-naive-bayes-with-lasso-elimination_m26
"[""## **Let's implement what we have introduce above, and start eliminting the features!**""]",27,fuhang,weighted-kernel-naive-bayes-with-lasso-elimination,fuhang_weighted-kernel-naive-bayes-with-lasso-elimination_m27
"['You can see the **weights w are becoming sparser and sparser** as train epoch goes on, from all ones to many zeros. So we are making progress in eliminting uesless features.']",28,fuhang,weighted-kernel-naive-bayes-with-lasso-elimination,fuhang_weighted-kernel-naive-bayes-with-lasso-elimination_m28
"[' as you see, the **test AUC are really hard to decrease** . However it doesn\'t matter, because we  want our final model to remain to be simple naive bayes with weights fix to 1.0, and the learned coef is not the key. **We just want to eliminate those useless features** using lasso shrinkage. So we mask those features that are sparse or ""almost sparse(<0.01)"". And use the mask to reduce the size of columns.']",29,fuhang,weighted-kernel-naive-bayes-with-lasso-elimination,fuhang_weighted-kernel-naive-bayes-with-lasso-elimination_m29
"['As we can see, after training, 53 of 201 features are eliminated. That is a total of 25% of the original data!   \n', '**And If we increase the lambda\\_w of LASSO from 2e-5 to 3e-5~4e-5, we can best eliminate 50% of the features without lossing too many socres!!!!**']",30,fuhang,weighted-kernel-naive-bayes-with-lasso-elimination,fuhang_weighted-kernel-naive-bayes-with-lasso-elimination_m30
"[""Let's submit the result with only 148 features and see what score we got.  \n"", 'The **AUC on public LB is 0.893** when using only 148 variables, a** very slight drop** from 0.894 using 201 features.']",31,fuhang,weighted-kernel-naive-bayes-with-lasso-elimination,fuhang_weighted-kernel-naive-bayes-with-lasso-elimination_m31
"[""Now,Let's see the weights distribution and rank the most important features by their feature weights.""]",32,fuhang,weighted-kernel-naive-bayes-with-lasso-elimination,fuhang_weighted-kernel-naive-bayes-with-lasso-elimination_m32
"[""As we see, more than 50 features are eliminated when lambda\\_w=2e-5. That's why it called feature elimination using LASSO.""]",33,fuhang,weighted-kernel-naive-bayes-with-lasso-elimination,fuhang_weighted-kernel-naive-bayes-with-lasso-elimination_m33
"[""As we saw above, prior is still the most importance term because it have largest weights. Over 25% of features are eliminated, we can check their logit's distribution in this kernel(https://www.kaggle.com/cdeotte/modified-naive-bayes-santander-0-899).   \n"", '  \n', 'These ""unimportant"" features in our model, such as **var_153, really doesn\'t have any instructive information for our prediction**,its logit vary from maxium of 0.115 to the minumn of 0.975.It is a very small shake.    \n', '**However, the ""good feature"" in our model -- such as var_105, its logit vary from max of 0.19 to min of 0.08. That\'s a lot of info lying here.**\n']",34,fuhang,weighted-kernel-naive-bayes-with-lasso-elimination,fuhang_weighted-kernel-naive-bayes-with-lasso-elimination_m34
"['Just curious, let\'s check if the ""unimportant"" feature have any pattern/order:  ']",35,fuhang,weighted-kernel-naive-bayes-with-lasso-elimination,fuhang_weighted-kernel-naive-bayes-with-lasso-elimination_m35
"['No pattern for the elimintaed features.. So the columns **must have been shuffle**.  \n', '\n', 'Moreover, we can **compare** some important features in LightGBM (https://www.kaggle.com/jiweiliu/lgb-2-leaves-augment) and see if they have high score in Lasso Weighted Naive Bayes:']",36,fuhang,weighted-kernel-naive-bayes-with-lasso-elimination,fuhang_weighted-kernel-naive-bayes-with-lasso-elimination_m36
"[""The weights are **basically consistent** with LGB's importance score. Most LGB's important feature are also important features for Lasso Weighted Naive Bayes.   \n"", '\n', 'But they are not exactly the same(e.g. var_9), so **it might still be helpful to combine these two algorithms together** because they share some diversity.']",37,fuhang,weighted-kernel-naive-bayes-with-lasso-elimination,fuhang_weighted-kernel-naive-bayes-with-lasso-elimination_m37
"['## more\n', '\n']",38,fuhang,weighted-kernel-naive-bayes-with-lasso-elimination,fuhang_weighted-kernel-naive-bayes-with-lasso-elimination_m38
['We can rethink our weighted naive bayes model and interprete it in this way:\n'],39,fuhang,weighted-kernel-naive-bayes-with-lasso-elimination,fuhang_weighted-kernel-naive-bayes-with-lasso-elimination_m39
"['<img src=""https://latex.codecogs.com/gif.latex?w_{0}\\cdot&space;log\\frac{p(y=1)}{p(y=0)}&plus;\\sum_{i=1}^{200}w_{i}\\cdot&space;log\\frac{p(x_{i}|y=1)}{p(x_{i}|y=0)}&space;=log[\\frac{p(y=1)}{p(y=0)}]^{w0}&plus;\\sum_{i=1}^{200}log[\\frac{p(x_{i}|y=1)}{p(x_{i}|y=0)}]^{w_{i}}"" title=""w_{0}\\cdot log\\frac{p(y=1)}{p(y=0)}+\\sum_{i=1}^{200}w_{i}\\cdot log\\frac{p(x_{i}|y=1)}{p(x_{i}|y=0)} =log[\\frac{p(y=1)}{p(y=0)}]^{w0}+\\sum_{i=1}^{200}log[\\frac{p(x_{i}|y=1)}{p(x_{i}|y=0)}]^{w_{i}}"" />']",40,fuhang,weighted-kernel-naive-bayes-with-lasso-elimination,fuhang_weighted-kernel-naive-bayes-with-lasso-elimination_m40
"['That provide us **a different view about how the weights works** -- if a sparse weight wi=0, then wi=0 make the logit^wi=logit^0 =1, and log(1)=0, making this logit terms equal to zero. So the our final result(as a linear combination) would not be influence by this features any more since adding zero term means adding nothing.']",41,fuhang,weighted-kernel-naive-bayes-with-lasso-elimination,fuhang_weighted-kernel-naive-bayes-with-lasso-elimination_m41
"['## Further  Discussion  \n', ""Is weighted NB model perfect? I don't think so, **althought it is good that our model eliminated useless features,but it still need to be digging why weighted features are not gaining AUC improvment. In theory weighted naive bayes should learn more than naive bayes. Maybe it is an overfitting problem or data noise problem.    **   ""]",42,fuhang,weighted-kernel-naive-bayes-with-lasso-elimination,fuhang_weighted-kernel-naive-bayes-with-lasso-elimination_m42
"['Futher more, weighted on every column a constant might not be as good as** weighting every columns differently by the attribute of this column**.  That model is called ** Attribute weighted Naive Bayesian**, and there have been many research done in that field for recent years. If you are interested you could check [this paper](https://ieeexplore.ieee.org/document/5593445/) as start.']",43,fuhang,weighted-kernel-naive-bayes-with-lasso-elimination,fuhang_weighted-kernel-naive-bayes-with-lasso-elimination_m43
"['**And,there are more interesting things to try using naive bayes model, such as using LGB to replace kernel density estimation to estimate p(xi|y) in this kernel: https://www.kaggle.com/b5strbal/lightgbm-naive-bayes-santander-0-900 . **']",44,fuhang,weighted-kernel-naive-bayes-with-lasso-elimination,fuhang_weighted-kernel-naive-bayes-with-lasso-elimination_m44
"['# Conclusion:\n', 'In this Kernel, we first explain Kernel Naive Bayes using Kernel Density Estimation(KDE), and illustrate the merits of Kernel NB over Gaussian NB.  \n', 'Secondly, we use log transformation that transform Naive Bayes\'s product into a sum of 201 logit terms, then **we strengthen the ""simple sum of logit term"" by giving every term a weight wi(w>=0),making it a standard weighted linear regression problem**. So we use gradient decent to learn these weights. For the purpose of eliminating useless feautures, **Lasso(L1-norm)** are used in the linear model. Also, **ReLU** are applied on the weights to make sure that weights either positive(>0) or sparse(=0).   \n', '\n', 'Our Lasso Weighted Naive Bayes model use back-prop&gradient decent to shrikage the weights. And the experiment shows that our model is **expert** in eliminating these useless features by itself.  By tuning the lambda of L1-norm from 2e-5 to 4e-5, at last we are able to gain almost **same level of AUC using only 50% of the features, which actually indicate that around half of the features are not informative (I prefer saying ""almost useless"") for our prediction.  ** Further analysis show that the feature selection process could be possibly applied to  LGB model.\n', '\n', '-------\n']",45,fuhang,weighted-kernel-naive-bayes-with-lasso-elimination,fuhang_weighted-kernel-naive-bayes-with-lasso-elimination_m45
"['*I would be happy if my post help you understand NB or the data better.*   \n', 'Give me a **thumb up or follow** if this kernel is helpful to you. Appreciate that!']",46,fuhang,weighted-kernel-naive-bayes-with-lasso-elimination,fuhang_weighted-kernel-naive-bayes-with-lasso-elimination_m46
['## **0. Introduction**'],0,gunesevitan,santander-customer-transaction-eda-fe-lgb,gunesevitan_santander-customer-transaction-eda-fe-lgb_m0
['## **1. Exploratory Data Analysis**'],1,gunesevitan,santander-customer-transaction-eda-fe-lgb,gunesevitan_santander-customer-transaction-eda-fe-lgb_m1
"['### **1.1 Overview**\n', '* Both training set and test set have **200000** rows\n', '* Training set have **202** features and test set have **201** features\n', '* One extra feature in the training set is `target` feature, which is the class of a row\n', '* `target` feature is binary (**0** or **1**), **1 = transaction** and **0 = no transaction**\n', ""* `ID_code` feature is the unique id of the row and it doesn't have any effect on target\n"", '* The other features are anonymized and labeled from `var_0` to `var_199`\n', '* There are no missing values in both training set and test set because the dataset is already processed']",2,gunesevitan,santander-customer-transaction-eda-fe-lgb,gunesevitan_santander-customer-transaction-eda-fe-lgb_m2
"['### **1.2 Target Distribution**\n', '* **10.05%** (20098/200000) of the training set is **Class 1**\n', '* **89.95%** (179902/200000) of the training set is **Class 0**']",3,gunesevitan,santander-customer-transaction-eda-fe-lgb,gunesevitan_santander-customer-transaction-eda-fe-lgb_m3
"['### **1.3 Correlations**\n', 'Features from `var_0` to `var_199` have extremely low correlation between each other in both training set and test set. The lowest correlation between variables is **2.7e-8** and it is in the training set (between `var_191` and `var_75`). The highest correlation between variables is **0.00986** and it is in the test set (between `var_139` and `var_75`). `target` has slightly higher correlations with other features. The highest correlation between a feature and `target` is **0.08** (between `var_81` and `target`).']",4,gunesevitan,santander-customer-transaction-eda-fe-lgb,gunesevitan_santander-customer-transaction-eda-fe-lgb_m4
"['### **1.4 Unique Value Count**\n', 'The lowest unique value count belongs to `var_68` which has only **451** unique values in training set and **428** unique values in test set. **451** and **428** unique values in **200000** rows are too less that `var_68` could even be a categorical feature. The highest unique value count belongs to`var_45` which has **169968** unique values in the training set and **92058** unique values in the test set. Every feature in training set have higher unique value counts compared to features in test set.\n', '\n', 'The lowest unique value count difference is in the `var_68` feature (Training Set Unique Count **451**, Test Set Unique Count **428**). The highest unique value count difference is in the `var_45` feature (Training Set Unique Count **169968**, Test Set Unique Count **92058**). When the unique value count of a feature increases, the difference between training set unique value count and test set unique value count also increases. The explanation of this situation is probably the synthetic records in the test set. ']",5,gunesevitan,santander-customer-transaction-eda-fe-lgb,gunesevitan_santander-customer-transaction-eda-fe-lgb_m5
"['### **1.5 Target Distribution in Quartiles**\n', 'Class 1 `target` distribution in feature quartiles are quite similar for each feature. Most of the class 1 `target` rows are either in the **1st** quartile or in the **4th** quartile of the features because of the winsorization. Winsorization clips the extreme values, so they are grouped up in the spikes inside **1st** quartile and **4th** quartile.\n', '* **94** features have highest class 1 `target` percentage in **1st** quartile\n', '* **101** features have highest class 1 `target` percentage in **4th** quartile\n', '* Only **5** features have highest class 1 `target` percetange in **2nd** and **3rd** quartile, and those features are `var_17`, `var_30`, `var_100`, `var_101`, `var_105`\n', '\n', 'Maximum class 1 `target` percentage for **1st** quartile is **14.35%** (**85.65%** class 0), and for **4th** quartile is **13.43%** (**86.57%** class 0). Maximum class 1 `target` percentage for **2nd** quartile is **10.34%** (**89.66%** class 0), and for **3rd** quartile is **10.05%** (**89.95%** class 0 `target`). To conclude, values in **1st** and **4th** quartiles have higher chance (**3-4%**) to be class 1 than values in **2nd** and **3rd** quartile for 195 features.']",6,gunesevitan,santander-customer-transaction-eda-fe-lgb,gunesevitan_santander-customer-transaction-eda-fe-lgb_m6
"['### **1.6 Feature Distributions in Training and Test Set**\n', 'Training and test set distributions of features are not perfectly identical. There are bumps on the distribution peaks of test set because the unique value counts are lesser than training set. Distribution tails are smoother than peaks and spikes are present in both training and test set.']",7,gunesevitan,santander-customer-transaction-eda-fe-lgb,gunesevitan_santander-customer-transaction-eda-fe-lgb_m7
"['### **1.7 Target Distributions in Features**\n', 'Majority of the features have good split points and huge spikes. This explains why a simple LightGBM model can achieve 0.90 AUC. Distribution difference is bigger in tails because of winsorization.']",8,gunesevitan,santander-customer-transaction-eda-fe-lgb,gunesevitan_santander-customer-transaction-eda-fe-lgb_m8
"['### **1.8 Conclusion**\n', 'Data imbalance is very common in customer datasets like this. Oversampling **Class 1** or undersampling **Class 0** are suitable solutions for this dataset because of its large size. Since the dataset is big enough, resampling would not introduce underfitting.\n', '\n', 'Training set has more unique values than test set so some part of test set is most likely synthetic. Rows with more frequent values are less reliable because test set has bumps over distribution peaks. This is also related to synthetic data in test set.\n', '\n', 'Features are not correlated with each other or not dependent to each other. However, `target` feature has the highest correlation with `var_81` (**0.08**). This relationship can bu used to make other features more informative. If a feature is target encoded on `var_81`, it could give information about `target`.\n', '\n', 'Values in **1st** and **4th** quartiles have higher chance to be **Class 1** than values in **2nd** and **3rd** quartile for almost every feature because of winsorization.']",9,gunesevitan,santander-customer-transaction-eda-fe-lgb,gunesevitan_santander-customer-transaction-eda-fe-lgb_m9
['## **2. Feature Engineering and Data Augmentation**'],10,gunesevitan,santander-customer-transaction-eda-fe-lgb,gunesevitan_santander-customer-transaction-eda-fe-lgb_m10
"['### **2.1 Separating Real/Synthetic Test Data and Magic Features**\n', 'Using unique value count in a row to identify synthetic samples. If a row has at least one unique value in a feature, then it is real, otherwise it is synthetic. This technique is shared by [YaG320](https://www.kaggle.com/yag320) in this kernel [List of Fake Samples and Public/Private LB split](https://www.kaggle.com/yag320/list-of-fake-samples-and-public-private-lb-split) and it successfuly identifies synthetic samples in entire test set. This way the unusual bumps on the distribution peaks of test set features are captured. The magic features are extracted from the combination of training set and real samples in the test set. ']",11,gunesevitan,santander-customer-transaction-eda-fe-lgb,gunesevitan_santander-customer-transaction-eda-fe-lgb_m11
"['### **2.2 Data Augmentation**\n', 'Oversampling the data increases CV and LB score significantly since the data is imbalanced. This oversampling technique is shared by [Jiwei Liu](https://www.kaggle.com/jiweiliu) in this kernel [LGB 2 leaves + augment](https://www.kaggle.com/jiweiliu/lgb-2-leaves-augment).']",12,gunesevitan,santander-customer-transaction-eda-fe-lgb,gunesevitan_santander-customer-transaction-eda-fe-lgb_m12
"['### **2.3 Quartile Rank (Not Used)**\n', ""This code ranks every value by their quartile. Ranking is done according to the features' **Class 1** distribution percentage in a quartile. In order to do that, every features' quartiles are sorted by **Class 1** percentage. After that, the ranks **(4, 3, 2, 1)** are mapped to the sorted quartiles. This way, the quartile with the highest **Class 1** distribution in a feature gets the highest rank. After every value in a row are ranked,the ranks are summed and scaled. This way the mean rank of a row is calculated. The problems with this feature are:\n"", '* The distributions are already captured by decision trees, so this feature is not very useful in LightGBM\n', '* If this feature is computed outside the folds, it leaks data']",13,gunesevitan,santander-customer-transaction-eda-fe-lgb,gunesevitan_santander-customer-transaction-eda-fe-lgb_m13
"['### **2.4 Target Encoding (Not Used)**\n', 'This function is for averaging the target value by feature. It computes the number of values and mean of each group. After that, the smooth mean is computed and replaced with the feature. Target encoding should be used in the folds otherwise it leaks data.']",14,gunesevitan,santander-customer-transaction-eda-fe-lgb,gunesevitan_santander-customer-transaction-eda-fe-lgb_m14
"['### **2.5 KMeansFeaturizer (Not Used)**\n', '`KMeansFeaturizer` is a pipeline of scikit-learn `KMeans` and `OneHotEncoder`. First, the records are grouped into **k** groups by `KMeans` with or without `target`. A return object of an $m * n$ matrix is $m * k$ group matrix which can be added to the previous matrix as features. This can be used to add likelihood features.\n', '* In order to make these features reliable, `KMeans` should be initialized with different seeds with many times and then blended\n', ""* The information gain from this approach doesn't worth it because it adds lot of new features to the dataset and takes too much time""]",15,gunesevitan,santander-customer-transaction-eda-fe-lgb,gunesevitan_santander-customer-transaction-eda-fe-lgb_m15
"['### **2.6 Feature Transformation (Not Used)**\n', ""This function is for simulating feature transformations. The transformation objective is to increase information gain by decreasing the overlapping area in the target distribution. By decreasing the overlapping area, LightGBM decision trees are able to make better splits. A transformed feature can be added to the data set as a new feature or it can replace the old one depending on the model's performance. A new feature can also be combinations of transformations and interactions between other features.""]",16,gunesevitan,santander-customer-transaction-eda-fe-lgb,gunesevitan_santander-customer-transaction-eda-fe-lgb_m16
['## **3. Model**'],17,gunesevitan,santander-customer-transaction-eda-fe-lgb,gunesevitan_santander-customer-transaction-eda-fe-lgb_m17
['### **3.1 LightGBM**'],18,gunesevitan,santander-customer-transaction-eda-fe-lgb,gunesevitan_santander-customer-transaction-eda-fe-lgb_m18
['### **3.2 ROC-AUC Score**'],19,gunesevitan,santander-customer-transaction-eda-fe-lgb,gunesevitan_santander-customer-transaction-eda-fe-lgb_m19
['### **3.3 Feature Importance**'],20,gunesevitan,santander-customer-transaction-eda-fe-lgb,gunesevitan_santander-customer-transaction-eda-fe-lgb_m20
['### **3.4 Submission**'],21,gunesevitan,santander-customer-transaction-eda-fe-lgb,gunesevitan_santander-customer-transaction-eda-fe-lgb_m21
['## bining'],0,hatunina,no-training-challenge,hatunina_no-training-challenge_m0
"['## smooth target encoding\n', 'reference: https://maxhalford.github.io/blog/target-encoding-done-the-right-way/']",1,hatunina,no-training-challenge,hatunina_no-training-challenge_m1
['## no training'],2,hatunina,no-training-challenge,hatunina_no-training-challenge_m2
"['# Deep Learning + LGBM + Weighted Combination\n', '\n', 'This kernel will always be running with different parameters and approaches until before the competition deadline.\n', '\n', 'Feel free to upvote,fork and test the presented models with different training options, to see if a better score with the following models is possible.\n', '\n', 'If forked Please try the different combinations:\n', '- Only Feature Engineering ( omitting some features maybe)\n', '- Only Augmented\n', '- Augmented + Feature Engineering (Augment before or after FE)\n', '- Augmented + Feature Engineering + folds\n', '- Augmented + Feature Engineering + full\n', '- Combination of different prediction weights\n', '- etc..\n', '\n', '\n', ""**Don't forget that with each combination you might need different hyper-parameters for the models**\n"", '\n', '\n', 'You can also check here for weighted CV approach that will make a minor better prediction that you might need in the competition:\n', 'https://www.kaggle.com/hjd810/introducing-weighted-cross-validation\n', '\n', 'Enjoy ! \n', '\n', 'Any comments are appreciated (added motivation <3)\n', '\n', '1. [Training Options](#options)\n', '2. [Sampling](#sample)\n', '3. [Feature Engineering](#fe)\n', '4. [Dim Reduction](#dim)\n', '5. [LGBM Model](#lgbm)\n', '6. [Keras Model](#keras)\n', '7. [Combination Vis](#vis)\n', '8. [Submission](#sub)']",0,hjd810,keras-lgbm-aug-feature-eng-sampling-prediction,hjd810_keras-lgbm-aug-feature-eng-sampling-prediction_m0
"['## Running Models Options\n', ""<a id='options'></a>\n"", 'The options here helps you check the different combinations for training and check which fits best.']",1,hjd810,keras-lgbm-aug-feature-eng-sampling-prediction,hjd810_keras-lgbm-aug-feature-eng-sampling-prediction_m1
['As you can see we are dealing with an unbalanced targets (10% vs 90%)'],2,hjd810,keras-lgbm-aug-feature-eng-sampling-prediction,hjd810_keras-lgbm-aug-feature-eng-sampling-prediction_m2
"['## Sampling from the full dataset (more work on this later)\n', ""<a id='sample'></a>""]",3,hjd810,keras-lgbm-aug-feature-eng-sampling-prediction,hjd810_keras-lgbm-aug-feature-eng-sampling-prediction_m3
"['## Simple Feature Engineering and Pre-processing\n', '* sum\n', '* min\n', '* max\n', '* mean\n', '* std\n', '* skew\n', '* kurt\n', '* med\n', '* Moving Average\n', '* percentiles\n', '* Augmentation\n', '* Log transformation\n', '* normalization\n', ""<a id='fe'></a>""]",4,hjd810,keras-lgbm-aug-feature-eng-sampling-prediction,hjd810_keras-lgbm-aug-feature-eng-sampling-prediction_m4
"['# Dimensionality Reduction\n', ""<a id='dim'></a>""]",5,hjd810,keras-lgbm-aug-feature-eng-sampling-prediction,hjd810_keras-lgbm-aug-feature-eng-sampling-prediction_m5
['We will Reduce our dimensions to n number of components and test whether this approach can also help in providing better results'],6,hjd810,keras-lgbm-aug-feature-eng-sampling-prediction,hjd810_keras-lgbm-aug-feature-eng-sampling-prediction_m6
"['# 1. LGBM Model\n', ""<a id='lgbm'></a>""]",7,hjd810,keras-lgbm-aug-feature-eng-sampling-prediction,hjd810_keras-lgbm-aug-feature-eng-sampling-prediction_m7
"['\n', '### Non-CV LGBM Approach']",8,hjd810,keras-lgbm-aug-feature-eng-sampling-prediction,hjd810_keras-lgbm-aug-feature-eng-sampling-prediction_m8
"['As we can see , many of the engineered features are present within the top 60 important features']",9,hjd810,keras-lgbm-aug-feature-eng-sampling-prediction,hjd810_keras-lgbm-aug-feature-eng-sampling-prediction_m9
"['# 2.Keras NN Model\n', ""<a id='keras'></a>""]",10,hjd810,keras-lgbm-aug-feature-eng-sampling-prediction,hjd810_keras-lgbm-aug-feature-eng-sampling-prediction_m10
"['## Combination Vis\n', ""<a id='vis'></a>""]",11,hjd810,keras-lgbm-aug-feature-eng-sampling-prediction,hjd810_keras-lgbm-aug-feature-eng-sampling-prediction_m11
"['**DistPlot Analysis:** \n', '\n', 'We can see from the plots how the predictions for the validation sets, and the final target test set follows closely a similar distribution.\n', 'Which tells us that the test set can be a resemblance of validation sets we are using. \n', 'Then we can proceed with improving our scores in the validation set knowing that there is a high chance they will also improve in the test set.\n', '\n', '**Combination Analysis**\n', '\n', 'We can also see that the combination of both is adding some noise to the prediction, which in some cases can prove helpful when each model\n', 'was able to predict with some features better than the others\n', '\n', 'more testing is going on here to see how effecient a combination model can get.']",12,hjd810,keras-lgbm-aug-feature-eng-sampling-prediction,hjd810_keras-lgbm-aug-feature-eng-sampling-prediction_m12
"['# Submissions\n', ""<a id='sub'></a>""]",13,hjd810,keras-lgbm-aug-feature-eng-sampling-prediction,hjd810_keras-lgbm-aug-feature-eng-sampling-prediction_m13
"['A Faster way to go up again and check what caused the results.\n', '[Training Options](#options)']",14,hjd810,keras-lgbm-aug-feature-eng-sampling-prediction,hjd810_keras-lgbm-aug-feature-eng-sampling-prediction_m14
['# Make NN and train it'],0,iggisv9t,autoencoder-for-data-structure-visualization,iggisv9t_autoencoder-for-data-structure-visualization_m0
"['# Draw result\n', '## Density']",1,iggisv9t,autoencoder-for-data-structure-visualization,iggisv9t_autoencoder-for-data-structure-visualization_m1
"['It seems like here is small number of original variables, that combines into this dense rectangles. Wide empty lines can be interpreted like suffisient difference in some of original variable values, maybe categorical features.']",2,iggisv9t,autoencoder-for-data-structure-visualization,iggisv9t_autoencoder-for-data-structure-visualization_m2
['## Scatter plot colored by target'],3,iggisv9t,autoencoder-for-data-structure-visualization,iggisv9t_autoencoder-for-data-structure-visualization_m3
['Hmmm...'],4,iggisv9t,autoencoder-for-data-structure-visualization,iggisv9t_autoencoder-for-data-structure-visualization_m4
"['High all,\n', '\n', 'I figured it would be interesting to compare the top scoring public kernels.\n', '\n', 'The analysis is also available(and may be updated) [here](https://ui.neptune.ml/jakub-czakon/santander/wiki/1-public_kernel_comparison).\n', '\n', '# Comparison Setup\n', '\n', '## Kernels considered:\n', '- https://www.kaggle.com/mhviraf/santander-compact-solution-14-lines-will-do\n', '- https://www.kaggle.com/kamalchhirang/simple-lightgbm-with-good-parameters\n', '- https://www.kaggle.com/sandeepkumar121995/magic-parameters\n', '- https://www.kaggle.com/jesucristo/30-lines-starter-solution-fast\n', '- https://www.kaggle.com/brandenkmurray/randomly-shuffled-data-also-works\n', '- https://www.kaggle.com/lavanyadml/santander-ls\n', '- https://www.kaggle.com/jesucristo/santander-magic-lgb\n', '- https://www.kaggle.com/gpreda/santander-fast-compact-solution\n', '- https://www.kaggle.com/jesucristo/40-lines-starter-solution-fast DELETED\n', '\n', ""I hope I didn't forget to upvote/fork any of those kernels. Good job people!\n"", '\n', '## Validation:\n', 'I adjusted all of them so that they would have the same validation schema:\n', '\n', '    N_SPLITS = 15\n', '    SEED = 2319   \n', '    folds = StratifiedKFold(n_splits=N_SPLITS, shuffle=False, random_state=SEED)\n', '    \n', '## Tracking\n', 'I logged all the experiment information to [Neptune](http://bit.ly/2FndEZO) to be able to easily compare it later:\n', '  - hyperparameters\n', '  - lightgbm training curves\n', '  - roc_auc metric on out of fold predictions\n', '  - confusion matrix\n', '  - ROC AUC curve\n', '  - prediction distribution plot\n', ' \n', '**Note** \n', ""You don't have to know how to track stuff with Neptune to follow this post. \n"", '\n', 'If you are interested, [here is an example neptune kernel](https://www.kaggle.com/jakubczakon/example-lightgbm-neptune-tracking).  \n', '[Neptune](http://bit.ly/2FndEZO) is free for non-organizations and you can easily use it from inside kaggle kernels (or any other place for that matter) to track your experiments. \n', '  \n', '# Results\n', ""Ok, let's explore the results.\n"", '\n', '## Scores\n', 'First, lets have a look at the validation results of those models:\n', '\n', '[Results dashboard](https://ui.neptune.ml/jakub-czakon/santander/experiments?filterId=288495a5-c965-4896-815c-7474fd95234f)\n', '\n', '![image](https://gist.githubusercontent.com/jakubczakon/f754769a39ea6b8fa9728ede49b9165c/raw/2507ca2f4ff706b4a358decf0ba714ead7010a0b/comparison8.png)\n', '\n', 'All the top kernels, which is no surprise, perform quite well and get us `Local CV &gt; 0.9`.\n', 'The results vary from `0.900464` to `0.901075`, which may not be a lot in a large picture of things, but for this particular competition could mean a difference of 1000+ places.\n', 'It seems that the `Random Shuffled Data Also Works` not only ""also works"" but works the best, which was surprising to me. \n', '\n', '## Hyperparameters\n', '\n', 'Looking at the parameters in the table above it seems that every kernel apart from the best one used `num_leaves=13`. Interesting.\n', 'To get more insights I decided to use the `plot_evaluations` function from the `skopt.plots` package: \n', '\n', '[Exploration experiment](https://ui.neptune.ml/jakub-czakon/santander/e/SAN1-170/channels)\n', '\n', '![image](https://gist.githubusercontent.com/jakubczakon/f754769a39ea6b8fa9728ede49b9165c/raw/2507ca2f4ff706b4a358decf0ba714ead7010a0b/comparison2.png)\n', '\n', ""It's a pretty useful tool, that lets you see what were the hyperparameter values that were explored, and which values were explored the most. The red dot shows the best run. \n"", '\n', 'Sometimes, you can see clear feature interactions (in the hyperparameter space) or the over/underexplored areas in the hyperparameter space. \n', 'In this particular case my conclusions are:\n', '- explore other `num_leaves` values between `3` and `13`\n', '- higher bagging fraction perform better\n', '- there seems to be a sweet spot when it comes to `early_stopping_rounds=3000`\n', '\n', '## Learning curves\n', 'Since I logged all the metrics during training with the `neptune_monitor` callback we can try and get some insights here. If you want to see how to create a custom callback for your tracking tool/setup [go to this example kernel](https://www.kaggle.com/jakubczakon/example-lightgbm-neptune-tracking).\n', ""Let's start with the comparison of the learning curves for all the experiments:\n"", '\n', '[Learning curves comparison](https://ui.neptune.ml/jakub-czakon/santander/compare?shortId=%5B%22SAN1-84%22%2C%22SAN1-59%22%2C%22SAN1-124%22%2C%22SAN1-142%22%2C%22SAN1-56%22%2C%22SAN1-140%22%2C%22SAN1-141%22%2C%22SAN1-58%22%2C%22SAN1-57%22%5D)\n', '\n', '![image](https://gist.githubusercontent.com/jakubczakon/f754769a39ea6b8fa9728ede49b9165c/raw/2507ca2f4ff706b4a358decf0ba714ead7010a0b/comparison9.png)\n', '\n', 'Apart from the `Randomly Shuffled Data Also Works` all those curves are quite similar. We can see that the ""shuffled"" gets to 0.9 slower but is overfitting considerably less. That is the result of the `num_leaves=3` but maybe shuffling features somehow plays a role here as well. It is probably worth exploring.\n', '\n', ""Let's now take a closer look at the learning curves of the top two kernels `Randomly Shuffled Data Also Works` and `Magic Parameters`.\n"", '\n', '**`Randomly Shuffled Data Also Works`**\n', '\n', '[Experiment](https://ui.neptune.ml/jakub-czakon/santander/e/SAN1-84/charts)\n', '\n', '![image](https://gist.githubusercontent.com/jakubczakon/f754769a39ea6b8fa9728ede49b9165c/raw/2507ca2f4ff706b4a358decf0ba714ead7010a0b/comparison12_random_shuffle.png)\n', '\n', 'We can see that there is some overfitting but not a lot. Training curves are almost identical for each fold, but when we look at the validation curves there is quite a lot of difference:\n', '\n', '![image](https://gist.githubusercontent.com/jakubczakon/f754769a39ea6b8fa9728ede49b9165c/raw/2507ca2f4ff706b4a358decf0ba714ead7010a0b/comparison13_random_shuffle.png)\n', '\n', 'The worst result was just 0.889 for fold 4 while the best one got 0.9111 on fold 13. \n', '\n', '**`Magic Parameters`**\n', 'Looking at this kernel:\n', '\n', '[Experiment](https://ui.neptune.ml/jakub-czakon/santander/e/SAN1-59/charts)\n', '\n', '![image](https://gist.githubusercontent.com/jakubczakon/f754769a39ea6b8fa9728ede49b9165c/raw/2507ca2f4ff706b4a358decf0ba714ead7010a0b/comparison10_magic_lgb.png)\n', '\n', 'Overfitting is significantly more visible here.\n', '\n', '![image](https://gist.githubusercontent.com/jakubczakon/f754769a39ea6b8fa9728ede49b9165c/raw/2507ca2f4ff706b4a358decf0ba714ead7010a0b/comparison11_magic_lgb.png)\n', '\n', 'The difference between best and worst folds is pretty much the same `fold 4 0.889` vs `fold 13 0.910`.\n', '\n', 'My conclusions are:\n', ' - `num_leaves=3` gives better results and controls overfitting, maybe other regularization params like l1/l2 could push the score up a bit more.\n', ' - the difference between best and worst folds is large.  Maybe the investigation of this problem can reveal insights about the underlying structure of the problem.\n', '\n', '## Prediction correlations\n', ""Let's look at the prediction correlations for out of fold train predictions and averaged test predictions respectively:\n"", '\n', '[Exploration experiment](https://ui.neptune.ml/jakub-czakon/santander/e/SAN1-170/channels)\n', '\n', '![image](https://gist.githubusercontent.com/jakubczakon/f754769a39ea6b8fa9728ede49b9165c/raw/2507ca2f4ff706b4a358decf0ba714ead7010a0b/comparison3.png)\n', '![image](https://gist.githubusercontent.com/jakubczakon/f754769a39ea6b8fa9728ede49b9165c/raw/2507ca2f4ff706b4a358decf0ba714ead7010a0b/comparison4.png)\n', '\n', 'Both for the train oof and test predictions, the correlations between public kernels are extremely high.\n', 'It seems that it will be very difficult to squeeze extra juice from blending/stacking those models.\n', '\n', 'My conclusions are:\n', '- we may need to add diversity to gain anything by blending\n', '\n', '## Predictions exploration\n', '\n', ""Let's start by looking at the standard model diagnostics for the best model:\n"", '\n', '[Best experiment](https://ui.neptune.ml/jakub-czakon/santander/e/SAN1-84/channels)\n', '![image](https://gist.githubusercontent.com/jakubczakon/f754769a39ea6b8fa9728ede49b9165c/raw/4d52325ae3ac0f2aeabf406a6c62a650b7f1b0c3/comparsion15.png)\n', '\n', 'Ok, it looks like it has trouble with giving a very low score to positive examples. \n', '\n', ""Let's take a look at the predictions that public kernels produce. \n"", '\n', 'I will begin by looking at the error distribution plot. I calculated it simply by substructing `prediction` from `target`.\n', '\n', '[Exploration experiment](https://ui.neptune.ml/jakub-czakon/santander/e/SAN1-170/channels)\n', '\n', '![image](https://gist.githubusercontent.com/jakubczakon/f754769a39ea6b8fa9728ede49b9165c/raw/2507ca2f4ff706b4a358decf0ba714ead7010a0b/comparison1.png)\n', '\n', 'Ok, it seems that our model is pretty good when it comes to predicting negative cases but has a lot of problems when it comes to predicting positive cases. Not only that, but it is quite confidently wrong, giving very low scores for some positive cases. \n', '\n', 'Now, I would like to take a look at the predictions themselves.\n', '[Exploration experiment](https://ui.neptune.ml/jakub-czakon/santander/e/SAN1-170/channels)\n', '\n', '![image](https://gist.githubusercontent.com/jakubczakon/f754769a39ea6b8fa9728ede49b9165c/raw/2507ca2f4ff706b4a358decf0ba714ead7010a0b/comparsion7.png)\n', '\n', 'And if we split it into positive and negative examples respectively:\n', '[Exploration experiment](https://ui.neptune.ml/jakub-czakon/santander/e/SAN1It is not super important to know how-170/channels)\n', '\n', '![image](https://gist.githubusercontent.com/jakubczakon/f754769a39ea6b8fa9728ede49b9165c/raw/2507ca2f4ff706b4a358decf0ba714ead7010a0b/comparison6.png)\n', '\n', '[Exploration experiment](https://ui.neptune.ml/jakub-czakon/santander/e/SAN1-170/channels)\n', '\n', '![image](https://gist.githubusercontent.com/jakubczakon/f754769a39ea6b8fa9728ede49b9165c/raw/2507ca2f4ff706b4a358decf0ba714ead7010a0b/comparison5.png)\n', '\n', 'Even though the prediction distributions at the first sight look very similar there could still be benefits to blending.\n', 'Especially those positive predictions differ from model to model quite a bit.\n', '\n', 'My conclusions are:\n', '- maybe there is some LB sauce that can be squeezed from blending after all\n', '- we should investigate how to make our model less sure when it is wrong about the positive cases (potentially add some models that do)\n', '\n', '# Final thoughts:\n', '- There is probably room to explore on the hyperparameter front\n', '- Difference between worst and best fold results is huge. Exploring it could bring insights\n', '- There is probably to blending/stacking public kernels but adding diversity may be important.\n', '\n', 'What do you think?\n', '\n', '\n', '# Edits:\n', '\n', 'Thank you @tilii7 for cleaning my thought.\n', 'Added: ""Not only that, it is quite confidently wrong (giving very low scores) for some positive cases.""  \n', 'Dropped: ""In simple words, some positive cases are given very low predictions by our models. ""']",0,jakubczakon,top-public-kernels-comparison,jakubczakon_top-public-kernels-comparison_m0
"['<h1><center><font size=""6"">Santander Customer Transaction Prediction</font></center></h1>\n', '<h1><center><font size=""5"">Can you identify who will make a transaction?</font></center></h1>\n', '\n', '<img src=""https://upload.wikimedia.org/wikipedia/commons/thumb/4/4a/Another_new_Santander_bank_-_geograph.org.uk_-_1710962.jpg/640px-Another_new_Santander_bank_-_geograph.org.uk_-_1710962.jpg"" width=""500""></img>\n', '\n', '<br>\n', '<b>\n', '    \n', 'Our data science team is continually challenging our machine learning algorithms, working with the global data science community to make sure we can more accurately identify new ways to solve our most common challenge, binary classification problems such as: is a customer satisfied? Will a customer buy this product? Can a customer pay this loan?\n', '\n', 'In this challenge, we invite Kagglers to help us identify which customers will make a specific transaction in the future, irrespective of the amount of money transacted. The data provided for this competition has the same structure as the real data we have available to solve this problem.\n', '\n', 'The data is anonimyzed, each row containing 200 numerical values identified just with a number.</b>\n', '     \n']",0,jesucristo,santander-magic-lgb-0-901,jesucristo_santander-magic-lgb-0-901_m0
['<a id=1><pre><b>Load Packages</b></pre></a>'],1,jesucristo,santander-magic-lgb-0-901,jesucristo_santander-magic-lgb-0-901_m1
['<a id=1><pre><b>Import the Data</b></pre></a>'],2,jesucristo,santander-magic-lgb-0-901,jesucristo_santander-magic-lgb-0-901_m2
"['**Target = 0 or Target = 1, binary classification**']",3,jesucristo,santander-magic-lgb-0-901,jesucristo_santander-magic-lgb-0-901_m3
"[""Let's see basic stats on the 2 different groups.""]",4,jesucristo,santander-magic-lgb-0-901,jesucristo_santander-magic-lgb-0-901_m4
['**Missing data**'],5,jesucristo,santander-magic-lgb-0-901,jesucristo_santander-magic-lgb-0-901_m5
['**There is no missing data**'],6,jesucristo,santander-magic-lgb-0-901,jesucristo_santander-magic-lgb-0-901_m6
['### Check for Class Imbalance'],7,jesucristo,santander-magic-lgb-0-901,jesucristo_santander-magic-lgb-0-901_m7
['<a id=1><pre><b>Classification augment</b></pre></a>'],8,jesucristo,santander-magic-lgb-0-901,jesucristo_santander-magic-lgb-0-901_m8
['# Build the Light GBM Model'],9,jesucristo,santander-magic-lgb-0-901,jesucristo_santander-magic-lgb-0-901_m9
['<a id=1><pre><b>Parameters</b></pre></a>'],10,jesucristo,santander-magic-lgb-0-901,jesucristo_santander-magic-lgb-0-901_m10
['<a id=1><pre><b>Run LGBM model</b></pre></a>'],11,jesucristo,santander-magic-lgb-0-901,jesucristo_santander-magic-lgb-0-901_m11
['# Submission'],12,jesucristo,santander-magic-lgb-0-901,jesucristo_santander-magic-lgb-0-901_m12
"[""This kernel is inspired by [@Branden Murray](https://www.kaggle.com/brandenkmurray)'s thoughts about [Any explanation why shuffling augmentation works?](https://www.kaggle.com/c/santander-customer-transaction-prediction/discussion/84847). I quote his hypothesis:\n"", '\n', "">Well, one explanation could be that it's how they generated the dataset in the first place. For each feature they had a distribution for target==0 and a distribution for target==1 and they randomly sampled from each and then put it together.\n"", '\n', ""I'm going to generate positive(target==1) and negative(target==0) samples for each feature, then combine the 200 simulated features together, I can get a synthetic data. With this simulated data, I can test Brander Murray's hypothesis.  \n"", '\n', 'If his hypothesis is true, then we can calculate the probability of positive sample(i.e. P(target=1|features)) using traditional Probability theory.']",0,jiazhuang,a-proof-of-synthetic-data,jiazhuang_a-proof-of-synthetic-data_m0
['### Calculate the mean/sd of postive and negative samples for each feature'],1,jiazhuang,a-proof-of-synthetic-data,jiazhuang_a-proof-of-synthetic-data_m1
"[""### Synthetic data using normal distribution with train's mean/sd""]",2,jiazhuang,a-proof-of-synthetic-data,jiazhuang_a-proof-of-synthetic-data_m2
['### Test the synthetic data'],3,jiazhuang,a-proof-of-synthetic-data,jiazhuang_a-proof-of-synthetic-data_m3
"[""We can achieve 0.885 just using train data's mean and sd, this is not bad! Maybe the data is generated using this way!""]",4,jiazhuang,a-proof-of-synthetic-data,jiazhuang_a-proof-of-synthetic-data_m4
"['### Calculate probability use hypothetical test\n', '\n', ""If each feature is generated by sample positive samples and negtive samples, then we can use hypothetical test to distinguish them. The positive samples and negative samples of each feature are slightly different. Let's take `var_0` and `var_1` as an example. ""]",5,jiazhuang,a-proof-of-synthetic-data,jiazhuang_a-proof-of-synthetic-data_m5
"[""Let's use Z-test:\n"", '- Null hypothesis: a sample is negative(target == 0)\n', '- Alternative hypothesis: a sample is not negative(target == 1)\n', '\n', 'If we get a small pvalue(< 0.05), we reject the null hypothesis, i.e. the smaller the pvalue, the more likely a sample is positive.']",6,jiazhuang,a-proof-of-synthetic-data,jiazhuang_a-proof-of-synthetic-data_m6
"['Since we have 200 feats, we get 200 pvalue for each sample, we can multiply them together.']",7,jiazhuang,a-proof-of-synthetic-data,jiazhuang_a-proof-of-synthetic-data_m7
"[""The smaller the prob1, the more likely a sample is positive. let's see the performance.""]",8,jiazhuang,a-proof-of-synthetic-data,jiazhuang_a-proof-of-synthetic-data_m8
"['If we test whether a sample is positive, we can get another hypothetical test:\n', '- Null hypothesis: a sample is positive(target == 1)\n', '- Alternative hypothesis: a sample is not positive(target == 0)\n', '\n', 'If we get a small pvalue(< 0.05), we reject the null hypothesis, i.e. the bigger the pvalue, the more likely a sample is positive.']",9,jiazhuang,a-proof-of-synthetic-data,jiazhuang_a-proof-of-synthetic-data_m9
['Combine the two prob together:'],10,jiazhuang,a-proof-of-synthetic-data,jiazhuang_a-proof-of-synthetic-data_m10
"[""**We can get 0.874 just using Probability theory, It's quite good I think.**""]",11,jiazhuang,a-proof-of-synthetic-data,jiazhuang_a-proof-of-synthetic-data_m11
[' ### Use this mothed to predict test.csv'],12,jiazhuang,a-proof-of-synthetic-data,jiazhuang_a-proof-of-synthetic-data_m12
"['### Conclusion\n', '\n', ""Branden Murray's hypothesis **For each feature they had a distribution for target==0 and a distribution for target==1 and they randomly sampled from each and then put it together** is a wonderful explanation of shuffling also works and weak interaction between features.\n"", '\n', 'We can even use tranditional Probability theory to calculate the P(target==1) value to achive 0.874 local cv. But this model is still too naive, the feature is not normal distribution(I try normality test, none of the 200 features passed), and the positive samples and negative samples is not variance homogeneity(2/3 of the features failed variance homogeneity test).\n', '\n', 'Hope this kernal can help, thanks!']",13,jiazhuang,a-proof-of-synthetic-data,jiazhuang_a-proof-of-synthetic-data_m13
"[""In the kernel [Modified Naive Bayes scores 0.899 LB - Santander](https://www.kaggle.com/cdeotte/modified-naive-bayes-santander-0-899), [Chris Deotte](https://www.kaggle.com/cdeotte) has demonstrated that Naive Bayes can be a simple but powful method when there is little or no interaction between the features. I think it's a good time to study Naive Bayes Method.\n"", '\n', ""In this kernel, I'll try to introduce Naive Bayes method step by step.""]",0,jiazhuang,demonstrate-naive-bayes,jiazhuang_demonstrate-naive-bayes_m0
"[""### Bayes' theorem\n"", '\n', ""Bayes' theorem can be deduced by conditional probability:\n"", '\n', '$$\n', 'P(A|B) = \\frac {P(A \\bigcap B)}{P(B)}\n', '$$\n', '\n', '$$\n', 'P(B|A) = \\frac {P(A \\bigcap B)}{P(A)}\n', '$$\n', '\n', '$$\n', '\\Rightarrow P(A|B) = \\frac {P(B|A) \\cdot P(A)} {P(B)}\n', '$$']",1,jiazhuang,demonstrate-naive-bayes,jiazhuang_demonstrate-naive-bayes_m1
"['Suppose we have features $X \\in R^n$, target $y \\in \\{+1, -1\\}$, for a given $x_0$ our goal is to predict $y=+1$ or $-1$.\n', '\n', 'we can achive this by calculate\n', '\n', '$$P(y=+1 | X=x_0)$$ \n', 'and\n', '$$P(y=-1 |  X=x_0)$$\n', '\n', 'then choose the one with bigger probability.']",2,jiazhuang,demonstrate-naive-bayes,jiazhuang_demonstrate-naive-bayes_m2
"[""How can we calculate $P(y=+1|X=x_0)$ ? We can use Bayes's theorem:\n"", '$$\n', 'P(y=+1|X=x_0) = \\frac {P(X=x_0 | y=+1) \\cdot P(y=+1)} {P(X=x_0)}\n', '$$\n', '\n', ""Because $P(X=X_0)$ is the probability(or frequency) of a sample in test set, it's the same for every sample, so we can simply write the formular as:\n"", '\n', '$$\n', 'P(y=+1|X=x_0) = P(X=x_0 | y=+1) \\cdot P(y=+1)\n', '$$\n', '\n', '$P(y=+1/-1)$ is called priori probability and $P(X=x_0 | y=+1)$ is called conditional probability. Training process is to estimate this two kind of probability.\n']",3,jiazhuang,demonstrate-naive-bayes,jiazhuang_demonstrate-naive-bayes_m3
"['### Naive Bayes\n', '\n', 'If we have many features, then:\n', '$$\n', 'P(X=x_0 | y=+1) = P(X^{(1)}=x_0^{(1)}, X^{(2)}=x_0^{(2)}, ..., X^{(n)}=x_0^{(n)} | y=+1)\n', '$$\n', '\n', 'To simplify this problem, we assume the features as independent(this is the Naive means):\n', '$$\n', 'P(X=x_0 | y=+1) = P(X^{(1)}=x_0^{(1)} | y=+1) \\cdot P(X^{(2)}=x_0^{(2)} | y=+1) \\cdot, ..., \\cdot P(X^{(n)}=x_0^{(n)} | y=+1)\n', '$$']",4,jiazhuang,demonstrate-naive-bayes,jiazhuang_demonstrate-naive-bayes_m4
"['### Discrete variable\n', '\n', 'We will use a sample example to demostrate the training and predicting process of Naive Bayes for discrete variable.']",5,jiazhuang,demonstrate-naive-bayes,jiazhuang_demonstrate-naive-bayes_m5
"['Try to use data to training a Naive Bayes classifier, and classify the sample $X_0=(2, S)$.']",6,jiazhuang,demonstrate-naive-bayes,jiazhuang_demonstrate-naive-bayes_m6
['Calculate priori probability $P(y=1)$ and $P(y=-1)$'],7,jiazhuang,demonstrate-naive-bayes,jiazhuang_demonstrate-naive-bayes_m7
"['$$\n', 'P(y=1) = \\frac {9}{15}, P(y=-1) = \\frac {6}{15}\n', '$$']",8,jiazhuang,demonstrate-naive-bayes,jiazhuang_demonstrate-naive-bayes_m8
['Calculate conditional probability'],9,jiazhuang,demonstrate-naive-bayes,jiazhuang_demonstrate-naive-bayes_m9
"['$$\n', 'P(X1=1 | y=1) = \\frac{2}{9}, P(X1=2 | y=1) = \\frac{3}{9}, P(X1=3 | y=1) = \\frac{4}{9}\n', '$$']",10,jiazhuang,demonstrate-naive-bayes,jiazhuang_demonstrate-naive-bayes_m10
"['$$\n', 'P(X2=S | y=1) = \\frac{1}{9}, P(X2=M | y=1) = \\frac{4}{9}, P(X2=L | y=1) = \\frac{4}{9}\n', '$$']",11,jiazhuang,demonstrate-naive-bayes,jiazhuang_demonstrate-naive-bayes_m11
"['$$\n', 'P(X1=1 | y=-1) = \\frac{3}{6}, P(X1=2 | y=-1) = \\frac{2}{6}, P(X1=3 | y=-1) = \\frac{1}{6}\n', '$$']",12,jiazhuang,demonstrate-naive-bayes,jiazhuang_demonstrate-naive-bayes_m12
"['$$\n', 'P(X2=S | y=1) = \\frac{3}{6}, P(X2=M | y=1) = \\frac{2}{6}, P(X2=L | y=1) = \\frac{1}{6}\n', '$$']",13,jiazhuang,demonstrate-naive-bayes,jiazhuang_demonstrate-naive-bayes_m13
"['For the given sample $X_0=(2, S)$:\n', '$$\n', 'P(y=1 | X=X_0) = P(y=1) \\cdot P(X1=2|y=1) \\cdot P(X2=S|y=1) = \\frac{9}{15} \\cdot \\frac{3}{9} \\cdot \\frac{1}{9} = \\frac{1}{45}\n', '$$\n', '\n', '$$\n', 'P(y=-1 | X=X_0) = P(y=-1) \\cdot P(X1=2|y=-1) \\cdot P(X2=S|y=-1) = \\frac{6}{15} \\cdot \\frac{2}{6} \\cdot \\frac{3}{6} = \\frac{1}{15}\n', '$$\n', '\n', 'Because $P(y=-1|X=X_0) > P(y=1|X=X_0)$, so $y=-1$\n', '\n', 'This is Naive Bayes for discrete variable, pretty simple.']",14,jiazhuang,demonstrate-naive-bayes,jiazhuang_demonstrate-naive-bayes_m14
['### Gaussian Naive Bayes'],15,jiazhuang,demonstrate-naive-bayes,jiazhuang_demonstrate-naive-bayes_m15
"[""When dealing with continuous variable, how do you calculate the probabilty for a given value(e.g. x=1)? The probability should be zero. So we'd better calculate the probability of a interval(e.g. $1-\\Delta < x < 1+\\Delta$).""]",16,jiazhuang,demonstrate-naive-bayes,jiazhuang_demonstrate-naive-bayes_m16
"['If the interval is small enough(i.e. $\\Delta \\rightarrow 0$), the probability of a given value(e.g. x=1) can be represented by probability density(pdf) value. How can we know the probability function of a variable? The convenient way is to estimate using normal distribution. This is the **Gaussian Naive Bayes**.  ']",17,jiazhuang,demonstrate-naive-bayes,jiazhuang_demonstrate-naive-bayes_m17
"[""Let's apply Gaussian Naive Bayes to our Santander data.""]",18,jiazhuang,demonstrate-naive-bayes,jiazhuang_demonstrate-naive-bayes_m18
['Calculate mean/sd of train data for each each feature.'],19,jiazhuang,demonstrate-naive-bayes,jiazhuang_demonstrate-naive-bayes_m19
['Using normal distribution to estimate each feature'],20,jiazhuang,demonstrate-naive-bayes,jiazhuang_demonstrate-naive-bayes_m20
"['Gaussian Naive Bayes can give us 0.890 AUC, which is quite good!']",21,jiazhuang,demonstrate-naive-bayes,jiazhuang_demonstrate-naive-bayes_m21
['### Remove the Gaussian constrain'],22,jiazhuang,demonstrate-naive-bayes,jiazhuang_demonstrate-naive-bayes_m22
"[""Infact our data is not normal distributed, we can achive better score with Gaussian constran removed. let's take `var_0` as an example.""]",23,jiazhuang,demonstrate-naive-bayes,jiazhuang_demonstrate-naive-bayes_m23
"[""We can see that the data is very different from normal distribution, we need use more accurate probability density function to estimate, this can be done by kernel function estimation. Let's use `scipy.stats.kde.gaussian_kde` ""]",24,jiazhuang,demonstrate-naive-bayes,jiazhuang_demonstrate-naive-bayes_m24
"['Kernel funtion can fit the data better, which will give us better accuracy.']",25,jiazhuang,demonstrate-naive-bayes,jiazhuang_demonstrate-naive-bayes_m25
"[""It's too slow, we can speed up by binize the variable values.""]",26,jiazhuang,demonstrate-naive-bayes,jiazhuang_demonstrate-naive-bayes_m26
"[""Using more accurate kernel function, we can achieve 0.909 AUC(maybe a little overfit, since we fit train's data, but it's not too much). Let's use this model to predict the test data.""]",27,jiazhuang,demonstrate-naive-bayes,jiazhuang_demonstrate-naive-bayes_m27
"['### Conclusion\n', '\n', 'In this kernel we demonstrate how Naive bayes works, we build Gaussian Naive Bayes, which gives us 0.890 AUC. By remove Gaussian constrain and choosing more accurate kernel function, we can get better performance.\n', '\n', 'Holp this can help, thanks!']",28,jiazhuang,demonstrate-naive-bayes,jiazhuang_demonstrate-naive-bayes_m28
"['In this kernel, I implement vectorized PDF caculation (without for loop) to get their correlation matrix. This is helpful to study feature grouping.\n', 'credits to @sibmike https://www.kaggle.com/sibmike/are-vars-mixed-up-time-intervals']",0,jiweiliu,fast-pdf-calculation-with-correlation-matrix,jiweiliu_fast-pdf-calculation-with-correlation-matrix_m0
['**Functions**'],1,jiweiliu,fast-pdf-calculation-with-correlation-matrix,jiweiliu_fast-pdf-calculation-with-correlation-matrix_m1
['**load data & group vars**'],2,jiweiliu,fast-pdf-calculation-with-correlation-matrix,jiweiliu_fast-pdf-calculation-with-correlation-matrix_m2
"[""**We can group features using this correlation matrix. For example, var_0 and var_2's pdfs is 0.97+ correlated. We can confirm it using the figure below.**""]",3,jiweiliu,fast-pdf-calculation-with-correlation-matrix,jiweiliu_fast-pdf-calculation-with-correlation-matrix_m3
['**We can find the group of a var using the following functions.**'],4,jiweiliu,fast-pdf-calculation-with-correlation-matrix,jiweiliu_fast-pdf-calculation-with-correlation-matrix_m4
[],5,jiweiliu,fast-pdf-calculation-with-correlation-matrix,jiweiliu_fast-pdf-calculation-with-correlation-matrix_m5
['# Data'],0,konradb,lgb-parameter-tuning,konradb_lgb-parameter-tuning_m0
['# FE'],1,konradb,lgb-parameter-tuning,konradb_lgb-parameter-tuning_m1
['## Clustering'],2,konradb,lgb-parameter-tuning,konradb_lgb-parameter-tuning_m2
['## Summary statistics'],3,konradb,lgb-parameter-tuning,konradb_lgb-parameter-tuning_m3
['# Parameter tuning'],4,konradb,lgb-parameter-tuning,konradb_lgb-parameter-tuning_m4
"['This is my first attempt to write the notebook from scratch. I have been playing around with exisitng kernels for competition until now. Comments, suggestions, recommendations are all very welcomed. \n', '\n', 'I will be modeling using the following algorithms -\n', '* Logistic Regression\n', '* Decision Tree Classifier\n', '* Random Forest Classifier\n', '* Light Gradient Boosting Method\n', '\n', ""Let's start by importing necessary packages -""]",0,krithi07,logistic-to-lightgbm-for-beginners,krithi07_logistic-to-lightgbm-for-beginners_m0
['### Import Datasets'],1,krithi07,logistic-to-lightgbm-for-beginners,krithi07_logistic-to-lightgbm-for-beginners_m1
"['The dataset consists of an ID_code, 200 input variables (all numeric) and a binary target variable representing the transaction-happened. Since the entire dataset is masked, cannot do much of exploratory data analysis']",2,krithi07,logistic-to-lightgbm-for-beginners,krithi07_logistic-to-lightgbm-for-beginners_m2
['This is an unbalanced classification problem with only 10% records having target variable = 1. '],3,krithi07,logistic-to-lightgbm-for-beginners,krithi07_logistic-to-lightgbm-for-beginners_m3
"['## Modeling\n', ""I would be trying a few algorithms, starting from the most simple Logistic Regression, followed by Decision Tree, Random Forest and finally, Light GBM. To build the modeling pipeline, let's import all the necessary packages we would need ""]",4,krithi07,logistic-to-lightgbm-for-beginners,krithi07_logistic-to-lightgbm-for-beginners_m4
"[""Let's separate input variables and target variable. Have also created a features list with all input variable names. ""]",5,krithi07,logistic-to-lightgbm-for-beginners,krithi07_logistic-to-lightgbm-for-beginners_m5
"['## Logistic Regression\n', ""We start with most basic algorithm used for classification problems. Initial model with defining only the regularization paramenter (C) yielded 0.6 AUC. Since this is an unbalanced dataset, we need to define another **paramenter 'class_weight = balanced'** which will give equal weights to both the targets irrespective of their reperesentation in the training dataset. We can even define classwise weights using this parameter, if needed ""]",6,krithi07,logistic-to-lightgbm-for-beginners,krithi07_logistic-to-lightgbm-for-beginners_m6
"['## Performance Function\n', 'Since we will be building multiple models, it is advisable to create a function that can be called with different outputs of each model. This is a simple function which takes in the Predicted Validation Target and Actual Validation Target. It then gives out classification summary like **confusion matrix and AUC score **']",7,krithi07,logistic-to-lightgbm-for-beginners,krithi07_logistic-to-lightgbm-for-beginners_m7
"['### Logistic Regresssion Result \n', 'This model gave out an **AUC of 0.854** on validation set and 0.855 on Public Leaderboard for the test file']",8,krithi07,logistic-to-lightgbm-for-beginners,krithi07_logistic-to-lightgbm-for-beginners_m8
"['## Decision Trees\n', 'Moving on to a slightly advanced algorithm, decision trees. Again, the parameters here are class_weight to deal with unbalanced target variable, random_state for reproducability of same trees. The feature max_features and min_sample_leaf are used to prune the tree and avoid overfitting to the training data. \n', '\n', '**Max_features** defines what proportion of available input features will be used to create tree. \n', '\n', '**Min_sample_leaf** restricts the minimum number of samples in a leaf node, making sure none of the leaf nodes has less than 80 samples in it. If leaf nodes have less samples it implies we have grown the tree too much and trying to predict each sample very precisely, thus leading to overfitting.  ']",9,krithi07,logistic-to-lightgbm-for-beginners,krithi07_logistic-to-lightgbm-for-beginners_m9
"['### Decision Tree Results:\n', 'Basic decision tree is giving us **0.651 AUC score** on the validation set and 0.650 AUC score on the test set submitted on public leaderboard ']",10,krithi07,logistic-to-lightgbm-for-beginners,krithi07_logistic-to-lightgbm-for-beginners_m10
"[""Let's take a look at these features and plot them on a box and whiskrers chart""]",11,krithi07,logistic-to-lightgbm-for-beginners,krithi07_logistic-to-lightgbm-for-beginners_m11
"['## Ensemble Learning\n', '[Ensemble Learning](https://en.wikipedia.org/wiki/Ensemble_learning) refers to the algorithms that created using ensembles of variour learning algorithms. So, to give you an example, random forests are ensembles of many decision tree estimators. \n', '\n', 'There are 2 types of ensemble learning algorithms -\n', '**1. Bagging Algorithms:** Bagging involves having each model in the ensemble vote with equal weight for the final output. In order to promote model variance, bagging trains each model in the ensemble using a randomly drawn subset of the training set\n', '**2. Boosting Algorithms:** As Wikipedia defines, boosting involves incrementally building an ensemble by training each new model instance to emphasize the training instances that previous models mis-classified.\n', '\n', '## Random Forest\n', ""Let's start with building a random forest, with parameters like class_weight, random_state, and hyperparameters like max_features and min_sample_leaf as earlier. We have also defined the n_estimators which is a compulsory parameter. This defines the number of decision trees that will be present in the forest. ""]",12,krithi07,logistic-to-lightgbm-for-beginners,krithi07_logistic-to-lightgbm-for-beginners_m12
"[' ### Random Forest Results:\n', ' Basic random forest is giving us **0.787 AUC score** on the validation set and 0.789 AUC score on the test set submitted on public leaderboard']",13,krithi07,logistic-to-lightgbm-for-beginners,krithi07_logistic-to-lightgbm-for-beginners_m13
['The feature importance we get from random forest is very similar to the list we got from decision trees '],14,krithi07,logistic-to-lightgbm-for-beginners,krithi07_logistic-to-lightgbm-for-beginners_m14
"['## Light Gradient Boosting Method\n', '\n', '**WHAT IS IT? **\n', '\n', 'Light GBM is a gradient boosting framework that uses tree based learning algorithm. It grows tree vertically while other algorithm grows trees horizontally meaning that Light GBM grows tree leaf-wise while other algorithm grows level-wise. Leaf-wise algorithm can reduce more loss than a level-wise algorithm.\n', '\n', '**WHY USE LGB?**\n', '\n', 'It is ‘Light’ because of its high speed. It can handle large data, requires low memory to run and focuses on accuracy of results. Also supports GPU learning and thus data scientists/ Kagglers are widely using LGBM for data science application development.\n', '\n', '**TIPS & TRICKS**\n', '\n', '* The algorithm easily overfits and thus, should not be used with small (< 10K rows) datasets.\n', '* Deal with overfitting using these parameters:\n', '    1. Small Maximum Depth\n', '    2. Large Minimum Data in a Leaf\n', '    3. Small Feature and Bagging Fraction\n', '* Improve the training speed\n', '    1. Small Bagging Fraction\n', '    2. Early Stopping Round \n', '* Use small\xa0learning_rate\xa0with large\xa0num_iterations for better accuracy\n', '* Ideally, the value of num_leaves should be less than or equal to 2^(max_depth). Value more than this will result in overfitting\n', '* **If you have a big enough dataset, use this algorithm at least once. It’s accuracy has challenged other boosting algorithms**']",15,krithi07,logistic-to-lightgbm-for-beginners,krithi07_logistic-to-lightgbm-for-beginners_m15
"['### Light GBM Results:\n', 'The AUC Score drastically improves from 0.650 in our Decision Tree model to **an AUC score of 0.89** in our ensemble of trees, Light GBM model. The public leaderboard scores after submitting the test predictions come out to be 0.891\n', '\n', 'The feature importance though, it has some variables similar to those we saw in the tree models but majority of them are new in the top 10 most important variable list']",16,krithi07,logistic-to-lightgbm-for-beginners,krithi07_logistic-to-lightgbm-for-beginners_m16
"['## Next Stpes:\n', 'Now that we have a considerably good AUC score to start with, we can improve on it. A very promising approach is to create new features based on the domain knowledge or based on the EDA we usually do as the first step. Tuning the model or creating a more sophisticated stacked architecture helps improve the score too.\n', '\n']",17,krithi07,logistic-to-lightgbm-for-beginners,krithi07_logistic-to-lightgbm-for-beginners_m17
"['Hi there guys. <br />\n', 'I\'m a **person who have jumped into kaggle three month ago**. During studying many enlighting expert\'s kernels, I\'ve felt kind of **embarrassed feeling** about using hyperparameters of major algorithms such as Xgboost and LightGBM. You guys could reply my opinion like this, **""Why you blame your fault to them?""** \n', '### But, I definitely have **HUGE THANKS TO THEM!!** **Thanks to SUPER BRILLIANT EXPERTS OF KAGGLE** <br />\n', '\n', 'The reason why I make this kernel is that some people use **""lightgbm.train""** and the others use **""lightgbm.LGBMClassifier""** for their model. When I see the differences of them, It makes me insane!! Because the **parameters btw two kinds of kernel as I said above seem pretty different!!** <br />\n', '\n', ""So, In this kernel, I'll discover <br />\n"", '**1. the true meaning of them and aliases of hyperparameters by looking official document of lightgbm.** <br />\n', ""**2. parameter tuning by referring two website where I'll comment below notebooks** <br />\n"", '\n', '### I hope that **two kinds of people** to see this kernel,\n', '\n', ""1. **One is for people who have felt simliar feeling like me.** For them, I'll describe as detail as I can what I've learned and I'd like to share magnificant post for explaining what the Gradient Boosting and the Xgboost are!!<br />\n"", '\n', 'The posts are below!!\n', '* Gradient Boost\n', '>  https://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/\n', '* Xgboost\n', '> https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/\n', '\n', '\n', ""2. **I hope THE OTHERS are enlighting experts** who could give comment about what I understands through this kernel, I would really happy if you guys comment this kernel !! and Could you guys give me a great post about LightGBM parameter or parmeter tuning?? cuz I already have few posts about GBM and XGBoost but I don't have about LightGBM!! (I know generally it seems same one but I think there is regularization in LightGBM) \n"", '\n', ""There is Korean comments for my studying for each sentences by Gabriel Preda's explaination. But I didn't only copy and paste this code. I've changed some code for my own!! \n"", '\n', ""# I'm staying to tune hyperparameters and I will frequently update this Kernel frequently!!.""]",0,kwonyoung234,too-many-names-of-params-in-lgbm,kwonyoung234_too-many-names-of-params-in-lgbm_m0
"['# Reference\n', '\n', ""Gabriel Preda's santander-eda-and-prediction\n"", '> https://www.kaggle.com/gpreda/santander-eda-and-prediction\n', 'this kernel uses lightgbm.train for prediction\n', '\n', ""Will Koehrsen's A Complete Introduction and Walkthrough [Costa Rican Houshold] \n"", '> https://www.kaggle.com/willkoehrsen/a-complete-introduction-and-walkthrough\n', 'this kernel uses LGBMClassifier for prediction\n', '\n', ""Rudolph's Porto: xgb+lgb kfold LB 0.282 [Porto Seguro’s Safe Driver Prediction]\n"", '> https://www.kaggle.com/rshally/porto-xgb-lgb-kfold-lb-0-282\n', 'this kernel uses lightgbm.train for prediction']",1,kwonyoung234,too-many-names-of-params-in-lgbm,kwonyoung234_too-many-names-of-params-in-lgbm_m1
"[""# <a id='1'>Introduction</a>  \n"", '\n', 'In this challenge, Santander invites Kagglers to help them identify which customers will make a specific transaction in the future, irrespective of the amount of money transacted. The data provided for this competition has the same structure as the real data they have available to solve this problem.  \n', '\n', '이번 컴피티션에서는 어느 소비자들이 훗날에 현금을 인출할 것인지를 구분하는 것이 목표입니다. 이번 대회의 데이터는 실제 데이터와 같은 구조로 제공되어있습니다.\n', '\n', 'The data is anonimyzed, each row containing 200 numerical values identified just with a number.  \n', '\n', '데이터는 무기명으로 되어있고 각각의 row는 200개의 서로 다른 컬럼을 가지고 있습니다.\n', '\n', 'In the following we will explore the data, prepare it for a model, train a model and predict the target value for the test set, then prepare a submission.\n', '\n', '다음에서 우리는 데이터를 살펴보고, 모델링 준비를하고, 모델을 훈련시키고 타겟 값을 테스트셋에서 예측하고 제출까지 해솝시다.']",2,kwonyoung234,too-many-names-of-params-in-lgbm,kwonyoung234_too-many-names-of-params-in-lgbm_m2
"['## Load data   \n', '\n', ""Let's check what data files are available.\n"", '\n', '우리가 사용가능한 데이터 파일들을 알아 봅시다.']",3,kwonyoung234,too-many-names-of-params-in-lgbm,kwonyoung234_too-many-names-of-params-in-lgbm_m3
"[""Let's load the train and test data files.""]",4,kwonyoung234,too-many-names-of-params-in-lgbm,kwonyoung234_too-many-names-of-params-in-lgbm_m4
"[""# <a id='3'>Data exploration</a>  \n"", '\n', ""## <a id='31'>Check the data</a>  \n"", '\n', ""Let's check the train and test set.\n"", '\n', '훈련셋과 테스트셋을 확인해봅시다.']",5,kwonyoung234,too-many-names-of-params-in-lgbm,kwonyoung234_too-many-names-of-params-in-lgbm_m5
"['Both train and test data have 200,000 entries and 202, respectivelly 201 columns. \n', '\n', '훈련셋과 테스트셋 모두 200,000개의 행을가지고 각각 202, 201 개의 컬럼수를 가지고 있습니다.\n', '\n', ""Let's glimpse train and test dataset.\n"", '\n', '간단하게 두 세트를 살펴볼까요.\n']",6,kwonyoung234,too-many-names-of-params-in-lgbm,kwonyoung234_too-many-names-of-params-in-lgbm_m6
"['Train contains:  \n', '\n', '* **ID_code** (string);  \n', '* **target**;  \n', '* **200** numerical variables, named from **var_0** to **var_199**;\n', '\n', '훈련세트는. ID, Target 그리고 200개의 숫자값들이 있습니다.\n', '\n', 'Test contains:  \n', '\n', '* **ID_code** (string);  \n', '* **200** numerical variables, named from **var_0** to **var_199**;\n', '\n', '테스트 셋에는 타겟값을 제외한 것들이 있습니다.\n', '\n', ""Let's check if there are any missing data. We will also chech(*k) the type of data.\n"", '\n', '손실값들에 대해서 한번 살펴볼까요> 그리고 데이터들의 타입에 대해서도 알아봅시다.\n', '\n', 'We check first train.\n', '\n', '먼저 훈련세트입니다.']",7,kwonyoung234,too-many-names-of-params-in-lgbm,kwonyoung234_too-many-names-of-params-in-lgbm_m7
"['We can make few observations here:   \n', '우리가 찾은 것들은 아래와 같습니다.\n', '\n', '* standard deviation is relatively large for both train and test variable data;\n', '훈련 데이터와 테스트 데이터 모두 표준편차가 크다는 것\n', '* min, max, mean, sdt values for train and test data looks quite close;\n', '최소,최대,평균,표준편차 값이 훈련과 테스트셋에서 밀접해 보인다는 것\n', '* mean values are distributed over a large range.\n', '평균값의 변동이 크다는 것\n', '\n', ""The number of values in train and test set is the same. Let's plot the scatter plot for train and test set for few of the features.\n"", '훈련과 테스트 셋에서의 값의 수는 동일하다. 그렇다면 몇몇 특징들에 대해서 산포도를 그려봅시다.\n']",8,kwonyoung234,too-many-names-of-params-in-lgbm,kwonyoung234_too-many-names-of-params-in-lgbm_m8
"['\n', ""## <a id='32'>Density plots of features</a>  \n"", '\n', ""Let's show now the density plot of variables in train dataset. \n"", '\n', 'We represent with different colors the distribution for values with **target** value **0** and **1**.']",9,kwonyoung234,too-many-names-of-params-in-lgbm,kwonyoung234_too-many-names-of-params-in-lgbm_m9
"['We can observe that there is a considerable number of features with significant different distribution for the two target values.  \n', 'For example, **var_0**, **var_1**, **var_2**, **var_5**, **var_9**, **var_13**, **var_106**, **var_109**, **var_139** and many others.\n', '\n', '우리는 두 개의 타겟값에 따라서 상당이 다른 분포를 가지고 있는 상당한 수의 특징들을 살펴볼 수 있습니다.\n', '예를 들면, **var_0**, **var_1**, **var_2**, **var_5**, **var_9**, **var_13**, **var_106**, **var_109**, **var_139** 와 다른 것들 말입니다.\n', '\n', 'Also some features, like **var_2**, **var_13**, **var_26**, **var_55**, **var_175**, **var_184**, **var_196** shows a distribution that resambles to a bivariate distribution.\n', '\n', '그리고 몇몇 특징들, **var_2**, **var_13**, **var_26**, **var_55**, **var_175**, **var_184**, **var_196**, 은 이변량분포와 닮은 분포를 보여줍니다.\n', '\n', 'We will take this into consideration in the future for the selection of the features for our prediction model.  \n', '\n', '우리는 이것들을 우리의 예측모델에 feature selection시에 고려하는 참고자료로 사용할 것입니다.\n', '\n', ""Le't s now look to the distribution of the same features in parallel in train and test datasets. \n"", '\n', '그렇다면 이제는 훈련셋과 테스트셋을 평행적으로 같이 보겠습니다.\n', '\n', ""The first 100 values are displayed in the following cell. Press <font color='red'>**Output**</font> to display the plots.\n"", '\n', '첫 번째 100개의 값들은 아래의 그림과 같이 생겼습니다.']",10,kwonyoung234,too-many-names-of-params-in-lgbm,kwonyoung234_too-many-names-of-params-in-lgbm_m10
"['The train and test seems to be well ballanced with respect with distribution of the numeric variables.  \n', '\n', '훈련셋과 테스트셋은 numeric 변수들의 분포들이 잘 균형을 갖추고 있는 듯 합니다.\n', '\n', ""## <a id='33'>Distribution of mean and std</a>  \n"", '평균과 표준편차의 분포\n', '\n', ""Let's check the distribution of the mean values per row in the train and test set.\n"", '\n', '행별로 훈련과 테스트셋의 평균 값의 분포를 알아봅시다.']",11,kwonyoung234,too-many-names-of-params-in-lgbm,kwonyoung234_too-many-names-of-params-in-lgbm_m11
"[""## <a id='34'>Features correlation</a>  컬럼간 상관관계\n"", '\n', 'We calculate now the correlations between the features in train set.  \n', 'The following table shows the first 10 the least correlated features.\n', '\n', '우리는 훈련세트에 컬럼간에 상관관계를 계산해보려고합니다. \n', '아래의 테이블은 처음 10개의 상관관계 특징들을 보여줍니다.\n', '\n', '\n', '> Reference from about guidelines about correlations <br />https://www.kaggle.com/willkoehrsen/a-complete-introduction-and-walkthrough\n', '\n', '#### The general guidelines for correlation values are below, but these will change depending on who you ask (source for these)\n', '\n', '* 00-.19 “very weak” <br />\n', '* 20-.39 “weak” <br />\n', '* 40-.59 “moderate” <br />\n', '* 60-.79 “strong” <br />\n', '* 80-1.0 “very strong” <br />\n', '\n', 'What these correlations show is that there are some weak relationships that hopefully our model will be able to use to learn a mapping from the features to the Target.\n', '\n', 'In that Kernel, he droped one of the columns what have high corrleation between them above 0.95\n', ""So, I'd like to drop them also here.\n"", ""But we don't have any columns what I told above. So I don't delete anything about 200 coulmns""]",12,kwonyoung234,too-many-names-of-params-in-lgbm,kwonyoung234_too-many-names-of-params-in-lgbm_m12
"['The correlation between the features is very small. \n', '\n', ""## <a id='35'>Duplicate values</a>  중복값 처리\n"", '\n', ""Let's now check how many duplicate values exists per columns.\n"", '\n', '컬럼당 얼마나 중복된 값들이 있는지 확인 해보자']",13,kwonyoung234,too-many-names-of-params-in-lgbm,kwonyoung234_too-many-names-of-params-in-lgbm_m13
"['Same columns in train and test set have the same or very close number of duplicates of same or very close values. This is an interesting pattern that we might be able to use in the future.\n', '\n', '훈련세트와 테스트세트에서 같은 컬럼들이 같거나 가까운 양의 중복값을 가지며 이 중복값의 값 또한 같거나 비슷했다. 이는 나중에 사용하기에도 흥미로운 패턴이다.']",14,kwonyoung234,too-many-names-of-params-in-lgbm,kwonyoung234_too-many-names-of-params-in-lgbm_m14
"[""# <a id='4'>Feature engineering</a>  \n"", '\n', 'This section is under construction.  \n', '\n', ""Let's calculate for starting few aggregated values for the existing features.""]",15,kwonyoung234,too-many-names-of-params-in-lgbm,kwonyoung234_too-many-names-of-params-in-lgbm_m15
"[""sum has perfect correaltion wth mean. So, I'd like to delete sum instead of mean.""]",16,kwonyoung234,too-many-names-of-params-in-lgbm,kwonyoung234_too-many-names-of-params-in-lgbm_m16
"['# Feature Selection\n', '\n', ""**In here, I'd like to select features via SFM and REFCV but I couldn't. Because this data set is so huge as you guys know!! So this I'll try later...""]",17,kwonyoung234,too-many-names-of-params-in-lgbm,kwonyoung234_too-many-names-of-params-in-lgbm_m17
['## SFM'],18,kwonyoung234,too-many-names-of-params-in-lgbm,kwonyoung234_too-many-names-of-params-in-lgbm_m18
['## REFCV'],19,kwonyoung234,too-many-names-of-params-in-lgbm,kwonyoung234_too-many-names-of-params-in-lgbm_m19
"[""# <a id='5'>Model</a>  \n"", '\n', 'From the train columns list, we drop the ID and target to form the features list.']",20,kwonyoung234,too-many-names-of-params-in-lgbm,kwonyoung234_too-many-names-of-params-in-lgbm_m20
"['## INFOMATION ABOUT PARAMS\n', '\n', ""### The params what used at Gabriel's code\n"", '\n', '    params = {\n', ""        'num_leaves': 6,\n"", ""        'max_bin': 63,\n"", ""        'min_data_in_leaf': 45,\n"", ""        'learning_rate': 0.01,\n"", ""        'min_sum_hessian_in_leaf': 0.000446,\n"", ""        'bagging_fraction': 0.55, \n"", ""        'bagging_freq': 5, \n"", ""        'max_depth': 14,\n"", ""        'save_binary': True,\n"", ""        'seed': 31452,\n"", ""        'feature_fraction_seed': 31415,\n"", ""        'feature_fraction': 0.51,\n"", ""        'bagging_seed': 31415,\n"", ""        'drop_seed': 31415,\n"", ""        'data_random_seed': 31415,\n"", ""        'objective': 'binary',\n"", ""        'boosting_type': 'gbdt',\n"", ""        'verbose': 1,\n"", ""        'metric': 'auc',\n"", ""        'is_unbalance': True,\n"", ""        'boost_from_average': False,\n"", '    }\n', '\n', '### The params when I make lightgbm.LGBMClassifier.get_params()\n', '\n', '    params = {   \n', ""      'boosting_type': 'gbdt', \n"", ""      'class_weight': None,\n"", ""      'colsample_bytree': 1.0,\n"", ""      'importance_type': 'split',\n"", ""      'learning_rate': 0.1,\n"", ""      'max_depth': -1,\n"", ""      'min_child_samples': 20,\n"", ""      'min_child_weight': 0.001,\n"", ""      'min_split_gain': 0.0,\n"", ""      'n_estimators': 100,\n"", ""      'n_jobs': -1,\n"", ""      'num_leaves': 31,\n"", ""      'objective': None,\n"", ""     'random_state': None,\n"", ""     'reg_alpha': 0.0,\n"", ""     'reg_lambda': 0.0,\n"", ""     'silent': True,\n"", ""     'subsample': 1.0,\n"", ""     'subsample_for_bin': 200000,\n"", ""     'subsample_freq': 0\n"", '        }\n', '\n', '> Reference from <br />\n', 'https://lightgbm.readthedocs.io/en/latest/Python-API.html <br />\n', 'https://lightgbm.readthedocs.io/en/latest/Parameters.html\n', '\n', ""* 'boosting_type': 'gbdt' <br />\n"", ' **alias with boosting** (Default:gbdt, options gbdt,gbrt,rf,random_forest,dart,goss)\n', ' \n', ""* 'class_weight': None <br />\n"", '(default=None) – Weights associated with classes in the form {class_label: weight}. **Use this parameter only for multi-class classification task**; **for binary classification task** you may use **is_unbalance or scale_pos_weight parameters.** The ‘balanced’ mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as n_samples / (n_classes * np.bincount(y)). If None, all classes are supposed to have weight one. Note, that these weights will be multiplied with sample_weight (passed through the fit method) if sample_weight is specified.  \n', ' \n', ""* 'colsample_bytree': 1.0 <br />\n"", '(Default=1.0 / constraints 0.0 < value <= 1.0) **alias: feature_fraction**[simliar with max_features of GBM]  <br />    \n', ' lightgbm will randomly select iteration if feature_fraction smaller than 1.0.\n', ' e.g if I set it 0.8 lightgbm will select 80% of features before training each tree.\n', ' can be used to speed up training.\n', ' can be used to deal with over-fitting.\n', '\n', '* \'importance_type\': \'split\' (default=""split"") <br />\n', 'How the importance is calculated. If “split”, result contains numbers of times the feature is used in a model. If “gain”, result contains total gains of splits which use the feature. <br />\n', '=> sort of method how to gain feature_importance\n', '\n', ""* 'learning_rate': 0.1 <br />\n"", ' (Default=1.0 / constraints learning_rate > 0.0) **alias with with shrinkage_rate,eta**\n', ' \n', ""* 'max_depth': -1 <br />\n"", '  limit the max depth for tree model. This is used to deal with overfitting when data is small\n', ' \n', ""* 'min_child_samples': 20 <br />\n"", '  (Default = 20 / constraints min_data_in_leaf >= 0) **alias with min_data_in_leaf,min_data-per_leaf,min_data,min_child_samples** \n', '\n', ""* 'min_child_weight': 0.001 <br />\n"", ' (Default = 1e-3 // Default min_sum_hessian_in_leaf >= 0.0) <br />\n', ' **alias with min_sum_hessian_in_leaf,min_sum_hessian_per_leaf,min_sum_hessian,min_hessian,min_child_weight **<br />\n', '\n', ""* 'min_split_gain': 0.0  **only in lightgbm.LGBMClassifier()[not in lightgbm.train()]<br />\n"", ' (Default =0.0 / constraints: min_gain_to_splot >= 0.0) **alias with min_gain_to_split,min_split_gain** <br />\n', ' the minimal gain to perform split  <br />\n', ' \n', ""* 'n_estimators': 100 <br />\n"", ' (Default = 100 / constraints n_estimator >= 0) **alias with num_iteration,n_iter,num_tree,num_trees,num_round,num_rounds,num_boost_round,n_estimators** <br />\n', ' number of boosting iterations\n', '\n', ""* 'n_jobs': -1 <br />\n"", '(Default = 0) **alias with num_thread,nthread,nthreads,n_jobs**\n', '\n', ""* 'num_leaves': 31 <br />\n"", ' (Default = 31 / constraints: num_leaves > 1) **aliases: num_leaf, max_leaves, max_leaf** <br />  \n', ' max number of leaves in one tree\n', '    \n', ""* 'objective': None <br />\n"", '(Default = regression / options: regression, regression_l1, huber, fair, poisson, quantile, mape, gammma, tweedie, binary, multiclass, multiclassova, xentropy, xentlambda, lambdarank)\n', ' **aliases: objective_type, app, application**\n', '    \n', ""* 'random_state': None <br />\n"", '(Default = None) **aliases: random_seed, random_state** <br />\n', 'this seed is used to generate other seeds, e.g. data_random_seed, feature_fraction_seed, etc. <br />\n', 'by default, this seed is unused in favor of default values of other seeds <br />\n', 'this seed has lower priority in comparison with other seeds, which means that it will be overridden, if you set other seeds explicitly <br />\n', '\n', ""* 'reg_alpha': 0.0 <br />\n"", '(Default = 0.0 / constraints: lambda_l1 >= 0.0) **aliases: reg_alpha** <br /> \n', 'L1 regularization <br />\n', '\n', ""* 'reg_lambda': 0.0 <br />\n"", '(Default = 0.0 /  constraints: lambda_l2 >= 0.0) **aliases: reg_lambda, lambda** <br />\n', 'L2 regularization <br />\n', '\n', ""* 'silent': True **only in lightgbm.LGBMClassifier()(not in lightgbm.train())**<br />\n"", 'silent (bool, optional (default=False)) – Whether to print messages during construction\n', '\n', ""* 'subsample': 1.0 <br />\n"", '(Default = 1.0 / constraints: 0.0 < bagging_fraction <= 1.0 ) **aliases: sub_row, subsample, bagging** <br /> \n', 'like feature_fraction, but this will randomly select part of data without resampling <br /> \n', 'can be used to speed up training <br /> \n', 'can be used to deal with over-fitting <br /> \n', 'Note: to enable bagging, bagging_freq should be set to a non zero value as well <br /> \n', '\n', ""* 'subsample_for_bin': 200000 <br />\n"", '(Default = 200000 / constraints: bin_construct_sample_cnt > 0)  **aliases: subsample_for_bin** <br /> \n', 'number of data that sampled to construct histogram bins <br />\n', 'setting this to larger value will give better training result, but will increase data loading time <br />\n', 'set this to larger value if data is very sparse <br />\n', '\n', ""* 'subsample_freq': 0\n"", '(Default = 0) **aliases: subsample_freq, frequency for bagging** <br />\n', '0 means disable bagging; k means perform bagging at every k iteration <br />\n', 'Note: to enable bagging, bagging_fraction should be set to value smaller than 1.0 as wel <br />l\n', '\n', ""* 'reg_alpha': 0.0\n"", '(default = 0.0) **aliases: reg_alpha**<br /> \n', 'constraints: lambda_l1 >= 0.0 //  L1 regularization<br />\n', '\n', ""* 'reg_lambda': 0.0\n"", '(default = 0.0) **aliases: reg_lambda, lambda** <br />\n', 'constraints: lambda_l2 >= 0.0 // L2 regularization']",21,kwonyoung234,too-many-names-of-params-in-lgbm,kwonyoung234_too-many-names-of-params-in-lgbm_m21
"['## So We could get some results about comparing two API ""lightgbm.train()"" and ""lightgbm.LGBMClassifier""\n', '\n', '### common params btw two APIs\n', '\n', ""* boosting_type': 'gbdt' ==  'boosting_type': 'gbdt' \n"", ""* 'feature_fraction' == 'colsample_bytree'\n"", ""* 'is_unbalance': True == 'class_weight': None \n"", ""* 'learning_rate' == 'learning_rate' \n"", ""* 'max_depth' == 'max_depth'\n"", ""* 'min_data_in_leaf' == 'min_child_samples'\n"", ""* 'min_sum_hessian_in_leaf' == 'min_child_weight'    \n"", ""* num_round == 'n_estimators'\n"", ""* 'num_leaves' ==  'num_leaves'\n"", ""* 'objective' == 'objective'\n"", ""* 'seed' == 'random_state'\n"", ""* 'subsample' == 'bagging_fraction'\n"", ""* 'subsample_freq' == 'baggin_freq'\n"", ""* 'subsample_for_bin' == 'bin_construct_sample_cnt' [**Gabriel didn't tuning it**]\n"", '\n', '### only in lightgbm.LGBMClassifier()\n', '\n', ""* 'importance_type'\n"", ""* 'min_split_gain'\n"", ""* 'silent'\n"", ""* 'class_weight'\n"", ""* 'reg_alpha'\n"", ""* 'reg_lambda'\n"", ""**(But, I don't know when I should tune about 'reg_xx' If someone knows it plz comment at this kernel)**""]",22,kwonyoung234,too-many-names-of-params-in-lgbm,kwonyoung234_too-many-names-of-params-in-lgbm_m22
"['## Hyperparameter Tunning using Hyperopt\n', '\n', '### The thing what I can do from below kernel is tunning parameters what we saw above through hyperopt!!\n', '> https://www.kaggle.com/willkoehrsen/a-complete-introduction-and-walkthrough\n', '\n', '#### In this phase we need to comply following 4 phases\n', '1. making objective function\n', '2. defining space for parameters\n', '3. choosing algorithm for hyperopt\n', '4. using all of them through fmin of hyperopt\n', '\n', ""### I'd like to complie all of precess using hyperopt but you guys know this process is pretty time-consuming!!!\n"", ""**So I'll post my hyperparameters via this process and finally I put in the gbm_model for making predictions!!**""]",23,kwonyoung234,too-many-names-of-params-in-lgbm,kwonyoung234_too-many-names-of-params-in-lgbm_m23
['### Making user metric for objective function'],24,kwonyoung234,too-many-names-of-params-in-lgbm,kwonyoung234_too-many-names-of-params-in-lgbm_m24
"['### Objective Function\n', '\n', 'P.S) I do this process **briefly**, cuz this is **time-consumming process** as I mentioned before <br />\n', 'So, I recommend to set like this if you do yourself in own environment <br />\n', '\n', '* n_estimators => 15000\n', '* early_stopping_rounds => 250\n', '* verbose => 1000\n']",25,kwonyoung234,too-many-names-of-params-in-lgbm,kwonyoung234_too-many-names-of-params-in-lgbm_m25
['### Defining Space for Hyperparameters'],26,kwonyoung234,too-many-names-of-params-in-lgbm,kwonyoung234_too-many-names-of-params-in-lgbm_m26
['### Make a sample via space what we defined'],27,kwonyoung234,too-many-names-of-params-in-lgbm,kwonyoung234_too-many-names-of-params-in-lgbm_m27
"['### Selecting algorithm\n', ""This algorithm is called by Tree Parzen Estimators. but I don't know how it works.. **So I'll keep trying to understanding!!! Or if you guys have a good site for TPE plz comment below!!**""]",28,kwonyoung234,too-many-names-of-params-in-lgbm,kwonyoung234_too-many-names-of-params-in-lgbm_m28
['### For recording our result of hyperopt'],29,kwonyoung234,too-many-names-of-params-in-lgbm,kwonyoung234_too-many-names-of-params-in-lgbm_m29
['### Final phase of hyperopt'],30,kwonyoung234,too-many-names-of-params-in-lgbm,kwonyoung234_too-many-names-of-params-in-lgbm_m30
['### making the plot using hyperopt'],31,kwonyoung234,too-many-names-of-params-in-lgbm,kwonyoung234_too-many-names-of-params-in-lgbm_m31
"[""## I could get the params below like that by changing params few times using colab, You guys shoud do that not getting someone's params!! \n"", ""If Kaggle's session was long, I'll use hyperparameters what I got from hyporpot above. But you know, it's not! So, I'll use my own parameters to be gotten through hyperopt in colab environment!!""]",32,kwonyoung234,too-many-names-of-params-in-lgbm,kwonyoung234_too-many-names-of-params-in-lgbm_m32
['## make a model using our own hyperparameters!!'],33,kwonyoung234,too-many-names-of-params-in-lgbm,kwonyoung234_too-many-names-of-params-in-lgbm_m33
"['# Introduction\n', '**""[Distilling the Knowledge in a Neural Network](http://arxiv.org/abs/1503.02531)"" was introduced by Geoffrey Hinton, Oriol Vinyals, Jeff Dean in Mar 2015. In this kernel, I would like to share some experiments to distill the knowledge from a [LGBM teacher](https://www.kaggle.com/tanreinama/lightgbm-minimize-leaves-with-gaussiannb) (LB:0.899) to a neural network. The student network has not surpassed the teacher model yet (LB:0.894). But, I hope I can make it happen before this competition ends.**\n', '\n']",0,mathormad,knowledge-distillation-with-nn-rankgauss,mathormad_knowledge-distillation-with-nn-rankgauss_m0
['# Please upvote if you find this kernel interesting ^_^'],1,mathormad,knowledge-distillation-with-nn-rankgauss,mathormad_knowledge-distillation-with-nn-rankgauss_m1
"['## **Load the dataset, and the prediction of 5-fold LGBM**\n']",2,mathormad,knowledge-distillation-with-nn-rankgauss,mathormad_knowledge-distillation-with-nn-rankgauss_m2
"['## Adding some features, the credit belong to these kernels: https://www.kaggle.com/karangautam/keras-nn, https://www.kaggle.com/ymatioun/santander-linear-model-with-additional-features\n']",3,mathormad,knowledge-distillation-with-nn-rankgauss,mathormad_knowledge-distillation-with-nn-rankgauss_m3
['## Normalize and split data\n'],4,mathormad,knowledge-distillation-with-nn-rankgauss,mathormad_knowledge-distillation-with-nn-rankgauss_m4
['## Define our student network\n'],5,mathormad,knowledge-distillation-with-nn-rankgauss,mathormad_knowledge-distillation-with-nn-rankgauss_m5
['## Some necessary functions\n'],6,mathormad,knowledge-distillation-with-nn-rankgauss,mathormad_knowledge-distillation-with-nn-rankgauss_m6
"['## Experiment 1\n', 'Firstly, we check the performance of simple feed forward neural network.']",7,mathormad,knowledge-distillation-with-nn-rankgauss,mathormad_knowledge-distillation-with-nn-rankgauss_m7
"['## Knowledge distillation\n', 'The basic idea is that you feed both groundtruth and the prediction from the teacher model to the student network.\n', 'Soft targets (the prediction of the teacher model) contains more information than the hard labels (groundtruth) due to the fact that they encode similarity measures between the classes.']",8,mathormad,knowledge-distillation-with-nn-rankgauss,mathormad_knowledge-distillation-with-nn-rankgauss_m8
"['## Experment 2\n', ""We set the ratio between teacher's prediction and groundtruth is 1:9, and use the basic binary crossentropy loss.""]",9,mathormad,knowledge-distillation-with-nn-rankgauss,mathormad_knowledge-distillation-with-nn-rankgauss_m9
"['## Experment 3\n', ""We set the ratio between teacher's prediction and groundtruth is 1:9, and use the focal loss.""]",10,mathormad,knowledge-distillation-with-nn-rankgauss,mathormad_knowledge-distillation-with-nn-rankgauss_m10
"['## Experment 4\n', 'Tuning hyper parameter ""Temperature"".']",11,mathormad,knowledge-distillation-with-nn-rankgauss,mathormad_knowledge-distillation-with-nn-rankgauss_m11
['# Please upvote if you find this kernel interesting ^_^'],12,mathormad,knowledge-distillation-with-nn-rankgauss,mathormad_knowledge-distillation-with-nn-rankgauss_m12
"['\n', 'Title: SANTANDER CUSTOMER TRANSACTION PREDICTION\n', '\n', 'Mikel Kengni / March 2019\n', '\n', 'Plan:\n', '\n', '**: )                                     First things First: snacks and chip checked, Coffee checked                                         : )**\n', '\n', '* **Introduction**\n', '* Competition overview\n', '* Kernel 1: Importance of Data Balancing\n', '\n', '* Outlines/Progression\n', '\n', '    1- Import the libraries\n', '\n', '    2- Run the data\n', '\n', '    3- Data  exploration and Feature Engineering\n', '    \n', '    4- How skewed is our dataset:\n', '    \n', '    5- Correlation between all features na dthe target features\n', '    \n', '    6- Modelling\n', '     * Part_1: Using a classifier with the **Class_weight = Balanced** parameter\n', '     * Part_2: Using the **SMOTE** oversampling technique\n', '   \n', '  7- Metric Traps\n', '  \n', '  8- Observation\n', '  \n', '  9- Conclusion\n', '  \n', '  10- Kernels and Materials used\n']",0,mikelkn,right-wrong-ways-to-oversample-for-beginners-0-788,mikelkn_right-wrong-ways-to-oversample-for-beginners-0-788_m0
"['# Introduction:\n', '\n', '**Competition Overview **:\n', '\n', 'In this challenge, we help identify which customers will make a specific transaction in the future, irrespective of the amount of money transacted.\n', '\n', 'We are provided an anonymized dataset with each row containing 200 numerical values identified just with a number, the binary target column, and a string ID_code column. \n', '\n', 'The task is to predict the value of target column in the test set.']",1,mikelkn,right-wrong-ways-to-oversample-for-beginners-0-788,mikelkn_right-wrong-ways-to-oversample-for-beginners-0-788_m1
"['**Kernel 1: Importance of Data Balancing**\n', '\n', 'This kernel is going to be about the importance of data balancing and the effects of unbalance datasets(uneven classes in this case just 2 classes) on our results. \n', '\n', '* We will be comparing 2 different ways of data balancing - \n', '\n', '1- Using the \'Balanced"" parameter in the Class_weight feature \n', '\n', '2- Using the SMOTE oversampling technique: \n', '\n', '* Under the SMOTE Method will determine the right and wrong ways to oversample using SMOTE.\n', '    \n', '    * We will do it in 2 ways. \n', '    \n', '        * We will apply SMOTE on the whole predictors features and outcome, then split them into train and validation set, then fit into a model.\n', '        \n', '        * We will also do it the other way.  Ie. We will split the datasets into train and validation sets, then we will apply  SMOTE on the X_train and y_train splits.\n', '            \n', '3- Finally, we will find out which of the 2 techniques above score better with this problem and why.']",2,mikelkn,right-wrong-ways-to-oversample-for-beginners-0-788,mikelkn_right-wrong-ways-to-oversample-for-beginners-0-788_m2
['# 1- Import the Libraries'],3,mikelkn,right-wrong-ways-to-oversample-for-beginners-0-788,mikelkn_right-wrong-ways-to-oversample-for-beginners-0-788_m3
['# 2- Read Train and Test Datasets'],4,mikelkn,right-wrong-ways-to-oversample-for-beginners-0-788,mikelkn_right-wrong-ways-to-oversample-for-beginners-0-788_m4
['# 3- Data Exploration and Feature Engineering'],5,mikelkn,right-wrong-ways-to-oversample-for-beginners-0-788,mikelkn_right-wrong-ways-to-oversample-for-beginners-0-788_m5
"['**OBSERVATION:**\n', 'Data contains:\n', '\n', '* ID_code (string)\n', '* Target\n', '* 200 numerical variables, var_0 to var_199\n', '* SHAPE = 200000 ROWS AND 202 COLUMNS\n', '\n', 'Test contains:\n', '\n', '* ID_code (string);\n', '* 200 numerical variables, var_0 to var_199\n', '* SHAPE = 200000 ROWS AND 201 COLUMNS']",6,mikelkn,right-wrong-ways-to-oversample-for-beginners-0-788,mikelkn_right-wrong-ways-to-oversample-for-beginners-0-788_m6
['Lets take a look at the dictribution of the target variables in both the train and test sets.'],7,mikelkn,right-wrong-ways-to-oversample-for-beginners-0-788,mikelkn_right-wrong-ways-to-oversample-for-beginners-0-788_m7
"['Our Target set is very imbalanced. About 90 percent of our target column is 0 while the remianing 10 percent are 1s. This si called Class Imbalanced. It occurs each class does not make up an equal portion of your data-set and It is important to properly adjust your metrics and methods to adjust for your goals. If this is not done, you may end up optimizing for a meaningless metric and hence getting a flawed outcome.']",8,mikelkn,right-wrong-ways-to-oversample-for-beginners-0-788,mikelkn_right-wrong-ways-to-oversample-for-beginners-0-788_m8
['# 4- How skewed is our a datasets?'],9,mikelkn,right-wrong-ways-to-oversample-for-beginners-0-788,mikelkn_right-wrong-ways-to-oversample-for-beginners-0-788_m9
"['* Why do we need to check for skewness you ask?\n', '\n', 'It’s often desirable to transform skewed data and to convert it into values between 0 and 1 because usually, different features in a datasets have values in diffeernet range. In order to have a reliable predictive model, it is important to bring all these features in the same range.\n', '\n', '\n', ""Let's take a look at skewness in our dataset:""]",10,mikelkn,right-wrong-ways-to-oversample-for-beginners-0-788,mikelkn_right-wrong-ways-to-oversample-for-beginners-0-788_m10
"['# 5- Correlation with the target feature\n', 'https://towardsdatascience.com/data-correlation-can-make-or-break-your-machine-learning-project-82ee11039cc9\n', '\n', 'Data correlation is the way in which one set of data may correspond to another set. It is important to determine hoe correlated your features are, as this knowledge may be useful in choosing the right algorithm but also, If you try to train a model on a set of features with no or very little correlation, you will get inaccurate results. \n', '\n', 'Lets se how all the features correlate with the target feature in the train set.']",11,mikelkn,right-wrong-ways-to-oversample-for-beginners-0-788,mikelkn_right-wrong-ways-to-oversample-for-beginners-0-788_m11
['WAOUWWW!!! Looks like there is not a lot of correlation between the feayures and the predcitor. I will use the random forest classifier later to get the most important features in the future if need arises. Okay!!! lets move on.'],12,mikelkn,right-wrong-ways-to-oversample-for-beginners-0-788,mikelkn_right-wrong-ways-to-oversample-for-beginners-0-788_m12
"['# 6- Preparing for Modelling\n', '   We are going to do the modelling in 2 part.\n', '   - Modelling part 1- Metric trap\n', '   - MOdelling part 2']",13,mikelkn,right-wrong-ways-to-oversample-for-beginners-0-788,mikelkn_right-wrong-ways-to-oversample-for-beginners-0-788_m13
"[' **Modelling part 1:** \n', '   \n', '   - We split the datasets in train and validation sets\n', '   - We discuss the importance of picking the right metric and why accuracy_score is not the best metric to choose when we have a class imbalance.\n', '   - Then weuse a simple algorithm in this case a Random Forest Classifier, train our unbalanec dataset on it , calculate the score with the accuarcy_score and then with the roc_auc_score.\n', '   - In order to proof that the accuracy_score is not the right metric, we will do a small test, pick just on feature and train it, then we calculate the accuracy_score and the roc_auc_score on it.']",14,mikelkn,right-wrong-ways-to-oversample-for-beginners-0-788,mikelkn_right-wrong-ways-to-oversample-for-beginners-0-788_m14
"['**Why do we need To balance out dataset:**\n', '\n', '* As we can see from above,the data set is severely imbalanced (90 : 10).\n', '* The main motivation behind the need to preprocess imbalanced data before we feed them into a classifier is that typically classifiers are more sensitive to detecting the majority class and less sensitive to the minority class.\n', '* Usually, data imbalance will lead to the classification output being biased, in many cases resulting in always predicting the majority class like we will see in  below.']",15,mikelkn,right-wrong-ways-to-oversample-for-beginners-0-788,mikelkn_right-wrong-ways-to-oversample-for-beginners-0-788_m15
"['# 8- Metric Trap:\n', '\n', 'One of the major issues begginers usually fall into when dealing with unbalanced datasets is the choice of their evalution metrics.  Using simpler metrics like accuracy_score my not always be the correct. \n', '\n', 'In a dataset with highly unbalanced classes, if the classifier always ""predicts"" the most common class without performing any analysis of the features, it will still have a high accuracy rate. ie, whatever the circumstance, the accuarcy_score will most likely always be the percentage of the majority classe. If you don\'t get it yet, Hnag on, it will be clearer with examples below..']",16,mikelkn,right-wrong-ways-to-oversample-for-beginners-0-788,mikelkn_right-wrong-ways-to-oversample-for-beginners-0-788_m16
"['But for this little experiement, i will be using the random forest classifier.and for the metric evaluation i will be using the accuarcy_score and the roc_auc_score.\n']",17,mikelkn,right-wrong-ways-to-oversample-for-beginners-0-788,mikelkn_right-wrong-ways-to-oversample-for-beginners-0-788_m17
"['The accuracy_score for this part is 0.8976 while the roc_auc_score is 0.7921. \n', '**Do you remeber the percentage of classe distribution in our dataset?** Here we go! look at the percentage dictribution for classe with 0. It is almost the same as the accuarcy_score.']",18,mikelkn,right-wrong-ways-to-oversample-for-beginners-0-788,mikelkn_right-wrong-ways-to-oversample-for-beginners-0-788_m18
"[""Lets do something else to confirm what we are already suspecting. Now let's run the same code, but using only one feature. Normally, the accuracy score should be very small given that we are only using one feature. \n""]",19,mikelkn,right-wrong-ways-to-oversample-for-beginners-0-788,mikelkn_right-wrong-ways-to-oversample-for-beginners-0-788_m19
"['As we can see, The accuracy_score whicch under normal circumstances should be really low, is atill stuck at 0.8976% which is not correct. This goes to show how important the choide of evalution metric especially when dealing with unbalanced datasets. The other metric we used for this(roc_auc_score) is behaving like it should, ie for a single feature, its score actually dropped to 0.51. ']",20,mikelkn,right-wrong-ways-to-oversample-for-beginners-0-788,mikelkn_right-wrong-ways-to-oversample-for-beginners-0-788_m20
"['**Modelling part 2:**\n', ' \n', '   - We train the data the same model used in modelling part1 but htis time around we balance the classes before feeding it into an algorithm for training.\n', '   - then we calculate the score using the same algorithm we used above.\n', '   - Then map out the importance of always using unbiased datasets(datasets with one classes a lot more present that the other.\n', '   ']",21,mikelkn,right-wrong-ways-to-oversample-for-beginners-0-788,mikelkn_right-wrong-ways-to-oversample-for-beginners-0-788_m21
"['**Modelling part 2: Balancing the classes using the class_weights parameters**\n', '\n', ""The paramenter **'class_weight = balanced**' will give equal weights to both classes  irrespective of their reperesentation in the training datase. ""]",22,mikelkn,right-wrong-ways-to-oversample-for-beginners-0-788,mikelkn_right-wrong-ways-to-oversample-for-beginners-0-788_m22
"['After balancing the classes by balancing their weights, we can see that the accuracy has dropped a little.']",23,mikelkn,right-wrong-ways-to-oversample-for-beginners-0-788,mikelkn_right-wrong-ways-to-oversample-for-beginners-0-788_m23
"['**Imbalanced data put accuracy out of business** as we proved above. It is usually not enought to rely on hight accuracy_score to evalute your model because the score may just be illusionary and a simple reflexion of the majority class. Using other evalution metric like the roc_auc_score, f1_score, classification report etc could give us a better evalution of the performance our our model w.r.t the dataset. But it is always a good idea and safer to work with balanced datasets and balancing a dataset can be as easy as just adjusting the class_weight parameters.\n', '\n', ""For algorithms with the Class_weight parameter, it sometimes suffices to set set **class_weight = 'Balanced'** like in this case the random_forest classifier.\n"", '\n', 'With some other algorithms, we may need to set the class weight parameter manually.  We set the class_weight such as to penalize mistakes on the minority class by an amount proportional to how under-represented it is. For example \n', '\n', '> class_weight = ({0 : ""0.25"", 1:  ""0.85""}).\n', '\n', 'Another alternative to using the class_weight parameter is to creat synthetic observations of the minority class using the **SMOTE = Synthetic Minority Oversampling Technique** from the sklearn.imblearn library.']",24,mikelkn,right-wrong-ways-to-oversample-for-beginners-0-788,mikelkn_right-wrong-ways-to-oversample-for-beginners-0-788_m24
"[""# - Data Balancing using 'SMOTE'""]",25,mikelkn,right-wrong-ways-to-oversample-for-beginners-0-788,mikelkn_right-wrong-ways-to-oversample-for-beginners-0-788_m25
['**Balancing classes using SMOTE before spltting dataset into train and validaton sets'],26,mikelkn,right-wrong-ways-to-oversample-for-beginners-0-788,mikelkn_right-wrong-ways-to-oversample-for-beginners-0-788_m26
"['**SMOTE Algorithm (Synthetic Minority Oversampling Technique)**\n', '\n', 'We will be using the SMOTE algorithm (Synthetic Minority Oversampling Technique) to over-sample our dataset. It is a powerful sampling method that goes beyonds simply increasing or decreasing the number of datas in a dataset. How it works is by.\n', '\n', '1- Finding the k-nearest-neighbors for minority class observations (finding similar observations)\n', '\n', '2- Randomly choosing one of the k-nearest-neighbors and using it to create a similar, but randomly tweaked, new observation.']",27,mikelkn,right-wrong-ways-to-oversample-for-beginners-0-788,mikelkn_right-wrong-ways-to-oversample-for-beginners-0-788_m27
"[""# 'SMOTE' on X_train and y_train only\n"", '\n', '**Let do a split before we apply smote on x_train amd y_train **\n', '\n']",28,mikelkn,right-wrong-ways-to-oversample-for-beginners-0-788,mikelkn_right-wrong-ways-to-oversample-for-beginners-0-788_m28
['# Submission Dataframe'],29,mikelkn,right-wrong-ways-to-oversample-for-beginners-0-788,mikelkn_right-wrong-ways-to-oversample-for-beginners-0-788_m29
"['We porved earlier that a very high accuracy can sometimes not be a relexion of how the model actually performs. So i will not consider the results from the accuracy in this section. The main purpose of using accuracy_score was to show its flaws especially when dealing with unbalanced datasets. \n', '\n', 'Lets focus on the roc_auc_score for both scenarios( applying SMOTE before splitting and applying SMOTE after splitting.).']",30,mikelkn,right-wrong-ways-to-oversample-for-beginners-0-788,mikelkn_right-wrong-ways-to-oversample-for-beginners-0-788_m30
"['# Observation:\n', '**Scenario 1: Resamplling then splitting into train - validation sets**\n', '\n', 'In this scenario, we oversampled the whole datasets then we split it into train and validation set. From our roc_auc_score evaluation, we can see that we have a 91.0% score on the validation set but a 49.9% score on the test data. That is a huge gap between the scsores and this is so because some information **""bleed""** from the validation set into the training set ofthe model.  \n', '\n', 'By oversampling before splitting the dataset into train and validation sets, we ended up with some of the information from  the validation set being used to create some of the synthetic observations in the training set. As a result, the model has already ""seen"" some of the datait is predicting in the and as such, is able to perfectly predict these data during validation hence increasing the roc_auc_score of the validation set. Hecnce the big gap between the score for the validation set an that of the test set.( 91.0% versus 49.9%)\n']",31,mikelkn,right-wrong-ways-to-oversample-for-beginners-0-788,mikelkn_right-wrong-ways-to-oversample-for-beginners-0-788_m31
"['**Scenario 2: Splitting the datasets into train and validation sets the resampling the X_train and y_train data**\n', '\n', 'In this scenario, we split the dataset into train and validation sets and then resampled the trained data. Here, the validation set is untouched so the result from this scenario is more generalizable. \n', '\n', 'As we see from the roc_auc_scores, the score for the validation set( 66.9 %) is very close to the score from the test set(50.0%). \n', '\n', 'Scenario 2  is the **right way to oversample data.** while scenario 1 is the **wrong way to oversample data.**\n', '\n']",32,mikelkn,right-wrong-ways-to-oversample-for-beginners-0-788,mikelkn_right-wrong-ways-to-oversample-for-beginners-0-788_m32
"['# Conclusion: \n', 'It is always advisable to **split your datatset into train and validation before oversampling**(Scenario 2). And only **apply your oversampling method on the training sets**. The validation set should be pristine.']",33,mikelkn,right-wrong-ways-to-oversample-for-beginners-0-788,mikelkn_right-wrong-ways-to-oversample-for-beginners-0-788_m33
['Thanks for reading to the end.'],34,mikelkn,right-wrong-ways-to-oversample-for-beginners-0-788,mikelkn_right-wrong-ways-to-oversample-for-beginners-0-788_m34
"['\n', 'If you found this kernel helpful, i would really appreciate an upvote. If you did not, please comment below with your suggestion or recommendations and lets make it better together. \n', '\n']",35,mikelkn,right-wrong-ways-to-oversample-for-beginners-0-788,mikelkn_right-wrong-ways-to-oversample-for-beginners-0-788_m35
"['References used for Data Balancing/ Resampling:\n', '\n', '- https://beckernick.github.io/oversampling-modeling/\n', '- https://elitedatascience.com/imbalanced-classes\n', '- https://towardsdatascience.com/handling-imbalanced-datasets-in-deep-learning-f48407a0e758a\n', '- https://towardsdatascience.com/deep-learning-unbalanced-training-data-solve-it-like-this-6c528e9efea6\n', '- https://towardsdatascience.com/dealing-with-imbalanced-classes-in-machine-learning-d43d6fa19d2\n', '- https://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/']",36,mikelkn,right-wrong-ways-to-oversample-for-beginners-0-788,mikelkn_right-wrong-ways-to-oversample-for-beginners-0-788_m36
"[' #  <div style=""text-align: center"">  Santander ML Explainability  </div> \n', '###  <div style=""text-align: center"">CLEAR DATA. MADE MODEL. </div> \n', ""<img src='https://galeria.bankier.pl/p/b/5/215103d7ace468-645-387-261-168-1786-1072.jpg' width=600 height=600>\n"", '<div style=""text-align:center""> last update: <b> 10/03/2019</b></div>\n', '\n', '\n', '\n', 'You can Fork code  and  Follow me on:\n', '\n', '> ###### [ GitHub](https://github.com/mjbahmani/10-steps-to-become-a-data-scientist)\n', '> ###### [Kaggle](https://www.kaggle.com/mjbahmani/)\n', '-------------------------------------------------------------------------------------------------------------\n', "" <b>I hope you find this kernel helpful and some <font color='red'>UPVOTES</font> would be very much appreciated.</b>\n"", '    \n', ' -----------']",0,mjbahmani,santander-ml-explainability,mjbahmani_santander-ml-explainability_m0
"[' <a id=""top""></a> <br>\n', '## Notebook  Content\n', '1. [Introduction](#1)\n', '1. [Load packages](#2)\n', '    1. [import](21)\n', '    1. [Setup](22)\n', '    1. [Version](23)\n', '1. [Problem Definition](#3)\n', '    1. [Problem Feature](#31)\n', '    1. [Aim](#32)\n', '    1. [Variables](#33)\n', '    1. [Evaluation](#34)\n', '1. [Exploratory Data Analysis(EDA)](#4)\n', '    1. [Data Collection](#41)\n', '    1. [Visualization](#42)\n', '    1. [Data Preprocessing](#43)\n', '1. [Machine Learning Explainability for Santander](#5)\n', '    1. [Permutation Importance](#51)\n', '    1. [How to calculate and show importances?](#52)\n', '    1. [What can be inferred from the above?](#53)\n', '    1. [Partial Dependence Plots](#54)\n', '1. [Model Development](#6)\n', '    1. [lightgbm](#61)\n', '    1. [RandomForestClassifier](#62)\n', '    1. [DecisionTreeClassifier](#63)\n', '    1. [CatBoostClassifier](#64)\n', '    1. [Funny Combine](#65)\n', '1. [References](#7)']",1,mjbahmani,santander-ml-explainability,mjbahmani_santander-ml-explainability_m1
"[' <a id=""1""></a> <br>\n', '## 1- Introduction\n', 'At [Santander](https://www.santanderbank.com) their mission is to help people and businesses prosper. they are always looking for ways to help our customers understand their financial health and identify which products and services might help them achieve their monetary goals.\n', ""<img src='https://www.smava.de/kredit/wp-content/uploads/2015/12/santander-bank.png' width=400 height=400>\n"", '\n', 'In this kernel we are going to create a **Machine Learning Explainability** for **Santander** based this perfect [course](https://www.kaggle.com/learn/machine-learning-explainability) in kaggle.\n', '><font color=""red""><b>Note: </b></font>\n', 'how to extract **insights** from models?']",2,mjbahmani,santander-ml-explainability,mjbahmani_santander-ml-explainability_m2
"['<a id=""2""></a> <br>\n', '## 2- A Data Science Workflow for Santander \n', 'Of course, the same solution can not be provided for all problems, so the best way is to create a **general framework** and adapt it to new problem.\n', '\n', '**You can see my workflow in the below image** :\n', '\n', ' <img src=""http://s8.picofile.com/file/8342707700/workflow2.png""  />\n', '\n', '**You should feel free\tto\tadjust \tthis\tchecklist \tto\tyour needs**\n', '###### [Go to top](#top)']",3,mjbahmani,santander-ml-explainability,mjbahmani_santander-ml-explainability_m3
"[' <a id=""2""></a> <br>\n', ' ## 2- Load packages\n', '  <a id=""21""></a> <br>\n', '## 2-1 Import']",4,mjbahmani,santander-ml-explainability,mjbahmani_santander-ml-explainability_m4
"[' <a id=""22""></a> <br>\n', '##  2-2 Setup']",5,mjbahmani,santander-ml-explainability,mjbahmani_santander-ml-explainability_m5
"[' <a id=""23""></a> <br>\n', '## 2-3 Version\n']",6,mjbahmani,santander-ml-explainability,mjbahmani_santander-ml-explainability_m6
"['<a id=""3""></a> \n', '<br>\n', '## 3- Problem Definition\n', 'In this **challenge**, we should help this **bank**  identify which **customers** will make a **specific transaction** in the future, irrespective of the amount of money transacted. The data provided for this competition has the same structure as the real data we have available to solve this **problem**.\n']",7,mjbahmani,santander-ml-explainability,mjbahmani_santander-ml-explainability_m7
"['<a id=""31""></a> \n', '### 3-1 Problem Feature\n', '\n', '1. train.csv - the training set.\n', '1. test.csv - the test set. The test set contains some rows which are not included in scoring.\n', '1. sample_submission.csv - a sample submission file in the correct format.\n']",8,mjbahmani,santander-ml-explainability,mjbahmani_santander-ml-explainability_m8
"['<a id=""32""></a> \n', '### 3-2 Aim\n', 'In this competition, The task is to predict the value of **target** column in the test set.']",9,mjbahmani,santander-ml-explainability,mjbahmani_santander-ml-explainability_m9
"['<a id=""33""></a> \n', '### 3-3 Variables\n', '\n', 'We are provided with an **anonymized dataset containing numeric feature variables**, the binary **target** column, and a string **ID_code** column.\n', '\n', 'The task is to predict the value of **target column** in the test set.']",10,mjbahmani,santander-ml-explainability,mjbahmani_santander-ml-explainability_m10
"['<a id=""34""></a> \n', '## 3-4 evaluation\n', '**Submissions** are evaluated on area under the [ROC curve](http://en.wikipedia.org/wiki/Receiver_operating_characteristic) between the predicted probability and the observed target.\n', ""<img src='https://upload.wikimedia.org/wikipedia/commons/6/6b/Roccurves.png' width=300 height=300>""]",11,mjbahmani,santander-ml-explainability,mjbahmani_santander-ml-explainability_m11
"['<a id=""4""></a> \n', '## 4- Exploratory Data Analysis(EDA)\n', "" In this section, we'll analysis how to use graphical and numerical techniques to begin uncovering the structure of your data. \n"", '*  Data Collection\n', '*  Visualization\n', '*  Data Preprocessing\n', '*  Data Cleaning\n', '<img src=""http://s9.picofile.com/file/8338476134/EDA.png"" width=400 height=400>']",12,mjbahmani,santander-ml-explainability,mjbahmani_santander-ml-explainability_m12
"[' <a id=""41""></a> <br>\n', '## 4-1 Data Collection']",13,mjbahmani,santander-ml-explainability,mjbahmani_santander-ml-explainability_m13
"['# Reducing  memory size by ~50%\n', ""Because we make a lot of calculations in this kernel, we'd better reduce the size of the data.\n"", '1. 300 MB before Reducing\n', '1. 150 MB after Reducing']",14,mjbahmani,santander-ml-explainability,mjbahmani_santander-ml-explainability_m14
['Reducing for train data set'],15,mjbahmani,santander-ml-explainability,mjbahmani_santander-ml-explainability_m15
['Reducing for test data set'],16,mjbahmani,santander-ml-explainability,mjbahmani_santander-ml-explainability_m16
"[' <a id=""41""></a> <br>\n', '##   4-1-1Data set fields']",17,mjbahmani,santander-ml-explainability,mjbahmani_santander-ml-explainability_m17
"[' <a id=""422""></a> <br>\n', '## 4-2-2 numerical values Describe']",18,mjbahmani,santander-ml-explainability,mjbahmani_santander-ml-explainability_m18
"[' <a id=""42""></a> <br>\n', '## 4-2 Visualization']",19,mjbahmani,santander-ml-explainability,mjbahmani_santander-ml-explainability_m19
"['<a id=""421""></a> \n', '## 4-2-1 hist']",20,mjbahmani,santander-ml-explainability,mjbahmani_santander-ml-explainability_m20
"[' <a id=""422""></a> <br>\n', '## 4-2-2 Mean Frequency']",21,mjbahmani,santander-ml-explainability,mjbahmani_santander-ml-explainability_m21
"['<a id=""423""></a> \n', '## 4-2-3 countplot']",22,mjbahmani,santander-ml-explainability,mjbahmani_santander-ml-explainability_m22
"['<a id=""424""></a> \n', '## 4-2-4 hist\n', 'If you check histogram for all feature, you will find that most of them are so similar']",23,mjbahmani,santander-ml-explainability,mjbahmani_santander-ml-explainability_m23
"['<a id=""426""></a> \n', '## 4-2-6 distplot\n', ' The target in data set is **imbalance**']",24,mjbahmani,santander-ml-explainability,mjbahmani_santander-ml-explainability_m24
"['<a id=""427""></a> \n', '## 4-2-7 violinplot']",25,mjbahmani,santander-ml-explainability,mjbahmani_santander-ml-explainability_m25
[],26,mjbahmani,santander-ml-explainability,mjbahmani_santander-ml-explainability_m26
"[' <a id=""43""></a> <br>\n', '## 4-3 Data Preprocessing\n', 'Before we start this section let me intrduce you, some other compitation that they were similar to this:\n', '\n', '1. https://www.kaggle.com/artgor/how-to-not-overfit\n', '1. https://www.kaggle.com/c/home-credit-default-risk\n', '1. https://www.kaggle.com/c/porto-seguro-safe-driver-prediction']",27,mjbahmani,santander-ml-explainability,mjbahmani_santander-ml-explainability_m27
"[' <a id=""431""></a> <br>\n', '## 4-3-1 Check missing data for test & train']",28,mjbahmani,santander-ml-explainability,mjbahmani_santander-ml-explainability_m28
"[' <a id=""432""></a> <br>\n', '## 4-3-2 Binary Classification']",29,mjbahmani,santander-ml-explainability,mjbahmani_santander-ml-explainability_m29
"[' <a id=""433""></a> <br>\n', '## 4-3-3 Is data set imbalance?']",30,mjbahmani,santander-ml-explainability,mjbahmani_santander-ml-explainability_m30
"['A large part of the data is unbalanced, but **how can we  solve it?**']",31,mjbahmani,santander-ml-explainability,mjbahmani_santander-ml-explainability_m31
"['1. **Imbalanced dataset** is relevant primarily in the context of supervised machine learning involving two or more classes. \n', '\n', '1. **Imbalance** means that the number of data points available for different the classes is different\n', '\n', ""<img src='https://www.datascience.com/hs-fs/hubfs/imbdata.png?t=1542328336307&width=487&name=imbdata.png'>\n"", '[Image source](http://api.ning.com/files/vvHEZw33BGqEUW8aBYm4epYJWOfSeUBPVQAsgz7aWaNe0pmDBsjgggBxsyq*8VU1FdBshuTDdL2-bp2ALs0E-0kpCV5kVdwu/imbdata.png)']",32,mjbahmani,santander-ml-explainability,mjbahmani_santander-ml-explainability_m32
['## 4-3-4 skewness and kurtosis'],33,mjbahmani,santander-ml-explainability,mjbahmani_santander-ml-explainability_m33
"[' <a id=""5""></a> <br>\n', '# 5- Machine Learning Explainability for Santander\n', 'In this section, I want to try extract **insights** from models with the help of this excellent [**Course**](https://www.kaggle.com/learn/machine-learning-explainability) in Kaggle.\n', 'The Goal behind of ML Explainability for Santander is:\n', '1. All features are senseless named.(var_1, var2,...) but certainly the importance of each one is different!\n', '1. Extract insights from models.\n', '1. Find the most inmortant feature in models.\n', ""1. Affect of each feature on the model's predictions.\n"", ""<img src='http://s8.picofile.com/file/8353215168/ML_Explain.png'>\n"", '\n', 'As you can see from the above, we will refer to three important and practical concepts in this section and try to explain each of them in detail.']",34,mjbahmani,santander-ml-explainability,mjbahmani_santander-ml-explainability_m34
"[' <a id=""51""></a> <br>\n', '## 5-1 Permutation Importance\n', ' In this section we will answer following question:\n', ' 1. What features have the biggest impact on predictions?\n', ' 1. how to extract insights from models?']",35,mjbahmani,santander-ml-explainability,mjbahmani_santander-ml-explainability_m35
['### Prepare our data for our model'],36,mjbahmani,santander-ml-explainability,mjbahmani_santander-ml-explainability_m36
['### Create  a sample model to calculate which feature are more important.'],37,mjbahmani,santander-ml-explainability,mjbahmani_santander-ml-explainability_m37
"[' <a id=""52""></a> <br>\n', '## 5-2 How to calculate and show importances?']",38,mjbahmani,santander-ml-explainability,mjbahmani_santander-ml-explainability_m38
['### Here is how to calculate and show importances with the [eli5](https://eli5.readthedocs.io/en/latest/) library:'],39,mjbahmani,santander-ml-explainability,mjbahmani_santander-ml-explainability_m39
"['<a id=""53""></a> <br>\n', '## 5-3 What can be inferred from the above?\n', '1. As you move down the top of the graph, the importance of the feature decreases.\n', '1. The features that are shown in green indicate that they have a positive impact on our prediction\n', '1. The features that are shown in white indicate that they have no effect on our prediction\n', '1. The features shown in red indicate that they have a negative impact on our prediction\n', '1.  The most important feature was **Var_110**.']",40,mjbahmani,santander-ml-explainability,mjbahmani_santander-ml-explainability_m40
"['<a id=""54""></a> <br>\n', '## 5-4 Partial Dependence Plots\n', 'While **feature importance** shows what **variables** most affect predictions, **partial dependence** plots show how a feature affects predictions.[6][7]\n', 'and partial dependence plots are calculated after a model has been fit. [partial-plots](https://www.kaggle.com/dansbecker/partial-plots)']",41,mjbahmani,santander-ml-explainability,mjbahmani_santander-ml-explainability_m41
"['For the sake of explanation, I use a Decision Tree which you can see below.']",42,mjbahmani,santander-ml-explainability,mjbahmani_santander-ml-explainability_m42
"['As guidance to read the tree:\n', '\n', '1. Leaves with children show their splitting criterion on the top\n', '1. The pair of values at the bottom show the count of True values and False values for the target respectively, of data points in that node of the tree.\n', '><font color=""red""><b>Note: </b></font>\n', 'Yes **Var_81** are more effective on our model.']",43,mjbahmani,santander-ml-explainability,mjbahmani_santander-ml-explainability_m43
"['<a id=""55""></a> <br>\n', '## 5-5  Partial Dependence Plot\n', 'In this section, we see the impact of the main variables discovered in the previous sections by using the [pdpbox](https://pdpbox.readthedocs.io/en/latest/).']",44,mjbahmani,santander-ml-explainability,mjbahmani_santander-ml-explainability_m44
"['<a id=""56""></a> <br>\n', '## 5-6 Chart analysis\n', '1. The y axis is interpreted as change in the prediction from what it would be predicted at the baseline or leftmost value.\n', '1. A blue shaded area indicates level of confidence']",45,mjbahmani,santander-ml-explainability,mjbahmani_santander-ml-explainability_m45
"['<a id=""57""></a> <br>\n', '## 5-7 SHAP Values\n', '**SHAP** (SHapley Additive exPlanations) is a unified approach to explain the output of **any machine learning model**. SHAP connects game theory with local explanations, uniting several previous methods [1-7] and representing the only possible consistent and locally accurate additive feature attribution method based on expectations (see the SHAP NIPS paper for details).\n', '\n', ""<img src='https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/shap_diagram.png' width=400 height=400>\n"", '[image credits](https://github.com/slundberg/shap)\n', '><font color=""red""><b>Note: </b></font>\n', 'Shap can answer to this qeustion : **how the model works for an individual prediction?**']",46,mjbahmani,santander-ml-explainability,mjbahmani_santander-ml-explainability_m46
"[""If you look carefully at the code where we created the SHAP values, you'll notice we reference Trees in  **shap.TreeExplainer(my_model)**. But the SHAP package has explainers for every type of model.\n"", '\n', '1. shap.DeepExplainer works with Deep Learning models.\n', '1. shap.KernelExplainer works with all models, though it is slower than other Explainers and it offers an approximation rather than exact Shap values.']",47,mjbahmani,santander-ml-explainability,mjbahmani_santander-ml-explainability_m47
"[' <a id=""6""></a> <br>\n', '# 6- Model Development\n', ""So far, we have used two  models, and at this point we add another model and we'll be expanding it soon.\n"", 'in this section you will see following model:\n', '1. lightgbm\n', '1. RandomForestClassifier\n', '1. DecisionTreeClassifier\n', '1. CatBoostClassifier']",48,mjbahmani,santander-ml-explainability,mjbahmani_santander-ml-explainability_m48
['## 6-1 lightgbm'],49,mjbahmani,santander-ml-explainability,mjbahmani_santander-ml-explainability_m49
"[' <a id=""62""></a> <br>\n', '## 6-2 RandomForestClassifier']",50,mjbahmani,santander-ml-explainability,mjbahmani_santander-ml-explainability_m50
"[' <a id=""63""></a> <br>\n', '## 6-3 DecisionTreeClassifier']",51,mjbahmani,santander-ml-explainability,mjbahmani_santander-ml-explainability_m51
"[' <a id=""64""></a> <br>\n', '## 6-4 CatBoostClassifier']",52,mjbahmani,santander-ml-explainability,mjbahmani_santander-ml-explainability_m52
['Now you can change your model and submit the results of other models.'],53,mjbahmani,santander-ml-explainability,mjbahmani_santander-ml-explainability_m53
"[' <a id=""65""></a> <br>\n', '## 6-5 Funny Combine ']",54,mjbahmani,santander-ml-explainability,mjbahmani_santander-ml-explainability_m54
"['you can follow me on:\n', '> ###### [ GitHub](https://github.com/mjbahmani/)\n', '> ###### [Kaggle](https://www.kaggle.com/mjbahmani/)\n', '\n', "" <b>I hope you find this kernel helpful and some <font color='red'>UPVOTES</font> would be very much appreciated.<b/>\n"", ' ']",55,mjbahmani,santander-ml-explainability,mjbahmani_santander-ml-explainability_m55
"[' <a id=""7""></a> <br>\n', '# 7- References & credits\n', 'Thanks fo following kernels that help me to create this kernel.']",56,mjbahmani,santander-ml-explainability,mjbahmani_santander-ml-explainability_m56
"['1. [https://www.kaggle.com/dansbecker/permutation-importance](https://www.kaggle.com/dansbecker/permutation-importance)\n', '1. [https://www.kaggle.com/dansbecker/partial-plots](https://www.kaggle.com/dansbecker/partial-plots)\n', '1. [https://www.kaggle.com/miklgr500/catboost-with-gridsearch-cv](https://www.kaggle.com/miklgr500/catboost-with-gridsearch-cv)\n', '1. [https://www.kaggle.com/dromosys/sctp-working-lgb](https://www.kaggle.com/dromosys/sctp-working-lgb)\n', '1. [https://www.kaggle.com/gpreda/santander-eda-and-prediction](https://www.kaggle.com/gpreda/santander-eda-and-prediction)\n', '1. [https://www.kaggle.com/dansbecker/permutation-importance](https://www.kaggle.com/dansbecker/permutation-importance)\n', '1. [https://www.kaggle.com/dansbecker/partial-plots](https://www.kaggle.com/dansbecker/partial-plots)\n', '1. [https://www.kaggle.com/dansbecker/shap-values](https://www.kaggle.com/dansbecker/shap-values)\n', '1. [https://docs.microsoft.com/en-us/azure/machine-learning/studio/algorithm-choice](https://docs.microsoft.com/en-us/azure/machine-learning/studio/algorithm-choice)\n', '1. [kernel https://www.kaggle.com/arjanso/reducing-dataframe-memory-size-by-65](kernel https://www.kaggle.com/arjanso/reducing-dataframe-memory-size-by-65)\n', '1. [https://www.kaggle.com/brandenkmurray/nothing-works](https://www.kaggle.com/brandenkmurray/nothing-works)']",57,mjbahmani,santander-ml-explainability,mjbahmani_santander-ml-explainability_m57
"['# _**🌹🏆RoseGold 🏆🌹**_\n', '\n', 'Contents:\n', '1. Exploratory Data Analysis\n', '2. Principle Components Analysis\n', '3. Build LightGBM model']",0,morenoh149,rosegold-eda-pca-and-lightgbm-baseline,morenoh149_rosegold-eda-pca-and-lightgbm-baseline_m0
['We have class imbalanced class problem.'],1,morenoh149,rosegold-eda-pca-and-lightgbm-baseline,morenoh149_rosegold-eda-pca-and-lightgbm-baseline_m1
"['## PCA\n', '\n', 'Borrowed from DataScience handbook Chapter 5\n', '\n', 'https://jakevdp.github.io/PythonDataScienceHandbook/05.09-principal-component-analysis.html']",2,morenoh149,rosegold-eda-pca-and-lightgbm-baseline,morenoh149_rosegold-eda-pca-and-lightgbm-baseline_m2
['The most descriptive feature in the dataset (component 1) is positively correlated with the target!'],3,morenoh149,rosegold-eda-pca-and-lightgbm-baseline,morenoh149_rosegold-eda-pca-and-lightgbm-baseline_m3
['## Decision Tree'],4,morenoh149,rosegold-eda-pca-and-lightgbm-baseline,morenoh149_rosegold-eda-pca-and-lightgbm-baseline_m4
['Score is 0.863 on leaderboard (lb).'],5,morenoh149,rosegold-eda-pca-and-lightgbm-baseline,morenoh149_rosegold-eda-pca-and-lightgbm-baseline_m5
"['TODO\n', '* https://jakevdp.github.io/PythonDataScienceHandbook/05.13-kernel-density-estimation.html\n', '* https://jakevdp.github.io/PythonDataScienceHandbook/05.11-k-means.html']",6,morenoh149,rosegold-eda-pca-and-lightgbm-baseline,morenoh149_rosegold-eda-pca-and-lightgbm-baseline_m6
"['**Santander Customer Transaction Prediction  \n', 'Can you identify who will make a transaction? **    \n', '![](https://bit.ly/2BJideW)  \n', 'Santander inivte fellow Kagglers to help them identify which customers will make a specific transaction in the future, irrespective of the amount of money transacted. The data provided for this competition has the same structure as the real data they have available to solve this problem.']",0,mytymohan,sct-prediction-eda-smote-lgbm,mytymohan_sct-prediction-eda-smote-lgbm_m0
"['**Analysis Playground**  \n', 'As in every data science prediction problem, I will start with Exploratory Data Analysis (EDA) and move on building model on different machine learning algorithms.']",1,mytymohan,sct-prediction-eda-smote-lgbm,mytymohan_sct-prediction-eda-smote-lgbm_m1
['**Loading the required packages for analysis**'],2,mytymohan,sct-prediction-eda-smote-lgbm,mytymohan_sct-prediction-eda-smote-lgbm_m2
['**Reading the Training and Testing Dataset**'],3,mytymohan,sct-prediction-eda-smote-lgbm,mytymohan_sct-prediction-eda-smote-lgbm_m3
"['This is really odd, as I have never come across a scenario where both the training and testing dataset have the same number of rows. Seems interesting. Here the number of features are bit higer in number. So, we will find which all variables are important based on missing values, correlation analysis etc.']",4,mytymohan,sct-prediction-eda-smote-lgbm,mytymohan_sct-prediction-eda-smote-lgbm_m4
['**Information on the training dataset**'],5,mytymohan,sct-prediction-eda-smote-lgbm,mytymohan_sct-prediction-eda-smote-lgbm_m5
"['.info() command in python give the brief glimpse of the dataset. Our traning dataset has three different types of datatypes. most of them are float which are contineous, one feature has integer which is most probably the ""Target"" column and the final one feature is of object type which i think will be the ""ID_code"" column.']",6,mytymohan,sct-prediction-eda-smote-lgbm,mytymohan_sct-prediction-eda-smote-lgbm_m6
['**Summary Statistics**'],7,mytymohan,sct-prediction-eda-smote-lgbm,mytymohan_sct-prediction-eda-smote-lgbm_m7
"['**Target Distribution**  \n', 'First let us look at the distribution of the target variable to understand whether the dataset is imbalanced or not.']",8,mytymohan,sct-prediction-eda-smote-lgbm,mytymohan_sct-prediction-eda-smote-lgbm_m8
"['From the target proportion and target pie chart its clearly evident that the target is highly imbalanced with ""0"" class occupying 90% of the target values and 10% of target values with ""1"" class.']",9,mytymohan,sct-prediction-eda-smote-lgbm,mytymohan_sct-prediction-eda-smote-lgbm_m9
"['**Missing Value Proportion**  \n', 'Now, Let us check the proportion of  many missing values in the training dataset.']",10,mytymohan,sct-prediction-eda-smote-lgbm,mytymohan_sct-prediction-eda-smote-lgbm_m10
['This Looks great as we have no missing values in the dataset.'],11,mytymohan,sct-prediction-eda-smote-lgbm,mytymohan_sct-prediction-eda-smote-lgbm_m11
"['**Correlation Coefficient Plot**  \n', ""As there are no missing values in the dataset and all the features are numberic let's try the correlation plot and see how the features are correlated to each other.""]",12,mytymohan,sct-prediction-eda-smote-lgbm,mytymohan_sct-prediction-eda-smote-lgbm_m12
"['The correlation coefficient values are very low and the maximum value is around 0.08 in negative side of the plot, with respect to positive side the maximum value is around 0.07.\n', '\n', 'Overall, the correlation of the features with respect to target are very low.\n', '\n', 'So, We will take some of the features which has high correlation values and plot the heatmap for further analysis.']",13,mytymohan,sct-prediction-eda-smote-lgbm,mytymohan_sct-prediction-eda-smote-lgbm_m13
"['Plotting heatmap is done to identify if there are any strong monotonic relationships between these important features. If the values are high, then probably we can choose to keep one of those variables in the model building process. But, we are doing this only for small set of features. we can even try other techniques to explore other features in the dataset.']",14,mytymohan,sct-prediction-eda-smote-lgbm,mytymohan_sct-prediction-eda-smote-lgbm_m14
"['Seems like none of the selected variables have spearman correlation more than 0.7 with each other.\n', '\n', 'The above plots helped us in identifying the important individual variables which are correlated with target. However we generally build many non-linear models in Kaggle competitions. So let us build some non-linear models and get variable importance from them.']",15,mytymohan,sct-prediction-eda-smote-lgbm,mytymohan_sct-prediction-eda-smote-lgbm_m15
"['**Feature Importance - eli5 library **  \n', ""For feature importance, I am going to use the Permutation Importance technique that's being used in the [tutorial](https://www.kaggle.com/dansbecker/permutation-importance)""]",16,mytymohan,sct-prediction-eda-smote-lgbm,mytymohan_sct-prediction-eda-smote-lgbm_m16
"['Interpreting Permutation Importances\n', 'The values towards the top are the most important features, and those towards the bottom matter least.\n', '\n', 'The first number in each row shows how much model performance decreased with a random shuffling (in this case, using ""accuracy"" as the performance metric).\n', '\n', 'Like most things in data science, there is some randomness to the exact performance change from a shuffling a column. We measure the amount of randomness in our permutation importance calculation by repeating the process with multiple shuffles. The number after the ± measures how performance varied from one-reshuffling to the next.\n', '\n', ""You'll occasionally see negative values for permutation importances. In those cases, the predictions on the shuffled (or noisy) data happened to be more accurate than the real data. This happens when the feature didn't matter (should have had an importance close to 0), but random chance caused the predictions on shuffled data to be more accurate. This is more common with small datasets, like the one in this example, because there is more room for luck/chance.\n"", '\n', 'In our case, the top 10 most important feature are var_81, var_53, var_139, var_179, var_174, var_40, var_26, var_13, var_24 and var_109. But, Still all the features seems to have value importance close to zero.']",17,mytymohan,sct-prediction-eda-smote-lgbm,mytymohan_sct-prediction-eda-smote-lgbm_m17
"['**SMOTE Over-Sampling**  \n', ""As we have more records for target '0', I am going to over sample the target '1' to the same level as target '0' which is basically oversampling the least class.""]",18,mytymohan,sct-prediction-eda-smote-lgbm,mytymohan_sct-prediction-eda-smote-lgbm_m18
"['**Time for Modelling**  \n', '**LGBM**  \n', ""Let's try with Lightgbm and see the accuracy.""]",19,mytymohan,sct-prediction-eda-smote-lgbm,mytymohan_sct-prediction-eda-smote-lgbm_m19
['**Submission File**'],20,mytymohan,sct-prediction-eda-smote-lgbm,mytymohan_sct-prediction-eda-smote-lgbm_m20
"['Decision trees and neural nets have trouble classifying examples when trained on imbalanced data. This kernel will explore a wide range of resampling techniques, how they vary, and their effect on XGBoost.\n', '\n', '### Contents\n', '1. [Introduction](#introduction)\n', '2. [Table of techniques](#technique-table)\n', '    1. [Random Undersampling](#random-undersampling)\n', '    2. [Tomek Links](#tomek-links)\n', '    3. [AllKNN](#allknn)\n', '    4. [Edited Nearest Neighbor](#enn)\n', '    5. [Random Oversampling](#random-oversampling)\n', '    6. [ADASYN](#adasyn)\n', '    7. [SMOTE](#smote)\n', '    2. [SMOTETOMEK](#smotetomek)\n', '    2. [SMOTEENN](#smoteenn)\n', '3. [Training XGBoost](#training-xgboost)\n', '4. [Conclusion](#conclusion)']",0,nholloway,comparing-resampling-techniques-and-xgboost,nholloway_comparing-resampling-techniques-and-xgboost_m0
"[""<a id='introduction'></a>\n"", '# Target Class Imbalance\n', '---\n', ""In the Santander customer transaction prediction data we have a binary target variable where 1 is a successful future transaction and 0 is no future transaction. The problem is that we have an imbalance of about 7:1. If we train on this data we are likely to have a model that will missclassify the minority class, 'yes', because it has seen so few examples. \n"", '\n', 'To deal with class imbalance we can resample. Resampling can mean that we oversample a minority class or undersample a majority class to introduce bias to select a more even distribution of classes. Class imbalance is something we will regularly see in tasks like network intrusion, rare disease diagnosing, and fraud detection. ']",1,nholloway,comparing-resampling-techniques-and-xgboost,nholloway_comparing-resampling-techniques-and-xgboost_m1
"[""<a id='technique-table'></a>\n"", '# Resampling Techniques\n', '---\n', '### Undersampling Techniques \n', '1. Random Undersampling\n', '2. Tomek Links\n', '3. AllKNN\n', '4. ENN (Edited Nearest Neighbours)\n', '\n', '### Oversampling Techniques\n', '1. Random Oversampling\n', '2. ADASYN (Adaptive Synthetic Sampling)\n', '3. SMOTE (Synthetic Minority Over-Sampling Technique)\n', '\n', '### Combined Resampling\n', '1. SMOTETomek\n', '2. SMOTEENN']",2,nholloway,comparing-resampling-techniques-and-xgboost,nholloway_comparing-resampling-techniques-and-xgboost_m2
"[""<a id='random-undersampling'></a>\n"", '### Random Undersampling\n', ""The simplest form of undersampling is to remove random records from the majority class. With imblearn's implementation we can choose to remove samples with or without replacement. The biggest drawback to this form of undersampling is loss of information.""]",3,nholloway,comparing-resampling-techniques-and-xgboost,nholloway_comparing-resampling-techniques-and-xgboost_m3
"[""<a id='tomek-links'></a>\n"", '### Tomek Links\n', 'Tomek links can be used as an under-sampling method or as a data cleaning method. A Tomek link is any place where two samples of different classes are nearest neighbors. When we find a Tomek link we can choose which observatin to delete- in undersampling we remove the majority class. \n', '\n', 'The difference between the data before and after Tomek links is subtle but clear- Tomek links is a great technique we can use to clear up our boundaries in classificatino problems. ']",4,nholloway,comparing-resampling-techniques-and-xgboost,nholloway_comparing-resampling-techniques-and-xgboost_m4
"[""<a id='allknn'></a>\n"", '### AllKNN\n', 'AllKNN is a method also created by the Ivan Tomek that deletes an object if a KNN classifier misclassifies it. In imblearn the default value of k is 3, but we can also pass a value. In the below cell its worth passing different values to `n_neighbors`. AllKNN tends to delete more datapoints than ENN, especially as the value of k increases. I think that it undersamples too haphazardly. ']",5,nholloway,comparing-resampling-techniques-and-xgboost,nholloway_comparing-resampling-techniques-and-xgboost_m5
"[""<a id='enn'></a>\n"", '### ENN (Edited Nearest Neighbours)\n', 'ENN removes examples whose class label differs from the class of at least half of its k nearest neighbors. The benefit of ENN is that we can remove examples of the majority class while retaining as much information as possible because we are only removing redundant observations. ']",6,nholloway,comparing-resampling-techniques-and-xgboost,nholloway_comparing-resampling-techniques-and-xgboost_m6
"[""<a id='random-oversampling'></a>\n"", '### Random Oversampling\n', 'The simplest implementation of oversampling is to duplicate random records from the minority class, this can cause overfitting. ']",7,nholloway,comparing-resampling-techniques-and-xgboost,nholloway_comparing-resampling-techniques-and-xgboost_m7
"[""<a id='adasyn'></a>\n"", '### ADASYN (Adaptive Synthetic Sampling)\n', 'ADASYN adaptively generates samples next to original observations which are wrongly classified by a KNN classifier. Unlike SMOTE that generates new samples that lie inside the class boundary, ADASYN tends to generate new samples near existing outliers. \n', '\n', 'You can run these code cells with different data samples to see how  ADASYN tends to change the data distribution, but especially in contrast to SMOTE we can see how it tends to constuct points on the frontier of our existing data. ']",8,nholloway,comparing-resampling-techniques-and-xgboost,nholloway_comparing-resampling-techniques-and-xgboost_m8
"[""<a id='smote'></a>\n"", '### SMOTE (Synthetic Minority Over-Sampling Technique)\n', 'SMOTE synthesizes new examples by interpolating existing observations. SMOTE begins by iterating over every minority class instace and choosing its k nearest neighbors. The algorithm then constructs new instances halfway between the chosen obervations and its k neighbors. The greatest limitation of SMOTE is that it can only construct examples within the body of observations, never outside. If we compare the rebalanced data in the SMOTE plot against the plot for ADASYN we can see this exact effect. \n', '\n', 'SMOTE has several variants like SVMSMOTE and BorderlineSMOTE.']",9,nholloway,comparing-resampling-techniques-and-xgboost,nholloway_comparing-resampling-techniques-and-xgboost_m9
"[""<a id='smotetomek'></a>\n"", '### SMOTETomek\n', 'SMOTETomek is the combination of using Tomek links to undersample the majoirty class and the use of SMOTE to oversample the minority class. ']",10,nholloway,comparing-resampling-techniques-and-xgboost,nholloway_comparing-resampling-techniques-and-xgboost_m10
"[""<a id='smoteenn'></a>\n"", '### SMOTEENN\n', 'SMOTEENN is the combination of SMOTE and Edited Nearest Neighbor. ENN removes any example whose class label differs from the class label of at least two of its three nearest neighbors. ENN tends to remove more examples then the Tomek links. \n', '\n', ""There's a really interesting difference here between SMOTETomek and SMOTEENN. There are so few minority class examples that Tomek Links are not nearly as effective at undersampling the majority class. If we we're using a built in method we could first perform SMOTE and then perform the Tomek Links step but ""]",11,nholloway,comparing-resampling-techniques-and-xgboost,nholloway_comparing-resampling-techniques-and-xgboost_m11
"[""<a id='training-xgboost'></a>\n"", '# XGBoost with Unbalanced Data\n', 'Now that we know about strategies to deal with unbalanced data, here is the effect of unbalanced data on our gradient boosted random forests. \n', '\n', 'We will fit XGBoost models with Bayesian hyperparameter optimization to two datasets, first our unbalanced dataset- and second, a dataset that we have balanced with Smotetomek. The hyperparameter optimization library Hyperopt is great because it will do the optimization for us if we pass (1) a hyperparameter feature space, (2) an objective function that fits the model and returns a score to minimize, and (3) a `Trials` object that we can store arbitary data in from the model training. ']",12,nholloway,comparing-resampling-techniques-and-xgboost,nholloway_comparing-resampling-techniques-and-xgboost_m12
"[""<a id='conclusion'></a>\n"", '# Imbalanced Dataset Conclusion\n', '---\n', 'In the confusion matrix we can see how much more often the imbalanced dataset correctly identifies class one observations as class one- the bottom right hand square. \n', '\n', 'The dropoff in accuracy between the two sets is a little over 5% depending on how the sample distribution shakes out, and the test auc scores tends to be about 10% different. This is the difference between a 2:1 target label imbalance and 9:1. \n', '\n', 'Imbalances in trees can have a significant effect on our classification power. While we focus on target label imbalance here, imbalances in the distributions of values and classes is an important topic in tree based models. If we go back and look at the charts of points that we use as examples we can see how tightly grouped points of two different classes can be- the job of XGBoost is to be able to disambiguate between these points and and that requires robust data. Imabalnce problems dont just have to be in the class of target label, we can face imbalances where there are two few examples of one category in a categorical variable, outliers in the distribution of a numerical variable, and imbalances between train and test sets. \n', '\n', 'Something you should try on your own is rerunning this kernel and seeing how a technique like ADASYN produces a different model than the SMOTE-based technique we used here. ']",13,nholloway,comparing-resampling-techniques-and-xgboost,nholloway_comparing-resampling-techniques-and-xgboost_m13
['# Load libraries and Functions'],0,nickycan,compress-70-of-dataset,nickycan_compress-70-of-dataset_m0
"['# Compress dataset \n', 'my computer has a small ram, and everytime I join a competition, the first thing I will do is to compress my dataset, after compressing, the size of the dataset drop from 619MB to 158MB (70%!!!), you can even set some unsigned datatypes, and the size of your dataset will be smaller. Hope you guys enjoy this competition!']",1,nickycan,compress-70-of-dataset,nickycan_compress-70-of-dataset_m1
"[""I have decided to see if there are any outliers in the dataset according to [Chauvenet's criterion](https://en.wikipedia.org/wiki/Chauvenet%27s_criterion)\n"", '\n', 'An observation $P_i$ is an outlier if the following equation is true:\n', '$$erfc\\Bigg(\\frac{|P_i - \\bar{P}|}{S_p}\\Bigg) < \\frac{1}{2n}$$\n', '\n', '$$erfc(x) = \\frac{2}{\\sqrt{\\pi}} \\int_x^\\infty \\mathrm{e}^{-t^2}\\;\\mathrm{d}t$$']",0,nroman,detecting-outliers-with-chauvenet-s-criterion,nroman_detecting-outliers-with-chauvenet-s-criterion_m0
['Plots with a distribution for 5 top features before removing an outliers and after removing them.'],1,nroman,detecting-outliers-with-chauvenet-s-criterion,nroman_detecting-outliers-with-chauvenet-s-criterion_m1
"['Big thanks to Jiwei Liu for Augment insight!\n', 'https://www.kaggle.com/jiweiliu/lgb-2-leaves-augment\n']",0,omgrodas,lightgbm-with-data-augmentation,omgrodas_lightgbm-with-data-augmentation_m0
"[""Forked from @VisheshShrivastav. Using the basic framework from vishesh's Kernel ""]",0,qqgeogor,keras-nn-mixup,qqgeogor_keras-nn-mixup_m0
['## Data Load and Exploration'],0,quanghm,fastai-1-0-tabular-learner-with-ensemble,quanghm_fastai-1-0-tabular-learner-with-ensemble_m0
"['## Normalize Data\n', 'We could let FastAI normalize the data automatically, but we choose to do so manually for more flexibility']",1,quanghm,fastai-1-0-tabular-learner-with-ensemble,quanghm_fastai-1-0-tabular-learner-with-ensemble_m1
"['## Feature Engineering\n', ""We don't really do anything special but polynomial features""]",2,quanghm,fastai-1-0-tabular-learner-with-ensemble,quanghm_fastai-1-0-tabular-learner-with-ensemble_m2
['## Split training data into train and validation sets'],3,quanghm,fastai-1-0-tabular-learner-with-ensemble,quanghm_fastai-1-0-tabular-learner-with-ensemble_m3
['Grab a statistic summary of the training set. We may use this later in adding noises to the data during training'],4,quanghm,fastai-1-0-tabular-learner-with-ensemble,quanghm_fastai-1-0-tabular-learner-with-ensemble_m4
"['## FastAI Tabular Learner\n', 'We start off with the default learner from FastAI']",5,quanghm,fastai-1-0-tabular-learner-with-ensemble,quanghm_fastai-1-0-tabular-learner-with-ensemble_m5
"['First we want to find the correct learning rate for this dataset/problem. This only needs to run once.\n', 'The *optimal* learning rate found is 0.01']",6,quanghm,fastai-1-0-tabular-learner-with-ensemble,quanghm_fastai-1-0-tabular-learner-with-ensemble_m6
"['This is the main train and evaluate function. Since we are training multiple learners, we choose to save the model to harddisk and load them later if needed.']",7,quanghm,fastai-1-0-tabular-learner-with-ensemble,quanghm_fastai-1-0-tabular-learner-with-ensemble_m7
['## Visualize ROC on the Validation Set'],8,quanghm,fastai-1-0-tabular-learner-with-ensemble,quanghm_fastai-1-0-tabular-learner-with-ensemble_m8
['## Test and Submit'],9,quanghm,fastai-1-0-tabular-learner-with-ensemble,quanghm_fastai-1-0-tabular-learner-with-ensemble_m9
['**Data Visualization**'],0,rajeshcv,understanding-features,rajeshcv_understanding-features_m0
"['Visualizing Data is a very important step in a Data Science project.  As per the recent Kagagle Survey 10-20% of the total Data Science Project time is spent on visualizing data. https://www.kaggle.com/rajeshcv/state-of-data-science-machine-learning-2018\n', '\n', 'SaS Data Visualization’s webpage explain Data visualization  beautifully.\n', '\n', ""*'The way the human brain processes information, using charts or graphs to visualize large amounts of complex data is easier than poring over spreadsheets or reports. Data visualization is a quick, easy way to convey concepts in a universal manner\u200a—\u200aand you can experiment with different scenarios by making slight adjustments.'*\n"", '\n', 'In the Santander Customer Transaction Prediction competition the features are predominently numeric.\n', '\n', ""This kernel's objective is to\n"", '*     Understand the value distribution in various features through boxplots and histograms. \n', '*     Seggregate features into groups based on range of values.\n', '*     Identify  features with similiar value distribution.\n', ""*     To understand if there is any difference in values between the two target groups 'transaction done and 'transaction not done'\n"", '*  Check whether feature values in test and train comes from the same sampling  distribution.\n', '\n', '\n']",1,rajeshcv,understanding-features,rajeshcv_understanding-features_m1
"[""All the 200 features have numeric values . Let's check first if some of these numeric features are categorical  or boolean .In that case they will have less than 500 unique values.""]",2,rajeshcv,understanding-features,rajeshcv_understanding-features_m2
"[""None of the features are categorical or boolean except 'target'. Let's understand the range of values of each of these features by plotting the max, min and median value of the features.""]",3,rajeshcv,understanding-features,rajeshcv_understanding-features_m3
['Combined all values in the features are between 80 and -90.  From the plot it looks like the features can be seggregrated into 10 groups  based on their max and min values.'],4,rajeshcv,understanding-features,rajeshcv_understanding-features_m4
['**Features with  positive values and maximum value less than 10**'],5,rajeshcv,understanding-features,rajeshcv_understanding-features_m5
"['var_68,var_91,var_103,var_148 and var_161 have comparatively lower range of values .\n', 'The histograms below shows the distribution of values in cases of transaction done in green color (target=1) and transaction not done (target=0) in red colour.']",6,rajeshcv,understanding-features,rajeshcv_understanding-features_m6
"['var_103 values lie  between 1.1 and 2 , var_148 between 3.4 and 4.6 , var_68 values are in a narrow range between 4.99 and 5.04,   var_161 between 5 and 6.2  &  var_91 between 6.6 and 7.4.  Considering var_166 with values between 2 and 4 and var_169 and var_133  they all appear to be in sequence.\n', '\n', 'However there is no significant difference in values between the ""transaction done"" and ""transaction not done"" groups']",7,rajeshcv,understanding-features,rajeshcv_understanding-features_m7
['**Features with  positive values and maximum value between 10 & 20**'],8,rajeshcv,understanding-features,rajeshcv_understanding-features_m8
"['var_12,  var_15 ,var_25, var_34,  var_43, var_108, var_125 have very low range of values further elaborated by the histogram below.']",9,rajeshcv,understanding-features,rajeshcv_understanding-features_m9
['All those variables with a short range of values have values in the range 10 to 15 and as in the earlier group appear to be in some sequence.'],10,rajeshcv,understanding-features,rajeshcv_understanding-features_m10
['**Features with  positive values and maximum value greater than 20**'],11,rajeshcv,understanding-features,rajeshcv_understanding-features_m11
"['var_85, var_194 and  var_198 appear to have similiar distribution of values.  var_0, var_46 , var_56, var_175 and var_177 also appear to have a similiar value distribution.']",12,rajeshcv,understanding-features,rajeshcv_understanding-features_m12
['**Features with  values between 10 and -10**'],13,rajeshcv,understanding-features,rajeshcv_understanding-features_m13
['**Features with  max value between 10 &  20  and min values between 0 & -10**'],14,rajeshcv,understanding-features,rajeshcv_understanding-features_m14
"[""From the above histogram for many of the features the ' transaction done'  group in green seems to have  lower range than the 'transaction not done' group in red.""]",15,rajeshcv,understanding-features,rajeshcv_understanding-features_m15
['**Features with  max value between 10 &  20  and min values between -10 & -20**'],16,rajeshcv,understanding-features,rajeshcv_understanding-features_m16
"['var_39,var_65 and var_138 appear to have similiar distribution of values and so is var_63 and var_128\n', ""For some of the features the ' transaction done'  group in green seems to have  lower range than the 'transaction not done' group in red.""]",17,rajeshcv,understanding-features,rajeshcv_understanding-features_m17
['**Features with  max value between 10 &  20  and min values less than  -20**'],18,rajeshcv,understanding-features,rajeshcv_understanding-features_m18
"['var_84, var _155, var_157 appear to have similiar distribution of values and so does  var_11, var_180 & var_185']",19,rajeshcv,understanding-features,rajeshcv_understanding-features_m19
['**Features with  max value greater than 20  and min values less than  -20**'],20,rajeshcv,understanding-features,rajeshcv_understanding-features_m20
"['(var_73 & var_158) , (var_92,var_154,var_159 & var_163) , ( var_20 & var_55) \n', ""The features within the above groups  appear to have similiar distribution of values . Here also the range  of values for the *'transaction done'* group in green appears to be shorter.""]",21,rajeshcv,understanding-features,rajeshcv_understanding-features_m21
['**Features with  max value more than 20 and min values between -10 and -20**'],22,rajeshcv,understanding-features,rajeshcv_understanding-features_m22
['var_21 & var_172 appear to have similiar distribution of values'],23,rajeshcv,understanding-features,rajeshcv_understanding-features_m23
['**Features with  max value more than 20 and min values less than -20**'],24,rajeshcv,understanding-features,rajeshcv_understanding-features_m24
['var_47 & var_187 appear to have similiar distribution of values'],25,rajeshcv,understanding-features,rajeshcv_understanding-features_m25
['**Checking for correlation between feature**s'],26,rajeshcv,understanding-features,rajeshcv_understanding-features_m26
['***No correlation between any features . Does this mean all features are important?***'],27,rajeshcv,understanding-features,rajeshcv_understanding-features_m27
['**Kolmogorov-Smirnov test**'],28,rajeshcv,understanding-features,rajeshcv_understanding-features_m28
"[""Before concluding let's do a check of whether feature values in test and train comes from the same sampling  distribution.\n"", 'Kolmogorov-Smirnov  is a two-sided test for the null hypothesis that 2 independent samples are drawn from the same continuous distribution.\n', 'If the K-S statistic is small or the p-value is high, then we cannot reject the hypothesis that the distributions of the two samples are the same.']",29,rajeshcv,understanding-features,rajeshcv_understanding-features_m29
"[""For the two tailed test at 95% confidence level the pvalue has to be less than 0.05 to reject the null hypothesis that both samples are from same distribution.Let's look for values less than 0.05""]",30,rajeshcv,understanding-features,rajeshcv_understanding-features_m30
"['As per the Kolmogorov-Smirnov test 46 features have a high probability of not being from the same sampling distribution.\n', 'Will this affect the models? \n', ""Let's combine the test and train data to compare these features and understand their didtribution in train and test.""]",31,rajeshcv,understanding-features,rajeshcv_understanding-features_m31
"['<h1><center><font size=""6"">Santander EDA, PCA and Light GBM Classification Model</font></center></h1>\n', '\n', '<img src=""https://upload.wikimedia.org/wikipedia/commons/thumb/4/4a/Another_new_Santander_bank_-_geograph.org.uk_-_1710962.jpg/640px-Another_new_Santander_bank_-_geograph.org.uk_-_1710962.jpg""></img>\n', '\n', '<br>\n', '<b>\n', 'In this challenge, Santander invites Kagglers to help them identify which customers will make a specific transaction in the future, irrespective of the amount of money transacted. The data provided for this competition has the same structure as the real data they have available to solve this problem. \n', 'The data is anonimyzed, each row containing 200 numerical values identified just with a number.</b>\n', '\n', ""<b>Inspired by Jiwei Liu's Kernel. I added Data Augmentation Segment to my kernel</b>\n"", '\n', '<pre>\n', ""<a id='0'><b>Content</b></a>\n"", ""- <a href='#1'><b>Import the Data</b></a>\n"", ""- <a href='#11'><b>Data Exploration</b></a>  \n"", ""- <a href='#2'><b>Check for the missing values</b></a>  \n"", ""- <a href='#3'><b>Visualizing the Satendar Customer Transactions Data</b></a>   \n"", "" - <a href='#31'><b>Check for Class Imbalance</b></a>   \n"", "" - <a href='#32'><b>Distribution of Mean and Standard Deviation</b></a>   \n"", "" - <a href='#33'><b>Distribution of Skewness</b></a>   \n"", "" - <a href='#34'><b>Distribution of Kurtosis</b></a>   \n"", ""- <a href='#4'><b>Principal Component Analysis</b></a>\n"", "" - <a href='#41'><b>Kernel PCA</b></a>\n"", '- <a href = ""#16""><b>Data Augmentation</b></a>\n', ""- <a href='#6'><b>Build the Light GBM Model</b></a></pre>""]",0,roydatascience,eda-pca-lgbm-santander-transactions,roydatascience_eda-pca-lgbm-santander-transactions_m0
['<a id=1><pre><b>Import the Data</b></pre></a>'],1,roydatascience,eda-pca-lgbm-santander-transactions,roydatascience_eda-pca-lgbm-santander-transactions_m1
['<a id=11><pre><b>Data Exploration</b></pre></a>'],2,roydatascience,eda-pca-lgbm-santander-transactions,roydatascience_eda-pca-lgbm-santander-transactions_m2
['<a id=2><b><pre>Check for the Missing Values.</pre></b></a> '],3,roydatascience,eda-pca-lgbm-santander-transactions,roydatascience_eda-pca-lgbm-santander-transactions_m3
['<pre>There are no missing values in the dataset</pre>'],4,roydatascience,eda-pca-lgbm-santander-transactions,roydatascience_eda-pca-lgbm-santander-transactions_m4
['<pre><a id = 3><b>Visualizing the Satendar Customer Transactions Data</b></a></pre>'],5,roydatascience,eda-pca-lgbm-santander-transactions,roydatascience_eda-pca-lgbm-santander-transactions_m5
['<pre><a id = 31 ><b>Check for Class Imbalance</b></a></pre>'],6,roydatascience,eda-pca-lgbm-santander-transactions,roydatascience_eda-pca-lgbm-santander-transactions_m6
"['<pre><a id = 32 ><b>Distribution of Mean and Standard Deviation</b></a></pre>\n', '\n', '<pre>EDA Reference : https://www.kaggle.com/gpreda/santander-eda-and-prediction</pre>']",7,roydatascience,eda-pca-lgbm-santander-transactions,roydatascience_eda-pca-lgbm-santander-transactions_m7
"[""<pre>Let's check the distribution of the mean of values per columns in the train and test datasets.</pre>""]",8,roydatascience,eda-pca-lgbm-santander-transactions,roydatascience_eda-pca-lgbm-santander-transactions_m8
['<pre>Distribution for Standard Deviation</pre>'],9,roydatascience,eda-pca-lgbm-santander-transactions,roydatascience_eda-pca-lgbm-santander-transactions_m9
"[""<pre>Let's check the distribution of the standard deviation of values per columns in the train and test datasets.</pre>""]",10,roydatascience,eda-pca-lgbm-santander-transactions,roydatascience_eda-pca-lgbm-santander-transactions_m10
"[""<pre>Let's check now the distribution of the mean value per row in the train dataset, grouped by value of target</pre>""]",11,roydatascience,eda-pca-lgbm-santander-transactions,roydatascience_eda-pca-lgbm-santander-transactions_m11
"[""<pre>Let's check now the distribution of the mean values per columns in the train and test datasets.</pre>""]",12,roydatascience,eda-pca-lgbm-santander-transactions,roydatascience_eda-pca-lgbm-santander-transactions_m12
"[""<pre>Let's check now the distribution of the standard deviation  per row in the train dataset, grouped by value of target</pre>""]",13,roydatascience,eda-pca-lgbm-santander-transactions,roydatascience_eda-pca-lgbm-santander-transactions_m13
"[""<pre>Let's check now the distribution of standard deviation per columns in the train and test datasets.</pre>""]",14,roydatascience,eda-pca-lgbm-santander-transactions,roydatascience_eda-pca-lgbm-santander-transactions_m14
"['<pre><a id = 33 ><b>Distribution of Skewness</b></a></pre>\n', '\n', ""<pre>Let's see now the distribution of skewness on rows in train separated for values of target 0 and 1. We found the distribution is left skewed</pre>""]",15,roydatascience,eda-pca-lgbm-santander-transactions,roydatascience_eda-pca-lgbm-santander-transactions_m15
"[""<pre>Let's see now the distribution of skewness on columns in train separated for values of target 0 and 1.</pre>""]",16,roydatascience,eda-pca-lgbm-santander-transactions,roydatascience_eda-pca-lgbm-santander-transactions_m16
['<pre><a id = 34 ><b>Distribution of Kurtosis</b></a></pre>'],17,roydatascience,eda-pca-lgbm-santander-transactions,roydatascience_eda-pca-lgbm-santander-transactions_m17
"[""<pre>Let's see now the distribution of kurtosis on rows in train separated for values of target 0 and 1. We found the distribution to be Leptokurtic</pre>""]",18,roydatascience,eda-pca-lgbm-santander-transactions,roydatascience_eda-pca-lgbm-santander-transactions_m18
"[""<pre>Let's see now the distribution of kurtosis on columns in train separated for values of target 0 and 1.</pre>""]",19,roydatascience,eda-pca-lgbm-santander-transactions,roydatascience_eda-pca-lgbm-santander-transactions_m19
['<a id=4><pre><b>Principal Component Analysis to check Dimentionality Reduction<b></pre></a>'],20,roydatascience,eda-pca-lgbm-santander-transactions,roydatascience_eda-pca-lgbm-santander-transactions_m20
"[""<pre><a id = 41><b>Kernel PCA (Since the Graph above doesn't represent meaningful analysis)</b></a></pre>""]",21,roydatascience,eda-pca-lgbm-santander-transactions,roydatascience_eda-pca-lgbm-santander-transactions_m21
"[""<pre>Since PCA hasn't been useful, I decided to proceed with the existing dataset</pre>""]",22,roydatascience,eda-pca-lgbm-santander-transactions,roydatascience_eda-pca-lgbm-santander-transactions_m22
['<pre><a id = 16><b>Data Augmentation</b></a></pre>'],23,roydatascience,eda-pca-lgbm-santander-transactions,roydatascience_eda-pca-lgbm-santander-transactions_m23
['<pre><a id = 6><b>Build the Light GBM Model</b></a></pre>'],24,roydatascience,eda-pca-lgbm-santander-transactions,roydatascience_eda-pca-lgbm-santander-transactions_m24
"['<h1><center><font size=""6"">Santander EDA, PCA and Light GBM Classification Model</font></center></h1>\n', '\n', '<img src=""https://upload.wikimedia.org/wikipedia/commons/thumb/4/4a/Another_new_Santander_bank_-_geograph.org.uk_-_1710962.jpg/640px-Another_new_Santander_bank_-_geograph.org.uk_-_1710962.jpg""></img>\n', '\n', '<br>\n', '<b>\n', 'In this challenge, Santander invites Kagglers to help them identify which customers will make a specific transaction in the future, irrespective of the amount of money transacted. The data provided for this competition has the same structure as the real data they have available to solve this problem. \n', 'The data is anonimyzed, each row containing 200 numerical values identified just with a number.</b>\n', '\n', ""<b>Inspired by Jiwei Liu's Kernel. I added Data Augmentation Segment to my kernel</b>\n"", '\n', '<pre>\n', ""<a id='0'><b>Content</b></a>\n"", ""- <a href='#1'><b>Import the Data</b></a>\n"", ""- <a href='#11'><b>Data Exploration</b></a>  \n"", ""- <a href='#2'><b>Check for the missing values</b></a>  \n"", ""- <a href='#3'><b>Visualizing the Satendar Customer Transactions Data</b></a>   \n"", "" - <a href='#31'><b>Check for Class Imbalance</b></a>   \n"", "" - <a href='#32'><b>Distribution of Mean and Standard Deviation</b></a>   \n"", "" - <a href='#33'><b>Distribution of Skewness</b></a>   \n"", "" - <a href='#34'><b>Distribution of Kurtosis</b></a>   \n"", ""- <a href='#4'><b>Principal Component Analysis</b></a>\n"", "" - <a href='#41'><b>Kernel PCA</b></a>\n"", '- <a href = ""#16""><b>Data Augmentation</b></a>\n', ""- <a href='#6'><b>Build the Light GBM Model</b></a></pre>""]",0,roydatascience,eda-pca-simple-lgbm-on-kfold-technique,roydatascience_eda-pca-simple-lgbm-on-kfold-technique_m0
['<a id=1><pre><b>Import the Data</b></pre></a>'],1,roydatascience,eda-pca-simple-lgbm-on-kfold-technique,roydatascience_eda-pca-simple-lgbm-on-kfold-technique_m1
['<a id=11><pre><b>Data Exploration</b></pre></a>'],2,roydatascience,eda-pca-simple-lgbm-on-kfold-technique,roydatascience_eda-pca-simple-lgbm-on-kfold-technique_m2
['<a id=2><b><pre>Check for the Missing Values.</pre></b></a> '],3,roydatascience,eda-pca-simple-lgbm-on-kfold-technique,roydatascience_eda-pca-simple-lgbm-on-kfold-technique_m3
['<pre>There are no missing values in the dataset</pre>'],4,roydatascience,eda-pca-simple-lgbm-on-kfold-technique,roydatascience_eda-pca-simple-lgbm-on-kfold-technique_m4
['<pre><a id = 3><b>Visualizing the Satendar Customer Transactions Data</b></a></pre>'],5,roydatascience,eda-pca-simple-lgbm-on-kfold-technique,roydatascience_eda-pca-simple-lgbm-on-kfold-technique_m5
['<pre><a id = 31 ><b>Check for Class Imbalance</b></a></pre>'],6,roydatascience,eda-pca-simple-lgbm-on-kfold-technique,roydatascience_eda-pca-simple-lgbm-on-kfold-technique_m6
"['<pre><a id = 32 ><b>Distribution of Mean and Standard Deviation</b></a></pre>\n', '\n', '<pre>EDA Reference : https://www.kaggle.com/gpreda/santander-eda-and-prediction</pre>']",7,roydatascience,eda-pca-simple-lgbm-on-kfold-technique,roydatascience_eda-pca-simple-lgbm-on-kfold-technique_m7
"[""<pre>Let's check the distribution of the mean of values per columns in the train and test datasets.</pre>""]",8,roydatascience,eda-pca-simple-lgbm-on-kfold-technique,roydatascience_eda-pca-simple-lgbm-on-kfold-technique_m8
['<pre>Distribution for Standard Deviation</pre>'],9,roydatascience,eda-pca-simple-lgbm-on-kfold-technique,roydatascience_eda-pca-simple-lgbm-on-kfold-technique_m9
"[""<pre>Let's check the distribution of the standard deviation of values per columns in the train and test datasets.</pre>""]",10,roydatascience,eda-pca-simple-lgbm-on-kfold-technique,roydatascience_eda-pca-simple-lgbm-on-kfold-technique_m10
"[""<pre>Let's check now the distribution of the mean value per row in the train dataset, grouped by value of target</pre>""]",11,roydatascience,eda-pca-simple-lgbm-on-kfold-technique,roydatascience_eda-pca-simple-lgbm-on-kfold-technique_m11
"[""<pre>Let's check now the distribution of the mean values per columns in the train and test datasets.</pre>""]",12,roydatascience,eda-pca-simple-lgbm-on-kfold-technique,roydatascience_eda-pca-simple-lgbm-on-kfold-technique_m12
"[""<pre>Let's check now the distribution of the standard deviation  per row in the train dataset, grouped by value of target</pre>""]",13,roydatascience,eda-pca-simple-lgbm-on-kfold-technique,roydatascience_eda-pca-simple-lgbm-on-kfold-technique_m13
"[""<pre>Let's check now the distribution of standard deviation per columns in the train and test datasets.</pre>""]",14,roydatascience,eda-pca-simple-lgbm-on-kfold-technique,roydatascience_eda-pca-simple-lgbm-on-kfold-technique_m14
"['<pre><a id = 33 ><b>Distribution of Skewness</b></a></pre>\n', '\n', ""<pre>Let's see now the distribution of skewness on rows in train separated for values of target 0 and 1. We found the distribution is left skewed</pre>""]",15,roydatascience,eda-pca-simple-lgbm-on-kfold-technique,roydatascience_eda-pca-simple-lgbm-on-kfold-technique_m15
"[""<pre>Let's see now the distribution of skewness on columns in train separated for values of target 0 and 1.</pre>""]",16,roydatascience,eda-pca-simple-lgbm-on-kfold-technique,roydatascience_eda-pca-simple-lgbm-on-kfold-technique_m16
['<pre><a id = 34 ><b>Distribution of Kurtosis</b></a></pre>'],17,roydatascience,eda-pca-simple-lgbm-on-kfold-technique,roydatascience_eda-pca-simple-lgbm-on-kfold-technique_m17
"[""<pre>Let's see now the distribution of kurtosis on rows in train separated for values of target 0 and 1. We found the distribution to be Leptokurtic</pre>""]",18,roydatascience,eda-pca-simple-lgbm-on-kfold-technique,roydatascience_eda-pca-simple-lgbm-on-kfold-technique_m18
"[""<pre>Let's see now the distribution of kurtosis on columns in train separated for values of target 0 and 1.</pre>""]",19,roydatascience,eda-pca-simple-lgbm-on-kfold-technique,roydatascience_eda-pca-simple-lgbm-on-kfold-technique_m19
['<a id=4><pre><b>Principal Component Analysis to check Dimentionality Reduction<b></pre></a>'],20,roydatascience,eda-pca-simple-lgbm-on-kfold-technique,roydatascience_eda-pca-simple-lgbm-on-kfold-technique_m20
"[""<pre><a id = 41><b>Kernel PCA (Since the Graph above doesn't represent meaningful analysis)</b></a></pre>""]",21,roydatascience,eda-pca-simple-lgbm-on-kfold-technique,roydatascience_eda-pca-simple-lgbm-on-kfold-technique_m21
"[""<pre>Since PCA hasn't been useful, I decided to proceed with the existing dataset</pre>""]",22,roydatascience,eda-pca-simple-lgbm-on-kfold-technique,roydatascience_eda-pca-simple-lgbm-on-kfold-technique_m22
['<pre><a id = 16><b>Data Augmentation</b></a></pre>'],23,roydatascience,eda-pca-simple-lgbm-on-kfold-technique,roydatascience_eda-pca-simple-lgbm-on-kfold-technique_m23
['<pre><a id = 6><b>Build the Light GBM Model</b></a></pre>'],24,roydatascience,eda-pca-simple-lgbm-on-kfold-technique,roydatascience_eda-pca-simple-lgbm-on-kfold-technique_m24
"['In this Module I have stacked the Validation and Submission outputs using KFold Cross Validation technique and Stratified K-Fold Cross validatiom technique. Referring to the my previous kernel\n', '\n', '**Stratified K Folds on Santander**\n', 'https://www.kaggle.com/roydatascience/eda-pca-simple-lgbm-santander-transactions\n', '\n', '**K Folds on Santander**\n', 'https://www.kaggle.com/roydatascience/fork-of-eda-pca-simple-lgbm-kfold\n', '\n', 'The attempt is to improve the accuracy using Baysian Ridge Stacking approach']",0,roydatascience,santander-transaction-stacking-1-0,roydatascience_santander-transaction-stacking-1-0_m0
['## Stats Feature'],0,sandeepkumar121995,magic-parameters,sandeepkumar121995_magic-parameters_m0
"['## Data Augmentation\n', '\n', 'Thanks to @Jiwei Liu Kernel\n', 'https://www.kaggle.com/jiweiliu/lgb-2-leaves-augment/output\n']",1,sandeepkumar121995,magic-parameters,sandeepkumar121995_magic-parameters_m1
['### Feature engineering ----- Continued'],2,sandeepkumar121995,magic-parameters,sandeepkumar121995_magic-parameters_m2
['Not really a GP competetion so just having fun'],0,scirpus,santander-gp,scirpus_santander-gp_m0
['Look at the diagonals'],0,scirpus,weird-data-structure,scirpus_weird-data-structure_m0
"['# Are vars mixed up time intervals? Lets sort it out!\n', '\n', 'Lately, I was gazing at the beatiful graphs created by Chris Deotte here:\n', 'https://www.kaggle.com/cdeotte/modified-naive-bayes-santander-0-899/notebook\n', '\n', '..and noticed that they look like mixed up cartoon frames. They have similar pattern, proportions, but they are just mixed up, on different scale and sometimes flipped. So I have cleaned it up; here is the result. ']",0,sibmike,are-vars-mixed-up-time-intervals,sibmike_are-vars-mixed-up-time-intervals_m0
"['# The list of columns that have to be reversed\n', ""If you take a look at the orginal graphs by Chris Deotte, notice how similar the probability graphs are. But some are directed to the right, while others are directed to the left. We will flip the graphs with the higest probabilty on the right. And we'll bring everything to one scale to make similarities more pronounced.""]",1,sibmike,are-vars-mixed-up-time-intervals,sibmike_are-vars-mixed-up-time-intervals_m1
['# Load data'],2,sibmike,are-vars-mixed-up-time-intervals,sibmike_are-vars-mixed-up-time-intervals_m2
"['# Scale and flip\n', 'I doubt that standard scaler is the correct way to restore the original scale for the frames. But alternatives looked even worse.']",3,sibmike,are-vars-mixed-up-time-intervals,sibmike_are-vars-mixed-up-time-intervals_m3
"['# Statistical Functions\n', 'Below are functions to calcuate various statistical things.']",4,sibmike,are-vars-mixed-up-time-intervals,sibmike_are-vars-mixed-up-time-intervals_m4
"['# The frames: Display Target Density and Target Probability\n', 'As described by Chris: ""Below are two plots for each of the 200 variables. The first is the density of `target=1` versus `target=0`. The second gives the probability that `target=1` given different values for `var_k`."" \n', '\n', 'Note how the shape and the range of graphs are consistnent. All we need to do is to sort them in the right order. Does not it remind you of the last Santander? I am pretty sure the correct order can be restored.']",5,sibmike,are-vars-mixed-up-time-intervals,sibmike_are-vars-mixed-up-time-intervals_m5
"['# Afterthoughts\n', ""These does not look random. I have not tried to go beyond this, Yet, if it works, don't forget to say thanks to Chris Deotte.""]",6,sibmike,are-vars-mixed-up-time-intervals,sibmike_are-vars-mixed-up-time-intervals_m6
"['\n', '# Subtracting normality\n', '\n', 'Lately, I was gazing at the beatiful graphs created by Chris Deotte once again:\n', 'https://www.kaggle.com/cdeotte/modified-naive-bayes-santander-0-899/notebook\n', '\n', '...thinking that they have told us to look for weird ubnormal stuff. To do so we have to substract normality, so here we go.']",0,sibmike,subtracting-normality,sibmike_subtracting-normality_m0
"[""# What happens when you're normal\n"", '\n', 'Lets generate two normal distributions and subtract one from the other:']",1,sibmike,subtracting-normality,sibmike_subtracting-normality_m1
"['Two things to note:\n', '1. There is single peak at zero.\n', '2. Substracting one sorted array from another is probably not the best way to ""substract normality"".\n', '\n', 'It maybe Ok for exploration, but for the actual data there should be a cleaner way to do this.']",2,sibmike,subtracting-normality,sibmike_subtracting-normality_m2
"['# A conspiracy theory\n', 'What if I would want to hide that my data is categorical? I could add gaussian noise to category values.']",3,sibmike,subtracting-normality,sibmike_subtracting-normality_m3
['The shapes look familiar.'],4,sibmike,subtracting-normality,sibmike_subtracting-normality_m4
['As you see substracting sorted array is not the best way to restore original categories. But categorical data with added noise looks different after the trick. Lets try doing it to our data.'],5,sibmike,subtracting-normality,sibmike_subtracting-normality_m5
['# Load our data'],6,sibmike,subtracting-normality,sibmike_subtracting-normality_m6
['# Substracting normality?'],7,sibmike,subtracting-normality,sibmike_subtracting-normality_m7
"['# Statistical Functions\n', 'Below are functions to calcuate various statistical things.']",8,sibmike,subtracting-normality,sibmike_subtracting-normality_m8
"['# Categories?: Display Target Density and Target Probability\n', 'As described by Chris: ""Below are two plots for each of the 200 variables. The first is the density of `target=1` versus `target=0`. The second gives the probability that `target=1` given different values for `var_k`."" \n', '\n', 'Note how the shape and the range of graphs are consistnent. All we need to do is to sort them in the right order. Does not it remind you of the last Santander? I am pretty sure the correct order can be restored.']",9,sibmike,subtracting-normality,sibmike_subtracting-normality_m9
"['# Afterthoughts\n', 'I have not tried to go beyond this, so I am not even sure that this works. But I will certainly try. All comments are welcome. So if you have reasons to think this is bollocks, please say so.']",10,sibmike,subtracting-normality,sibmike_subtracting-normality_m10
['### Import training data'],0,simongrest,finding-peaks-in-the-histograms-of-the-variables,simongrest_finding-peaks-in-the-histograms-of-the-variables_m0
"['### Define peak finding function on histogram of a series\n', '\n', 'This function uses the `find_peaks` function from `scipy.signal` which takes various parameters describing constraints on the peaks that you wish to find. I have specified values for `prominence` and `width` to try and highlight the spikes in the density rather than just the peak of the distribution.']",1,simongrest,finding-peaks-in-the-histograms-of-the-variables,simongrest_finding-peaks-in-the-histograms-of-the-variables_m1
['#### Get a list of peaks for the series that have them'],2,simongrest,finding-peaks-in-the-histograms-of-the-variables,simongrest_finding-peaks-in-the-histograms-of-the-variables_m2
['![](https://github.com/fmfn/BayesianOptimization/blob/master/examples/func.png?raw=true)'],0,somang1418,tuning-hyperparameters-under-10-minutes-lgbm,somang1418_tuning-hyperparameters-under-10-minutes-lgbm_m0
"['# **Introduction**\n', '<br>\n', 'Hi guys! <br>\n', 'Have you guys experienced some frustrating moments when you tuned hyperparameters with Grid Search and Random Search?  <br>\n', 'Well...I have! I waited for two hours or more to run codes for both methods and ended up losing my focus by watching Youtube videos...\n', '<br>\n', '![](https://media.giphy.com/media/qjF9Akev3QPNC/giphy.gif)\n', '<br>\n', 'So, I thought that there must be a faster way to tune hyperparameters and did some research about the new method of tuning, which is called Baysian Optimization.<br>\n', 'If you have done Kaggle for a while or are an expert in this field, you probably have used or heard the Baysian Optimization. <br>\n', 'This kernel will give you a good idea of Baysian Optimization and the simple implementation of Baysian Optimization with BayesianOptimization, which I found that it is faster than other Baysian Optimization functions in Python (ex. Scikit-Optimize and Hyperopt) for my model.  <br> \n', '\n', '**The goal is to identify which customers will make a specific transaction in the future and maximize the evaluation function (AUC)**\n', '<br> <br>\n', '\n', '# **Special Thanks to:** <br> \n', '\n', ""I recommend you to see  [Fayzur's kernel](https://www.kaggle.com/fayzur/lgb-bayesian-parameters-finding-rank-average) and [sz8416's kernel](https://www.kaggle.com/sz8416/simple-bayesian-optimization-for-lightgbm) if you are interested in seeing what other people have done. Their kernels were very helpful to understand about the Baysian optimization process in Python. ""]",1,somang1418,tuning-hyperparameters-under-10-minutes-lgbm,somang1418_tuning-hyperparameters-under-10-minutes-lgbm_m1
"['# **Ready, Set, Go!** <br>\n', '\n', 'Before starting the kernel, I guarantee that tuning hyperparameter process will not take more than 10 minutes. And the whole process (loading dataset, tuning the hyperparameter, and training LightGBM) will not take more than 20 minutes.  **Time is important!!** <br>\n', '![](https://media.giphy.com/media/3oz8xKaR836UJOYeOc/giphy.gif)\n']",2,somang1418,tuning-hyperparameters-under-10-minutes-lgbm,somang1418_tuning-hyperparameters-under-10-minutes-lgbm_m2
"['<br>\n', '# ** CONTENTS**\n', '\n', '1. [What Is Bayesian Optimization and Why Do We Care?](#1)\n', '2. [Loading Library and Dataset](#2)\n', '3. [Bayesian Optimization with LightGBM](#3)\n', '4. [Training LightGBM](#4)\n', '5. [Understanding the Model Better](#5)\n']",3,somang1418,tuning-hyperparameters-under-10-minutes-lgbm,somang1418_tuning-hyperparameters-under-10-minutes-lgbm_m3
"['<a id=""1""></a> <br>\n', '\n', '# **What Is Bayesian Optimization and Why Do We Care?**']",4,somang1418,tuning-hyperparameters-under-10-minutes-lgbm,somang1418_tuning-hyperparameters-under-10-minutes-lgbm_m4
"['**Bayesian Optimization** is a probabilistic model based approach for finding the minimum of any function that returns a real-value metric. [(source)](https://towardsdatascience.com/an-introductory-example-of-bayesian-optimization-in-python-with-hyperopt-aae40fff4ff0)<br> It is very effective with real-world applications in high-dimensional parameter-tuning for complex machine learning algorithms. Bayesian optimization utilizes the Bayesian technique of setting a prior over the objective function and\n', 'combining it with evidence to get a posterior function. I attached one graph that demonstrates Bayes’ theorem below. <br> <br>\n', '\n', '<img src=""https://www.analyticsvidhya.com/wp-content/uploads/2016/06/12-768x475.jpg""  alt=""Drawing"" style=""width: 600px;""/>\n', '<br> <br> \n', ' The prior belief is our belief in parameters before modeling process. The posterior belief is our belief in our parameters after observing the evidence.\n', '<br> Another way to present the Bayes’ theorem is: \n', '\n', '<img src=""https://www.maths.ox.ac.uk/system/files/attachments/Bayes_0.png""  alt=""Drawing"" style=""width: 600px;""/> <br> \n', '\n', 'For continuous functions, Bayesian optimization typically works by assuming the unknown function was sampled from a Gaussian process and maintains a posterior distribution for this function as observations are made, which means that we need to give range of values of hyperparameters (ex. learning rate range from 0.1 to 1).  So, in our case, the Gaussian process gives us a prior distribution on functions. Gaussian process approach is a non-parametric approach, in that it finds a distribution over the possible functions \n', 'f(x) that are consistent with the observed data. Gaussian processes have proven to be useful surrogate models for computer experiments and good\n', 'practices have been established in this context for sensitivity analysis, calibration and prediction While these strategies are not considered in the context of optimization, they can be useful to researchers in machine learning who wish to understand better the sensitivity of their models to various hyperparameters. [(source)](http://papers.nips.cc/paper/4522-practical-bayesian-optimization-of-machine-learning-algorithms.pdf)\n', '\n', '<br>\n', '**Well..why do we care about it?** <br> \n', 'According to the [study](http://proceedings.mlr.press/v28/bergstra13.pdf), hyperparameter tuning by Bayesian Optimization of machine learnin models is more efficient than Grid Search and Random Search. Bayesian Optimization has better overall performance on the test data and takes less time for optimization. Also, we do not need to set a certain values of parameters like we do in Random Search and Grid Search. For Bayesian Optimization tuning, we just give a range of a hyperparameter. \n', '\n']",5,somang1418,tuning-hyperparameters-under-10-minutes-lgbm,somang1418_tuning-hyperparameters-under-10-minutes-lgbm_m5
"['<a id=""2""></a> <br>\n', '# **Loading Library and Dataset**\n']",6,somang1418,tuning-hyperparameters-under-10-minutes-lgbm,somang1418_tuning-hyperparameters-under-10-minutes-lgbm_m6
"[""By Changing the data type of each column, I reduced memory usages by 75%. By taking the minimum and the maximum of each column, the function assigns which numeric data type is optimal for the column and change the data type. If you want to know more about how it works, I suggest you to read [Eryk's article](https://towardsdatascience.com/make-working-with-large-dataframes-easier-at-least-for-your-memory-6f52b5f4b5c4)! ""]",7,somang1418,tuning-hyperparameters-under-10-minutes-lgbm,somang1418_tuning-hyperparameters-under-10-minutes-lgbm_m7
"['<a id=""3""></a> <br>\n', '# **Bayesian Optimization with LightGBM**\n', '<br>\n', 'Now I am going to prepare data for modeling and a Baysian Optimization function. You can put more parameters (ex. lambda_l1 and lambda_l2) into the function.  ']",8,somang1418,tuning-hyperparameters-under-10-minutes-lgbm,somang1418_tuning-hyperparameters-under-10-minutes-lgbm_m8
['Here is my optimal parameter for LightGBM. '],9,somang1418,tuning-hyperparameters-under-10-minutes-lgbm,somang1418_tuning-hyperparameters-under-10-minutes-lgbm_m9
"['<a id=""4""></a> <br>\n', '# **Training LightGBM** <br> <br>\n', '\n', 'Based on the parameter from the previous step, I am going to train LightGBM. ']",10,somang1418,tuning-hyperparameters-under-10-minutes-lgbm,somang1418_tuning-hyperparameters-under-10-minutes-lgbm_m10
"['<a id=""5""></a> <br>\n', '# **Understanding the Model Better**\n', '<br>\n', '\n', 'To get an overview of which features are most important for a model, we can plot the SHAP values of every feature for every sample. The plot below sorts features by the sum of SHAP value magnitudes over all samples, and uses SHAP values to show the distribution of the impacts each feature has on the model output. [(source)](https://github.com/slundberg/shap) The color represents the feature value (red high, blue low). This reveals for example that a high var_139 lowers the probability of being a customer who will make a specific transaction in the future. ']",11,somang1418,tuning-hyperparameters-under-10-minutes-lgbm,somang1418_tuning-hyperparameters-under-10-minutes-lgbm_m11
['We can also plot a tree from the model and see each tree! '],12,somang1418,tuning-hyperparameters-under-10-minutes-lgbm,somang1418_tuning-hyperparameters-under-10-minutes-lgbm_m12
"['# **Work in Progress..** <br><br>\n', '\n', 'Although I feel confident that I can implement Bayesian Optimization with LightGBM by Python and understand what my Python code does, I do not feel so comfortable with math behind it...😥  I will keep working on researching more about math behind Bayesian Optimization and share with you! <br><br>\n', 'Here are some academic papers about Bayesian Optimization just in case you are interested in:<br>\n', '1) http://papers.nips.cc/paper/4522-practical-bayesian-optimization-of-machine-learning-algorithms.pdf<br>\n', '2) https://arxiv.org/pdf/1012.2599v1.pdf\n', '\n', '<br>\n', ""<center>**I hope you guys enjoyed my kernel and do not forget to upvote if you think that it's helpful!**</center> <br>\n"", '\n', '![](https://media.giphy.com/media/osjgQPWRx3cac/giphy.gif)\n']",13,somang1418,tuning-hyperparameters-under-10-minutes-lgbm,somang1418_tuning-hyperparameters-under-10-minutes-lgbm_m13
"['**Data Augment**\n', 'Augmentation is a method to increase the amount of training data by randomly shuffle/transform the features in a certain way. It improves accuracy by letting the model see more cases of both ""1"" and ""0"" samples in training so the model can generalize better to new data.\n', '\n', 'Thanks to Jiwei Lu for teaching this new concept . *https://www.kaggle.com/jiweiliu/lgb-2-leaves-augment*']",0,subhamsharma96,santander-augment-to-the-rescue,subhamsharma96_santander-augment-to-the-rescue_m0
"['## Pytorch to implement simple feed-forward NN model (0.89+)\n', '\n', '* As below discussion, NN model can get lB 0.89+\n', '* https://www.kaggle.com/c/santander-customer-transaction-prediction/discussion/82499#latest-483679\n', '* Add Cycling learning rate , K-fold cross validation (0.85 to 0.86)\n', '* Add flatten layer as below discussion (0.86 to 0.897)\n', '* https://www.kaggle.com/c/santander-customer-transaction-prediction/discussion/82863\n', '\n', '## LightGBM (LB 0.899)\n', '\n', '* Fine tune parameters (0.898 to 0.899)\n', '* Reference this kernel : https://www.kaggle.com/chocozzz/santander-lightgbm-baseline-lb-0-899\n', '\n', '\n', '## Plan to do\n', '* Modify model structure on NN model\n', '* Focal loss\n', '* Feature engineering\n', '* Tune parameters oof LightGBM']",0,super13579,pytorch-nn-cyclelr-k-fold-0-897-lightgbm-0-899,super13579_pytorch-nn-cyclelr-k-fold-0-897-lightgbm-0-899_m0
['## Load Data'],1,super13579,pytorch-nn-cyclelr-k-fold-0-897-lightgbm-0-899,super13579_pytorch-nn-cyclelr-k-fold-0-897-lightgbm-0-899_m1
['## Split K- fold validation'],2,super13579,pytorch-nn-cyclelr-k-fold-0-897-lightgbm-0-899,super13579_pytorch-nn-cyclelr-k-fold-0-897-lightgbm-0-899_m2
"['## Cycling learning rate\n', '\n', '*copy from ==> https://github.com/anandsaha/pytorch.cyclic.learning.rate/blob/master/cls.py']",3,super13579,pytorch-nn-cyclelr-k-fold-0-897-lightgbm-0-899,super13579_pytorch-nn-cyclelr-k-fold-0-897-lightgbm-0-899_m3
"['## Build Simple NN model (Pytorch)\n', '\n', '* add flatten layer before fc layer (improve to 0.89+)\n', '* https://www.kaggle.com/c/santander-customer-transaction-prediction/discussion/82863\n', '\n', '* Model structure\n', '* (batch_size, 200) ==> Flatten ==> (batch_size* 200,1) ==> fc1 ==> (batch_size* 200, hidden_layer) ==>Reshape ==>(batch_size, hidden_layer * 200) ==> fc2 ==> (batch_size, 1)']",4,super13579,pytorch-nn-cyclelr-k-fold-0-897-lightgbm-0-899,super13579_pytorch-nn-cyclelr-k-fold-0-897-lightgbm-0-899_m4
"['## Start training\n', '* Epoch = 40\n', '* Batch size = 256\n', '* Cycling step = 150']",5,super13579,pytorch-nn-cyclelr-k-fold-0-897-lightgbm-0-899,super13579_pytorch-nn-cyclelr-k-fold-0-897-lightgbm-0-899_m5
"['## LightGBM Model\n', '* reference this kernel : https://www.kaggle.com/chocozzz/santander-lightgbm-baseline-lb-0-899 ']",6,super13579,pytorch-nn-cyclelr-k-fold-0-897-lightgbm-0-899,super13579_pytorch-nn-cyclelr-k-fold-0-897-lightgbm-0-899_m6
['## LGBM training'],7,super13579,pytorch-nn-cyclelr-k-fold-0-897-lightgbm-0-899,super13579_pytorch-nn-cyclelr-k-fold-0-897-lightgbm-0-899_m7
[],8,super13579,pytorch-nn-cyclelr-k-fold-0-897-lightgbm-0-899,super13579_pytorch-nn-cyclelr-k-fold-0-897-lightgbm-0-899_m8
"['## Ensemble two model (NN+ LGBM)\n', ""* NN model accuracy is too low, ensemble looks don't work.""]",9,super13579,pytorch-nn-cyclelr-k-fold-0-897-lightgbm-0-899,super13579_pytorch-nn-cyclelr-k-fold-0-897-lightgbm-0-899_m9
['## Create submit file'],10,super13579,pytorch-nn-cyclelr-k-fold-0-897-lightgbm-0-899,super13579_pytorch-nn-cyclelr-k-fold-0-897-lightgbm-0-899_m10
"['* This notebook was part of UpGrad Kagglethon, initiative to help their cohort getting started with Kaggle competitions. To be compliant with rules, I am sharing everything that was discussed during those sessions. ']",0,tezdhar,getting-started-santander,tezdhar_getting-started-santander_m0
"['** Why do Kaggle**\n', '\n', '* Learning new things\n', '* strenghtnen intuition for ml algorithms and techniques\n', '* like competing with fellow kagglers']",1,tezdhar,getting-started-santander,tezdhar_getting-started-santander_m1
"['** Problem statement **\n', 'https://www.kaggle.com/c/santander-customer-transaction-prediction\n', '\n']",2,tezdhar,getting-started-santander,tezdhar_getting-started-santander_m2
"['## EDA\n', '\n', '### Pointers\n', '* Check out existing kernels\n', 'https://www.kaggle.com/gpreda/santander-eda-and-prediction\n', 'https://www.kaggle.com/artgor/santander-eda-fe-fs-and-models\n', 'https://www.kaggle.com/mjbahmani/santander-ml-explainability\n', '\n', '* Check distributions\n', '* Compare train and test distributions\n', '* Identify important features (Most of the times feature engineering is going to be around features with high predictive power)\n', '* Attach a logic to why featurea are important ( Note: data is anonymised  here so hard to do this)\n', '* Check previous solutions to similar problems\n', '\n', '\n', '### Observations\n', '* Data normalization and imputation\n', '* Weak corelations between features and target\n', '* IV values ??\n', '* Most variables have distribution close to normal\n', '* Almost no corelation between differnt variable - What does it mean ??\n', '* No NA values (already imputed??)\n', '* Some features seem to have been clipped at one end\n', '* Spikes in distributions (imputed values??)\n', '* less unique ']",3,tezdhar,getting-started-santander,tezdhar_getting-started-santander_m3
"['### Method -1 : train on full and predict on test\n', ' - rule  - scale boosting rounds by train data ratio to data during validation - 1500 ']",4,tezdhar,getting-started-santander,tezdhar_getting-started-santander_m4
['### Method 2 - use validation fold models to predict on test set\n'],5,tezdhar,getting-started-santander,tezdhar_getting-started-santander_m5
"['### Modelling\n', '\n', 'Pointers:\n', '*  Validation strategy -- Random KFold, holdout or temporal split ??\n', '* What to trust validation score or LB socre?? trust score from more data; if test data is more we should treat LB as additional fold\n', '* Hyperparamter tuning -- Combination of manual tuning and bayesian optimization libraries like `hyperopt` and `scikit-optimize`. Initial tuninng on single fold and then move to 5 folds.\n', '* Always check validation and test set prediction distributions\n', '* ** Read forums and participate in discussions **\n', '\n', 'Disussions:\n', '* Sometimes using geometric mean of probabilities is better than using simple mean\n', '* When metric is ROC_AUC, even rank average can be used\n', '* Blending -- blend of your solution and public solution can be used to improve LB score. But, better approach is to understand what is working for other people and integrate in your models.\n']",6,tezdhar,getting-started-santander,tezdhar_getting-started-santander_m6
"['# Next steps:\n', '\n', '* Feature engineering - interactions, bucketing etc\n', '* try other algorithms -- catboost, xgboost, RGF (regularized greedy forest), different NN architecture\n', '* weighted average\n', '* add more public solutions to blend\n', '* submit and keep making progress\n', '* maintain a list of ideas to be executed, you should never run out of things to do\n', '\n', '### ** Happy Kaggling and thank you :) **\n', '\n', 'Meanwhile something to inspire you from one of the greats:  https://www.youtube.com/watch?v=7XEMPU17-Wo']",7,tezdhar,getting-started-santander,tezdhar_getting-started-santander_m7
"[""Even though the local-leaderboard CV congruence is pretty good, it's far from perfect. Most of the discrepancy seems to be con""]",0,tunguz,adversarial-santander,tunguz_adversarial-santander_m0
[],1,tunguz,adversarial-santander,tunguz_adversarial-santander_m1
"[""Let's take a look at the overall oof CV AUC:""]",2,tunguz,adversarial-santander,tunguz_adversarial-santander_m2
"['AUC of 0.53 is overall not a major issue, but it **is** slightly over 0.50, which can have an impact when you are chasing the 4th decimal point.\n']",3,tunguz,adversarial-santander,tunguz_adversarial-santander_m3
"['[H2O AutoML](http://docs.h2o.ai/h2o/latest-stable/h2o-docs/automl.html) is an automated machine learning meta-algorithm that is part of the [H2O software library](http://docs.h2o.ai/h2o/latest-stable/h2o-py/docs/intro.html#what-is-h2o). (It shold not be confused with [H2O DriverlessAI](https://www.h2o.ai/products/h2o-driverless-ai/), which is a commercial product and built from an entirely different code base.) H2O’s AutoML can be used for automating the machine learning workflow, which includes automatic training and tuning of many models within a user-specified time-limit. Stacked Ensembles – one based on all previously trained models, another one on the best model of each family – will be automatically trained on collections of individual models to produce highly predictive ensemble models which, in most cases, will be the top performing models in the AutoML Leaderboard.']",0,tunguz,santander-with-h2o-automl,tunguz_santander-with-h2o-automl_m0
"['# GPU-accelerated LightGBM\n', '\n', 'This kernel explores a GPU-accelerated LGBM model to predict customer transaction.\n', '\n', '## Notebook  Content\n', '1. [Re-compile LGBM with GPU support](#1)\n', '1. [Loading the data](#2)\n', '1. [Training the model on CPU](#3)\n', '1. [Training the model on GPU](#4)\n', '1. [Submission](#5)']",0,vinhnguyen,gpu-acceleration-for-lightgbm,vinhnguyen_gpu-acceleration-for-lightgbm_m0
"['<a id=""1""></a> \n', '## 1. Re-compile LGBM with GPU support\n', 'In Kaggle notebook setting, set the `Internet` option to `Internet connected`, and `GPU` to `GPU on`. \n', '\n', 'We first remove the existing CPU-only lightGBM library and clone the latest github repo.']",1,vinhnguyen,gpu-acceleration-for-lightgbm,vinhnguyen_gpu-acceleration-for-lightgbm_m1
"['Next, the Boost development library must be installed.']",2,vinhnguyen,gpu-acceleration-for-lightgbm,vinhnguyen_gpu-acceleration-for-lightgbm_m2
['The next step is to build and re-install lightGBM with GPU support.'],3,vinhnguyen,gpu-acceleration-for-lightgbm,vinhnguyen_gpu-acceleration-for-lightgbm_m3
"['Last, carry out some post processing tricks for OpenCL to work properly, and clean up.']",4,vinhnguyen,gpu-acceleration-for-lightgbm,vinhnguyen_gpu-acceleration-for-lightgbm_m4
"['<a id=""2""></a> \n', '## 2. Loading the data']",5,vinhnguyen,gpu-acceleration-for-lightgbm,vinhnguyen_gpu-acceleration-for-lightgbm_m5
"['<a id=""3""></a>\n', '## 3. Training the model on CPU']",6,vinhnguyen,gpu-acceleration-for-lightgbm,vinhnguyen_gpu-acceleration-for-lightgbm_m6
"['<a id=""4""></a>\n', '## 4. Train model on GPU']",7,vinhnguyen,gpu-acceleration-for-lightgbm,vinhnguyen_gpu-acceleration-for-lightgbm_m7
"['First, check the GPU availability.']",8,vinhnguyen,gpu-acceleration-for-lightgbm,vinhnguyen_gpu-acceleration-for-lightgbm_m8
"['In order to leverage the GPU, we need to set the following parameters: \n', '\n', ""        'device': 'gpu',\n"", ""        'gpu_platform_id': 0,\n"", ""        'gpu_device_id': 0\n"", '        \n', '        ']",9,vinhnguyen,gpu-acceleration-for-lightgbm,vinhnguyen_gpu-acceleration-for-lightgbm_m9
"['<a id=""5""></a>\n', '## 5. Submission']",10,vinhnguyen,gpu-acceleration-for-lightgbm,vinhnguyen_gpu-acceleration-for-lightgbm_m10
"['I have recently checked the following tweet:\n', '\n', '<blockquote class=""twitter-tweet"" data-lang=""fr""><p lang=""en"" dir=""ltr"">stop plotting histograms.</p>&mdash; Hugo Bowne-Anderson (@hugobowne) <a href=""https://twitter.com/hugobowne/status/1111657955248783366?ref_src=twsrc%5Etfw"">29 mars 2019</a></blockquote>\n', '<script async src=""https://platform.twitter.com/widgets.js"" charset=""utf-8""></script>\n', '\n', '\n', '\n', 'Indeed, an [ECDF](https://en.wikipedia.org/wiki/Empirical_distribution_function) \n', 'is often easier to explore and think about. Here is a [**blog post**](https://ericmjl.github.io/blog/2018/7/14/ecdfs/) explaining some of the logic behind this claim.   \n', '\n', ""Let's see how it translates to this competition's dataset!""]",0,yassinealouini,are-ecdfs-better-than-histograms,yassinealouini_are-ecdfs-better-than-histograms_m0
"['Based on the above plots, it appears that:\n', '\n', '* it is indeed easier to see how much two distriubtions differ by inspecting\n', 'the ecdfs.  \n', '* median values (and othe statistics) are easier to observe. \n', '\n', 'Something to try: plot the ECDF for a normal distribution having the same mean\n', 'and standard deviation and compare it with the ones plotted above. \n', '\n', 'If you have more suggestions, leave them in the comments section. \n', '\n', 'Thanks. :)']",1,yassinealouini,are-ecdfs-better-than-histograms,yassinealouini_are-ecdfs-better-than-histograms_m1
"['Thanks to this discussion for the observation: https://www.kaggle.com/c/santander-customer-transaction-prediction/discussion/84450\n', '\n', 'In this notebook, I transform this column and re-order the train dataset using this column, and see what \n', 'happens.']",0,yassinealouini,mystery-behind-var-68,yassinealouini_mystery-behind-var-68_m0
['#\xa0Before you read'],1,yassinealouini,mystery-behind-var-68,yassinealouini_mystery-behind-var-68_m1
"['This exploration is an attempt to discover some hidden things behind the annonymization.\n', 'Nothing is certain of course and this is so far specualative. \n', 'Use this knowledge accordingly. ']",2,yassinealouini,mystery-behind-var-68,yassinealouini_mystery-behind-var-68_m2
['#\xa0Preliminary work'],3,yassinealouini,mystery-behind-var-68,yassinealouini_mystery-behind-var-68_m3
"['Two things to observe: \n', '    \n', '- data has been annonymized\n', '- it comes from a business setting\n', '\n', 'Thus, it is most likely (but not 100% sure) that some of the features\n', ""contain date-like information (and also categorical features but that's \n"", 'for another day). \n', '\n', ""How to find potential columns? Let's try to sort the columns using the number of unique \n"", ""values. What's the heurestic behind this choice? \n"", ""Well there shouldn't be a lot of dates, maybe few thousand top.""]",4,yassinealouini,mystery-behind-var-68,yassinealouini_mystery-behind-var-68_m4
"['==> `var_68` has the least number of uniques, thus it **might** be a date-like column\n', '(it could also be a categorical column).\n', 'There is also a possibility that this small number of uniques is a coincidence due to the rounding to 4 decimal numbers (bonus question: could you compute the probability of this event?)']",5,yassinealouini,mystery-behind-var-68,yassinealouini_mystery-behind-var-68_m5
"['So how to extract a date?\n', 'Well, first, get ride of the decimal values.\n', 'Then transform to a datetime object supposing that it is an ordinal datetime.\n', 'Try different offsets until you get a meaningful date range.\n', ""That's it. Let's see this in action.""]",6,yassinealouini,mystery-behind-var-68,yassinealouini_mystery-behind-var-68_m6
['#\xa0Some plots '],7,yassinealouini,mystery-behind-var-68,yassinealouini_mystery-behind-var-68_m7
['# Date column exploration'],8,yassinealouini,mystery-behind-var-68,yassinealouini_mystery-behind-var-68_m8
"[""Alright, let's now explore this newly created column.""]",9,yassinealouini,mystery-behind-var-68,yassinealouini_mystery-behind-var-68_m9
['=> I will thus use the `date` column to group rows. '],10,yassinealouini,mystery-behind-var-68,yassinealouini_mystery-behind-var-68_m10
"[""==> Uniform day of week distribution. That's a good sign!""]",11,yassinealouini,mystery-behind-var-68,yassinealouini_mystery-behind-var-68_m11
['#\xa0What about the test?'],12,yassinealouini,mystery-behind-var-68,yassinealouini_mystery-behind-var-68_m12
"[""Let's see if our observation transfers well to the test dataset.""]",13,yassinealouini,mystery-behind-var-68,yassinealouini_mystery-behind-var-68_m13
['# Test and train date column comparaison'],14,yassinealouini,mystery-behind-var-68,yassinealouini_mystery-behind-var-68_m14
['==> Most of the dates overlap. '],15,yassinealouini,mystery-behind-var-68,yassinealouini_mystery-behind-var-68_m15
"['Idea to try: predict the mean of the target (using the date \n', 'for grouping) for the overlapping dates. ']",16,yassinealouini,mystery-behind-var-68,yassinealouini_mystery-behind-var-68_m16
['# What to do now?'],17,yassinealouini,mystery-behind-var-68,yassinealouini_mystery-behind-var-68_m17
"['Some of the things I will try to do: \n', '- Use this transformed column for a better temporal CV. Some ideas I have tried: stratification using years, day of weeks, and so on.\n', '- Transform other columns using this new one\n', '\n', 'Stay tuned for more insights. :)']",18,yassinealouini,mystery-behind-var-68,yassinealouini_mystery-behind-var-68_m18
"['A short notebook where I explore the concept of permutation importance using \n', 'the [**ELI5 library**](https://eli5.readthedocs.io/en/latest/blackbox/permutation_importance.html). \n', '\n', 'Inspired from this great tutorial: https://www.kaggle.com/dansbecker/permutation-importance. \n', '\n', 'Enjoy!']",0,yassinealouini,permutation-importance,yassinealouini_permutation-importance_m0
['# 1. Read dataset'],0,youhanlee,yh-eda-i-want-to-see-all,youhanlee_yh-eda-i-want-to-see-all_m0
['## 1.1 Target  check'],1,youhanlee,yh-eda-i-want-to-see-all,youhanlee_yh-eda-i-want-to-see-all_m1
"['- This competiiton is Imbalanced target competition.\n', '- You can check similar competitions, Porto, Homecredit competition.\n', '- Specially, Porto also gave use anonymized dataset. \n', '- https://www.kaggle.com/c/home-credit-default-risk\n', '- https://www.kaggle.com/c/porto-seguro-safe-driver-prediction']",2,youhanlee,yh-eda-i-want-to-see-all,youhanlee_yh-eda-i-want-to-see-all_m2
['## 1.2 Null data check'],3,youhanlee,yh-eda-i-want-to-see-all,youhanlee_yh-eda-i-want-to-see-all_m3
"['- There is no missing values.\n', ""- Because we don't know the exact meaning of variables, we need to check some values as null value.""]",4,youhanlee,yh-eda-i-want-to-see-all,youhanlee_yh-eda-i-want-to-see-all_m4
['# 2. Exploratory Data Analysis'],5,youhanlee,yh-eda-i-want-to-see-all,youhanlee_yh-eda-i-want-to-see-all_m5
"[""- Before EDA, let's group the features into category and non-category based on the number of uniqueness.""]",6,youhanlee,yh-eda-i-want-to-see-all,youhanlee_yh-eda-i-want-to-see-all_m6
"['- Oh, Most features have more than thousands of values for each variable except var_68 (435)']",7,youhanlee,yh-eda-i-want-to-see-all,youhanlee_yh-eda-i-want-to-see-all_m7
"[""- Let's see var 68""]",8,youhanlee,yh-eda-i-want-to-see-all,youhanlee_yh-eda-i-want-to-see-all_m8
"['- It also has float numbers. \n', '- Uncovering these values will be intersting job!\n', '- Multiplying and dividing with some values can make the hidden categories, See Radder work(https://www.kaggle.com/raddar/target-true-meaning-revealed)']",9,youhanlee,yh-eda-i-want-to-see-all,youhanlee_yh-eda-i-want-to-see-all_m9
['# 2.1 Correlation'],10,youhanlee,yh-eda-i-want-to-see-all,youhanlee_yh-eda-i-want-to-see-all_m10
"['- The largest correlation value is 0.08\n', ""- Actually, the target is binary and variables are continous, so correlation is not enough to judge. Let's see the distribution!""]",11,youhanlee,yh-eda-i-want-to-see-all,youhanlee_yh-eda-i-want-to-see-all_m11
['# 2.2 Distribution regarding to target'],12,youhanlee,yh-eda-i-want-to-see-all,youhanlee_yh-eda-i-want-to-see-all_m12
['# TODO'],13,youhanlee,yh-eda-i-want-to-see-all,youhanlee_yh-eda-i-want-to-see-all_m13
"['- As you know, Santander hosted other competition 6 month before.\n', '- So you can check this competition. https://www.kaggle.com/c/santander-value-prediction-challenge']",14,youhanlee,yh-eda-i-want-to-see-all,youhanlee_yh-eda-i-want-to-see-all_m14
['- I will do time series analysis for this dataset.'],15,youhanlee,yh-eda-i-want-to-see-all,youhanlee_yh-eda-i-want-to-see-all_m15
['## Load Data'],0,yuzusan,santander-draft-v3-eda-lgb-nn,yuzusan_santander-draft-v3-eda-lgb-nn_m0
['##   Data Exploration'],1,yuzusan,santander-draft-v3-eda-lgb-nn,yuzusan_santander-draft-v3-eda-lgb-nn_m1
['## Data Preprocessing'],2,yuzusan,santander-draft-v3-eda-lgb-nn,yuzusan_santander-draft-v3-eda-lgb-nn_m2
['## Variable Engineering'],3,yuzusan,santander-draft-v3-eda-lgb-nn,yuzusan_santander-draft-v3-eda-lgb-nn_m3
['#### PCA'],4,yuzusan,santander-draft-v3-eda-lgb-nn,yuzusan_santander-draft-v3-eda-lgb-nn_m4
['#### Summary Stats'],5,yuzusan,santander-draft-v3-eda-lgb-nn,yuzusan_santander-draft-v3-eda-lgb-nn_m5
['## Feature importance'],6,yuzusan,santander-draft-v3-eda-lgb-nn,yuzusan_santander-draft-v3-eda-lgb-nn_m6
['### Permutation Importance'],7,yuzusan,santander-draft-v3-eda-lgb-nn,yuzusan_santander-draft-v3-eda-lgb-nn_m7
[' ## Model Development'],8,yuzusan,santander-draft-v3-eda-lgb-nn,yuzusan_santander-draft-v3-eda-lgb-nn_m8
['### lightgbm'],9,yuzusan,santander-draft-v3-eda-lgb-nn,yuzusan_santander-draft-v3-eda-lgb-nn_m9
['### Neural Net'],10,yuzusan,santander-draft-v3-eda-lgb-nn,yuzusan_santander-draft-v3-eda-lgb-nn_m10
['## Submission Files'],11,yuzusan,santander-draft-v3-eda-lgb-nn,yuzusan_santander-draft-v3-eda-lgb-nn_m11
"[' <a id=""55""></a> <br>\n', '## Stacking']",12,yuzusan,santander-draft-v3-eda-lgb-nn,yuzusan_santander-draft-v3-eda-lgb-nn_m12
"['# References & credits\n', 'Thanks fo following kernels that help me to create this kernel.']",13,yuzusan,santander-draft-v3-eda-lgb-nn,yuzusan_santander-draft-v3-eda-lgb-nn_m13
"['1. [https://www.kaggle.com/mjbahmani/santander-ml-explainability](https://www.kaggle.com/mjbahmani/santander-ml-explainability)  \n', '1. [https://www.kaggle.com/super13579/pytorch-nn-cyclelr-k-fold-0-897-lightgbm-0-899](https://www.kaggle.com/super13579/pytorch-nn-cyclelr-k-fold-0-897-lightgbm-0-899)  \n', '1. [https://www.kaggle.com/dansbecker/permutation-importance](https://www.kaggle.com/dansbecker/permutation-importance)\n', '1. [https://www.kaggle.com/dansbecker/partial-plots](https://www.kaggle.com/dansbecker/partial-plots)\n', '1. [https://www.kaggle.com/miklgr500/catboost-with-gridsearch-cv](https://www.kaggle.com/miklgr500/catboost-with-gridsearch-cv)\n', '1. [https://www.kaggle.com/dromosys/sctp-working-lgb](https://www.kaggle.com/dromosys/sctp-working-lgb)\n', '1. [https://www.kaggle.com/gpreda/santander-eda-and-prediction](https://www.kaggle.com/gpreda/santander-eda-and-prediction)\n', '1. [permutation-importance](https://www.kaggle.com/dansbecker/permutation-importance)\n', '1. [partial-plots](https://www.kaggle.com/dansbecker/partial-plots)\n', '1. [https://www.kaggle.com/dansbecker/shap-values](https://www.kaggle.com/dansbecker/shap-values)\n', '1. [algorithm-choice](https://docs.microsoft.com/en-us/azure/machine-learning/studio/algorithm-choice)']",14,yuzusan,santander-draft-v3-eda-lgb-nn,yuzusan_santander-draft-v3-eda-lgb-nn_m14
['# Not Completed yet!!!'],15,yuzusan,santander-draft-v3-eda-lgb-nn,yuzusan_santander-draft-v3-eda-lgb-nn_m15
['**1) Clean test from fake data**'],0,zfturbo,magic-feature-generator,zfturbo_magic-feature-generator_m0
['**2) Create Magic features**'],1,zfturbo,magic-feature-generator,zfturbo_magic-feature-generator_m1
['3) Read data with magic features and create additional useful features var_N **mul** magic_N and var_N **div** magic_N'],2,zfturbo,magic-feature-generator,zfturbo_magic-feature-generator_m2
"['4) Create model\n', '\n', '5) Use random shuffle of columns during training']",3,zfturbo,magic-feature-generator,zfturbo_magic-feature-generator_m3
['**6) Predict on test**'],4,zfturbo,magic-feature-generator,zfturbo_magic-feature-generator_m4
"['![](http://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1513868561/output_65_0_knd6e9.png)']",0,zikazika,reusable-insightful-eda,zikazika_reusable-insightful-eda_m0
"['Plots on top of plot on top of plots. It seems that most of the EDA these days is just throwing around fancy plots from fancy libraries. There is no real **insight** or **reusability** from those kinds of notebooks, just to fill in the space.\n', '\n', '\n', '# GOAL: This notebook should serve as a reusable template for **INSIGHTFUL** EDA when approaching a DS problem. \n', '\n', 'Ofcourse there is not a universal solution and it always needs to be modified but I feel like that outlining a couple of general ideas and principles will be usefull since they will repeat themselves.']",1,zikazika,reusable-insightful-eda,zikazika_reusable-insightful-eda_m1
['Dataset will be [Santander Customer Transaction Prediction](https://www.kaggle.com/c/santander-customer-transaction-prediction/data) where we have 200 columns of anonymised data. '],2,zikazika,reusable-insightful-eda,zikazika_reusable-insightful-eda_m2
"['# What should good EDA be capable of?\n', '1. Verify expected relationships actually exist in the data, thus formulating and validating planned techniques of analysis.\n', '2. To find some unexpected structure in the data that must be taken into account, thereby suggesting some changes in the planned analysis.\n', '3. Deliver data-driven insights to business stakeholders by confirming they are asking the right questions and not biasing the investigation with their assumptions.\n', '4. Provide the context around the problem to make sure the potential value of the data scientist’s output can be maximized.']",3,zikazika,reusable-insightful-eda,zikazika_reusable-insightful-eda_m3
"['Main point is gathering and automising as much as possible. So we will plot all of the variables together (modifying the code for different problems), than ""zoom in"" in case of suspicion. Since there are a lot of indicators I only took some of them to speed up the computation. We can see distributions as well as plots in relationship with other variables.']",4,zikazika,reusable-insightful-eda,zikazika_reusable-insightful-eda_m4
['Same for test set.'],5,zikazika,reusable-insightful-eda,zikazika_reusable-insightful-eda_m5
['We should also plot other variables in dependence to the dependent variable--**target**. Thats the first column so lets just take a couple of the first columns. (run-time!!!)'],6,zikazika,reusable-insightful-eda,zikazika_reusable-insightful-eda_m6
"['# Take-off notes from the first analysis:\n', '\n', '**Ofcourse this is individual but we can see unbalanced classes in the target variable, not much correlation (we will check it more subsequently!). Mostly normaly distribution among predictors, tough to distinguish which values are to be associated with 0 & 1 class etc...**\n', '\n', '\n', 'As already mentioned one ought to ""zoom in"" and take one special predictor and to subsequent analysis. Doing a 3-d plot with some special interest variables etc. So it really depends on the problem and the domain knowledge of the problem.']",7,zikazika,reusable-insightful-eda,zikazika_reusable-insightful-eda_m7
['Let us also assume that no pre-processing will be done (often times) before EDA. We will use EDA to help us with that too.'],8,zikazika,reusable-insightful-eda,zikazika_reusable-insightful-eda_m8
"['Dealing with **missing values**:\n', '\n', 'This is a bit specific dataset with no missing values:']",9,zikazika,reusable-insightful-eda,zikazika_reusable-insightful-eda_m9
"['BUT, this is a boiler plat code that can be re-used later on different projects!']",10,zikazika,reusable-insightful-eda,zikazika_reusable-insightful-eda_m10
"['When **dealing with outliers** one should be careful and look also at the distribution of the variable at hand. For example let us say that we have a uniform distributed variable, does 2 points of std really say anything about a potential outlier? Best thing one could do is assume (or better yet test with kolmogorov smirnov test) a distribution of a variable. Than depending on the result just throw away values that are to be found far away on the distribution graph.']",11,zikazika,reusable-insightful-eda,zikazika_reusable-insightful-eda_m11
"['Since distribution of independent variables is mostly normal, lets see what happens with outliers when measured with (different) points of standard deviation. (one can see it as z-score)']",12,zikazika,reusable-insightful-eda,zikazika_reusable-insightful-eda_m12
"['Another way to look at the outliers but also in the same time get some more information about distribution (IQR, median, mean etc...) is with the box-plot. But we need to do it efficiently:\n']",13,zikazika,reusable-insightful-eda,zikazika_reusable-insightful-eda_m13
[],14,zikazika,reusable-insightful-eda,zikazika_reusable-insightful-eda_m14
['**Correlation map**- after throwing the outliers and missing values away (since it is neccessary before calculating pearson correlation coefficient)'],15,zikazika,reusable-insightful-eda,zikazika_reusable-insightful-eda_m15
['As we noticed from the first plots (scattered ones) there is not really much correlation between the variables.'],16,zikazika,reusable-insightful-eda,zikazika_reusable-insightful-eda_m16
['So what are some other insights that can be gathered using EDA about the data? One interesting thing is the distribution (density) plot of different predicators when in contrast to different classes (0 or 1).'],17,zikazika,reusable-insightful-eda,zikazika_reusable-insightful-eda_m17
"['**Interesting observation** is its not always normal distribution, in some cases and classes we can observe almost bimodal distribution. **Implication?** Normality assumption is not met, be careful in model choices etc if we were to use these predicators.']",18,zikazika,reusable-insightful-eda,zikazika_reusable-insightful-eda_m18
"['**Another** thing that should be important to us (to ensure could prediction power) is that **test and train sets are the same**, i.e. they come from the same sample and they represent the whole population. Lets plot it for first 50 variables.']",19,zikazika,reusable-insightful-eda,zikazika_reusable-insightful-eda_m19
"['**Additionally** we can speaak about skewness distriibution (here it is normal), additional exploration with some specific variables/domain specific knowledge, contrasting different scatter plots with some categorical variables (here we do not have classes other than the dependent variable), in case of text some word clouds, tf-idf distribution etc etc....\n', 'There are many options but I think these steps are essential no matter what the dataset at hand is. \n', 'Additional **(part 2) tutorial** can be made concerning purely textual data and good EDA there.']",20,zikazika,reusable-insightful-eda,zikazika_reusable-insightful-eda_m20
[],21,zikazika,reusable-insightful-eda,zikazika_reusable-insightful-eda_m21
[],22,zikazika,reusable-insightful-eda,zikazika_reusable-insightful-eda_m22
[],23,zikazika,reusable-insightful-eda,zikazika_reusable-insightful-eda_m23
[],24,zikazika,reusable-insightful-eda,zikazika_reusable-insightful-eda_m24
[],25,zikazika,reusable-insightful-eda,zikazika_reusable-insightful-eda_m25
[],26,zikazika,reusable-insightful-eda,zikazika_reusable-insightful-eda_m26
[],27,zikazika,reusable-insightful-eda,zikazika_reusable-insightful-eda_m27
[],28,zikazika,reusable-insightful-eda,zikazika_reusable-insightful-eda_m28
[],29,zikazika,reusable-insightful-eda,zikazika_reusable-insightful-eda_m29
"['## Assuming the start date to be 1-Dec-2017 as hypothesized in the ""TransactionDT startdate"" kernel ']",0,ab90hi,a-time-series-perspective-and-insights,ab90hi_a-time-series-perspective-and-insights_m0
['## Observed fraud rates each day'],1,ab90hi,a-time-series-perspective-and-insights,ab90hi_a-time-series-perspective-and-insights_m1
"['Fraud rates are lower in December with the lowest point around christmas day. The fraud rate from January onwards seems to hover ~4%. \n', '\n', 'The lower fraud rates in December could be due to higher number of genuine transactions in December. Lets test this by looking at the number of fraud transactions.']",2,ab90hi,a-time-series-perspective-and-insights,ab90hi_a-time-series-perspective-and-insights_m2
['## Number of fraud transactions each day '],3,ab90hi,a-time-series-perspective-and-insights,ab90hi_a-time-series-perspective-and-insights_m3
"['Unlike the fraud rate, the rolling average seems pretty stable until May when we see a drop and possibly a shift in mean value?']",4,ab90hi,a-time-series-perspective-and-insights,ab90hi_a-time-series-perspective-and-insights_m4
['## Checking stationarity '],5,ab90hi,a-time-series-perspective-and-insights,ab90hi_a-time-series-perspective-and-insights_m5
"[""Fraud rate doesn't pass the ADF test for stationarity as expected from looking at the graph""]",6,ab90hi,a-time-series-perspective-and-insights,ab90hi_a-time-series-perspective-and-insights_m6
['Number of fraudulent activities passes the ADF test for stationarity despite the drop we observed around May-2018'],7,ab90hi,a-time-series-perspective-and-insights,ab90hi_a-time-series-perspective-and-insights_m7
['## How do these insights help with my modeling?'],8,ab90hi,a-time-series-perspective-and-insights,ab90hi_a-time-series-perspective-and-insights_m8
"['1. If anyone is using undersampling/oversampling approaches to deal with the class imbalance. It could possibly be benefitial to consider the month/date of transactions, try balancing classes over each month rather than over the entire training set.  \n', '\n', '2. If the drop in number of fraudulent activies we observed in May-2018, does indicate a shift in mean value. Then the expected number of fraudlent activities in test set could be lower than what we observed in the train set. \n', 'A speculation but this could also possibly explain why models are getting higher LB AUC values than local CV AUC values.']",9,ab90hi,a-time-series-perspective-and-insights,ab90hi_a-time-series-perspective-and-insights_m9
"['> # Introduction <br>\n', ""Hi everyone! In this kernel I'd like to share some ideas about NN's and loss functions. <br>\n"", 'Core points of this kernel: \n', ""* Preparing tabular data for NN's\n"", '* Handling skewed continuous features\n', '* Implementing custom loss function\n', '* Which models are good for ensembling']",0,abazdyrev,keras-nn-focal-loss-experiments,abazdyrev_keras-nn-focal-loss-experiments_m0
"['# Data Loading and Feature Selection <br>\n', ""In this particular kernel we will use only features from transaction table, 'cause with NN after a brief investigation I didn't get any significant improvement by using all features.""]",1,abazdyrev,keras-nn-focal-loss-experiments,abazdyrev_keras-nn-focal-loss-experiments_m1
"['# Data Processing\n', 'For continuous right-skewed features we wil apply log-transform, so that will make them look more like normal distributed.']",2,abazdyrev,keras-nn-focal-loss-experiments,abazdyrev_keras-nn-focal-loss-experiments_m2
"['For categorical features we will apply OneHot transformation, but only for most common values for each feature to reduce sparsity. <br>\n', 'Also there is an embedding approach for categorical features transformation. It was implemented in this kernel https://www.kaggle.com/ryches/keras-nn-starter-w-time-series-split <br>\n', ""With embedding approach I didn't get any significant improvement comparing to this.""]",3,abazdyrev,keras-nn-focal-loss-experiments,abazdyrev_keras-nn-focal-loss-experiments_m3
"['# Validation\n', 'For validation I use time-based holdout. For these and other models it has a good correlation between val and lb.']",4,abazdyrev,keras-nn-focal-loss-experiments,abazdyrev_keras-nn-focal-loss-experiments_m4
['# Modeling'],5,abazdyrev,keras-nn-focal-loss-experiments,abazdyrev_keras-nn-focal-loss-experiments_m5
"[""So, we've got a pretty interesting results. The same models with almost the same validation scores give predictions with correlation ~80% by optimizing different loss functions, so that makes them good for ensembling.""]",6,abazdyrev,keras-nn-focal-loss-experiments,abazdyrev_keras-nn-focal-loss-experiments_m6
['# Fine-tuning and Predicting'],7,abazdyrev,keras-nn-focal-loss-experiments,abazdyrev_keras-nn-focal-loss-experiments_m7
"['# Final thoughts <br>\n', ""Obviously as a single model NN doesn't perform as well as Gradient Boosting and to addition NN requires more sophisticated data processing approaches. But I'm sure, that it will be good for stacking as one of the first-level models. <br>\n"", 'I also think, that using focal loss with gradient boosting can improve score.<br>\n', ""Maybe, It is a good idea to research different loss functions, 'cause the same model can give uncorrelated predicts by optimizing different loss functions.\n""]",8,abazdyrev,keras-nn-focal-loss-experiments,abazdyrev_keras-nn-focal-loss-experiments_m8
"['**Let\'s investigate ""D"" features!**\n', '\n', 'We all know that the data provided in this competition consist of transactions. I was curious about how to correctly identify the same cardholder/card and group that transactions. Organizers revealed that ""D"" features contain information about different time deltas and ""card"" features apply to a payment card info. \n']",0,akasyanama13,eda-what-s-behind-d-features,akasyanama13_eda-what-s-behind-d-features_m0
"[""Let's group by 'card' features and look closely on those combinations that give us around 5-10 rows.""]",1,akasyanama13,eda-what-s-behind-d-features,akasyanama13_eda-what-s-behind-d-features_m1
"[""Now let's look closer at these rows.""]",2,akasyanama13,eda-what-s-behind-d-features,akasyanama13_eda-what-s-behind-d-features_m2
"['Now we can add ""DaysFromStart"" column by divining TransactionDT on 60*60*24 and then round it to get a number of days from a starting point.']",3,akasyanama13,eda-what-s-behind-d-features,akasyanama13_eda-what-s-behind-d-features_m3
"[""**Time to identify what's behind 'D' columns!**\n"", '\n', 'Create feature:\n', '    DaysFromPreviousTransaction = DaysFromStart[row_(i)] - DaysFromStart[row_(i-1)]']",4,akasyanama13,eda-what-s-behind-d-features,akasyanama13_eda-what-s-behind-d-features_m4
"['I can be wrong but I believe these transactions belong to the same user. One can see that DaysFromPreviousTransaction is equal to D3 which drives me to think that **D3 indicates number of days from the previous transaction**.\n', '\n', 'Also D1 is cumulatively increasing and for example 481 = 449 + 32 and 510 = 481 + 29, i.e. **D1 could indicate days from the first transaction**. \n', '\n', 'D2 is almost always equal to D1 but for the first transaction when D1 is equal to 0 D2 is nan.']",5,akasyanama13,eda-what-s-behind-d-features,akasyanama13_eda-what-s-behind-d-features_m5
"[""**If I'm not wrong this should be a useful knowledge to identify users and proceed with a meaningful FE.**\n"", '\n', '** Please share your thoughts! **']",6,akasyanama13,eda-what-s-behind-d-features,akasyanama13_eda-what-s-behind-d-features_m6
"['# Can we find unique clients in data?\n', 'Transactions in data looks like are independent of each other. Perhaps the organizers made it for better data anonymization. But what if we find transactions belonging to the same user? May be it will help someone in this competition.\n', '\n', 'I accidentally saw some magic in the feature V307']",0,alexanderzv,find-unique-clients,alexanderzv_find-unique-clients_m0
['### Functions that will help us find unique devices and unique cards. (I think it is clear for you why we use hash functions)'],1,alexanderzv,find-unique-clients,alexanderzv_find-unique-clients_m1
"[""# Don't see anything?""]",2,alexanderzv,find-unique-clients,alexanderzv_find-unique-clients_m2
"[""> # Coincidence? i don't think so ""]",3,alexanderzv,find-unique-clients,alexanderzv_find-unique-clients_m3
['![](https://www.meme-arsenal.com/memes/06308418f56ad0d8d674c247e5ccba49.jpg )'],4,alexanderzv,find-unique-clients,alexanderzv_find-unique-clients_m4
"['### Highly likely V307 is a cumulative sum of transactions for a certain period for one unique client\n', '\n', 'This group of transactions has the same card hash and device hash, and 41 consecutively increasing numbers. Also all this transactions are fraud. \n', '\n', 'May be this knowledge will help you to significantly improve your models. \n', '\n', ""It doesn't work for all pairs of card and devices hashes, so you can improve alghoritm for searching unique clients.""]",5,alexanderzv,find-unique-clients,alexanderzv_find-unique-clients_m5
['# Bonus: You can try to decipher the values of some features'],6,alexanderzv,find-unique-clients,alexanderzv_find-unique-clients_m6
['![](https://www.dictionary.com/e/wp-content/uploads/2018/04/another-one.jpg )'],7,alexanderzv,find-unique-clients,alexanderzv_find-unique-clients_m7
"['## TL;DR:\n', '\n', '1. Classic variant:\n', '   * Mem. usage decreased from 4867.46 Mb to 2452.37 Mb (49.6% reduction)\n', '   * Number of unique values: 3285120 -> 3216884 **(2.0% lost)**\n', '2. My variant:\n', '   * Mem. usage decreased from 4847.46 Mb to 2515.03 Mb (48.1% reduction)\n', '   * Number of unique values: 3285120 -> 3285120 **(0.0% lost)**\n', '3. My variant with optional object -> category conversion (read the **Objects -> categories** section before using!):\n', '   * Mem. usage decreased from 4847.46 Mb to **1086.85 Mb (77.6% reduction)**\n', '   * Number of unique values: 3285120 -> 3285120 **(0.0% lost)**\n', '\n', '## Rationale\n', '\n', 'It seems that many competition teams use the same `reduce_mem_usage` function (with modifications) e.g. https://www.kaggle.com/kyakovlev/ieee-data-minification .\n', '\n', 'Though I see a few major drawbacks in using it as is:\n', '1. Such functions either:\n', ""    1. Don't use minimal possible types for the sake of (imaginary) safety, and therefore use more memory than actually needed.\n"", ""    2. Use float16 but don't guarantee that you don't lose precision or unique values much (see **Issue 1** below)\n"", '2. None of them try to perform float to int conversion.\n', ""3. It's done only once and don't allow you to easily minify newly created features.\n"", '\n', 'So my functons address all of these problems.\n', 'They allow using really minimal amount of memory and guarantee not losing anything (precision, na values, unique values, etc.).\n', ""And you can do minification on the fly for new columns: `df['a/b'] = sd(df['a']/df['b'])`.\n"", '\n', 'Also my `sd` (stands for `safe downcast`) function is very flexible. If you consider you can allow to lose 0.1 precision when rounding but wanna save more memory, then no problem, just set `sd(col, max_loss_limit=0.1, avg_loss_limit=0.1)`.\n', '\n', '## Objects -> categories:\n', '\n', ""My functions can do object -> category conversion as well. But it's important to remember that if you do this for train and test separately they will have different internal representation (see **Issue 2** below) and may cause issues with ML algorithms if they mess with codes.\n"", ""In my code I use a concatenated dataset with 2-level indexes so it's not a problem. See the **Load dataset** section below.\n"", '\n', 'If you want them to be converted, use `obj_to_cat=True` arg.\n', ""In this case you'll get:\n"", '* **Mem. usage decreased from 4847.46 Mb to 1086.85 Mb (77.6% reduction)**\n']",0,alexeykupershtokh,safe-memory-reduction,alexeykupershtokh_safe-memory-reduction_m0
['## Issue 1'],1,alexeykupershtokh,safe-memory-reduction,alexeykupershtokh_safe-memory-reduction_m1
['## Issue 2'],2,alexeykupershtokh,safe-memory-reduction,alexeykupershtokh_safe-memory-reduction_m2
['## Functions'],3,alexeykupershtokh,safe-memory-reduction,alexeykupershtokh_safe-memory-reduction_m3
"['## Load dataset\n', '\n', 'The dataset is prepared in https://www.kaggle.com/alexeykupershtokh/concat-dataframes']",4,alexeykupershtokh,safe-memory-reduction,alexeykupershtokh_safe-memory-reduction_m4
['## Minify'],5,alexeykupershtokh,safe-memory-reduction,alexeykupershtokh_safe-memory-reduction_m5
['## Example 1: float conversion params'],6,alexeykupershtokh,safe-memory-reduction,alexeykupershtokh_safe-memory-reduction_m6
"[""Let's try to create a new feature Series""]",7,alexeykupershtokh,safe-memory-reduction,alexeykupershtokh_safe-memory-reduction_m7
"[""Let's try to minify it with default settings""]",8,alexeykupershtokh,safe-memory-reduction,alexeykupershtokh_safe-memory-reduction_m8
"[""Oops, it didn't work. The reason is most likely in that the values are too dense (e.g. there could be values like 100.0001 and 100.0002). But as far as this feature is ordinal and we don't care about preserving all of the unique values, let's losen our minification rules.""]",9,alexeykupershtokh,safe-memory-reduction,alexeykupershtokh_safe-memory-reduction_m9
['You can see that we lost `11957 - 11882 = 75` unique values but saved 50% of memory.'],10,alexeykupershtokh,safe-memory-reduction,alexeykupershtokh_safe-memory-reduction_m10
['## Example 2: automatic float to int conversion'],11,alexeykupershtokh,safe-memory-reduction,alexeykupershtokh_safe-memory-reduction_m11
"[""let's try frequency encoding""]",12,alexeykupershtokh,safe-memory-reduction,alexeykupershtokh_safe-memory-reduction_m12
['## Example 3: C1-C14 column compression (lossy for 3 rows)'],13,alexeykupershtokh,safe-memory-reduction,alexeykupershtokh_safe-memory-reduction_m13
"['<a id=""home""></a>\n', '# IEEE Fraud Detection transactions columns reference']",0,alijs1,ieee-transaction-columns-reference,alijs1_ieee-transaction-columns-reference_m0
['## Ensembling With StackNet'],0,carlolepelaars,ensembling-with-stacknet,carlolepelaars_ensembling-with-stacknet_m0
['![](https://github.com/kaz-Anova/StackNet/raw/master/images/StackNet_Logo.png?raw=true)'],1,carlolepelaars,ensembling-with-stacknet,carlolepelaars_ensembling-with-stacknet_m1
"['In this kernel we will take a look on how to use StackNet to stack multiple levels of models in order to efficiently blend models. StackNet is a powerful package that works really well for competitions! We are going to stack a random forest on top of 3 GBM models as an example. We will use data from the [IEEE Fraud Detection](https://www.kaggle.com/c/ieee-fraud-detection) competition to explain StackNet.\n', '\n', 'StackNet was created by Kaggle Grandmaster Marios Michailidis ([kazanova](https://www.kaggle.com/kazanova)) as part of his PhD. Thanks to [Kiran Kunapuli](https://www.kaggle.com/kirankunapuli) for uploading the package as [a Kaggle dataset](https://www.kaggle.com/kirankunapuli/pystacknet) so it can conveniently be used with Kaggle kernels.\n', '\n', ""Let's dive in!""]",2,carlolepelaars,ensembling-with-stacknet,carlolepelaars_ensembling-with-stacknet_m2
['## Table Of Contents'],3,carlolepelaars,ensembling-with-stacknet,carlolepelaars_ensembling-with-stacknet_m3
"['- [Dependencies](#1)\n', '- [Metric (AUC)](#2)\n', '- [Data Preparation](#3)\n', '- [Modeling](#4)\n', '- [Evaluation](#5)\n', '- [Submission](#6)']",4,carlolepelaars,ensembling-with-stacknet,carlolepelaars_ensembling-with-stacknet_m4
"['## Dependencies <a id=""1""></a>']",5,carlolepelaars,ensembling-with-stacknet,carlolepelaars_ensembling-with-stacknet_m5
"['## Metric (AUC) <a id=""2""></a>']",6,carlolepelaars,ensembling-with-stacknet,carlolepelaars_ensembling-with-stacknet_m6
"['The Metric used in this competition is ""[Area Under ROC Curve (AUC)](https://en.wikipedia.org/wiki/Receiver_operating_characteristic)"". We create this curve by plotting the true positive rate (TPR) against the false positive rate (FPR) at different threshold settings. \n', 'This is very convenient since with binary classification problems like fraud detection the accuracy score is not that informative. For example, if we predict only 0 (not fraud) on this dataset, then we will get an accuracy score of 0.965. The AUC score will be 0.5 (no better than random). All naive baselines will get an AUC score of approximately 0.5.\n', '\n', ""To calculate the AUC score we can use [sklearn's roc_auc_score function](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html) straight out of the box.""]",7,carlolepelaars,ensembling-with-stacknet,carlolepelaars_ensembling-with-stacknet_m7
"['Image: An example of an ROC curve. AOC is a typo and should be AUC.\n', '\n', '![](https://miro.medium.com/max/722/1*pk05QGzoWhCgRiiFbz-oKQ.png)']",8,carlolepelaars,ensembling-with-stacknet,carlolepelaars_ensembling-with-stacknet_m8
['To plot the ROC curve we will use a function using sklearn and matplotlib. An example of this visualization is shown in the evaluation section of this kernel.'],9,carlolepelaars,ensembling-with-stacknet,carlolepelaars_ensembling-with-stacknet_m9
"['## Data Preparation <a id=""3""></a>']",10,carlolepelaars,ensembling-with-stacknet,carlolepelaars_ensembling-with-stacknet_m10
"['Since this kernel is meant to explain StackNet and establish a baseline we will not go into advanced feature engineering and EDA here. However, your performance will greatly benefit from feature engineering so I encourage you to explore it. A good kernel which does that for this competition can be found [here](https://www.kaggle.com/kyakovlev/ieee-gb-2-make-amount-useful-again?scriptVersionId=18874747).']",11,carlolepelaars,ensembling-with-stacknet,carlolepelaars_ensembling-with-stacknet_m11
"[""StackNet does not accept missing values (NaN's), Infinity values (inf) or values higher than 32 bytes (for example float64 or int64). Therefore, we have to fill in missing values and compress certain columns as the Pandas standard is 64 bytes. Big thanks to [Arjan Groen](https://www.kaggle.com/arjanso) for creating this convenient function. The function is taken from [this Kaggle kernel](https://www.kaggle.com/arjanso/reducing-dataframe-memory-size-by-65).""]",12,carlolepelaars,ensembling-with-stacknet,carlolepelaars_ensembling-with-stacknet_m12
"['## Modeling <a id=""4""></a>']",13,carlolepelaars,ensembling-with-stacknet,carlolepelaars_ensembling-with-stacknet_m13
"['StackNet allows you to define all kinds of models. For example, Sklearn models, LightGBM, XGBoost, CatBoost and Keras models can all be used with StackNet.\n', '\n', ""For the individual models, you are responsible for not overfitting. Therefore, it is advisable to first experiment with individual models and make sure they are sound, before combining them into StackNet. For this example we will use [sklearn's Gradient Boosting Regressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html), a [LightGBM Regressor](https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMRegressor.html#lightgbm.LGBMRegressor) and a [CatBoost Regressor](https://catboost.ai/docs/concepts/python-reference_catboostregressor.html) in the 1st level. Then we will train a [Random Forest Regressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html) in level 2, which takes the predictions of the models in the 1st level as input. StackNet takes care of the stacking and cross validation.""]",14,carlolepelaars,ensembling-with-stacknet,carlolepelaars_ensembling-with-stacknet_m14
"['The model tree that StackNet takes as input is a list of lists. The 1st list defines the 1st level, the 2nd one the 2nd level, etc. You can build a model tree of arbitrary depth and width.']",15,carlolepelaars,ensembling-with-stacknet,carlolepelaars_ensembling-with-stacknet_m15
"['The model is compiled and fitted through the a familiar sklearn-like API. The StackNetClassifier will perform cross-validation (CV) and will output the CV scores for each model. To make sure we can output a probability of fraud we specify ""use_proba=True"".\n', '\n', 'The ""folds"" argument in StackNetClassifier can also accept an iterable of train/test splits. Since the target distibution is imbalanced you can probably improve on the CV strategy by first yielding stratified train/test split with for example [sklearn\'s StratifiedKFold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html).']",16,carlolepelaars,ensembling-with-stacknet,carlolepelaars_ensembling-with-stacknet_m16
"['## Evaluation <a id=""5""></a>']",17,carlolepelaars,ensembling-with-stacknet,carlolepelaars_ensembling-with-stacknet_m17
"['The blue line signifies the baseline AUC which is 0.5. The final validation score is the area under the orange curve, which is mentioned in the plot.']",18,carlolepelaars,ensembling-with-stacknet,carlolepelaars_ensembling-with-stacknet_m18
"['## Submission <a id=""6""></a>']",19,carlolepelaars,ensembling-with-stacknet,carlolepelaars_ensembling-with-stacknet_m19
"['To check if the predictions are sound, we check the format of our submission and compare our prediction distribution with that of the target distribution in the training set.']",20,carlolepelaars,ensembling-with-stacknet,carlolepelaars_ensembling-with-stacknet_m20
"['Try to experiment with [StackNet](https://github.com/h2oai/pystacknet) yourself. The possibilities are almost endless!\n', '\n', 'If you want to check out another solution using PyStackNet, check out [this Kaggle kernel on the Titanic dataset by Yann Berthelot](https://www.kaggle.com/yannberthelot/pystacknet-working-implementation).\n', '\n', '**If you like this Kaggle kernel, feel free to give an upvote and leave a comment! I will try to implement your suggestions in this kernel!**']",21,carlolepelaars,ensembling-with-stacknet,carlolepelaars_ensembling-with-stacknet_m21
"['# RAPIDS - Feature Engineering - 1st Place Fraud Comp - [0.96]\n', ""The secret to creating a high scoring model in Kaggle's IEEE CIS Fraud Competition is feature engineering. A list of feature engineering techniques is posted [here][1]. The most important features in Fraud Comp are new columns created from group aggregations of other columns. Why this works is explained [here][2]. Computing group aggregations can naturally be done in parallel and benefit from using GPU instead of CPU.\n"", '\n', 'This notebook contains the XGBoost model of the 1st place Fraud Comp solution converted to use RAPIDS cuDF. (The entire 1st place solution is an ensemble of XGBoost, CatBoost, and LightGBM with additional post processing described [here][4]). To read one million rows from disk and create 262 features on CPU using Pandas takes 5 minutes. To read and create those features on GPU with RAPIDS cuDF takes 20 seconds as shown below. RAPIDS is 15x faster!\n', '\n', '![speedup2.JPG](attachment:speedup2.JPG)\n', '\n', 'Individual GPU times are listed beneath code blocks below. Pandas CPU times are displayed in the notebook [here][3] beneath code blocks.\n', '\n', '[1]: https://www.kaggle.com/c/ieee-fraud-detection/discussion/108575\n', '[2]: https://www.kaggle.com/c/ieee-fraud-detection/discussion/111453\n', '[3]: https://www.kaggle.com/cdeotte/xgb-fraud-with-magic-0-9600\n', '[4]: https://www.kaggle.com/c/ieee-fraud-detection/discussion/111284']",0,cdeotte,rapids-feature-engineering-fraud-0-96,cdeotte_rapids-feature-engineering-fraud-0-96_m0
"['# Install RAPIDS\n', ""Here we install RAPIDS from a Kaggle dataset taking 1 minute. (Install from Conda shown [here][1], if this doesn't work).\n"", '\n', '[1]: https://www.kaggle.com/cdeotte/rapids-data-augmentation-mnist-0-985']",1,cdeotte,rapids-feature-engineering-fraud-0-96,cdeotte_rapids-feature-engineering-fraud-0-96_m1
"['# GPU Load Data\n', 'Here we read the data from the disk with cuDF directly into the GPU. With CPU Pandas this takes 46 seconds. With GPU RAPIDS this takes 4.7 seconds! ']",2,cdeotte,rapids-feature-engineering-fraud-0-96,cdeotte_rapids-feature-engineering-fraud-0-96_m2
"['# GPU Preprocess\n', ""First we normalize D Columns, label encode all categorical columns, shift numerics postive, and fill NaN with -1. Note that RAPIDS cuDF has already label encoded all the categorical variables when they were read from disk if `dtype='category'` was used instead of `dtype='str'`.""]",3,cdeotte,rapids-feature-engineering-fraud-0-96,cdeotte_rapids-feature-engineering-fraud-0-96_m3
"['# GPU Encoding Functions\n', ""The following four functions are Numba CUDA JIT kernels. These functions are optimized to use Nvidia GPU. We will use these together with RAPIDS cuDF's `groupyby(col,method='cudf').apply_grouped(func)` to create blazingly fast custom feature engineering functions! Tutorials about this are [here][1], [here][2], and [here][3].\n"", '\n', '[1]: https://rapidsai.github.io/projects/cudf/en/0.11.0/guide-to-udfs.html\n', '[2]: https://github.com/daxiongshu/notebooks-extended/blob/kdd_plasticc/advanced_notebooks/tutorials/rapids_customized_kernels.ipynb\n', '[3]: https://numba.pydata.org/numba-doc/latest/cuda/index.html']",4,cdeotte,rapids-feature-engineering-fraud-0-96,cdeotte_rapids-feature-engineering-fraud-0-96_m4
"['# GPU Feature Engineering\n', 'Below is where we create all our new engineered features. The work below takes 10 seconds. The work above took 10 seconds. Using RAPIDS GPU is 15x faster than Pandas CPU.']",5,cdeotte,rapids-feature-engineering-fraud-0-96,cdeotte_rapids-feature-engineering-fraud-0-96_m5
['# Local Holdout Validation'],6,cdeotte,rapids-feature-engineering-fraud-0-96,cdeotte_rapids-feature-engineering-fraud-0-96_m6
['# Cross Validation and Inference'],7,cdeotte,rapids-feature-engineering-fraud-0-96,cdeotte_rapids-feature-engineering-fraud-0-96_m7
"['# Submit to Kaggle\n', 'This submission achieves LB 0.960. If we post process these predictions we achieve LB 0.962. If we ensemble these with CatBoost and LGBM models, we achieve LB 0.968.']",8,cdeotte,rapids-feature-engineering-fraud-0-96,cdeotte_rapids-feature-engineering-fraud-0-96_m8
"['I have just taken four columns \n', '\n', '* TransactionAmt\n', '* ProductCD\n', '* card4 \n', '* isFraud\n', '\n', 'I want to see the relationship of first three that is TransactionAmt, ProductCD, card4 with isFraud and also want to **fit** Decesion tree and see the performance of it']",0,dataraj,eda-and-decision-tree-with-data-imbalancetackling,dataraj_eda-and-decision-tree-with-data-imbalancetackling_m0
"['### From output above, it is sure that this data is imbalance. ']",1,dataraj,eda-and-decision-tree-with-data-imbalancetackling,dataraj_eda-and-decision-tree-with-data-imbalancetackling_m1
"['### Description of TransactionAmt groupd on isFraud column \n', '\n', '* Mean value of TransactionAmt for catogry  1 is high\n', '* Median value of TransactionAmt for catogry  1 is less than catogry 0 \n', '* The TransactionAmt for catogry  1 is  more skewed on right side for category 1']",2,dataraj,eda-and-decision-tree-with-data-imbalancetackling,dataraj_eda-and-decision-tree-with-data-imbalancetackling_m2
['### How many type of cards are used for transactions ?'],3,dataraj,eda-and-decision-tree-with-data-imbalancetackling,dataraj_eda-and-decision-tree-with-data-imbalancetackling_m3
['### Stratified sampling '],4,dataraj,eda-and-decision-tree-with-data-imbalancetackling,dataraj_eda-and-decision-tree-with-data-imbalancetackling_m4
['### Deviding data into training and testing part'],5,dataraj,eda-and-decision-tree-with-data-imbalancetackling,dataraj_eda-and-decision-tree-with-data-imbalancetackling_m5
['# Hope you have enjoyed this kernel. If enjoyed kindly upvote it '],6,dataraj,eda-and-decision-tree-with-data-imbalancetackling,dataraj_eda-and-decision-tree-with-data-imbalancetackling_m6
"['## About this kernel\n', '\n', 'Hey, everyone!\n', '\n', ""This was my first solution to this competition. There is nothing revolutionary here as this is my first competition and I'm trying to learn with all the public kernels.\n"", '\n', 'I decided to share this solution to help people that are beginners like me, to at least give them an idea of what to do and where to begin.\n', '\n', ""Later, I'll add a cell with all the advice people gave me in the discussions so we can talk more about it here. I'll also add a reference to the kernels I used here.\n"", '\n', 'Please, give me some feedback if you can. I would love to hear what you believe could be improved in this kernel.\n', '\n', 'Hope you are enjoying this competition as much as I am. Good luck and I hope you like this kernel.\n', '\n', '[**UPDATE**]  \n', ""In the comments section, @cebeci told me the merging process was wrong and it really was. I'm sorry about it, merging is now correct.  \n"", 'Also in the comments, @lftuwujie told me the GPU makes almost no difference to LGBM. I tested it and he was (as expected) correct, so this was also changed.\n', '\n', ""Thank you for all the feedback and support. It's been really nice to learn with all of you.""]",0,davidcairuz,feature-engineering-lightgbm,davidcairuz_feature-engineering-lightgbm_m0
"['# Dating Dataset Creation with a Browser\n', '\n', 'In this kernel I do some basic exploratory data analysis on the IEEE Fraud Detection dataset. The purpose of this simple notebook is to estimate the starting date of this dataset.\n', '\n', ""For that, we will analyze the timestamp `TransactionDT` provided for each transaction. `TransactionDT` is the time elapsed in second since dataset's starting date and the transaction date. We will combine this piece of data with the browser used to perform the transaction (`id_31`) to discover the dataset's starting date.\n"", '\n', '![adam-tinworth-OJWivczp3aY-unsplash-resized.jpg](attachment:adam-tinworth-OJWivczp3aY-unsplash-resized.jpg)']",0,ebouteillon,dating-dataset-creation-with-a-browser,ebouteillon_dating-dataset-creation-with-a-browser_m0
['###\xa0Import libraries'],1,ebouteillon,dating-dataset-creation-with-a-browser,ebouteillon_dating-dataset-creation-with-a-browser_m1
['### Load data'],2,ebouteillon,dating-dataset-creation-with-a-browser,ebouteillon_dating-dataset-creation-with-a-browser_m2
"['### Browsers available\n', '\n', '`id_31` contains browser in use during a card-not-present transaction (CNP). What are the most used browsers?']",3,ebouteillon,dating-dataset-creation-with-a-browser,ebouteillon_dating-dataset-creation-with-a-browser_m3
"['There are several versions of **chrome** browser in our top of the most used browsers in this dataset. It seems a good candidate for further analysis.\n', '\n', '###\xa0Chrome versions\n', '\n', 'List of chrome version in this dataset is a good starting point:']",4,ebouteillon,dating-dataset-creation-with-a-browser,ebouteillon_dating-dataset-creation-with-a-browser_m4
"['We have a large set of different chrome releases ranging from version **39 to 71** and for **3 different platforms** (Desktop, Android and iOS).\n', '\n', '### Chrome daily usage\n', '\n', 'What is the life cycle of chrome versions?']",5,ebouteillon,dating-dataset-creation-with-a-browser,ebouteillon_dating-dataset-creation-with-a-browser_m5
['What are the **Desktop** version of chrome used?'],6,ebouteillon,dating-dataset-creation-with-a-browser,ebouteillon_dating-dataset-creation-with-a-browser_m6
"['We can clearly see that the **Desktop version** of Chrome is regurlarly updated by Google and users are mostly up-to date thanks to the automatic chrome updater.\n', '\n', 'I also notice that versions are not deployed immediately to all users. When a new version is relased, there is first a linear ramp-up of the number of updated users and then it is deployed to all users. This can be seen for versions 68, 69, 70 and 71. Probably Google is monitoring first updated installations to detect problems before to make a versionglobally available.\n', '\n', 'What about **Android** version?']",7,ebouteillon,dating-dataset-creation-with-a-browser,ebouteillon_dating-dataset-creation-with-a-browser_m7
"['**Android** users have mostly up-to-date chrome version on their devices. But old versions of chrome looks more frequent and older than on desktop (version 39 still in use compared to 49 on desktop).\n', '\n', 'Lastly, the second most frequent mobile system: **Apple iOS**']",8,ebouteillon,dating-dataset-creation-with-a-browser,ebouteillon_dating-dataset-creation-with-a-browser_m8
"['**Apple** users are the small portion of users, but they are keeping their browser up-to-date as they have the smallest number of different chrome versions in the field.']",9,ebouteillon,dating-dataset-creation-with-a-browser,ebouteillon_dating-dataset-creation-with-a-browser_m9
"['From these graphs, we easily guess when a new chrome browser is relased by looking the rapid increase of the version count for each platform. Apparently this dataset covers the release date of chrome 63 to 71 for all the 3 platforms.']",10,ebouteillon,dating-dataset-creation-with-a-browser,ebouteillon_dating-dataset-creation-with-a-browser_m10
"['### Chrome release dates\n', '\n', 'A quick search on Internet and [wikipedia](https://en.wikipedia.org/wiki/Google_Chrome_version_history) in peculiar gives us the following release dates for the different versions of chrome. It is ranging from 2014 to 2019.']",11,ebouteillon,dating-dataset-creation-with-a-browser,ebouteillon_dating-dataset-creation-with-a-browser_m11
"['### Estimate dataset start point for each chrome version\n', '\n', 'We can now estimate the `TransactionDT` associated with the release date of chrome 63 to 71 by taking the smallest `TransactionDT` for each of these versions in the dataset.']",12,ebouteillon,dating-dataset-creation-with-a-browser,ebouteillon_dating-dataset-creation-with-a-browser_m12
"['### And the origin is...\n', '\n', ""To reduce outliners influence, we'll take the median of estimated dates. Therefore we estimate *the beginning of time* for this dataset is around **28th november 2017**:""]",13,ebouteillon,dating-dataset-creation-with-a-browser,ebouteillon_dating-dataset-creation-with-a-browser_m13
['### Conclusion'],14,ebouteillon,dating-dataset-creation-with-a-browser,ebouteillon_dating-dataset-creation-with-a-browser_m14
"['We were supposing in our analysis that all users were using the [""stable"" channel](https://www.chromium.org/getting-involved/dev-channel) of chrome browser to determine this dataset\'s starting date. But if a single user is using a ""beta"", ""dev"" or ""canary"" channel version of chrome our estimation is biased. These non-""stable"" channel versions are generally released before the ""stable"" version, then our estimate will be biased to an earlier date than the right one.\n', '\n', 'This date is close to what other kagglers found (30th november) by considering days with highest peaks as christmas or cybermonday.\n', '\n', '\n', 'I hope you enjoyed reading this notebook as much as I am while writing it. I let the reader do the same analysis with other browsers like **safari** or **firefox** as an exercice.\n', '\n', 'Take care and Happy kaggling! 👍']",15,ebouteillon,dating-dataset-creation-with-a-browser,ebouteillon_dating-dataset-creation-with-a-browser_m15
"['# Missing data\n', '\n', 'This kernel aims to do some exploratory analysis of the missing data in the training and test sets. \n', '\n', '## Key findings:\n', '\n', '- The distribution of missing values is different between the training and test set (Figure 1).\n', '- There seems to be a distinct point in time when this distribution changes (Figures 2 and 3).\n', '- We have identified the main features where this change is occuring. \n', '\n', '\n', 'The different distributions could affect models ability to generalize well to the test set, and so the missing data will have to be appropiately handled.']",0,fchmiel,eda-of-missing-data,fchmiel_eda-of-missing-data_m0
"['This notebook will originally focus on the number of NaNs (i.e., missing values) for each instance (row) in training and tests set.']",1,fchmiel,eda-of-missing-data,fchmiel_eda-of-missing-data_m1
"['## Overview of the missing values per training instance \n', '\n', 'The graph below (Figure 1), displays a histogram of the number of NaN (missing) values for a given training instance. Overall it shows there are, on average, a greater number of missing values in the training set than compared to the test set. ']",2,fchmiel,eda-of-missing-data,fchmiel_eda-of-missing-data_m2
"['### Figure 1\n', '\n', 'This shows the distribution of the number of missing values for the training and test sets. ']",3,fchmiel,eda-of-missing-data,fchmiel_eda-of-missing-data_m3
"['### Temporal split in missing data\n', '\n', ""In Figure 2 (below) you can clearly see there is shift in the number of missing values per training instance partially through the test set data (I've confirmed this with additional moving average plots - not shown). \n"", '\n', 'This could reflect a change in recording practices or a temporal dependence for more values to be missing (e.g., some seasonal dependence to the transactions?)']",4,fchmiel,eda-of-missing-data,fchmiel_eda-of-missing-data_m4
"['### Figure 2\n', '\n', ""This is a scatter plot of the number of missing values for each instance as a function of 'time', for both the training and test sets.  \n"", '\n', 'The dashed grey lines help to guide the eyes to the regions used in the histograms of Figure 3']",5,fchmiel,eda-of-missing-data,fchmiel_eda-of-missing-data_m5
"['### Figure 3\n', '\n', ""Next we look at histograms of the number of missing data entries for each instance at three key time periods: during the training set, before the 'step' in the test set and after the 'step' in the test set. \n"", '\n', 'It is clear that the train and test at early times appear equivalent, but after approximately Transaction DT = 2.3e7 the test set changes: there are signifcantly less missing entries for a given instance of the test set.']",6,fchmiel,eda-of-missing-data,fchmiel_eda-of-missing-data_m6
"['## Which features have reduced in the number of missing values\n', '\n', 'We can see there are some features with a lot less missing values in the test sets (namely D12 and a number of the VI features).\n', '\n', 'We will have to look into what these features are and how we can deal with the missing data. \n', '\n']",7,fchmiel,eda-of-missing-data,fchmiel_eda-of-missing-data_m7
['# Work in progress'],8,fchmiel,eda-of-missing-data,fchmiel_eda-of-missing-data_m8
"['## Looking at the distribution of values for a feature which changes significantly.\n', '\n', 'We will look at D15. ']",9,fchmiel,eda-of-missing-data,fchmiel_eda-of-missing-data_m9
['## Does more missing data increase the chance of a Fradulent transaction'],10,fchmiel,eda-of-missing-data,fchmiel_eda-of-missing-data_m10
"['Clearly, there are certain numbers of missing data which correlates with a an increased chance of the transaction being fradulent. \n', '\n', 'Where there is a 100% chance of the transaction being fradulent this corresponds to there being only one training instance with this number of missing variables.\n', '\n', '\n', 'We will have to investigate which features are missing and what this can tell us about the transaction.']",11,fchmiel,eda-of-missing-data,fchmiel_eda-of-missing-data_m11
"['# Fraud Detection \n', '* Author: Grant Gasser\n', '* Last Edit: 8/18/2019\n', '* Kaggle: ""In this competition you are predicting the probability that an online transaction is fraudulent, as denoted by the binary target `isFraud.`""']",0,grantgasser,fraud-detection,grantgasser_fraud-detection_m0
"['## Summary\n', ""**This is my first serious attempt at a Kaggle competition. As such, I would really appreciate some feedback or tips for improving performance. If you enjoyed this notebook and it helped you, please leave a thumbs up! Though I've written most of the code myself, I have found the other public kernels very helpful and would encourage you do browse through them to look for other good ideas.**\n"", '\n', '* **Public Leaderboard Results:** (I plan on using some of the previous submission files for ensembling)\n', '* Random Forest filled NaNs with -999: `.872`\n', '* XGBoost filled NaNs with -999: `.938`, submission file: `baseline_xgboost.csv`\n', '* XGBoost impute mean for numerical NaNs and most common cat for categorical NaNs, also normalized numerical vars: `.878`\n', '* XGBoost impute mean for numerical NaNs and most common cat for categorical NaNs, no normalization: `.932`, submission file: `preprocessed_xgboost`\n', '* XGBoost, impute mean for numerical NaNs, do not impute most common category for categorical NaNs, no normalization: `.934`, file: `preprocessed2_xgboost`. **NOTE**: imputing mean for numerical NaNs and most common category for categorical NaNs did not seem to help for XGBoost. \n', '* Version 21: hyperparameter tuning with XGBoost (Grid Search or Random Search), `.9284`, `xgboost_with_tuning`\n', '* `.9226`, `xgboost_with_tuning2`']",1,grantgasser,fraud-detection,grantgasser_fraud-detection_m1
"['## ENSEMBLING:\n', '* Averaging out my previous predictions using the files listed above ^ \n', '* data: `https://www.kaggle.com/grantgasser/previous-submissions`']",2,grantgasser,fraud-detection,grantgasser_fraud-detection_m2
['## Libraries'],3,grantgasser,fraud-detection,grantgasser_fraud-detection_m3
['## View provided files'],4,grantgasser,fraud-detection,grantgasser_fraud-detection_m4
"['## Ensemble\n', '* Will have to comment out everything below this\n', '* Load previous submissions']",5,grantgasser,fraud-detection,grantgasser_fraud-detection_m5
"['### Weighted Avg\n', '* Based on scores (more weight for model outputs that had better scores)\n', ""* `.05, .05, .1, .8` -> `.9392`, minor improvement from the best model's score of `.9381`, file: `ensemble5.csv`""]",6,grantgasser,fraud-detection,grantgasser_fraud-detection_m6
"['# Data Description \n', '* As provided by VESTA: https://www.kaggle.com/c/ieee-fraud-detection/discussion/101203#latest-586800\n', '\n', '#### Transaction Table\n', '* TransactionDT: timedelta from a given reference datetime (not an actual timestamp)\n', '* TransactionAMT: transaction payment amount in USD\n', '* ProductCD: product code, the product for each transaction\n', '* card1 - card6: payment card information, such as card type, card category, issue bank, country, etc.\n', '* addr: address\n', '* dist: distances between (not limited) billing address, mailing address, zip code, IP address, phone area, etc.\n', '* P_ and (R__) emaildomain: purchaser and recipient email domain\n', '* C1-C14: counting, such as how many addresses are found to be associated with the payment card, etc. The actual meaning is masked.\n', '* D1-D15: timedelta, such as days between previous transaction, etc.\n', '* M1-M9: match, such as names on card and address, etc.\n', '* Vxxx: Vesta engineered rich features, including ranking, counting, and other entity relations.\n', '\n', '** Categorical Features: **\n', '* ProductCD\n', '* card1 - card6\n', '* addr1, addr2\n', '* Pemaildomain Remaildomain\n', '* M1 - M9\n', '\n', '---\n', '\n', '#### Identity Table\n', 'Variables in this table are identity information – network connection information (IP, ISP, Proxy, etc) and digital signature (UA/browser/os/version, etc) associated with transactions. \n', ""They're collected by Vesta’s fraud protection system and digital security partners.\n"", '(The field names are masked and pairwise dictionary will not be provided for privacy protection and contract agreement)\n', '\n', '** Categorical Features: **\n', '* DeviceType\n', '* DeviceInfo\n', '* id12 - id38']",7,grantgasser,fraud-detection,grantgasser_fraud-detection_m7
['## Load and explore data'],8,grantgasser,fraud-detection,grantgasser_fraud-detection_m8
['### View tables'],9,grantgasser,fraud-detection,grantgasser_fraud-detection_m9
"['# Merge identity and transaction tables\n', '* Per Kaggle: ""The data is broken into two files `identity` and `transaction`, which are joined by `TransactionID`. Not all transactions have corresponding identity information.\n', '* Merge identity and transaction tables with `TransactionID` as the key""\n', '* Since ""not all transactions have corresponding identity information,"" we will use a (left) outer join, using pandas merge function since a key might not appear in both tables']",10,grantgasser,fraud-detection,grantgasser_fraud-detection_m10
['# Baseline Model with minimal pre-processing (.938)'],11,grantgasser,fraud-detection,grantgasser_fraud-detection_m11
"['### XGBoost Classifier\n', '* See [notebook](https://www.kaggle.com/inversion/ieee-simple-xgboost) for starter code']",12,grantgasser,fraud-detection,grantgasser_fraud-detection_m12
"['### Replacing with Missing Values\n', '* Using code from: https://www.kaggle.com/inversion/ieee-simple-xgboost']",13,grantgasser,fraud-detection,grantgasser_fraud-detection_m13
['## Train XGBoost model'],14,grantgasser,fraud-detection,grantgasser_fraud-detection_m14
['### XGBoost AUC = .938'],15,grantgasser,fraud-detection,grantgasser_fraud-detection_m15
"['### Data Types\n', '* Before diving into EDA, look at data types of current features and see if they need to be changed']",16,grantgasser,fraud-detection,grantgasser_fraud-detection_m16
"['**Categorical Features:**\n', '* ProductCD\n', '* card1 - card6\n', '* addr1, addr2\n', '* Pemaildomain Remaildomain\n', '* M1 - M9\n', '* DeviceType\n', '* DeviceInfo\n', '* id12 - id38']",17,grantgasser,fraud-detection,grantgasser_fraud-detection_m17
"['### Thoughts\n', ""* Some of these should not be numerical data (e.g. card1-card6 should be 'object' types, not int64 or float64)\n"", '* The next few cells changes this']",18,grantgasser,fraud-detection,grantgasser_fraud-detection_m18
['# EDA'],19,grantgasser,fraud-detection,grantgasser_fraud-detection_m19
"['## Check Missing Values\n', '* Hint: there are lots of them']",20,grantgasser,fraud-detection,grantgasser_fraud-detection_m20
['## Analyze Categorical Variables'],21,grantgasser,fraud-detection,grantgasser_fraud-detection_m21
"['### Thoughts\n', '* There are several categorical variables with many categories, suggesting that 1-Hot encoding might be too high dimensional\n', '* It may be more prudent to do label encoding (1,2,3,..) to limit dimensionality\n', '* Drawback with Label Encoding: softly implies there is some order to the categories since the categories are now numbered']",22,grantgasser,fraud-detection,grantgasser_fraud-detection_m22
"['## Explore Labels\n', '* Note the class imbalance\n', '* About 3.5% of train examples are fraudulent']",23,grantgasser,fraud-detection,grantgasser_fraud-detection_m23
"['## Compare fraud and non-fraud (within training set)\n', '1. Compare the difference in means of numerical features between the fraud and non-fraud transactions. \n', '\n', '2. Compare the difference in distributions of categorical features between the fraud and non-fraud transactions. \n', ' ']",24,grantgasser,fraud-detection,grantgasser_fraud-detection_m24
['### Look at a few fraudulent transactions'],25,grantgasser,fraud-detection,grantgasser_fraud-detection_m25
['## Compare train and test'],26,grantgasser,fraud-detection,grantgasser_fraud-detection_m26
"['### Note TransactionDT has no overlap\n', '* As mentioned: https://www.kaggle.com/robikscube/ieee-fraud-detection-first-look-and-eda\n', '* Not sure what to do here. Maybe transform so that each value is relative to its range?']",27,grantgasser,fraud-detection,grantgasser_fraud-detection_m27
"['## Takeaways from EDA\n', '### There are lots of missing values\n', '### There is significant class imbalance (Only ~20,000 out of 590,000 are fraudulent, or 3.5 %)\n', '* Thus, a classifier that always predicts not fraud (0) would have 96.5% accuracy (on the training set, the test set is similar)\n', '\n', '\n', '### TRAIN SET: Comparing means of numerical features among fraud and non-fraud transactions:\n', '* `TransactionDT` - fraudulent transactions 4.5% higher\n', '* `TransactionAmt` - fraudulent transactions 11% more expensive\n', '\n', '### TRAIN SET: Comparing distributions of categorical variables among fraud and non-fraud transactions:\n', '* Take a look at the above cell to see the comparison\n', '* Some of these may spurious, but with 20,000 fraudulent examples, they could imply something\n', ""* `ProductCD` - 39% of fraud transactions are 'C', but only 11% of non-fraud transactions are 'C'\n"", '* `card1` - looks similar\n', '* `card4` - distribution looks similar\n', '* `card6` - fraud transactions distributed evenly (52/48) between debit and credit whereas non-fraud transactions are mostly debit (76%)\n', '* `P_emaildomain` - 13% of fraud comes from hotmail email vs. 9% non-fraud is hotmail email \n', '* `R_emaildomain` - 60% of emails on receiving end of fraud are gmail vs. only 40% for non-fraud\n', ""* `id_29` - 70% are 'Found' in the fraud examples vs. 52% in the non-fraud\n"", '* `id_30` - Though MAC OS versions show up on non-fraud top 10, do not show up in top 10 for fraud, implying fraud less common on MAC\n', '* `DeviceType` - fraud was about evenly distributed (50/50) between mobile and desktop, most non-fraud on desktop (61%)\n', '* `DeviceInfo` - similar to what id_30 implied, MAC used for 11% of non-fraud transactions but just 3% of fraud transactions\n', '\n', '\n', '### Comparing train distribution and test distribution\n', '* Remember, train size is $560,000$ and test size is $500,000$\n', '* Other than `TransactionDT`, the distributions look similar\n', '* Note that since the test set is later in time, there are some features where the distributions are almost certain to be different\n', '* e.g. `id_31` represents the browser used. For the train set, the most common browser was **chrome 63** at 16%. In the test set, the most common was **chrome 70**.\n', '7 versions later and **chrome 63** did not even show up in the top 10 most common browser for the test set, unsurprisingly.\n', '* Should I drop `id_31` and other columns affected by time or let the model weight it?\n', '* Also, looking at `DeviceType`, 60% of transactions in the train set were done on desktop vs. 54% on desktop in test set. \n', 'Could this represent the increasing usage of mobile? Is there that much of a time difference between the train and test set?']",28,grantgasser,fraud-detection,grantgasser_fraud-detection_m28
['# Pre-processing'],29,grantgasser,fraud-detection,grantgasser_fraud-detection_m29
"['### Remove features with large amounts of missing data\n', '* For computational purposes, removing features that have 80% (arbitrary number) or more missing values in the training set\n', '* May come back and try different values for this cutoff\n', '* Fill in remaining missing values with mean or median']",30,grantgasser,fraud-detection,grantgasser_fraud-detection_m30
"['### In next iteration: come back and inspect columns being dropped\n', '* See if some should be kept and receive imputed values']",31,grantgasser,fraud-detection,grantgasser_fraud-detection_m31
['## Replace missing values with mean (numerical) and most common category (categorical)'],32,grantgasser,fraud-detection,grantgasser_fraud-detection_m32
"[' ### Reminder\n', ' **NOTE:** with fillna, replace and other pandas functions, make sure you set the variable, because it returns the transformed object\n', ' * e.g. `df[feature] = df[feature].replace()` instead of just `df[feature].replace()`\n', ' * **Note:** Just replacing NaNs with -999 and letting XGBoost handle it, will use this function if try different models such as Neural Network']",33,grantgasser,fraud-detection,grantgasser_fraud-detection_m33
['### Any remaining missing values?'],34,grantgasser,fraud-detection,grantgasser_fraud-detection_m34
"['## Label Encoding\n', '* Change categorical variable data to numbers so that computer can understand\n', ""* e.g. if the encoding is: `['mastercard', 'discover', 'visa']` based on index, then data like `['visa', 'visa', 'mastercard', 'discover', 'mastercard']` would be encoded as `[2, 2, 0, 1, 0]`""]",35,grantgasser,fraud-detection,grantgasser_fraud-detection_m35
"['### Normalize Variables\n', '* For speed of convergence and numerical stability\n', '* Also to ensure variables with larger numbers do not dominate the model (e.g. TransactionAmt)\n', '* Normalize numerical variables: $x_{i,j} = \\frac{x_{i,j} - \\mu_j}{\\sigma_j}$ where $i$ is the row, $j$ is the column, $\\mu_j$ is the mean of the column and $\\sigma_j$ is the std of the col\n', '* After the transformation, we will have $\\mu_j = 0$ and $\\sigma_j = 1$ for each numerical column/feature $j$\n', '* Could also try Min-Max scaling too which gives $x_j \\in (0,1)$ for all $i$.']",36,grantgasser,fraud-detection,grantgasser_fraud-detection_m36
"['## Skip normalize to see effect on performance\n', '* Pre-processed XGBoost score `.878` vs. `.938` with no pre-processing\n', '* **Note:** After removing normalization for XGBoost, performance jumped from `.878` to `.932`. Normalization may only be necessary or helpful with neural nets and similar algorithms']",37,grantgasser,fraud-detection,grantgasser_fraud-detection_m37
"['### Reduce memory usage before fitting XGBoost\n', '* Thanks to https://www.kaggle.com/iasnobmatsu/xgb-model-with-feature-engineering']",38,grantgasser,fraud-detection,grantgasser_fraud-detection_m38
"['## XGBoost\n', '* https://developer.ibm.com/code/2018/06/20/handle-imbalanced-data-sets-xgboost-scikit-learn-python-ibm-watson-studio/\n', '* XGBoost is an extreme gradient boosting algorithm based on trees that tends to perform very well out of the box compared to other ML algorithms.\n', '* XGBoost is popular with data scientists and is one of the most common ML algorithms used in Kaggle Competitions.\n', '* XGBoost allows you to tune various parameters.\n', '* XGBoost allows parallel processing.']",39,grantgasser,fraud-detection,grantgasser_fraud-detection_m39
"['## Fit the XGBoost Classifier Again using Cross Validation\n', '* See how the performance differs after imputing values and normalizing data\n', '* The baseline score was `.938`']",40,grantgasser,fraud-detection,grantgasser_fraud-detection_m40
"['### Hyperparameter tuning with GridSearch and RandomizedSearch\n', '* [XGBoost parameters](https://xgboost.readthedocs.io/en/latest/parameter.html)\n', '* [GridSearch](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV) - **Takes too much RAM**, exhaustive search of the parameters, expensive but finds the optimal set\n', '* set `scale_pos_weight` to adjust for class imbalance, common to do `sum(neg samples) / sum(pos samples)` which would be about 30 in this data set\n', '* control overfitting: `max_depth`, `min_child_weight`, `gamma` per xgboost docs']",41,grantgasser,fraud-detection,grantgasser_fraud-detection_m41
"['## TODO\n', '* GridSearchCV and RandomizedSearchCV take too much RAM\n', '* Will write my own grid search loop and be more efficient with RAM']",42,grantgasser,fraud-detection,grantgasser_fraud-detection_m42
"['### ^ Stopped early\n', '* Changing `gamma` does not seem to affect the performance\n', '* Adding more estimators and more max depth will improve performance on a subset of the test set, but has not led to improvement on the test set']",43,grantgasser,fraud-detection,grantgasser_fraud-detection_m43
"[""In this kernel I'm trying to fill some NaNs values using one intresting observation.""]",0,grazder,filling-card-nans,grazder_filling-card-nans_m0
"['*just random pic idk*\n', '![](https://sun9-31.userapi.com/c857628/v857628861/4c59e/JiPqE9xmzjs.jpg)']",1,grazder,filling-card-nans,grazder_filling-card-nans_m1
"[""I've noticed that there are cases in card columns that depends on other card columns. So using that approach we can fill some NaNs in data. Let's look at the data!""]",2,grazder,filling-card-nans,grazder_filling-card-nans_m2
"[""Let's count all NaNs in every card columns""]",3,grazder,filling-card-nans,grazder_filling-card-nans_m3
"['We can see that card2 is the most NaN card feature. What is more, card3, card4 and car6 in test have 2 times more NaNs values.']",4,grazder,filling-card-nans,grazder_filling-card-nans_m4
"[""Let's look ratio of missing values to the total number of rows.""]",5,grazder,filling-card-nans,grazder_filling-card-nans_m5
['Not very high ratios though.'],6,grazder,filling-card-nans,grazder_filling-card-nans_m6
['# Card1 and Card2'],7,grazder,filling-card-nans,grazder_filling-card-nans_m7
['There is dependency between сard2 and card1 values.  '],8,grazder,filling-card-nans,grazder_filling-card-nans_m8
"['In the dataset we can found a lot of cases like that. Where most of the values are the same, but there are some missing values. So we can assume that in NaN rows should be that only value which occurs in that card1 category. ']",9,grazder,filling-card-nans,grazder_filling-card-nans_m9
"[""Let's count unique values for each card1 category.""]",10,grazder,filling-card-nans,grazder_filling-card-nans_m10
['We can see that most of the card1 category have only one unique value. '],11,grazder,filling-card-nans,grazder_filling-card-nans_m11
"[""Now let's count amount of the missing values for amount of the unique values.""]",12,grazder,filling-card-nans,grazder_filling-card-nans_m12
"['Hm. There are a lot of missing values for categorys where there is no values(only NaNs) and where only one value.\n', 'So we can do that:\n', '\n', '* Fill NaNs in 1-amount category with most frequent value\n', '* Treat 0-amount category NaNs as only one category. We can just encode it somehow.']",13,grazder,filling-card-nans,grazder_filling-card-nans_m13
['But right now we will focus only on 1-amount category and fill NaNs with most frequent value in card1 category.'],14,grazder,filling-card-nans,grazder_filling-card-nans_m14
['# Card1 and Card3'],15,grazder,filling-card-nans,grazder_filling-card-nans_m15
"[""Let's do all the same but for card3 category.""]",16,grazder,filling-card-nans,grazder_filling-card-nans_m16
['So we filled almost all NaNs in card3.'],17,grazder,filling-card-nans,grazder_filling-card-nans_m17
['# Card1 and Card4'],18,grazder,filling-card-nans,grazder_filling-card-nans_m18
"['Ok, here is the same dependency.']",19,grazder,filling-card-nans,grazder_filling-card-nans_m19
['Here we have the same problem. And that approach can solve it too.'],20,grazder,filling-card-nans,grazder_filling-card-nans_m20
['# Card1 and Card5'],21,grazder,filling-card-nans,grazder_filling-card-nans_m21
['# Card1 and Card6'],22,grazder,filling-card-nans,grazder_filling-card-nans_m22
"[""### Ok. Let's look at number on NaNs now.""]",23,grazder,filling-card-nans,grazder_filling-card-nans_m23
"[""Still there are a lot of NaNs in the card2 and card5. Let's try some other fill combinations.""]",24,grazder,filling-card-nans,grazder_filling-card-nans_m24
"[""Let's find another dependent feature for card2.""]",25,grazder,filling-card-nans,grazder_filling-card-nans_m25
['We can see that there are too many unique values to implement this approch to fill remaining NaNs in card2.'],26,grazder,filling-card-nans,grazder_filling-card-nans_m26
"[""Let's find another dependent feature for card5.""]",27,grazder,filling-card-nans,grazder_filling-card-nans_m27
['Same for card5.'],28,grazder,filling-card-nans,grazder_filling-card-nans_m28
"[""Let's try some other features.""]",29,grazder,filling-card-nans,grazder_filling-card-nans_m29
['# Card1 and Addr2'],30,grazder,filling-card-nans,grazder_filling-card-nans_m30
"['There are really a lot of missing values in addr2, especially for 1-amount category.']",31,grazder,filling-card-nans,grazder_filling-card-nans_m31
"['So, we can fill to many values using this approach. But we cannot be 100% sure that missing values in 1-amount category is most frequent category.']",32,grazder,filling-card-nans,grazder_filling-card-nans_m32
"[""# Let's find all the features that depends on card1""]",33,grazder,filling-card-nans,grazder_filling-card-nans_m33
['There are a lot of columns that we can suspect in dependency. And some of them we can fill like above.'],34,grazder,filling-card-nans,grazder_filling-card-nans_m34
['## If you want to apply my kernel you can use that function:'],35,grazder,filling-card-nans,grazder_filling-card-nans_m35
['If you find this kernel helpful please upvote!'],36,grazder,filling-card-nans,grazder_filling-card-nans_m36
"['**THIS KERNAL IS BLEND OF **\n', 'So awesome kernals present Right now \n', '\n', '**vote if you love blend**\n']",0,hamditarek,ensemble-0-9429,hamditarek_ensemble-0-9429_m0
"['## phase 1 [Ensemble]\n', '\n', '1. https://www.kaggle.com/jazivxt/safe-box with 0.9420\n', '2. https://www.kaggle.com/artkulak/ieee-fraud-simple-baseline-0-9383-lb with 0.9398\n', '3. https://www.kaggle.com/artgor/eda-and-models with 0.9397\n', '4. https://www.kaggle.com/stocks/under-sample-with-multiple-runs with 0.9391\n']",1,hamditarek,ensemble-0-9429,hamditarek_ensemble-0-9429_m1
"[""I'm new to Kaggle so this kernel is just how I'm learning from others...\n"", '\n', 'Good luck!']",0,iasnobmatsu,xgb-model-with-feature-engineering,iasnobmatsu_xgb-model-with-feature-engineering_m0
"['link to kernels I found useful:\n', '\n', '* [**P_emaildomain, R_emaildomain effectiveness** by *Manraj Singh*](https://www.kaggle.com/c/ieee-fraud-detection/discussion/100778)\n', '* [**lgb-single-model** by *Roman*](https://www.kaggle.com/nroman/lgb-single-model-lb-0-9419)\n', '* [**Fraud makers are earnest people about browser** by *yasagure* ](https://www.kaggle.com/yasagure/fraud-makers-are-earnest-people-about-browser)\n', '* [**LGB starter - R** by *Shuo-Jen, Chang*](https://www.kaggle.com/andrew60909/lgb-starter-r)\n', '* [**eda-and-models** by *Andrew Lukyanenko*](https://www.kaggle.com/artgor/eda-and-models)\n', '* [**Feature Engineering + LightGBM w/ GPU** by *David Cairus*](https://www.kaggle.com/davidcairuz/feature-engineering-lightgbm-w-gpu)\n']",1,iasnobmatsu,xgb-model-with-feature-engineering,iasnobmatsu_xgb-model-with-feature-engineering_m1
"['### Import libraries and data, reduce memory usage']",2,iasnobmatsu,xgb-model-with-feature-engineering,iasnobmatsu_xgb-model-with-feature-engineering_m2
"['### Some Feature Engineering\n', '\n', 'drop columns, count encoding, aggregation, fillna']",3,iasnobmatsu,xgb-model-with-feature-engineering,iasnobmatsu_xgb-model-with-feature-engineering_m3
['### XGB model and training'],4,iasnobmatsu,xgb-model-with-feature-engineering,iasnobmatsu_xgb-model-with-feature-engineering_m4
"['# Prepare data and model\n', 'I use only numeric cols. In order to use categorical features we need to get their embeddings first. \n', 'Time and ID is not included, because I want to check is there information about time in other features.']",0,iggisv9t,search-data-hidden-structure-with-autoencoder,iggisv9t_search-data-hidden-structure-with-autoencoder_m0
"['### Model\n', 'Tanh in the output of the decoder is good choice for visualisation: all objects will be projected on square from -1 to 1.']",1,iggisv9t,search-data-hidden-structure-with-autoencoder,iggisv9t_search-data-hidden-structure-with-autoencoder_m1
"[""# Let's look at 2d density\n"", 'It seems like there is some clusters']",2,iggisv9t,search-data-hidden-structure-with-autoencoder,iggisv9t_search-data-hidden-structure-with-autoencoder_m2
"['### How fraudent transactions is distributed?\n', 'Looks like fraud is distributed almost like normal transactions, but there is some regions where fraudent transactions appear more often.']",3,iggisv9t,search-data-hidden-structure-with-autoencoder,iggisv9t_search-data-hidden-structure-with-autoencoder_m3
"['### Is there time leak in numerical features?\n', 'Obviously there is information about time.']",4,iggisv9t,search-data-hidden-structure-with-autoencoder,iggisv9t_search-data-hidden-structure-with-autoencoder_m4
"['### What causes clustering?\n', 'Colouring by ProductCD gives answer for this question. Remember that categorical features was not included in visualization. It means that numerical features distribution in different categories is different. I think, we need to separate models for different ProductCD.']",5,iggisv9t,search-data-hidden-structure-with-autoencoder,iggisv9t_search-data-hidden-structure-with-autoencoder_m5
"['# What about test?\n', 'All the same.']",6,iggisv9t,search-data-hidden-structure-with-autoencoder,iggisv9t_search-data-hidden-structure-with-autoencoder_m6
"[""# THAT'S ALL, FOLKS!""]",7,iggisv9t,search-data-hidden-structure-with-autoencoder,iggisv9t_search-data-hidden-structure-with-autoencoder_m7
"['<span style=""font-family:Calibri; font-size:3em; color:blue"">IEEE Fraud Detection</span>\n', '\n', '<br>\n', '<img src=""https://cdn.datafloq.com/cache/blog_pictures/878x531/fraud-analytics-protect-banking-sector.jpg"" width=""500"" height=""600"">\n', '<br>\n', '\n', '\n', '**Why fraud detection?**\n', '> Fraud is a billion-dollar business and it is increasing every year. The PwC global economic crime survey of 2018[1] found that half (49 percent) of the 7,200 companies they surveyed had experienced fraud of some kind. This is an increase from the PwC 2016 study in which slightly more than a third of organizations surveyed (36%) had experienced economic crime.\n', '\n', '\n', 'This competition is a **binary classification** problem - i.e. our target variable is a binary attribute (Is the user making the click fraudlent or not?) and our goal is to classify users into ""fraudlent"" or ""not fraudlent"" as well as possible.\n', '\n', ""Unlike metrics such as ```LogLoss```, the **AUC score** only depends on how well you well you can separate the two classes. In practice, this means that only the order of your predictions matter, as a result of this, any rescaling done to your model's output probabilities will have no effect on your score. [click here to read more about AUC-ROC](https://stats.stackexchange.com/questions/132777/what-does-auc-stand-for-and-what-is-it)\n"", '\n', ""<img src='https://upload.wikimedia.org/wikipedia/commons/6/6b/Roccurves.png' width=300 height=300>\n"", '\n', '\n', '### Content\n', '\n', '- Data exploration\n', '- Missing Data.\n', '- Imbalanced problem.\n', '\n', '\n', '- Plots\n', '    - Distribution plots\n', '    - Count plots\n', '    - Unique values\n', '    - Groups\n', '    \n', '    \n', '- Memory reduction  \n', '\n', '- PCA\n', '\n', '\n', '- Models\n', '    - XGBoost Model.\n', '    - LGBM\n', '    \n', '**Remember the <span style=""color:red"">upvote</span> button is next to the fork button, and it\'s free too! ;)**\n', '\n', '----\n', '\n', '### References:\n', '\n', '- https://www.kaggle.com/artgor/eda-and-models/data\n', '- https://www.kaggle.com/artkulak/ieee-fraud-simple-baseline-0-9383-lb\n', '- https://www.kaggle.com/robikscube/ieee-fraud-detection-first-look-and-eda\n', '- https://www.kaggle.com/mjbahmani/reducing-memory-size-for-ieee\n', '\n', '<br>']",0,jesucristo,fraud-complete-eda,jesucristo_fraud-complete-eda_m0
"['# Data\n', '\n', '\n', 'In this competition you are predicting the probability that an online transaction is fraudulent, as denoted by the binary target ```isFraud```.\n', '\n', 'The data is broken into two files **identity** and **transaction**, which are joined by ```TransactionID```. \n', '\n', '> Note: Not all transactions have corresponding identity information.\n', '\n', '**Categorical Features - Transaction**\n', '\n', '- ProductCD\n', '- emaildomain\n', '- card1 - card6\n', '- addr1, addr2\n', '- P_emaildomain\n', '- R_emaildomain\n', '- M1 - M9\n', '\n', '**Categorical Features - Identity**\n', '\n', '- DeviceType\n', '- DeviceInfo\n', '- id_12 - id_38\n', '\n', '**The TransactionDT feature is a timedelta from a given reference datetime (not an actual timestamp).**\n', '\n', '**Files**\n', '\n', '- train_{transaction, identity}.csv - the training set\n', '- test_{transaction, identity}.csv - the test set (**you must predict the isFraud value for these observations**)\n', '- sample_submission.csv - a sample submission file in the correct format\n']",1,jesucristo,fraud-complete-eda,jesucristo_fraud-complete-eda_m1
"['**Interactive Plots Utils**\n', '> from https://www.kaggle.com/kabure/baseline-fraud-detection-eda-interactive-views (more about Interactive plots there)']",2,jesucristo,fraud-complete-eda,jesucristo_fraud-complete-eda_m2
['**Load data**'],3,jesucristo,fraud-complete-eda,jesucristo_fraud-complete-eda_m3
"['OK, there are a lot of **NaN** and **interesting columns**: \n', '\n', '- ``` C1, C2 ... D1, V300, V339 ... ``` \n', '- ``` id_01 ... id_38``` \n', '\n', ""The columns with those names don't look friendly.\n"", ""Apparently we don't have **dates**.""]",4,jesucristo,fraud-complete-eda,jesucristo_fraud-complete-eda_m4
"['### 1st problem: NaN\n', '\n', 'Remember\n', '> Not all transactions have corresponding identity information']",5,jesucristo,fraud-complete-eda,jesucristo_fraud-complete-eda_m5
['**train_transaction**'],6,jesucristo,fraud-complete-eda,jesucristo_fraud-complete-eda_m6
['**train_identity**'],7,jesucristo,fraud-complete-eda,jesucristo_fraud-complete-eda_m7
"['### 2nd Problem ...\n', '\n', 'Notice how **imbalanced** is our original dataset! Most of the transactions are non-fraud. If we use this dataframe as the base for our predictive models and analysis we might get a lot of errors and our algorithms will probably overfit since it will ""assume"" that most transactions are not fraud. But we don\'t want our model to assume, we want our model to detect patterns that give signs of fraud!\n', '\n', '**Imbalance** means that the number of data points available for different the classes is different\n', '\n', ""<img src='https://www.datascience.com/hs-fs/hubfs/imbdata.png?t=1542328336307&width=487&name=imbdata.png'>""]",8,jesucristo,fraud-complete-eda,jesucristo_fraud-complete-eda_m8
"['# Time vs fe\n', '> **The TransactionDT feature is a timedelta from a given reference datetime (not an actual timestamp).**\n', '\n', '**Important ! read the post [The timespan of the dataset is 1 year ?\n', '](https://www.kaggle.com/c/ieee-fraud-detection/discussion/100071#latest-577632) by Suchith**\n', '\n', '```\n', 'Train: min = 86400 max = 15811131\n', 'Test: min = 18403224 max = 34214345\n', '```\n', '\n', ""The difference train.min() and test.max() is ```x = 34214345 - 86400 = 34127945``` but we don't know is it in seconds,minutes or hours.\n"", '\n', '```\n', 'Time span of the total dataset is 394.9993634259259 days\n', 'Time span of Train dataset is  181.99920138888888 days\n', 'Time span of Test dataset is  182.99908564814814 days\n', 'The gap between train and test is 30.00107638888889 days\n', '```\n', '\n', 'If it is in seconds then dataset timespan will be ```x/(3600*24*365) = 1.0821``` years which seems reasonable to me. So if the **transactionDT** is in **seconds** then\n', '\n', '```\n', 'Time span of the total dataset is 394.9993634259259 days\n', 'Time span of Train dataset is  181.99920138888888 days\n', 'Time span of Test dataset is  182.99908564814814 days\n', 'The gap between train and test is 30.00107638888889 days\n', '```\n', '\n', '\n', '![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F2370491%2Fc9bf5af5e902595b737df5470adc193b%2Fdownload-1.png?generation=1563312982845419&alt=media)\n', '\n', '**source: [FChmiel](https://www.kaggle.com/fchmiel)**\n', '<br>']",9,jesucristo,fraud-complete-eda,jesucristo_fraud-complete-eda_m9
"['```24.4%``` of TransactionIDs in train (144233 / 590540) have an associated train_identity.\n', '\n', '```28.0%``` of TransactionIDs in test (144233 / 590540) have an associated train_identity.']",10,jesucristo,fraud-complete-eda,jesucristo_fraud-complete-eda_m10
"['**TransactionDT** is not a timestamp, but somehow we use it to measure time.']",11,jesucristo,fraud-complete-eda,jesucristo_fraud-complete-eda_m11
"[""As you can see it seems that train and test transaction dates don't overlap, so it would be prudent to use time-based split for validation. Rob discovered this here: https://www.kaggle.com/robikscube/ieee-fraud-detection-first-look-and-eda.\n"", '\n', 'Also we can see the **30 days** gap between train and test.\n']",12,jesucristo,fraud-complete-eda,jesucristo_fraud-complete-eda_m12
"['Also you should read this post by Rob [Plotting features over time shows something.... interesting\n', ""](https://www.kaggle.com/c/ieee-fraud-detection/discussion/100167#latest-577688) he discovered a weird correlation between C and D features, and that's why I do the following plots :)""]",13,jesucristo,fraud-complete-eda,jesucristo_fraud-complete-eda_m13
['### isFraud vs time'],14,jesucristo,fraud-complete-eda,jesucristo_fraud-complete-eda_m14
"['### C features: C1, C2 ... C14']",15,jesucristo,fraud-complete-eda,jesucristo_fraud-complete-eda_m15
['### D features: D1 ... D15'],16,jesucristo,fraud-complete-eda,jesucristo_fraud-complete-eda_m16
"['OK, the problem here is that ```D``` features are mostly NaNs!']",17,jesucristo,fraud-complete-eda,jesucristo_fraud-complete-eda_m17
"[""If we consider D features, de 58.15% are missing values ... Let's plot without missing values""]",18,jesucristo,fraud-complete-eda,jesucristo_fraud-complete-eda_m18
['### M features: M1 .. M9'],19,jesucristo,fraud-complete-eda,jesucristo_fraud-complete-eda_m19
['## V150'],20,jesucristo,fraud-complete-eda,jesucristo_fraud-complete-eda_m20
"['<br>\n', '# Groups']",21,jesucristo,fraud-complete-eda,jesucristo_fraud-complete-eda_m21
['Remove ```.head(20)``` and check the entire list.'],22,jesucristo,fraud-complete-eda,jesucristo_fraud-complete-eda_m22
"['<br>\n', '# TransactionAmt']",23,jesucristo,fraud-complete-eda,jesucristo_fraud-complete-eda_m23
['# Unique Values'],24,jesucristo,fraud-complete-eda,jesucristo_fraud-complete-eda_m24
['### D Features'],25,jesucristo,fraud-complete-eda,jesucristo_fraud-complete-eda_m25
['### C features'],26,jesucristo,fraud-complete-eda,jesucristo_fraud-complete-eda_m26
['### V features'],27,jesucristo,fraud-complete-eda,jesucristo_fraud-complete-eda_m27
['### id_code'],28,jesucristo,fraud-complete-eda,jesucristo_fraud-complete-eda_m28
"['<br>\n', '# Categorical Features\n', '\n', '- ProductCD\n', '- emaildomain\n', '- card1 - card6\n', '- addr1, addr2\n', '- P_emaildomain\n', '- R_emaildomain\n', '- M1 - M9\n', '- DeviceType\n', '- DeviceInfo\n', '- id_12 - id_38']",29,jesucristo,fraud-complete-eda,jesucristo_fraud-complete-eda_m29
['### ProductCD'],30,jesucristo,fraud-complete-eda,jesucristo_fraud-complete-eda_m30
['### Device Type & Device Info'],31,jesucristo,fraud-complete-eda,jesucristo_fraud-complete-eda_m31
['**Device information**'],32,jesucristo,fraud-complete-eda,jesucristo_fraud-complete-eda_m32
['### Card'],33,jesucristo,fraud-complete-eda,jesucristo_fraud-complete-eda_m33
"[""As you can see, ``` Card 1``` column is given as Categorical but it is behaving like Continuous Data. Having '13553' unique Values.\n"", '\n', '> **From organizer: ** This is a encoded categorical variable. \n', ""The dataset contains many high-cardinality variables, and it's challenge to model such variable. Meanwhile, it's worthy to see how you talented people deal with them.\n"", '\n', 'Check this post: https://www.kaggle.com/c/ieee-fraud-detection/discussion/100340#latest-578626']",34,jesucristo,fraud-complete-eda,jesucristo_fraud-complete-eda_m34
['### Email Domain'],35,jesucristo,fraud-complete-eda,jesucristo_fraud-complete-eda_m35
['It seems that criminals prefer gmail'],36,jesucristo,fraud-complete-eda,jesucristo_fraud-complete-eda_m36
['# Memory reduction'],37,jesucristo,fraud-complete-eda,jesucristo_fraud-complete-eda_m37
['**Merge transaction & identity + Label Encoder**'],38,jesucristo,fraud-complete-eda,jesucristo_fraud-complete-eda_m38
"['### Reduce Memory Usage\n', '> 2 options\n', '\n', '**Note** Using te option1 the missing values are encoded as -1, you have to update the XGBoost model and set ```missing=-1```']",39,jesucristo,fraud-complete-eda,jesucristo_fraud-complete-eda_m39
['this takes 6-7 mins. You can click and check the ``` output ```'],40,jesucristo,fraud-complete-eda,jesucristo_fraud-complete-eda_m40
['### Now memory should be around 4 GB !'],41,jesucristo,fraud-complete-eda,jesucristo_fraud-complete-eda_m41
"['**Drop some columns**\n', '> from: https://www.kaggle.com/jazivxt/safe-box/notebook']",42,jesucristo,fraud-complete-eda,jesucristo_fraud-complete-eda_m42
['**Fill NaN**'],43,jesucristo,fraud-complete-eda,jesucristo_fraud-complete-eda_m43
['# PCA'],44,jesucristo,fraud-complete-eda,jesucristo_fraud-complete-eda_m44
['**PCA 2 components**'],45,jesucristo,fraud-complete-eda,jesucristo_fraud-complete-eda_m45
"['<br>\n', '# Models\n', '---\n']",46,jesucristo,fraud-complete-eda,jesucristo_fraud-complete-eda_m46
"['## XGBoost Model + FE Importance\n', '\n', '> This part is from [can_we_beat_it](https://www.kaggle.com/konradb/can-we-beat-it) by Konrad\n', '\n', '> Also check this kernel [IEEE Fraud Simple Baseline [0.9383 LB]](https://www.kaggle.com/artkulak/ieee-fraud-simple-baseline-0-9383-lb)']",47,jesucristo,fraud-complete-eda,jesucristo_fraud-complete-eda_m47
"['**Important** Check the [XGB official documentation](https://xgboost.readthedocs.io/en/latest/parameter.html) in order to know more about the parameters.\n', '\n', 'Also check this thread [CV vs Public LB](https://www.kaggle.com/c/ieee-fraud-detection/discussion/100255#latest-578503)']",48,jesucristo,fraud-complete-eda,jesucristo_fraud-complete-eda_m48
"['### Importance PLOT\n', '> last FOLD']",49,jesucristo,fraud-complete-eda,jesucristo_fraud-complete-eda_m49
['# Submission'],50,jesucristo,fraud-complete-eda,jesucristo_fraud-complete-eda_m50
"['### To be continued ...\n', ""**I'll keep updating almost every day :)**""]",51,jesucristo,fraud-complete-eda,jesucristo_fraud-complete-eda_m51
"['# IEEE-CIS Fraud Detection &mdash; LightGBM Split Points\n', '\n', 'This notebook shows some techniques to snoop on the gradient boosting process used by LightGBM - using its own APIs.\n', '\n', 'By counting the split points used in the decision trees, we can see the ways the algorithm divides the input space up. This may lead to new insights about what indicates fraud, and may help in smoothing or binning the data to reduce splits that model only noise.\n', '\n', 'For more info on LightGBM see [pdf by Microsoft][3] or the [LightGBM github][4].\n', '\n', 'For another example of gradient boosting model analysis with XGBoost see the great [xgbfi][2] tool by [Faron][1].\n', '\n', '___\n', '\n', 'We start by building a model...\n', '\n', ' [1]: https://www.kaggle.com/mmueller\n', ' [2]: https://github.com/Far0n/xgbfi\n', ' [3]: https://www.microsoft.com/en-us/research/wp-content/uploads/2017/11/lightgbm.pdf\n', ' [4]: https://github.com/Microsoft/LightGBM\n']",0,jtrotman,ieee-fraud-lgb-split-points,jtrotman_ieee-fraud-lgb-split-points_m0
['Add count features...'],1,jtrotman,ieee-fraud-lgb-split-points,jtrotman_ieee-fraud-lgb-split-points_m1
['Add some simple extra features.'],2,jtrotman,ieee-fraud-lgb-split-points,jtrotman_ieee-fraud-lgb-split-points_m2
"['Simple time based validation split, first 75% is training data, rest is validation.']",3,jtrotman,ieee-fraud-lgb-split-points,jtrotman_ieee-fraud-lgb-split-points_m3
"['## Note &mdash; TransactionDT\n', '\n', 'I will use all columns as features, even `TransactionDT` which is terrible as a feature - none of the test set values overlap with the training set values. By leaving it in here though we get to see if there are any hotspots of `TransactionDT` that get frequently used as a split point, showing us a potential *regime change* or shift in fraud behaviour.']",4,jtrotman,ieee-fraud-lgb-split-points,jtrotman_ieee-fraud-lgb-split-points_m4
"[""Save the model - it saves the trees in an easy to parse text format. (The file won't be used here but it is useful in general to save.)""]",5,jtrotman,ieee-fraud-lgb-split-points,jtrotman_ieee-fraud-lgb-split-points_m5
"['# Booster.dump_model()\n', '\n', 'The returned LightGBM model format is hierarchical, trees are nested `dict` objects containing `left_child` and `right_child` subtrees. Walking over the trees and summarizing the splits can be done with a short recursive function...\n', '\n', '    tree_info  - list of dicts\n', '    (each contains):\n', '        tree_structure\n', '            left_child\n', '            right_child\n', '\n', ""The `dump_model()` information records 'gain' at each split, and we simply re-use that.""]",6,jtrotman,ieee-fraud-lgb-split-points,jtrotman_ieee-fraud-lgb-split-points_m6
"['Each feature indexes a Counter object in the `split_points` dict. In each Counter, the keys are feature values, and the values are sum of gain, for example, 3.5 is the most used value in feature `C1`:']",7,jtrotman,ieee-fraud-lgb-split-points,jtrotman_ieee-fraud-lgb-split-points_m7
"['Dump all the split point data to an xlsx file (can be opened with open-source *Open Office* or *[Libre Office][1]*)\n', '\n', ' [1]: https://www.libreoffice.org/download/download/']",8,jtrotman,ieee-fraud-lgb-split-points,jtrotman_ieee-fraud-lgb-split-points_m8
"['# Plotting Code\n', '\n', ""Warning: this only shows the 50 split points with the most gain, so the x-axis will be a bit nonlinear, some values won't appear. See the xlsx file for all the values.""]",9,jtrotman,ieee-fraud-lgb-split-points,jtrotman_ieee-fraud-lgb-split-points_m9
"['# Plots For IEEE Features\n', '\n', 'All the features with 4 or more unique values are shown (to avoid ""Too many output files (max 500)"" error).\n', '\n', '## Notes\n', '\n', 'Most of the split points have long decimal values like `379.00000000000006` - the LightGBM algorithm only sees binned data, so it sets split thresholds as values [halfway between neighbouring bin lower/upper edges][6], but bumped upwards a tiny fraction using `std::nextafter` in the [C++ standard library][5], resulting in strangely precise [floating point format][1] values :)\n', '\n', 'Zero is checked for using a [kZeroThreshold = 1e-35f][7] variable - this comes out of the model as a split point of 1.0000000180025095e-35 &mdash; a tiny number. When you see that, think *zero*.\n', '\n', 'Split points for categorical dtypes depends on the `max_cat_to_onehot` which I have set to 128 - so categoricals in this data set are treated with a one-vs-all split. This means `feature==value` in the node split test, instead of the usual `feature<=value`. `max_cat_to_onehot` is by default set to 4, meaning categories with more values than this use splits based on target statistics, and the resulting split points have values like `1||3||5||7||8||9` which indicate which category codes go down the *left* branch. (But this is hard to show in bar charts... hence I used *one-vs-all splits*.)\n', '\n', 'LightGBM keeps a separate bin for NaN values and at all node tests, records whether that bin goes left/right separately - this is not shown here (yet! Upvote to make me attempt something!)\n', '\n', '## What to Look For\n', '\n', ""In some ways what we **don't** see is more interesting than what we **do**. As with normal feature importances: if we see a feature is not used at all it is clearly redudant and should be removed. So seeing low gain (0) is very reliable but seeing high gain can *can* be misleading - it may be fitting noise.\n"", '\n', ""If there is **one prominent peak** it means the feature acts a bit like a boolean, and perhaps would be better fed into the model that way (e.g. seeing a split value of 100, for feature 'foo', you could instead change column 'foo' to `foo<=100`.)  \n"", '\n', 'Similarly if there are **several prominent peaks** it could imply the feature should be discretized/binned in a pre-processing step, as the less-often used split points may just be picking up on noise. See this [interesting old discussion thread on this subject of feature discretization][2].\n', '\n', 'For the TimeOfDay feature you may like to check out [my time series heatmaps notebook][4] that shows the density of transactions over time, and clearly indicates night time.\n', '\n', ' [1]: https://en.wikipedia.org/wiki/Double-precision_floating-point_format\n', ' [2]: https://www.kaggle.com/c/porto-seguro-safe-driver-prediction/discussion/43886\n', ' [3]: https://www.kaggle.com/tilii7\n', ' [4]: https://www.kaggle.com/jtrotman/ieee-fraud-time-series-heatmaps\n', ' [5]: https://en.cppreference.com/w/cpp/numeric/math/nextafter\n', ' [6]: https://github.com/microsoft/LightGBM/blob/master/src/io/bin.cpp\n', ' [7]: https://github.com/microsoft/LightGBM/blob/master/include/LightGBM/meta.h ']",10,jtrotman,ieee-fraud-lgb-split-points,jtrotman_ieee-fraud-lgb-split-points_m10
"['# Gain Over Time\n', '\n', 'This part is more for illustration/teaching about gradient boosting.\n', '\n', 'As well as counting split points, we can look at how feature gain evolves as trees are added to the model.\n', '\n', 'In the gradient boosting learning process, each tree adds something to the training set predictions that moves the overall predictions closer to the target. It takes small steps towards lowering the loss function. Early trees are more like standard decision trees, fitting the big patterns. Later trees are more specialised, correcting small deviations, fine-grained wrinkles in the loss function: often little patterns, sometimes noise.\n', '\n', 'As features are incorporated into the model in early trees, their predictive power can run out, which is most notable for boolean features; at some point the existing predictions have accounted for all of the variance of the feature and they are no longer used in new trees.\n', '\n', 'We can see this by looking at gain statistics over time by passing the `iteration` parameter to the `feature_importance()` method.\n']",11,jtrotman,ieee-fraud-lgb-split-points,jtrotman_ieee-fraud-lgb-split-points_m11
['`V258` and `V258_count` reach high gain by 20% of the way through and are not used much after that. `card1` and `card2` are still being used with high gain throughout.'],12,jtrotman,ieee-fraud-lgb-split-points,jtrotman_ieee-fraud-lgb-split-points_m12
"['The blue bar indicates gain at 40% of the way through the learning process, and red marks the gain at the end.']",13,jtrotman,ieee-fraud-lgb-split-points,jtrotman_ieee-fraud-lgb-split-points_m13
"['Here we see `V258` and `V258_count` have *run out of steam* early, whilst other features are still gaining in importance...']",14,jtrotman,ieee-fraud-lgb-split-points,jtrotman_ieee-fraud-lgb-split-points_m14
"['A different plot, show the most used features at the 20% point of training, and how their gain evolves after that...']",15,jtrotman,ieee-fraud-lgb-split-points,jtrotman_ieee-fraud-lgb-split-points_m15
"['# Conclusions\n', '\n', 'Now we can inspect trained models to see **which points** in the feature space matter for fraud detection... You can build this in to your pipeline to help with reducing the resolution of the data in later modelling iterations, and aid further feature engineering.\n', '\n', 'If this kernel gets enough votes I will apply it to adversarial validation too to see **where** in the feature space the train and test set differ the most. Perhaps this could even be integrated into an *auto-relaxing* function that buckets the data for us in a way that makes the  train and test sets more similar, without any tedious manual inspection of plots :)\n', '\n', '<font color=red>Update</font>: [adversarial version here][2].\n', '\n', '___\n', '\n', 'A note to any n00bs reading: the original features used here are only a starting point, used just to demonstrate. If (say) `DeviceInfo` of `hi6210sft Build/MRA58K` comes along in the training set and makes a fast burst of transactions (all marked fraud), then appears in the test set but spread out and on many separate days, it does not make sense to predict a high fraud likelihood, simply because of that one feature. Features that capture *event* timing & behaviour are needed :)\n', '\n', 'For inspiration you should check out [an **extensive** index of **winning** and high ranking Kaggle **solutions** here][1] (and upvote if this helps you find something useful &mdash; I guarantee there are useful links there ;)\n', '\n', ' [1]: https://www.kaggle.com/jtrotman/high-ranking-solution-posts\n', ' [2]: https://www.kaggle.com/jtrotman/ieee-fraud-adversarial-lgb-split-points\n']",16,jtrotman,ieee-fraud-lgb-split-points,jtrotman_ieee-fraud-lgb-split-points_m16
"['# IEEE-CIS Fraud Detection Time Series Heatmaps\n', '\n', 'This notebook shows counts of transactions over time in a 2D heatmap, as a simple exploration of the time series structure of the train/test sets.\n', '\n', 'One row of pixels in each image is 1 day, with about 183 rows in each image, derived from the training set, and later on, the test set.']",0,jtrotman,ieee-fraud-time-series-heatmaps,jtrotman_ieee-fraud-time-series-heatmaps_m0
"['TransactionDT is in seconds, with 15811131 maximum.']",1,jtrotman,ieee-fraud-time-series-heatmaps,jtrotman_ieee-fraud-time-series-heatmaps_m1
"['183 days, put them on one row each in a heatmap, with 480 columns']",2,jtrotman,ieee-fraud-time-series-heatmaps,jtrotman_ieee-fraud-time-series-heatmaps_m2
['So each pixel will represent 180 seconds of a day. All transactions in each 3 minute block will be counted.'],3,jtrotman,ieee-fraud-time-series-heatmaps,jtrotman_ieee-fraud-time-series-heatmaps_m3
"[""`TransactionID>0` is a simple way to say 'all the training set' - it is true for all rows.""]",4,jtrotman,ieee-fraud-time-series-heatmaps,jtrotman_ieee-fraud-time-series-heatmaps_m4
['Take a preview look at what we made...'],5,jtrotman,ieee-fraud-time-series-heatmaps,jtrotman_ieee-fraud-time-series-heatmaps_m5
"['Looks good, now do it for all values that appear 5000 times or more, for every single column.']",6,jtrotman,ieee-fraud-time-series-heatmaps,jtrotman_ieee-fraud-time-series-heatmaps_m6
['Over 300 plots :)'],7,jtrotman,ieee-fraud-time-series-heatmaps,jtrotman_ieee-fraud-time-series-heatmaps_m7
['Now the V columns too:'],8,jtrotman,ieee-fraud-time-series-heatmaps,jtrotman_ieee-fraud-time-series-heatmaps_m8
"['# Matplotlib Display\n', '\n', 'Show some plots selected for interesting features...']",9,jtrotman,ieee-fraud-time-series-heatmaps,jtrotman_ieee-fraud-time-series-heatmaps_m9
"[""This is all transactions in the training set - I'm using the raw date values from the training data so the values are on the time axis are possibly the wrong timezone - night time appears clearly visible but about 3 hours late (depending on your lifestyle ;)\n"", '\n', 'Also a fat peak around days 20-30, and a thin (1 day) peak at about day 92.\n', '\n', 'Looking closely (right click &rarr; *View Image* helps) there is some weekly seasonality - a darker line in the mornings - presumably Sunday.']",10,jtrotman,ieee-fraud-time-series-heatmaps,jtrotman_ieee-fraud-time-series-heatmaps_m10
['Fraud does not seem to dip so much overnight... (Note: max value is 8 - no three-minute block has more than 8 fraudulent transactions.)'],11,jtrotman,ieee-fraud-time-series-heatmaps,jtrotman_ieee-fraud-time-series-heatmaps_m11
['Now some covariate shifts...'],12,jtrotman,ieee-fraud-time-series-heatmaps,jtrotman_ieee-fraud-time-series-heatmaps_m12
"['card1 value 7919 seems periodic, at about 30 days, with some double peaks too...']",13,jtrotman,ieee-fraud-time-series-heatmaps,jtrotman_ieee-fraud-time-series-heatmaps_m13
['seems to be correlated with card2 value 194'],14,jtrotman,ieee-fraud-time-series-heatmaps,jtrotman_ieee-fraud-time-series-heatmaps_m14
"['card2 is not missing at random, but missing in streaks:']",15,jtrotman,ieee-fraud-time-series-heatmaps,jtrotman_ieee-fraud-time-series-heatmaps_m15
['card5 stops being equal to 202 at about day 80'],16,jtrotman,ieee-fraud-time-series-heatmaps,jtrotman_ieee-fraud-time-series-heatmaps_m16
['but value 126 starts appearing more about that time... perhaps they have a similar meaning?'],17,jtrotman,ieee-fraud-time-series-heatmaps,jtrotman_ieee-fraud-time-series-heatmaps_m17
"['A bit hard to see, but gmail.com as R_emaildomain has a drop (dark band) around day 100.']",18,jtrotman,ieee-fraud-time-series-heatmaps,jtrotman_ieee-fraud-time-series-heatmaps_m18
['A bit easier to see here in D6...'],19,jtrotman,ieee-fraud-time-series-heatmaps,jtrotman_ieee-fraud-time-series-heatmaps_m19
"['The ""D"" columns refer to time, as stated by organizers. D9 being 0.75 means 6pm-7pm for example:']",20,jtrotman,ieee-fraud-time-series-heatmaps,jtrotman_ieee-fraud-time-series-heatmaps_m20
"['Value ""S"" for ProductCD only appears late - the natural choice for a validation era:']",21,jtrotman,ieee-fraud-time-series-heatmaps,jtrotman_ieee-fraud-time-series-heatmaps_m21
"['More complicated expressions are possible, you could extract a decision tree path and plug it in as query string, a simple (uninsightful?) example (depth 2) is:']",22,jtrotman,ieee-fraud-time-series-heatmaps,jtrotman_ieee-fraud-time-series-heatmaps_m22
['# Test Set'],23,jtrotman,ieee-fraud-time-series-heatmaps,jtrotman_ieee-fraud-time-series-heatmaps_m23
['Test starts at day 213:'],24,jtrotman,ieee-fraud-time-series-heatmaps,jtrotman_ieee-fraud-time-series-heatmaps_m24
"['To make test look like train, subtract 213 days, then reuse the above code. Note `day0 .. day183` in the plots now refers to test set days.']",25,jtrotman,ieee-fraud-time-series-heatmaps,jtrotman_ieee-fraud-time-series-heatmaps_m25
['Generate test set plots for offline use.'],26,jtrotman,ieee-fraud-time-series-heatmaps,jtrotman_ieee-fraud-time-series-heatmaps_m26
"['## Test Set Plots\n', '\n', 'The leaderboard page says **This leaderboard is calculated with approximately 20% of the test data**. [This great discussion topic][1] says the public/private split is by time. So, the public LB will be day 0 - day 37, all after is the private LB...\n', '\n', ' [1]: https://www.kaggle.com/c/ieee-fraud-detection/discussion/101040\n']",27,jtrotman,ieee-fraud-time-series-heatmaps,jtrotman_ieee-fraud-time-series-heatmaps_m27
"['Overall shape looks similar, perhaps night time drifts later towards the end?']",28,jtrotman,ieee-fraud-time-series-heatmaps,jtrotman_ieee-fraud-time-series-heatmaps_m28
"['Date columns appear to mean the same thing, no daylight savings shift...']",29,jtrotman,ieee-fraud-time-series-heatmaps,jtrotman_ieee-fraud-time-series-heatmaps_m29
"[""D15 continues it's erratic behaviour - and is different between public/private periods - though it is only null for 12069 rows (of ~500k), so perhaps it's ok to ignore this. Or just fillna(0) or fillna(1).""]",30,jtrotman,ieee-fraud-time-series-heatmaps,jtrotman_ieee-fraud-time-series-heatmaps_m30
"['card1 value 7919 seems familiar, similar to train...']",31,jtrotman,ieee-fraud-time-series-heatmaps,jtrotman_ieee-fraud-time-series-heatmaps_m31
"['Above we saw that value ""S"" for ProductCD only appears late in the train set - but here in the test set it is more uniform, though around the same number of transactions.']",32,jtrotman,ieee-fraud-time-series-heatmaps,jtrotman_ieee-fraud-time-series-heatmaps_m32
"['However - value ""R"" appears quite late in the *test* set - in the private LB zone. This is similar to how ""S"" appeared only late in the *train* set.']",33,jtrotman,ieee-fraud-time-series-heatmaps,jtrotman_ieee-fraud-time-series-heatmaps_m33
"['Similarly, ""H"" for ProductCD ramps up in the test set.']",34,jtrotman,ieee-fraud-time-series-heatmaps,jtrotman_ieee-fraud-time-series-heatmaps_m34
"['# Clean Up\n', '\n', 'Compress some of the generated pngs - Kaggle does not allow more than 500 output files.']",35,jtrotman,ieee-fraud-time-series-heatmaps,jtrotman_ieee-fraud-time-series-heatmaps_m35
"['## Notes\n', '\n', 'Much more is possible here, for example, count all transactions, then count subsets like\n', ' - `ProductCD==X`\n', ' \n', 'and\n', ' - `ProductCD==X and isFraud==1` \n', ' \n', 'then apply Bayes rule to get **p(isFraud | ProductCD==X)** and color cells in accordingly, e.g. p(isFraud) as the red channel in RGB.\n', '\n', 'Also `np.add.at(c, ts, 1)` can be changed to `np.add.at(c, ts, df.isFraud)` to accumulate values instead of simply count transctions.\n', '\n', '____\n', '\n', '*to be continued...*  **(HOWEVER: feel free to fork this notebook and try it out yourself :)**']",36,jtrotman,ieee-fraud-time-series-heatmaps,jtrotman_ieee-fraud-time-series-heatmaps_m36
"['# Feature Engineering \n', '## ~Almost~ all features of IEEE fraud detectetion dataset \n', '\n', '\n', '## Purpose of the kernel: \n', ""In the last week I worked in all features trying to improve my model. I've worked in almost all features and I decided to share with the kaggle fellows. \n"", '\n', '### NOTE: Maybe not all features could be useful, but I think that this work could help other kagglers in Data manipulation or to create another interesting feature\n']",0,kabure,almost-complete-feature-engineering-ieee-data,kabure_almost-complete-feature-engineering-ieee-data_m0
"['## <font color=""red"">Please if this kernel were useful for you, please <b>UPVOTE</b> the kernel</font>']",1,kabure,almost-complete-feature-engineering-ieee-data,kabure_almost-complete-feature-engineering-ieee-data_m1
['## Importing Libraries'],2,kabure,almost-complete-feature-engineering-ieee-data,kabure_almost-complete-feature-engineering-ieee-data_m2
['## Some functions'],3,kabure,almost-complete-feature-engineering-ieee-data,kabure_almost-complete-feature-engineering-ieee-data_m3
['## Concatenating train and test'],4,kabure,almost-complete-feature-engineering-ieee-data,kabure_almost-complete-feature-engineering-ieee-data_m4
['# V Features'],5,kabure,almost-complete-feature-engineering-ieee-data,kabure_almost-complete-feature-engineering-ieee-data_m5
['## Shape after transforming V features'],6,kabure,almost-complete-feature-engineering-ieee-data,kabure_almost-complete-feature-engineering-ieee-data_m6
['# C Features'],7,kabure,almost-complete-feature-engineering-ieee-data,kabure_almost-complete-feature-engineering-ieee-data_m7
['## Shape after transforming C features'],8,kabure,almost-complete-feature-engineering-ieee-data,kabure_almost-complete-feature-engineering-ieee-data_m8
"['# M features\n', ""- Only feeling Na's""]",9,kabure,almost-complete-feature-engineering-ieee-data,kabure_almost-complete-feature-engineering-ieee-data_m9
['# Id datasets'],10,kabure,almost-complete-feature-engineering-ieee-data,kabure_almost-complete-feature-engineering-ieee-data_m10
"[""## Filling NaN's in Numerical features\n"", ""id_num_cols = ['id_01', 'id_02', 'id_03', 'id_04', 'id_05', \n"", ""               'id_06', 'id_07', 'id_08', 'id_09', 'id_10', 'id_11']\n"", '\n', 'for col in id_num_cols:\n', '    df_id[col].fillna(df_id[col].min() - 10, inplace=True)\n', '\n', '## Categorical features\n', ""id_num = ['id_13', 'id_14', 'id_17', 'id_18', 'id_19', 'id_20', \n"", ""          'id_21', 'id_21', 'id_22', 'id_24', 'id_25', 'id_26']\n"", '\n', 'for col in id_num:\n', '    df_id[col].fillna(df_id[col].min() - 100, inplace=True)\n', '    \n', ""to_fill_none = ['id_15', 'id_16', 'id_27', \n"", ""                'id_28', 'id_29', 'id_35', \n"", ""                'id_36', 'id_37', 'id_38']\n"", '\n', 'for col in to_fill_none:\n', ""    df_id[col].fillna('None', inplace=True)\n"", '    \n', ""df_id['device_name'] = df_id['DeviceInfo'].str.split('/', expand=True)[0]\n"", ""df_id['device_version'] = df_id['DeviceInfo'].str.split('/', expand=True)[1]\n"", ""df_id.drop('DeviceInfo', axis=1, inplace=True)\n"", ""df_id['device_name'].fillna('None', inplace=True)\n"", ""df_id['device_version'].fillna('None', inplace=True)\n"", ""df_id['DeviceType'].fillna('None', inplace=True)\n"", '\n', ""df_id['OS_id_30'] = df_id['id_30'].str.split(' ', expand=True)[0]\n"", ""df_id['version_id_30'] = df_id['id_30'].str.split(' ', expand=True)[1]\n"", ""df_id['OS_id_30'].fillna('None', inplace=True)\n"", ""df_id['version_id_30'].fillna('None', inplace=True)\n"", ""df_id.drop('id_30', axis=1, inplace=True)\n"", '\n', ""df_id['browser_id_31'] = df_id['id_31'].str.split(' ', expand=True)[0]\n"", ""df_id['version_id_31'] = df_id['id_31'].str.split(' ', expand=True)[1]\n"", ""df_id['browser_id_31'].fillna('None', inplace=True)\n"", ""df_id['version_id_31'].fillna('None', inplace=True)\n"", ""df_id.drop('id_31', axis=1, inplace=True)\n"", '\n', ""df_id['screen_width'] = df_id['id_33'].str.split('x', expand=True)[0]\n"", ""df_id['screen_height'] = df_id['id_33'].str.split('x', expand=True)[1]\n"", ""df_id['screen_width'].fillna(-1, inplace=True)\n"", ""df_id['screen_height'].fillna(-1, inplace=True)\n"", ""df_id.drop('id_33', axis=1, inplace=True)\n"", '\n', ""df_id['id_34'] = df_id['id_34'].str.split(':', expand=True)[1]\n"", ""df_id['id_23'] = df_id['id_23'].str.split(':', expand=True)[1]\n"", ""df_id['id_23'].fillna('None', inplace=True)\n"", ""df_id['id_34'].fillna(-2, inplace=True)\n"", '\n', ""to_fill_minone = 'id_32'   \n"", ""df_id['id_32'].fillna(df_id['id_32'].min() -10, inplace=True)\n"", '\n', '## Device renaming\n', ""df_id.loc[df_id['device_name'].str.contains('SM', na=False), 'device_name'] = 'Samsung'\n"", ""df_id.loc[df_id['device_name'].str.contains('SAMSUNG', na=False), 'device_name'] = 'Samsung'\n"", ""df_id.loc[df_id['device_name'].str.contains('GT-', na=False), 'device_name'] = 'Samsung'\n"", ""df_id.loc[df_id['device_name'].str.contains('Moto G', na=False), 'device_name'] = 'Motorola'\n"", ""df_id.loc[df_id['device_name'].str.contains('Moto', na=False), 'device_name'] = 'Motorola'\n"", ""df_id.loc[df_id['device_name'].str.contains('moto', na=False), 'device_name'] = 'Motorola'\n"", ""df_id.loc[df_id['device_name'].str.contains('LG-', na=False), 'device_name'] = 'LG'\n"", ""df_id.loc[df_id['device_name'].str.contains('rv:', na=False), 'device_name'] = 'RV'\n"", ""df_id.loc[df_id['device_name'].str.contains('HUAWEI', na=False), 'device_name'] = 'Huawei'\n"", ""df_id.loc[df_id['device_name'].str.contains('ALE-', na=False), 'device_name'] = 'Huawei'\n"", ""df_id.loc[df_id['device_name'].str.contains('-L', na=False), 'device_name'] = 'Huawei'\n"", ""df_id.loc[df_id['device_name'].str.contains('Blade', na=False), 'device_name'] = 'ZTE'\n"", ""df_id.loc[df_id['device_name'].str.contains('BLADE', na=False), 'device_name'] = 'ZTE'\n"", ""df_id.loc[df_id['device_name'].str.contains('Linux', na=False), 'device_name'] = 'Linux'\n"", ""df_id.loc[df_id['device_name'].str.contains('XT', na=False), 'device_name'] = 'Sony'\n"", ""df_id.loc[df_id['device_name'].str.contains('HTC', na=False), 'device_name'] = 'HTC'\n"", ""df_id.loc[df_id['device_name'].str.contains('ASUS', na=False), 'device_name'] = 'Asus'\n"", '\n', 'df_id.loc[df_id.device_name.isin(df_id.device_name.value_counts()[df_id.device_name.value_counts() < 200].index), \'device_name\'] = ""Others""\n', ""df_id['has_id'] = 1""]",11,kabure,almost-complete-feature-engineering-ieee-data,kabure_almost-complete-feature-engineering-ieee-data_m11
['## Encoding Id categoricals'],12,kabure,almost-complete-feature-engineering-ieee-data,kabure_almost-complete-feature-engineering-ieee-data_m12
['# Joining Ids in Transaction df'],13,kabure,almost-complete-feature-engineering-ieee-data,kabure_almost-complete-feature-engineering-ieee-data_m13
['# Transforming Id features in minmaxscale and filling null values'],14,kabure,almost-complete-feature-engineering-ieee-data,kabure_almost-complete-feature-engineering-ieee-data_m14
['# Getting PCA of Id num cols '],15,kabure,almost-complete-feature-engineering-ieee-data,kabure_almost-complete-feature-engineering-ieee-data_m15
"[""df = PCA_change(df, id_num_cols, prefix='PCA_ID_', n_components=3)""]",16,kabure,almost-complete-feature-engineering-ieee-data,kabure_almost-complete-feature-engineering-ieee-data_m16
['## Geting ID Cards'],17,kabure,almost-complete-feature-engineering-ieee-data,kabure_almost-complete-feature-engineering-ieee-data_m17
['# D features'],18,kabure,almost-complete-feature-engineering-ieee-data,kabure_almost-complete-feature-engineering-ieee-data_m18
['### Sum all clusters'],19,kabure,almost-complete-feature-engineering-ieee-data,kabure_almost-complete-feature-engineering-ieee-data_m19
['## Slicing df to df_train and df_test'],20,kabure,almost-complete-feature-engineering-ieee-data,kabure_almost-complete-feature-engineering-ieee-data_m20
['## Email Features'],21,kabure,almost-complete-feature-engineering-ieee-data,kabure_almost-complete-feature-engineering-ieee-data_m21
[],22,kabure,almost-complete-feature-engineering-ieee-data,kabure_almost-complete-feature-engineering-ieee-data_m22
['## Geting some card features'],23,kabure,almost-complete-feature-engineering-ieee-data,kabure_almost-complete-feature-engineering-ieee-data_m23
['## TransactionAmt to log'],24,kabure,almost-complete-feature-engineering-ieee-data,kabure_almost-complete-feature-engineering-ieee-data_m24
['## Get first nums in card1'],25,kabure,almost-complete-feature-engineering-ieee-data,kabure_almost-complete-feature-engineering-ieee-data_m25
['## Encoding transaction categoricals'],26,kabure,almost-complete-feature-engineering-ieee-data,kabure_almost-complete-feature-engineering-ieee-data_m26
"['# Feature selection \n', '- I will use the correlation to drop some features that are high correlated with each other']",27,kabure,almost-complete-feature-engineering-ieee-data,kabure_almost-complete-feature-engineering-ieee-data_m27
['## Setting X and y'],28,kabure,almost-complete-feature-engineering-ieee-data,kabure_almost-complete-feature-engineering-ieee-data_m28
['## Shape of last datasets'],29,kabure,almost-complete-feature-engineering-ieee-data,kabure_almost-complete-feature-engineering-ieee-data_m29
"['## Defining the objective function that will try optimize\n', '- testing sk fold and timeseriessplit']",30,kabure,almost-complete-feature-engineering-ieee-data,kabure_almost-complete-feature-engineering-ieee-data_m30
['## Running HyperOpt'],31,kabure,almost-complete-feature-engineering-ieee-data,kabure_almost-complete-feature-engineering-ieee-data_m31
"['- to see running outputs click in ""show code"" in the code above']",32,kabure,almost-complete-feature-engineering-ieee-data,kabure_almost-complete-feature-engineering-ieee-data_m32
['## Best parameters'],33,kabure,almost-complete-feature-engineering-ieee-data,kabure_almost-complete-feature-engineering-ieee-data_m33
['## Predicting with the best parameters '],34,kabure,almost-complete-feature-engineering-ieee-data,kabure_almost-complete-feature-engineering-ieee-data_m34
"['## I hope you enjoyned the kernel. \n', '## I will work in different features and publish trought this kernel.\n', '# Stay tuned and please give me your feedback and upvote the kernel =)\n', ""I'm not a ML pro, so your feedback is very important. ""]",35,kabure,almost-complete-feature-engineering-ieee-data,kabure_almost-complete-feature-engineering-ieee-data_m35
"['## Welcome to my fraud detection Kernel. \n', '\n']",0,kabure,baseline-fraud-detection-eda-interactive-views,kabure_baseline-fraud-detection-eda-interactive-views_m0
"['\n', '![](http://technosavvy.co.ke/wp-content/uploads/2015/12/fraud.jpg)']",1,kabure,baseline-fraud-detection-eda-interactive-views,kabure_baseline-fraud-detection-eda-interactive-views_m1
"['# Competition Objective is to detect fraud in transactions; \n', '\n', '## Data\n', '\n', '\n', 'In this competition you are predicting the probability that an online transaction is fraudulent, as denoted by the binary target ```isFraud```.\n', '\n', 'The data is broken into two files **identity** and **transaction**, which are joined by ```TransactionID```. \n', '\n', '> Note: Not all transactions have corresponding identity information.\n', '\n', '**Categorical Features - Transaction**\n', '\n', '- ProductCD\n', '- emaildomain\n', '- card1 - card6\n', '- addr1, addr2\n', '- P_emaildomain\n', '- R_emaildomain\n', '- M1 - M9\n', '\n', '**Categorical Features - Identity**\n', '\n', '- DeviceType\n', '- DeviceInfo\n', '- id_12 - id_38\n', '\n', '**The TransactionDT feature is a timedelta from a given reference datetime (not an actual timestamp).**\n', '\n', '## Questions\n', 'I will start exploring based on Categorical Features and Transaction Amounts.\n', 'The aim is answer some questions like:\n', '- What type of data we have on our data?\n', '- How many cols, rows, missing values we have?\n', '- Whats the target distribution?\n', ""- What's the Transactions values distribution of fraud and no fraud transactions?\n"", '- We have predominant fraudulent products? \n', '- What features or target shows some interesting patterns? \n', '- And a lot of more questions that will raise trought the exploration. \n', '\n', '\n', 'I hope you enjoy my kernel and if it be useful for you, <b>upvote</b> the kernel']",2,kabure,baseline-fraud-detection-eda-interactive-views,kabure_baseline-fraud-detection-eda-interactive-views_m2
"['## <font color=""red"">Please if this kernel were useful for you, please <b>UPVOTE</b> the kernel and give me your feedback =)</font>\n']",3,kabure,baseline-fraud-detection-eda-interactive-views,kabure_baseline-fraud-detection-eda-interactive-views_m3
['## Importing Libraries'],4,kabure,baseline-fraud-detection-eda-interactive-views,kabure_baseline-fraud-detection-eda-interactive-views_m4
['## Reading dataset'],5,kabure,baseline-fraud-detection-eda-interactive-views,kabure_baseline-fraud-detection-eda-interactive-views_m5
['## Functions to epxlore the data'],6,kabure,baseline-fraud-detection-eda-interactive-views,kabure_baseline-fraud-detection-eda-interactive-views_m6
"['## As we have a high dimensional data, I will reduce the memory usage']",7,kabure,baseline-fraud-detection-eda-interactive-views,kabure_baseline-fraud-detection-eda-interactive-views_m7
"['## Knowning the Identity dataset\n', '- What type of data we have on our data?']",8,kabure,baseline-fraud-detection-eda-interactive-views,kabure_baseline-fraud-detection-eda-interactive-views_m8
"['We have shape 144.2 rows by 41 columns. <br>\n', 'Also, we can see that almost all features has missing values. We will need to work with that. <br>\n', ""Let's see the transactions table and see the details ""]",9,kabure,baseline-fraud-detection-eda-interactive-views,kabure_baseline-fraud-detection-eda-interactive-views_m9
"['## Knowing the transactions\n', '- What type of data we have on our data?']",10,kabure,baseline-fraud-detection-eda-interactive-views,kabure_baseline-fraud-detection-eda-interactive-views_m10
"['Wow, We have a bizarre high dimension. The shape of Transactions is: 506691, 393<br>\n', 'I will need some time to explore it further. The first aim is start simple. ']",11,kabure,baseline-fraud-detection-eda-interactive-views,kabure_baseline-fraud-detection-eda-interactive-views_m11
['## Understanding the Target Distribution'],12,kabure,baseline-fraud-detection-eda-interactive-views,kabure_baseline-fraud-detection-eda-interactive-views_m12
"['Nice. <br>\n', ""We have only 3.5% of positive values in our target. It's an unbalanced data and we will keep investiganting the data to find some insight.""]",13,kabure,baseline-fraud-detection-eda-interactive-views,kabure_baseline-fraud-detection-eda-interactive-views_m13
['We can see that fraudulent transactions has a higher mean than No-Fraudulent Transactions'],14,kabure,baseline-fraud-detection-eda-interactive-views,kabure_baseline-fraud-detection-eda-interactive-views_m14
"['To avoid us of outliers and a better view of distribution, I will filter the data and get only values equal or lower than 800']",15,kabure,baseline-fraud-detection-eda-interactive-views,kabure_baseline-fraud-detection-eda-interactive-views_m15
['## Ploting and Knowing Transaction Amount distribution \n'],16,kabure,baseline-fraud-detection-eda-interactive-views,kabure_baseline-fraud-detection-eda-interactive-views_m16
"[""We don't have a high correlation between Transaction Amount and Fraud Transactions. <br>\n"", 'Also, we can see many cases of fraud transactions with values between 5 to 14 and other peak in 75 -  85.\n', '\n', ""Let's keep investigating this data""]",17,kabure,baseline-fraud-detection-eda-interactive-views,kabure_baseline-fraud-detection-eda-interactive-views_m17
"['## Knowing the Product feature\n', '- We have predominant fraudulent products? ']",18,kabure,baseline-fraud-detection-eda-interactive-views,kabure_baseline-fraud-detection-eda-interactive-views_m18
"['Cool!!! I think that this chart is very insightful. <br>\n', 'Altought the W is the most frequent Product we can see higher values in C, R and S products altought we have many lowest values in these categories. ']",19,kabure,baseline-fraud-detection-eda-interactive-views,kabure_baseline-fraud-detection-eda-interactive-views_m19
"['## Exploring Card Features \n', 'We have 6 columns that are about the Card of the transaction.<br>\n', 'I will start by the categoricals and after it, I will explore the continuous ']",20,kabure,baseline-fraud-detection-eda-interactive-views,kabure_baseline-fraud-detection-eda-interactive-views_m20
"[""Cool!! Again, we can clearly see that card4 we can't see different patterns, but in Card6 we can note that Credit has higher incidence of fraud than Debit payment""]",21,kabure,baseline-fraud-detection-eda-interactive-views,kabure_baseline-fraud-detection-eda-interactive-views_m21
['I will transform Card1 and Card2 to Logarithm scale to we better understand the distribution '],22,kabure,baseline-fraud-detection-eda-interactive-views,kabure_baseline-fraud-detection-eda-interactive-views_m22
['## Card1 feature by Target'],23,kabure,baseline-fraud-detection-eda-interactive-views,kabure_baseline-fraud-detection-eda-interactive-views_m23
['## Card2 feature by Target'],24,kabure,baseline-fraud-detection-eda-interactive-views,kabure_baseline-fraud-detection-eda-interactive-views_m24
"['## Card3 feature by Target\n', '- As we have many values with low frequency, I will set all values with frequency lower than 10 as -99']",25,kabure,baseline-fraud-detection-eda-interactive-views,kabure_baseline-fraud-detection-eda-interactive-views_m25
"['## Card5 feature by Target\n', '- Again, as we have many values with low frequency I will set all values with frequency lower than 20 as -99']",26,kabure,baseline-fraud-detection-eda-interactive-views,kabure_baseline-fraud-detection-eda-interactive-views_m26
"['## Exploring the M2-M9 features\n', '- Seen']",27,kabure,baseline-fraud-detection-eda-interactive-views,kabure_baseline-fraud-detection-eda-interactive-views_m27
[],28,kabure,baseline-fraud-detection-eda-interactive-views,kabure_baseline-fraud-detection-eda-interactive-views_m28
['## ScatterPollar of Binary Features'],29,kabure,baseline-fraud-detection-eda-interactive-views,kabure_baseline-fraud-detection-eda-interactive-views_m29
"['from sklearn.preprocessing import LabelEncoder\n', 'tmp = df_train_trans[]\n', '#Label encoding Binary columns\n', 'le = LabelEncoder()\n', '\n', ""tmp_churn = df_train_trans[df_train_trans['isFraud'] == 1]\n"", ""tmp_no_churn = df_train_trans[df_train_trans['isFraud'] == 0]\n"", '\n', 'bi_cs = df_train_trans.nunique()[df_train_trans.nunique() == 2].keys()\n', 'dat_rad = df_train_trans[bi_cs]\n', '\n', 'for cols in bi_cs :\n', '    tmp_churn[cols] = le.fit_transform(tmp_churn[cols])\n', '    \n', 'data_frame_x = tmp_churn[bi_cs].sum().reset_index()\n', 'data_frame_x.columns  = [""feature"",""yes""]\n', 'data_frame_x[""no""]    = tmp_churn.shape[0]  - data_frame_x[""yes""]\n', 'data_frame_x  = data_frame_x[data_frame_x[""feature""] != ""Churn""]\n', '\n', ""#count of 1's(yes)\n"", 'trace1 = go.Scatterpolar(r = data_frame_x[""yes""].values.tolist(), \n', '                         theta = data_frame_x[""feature""].tolist(),\n', '                         fill  = ""toself"",name = ""Fraud 1\'s"",\n', '                         mode = ""markers+lines"", visible=True,\n', '                         marker = dict(size = 5)\n', '                        )\n', '\n', ""#count of 0's(No)\n"", 'trace2 = go.Scatterpolar(r = data_frame_x[""no""].values.tolist(),\n', '                         theta = data_frame_x[""feature""].tolist(),\n', '                         fill  = ""toself"",name = ""Fraud 0\'s"",\n', '                         mode = ""markers+lines"", visible=True,\n', '                         marker = dict(size = 5)\n', '                        ) \n', 'for cols in bi_cs :\n', '    tmp_no_churn[cols] = le.fit_transform(tmp_no_churn[cols])\n', '    \n', 'data_frame_x = tmp_no_churn[bi_cs].sum().reset_index()\n', 'data_frame_x.columns  = [""feature"",""yes""]\n', 'data_frame_x[""no""]    = tmp_no_churn.shape[0]  - data_frame_x[""yes""]\n', 'data_frame_x  = data_frame_x[data_frame_x[""feature""] != ""Churn""]\n', '\n', ""#count of 1's(yes)\n"", 'trace3 = go.Scatterpolar(r = data_frame_x[""yes""].values.tolist(),\n', '                         theta = data_frame_x[""feature""].tolist(),\n', '                         fill  = ""toself"",name = ""NoFraud 1\'s"",\n', '                         mode = ""markers+lines"", visible=False,\n', '                         marker = dict(size = 5)\n', '                        )\n', '\n', ""#count of 0's(No)\n"", 'trace4 = go.Scatterpolar(r = data_frame_x[""no""].values.tolist(),\n', '                         theta = data_frame_x[""feature""].tolist(),\n', '                         fill  = ""toself"",name = ""NoFraud 0\'s"",\n', '                         mode = ""markers+lines"", visible=False,\n', '                         marker = dict(size = 5)\n', '                        ) \n', '\n', 'data = [trace1, trace2, trace3, trace4]\n', '\n', 'updatemenus = list([\n', '    dict(active=0,\n', '         x=-0.15,\n', '         buttons=list([  \n', '            dict(\n', ""                label = 'Fraud Dist',\n"", ""                 method = 'update',\n"", ""                 args = [{'visible': [True, True, False, False]}, \n"", ""                     {'title': 'Transaction Fraud Binary Counting Distribution'}]),\n"", '             \n', '             dict(\n', ""                  label = 'No-Fraud Dist',\n"", ""                 method = 'update',\n"", ""                 args = [{'visible': [False, False, True, True]},\n"", ""                     {'title': 'Transaction No-Fraud Binary Counting Distribution'}]),\n"", '\n', '        ]),\n', '    )\n', '])\n', '\n', ""layout = dict(title='ScatterPolar Distribution of Fraud and No-Fraud Transactions (Select from Dropdown)', \n"", '              showlegend=False,\n', '              updatemenus=updatemenus)\n', '\n', 'fig = dict(data=data, layout=layout)\n', '\n', 'iplot(fig)']",30,kabure,baseline-fraud-detection-eda-interactive-views,kabure_baseline-fraud-detection-eda-interactive-views_m30
['# NOTE: THIS KERNEL IS NOT FINISHED. I WILL KEEP EXPLORING IT. '],31,kabure,baseline-fraud-detection-eda-interactive-views,kabure_baseline-fraud-detection-eda-interactive-views_m31
"['### As my other kernel has running very slow because the interactive plots, I decided to start again using only Seaborn and matplotlib.\n', ""You can visit here: <a href='https://www.kaggle.com/kabure/baseline-fraud-detection-eda-interactive-views?scriptVersionId=17308287'> Interactive IEEE Fraud Detection</a> <br>\n"", '\n', 'Also, I worked in all features of this dataset and you can access the Kernel here: \n', ""<a href='https://www.kaggle.com/kabure/almost-complete-feature-engineering-ieee-data'> ~Almost~ complete Feature Engineering IEEE data</a> <br>""]",0,kabure,extensive-eda-and-modeling-xgb-hyperopt,kabure_extensive-eda-and-modeling-xgb-hyperopt_m0
"['# Competition Objective is to detect fraud in transactions; \n', '\n', '## Data\n', '\n', '\n', 'In this competition you are predicting the probability that an online transaction is fraudulent, as denoted by the binary target ```isFraud```.\n', '\n', 'The data is broken into two files **identity** and **transaction**, which are joined by ```TransactionID```. \n', '\n', '> Note: Not all transactions have corresponding identity information.\n', '\n', '**Categorical Features - Transaction**\n', '\n', '- ProductCD\n', '- emaildomain\n', '- card1 - card6\n', '- addr1, addr2\n', '- P_emaildomain\n', '- R_emaildomain\n', '- M1 - M9\n', '\n', '**Categorical Features - Identity**\n', '\n', '- DeviceType\n', '- DeviceInfo\n', '- id_12 - id_38\n', '\n', '**The TransactionDT feature is a timedelta from a given reference datetime (not an actual timestamp).**\n', '\n', '# Questions\n', 'I will start exploring based on Categorical Features and Transaction Amounts.\n', 'The aim is answer some questions like:\n', '- What type of data we have on our data?\n', '- How many cols, rows, missing values we have?\n', '- Whats the target distribution?\n', ""- What's the Transactions values distribution of fraud and no fraud transactions?\n"", '- We have predominant fraudulent products? \n', '- What features or target shows some interesting patterns? \n', '- And a lot of more questions that will raise trought the exploration. \n', '\n', '\n', '## I hope you enjoy my kernel and if it be useful for you, <b>upvote</b> the kernel']",1,kabure,extensive-eda-and-modeling-xgb-hyperopt,kabure_extensive-eda-and-modeling-xgb-hyperopt_m1
['## Importing necessary libraries'],2,kabure,extensive-eda-and-modeling-xgb-hyperopt,kabure_extensive-eda-and-modeling-xgb-hyperopt_m2
['# Importing train datasets'],3,kabure,extensive-eda-and-modeling-xgb-hyperopt,kabure_extensive-eda-and-modeling-xgb-hyperopt_m3
['I will set all functions in the cell bellow.'],4,kabure,extensive-eda-and-modeling-xgb-hyperopt,kabure_extensive-eda-and-modeling-xgb-hyperopt_m4
"['To see the output of the Resume Table, click to see the output ']",5,kabure,extensive-eda-and-modeling-xgb-hyperopt,kabure_extensive-eda-and-modeling-xgb-hyperopt_m5
['# Knowing the data'],6,kabure,extensive-eda-and-modeling-xgb-hyperopt,kabure_extensive-eda-and-modeling-xgb-hyperopt_m6
['# Target Distribution'],7,kabure,extensive-eda-and-modeling-xgb-hyperopt,kabure_extensive-eda-and-modeling-xgb-hyperopt_m7
"['We have 3.5% of Fraud transactions in our dataset. <br>I think that it would be interesting to see if the amount percentual is higher or lower than 3.5% of total. I will see it later. <br>\n', 'We have the same % when considering the Total Transactions Amount by Fraud and No Fraud. <br>\n', ""Let's explore the Transaction amount further below.""]",8,kabure,extensive-eda-and-modeling-xgb-hyperopt,kabure_extensive-eda-and-modeling-xgb-hyperopt_m8
['# Transaction Amount Quantiles'],9,kabure,extensive-eda-and-modeling-xgb-hyperopt,kabure_extensive-eda-and-modeling-xgb-hyperopt_m9
"[""Before Ploting the Transaction Amount, let's see the quantiles of Transaction Amount""]",10,kabure,extensive-eda-and-modeling-xgb-hyperopt,kabure_extensive-eda-and-modeling-xgb-hyperopt_m10
['# Ploting Transaction Amount Values Distribution'],11,kabure,extensive-eda-and-modeling-xgb-hyperopt,kabure_extensive-eda-and-modeling-xgb-hyperopt_m11
"['Nice! Now, we can see clearly the distribution of ']",12,kabure,extensive-eda-and-modeling-xgb-hyperopt,kabure_extensive-eda-and-modeling-xgb-hyperopt_m12
['# Seeing the Quantiles of Fraud and No Fraud Transactions'],13,kabure,extensive-eda-and-modeling-xgb-hyperopt,kabure_extensive-eda-and-modeling-xgb-hyperopt_m13
"['# Transaction Amount Outliers\n', ""- It's considering outlier values that are highest than 3 times the std from the mean""]",14,kabure,extensive-eda-and-modeling-xgb-hyperopt,kabure_extensive-eda-and-modeling-xgb-hyperopt_m14
"['If we consider only values between >= 0 to 800 we will avoid the outliers and has more confidence in our distribution. <br>\n', 'We have 10k rows with outliers that represents 1.74% of total rows.']",15,kabure,extensive-eda-and-modeling-xgb-hyperopt,kabure_extensive-eda-and-modeling-xgb-hyperopt_m15
"[""# Now, let's known the Product Feature\n"", '- Distribution Products\n', '- Distribution of Frauds by Product\n', '- Has Difference between Transaction Amounts in Products? ']",16,kabure,extensive-eda-and-modeling-xgb-hyperopt,kabure_extensive-eda-and-modeling-xgb-hyperopt_m16
"['W, C and R are the most frequent values. <br>\n', 'We can note that in W, H and R the distribution of Fraud values are slightly higher than the Non-Fraud Transactions']",17,kabure,extensive-eda-and-modeling-xgb-hyperopt,kabure_extensive-eda-and-modeling-xgb-hyperopt_m17
"['# Card Features\n', '- Based on Competition Description, card features are categoricals.\n', '- Lets understand the distribution of values\n', ""- What's the different in transactions and % of Fraud for each values in these features\n"", '- Card features has 6 columns, and 4 of them seems to be numericals, so lets see the quantiles and distributions']",18,kabure,extensive-eda-and-modeling-xgb-hyperopt,kabure_extensive-eda-and-modeling-xgb-hyperopt_m18
['Card2-Card6 has some missing values. We will need to due with it later.'],19,kabure,extensive-eda-and-modeling-xgb-hyperopt,kabure_extensive-eda-and-modeling-xgb-hyperopt_m19
['# Numericals Feature Card Quantiles'],20,kabure,extensive-eda-and-modeling-xgb-hyperopt,kabure_extensive-eda-and-modeling-xgb-hyperopt_m20
"['We can see that Card 1 and Card 2 has a large distribution of values, so maybe it will be better to get the log of these columns']",21,kabure,extensive-eda-and-modeling-xgb-hyperopt,kabure_extensive-eda-and-modeling-xgb-hyperopt_m21
"['# Visualizing Card 1, Card 2 and Card 3 Distributions\n', '- As the Card 1 and 2 are numericals, I will plot the distribution of them\n', '- in Card 3, as we have many values with low frequencies, I decided to set value to ""Others"" \n', '- Also, in Card 3 I set the % of Fraud ratio in yaxis2']",22,kabure,extensive-eda-and-modeling-xgb-hyperopt,kabure_extensive-eda-and-modeling-xgb-hyperopt_m22
"['Cool and Very Meaningful information. <br>\n', 'In Card3 we can see that 100 and 106 are the most common values in the column. <br>\n', 'We have 4.95% of Frauds in 100 and 1.52% in 106; The values with highest Fraud Transactions are 185, 119 and 119; <br>\n', '\n', 'In card5 the most frequent values are 226, 224, 166 that represents 73% of data. Also is posible to see high % of frauds in 137, 147, 141 that has few entries for values.']",23,kabure,extensive-eda-and-modeling-xgb-hyperopt,kabure_extensive-eda-and-modeling-xgb-hyperopt_m23
['# Card 4 - Categorical'],24,kabure,extensive-eda-and-modeling-xgb-hyperopt,kabure_extensive-eda-and-modeling-xgb-hyperopt_m24
"['We can see that 97% of our data are in Mastercard(32%) and Visa(65%);  <br>\n', 'we have a highest value in discover(~8%) against ~3.5% of Mastercard and Visa and 2.87% in American Express']",25,kabure,extensive-eda-and-modeling-xgb-hyperopt,kabure_extensive-eda-and-modeling-xgb-hyperopt_m25
['# Card 6 - Categorical'],26,kabure,extensive-eda-and-modeling-xgb-hyperopt,kabure_extensive-eda-and-modeling-xgb-hyperopt_m26
"['All data is on Credit and Debit. We can see a high percentual of Frauds in Credit than Debit transactions. <br>\n', ""The Distribution of Transaction Amount don't shows clear differences.""]",27,kabure,extensive-eda-and-modeling-xgb-hyperopt,kabure_extensive-eda-and-modeling-xgb-hyperopt_m27
['# Exploring M1-M9 Features '],28,kabure,extensive-eda-and-modeling-xgb-hyperopt,kabure_extensive-eda-and-modeling-xgb-hyperopt_m28
"['## M distributions:  Count, %Fraud and Transaction Amount distribution']",29,kabure,extensive-eda-and-modeling-xgb-hyperopt,kabure_extensive-eda-and-modeling-xgb-hyperopt_m29
"['Very cool!!! This graphs give us many interesting intuition about the M features.<br>\n', ""Only in M4 the Missing values haven't the highest % of Fraud.\n"", '\n']",30,kabure,extensive-eda-and-modeling-xgb-hyperopt,kabure_extensive-eda-and-modeling-xgb-hyperopt_m30
['# Addr1 and Addr2'],31,kabure,extensive-eda-and-modeling-xgb-hyperopt,kabure_extensive-eda-and-modeling-xgb-hyperopt_m31
"['I will set all values in Addr1 that has less than 5000 entries to ""Others""<br>\n', 'In Addr2 I will set as ""Others"" all values with less than 50 entries']",32,kabure,extensive-eda-and-modeling-xgb-hyperopt,kabure_extensive-eda-and-modeling-xgb-hyperopt_m32
['## Addr1 Distributions'],33,kabure,extensive-eda-and-modeling-xgb-hyperopt,kabure_extensive-eda-and-modeling-xgb-hyperopt_m33
['We can note interesting patterns on Addr1.'],34,kabure,extensive-eda-and-modeling-xgb-hyperopt,kabure_extensive-eda-and-modeling-xgb-hyperopt_m34
['## Addr2 Distributions'],35,kabure,extensive-eda-and-modeling-xgb-hyperopt,kabure_extensive-eda-and-modeling-xgb-hyperopt_m35
"['Almost all entries in Addr2 are in the same value. <br>\n', 'Interestingly in the value 65 , the percent of frauds are almost 60% <br>\n', 'Altought the value 87 has 88% of total entries, it has 96% of Total Transaction Amounts']",36,kabure,extensive-eda-and-modeling-xgb-hyperopt,kabure_extensive-eda-and-modeling-xgb-hyperopt_m36
"['# P emaildomain Distributions\n', '- I will group all e-mail domains by the respective enterprises.\n', '- Also, I will set as ""Others"" all values with less than 500 entries.']",37,kabure,extensive-eda-and-modeling-xgb-hyperopt,kabure_extensive-eda-and-modeling-xgb-hyperopt_m37
['## Ploting P-Email Domain'],38,kabure,extensive-eda-and-modeling-xgb-hyperopt,kabure_extensive-eda-and-modeling-xgb-hyperopt_m38
[],39,kabure,extensive-eda-and-modeling-xgb-hyperopt,kabure_extensive-eda-and-modeling-xgb-hyperopt_m39
"['## R-Email Domain plot distribution\n', '- I will group all e-mail domains by the respective enterprises.\n', '- I will set as ""Others"" all values with less than 300 entries.']",40,kabure,extensive-eda-and-modeling-xgb-hyperopt,kabure_extensive-eda-and-modeling-xgb-hyperopt_m40
"['We can see a very similar distribution in both email domain features. <br>\n', ""It's interesting that we have high values in google and icloud frauds""]",41,kabure,extensive-eda-and-modeling-xgb-hyperopt,kabure_extensive-eda-and-modeling-xgb-hyperopt_m41
[],42,kabure,extensive-eda-and-modeling-xgb-hyperopt,kabure_extensive-eda-and-modeling-xgb-hyperopt_m42
"['# C1-C14 features\n', ""- Let's understand what this features are.\n"", ""- What's the distributions? ""]",43,kabure,extensive-eda-and-modeling-xgb-hyperopt,kabure_extensive-eda-and-modeling-xgb-hyperopt_m43
['## C1 Distribution Plot'],44,kabure,extensive-eda-and-modeling-xgb-hyperopt,kabure_extensive-eda-and-modeling-xgb-hyperopt_m44
[],45,kabure,extensive-eda-and-modeling-xgb-hyperopt,kabure_extensive-eda-and-modeling-xgb-hyperopt_m45
[],46,kabure,extensive-eda-and-modeling-xgb-hyperopt,kabure_extensive-eda-and-modeling-xgb-hyperopt_m46
"['Top 3 values are 1, 2 and 3 and is the same on Total Amounts. We see the same pattern on fraud ratios']",47,kabure,extensive-eda-and-modeling-xgb-hyperopt,kabure_extensive-eda-and-modeling-xgb-hyperopt_m47
"['# TimeDelta Feature\n', ""- Let's see if the frauds have some specific hour that has highest % of frauds ""]",48,kabure,extensive-eda-and-modeling-xgb-hyperopt,kabure_extensive-eda-and-modeling-xgb-hyperopt_m48
"['## Converting to Total Days, Weekdays and Hours\n', 'In discussions tab I read an excellent solution to Timedelta column, I will set the link below; <br>\n', 'We will use the first date as 2017-12-01 and use the delta time to compute datetime features\n']",49,kabure,extensive-eda-and-modeling-xgb-hyperopt,kabure_extensive-eda-and-modeling-xgb-hyperopt_m49
['## Top Days with highest Total Transaction Amount'],50,kabure,extensive-eda-and-modeling-xgb-hyperopt,kabure_extensive-eda-and-modeling-xgb-hyperopt_m50
['## Ploting WeekDays Distributions'],51,kabure,extensive-eda-and-modeling-xgb-hyperopt,kabure_extensive-eda-and-modeling-xgb-hyperopt_m51
"[""We don't have the reference of date but we can see that two days has lower transactions, that we can infer it is weekend days""]",52,kabure,extensive-eda-and-modeling-xgb-hyperopt,kabure_extensive-eda-and-modeling-xgb-hyperopt_m52
['## Ploting Hours Distributions'],53,kabure,extensive-eda-and-modeling-xgb-hyperopt,kabure_extensive-eda-and-modeling-xgb-hyperopt_m53
[],54,kabure,extensive-eda-and-modeling-xgb-hyperopt,kabure_extensive-eda-and-modeling-xgb-hyperopt_m54
['## Transactions and Total Amount by each day'],55,kabure,extensive-eda-and-modeling-xgb-hyperopt,kabure_extensive-eda-and-modeling-xgb-hyperopt_m55
[],56,kabure,extensive-eda-and-modeling-xgb-hyperopt,kabure_extensive-eda-and-modeling-xgb-hyperopt_m56
"['## FRAUD TRANSACTIONS BY DATE\n', '- Visualizing only Fraud Transactions by Date']",57,kabure,extensive-eda-and-modeling-xgb-hyperopt,kabure_extensive-eda-and-modeling-xgb-hyperopt_m57
[],58,kabure,extensive-eda-and-modeling-xgb-hyperopt,kabure_extensive-eda-and-modeling-xgb-hyperopt_m58
"['# Features [id_12 to id_38]\n', '- categorical features in training identity dataset']",59,kabure,extensive-eda-and-modeling-xgb-hyperopt,kabure_extensive-eda-and-modeling-xgb-hyperopt_m59
['## Ploting columns with few unique values'],60,kabure,extensive-eda-and-modeling-xgb-hyperopt,kabure_extensive-eda-and-modeling-xgb-hyperopt_m60
['## Id 30'],61,kabure,extensive-eda-and-modeling-xgb-hyperopt,kabure_extensive-eda-and-modeling-xgb-hyperopt_m61
['## Id 31'],62,kabure,extensive-eda-and-modeling-xgb-hyperopt,kabure_extensive-eda-and-modeling-xgb-hyperopt_m62
"['## Modelling \n', 'To start simple, I will start using as base the kernels below: <br>\n', 'https://www.kaggle.com/artkulak/ieee-fraud-simple-baseline-0-9383-lb - (@artkulak - Art) <br>\n', 'https://www.kaggle.com/artgor/eda-and-models - (@artgor - Andrew Lukyanenko)\n', '\n']",63,kabure,extensive-eda-and-modeling-xgb-hyperopt,kabure_extensive-eda-and-modeling-xgb-hyperopt_m63
['# reducing memory usage'],64,kabure,extensive-eda-and-modeling-xgb-hyperopt,kabure_extensive-eda-and-modeling-xgb-hyperopt_m64
['# Mapping emails'],65,kabure,extensive-eda-and-modeling-xgb-hyperopt,kabure_extensive-eda-and-modeling-xgb-hyperopt_m65
['# Encoding categorical features'],66,kabure,extensive-eda-and-modeling-xgb-hyperopt,kabure_extensive-eda-and-modeling-xgb-hyperopt_m66
['# Some feature engineering'],67,kabure,extensive-eda-and-modeling-xgb-hyperopt,kabure_extensive-eda-and-modeling-xgb-hyperopt_m67
['# Concating dfs to get PCA of V features'],68,kabure,extensive-eda-and-modeling-xgb-hyperopt,kabure_extensive-eda-and-modeling-xgb-hyperopt_m68
['# Getting PCA '],69,kabure,extensive-eda-and-modeling-xgb-hyperopt,kabure_extensive-eda-and-modeling-xgb-hyperopt_m69
['# Seting train and test back'],70,kabure,extensive-eda-and-modeling-xgb-hyperopt,kabure_extensive-eda-and-modeling-xgb-hyperopt_m70
['# Seting X and y'],71,kabure,extensive-eda-and-modeling-xgb-hyperopt,kabure_extensive-eda-and-modeling-xgb-hyperopt_m71
['# Defining the HyperOpt function with parameters space and model'],72,kabure,extensive-eda-and-modeling-xgb-hyperopt,kabure_extensive-eda-and-modeling-xgb-hyperopt_m72
['# Running the optimizer'],73,kabure,extensive-eda-and-modeling-xgb-hyperopt,kabure_extensive-eda-and-modeling-xgb-hyperopt_m73
['# Best parameters'],74,kabure,extensive-eda-and-modeling-xgb-hyperopt,kabure_extensive-eda-and-modeling-xgb-hyperopt_m74
['# Trainning and Predicting with best Parameters'],75,kabure,extensive-eda-and-modeling-xgb-hyperopt,kabure_extensive-eda-and-modeling-xgb-hyperopt_m75
['## Predicting X test'],76,kabure,extensive-eda-and-modeling-xgb-hyperopt,kabure_extensive-eda-and-modeling-xgb-hyperopt_m76
['# Top 20 Feature importance'],77,kabure,extensive-eda-and-modeling-xgb-hyperopt,kabure_extensive-eda-and-modeling-xgb-hyperopt_m77
['## Seting y_pred to csv'],78,kabure,extensive-eda-and-modeling-xgb-hyperopt,kabure_extensive-eda-and-modeling-xgb-hyperopt_m78
"[""## I'm working in this kernel yet.\n"", '# <font color=""red"">Please if this kernel were useful for you, please <b>UPVOTE</b> =)</font>']",79,kabure,extensive-eda-and-modeling-xgb-hyperopt,kabure_extensive-eda-and-modeling-xgb-hyperopt_m79
"['From the [data description page](https://www.kaggle.com/c/ieee-fraud-detection/data) we know that ""The TransactionDT feature is a timedelta from a given reference datetime (not an actual timestamp).""\n', '\n', 'In this kernel we analyse the TransactionDT column, and support our hypothesis that the TransactionDT column starts at the 1st of December.\n', '\n', 'We can use the TransactionDT column to calculate features such as weekday and hour.']",0,kevinbonnes,transactiondt-starting-at-2017-12-01,kevinbonnes_transactiondt-starting-at-2017-12-01_m0
"[""The transaction datetime seems to end at '2018-12-31 23:59:05'.""]",1,kevinbonnes,transactiondt-starting-at-2017-12-01,kevinbonnes_transactiondt-starting-at-2017-12-01_m1
"['There are peaks in transactions around christmas 2017 and christmas 2018. For both years, the peaks end at the 26th of December.']",2,kevinbonnes,transactiondt-starting-at-2017-12-01,kevinbonnes_transactiondt-starting-at-2017-12-01_m2
['## 0. Context'],0,kimchiwoong,simple-eda-ensemble-for-xgboost-and-lgbm,kimchiwoong_simple-eda-ensemble-for-xgboost-and-lgbm_m0
"['- Loading Library\n', '- Read Data SET\n', '- EDA\n', '- Preprocessing\n', '* Feature Engineering\n', '- Modeling\n', '- Evaluation']",1,kimchiwoong,simple-eda-ensemble-for-xgboost-and-lgbm,kimchiwoong_simple-eda-ensemble-for-xgboost-and-lgbm_m1
['## 1. Loading Library'],2,kimchiwoong,simple-eda-ensemble-for-xgboost-and-lgbm,kimchiwoong_simple-eda-ensemble-for-xgboost-and-lgbm_m2
['* Install LightGBM GPU VERSION'],3,kimchiwoong,simple-eda-ensemble-for-xgboost-and-lgbm,kimchiwoong_simple-eda-ensemble-for-xgboost-and-lgbm_m3
['## 2. Reading Data SET'],4,kimchiwoong,simple-eda-ensemble-for-xgboost-and-lgbm,kimchiwoong_simple-eda-ensemble-for-xgboost-and-lgbm_m4
['### 1) Reduce Memory using down sizing Data SET'],5,kimchiwoong,simple-eda-ensemble-for-xgboost-and-lgbm,kimchiwoong_simple-eda-ensemble-for-xgboost-and-lgbm_m5
['## 3. EDA'],6,kimchiwoong,simple-eda-ensemble-for-xgboost-and-lgbm,kimchiwoong_simple-eda-ensemble-for-xgboost-and-lgbm_m6
"['- From above on, Usage of RAM is 3.8GB']",7,kimchiwoong,simple-eda-ensemble-for-xgboost-and-lgbm,kimchiwoong_simple-eda-ensemble-for-xgboost-and-lgbm_m7
"['- In this competition you are predicting the probability that an online transaction is fraudulent, as denoted by the binary target isFraud.\n', '\n', '- The data is broken into two files identity and transaction, which are joined by TransactionID. Not all transactions have corresponding identity information.']",8,kimchiwoong,simple-eda-ensemble-for-xgboost-and-lgbm,kimchiwoong_simple-eda-ensemble-for-xgboost-and-lgbm_m8
"['- In this competition you are predicting the probability that an online transaction is fraudulent, as denoted by the binary target isFraud.\n', '\n', '- The data is broken into two files identity and transaction, which are joined by TransactionID. Not all transactions have corresponding identity information.\n', '------------------------------\n', '- *Categorical Features - Transaction\n', '- ProductCD\n', '- card1 - card6\n', '- addr1, addr2\n', '- P_emaildomain\n', '- R_emaildomain\n', '- M1 - M9\n', '------------------------------\n', '- *Categorical Features - Identity\n', '- DeviceType\n', '- DeviceInfo id_12 - id_38\n', '------------------------------\n', '- The TransactionDT feature is a timedelta from a given reference datetime (not an actual timestamp).']",9,kimchiwoong,simple-eda-ensemble-for-xgboost-and-lgbm,kimchiwoong_simple-eda-ensemble-for-xgboost-and-lgbm_m9
['### 1) Check Missing Data'],10,kimchiwoong,simple-eda-ensemble-for-xgboost-and-lgbm,kimchiwoong_simple-eda-ensemble-for-xgboost-and-lgbm_m10
['### 2) Check Numeric Columns Properties'],11,kimchiwoong,simple-eda-ensemble-for-xgboost-and-lgbm,kimchiwoong_simple-eda-ensemble-for-xgboost-and-lgbm_m11
['### 3) Check Categorical Columns Properties'],12,kimchiwoong,simple-eda-ensemble-for-xgboost-and-lgbm,kimchiwoong_simple-eda-ensemble-for-xgboost-and-lgbm_m12
['## 4. Feature Engineering'],13,kimchiwoong,simple-eda-ensemble-for-xgboost-and-lgbm,kimchiwoong_simple-eda-ensemble-for-xgboost-and-lgbm_m13
['### 1) Correlation Analysis of Numeric Values'],14,kimchiwoong,simple-eda-ensemble-for-xgboost-and-lgbm,kimchiwoong_simple-eda-ensemble-for-xgboost-and-lgbm_m14
['### 2) Feature Slicing in Time Series Data'],15,kimchiwoong,simple-eda-ensemble-for-xgboost-and-lgbm,kimchiwoong_simple-eda-ensemble-for-xgboost-and-lgbm_m15
['### 3) Feature Values Filtering in Categorical Columns'],16,kimchiwoong,simple-eda-ensemble-for-xgboost-and-lgbm,kimchiwoong_simple-eda-ensemble-for-xgboost-and-lgbm_m16
['## 5. Modeling'],17,kimchiwoong,simple-eda-ensemble-for-xgboost-and-lgbm,kimchiwoong_simple-eda-ensemble-for-xgboost-and-lgbm_m17
['### 1) PreProcessing'],18,kimchiwoong,simple-eda-ensemble-for-xgboost-and-lgbm,kimchiwoong_simple-eda-ensemble-for-xgboost-and-lgbm_m18
['### 2) Train / Validation Split'],19,kimchiwoong,simple-eda-ensemble-for-xgboost-and-lgbm,kimchiwoong_simple-eda-ensemble-for-xgboost-and-lgbm_m19
['### 3) XGBoost Fitting'],20,kimchiwoong,simple-eda-ensemble-for-xgboost-and-lgbm,kimchiwoong_simple-eda-ensemble-for-xgboost-and-lgbm_m20
['#### 4) LightGBM Fitting'],21,kimchiwoong,simple-eda-ensemble-for-xgboost-and-lgbm,kimchiwoong_simple-eda-ensemble-for-xgboost-and-lgbm_m21
['### 5) Submission to Score Board'],22,kimchiwoong,simple-eda-ensemble-for-xgboost-and-lgbm,kimchiwoong_simple-eda-ensemble-for-xgboost-and-lgbm_m22
"['This kernel demonstrates a way of using LightGBM with GPU support in Kaggle kernels.\n', '\n', ""Basically to avoid the following error which we get when we give `device='gpu'` in LightGBM parameters.\n"", '\n', '`LightGBMError: GPU Tree Learner was not enabled in this build.\n', 'Please recompile with CMake option -DUSE_GPU=1`\n', '\n', 'The code & idea are heavily inspired from the following:\n', '* https://www.kaggle.com/inversion/ieee-simple-xgboost\n', '* https://www.kaggle.com/xhlulu/ieee-fraud-xgboost-with-gpu-fit-in-40s\n', '* https://www.kaggle.com/vinhnguyen/gpu-acceleration-for-lightgbm\n', '* https://lightgbm.readthedocs.io/en/latest/GPU-Tutorial.html\n', '* https://lightgbm.readthedocs.io/en/latest/GPU-Performance.html']",0,kirankunapuli,ieee-fraud-lightgbm-with-gpu,kirankunapuli_ieee-fraud-lightgbm-with-gpu_m0
['## LightGBM GPU Installation'],1,kirankunapuli,ieee-fraud-lightgbm-with-gpu,kirankunapuli_ieee-fraud-lightgbm-with-gpu_m1
['### Build and re-install LightGBM with GPU support'],2,kirankunapuli,ieee-fraud-lightgbm-with-gpu,kirankunapuli_ieee-fraud-lightgbm-with-gpu_m2
['## Imports'],3,kirankunapuli,ieee-fraud-lightgbm-with-gpu,kirankunapuli_ieee-fraud-lightgbm-with-gpu_m3
['## Preprocessing'],4,kirankunapuli,ieee-fraud-lightgbm-with-gpu,kirankunapuli_ieee-fraud-lightgbm-with-gpu_m4
['## Modeling'],5,kirankunapuli,ieee-fraud-lightgbm-with-gpu,kirankunapuli_ieee-fraud-lightgbm-with-gpu_m5
['## Feature Importances'],6,kirankunapuli,ieee-fraud-lightgbm-with-gpu,kirankunapuli_ieee-fraud-lightgbm-with-gpu_m6
['## Submission'],7,kirankunapuli,ieee-fraud-lightgbm-with-gpu,kirankunapuli_ieee-fraud-lightgbm-with-gpu_m7
"[""With `device='gpu'` parameter commented, it takes ~ 22 minutes to fit on CPU.\n"", '\n', '`CPU times: user 42min 4s, sys: 13 s, total: 42min 17s\n', 'Wall time: 21min 47s`\n', '\n', ""With `device='gpu'`, it takes ~ 3 minutes to fit on GPU.\n"", '\n', '`CPU times: user 3min 59s, sys: 46 s, total: 4min 45s\n', 'Wall time: 2min 34s`\n', '\n', '*Note: The CPU provided in Kaggle GPU kernel is 2 core, so the time to fit with above parameters might take half the time(~11 minutes) on a CPU only kernel(4 core CPU), which is still slower than LightGBM GPU implementation.*']",8,kirankunapuli,ieee-fraud-lightgbm-with-gpu,kirankunapuli_ieee-fraud-lightgbm-with-gpu_m8
['## StackNet Installation'],0,kirankunapuli,ieee-fraud-stacknet-on-gpu-lgb-xgb-cb,kirankunapuli_ieee-fraud-stacknet-on-gpu-lgb-xgb-cb_m0
"['This kernel demonstrates a way of using [StackNet](https://github.com/h2oai/pystacknet) with LightGBM, XGBoost & Catboost with GPU support in Kaggle kernels.']",1,kirankunapuli,ieee-fraud-stacknet-on-gpu-lgb-xgb-cb,kirankunapuli_ieee-fraud-stacknet-on-gpu-lgb-xgb-cb_m1
"['### LightGBM GPU Installation\n', 'Full implementation and tutorial at https://www.kaggle.com/kirankunapuli/ieee-fraud-lightgbm-with-gpu']",2,kirankunapuli,ieee-fraud-stacknet-on-gpu-lgb-xgb-cb,kirankunapuli_ieee-fraud-stacknet-on-gpu-lgb-xgb-cb_m2
['## Imports'],3,kirankunapuli,ieee-fraud-stacknet-on-gpu-lgb-xgb-cb,kirankunapuli_ieee-fraud-stacknet-on-gpu-lgb-xgb-cb_m3
['## Preprocessing'],4,kirankunapuli,ieee-fraud-stacknet-on-gpu-lgb-xgb-cb,kirankunapuli_ieee-fraud-stacknet-on-gpu-lgb-xgb-cb_m4
['## StackNet Model'],5,kirankunapuli,ieee-fraud-stacknet-on-gpu-lgb-xgb-cb,kirankunapuli_ieee-fraud-stacknet-on-gpu-lgb-xgb-cb_m5
['## Submission'],6,kirankunapuli,ieee-fraud-stacknet-on-gpu-lgb-xgb-cb,kirankunapuli_ieee-fraud-stacknet-on-gpu-lgb-xgb-cb_m6
"['# Summary\n', '\n', ""I'll try to make a small summary for this blend baseline:\n"", '\n', ""Step: 0. EDA (missing kernel here, I'll post later)\n"", '\n', '\n', 'Step: 1. Minify Data \n', '> https://www.kaggle.com/kyakovlev/ieee-data-minification\n', '\n', '\n', 'Step: 2. Make ground baseline with no fe:\n', '> https://www.kaggle.com/kyakovlev/ieee-ground-baseline and \n', '> https://www.kaggle.com/kyakovlev/ieee-ground-baseline-deeper-learning\n', '\n', '\n', 'Step: 3. Make a small FE and see I you can understand data you have\n', '>  https://www.kaggle.com/kyakovlev/ieee-ground-baseline-make-amount-useful-again and\n', '>  https://www.kaggle.com/kyakovlev/ieee-gb-2-make-amount-useful-again\n', '\n', '\n', 'Step: 4. Find good CV strategy \n', '>  https://www.kaggle.com/kyakovlev/ieee-cv-options\n', 'and same with gap to compare results (gap in values is what we have in test set)\n', 'https://www.kaggle.com/kyakovlev/ieee-cv-options-with-gap\n', '\n', 'Step: 4(1). Groupkfold (by timeblocks) application\n', '> https://www.kaggle.com/kyakovlev/ieee-lgbm-with-groupkfold-cv\n', '\n', '\n', 'Step: 5. Try different set of features\n', '>  https://www.kaggle.com/kyakovlev/ieee-experimental\n', '\n', '\n', 'Step: 6. Make deeper FE (brute force option)\n', '> https://www.kaggle.com/kyakovlev/ieee-fe-with-some-eda\n', '\n', '\n', ""Step: 7. Features selection (missing kernel here, I'll post later)\n"", '\n', '\n', ""Step: 8. Hyperopt (missing kernel here, I'll post later)\n"", '\n', '\n', ""Step: 9. Try other models (XGBoost, CatBoost, NN - missing kernel here, I'll post later)\n"", '> CatBoost (with categorical transformations)  https://www.kaggle.com/kyakovlev/ieee-catboost-baseline-with-groupkfold-cv\n', '\n', ""Step: 10. Try blending and stacking (missing kernel here, I'll post later)\n"", '\n', '---\n', '\n', '(Utils)\n', '\n', 'Some tricks that where used in fe kernel\n', '> https://www.kaggle.com/kyakovlev/ieee-small-tricks\n', '\n', 'Part of EDA (Just few things)\n', '> https://www.kaggle.com/kyakovlev/ieee-check-noise and https://www.kaggle.com/kyakovlev/ieee-simple-eda\n', '\n', '---\n', '\n', 'https://www.kaggle.com/c/ieee-fraud-detection/discussion/104142']",0,kyakovlev,ieee-internal-blend,kyakovlev_ieee-internal-blend_m0
"['Saving memory size can greatly reduce the burden to your system memory while improve your analysis experience.\n', 'Here there are 3 steps to do it. ']",0,ljjblackpig,3-steps-to-reduce-memory-size-for-the-dataset,ljjblackpig_3-steps-to-reduce-memory-size-for-the-dataset_m0
['## 1. Downcasting Numeric Columns'],1,ljjblackpig,3-steps-to-reduce-memory-size-for-the-dataset,ljjblackpig_3-steps-to-reduce-memory-size-for-the-dataset_m1
"['As you can see, the tran_id dataframe has been downcasted from 161.9MB to 148.7MB (8.1%), while the trin_tran dataframe has been downcasted from 2.1GB to 1.2GB (42.9%). Of course, the more int and float column you have, the chance that these functions can better improve your performaces will be.\n', '\n', 'The idea behind the scene is pandas automatically read in your dataframe using int 64 or float 64, most of the time you do not need it to be this big, here is a size chart from StackOverflow:\n', '**   Type      Capacity\n', '\n', '   Int16 -- (-32,768 to +32,767)\n', '\n', '   Int32 -- (-2,147,483,648 to +2,147,483,647)\n', '\n', '   Int64 -- (-9,223,372,036,854,775,808 to +9,223,372,036,854,775,807)**\n', '   \n', 'So most of the time, int16 or even int8 could do the job and thus save you the space. ']",2,ljjblackpig,3-steps-to-reduce-memory-size-for-the-dataset,ljjblackpig_3-steps-to-reduce-memory-size-for-the-dataset_m2
['## 2.Downcasting the object column by converting them to categorical'],3,ljjblackpig,3-steps-to-reduce-memory-size-for-the-dataset,ljjblackpig_3-steps-to-reduce-memory-size-for-the-dataset_m3
"['The idea behind the this is based on the fact that most of the column only takes few values and pandas can limit those values only to the few categorical value to save the memory. \n', '\n', '""Often in real-time, data includes the text columns, which are repetitive. Features like gender, country, and codes are always repetitive. These are the examples for categorical data.\n', '\n', 'Categorical variables can take on only a limited, and usually fixed number of possible values. Besides the fixed length, categorical data might have an order but cannot perform numerical operation. Categorical are a Pandas data type.\n', '\n', 'The categorical data type is useful in the following cases −\n', '\n', 'A string variable consisting of only a few different values. Converting such a string variable to a categorical variable will save some memory.\n', '\n', 'The lexical order of a variable is not the same as the logical order (“one”, “two”, “three”). By converting to a categorical and specifying an order on the categories, sorting and min/max will use the logical order instead of the lexical order.\n', '\n', 'As a signal to other python libraries that this column should be treated as a categorical variable (e.g. to use suitable statistical methods or plot types).""\n', '(https://www.tutorialspoint.com/python_pandas/python_pandas_categorical_data)']",4,ljjblackpig,3-steps-to-reduce-memory-size-for-the-dataset,ljjblackpig_3-steps-to-reduce-memory-size-for-the-dataset_m4
['## 3.Saving your dataframe as pickle file for fast read'],5,ljjblackpig,3-steps-to-reduce-memory-size-for-the-dataset,ljjblackpig_3-steps-to-reduce-memory-size-for-the-dataset_m5
['**Do not Forget to save your work. This way you can access the downcasted df everytime you load it!**'],6,ljjblackpig,3-steps-to-reduce-memory-size-for-the-dataset,ljjblackpig_3-steps-to-reduce-memory-size-for-the-dataset_m6
['This code refers to Kaggle kernal: https://www.kaggle.com/frankherfert/tips-tricks-for-working-with-large-datasets?source=post_page---------------------------'],7,ljjblackpig,3-steps-to-reduce-memory-size-for-the-dataset,ljjblackpig_3-steps-to-reduce-memory-size-for-the-dataset_m7
"['# IEEE-CIS Fraud Detection: Split Value Histogram\n', '\n', 'I really enjoyed the [IEEE-CIS Fraud Detection — LightGBM Split Points](https://www.kaggle.com/jtrotman/ieee-fraud-lgb-split-points) kernel from [jtrotman](https://www.kaggle.com/jtrotman). It shows us how to visualize the decision tree splitting values count for a given feature. By counting the split values, we can gain new insights and helps us to explain the decision tree. LightGBM introduced a new API [plot_split_value_histogram](https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.plot_split_value_histogram.html) in version `2.3.0` that makes it even easier for us. This kernel shows you demonstration of the API for this competition.']",0,madiyar,ieee-fraud-lightgbm-split-value-histogram,madiyar_ieee-fraud-lightgbm-split-value-histogram_m0
"[""Let's start from installing the latest version of LightGBM:""]",1,madiyar,ieee-fraud-lightgbm-split-value-histogram,madiyar_ieee-fraud-lightgbm-split-value-histogram_m1
['# Data Preperation'],2,madiyar,ieee-fraud-lightgbm-split-value-histogram,madiyar_ieee-fraud-lightgbm-split-value-histogram_m2
['# Training'],3,madiyar,ieee-fraud-lightgbm-split-value-histogram,madiyar_ieee-fraud-lightgbm-split-value-histogram_m3
['# Visuals'],4,madiyar,ieee-fraud-lightgbm-split-value-histogram,madiyar_ieee-fraud-lightgbm-split-value-histogram_m4
"[""Let's plot split value histogram for several important features by using the new  API [plot_split_value_histogram](https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.plot_split_value_histogram.html). Note: the API does not handle categorical variables as of 29 September 2019. ""]",5,madiyar,ieee-fraud-lightgbm-split-value-histogram,madiyar_ieee-fraud-lightgbm-split-value-histogram_m5
"['# Conclusion\n', '\n', 'Hopefully, this kernel brings some value :). ']",6,madiyar,ieee-fraud-lightgbm-split-value-histogram,madiyar_ieee-fraud-lightgbm-split-value-histogram_m6
"['### Why this kernel?\n', 'Whenever the size of dataset goes above 1.5GB there are some memory issues when working with Kaggle Kernels, particlarly when you want to fit everything in one kernel. In the public kernls of this competition I saw a very commonly used function `reduce_mem_usage()` to reduce memory usage introduced in [here](https://www.kaggle.com/mjbahmani/reducing-memory-size-for-ieee) that is basically using the function first introduced in [here](https://www.kaggle.com/arjanso/reducing-dataframe-memory-size-by-65). The same (or very similar function) was being used in Predicting Molecular Properties competition as well. \n', '\n', ""As cool as it seems to use this function, it is not the best idea to use a function blindly. First of all, this function automatically fills in your null values for you! that is not exactly what you asked for. Moreover, there are some hidden pitfalls in using that function as described in [here](https://www.kaggle.com/c/champs-scalar-coupling/discussion/96655#latest-566225) and I identified them in [here](https://www.kaggle.com/mhviraf/why-i-wouldn-t-use-reduce-mem-usage). I think that is the reason why Pandas (with all of its genious developers) doesn't have this basic function built-in. Hence, I believe there is a better solution to this problem here.\n"", '\n', '### What is the problem in first place?\n', 'The fundamental problem is the null values we have in dataset. Since `numpy` treats `NaN` cells as `float`, whenever you have null values in a column even if the natural data type of that column is integer Pandas forces that column to be `float64`. This results in a significantly higher memory usage because many of the features we have in this competition are integers but when loaded as a Pandas DataFrame, they will be stored in memory as `float64`.\n', '\n', '### Solution\n', ""The solution is simple. Load integers as integers in the first place. The easiest way to do so is to identify data types and use `dtypes={'columns': 'dtype'}` when calling `pd.read_csv()`. However, when you want to use `intXX` as dtype in Pandas (versions earlier than 0.24.0) it doesn't let you use it for columns that contain `NaN` values because as I said before, Pandas uses Numpy `NaN` which is by definition a float (this is why using `reduce_mem_usage()` would fill your `NaN` values for you). However, *starting version 0.24.0* Pandas has introduced a new nullable integer datatype that actually lets you have `NaN` values in integer columns (signed and unsigned). We are going to use this new datatype.\n"", '\n', '### References:\n', '* https://docs.scipy.org/doc/numpy-1.13.0/user/basics.types.html\n', '* https://pandas.pydata.org/pandas-docs/stable/user_guide/integer_na.html']",0,mhviraf,reducing-memory-size-an-alternative,mhviraf_reducing-memory-size-an-alternative_m0
"['First we need to install pandas versions later than 0.24.0. In this kernel I will install pandas==0.24.0.\n', '\n', ""Don't forget to turn on the Internet under the Settings of your kernel.""]",1,mhviraf,reducing-memory-size-an-alternative,mhviraf_reducing-memory-size-an-alternative_m1
"[""As can be seen, if we import data as is, by default it uses Numpy and its memory usage is 2123.15 MB. Now let's look at the data types""]",2,mhviraf,reducing-memory-size-an-alternative,mhviraf_reducing-memory-size-an-alternative_m2
"['376 `float64`s. But do we really have that many columns of type `float64`? Given that we have 590540 rows, using correct data types will make a significant difference. I identified the unsigned nullable integer columns and listed them below as a Python dictionary. To use it, copy this dictionary and simply pass `dtype=proper_dtypes` as an argument in `read_csv()`.']",3,mhviraf,reducing-memory-size-an-alternative,mhviraf_reducing-memory-size-an-alternative_m3
['#### Test set'],4,mhviraf,reducing-memory-size-an-alternative,mhviraf_reducing-memory-size-an-alternative_m4
"['## Advantages over reduce_mem_usage():\n', '* You get to choose how to handle your null values.\n', ""* You don't loose percision.\n"", '\n', ""(Don't forget that you will need to install Pandas version > 0.24.0)""]",5,mhviraf,reducing-memory-size-an-alternative,mhviraf_reducing-memory-size-an-alternative_m5
"[' #  <div style=""text-align: center"">  Reducing  Memory Size for IEEE </div> \n', ' <div style=""text-align:center"">  </div>\n', '![mem](http://s8.picofile.com/file/8367719234/mem.png) \n', '<div style=""text-align:center""> last update: <b> 19/07/2019</b></div>\n']",0,mjbahmani,reducing-memory-size-for-ieee,mjbahmani_reducing-memory-size-for-ieee_m0
"['## Objective of the Kernel: Save Time & Memory\n', 'If you would like to create a kernel for this Competition. this is a good idea to add this kernel as a **data set** to your own kernel. due to you can save your time and memory.']",1,mjbahmani,reducing-memory-size-for-ieee,mjbahmani_reducing-memory-size-for-ieee_m1
"['___MEMORY USAGE  BEFORE AND AFTER COMPLETION FOR TRAIN:___\n', '<br/>\n', 'Memory usage before running this script : 1975.3707885742188  MB\n', '<br/>\n', 'Memory usage after running this script  : ~ **480  MB**\n', '<br/>\n', 'This is ~ 28 % of the initial size']",2,mjbahmani,reducing-memory-size-for-ieee,mjbahmani_reducing-memory-size-for-ieee_m2
"['\n', '___MEMORY USAGE  BEFORE AND AFTER COMPLETION FOR TEST:___\n', '<br/>\n', 'Memory usage before running this script : 1693.867820739746  MB\n', '<br/>\n', 'Memory usage after running this script: ~ **480  MB**\n', '<br/>\n', 'This is ~  28  % of the initial size']",3,mjbahmani,reducing-memory-size-for-ieee,mjbahmani_reducing-memory-size-for-ieee_m3
['## Import'],4,mjbahmani,reducing-memory-size-for-ieee,mjbahmani_reducing-memory-size-for-ieee_m4
['What do we have in input'],5,mjbahmani,reducing-memory-size-for-ieee,mjbahmani_reducing-memory-size-for-ieee_m5
['## Import Dataset to play with it'],6,mjbahmani,reducing-memory-size-for-ieee,mjbahmani_reducing-memory-size-for-ieee_m6
['### Creat our train & test dataset'],7,mjbahmani,reducing-memory-size-for-ieee,mjbahmani_reducing-memory-size-for-ieee_m7
"['### Before Reducing Memory\n', 'When I have just read the data set and join them!I saw that the status of my RAM is more than 9GB!']",8,mjbahmani,reducing-memory-size-for-ieee,mjbahmani_reducing-memory-size-for-ieee_m8
['![ram1](http://s9.picofile.com/file/8366931918/ram1.png)'],9,mjbahmani,reducing-memory-size-for-ieee,mjbahmani_reducing-memory-size-for-ieee_m9
['Then we shoud just delete some dt!'],10,mjbahmani,reducing-memory-size-for-ieee,mjbahmani_reducing-memory-size-for-ieee_m10
"['![ram2](http://s8.picofile.com/file/8366932526/ram2.png)\n', '3GB of RAM has got free! now just check the size of our train & test']",11,mjbahmani,reducing-memory-size-for-ieee,mjbahmani_reducing-memory-size-for-ieee_m11
"['# IEEE Reducing  Memory Size\n', 'It is necessary that after using this code, carefully check the output results for each column.']",12,mjbahmani,reducing-memory-size-for-ieee,mjbahmani_reducing-memory-size-for-ieee_m12
['Reducing for train data set:'],13,mjbahmani,reducing-memory-size-for-ieee,mjbahmani_reducing-memory-size-for-ieee_m13
['Reducing for test data set:'],14,mjbahmani,reducing-memory-size-for-ieee,mjbahmani_reducing-memory-size-for-ieee_m14
['Check again! our RAM. 2 GB has got free!'],15,mjbahmani,reducing-memory-size-for-ieee,mjbahmani_reducing-memory-size-for-ieee_m15
['![ram3](http://s8.picofile.com/file/8366940442/ram3.png)'],16,mjbahmani,reducing-memory-size-for-ieee,mjbahmani_reducing-memory-size-for-ieee_m16
"['## Add this kernel as Dataset\n', 'Now we just save our output as csv files. then you can simply add them to your own kernel.you will save time and  memory.']",17,mjbahmani,reducing-memory-size-for-ieee,mjbahmani_reducing-memory-size-for-ieee_m17
"['## How about other ways!\n', 'I have used this [great kernel](https://www.kaggle.com/arjanso/reducing-dataframe-memory-size-by-65) but there are also other ways such as:\n', '1. https://www.dataquest.io/blog/pandas-big-data/\n', '2. [optimizing-the-size-of-a-pandas-dataframe-for-low-memory-environment](https://medium.com/@vincentteyssier/optimizing-the-size-of-a-pandas-dataframe-for-low-memory-environment-5f07db3d72e)\n', '3. [pandas-making-dataframe-smaller-faster](https://www.ritchieng.com/pandas-making-dataframe-smaller-faster/)']",18,mjbahmani,reducing-memory-size-for-ieee,mjbahmani_reducing-memory-size-for-ieee_m18
"['Reference: https://www.kaggle.com/nroman/lgb-single-model-lb-0-9419\n', '> https://www.kaggle.com/roydatascience/light-gbm-with-complete-eda\n', '* https://www.kaggle.com/ragnar123/e-d-a-and-baseline-mix-lgbm']",0,navneetkr123,ieee-fraud-play-with-count-lightgbm,navneetkr123_ieee-fraud-play-with-count-lightgbm_m0
['> Please give your feedback'],1,navneetkr123,ieee-fraud-play-with-count-lightgbm,navneetkr123_ieee-fraud-play-with-count-lightgbm_m1
['**Importing necessary library**'],2,navneetkr123,ieee-fraud-play-with-count-lightgbm,navneetkr123_ieee-fraud-play-with-count-lightgbm_m2
['**Importing datasets**'],3,navneetkr123,ieee-fraud-play-with-count-lightgbm,navneetkr123_ieee-fraud-play-with-count-lightgbm_m3
['**Merging transaction and Identity **'],4,navneetkr123,ieee-fraud-play-with-count-lightgbm,navneetkr123_ieee-fraud-play-with-count-lightgbm_m4
['**Negative Downsampling**'],5,navneetkr123,ieee-fraud-play-with-count-lightgbm,navneetkr123_ieee-fraud-play-with-count-lightgbm_m5
['> From below we can see that there are a lot of features with almost 99% nan values'],6,navneetkr123,ieee-fraud-play-with-count-lightgbm,navneetkr123_ieee-fraud-play-with-count-lightgbm_m6
['> Sorting features on basis of TransactionDT'],7,navneetkr123,ieee-fraud-play-with-count-lightgbm,navneetkr123_ieee-fraud-play-with-count-lightgbm_m7
"['**Taking all features**\n', '> Initially I will start with all the features and then will drop most of the features on the basis of count']",8,navneetkr123,ieee-fraud-play-with-count-lightgbm,navneetkr123_ieee-fraud-play-with-count-lightgbm_m8
['> From below we can see that length of features is 434'],9,navneetkr123,ieee-fraud-play-with-count-lightgbm,navneetkr123_ieee-fraud-play-with-count-lightgbm_m9
['**Displaying all the columns**'],10,navneetkr123,ieee-fraud-play-with-count-lightgbm,navneetkr123_ieee-fraud-play-with-count-lightgbm_m10
['**Concatinating train and test as one dataframe**'],11,navneetkr123,ieee-fraud-play-with-count-lightgbm,navneetkr123_ieee-fraud-play-with-count-lightgbm_m11
['**Card feature**'],12,navneetkr123,ieee-fraud-play-with-count-lightgbm,navneetkr123_ieee-fraud-play-with-count-lightgbm_m12
['**Id Feaures**'],13,navneetkr123,ieee-fraud-play-with-count-lightgbm,navneetkr123_ieee-fraud-play-with-count-lightgbm_m13
['**Adding few more features**'],14,navneetkr123,ieee-fraud-play-with-count-lightgbm,navneetkr123_ieee-fraud-play-with-count-lightgbm_m14
['**This block of code count every features and drop original features**'],15,navneetkr123,ieee-fraud-play-with-count-lightgbm,navneetkr123_ieee-fraud-play-with-count-lightgbm_m15
['**Dropping below features as these seems to be repeating**'],16,navneetkr123,ieee-fraud-play-with-count-lightgbm,navneetkr123_ieee-fraud-play-with-count-lightgbm_m16
['**Log**'],17,navneetkr123,ieee-fraud-play-with-count-lightgbm,navneetkr123_ieee-fraud-play-with-count-lightgbm_m17
['> Below we can see that all I am left with is count'],18,navneetkr123,ieee-fraud-play-with-count-lightgbm,navneetkr123_ieee-fraud-play-with-count-lightgbm_m18
['**Again seperating data into train and test**'],19,navneetkr123,ieee-fraud-play-with-count-lightgbm,navneetkr123_ieee-fraud-play-with-count-lightgbm_m19
['**Train test and split**'],20,navneetkr123,ieee-fraud-play-with-count-lightgbm,navneetkr123_ieee-fraud-play-with-count-lightgbm_m20
['> **Lightgbm**'],21,navneetkr123,ieee-fraud-play-with-count-lightgbm,navneetkr123_ieee-fraud-play-with-count-lightgbm_m21
['**Submission**'],22,navneetkr123,ieee-fraud-play-with-count-lightgbm,navneetkr123_ieee-fraud-play-with-count-lightgbm_m22
"['> thank you all please let me know where did I go wrong.\n', '> Thankyou']",23,navneetkr123,ieee-fraud-play-with-count-lightgbm,navneetkr123_ieee-fraud-play-with-count-lightgbm_m23
"['# Tree Split Feature Selection - LGBM GPU EarlyStop  \n', '_By Nick Brooks_\n', '\n', 'V1 - 29/07/2019 - First Commit <br>\n', 'V2 - 03/08/2019 - PCA, Metric Convergence, Submission with 21 features <br>\n', 'V3 - 05/08/2019 - Fix GPU implementation <br>\n', '\n', '**Motivation:** <br>\n', 'How much of these features have actual signal? How does predictive power react when the number of features is decreased? What can PCA tell us about the  amount of variance in the features? Does reducing the number of features lead to smoother convergence?\n', '\n', '**Methodology:** <br>\n', '*Tree-Split Feature Selection* - Experiment with Iteratively removing feature using Gradient Boosting Ensemble split importance. LGBM is trained with a single validation fold and shuffles the train / validation set each iteration.\n', '\n', 'Very hyped for the GPU speed boost to Gradient Boosting Algorithmns, this will enable many fun experiments.\n', '\n', '**Other Links:** <Br>\n', '[ELI5 Permutation Importance](https://eli5.readthedocs.io/en/latest/blackbox/permutation_importance.html)\n', '    \n', '**My Other Fraud Notebooks:** <br>\n', 'https://www.kaggle.com/nicapotato/auc-performance-vs-training-size-gpu-catboost <br>\n', 'https://www.kaggle.com/nicapotato/fraud-shap-xgboost <br>']",0,nicapotato,tree-split-feature-selection-lgbm-gpu-earlystop,nicapotato_tree-split-feature-selection-lgbm-gpu-earlystop_m0
"['#### GPU Installation from [kirankunapuli](https://www.kaggle.com/kirankunapuli/)\n', 'Source: https://www.kaggle.com/kirankunapuli/ieee-fraud-lightgbm-with-gpu/comments']",1,nicapotato,tree-split-feature-selection-lgbm-gpu-earlystop,nicapotato_tree-split-feature-selection-lgbm-gpu-earlystop_m1
['#### Prepare Data'],2,nicapotato,tree-split-feature-selection-lgbm-gpu-earlystop,nicapotato_tree-split-feature-selection-lgbm-gpu-earlystop_m2
['### LGBM Model'],3,nicapotato,tree-split-feature-selection-lgbm-gpu-earlystop,nicapotato_tree-split-feature-selection-lgbm-gpu-earlystop_m3
"['#### Principle Component Analysis\n', 'How features explain the majority of the variance amongst these features?\n', '\n', ""[Jake VanderPlas's PythonDataScienceHandbook](https://jakevdp.github.io/PythonDataScienceHandbook/05.09-principal-component-analysis.html)""]",4,nicapotato,tree-split-feature-selection-lgbm-gpu-earlystop,nicapotato_tree-split-feature-selection-lgbm-gpu-earlystop_m4
"['## Iterative Split Feature Importance, Feature Selection with Early Stopping']",5,nicapotato,tree-split-feature-selection-lgbm-gpu-earlystop,nicapotato_tree-split-feature-selection-lgbm-gpu-earlystop_m5
['#### Run Submission set on Final Features:'],6,nicapotato,tree-split-feature-selection-lgbm-gpu-earlystop,nicapotato_tree-split-feature-selection-lgbm-gpu-earlystop_m6
['#### Submit'],7,nicapotato,tree-split-feature-selection-lgbm-gpu-earlystop,nicapotato_tree-split-feature-selection-lgbm-gpu-earlystop_m7
"['**Reflection:** <br>\n', 'Ideally I wanted to use feature permutation to do this iterative feature selection, but it is too computationally expensive (even on CPU kernel)']",8,nicapotato,tree-split-feature-selection-lgbm-gpu-earlystop,nicapotato_tree-split-feature-selection-lgbm-gpu-earlystop_m8
"['In this kernel I will do my EDA on the dataset, make some visualizations, try to find any insights and create some new features.\n', '\n', 'Join me, it promises to be a thrilling adventure.\n', '\n', 'Some tricks being used:\n', '* [card1 count encoding](#1)\n', '* [Covariate Shift](#2)\n', '* [features interaction](#3)\n', '* [data relaxation](#4)\n', '\n', 'New engineered features:\n', '* [Number of NaNs](#5)\n', ""* [TransactionAmt and it's decimal part](#6)""]",0,nroman,eda-for-cis-fraud-detection,nroman_eda-for-cis-fraud-detection_m0
['Loading all datasets using multiprocessing. This speads up a process a bit.'],1,nroman,eda-for-cis-fraud-detection,nroman_eda-for-cis-fraud-detection_m1
"['# Transaction DT\n', ""According to the official description 'TransactionDT feature is a timedelta from a given reference datetime (not an actual timestamp).' I see people in some kernels assume that a start date is a 1 of December 2017, but to be honest the exact start date is not that important. \n"", '\n', 'So lets transform TransactionDT into a datetime.']",2,nroman,eda-for-cis-fraud-detection,nroman_eda-for-cis-fraud-detection_m2
['And now combining both mean of isFraud by day and number of training examples by day into a single plot.'],3,nroman,eda-for-cis-fraud-detection,nroman_eda-for-cis-fraud-detection_m3
"['<a id=""1""></a>\n', '# card1\n', 'I have decided to start from one of the most important features of this dataset according to LightGBM feature_importance. And **card1** is one of those features.\n', '\n', ""What I did is I've created a separate dataset with only this feature in it and also I added one more feature to this new dataset, which is an original feature's frequency (count) encoding. Why I did this? Well, you can reference [Santander Customer Transaction Prediction](https://www.kaggle.com/c/santander-customer-transaction-prediction) competition, where this kind of encoding really boosted a score up. \n"", '\n', ""I'll make some visualizations (shoutout to [Chris Deotte](https://www.kaggle.com/cdeotte)) to show you why that works and might work in this case as well.""]",4,nroman,eda-for-cis-fraud-detection,nroman_eda-for-cis-fraud-detection_m4
"[""So if we train a simple decision tree, using this two features we have an AUC slightly higher that 0.5. Let's see why by plotting this tree as a graph""]",5,nroman,eda-for-cis-fraud-detection,nroman_eda-for-cis-fraud-detection_m5
['The first split is by the values less than or equal to 10881.5 (black line) and the second one is 8750.0 (red line) and a tree does not use a count feature at all.'],6,nroman,eda-for-cis-fraud-detection,nroman_eda-for-cis-fraud-detection_m6
['But lets take a little step back and train a boosting model on only one original feature card1'],7,nroman,eda-for-cis-fraud-detection,nroman_eda-for-cis-fraud-detection_m7
"['This is a heatmap with a probability of isFraud=1 for every unique value in the **card1** feature.\n', '\n', 'This picture reminds me an opening from a Total Recall movie. ']",8,nroman,eda-for-cis-fraud-detection,nroman_eda-for-cis-fraud-detection_m8
['Now lets add a second feature - count encoded **card1** values.'],9,nroman,eda-for-cis-fraud-detection,nroman_eda-for-cis-fraud-detection_m9
"['Holdout score has significantly increased. Lets create another heatmap and see why. \n', '\n', ""There are some darker spots in some intersections of the variable **card1** values and it's count encoded values. This is the reason of the holdout score improvement.\n"", '\n', '*The image is pre-rendered since rendering takes some significant amount of time*']",10,nroman,eda-for-cis-fraud-detection,nroman_eda-for-cis-fraud-detection_m10
['![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F1696976%2F7153f1242daa586d6849c83242c3fe40%2F35267aee89a7552caf082b6bb0039aa5-full.png?generation=1564585074348507&alt=media)'],11,nroman,eda-for-cis-fraud-detection,nroman_eda-for-cis-fraud-detection_m11
"['Plotting this variable gives us such information as:\n', '* distribution in train and test set is almost equal.\n', '* distribution between target values differs, which make this feature so valuable\n', ""* this feature doesn't have any NaNs""]",12,nroman,eda-for-cis-fraud-detection,nroman_eda-for-cis-fraud-detection_m12
"['<a id=""2""></a>\n', 'Lets check a Covariate Shift of the feature. This means that we will try to distinguish whether a values correspond to a training set or to a testing set.']",13,nroman,eda-for-cis-fraud-detection,nroman_eda-for-cis-fraud-detection_m13
"['ROC AUC score is close to 0.5, this means that this feature almost does not have any shift between train and test and is definitely worth keeping it.']",14,nroman,eda-for-cis-fraud-detection,nroman_eda-for-cis-fraud-detection_m14
['# ProductCD'],15,nroman,eda-for-cis-fraud-detection,nroman_eda-for-cis-fraud-detection_m15
['# card2'],16,nroman,eda-for-cis-fraud-detection,nroman_eda-for-cis-fraud-detection_m16
"[""Making a count feature for card2 to perform the same experiment as with card1. First the heatmap for all possible interactions of card2 feature and it's count.""]",17,nroman,eda-for-cis-fraud-detection,nroman_eda-for-cis-fraud-detection_m17
"['And a scatter plot with a ""decision boundary"" of the model. White \'X\' marks represents a test set examples.']",18,nroman,eda-for-cis-fraud-detection,nroman_eda-for-cis-fraud-detection_m18
['# card3'],19,nroman,eda-for-cis-fraud-detection,nroman_eda-for-cis-fraud-detection_m19
['# card4'],20,nroman,eda-for-cis-fraud-detection,nroman_eda-for-cis-fraud-detection_m20
['# card5'],21,nroman,eda-for-cis-fraud-detection,nroman_eda-for-cis-fraud-detection_m21
['# card6'],22,nroman,eda-for-cis-fraud-detection,nroman_eda-for-cis-fraud-detection_m22
['# addr1 '],23,nroman,eda-for-cis-fraud-detection,nroman_eda-for-cis-fraud-detection_m23
"['Another feature with a relatively high importance is **addr1**. According to the name of the feature we can assume that it contains some kind of users address, but in an encoded way. Also this time a feature have some missing values. We are going to fill them with 0.']",24,nroman,eda-for-cis-fraud-detection,nroman_eda-for-cis-fraud-detection_m24
['Again training a gradient boosting model with only one feature.'],25,nroman,eda-for-cis-fraud-detection,nroman_eda-for-cis-fraud-detection_m25
['Predictions heatmap.'],26,nroman,eda-for-cis-fraud-detection,nroman_eda-for-cis-fraud-detection_m26
['So far we are doing exactly the same thing that we have been doing for the previous variable.'],27,nroman,eda-for-cis-fraud-detection,nroman_eda-for-cis-fraud-detection_m27
"[""Distribution is the same, amount of NaN's is the same. Some difference in target value distribution. \n"", '\n', 'Next checking Covariate Shift for addr1.']",28,nroman,eda-for-cis-fraud-detection,nroman_eda-for-cis-fraud-detection_m28
"['ROC AUC score is close to 0.5\n', '\n', 'This feature also does not have any shift between train and test set.']",29,nroman,eda-for-cis-fraud-detection,nroman_eda-for-cis-fraud-detection_m29
"['<a id=""3""></a>\n', '# card1 to addr1 interaction\n', '\n', 'Next I am going to create a new feature out of this two features interaction and train on the result.']",30,nroman,eda-for-cis-fraud-detection,nroman_eda-for-cis-fraud-detection_m30
"['First training a model only using this two features, without their interaction.']",31,nroman,eda-for-cis-fraud-detection,nroman_eda-for-cis-fraud-detection_m31
['And now WITH interaction'],32,nroman,eda-for-cis-fraud-detection,nroman_eda-for-cis-fraud-detection_m32
['Predictions heatmap of the two features interaction.'],33,nroman,eda-for-cis-fraud-detection,nroman_eda-for-cis-fraud-detection_m33
"['Finally adding count features, so all in all we have 5 features']",34,nroman,eda-for-cis-fraud-detection,nroman_eda-for-cis-fraud-detection_m34
"['<a id=""5""></a>\n', ""# New feature: number of NaN's\n"", ""We have plenty of NaN's in this dataset and they can have a significant effect so why don't we use them?\n"", ""I am adding a new column to the dateset, which will contain a number of NaN for each row. So if a row (a single training example) contain, say, 10 NaNs, a new feature's value for this row will be 10.""]",35,nroman,eda-for-cis-fraud-detection,nroman_eda-for-cis-fraud-detection_m35
"['We can see that this feature might be useful, but also keep in mind that covatiate shift is almost 0.7, which tells us that the distribution between train and test set has some difference.']",36,nroman,eda-for-cis-fraud-detection,nroman_eda-for-cis-fraud-detection_m36
"['<a id=""6""></a>\n', ""# TransactionAmt and it's decimal part\n"", '\n', ""First let's take a look at TransactionAmt feature and them I will create a new one - it's decimal part, which is a very popular way of creating a new features.""]",37,nroman,eda-for-cis-fraud-detection,nroman_eda-for-cis-fraud-detection_m37
['Moving average for TransactionAmt over time.'],38,nroman,eda-for-cis-fraud-detection,nroman_eda-for-cis-fraud-detection_m38
['A relationship between mean of TransactionAmt by day and a mean of isFraud by day.'],39,nroman,eda-for-cis-fraud-detection,nroman_eda-for-cis-fraud-detection_m39
['Decimal part of transaction amount.'],40,nroman,eda-for-cis-fraud-detection,nroman_eda-for-cis-fraud-detection_m40
['A relationship between mean of TransactionAmt_decimal by day and a mean of isFraud by day.'],41,nroman,eda-for-cis-fraud-detection,nroman_eda-for-cis-fraud-detection_m41
"[""Lenght of the decimal part of transaction amount. What does it mean? Well, if lenght is 1 or 2 signs it is totaly understandable - it might be cents. But what is wrong with a decimal part's lenght being 3 and more sings? Maybe it is due to a currency convertion?""]",42,nroman,eda-for-cis-fraud-detection,nroman_eda-for-cis-fraud-detection_m42
['Covariate shift for all 3 features.'],43,nroman,eda-for-cis-fraud-detection,nroman_eda-for-cis-fraud-detection_m43
['# V1'],44,nroman,eda-for-cis-fraud-detection,nroman_eda-for-cis-fraud-detection_m44
['# V2'],45,nroman,eda-for-cis-fraud-detection,nroman_eda-for-cis-fraud-detection_m45
['# V3'],46,nroman,eda-for-cis-fraud-detection,nroman_eda-for-cis-fraud-detection_m46
['# V4'],47,nroman,eda-for-cis-fraud-detection,nroman_eda-for-cis-fraud-detection_m47
['# V5'],48,nroman,eda-for-cis-fraud-detection,nroman_eda-for-cis-fraud-detection_m48
['# V6'],49,nroman,eda-for-cis-fraud-detection,nroman_eda-for-cis-fraud-detection_m49
['# V7'],50,nroman,eda-for-cis-fraud-detection,nroman_eda-for-cis-fraud-detection_m50
['# V258'],51,nroman,eda-for-cis-fraud-detection,nroman_eda-for-cis-fraud-detection_m51
"['<a id=""4""></a>\n', ""This is where I want to introduce a little trick to you, called data relaxation. So what is it? In order to understand it take a look at the plot above. See the distibution difference between train and test set at a certain point? Gradient boosting algorithm doesn't know what to do with a data it has never seen so it will not approximate it well. And what we do by relaxing data is we are removing all the values from the train set that appears in it 3 times more often than in a test set and vice versa, also cleaning all the data that appears in train and test set only couple of times.\n"", '\n', '## V258 after data relaxation']",52,nroman,eda-for-cis-fraud-detection,nroman_eda-for-cis-fraud-detection_m52
['# V294'],53,nroman,eda-for-cis-fraud-detection,nroman_eda-for-cis-fraud-detection_m53
['## V294 after data relaxation'],54,nroman,eda-for-cis-fraud-detection,nroman_eda-for-cis-fraud-detection_m54
['# C1'],55,nroman,eda-for-cis-fraud-detection,nroman_eda-for-cis-fraud-detection_m55
['## C1 after data relaxation'],56,nroman,eda-for-cis-fraud-detection,nroman_eda-for-cis-fraud-detection_m56
['# C2'],57,nroman,eda-for-cis-fraud-detection,nroman_eda-for-cis-fraud-detection_m57
['## C2 after data relaxation.'],58,nroman,eda-for-cis-fraud-detection,nroman_eda-for-cis-fraud-detection_m58
['# C3'],59,nroman,eda-for-cis-fraud-detection,nroman_eda-for-cis-fraud-detection_m59
['## C3 after data relaxation'],60,nroman,eda-for-cis-fraud-detection,nroman_eda-for-cis-fraud-detection_m60
['# C4'],61,nroman,eda-for-cis-fraud-detection,nroman_eda-for-cis-fraud-detection_m61
['## C4 after data relaxation'],62,nroman,eda-for-cis-fraud-detection,nroman_eda-for-cis-fraud-detection_m62
['# C5'],63,nroman,eda-for-cis-fraud-detection,nroman_eda-for-cis-fraud-detection_m63
['## C5 after data relaxation'],64,nroman,eda-for-cis-fraud-detection,nroman_eda-for-cis-fraud-detection_m64
['# C6'],65,nroman,eda-for-cis-fraud-detection,nroman_eda-for-cis-fraud-detection_m65
['## C6 after data relaxation'],66,nroman,eda-for-cis-fraud-detection,nroman_eda-for-cis-fraud-detection_m66
['# C7'],67,nroman,eda-for-cis-fraud-detection,nroman_eda-for-cis-fraud-detection_m67
['## C7 after data relaxation'],68,nroman,eda-for-cis-fraud-detection,nroman_eda-for-cis-fraud-detection_m68
['# C8'],69,nroman,eda-for-cis-fraud-detection,nroman_eda-for-cis-fraud-detection_m69
['## C8 after data relaxation'],70,nroman,eda-for-cis-fraud-detection,nroman_eda-for-cis-fraud-detection_m70
['# C9'],71,nroman,eda-for-cis-fraud-detection,nroman_eda-for-cis-fraud-detection_m71
['## C9 after data relaxation'],72,nroman,eda-for-cis-fraud-detection,nroman_eda-for-cis-fraud-detection_m72
['# C10'],73,nroman,eda-for-cis-fraud-detection,nroman_eda-for-cis-fraud-detection_m73
['## C10 after data relaxation'],74,nroman,eda-for-cis-fraud-detection,nroman_eda-for-cis-fraud-detection_m74
['# C11'],75,nroman,eda-for-cis-fraud-detection,nroman_eda-for-cis-fraud-detection_m75
['## C11 after data relaxation'],76,nroman,eda-for-cis-fraud-detection,nroman_eda-for-cis-fraud-detection_m76
['# C12'],77,nroman,eda-for-cis-fraud-detection,nroman_eda-for-cis-fraud-detection_m77
['# C12 after data relaxation'],78,nroman,eda-for-cis-fraud-detection,nroman_eda-for-cis-fraud-detection_m78
['# C13'],79,nroman,eda-for-cis-fraud-detection,nroman_eda-for-cis-fraud-detection_m79
['# C13 after data relaxation'],80,nroman,eda-for-cis-fraud-detection,nroman_eda-for-cis-fraud-detection_m80
['# C14'],81,nroman,eda-for-cis-fraud-detection,nroman_eda-for-cis-fraud-detection_m81
['## C14 after data relaxation'],82,nroman,eda-for-cis-fraud-detection,nroman_eda-for-cis-fraud-detection_m82
['# D1'],83,nroman,eda-for-cis-fraud-detection,nroman_eda-for-cis-fraud-detection_m83
['# D2'],84,nroman,eda-for-cis-fraud-detection,nroman_eda-for-cis-fraud-detection_m84
['# D3'],85,nroman,eda-for-cis-fraud-detection,nroman_eda-for-cis-fraud-detection_m85
['# D4'],86,nroman,eda-for-cis-fraud-detection,nroman_eda-for-cis-fraud-detection_m86
['# D5'],87,nroman,eda-for-cis-fraud-detection,nroman_eda-for-cis-fraud-detection_m87
['# D6'],88,nroman,eda-for-cis-fraud-detection,nroman_eda-for-cis-fraud-detection_m88
['# D7'],89,nroman,eda-for-cis-fraud-detection,nroman_eda-for-cis-fraud-detection_m89
['# D8'],90,nroman,eda-for-cis-fraud-detection,nroman_eda-for-cis-fraud-detection_m90
['# D9'],91,nroman,eda-for-cis-fraud-detection,nroman_eda-for-cis-fraud-detection_m91
['# D10'],92,nroman,eda-for-cis-fraud-detection,nroman_eda-for-cis-fraud-detection_m92
['# D11'],93,nroman,eda-for-cis-fraud-detection,nroman_eda-for-cis-fraud-detection_m93
['# D12'],94,nroman,eda-for-cis-fraud-detection,nroman_eda-for-cis-fraud-detection_m94
['# D13'],95,nroman,eda-for-cis-fraud-detection,nroman_eda-for-cis-fraud-detection_m95
['# D14'],96,nroman,eda-for-cis-fraud-detection,nroman_eda-for-cis-fraud-detection_m96
['# D15'],97,nroman,eda-for-cis-fraud-detection,nroman_eda-for-cis-fraud-detection_m97
['# id_01'],98,nroman,eda-for-cis-fraud-detection,nroman_eda-for-cis-fraud-detection_m98
['# id_02'],99,nroman,eda-for-cis-fraud-detection,nroman_eda-for-cis-fraud-detection_m99
['# id_03'],100,nroman,eda-for-cis-fraud-detection,nroman_eda-for-cis-fraud-detection_m100
['# id_04'],101,nroman,eda-for-cis-fraud-detection,nroman_eda-for-cis-fraud-detection_m101
['# id_05'],102,nroman,eda-for-cis-fraud-detection,nroman_eda-for-cis-fraud-detection_m102
['# id_06'],103,nroman,eda-for-cis-fraud-detection,nroman_eda-for-cis-fraud-detection_m103
['# id_07'],104,nroman,eda-for-cis-fraud-detection,nroman_eda-for-cis-fraud-detection_m104
['# id_08'],105,nroman,eda-for-cis-fraud-detection,nroman_eda-for-cis-fraud-detection_m105
['# id_09'],106,nroman,eda-for-cis-fraud-detection,nroman_eda-for-cis-fraud-detection_m106
['# id_10'],107,nroman,eda-for-cis-fraud-detection,nroman_eda-for-cis-fraud-detection_m107
['# id_11'],108,nroman,eda-for-cis-fraud-detection,nroman_eda-for-cis-fraud-detection_m108
['# id_12'],109,nroman,eda-for-cis-fraud-detection,nroman_eda-for-cis-fraud-detection_m109
['# id_13'],110,nroman,eda-for-cis-fraud-detection,nroman_eda-for-cis-fraud-detection_m110
['# id_14'],111,nroman,eda-for-cis-fraud-detection,nroman_eda-for-cis-fraud-detection_m111
['# id_15'],112,nroman,eda-for-cis-fraud-detection,nroman_eda-for-cis-fraud-detection_m112
['# id_16'],113,nroman,eda-for-cis-fraud-detection,nroman_eda-for-cis-fraud-detection_m113
['# id_17'],114,nroman,eda-for-cis-fraud-detection,nroman_eda-for-cis-fraud-detection_m114
['# id_31'],115,nroman,eda-for-cis-fraud-detection,nroman_eda-for-cis-fraud-detection_m115
['## id_31 after data relaxation'],116,nroman,eda-for-cis-fraud-detection,nroman_eda-for-cis-fraud-detection_m116
"['We can eleminate some useless features already at the begining. Such as:\n', '* Features with only 1 unique value\n', '* Features with more than 90% missing values\n', '* Features with the top value appears more than 90% of the time']",0,nroman,recursive-feature-elimination,nroman_recursive-feature-elimination_m0
['And here we go. This would take a while.'],1,nroman,recursive-feature-elimination,nroman_recursive-feature-elimination_m1
['Printing out all features with rank 1'],2,nroman,recursive-feature-elimination,nroman_recursive-feature-elimination_m2
"['# Hot to track you Data Science experiments with neptune.ml\n', '![](https://neptune.ml/wp-content/uploads/2018/08/Company-Header-Neptune.ml_-2-e1560327936998.png)\n', '\n', ""*Important disclaimer: I am not an owner nor a developer of the presented service. I only use it for my own projects (including Kaggle competitions) because I've found it very useful.*\n"", '\n', '* Have you ever faced a situation when some of your experiments have finished, you got the results, but you completely forgot what exactly you have changed?\n', '* Maybe you want to run several of experiments to check multiple ideas and see the results in a convenient form?\n', '* Or you want to collaborate with your teammates more effectively?\n', '\n', ""If your answer is 'yes' to any of those questions then you will like a [neptune](https://neptune.ml/) project.\n"", '\n', 'In this kernel I will show you how to set it up and use in your kaggle competitions.']",0,nroman,tracking-experiments-with-neptune,nroman_tracking-experiments-with-neptune_m0
"['1. [Registering and installing neptune client](#1)\n', '2. [Setting up a project](#2)\n', '3. [Running an experiment](#3)\n', '4. [Track parameters](#4)\n', '5. [Track images](#5)\n', '6. [Track artifacts](#6)\n', '7. [Monitoring resources](#7)\n', '8. [Conclusion](#8)']",1,nroman,tracking-experiments-with-neptune,nroman_tracking-experiments-with-neptune_m1
"['<a id=""1""></a>\n', '## Registering and installing neptune client\n', '\n', ""I think there is no need to describe how to register in neptune, just go to the project page and click 'Sign up'. Google, Facebook and Github SSO works as well.\n"", '\n', ""Next step is to install a neptune client. In order to do this enable an Internet access in your kernel's settings.\n"", '![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F1696976%2Ffc475a6556225863484d04e9ed1baecb%2Fneptune_kernel_1.png?generation=1564558569759774&alt=media)\n', '\n', 'Then run *pip install neptune-client*']",2,nroman,tracking-experiments-with-neptune,nroman_tracking-experiments-with-neptune_m2
"['<a id=""2""></a>\n', '## Setting up a project\n', 'Neptune has a good documentation let alone an interface is intuitive and easy to use. But I still will guide you with your first experiment.\n', '\n', ""First of all we need to create a project for our experiments. Let's do it.\n"", '\n', ""Go to 'Projects' in the upper-left corner and then click a 'New project' button. You will see a modal window with some fields. Let's fill them and proceed.\n"", '\n', '![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F1696976%2Fbd37ce0d4cda1c6908f808941b59f358%2Fneptune_kernel_2.png?generation=1564558568867977&alt=media)']",3,nroman,tracking-experiments-with-neptune,nroman_tracking-experiments-with-neptune_m3
"[""Let's get familiar with an interface. \n"", '\n', '![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F1696976%2F7fbbbc2d193fcb0a30ea2438b5fc90c6%2Fneptune_kernel_3.png?generation=1564558568764085&alt=media)\n', '\n', 'We have 4 tabs at the top:\n', '* Wiki\n', '* Notebooks\n', '* Experiments\n', '* Settings\n', '\n', 'Wiki is a README and comments for you project\n', '\n', 'Notebooks contains all of the notebooks you are tracking. If you are using a Jupyter notebook (which I bet you are) then you can install a jupyter extension called *neptune-notebooks* and integrate it. After that by simply clicking on one button you will save a notebook checkpoint to your project and then you can keep working without a fear to remove some cell or rewrite a code in it. You will always have a backup. The outputs of the cells are being saved as well.\n', '\n', '![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F1696976%2Ff7d9882ddc9c732be20bcd438d9677d9%2Fneptune_kernel_4.png?generation=1564564644071941&alt=media)\n', '\n', 'Unfortunately kaggle kernels does not support such integration but you can upvote my [feature request](https://www.kaggle.com/product-feedback/101200#583902) for it.']",4,nroman,tracking-experiments-with-neptune,nroman_tracking-experiments-with-neptune_m4
"['<a id=""3""></a>\n', '## Running an experiment\n', '\n', 'It is time to create and run our first experiment. To track you experiment first you need to initialize it using neptune.init() method with your token.\n', '\n', ""You can obtain a token by clicking your user icon in the upper-right corner and selecting 'Get API Token'\n"", '\n', '![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F1696976%2F03662a1b9f79b4da78120f1f390313c8%2Fneptune_kernel_5.png?generation=1564558561773321&alt=media)\n', '\n', 'Next I will create a small dataset with a synthetic data using sklearn.datasets.make_classification and run our first experiment.']",5,nroman,tracking-experiments-with-neptune,nroman_tracking-experiments-with-neptune_m5
"[""As an output we have an experiment ID and a link to it. Let's see how the experiment results look in neptune interface. \n"", '\n', 'Here is a result on the dashboard:\n', '\n', '![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F1696976%2Fc3c9eb7e9965ee7317ca0fcd350c770e%2Fneptune_kernel_6.png?generation=1564559613195430&alt=media)\n', '\n', ""We called our metric 'AUC', but this is not a default column in neptune, so in order to see it we need to add it the dasboard.\n"", '\n', '![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F1696976%2F9324a9242a9789c6d9fbbd92a9f3d06f%2Fneptune_kernel_7.png?generation=1564559774143612&alt=media)\n']",6,nroman,tracking-experiments-with-neptune,nroman_tracking-experiments-with-neptune_m6
"['<a id=""4""></a>\n', '## Track parameters\n', '\n', 'Another useful feature is an ability to save the parameters of the model for your experiment. Here is how you can do it.']",7,nroman,tracking-experiments-with-neptune,nroman_tracking-experiments-with-neptune_m7
"[""You can now find this parameters in the experiment's page if you follow the link above.\n"", '\n', '![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F1696976%2F9ee8902a6a74a207a3d475c205414f4a%2Fneptune_kernel_8.png?generation=1564561096062206&alt=media)\n', '\n', 'As for dashboard - you should select parameters you want to be displayed. You can then sort and filter your experiments by any of them.']",8,nroman,tracking-experiments-with-neptune,nroman_tracking-experiments-with-neptune_m8
"['<a id=""5""></a>\n', '## Track images\n', '\n', ""Another thing you can log is an images. If you are more visual person this might be really helpful feature for you. Let's, for example, train another model and...""]",9,nroman,tracking-experiments-with-neptune,nroman_tracking-experiments-with-neptune_m9
"[""An image is now available in 'Logs' section of the experiment's page.\n"", '\n', '![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F1696976%2F545fec8350e6c060d49eac18b0d33ddb%2Fneptune_kernel_9.png?generation=1564562627500744&alt=media)']",10,nroman,tracking-experiments-with-neptune,nroman_tracking-experiments-with-neptune_m10
"['<a id=""6""></a>\n', '## Track artifacts\n', '\n', ""You can also send some artifacts. For example lets dump the model to pkl and send it to neptune. It is going to be stored in 'Artifacts' section.\n"", '\n', 'Also we can use neptune experiment in more pythonic way.']",11,nroman,tracking-experiments-with-neptune,nroman_tracking-experiments-with-neptune_m11
"['<a id=""7""></a>\n', '## Monitoring resources\n', '\n', ""Neptun client has another cool feature - by default it tracks all the resources usage metrics, such as CPU, Memory and GPU usage. They can be found in the 'Monitoring' section of the experiment's page. \n"", '\n', 'For example you can see how your model utilizes a GPU during the experiment.\n', '\n', '![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F1696976%2F0946f5f0d667c85296a0cddfe23497e8%2Fneptune_kernel_10.png?generation=1564563636462867&alt=media)']",12,nroman,tracking-experiments-with-neptune,nroman_tracking-experiments-with-neptune_m12
"['<a id=""8""></a>\n', '## Conclusion\n', '\n', ""I have described only basic features and functionality of neptune and if you like it then don't hesitate to explore this servis on our own. Trust me there is much more cool stuff to see.""]",13,nroman,tracking-experiments-with-neptune,nroman_tracking-experiments-with-neptune_m13
"[""# Detailed exploration of IEEE-CIS 'Fraud Detection' dataframe\n"", 'Here I examine and plot all features/variables from the training dataset, adding notes for all plots for later development\n', '\n', ""**This kernel is a bit long, so I'm continuining here with missing values analysis:**  \n"", 'https://www.kaggle.com/pabloinsente/ieee-missing-nan-values-analysis-and-imputation\n', '\n', '## Description variables/features:\n', '(https://www.kaggle.com/c/ieee-fraud-detection/discussion/101203#latest-583068) \n', '\n', '### Transaction Table:\n', '- TransactionDT: timedelta from a given reference datetime (not an actual timestamp)\n', '- TransactionAMT: transaction payment amount in USD\n', '- ProductCD: product code, the product for each transaction\n', '- card1 - card6: payment card information, such as card type, card category, issue bank, country, etc.\n', '- addr: address\n', '- dist: distance\n', '- P_ and (R__) emaildomain: purchaser and recipient email domain\n', '- C1-C14: counting, such as how many addresses are found to be associated with the payment card, etc. The actual meaning is masked.\n', '- D1-D15: timedelta, such as days between previous transaction, etc.\n', '- M1-M9: match, such as names on card and address, etc.\n', '- Vxxx: Vesta engineered rich features, including ranking, counting, and other entity relations.\n', '\n', '\n', '### Identity Table:\n', '- Variables in this table are identity information – network connection information (IP, ISP, Proxy, etc) and digital signature (UA/browser/os/version, etc) associated with transactions. \n', ""- They're collected by Vesta’s fraud protection system and digital security partners.\n"", '- (The field names are masked and pairwise dictionary will not be provided for privacy protection and contract agreement)']",0,pabloinsente,ieee-fraud-detailed-exploration-plots-all-var,pabloinsente_ieee-fraud-detailed-exploration-plots-all-var_m0
['# I. Import data'],1,pabloinsente,ieee-fraud-detailed-exploration-plots-all-var,pabloinsente_ieee-fraud-detailed-exploration-plots-all-var_m1
['# II. Explore data: describe single variables'],2,pabloinsente,ieee-fraud-detailed-exploration-plots-all-var,pabloinsente_ieee-fraud-detailed-exploration-plots-all-var_m2
['## Plot univariate distributions'],3,pabloinsente,ieee-fraud-detailed-exploration-plots-all-var,pabloinsente_ieee-fraud-detailed-exploration-plots-all-var_m3
"['### Categorical variables according to dataset documentation\n', '** Categorical Features - Transaction:**  \n', '- ProductCD\n', '- card1 - card6\n', '- addr1, addr2\n', '- P_emaildomain\n', '- R_emaildomain\n', '- M1 - M9\n', '\n', '** Categorical Features - Identity:**\n', '- DeviceType\n', '- DeviceInfo\n', '- id_12 - id_38']",4,pabloinsente,ieee-fraud-detailed-exploration-plots-all-var,pabloinsente_ieee-fraud-detailed-exploration-plots-all-var_m4
['## Plot categorical variables'],5,pabloinsente,ieee-fraud-detailed-exploration-plots-all-var,pabloinsente_ieee-fraud-detailed-exploration-plots-all-var_m5
"['**Plot I:  target, ProductCD, Devicetype, DeviceInfo**\n']",6,pabloinsente,ieee-fraud-detailed-exploration-plots-all-var,pabloinsente_ieee-fraud-detailed-exploration-plots-all-var_m6
"['**Plot I notes:**\n', '- Very unbalance target (isFraud)\n', '- Very unbalance product type purchase\n', '- Most purchases are made on desktop devices']",7,pabloinsente,ieee-fraud-detailed-exploration-plots-all-var,pabloinsente_ieee-fraud-detailed-exploration-plots-all-var_m7
['**Plot II: DeviceInfo**'],8,pabloinsente,ieee-fraud-detailed-exploration-plots-all-var,pabloinsente_ieee-fraud-detailed-exploration-plots-all-var_m8
"['**Plot II notes:**\n', '\n', 'The top devices are:\n', '1. Windows\n', '2. iOS\n', '3. Trident\n', '4. MacOS']",9,pabloinsente,ieee-fraud-detailed-exploration-plots-all-var,pabloinsente_ieee-fraud-detailed-exploration-plots-all-var_m9
"['**Plot III: cards 1,2,3, and 5**']",10,pabloinsente,ieee-fraud-detailed-exploration-plots-all-var,pabloinsente_ieee-fraud-detailed-exploration-plots-all-var_m10
"['**Plot III notes:**  \n', '- The bulk of the transactions are on card1 and2  \n', '- Not sure about identity of card3 and card5\n', '- They may be dollars amount per transaction, or some sort of identifier ']",11,pabloinsente,ieee-fraud-detailed-exploration-plots-all-var,pabloinsente_ieee-fraud-detailed-exploration-plots-all-var_m11
"['**Plot IV notes:**  \n', '- Card4 refers to visa brand; most transactions are on Visa and Mastercard \n', '- Card5 refers to type of card; most transactions are debit ']",12,pabloinsente,ieee-fraud-detailed-exploration-plots-all-var,pabloinsente_ieee-fraud-detailed-exploration-plots-all-var_m12
['**Plot V: addr1**'],13,pabloinsente,ieee-fraud-detailed-exploration-plots-all-var,pabloinsente_ieee-fraud-detailed-exploration-plots-all-var_m13
['**Plot VI: addr2**'],14,pabloinsente,ieee-fraud-detailed-exploration-plots-all-var,pabloinsente_ieee-fraud-detailed-exploration-plots-all-var_m14
"['**Plot V-VI notes:**\n', '- Transactions on addr1 are more evenly distributed\n', '- Transactions on addr2 has 1 big outlier ']",15,pabloinsente,ieee-fraud-detailed-exploration-plots-all-var,pabloinsente_ieee-fraud-detailed-exploration-plots-all-var_m15
['**Plot VII: emaildomains**'],16,pabloinsente,ieee-fraud-detailed-exploration-plots-all-var,pabloinsente_ieee-fraud-detailed-exploration-plots-all-var_m16
"['**Plot VII notes:**\n', '- As expected, gmail is at the top.\n', ""- There is one interesting 'anonymous.com' domain""]",17,pabloinsente,ieee-fraud-detailed-exploration-plots-all-var,pabloinsente_ieee-fraud-detailed-exploration-plots-all-var_m17
"['**Plot VIII notes:**\n', '- Identity of M1-M9 is still unclear\n', '- Basically boolean variables; M4 seems to be different']",18,pabloinsente,ieee-fraud-detailed-exploration-plots-all-var,pabloinsente_ieee-fraud-detailed-exploration-plots-all-var_m18
"['**Notes id12-id38**\n', '- There is a mix of data types\n', '- Mostly NaN values\n', '- id30 is OS again\n', '- id31 is browser\n']",19,pabloinsente,ieee-fraud-detailed-exploration-plots-all-var,pabloinsente_ieee-fraud-detailed-exploration-plots-all-var_m19
['**Plot IX: id_30**'],20,pabloinsente,ieee-fraud-detailed-exploration-plots-all-var,pabloinsente_ieee-fraud-detailed-exploration-plots-all-var_m20
['**Plot X: id_31**'],21,pabloinsente,ieee-fraud-detailed-exploration-plots-all-var,pabloinsente_ieee-fraud-detailed-exploration-plots-all-var_m21
"['**Plot X notes:**\n', '- Most transactions are done with Windows 7 and 10, and iOS\n', '- Most transactions are done with chrome and safari']",22,pabloinsente,ieee-fraud-detailed-exploration-plots-all-var,pabloinsente_ieee-fraud-detailed-exploration-plots-all-var_m22
['**Plot XI: ProductCD**'],23,pabloinsente,ieee-fraud-detailed-exploration-plots-all-var,pabloinsente_ieee-fraud-detailed-exploration-plots-all-var_m23
['## Plot continuous variables '],24,pabloinsente,ieee-fraud-detailed-exploration-plots-all-var,pabloinsente_ieee-fraud-detailed-exploration-plots-all-var_m24
"['**Plot XII notes:**\n', '- TransactionDT is evenly distributed, unclear identity\n', '- Transaction amount follows a log distribution, with a few large outliers']",25,pabloinsente,ieee-fraud-detailed-exploration-plots-all-var,pabloinsente_ieee-fraud-detailed-exploration-plots-all-var_m25
['**Plot XIII: C7 - C14**'],26,pabloinsente,ieee-fraud-detailed-exploration-plots-all-var,pabloinsente_ieee-fraud-detailed-exploration-plots-all-var_m26
"['**Plot XIII notes:**\n', '- All variables follow roughly a log distribution \n', '- Identity is unclear']",27,pabloinsente,ieee-fraud-detailed-exploration-plots-all-var,pabloinsente_ieee-fraud-detailed-exploration-plots-all-var_m27
['**Plot XIV: D1 - D15**'],28,pabloinsente,ieee-fraud-detailed-exploration-plots-all-var,pabloinsente_ieee-fraud-detailed-exploration-plots-all-var_m28
"['**Plot XIV notes:**\n', '- D11-D15 have negative values, which may say something about the identity of the feature\n', '- D9 has a different distribution, kinda binomial\n', '- The rest roughly a log distribution']",29,pabloinsente,ieee-fraud-detailed-exploration-plots-all-var,pabloinsente_ieee-fraud-detailed-exploration-plots-all-var_m29
['**Exploration V1 - V339**'],30,pabloinsente,ieee-fraud-detailed-exploration-plots-all-var,pabloinsente_ieee-fraud-detailed-exploration-plots-all-var_m30
"['**Notes:**\n', '- From V1-V305 & V322-V339 seems to be mostly 0 - 1 values, which may indicate that they are actually a categorical feature \n', '- From V306-V321 seems to be true continuous variables \n', '- More interesting insights may come from computing averages by target feature']",31,pabloinsente,ieee-fraud-detailed-exploration-plots-all-var,pabloinsente_ieee-fraud-detailed-exploration-plots-all-var_m31
['**Plot XV: id_01 - id_11**'],32,pabloinsente,ieee-fraud-detailed-exploration-plots-all-var,pabloinsente_ieee-fraud-detailed-exploration-plots-all-var_m32
['**Plot XV: id_01 - id_11 / SAME as LOG distributions**'],33,pabloinsente,ieee-fraud-detailed-exploration-plots-all-var,pabloinsente_ieee-fraud-detailed-exploration-plots-all-var_m33
"['**Plot XV notes:**\n', '- id_02 may be dollar amounts, with log distribution\n', '- id_01 - id_10 have negative values, but it is unlikely to indicate debt given values\n', '- id_07 - id_08 are kinda normally distributed']",34,pabloinsente,ieee-fraud-detailed-exploration-plots-all-var,pabloinsente_ieee-fraud-detailed-exploration-plots-all-var_m34
['# III. Explore data: describe variables by target (Fraud/not Not Fraud)'],35,pabloinsente,ieee-fraud-detailed-exploration-plots-all-var,pabloinsente_ieee-fraud-detailed-exploration-plots-all-var_m35
['## Plot/Explore bivariate relationships'],36,pabloinsente,ieee-fraud-detailed-exploration-plots-all-var,pabloinsente_ieee-fraud-detailed-exploration-plots-all-var_m36
"['**Plot I:  target, ProductCD, Devicetype, DeviceInfo / Target**']",37,pabloinsente,ieee-fraud-detailed-exploration-plots-all-var,pabloinsente_ieee-fraud-detailed-exploration-plots-all-var_m37
['**Plot I as percentage**'],38,pabloinsente,ieee-fraud-detailed-exploration-plots-all-var,pabloinsente_ieee-fraud-detailed-exploration-plots-all-var_m38
"['**Notes:**\n', '- ProductCD: C and S types have the highest number AND proportion of Fraud Transactions\n', '- DeviceType: mobile has the highest number AND proportion of Fraud Transactions (not by much though)']",39,pabloinsente,ieee-fraud-detailed-exploration-plots-all-var,pabloinsente_ieee-fraud-detailed-exploration-plots-all-var_m39
['**Plot II: Fraud transactions by OS**'],40,pabloinsente,ieee-fraud-detailed-exploration-plots-all-var,pabloinsente_ieee-fraud-detailed-exploration-plots-all-var_m40
"['**Notes**\n', '- Fraud transaction cases come mostly from Windows and iOS devices. This is predictable given the vast majority of all transactions come from those systems. Still, the problem is this feature will still send the signal to the model that Windos/iOS_Device transactions -> likely fraud relative to other systems\n', '- Trident OS drop 5 places (3th overall, 8th on Fraud transactions)']",41,pabloinsente,ieee-fraud-detailed-exploration-plots-all-var,pabloinsente_ieee-fraud-detailed-exploration-plots-all-var_m41
"['**Plot III: cards 1,2,3, and 5**']",42,pabloinsente,ieee-fraud-detailed-exploration-plots-all-var,pabloinsente_ieee-fraud-detailed-exploration-plots-all-var_m42
"['**Notes:**\n', '- card1, card2 and card3 show similar distribution patterns fraud/no_fraud\n', ""- card5 reverse proportion around value '225' ""]",43,pabloinsente,ieee-fraud-detailed-exploration-plots-all-var,pabloinsente_ieee-fraud-detailed-exploration-plots-all-var_m43
['**Plot IV: cards 4 and 6**'],44,pabloinsente,ieee-fraud-detailed-exploration-plots-all-var,pabloinsente_ieee-fraud-detailed-exploration-plots-all-var_m44
['**Plot IV as percentage**'],45,pabloinsente,ieee-fraud-detailed-exploration-plots-all-var,pabloinsente_ieee-fraud-detailed-exploration-plots-all-var_m45
"['**Notes:**\n', '- Visa has the higher NUMBER of Fraud, but such number is a minor proportion of all VISA transactions\n', '- Discover has very few Fraud transaction, yet as percentage of all Discover transactions is a bit higher\n', '- Most Fraud transactions are done with Debit, but there is a higher proportion of Fraud Transactions within Credit ']",46,pabloinsente,ieee-fraud-detailed-exploration-plots-all-var,pabloinsente_ieee-fraud-detailed-exploration-plots-all-var_m46
['**Plot V: addr1**'],47,pabloinsente,ieee-fraud-detailed-exploration-plots-all-var,pabloinsente_ieee-fraud-detailed-exploration-plots-all-var_m47
"['**Notes:**\n', '- Most fraud transactions come from addr1 204; most not fraud come from 299\n', '- First 5 addr1 are the same, but in different rank-order']",48,pabloinsente,ieee-fraud-detailed-exploration-plots-all-var,pabloinsente_ieee-fraud-detailed-exploration-plots-all-var_m48
['**Plot VI: addr2**'],49,pabloinsente,ieee-fraud-detailed-exploration-plots-all-var,pabloinsente_ieee-fraud-detailed-exploration-plots-all-var_m49
['**Plot VII: emaildomains by Fraud status**'],50,pabloinsente,ieee-fraud-detailed-exploration-plots-all-var,pabloinsente_ieee-fraud-detailed-exploration-plots-all-var_m50
['**Plot VII: emaildomains by Fraud status as percentage**'],51,pabloinsente,ieee-fraud-detailed-exploration-plots-all-var,pabloinsente_ieee-fraud-detailed-exploration-plots-all-var_m51
"['**Notes:**  \n', ""- **'Protonmail.com'**,  **'mail.com'**, **'outlook.es'**, and, **'net.zero'** have a high proportion of Fraud transaction, yet they account for an small total number of fraud transactions""]",52,pabloinsente,ieee-fraud-detailed-exploration-plots-all-var,pabloinsente_ieee-fraud-detailed-exploration-plots-all-var_m52
['**Plot VIII: M1 - M9 by Fraud status variables**'],53,pabloinsente,ieee-fraud-detailed-exploration-plots-all-var,pabloinsente_ieee-fraud-detailed-exploration-plots-all-var_m53
['**Plot VIII: M1 - M9 variables by Fraud status as percentage**'],54,pabloinsente,ieee-fraud-detailed-exploration-plots-all-var,pabloinsente_ieee-fraud-detailed-exploration-plots-all-var_m54
"['**Notes:**\n', '- As frequency is hard to catch meaningful differences between classes\n', ""- As percentage there are some interesting patterns on 'M4' where class M2 get the highest proportion of Fraud transactions, or M1 where 'F' doesn't get any Fraud cases""]",55,pabloinsente,ieee-fraud-detailed-exploration-plots-all-var,pabloinsente_ieee-fraud-detailed-exploration-plots-all-var_m55
['**Plot IX**'],56,pabloinsente,ieee-fraud-detailed-exploration-plots-all-var,pabloinsente_ieee-fraud-detailed-exploration-plots-all-var_m56
['**Plot IX as percentage**'],57,pabloinsente,ieee-fraud-detailed-exploration-plots-all-var,pabloinsente_ieee-fraud-detailed-exploration-plots-all-var_m57
"['**Notes**\n', '- **id_30**: Other and Android 5.1.1 have the highest proportion of Fraud, **BUT**  negligible frequency: Other have 6 cases and Android 5.1.1, 101 cases\n', ""- **id_31**: Lanix, Mozilla, comodo, and lanix have really high proportions of 'Fraud', *BUT*, negible frequency: Lanix/Ilium 1 fraud, Mozilla/Firefox 5 fraud cases, comodo 2, lanix 1""]",58,pabloinsente,ieee-fraud-detailed-exploration-plots-all-var,pabloinsente_ieee-fraud-detailed-exploration-plots-all-var_m58
['**Plot XI: ProductCD by Fraud status**'],59,pabloinsente,ieee-fraud-detailed-exploration-plots-all-var,pabloinsente_ieee-fraud-detailed-exploration-plots-all-var_m59
['**Plot XI: ProductCD by fraud status as percentage**'],60,pabloinsente,ieee-fraud-detailed-exploration-plots-all-var,pabloinsente_ieee-fraud-detailed-exploration-plots-all-var_m60
"['**Notes**  \n', '  \n', ""**ProductCD is 'Production code'**   \n"", ""- 'C': has both the highest number AND the highest proportion of Fraud transactions\n"", ""- 'W': has a similar frequency of Fraud transactions for a minor proportion of the W class""]",61,pabloinsente,ieee-fraud-detailed-exploration-plots-all-var,pabloinsente_ieee-fraud-detailed-exploration-plots-all-var_m61
"['**Plot XII: TransactionDT, TransactionAmt by Fraud status**']",62,pabloinsente,ieee-fraud-detailed-exploration-plots-all-var,pabloinsente_ieee-fraud-detailed-exploration-plots-all-var_m62
"['**Notes:**\n', ""- **TransactionDT (time delta from some reference time)**: Not-Fraud transactions tend to be more close to the 'Time zero reference' for the transactions; Fraud transactions tend to be a bit more evenly distributed. There is a pick around 0.55\n"", '- **TransanctionAmt (on dollars)**: Not-Fraud transactions are concentrated on the middle of the distribution, while Fraud transactions are a bit more concentrated on the tails (really small or really bit). This makes a lot of intuitive sense: micro-frauds and large-amounts-frauds are more likely. ']",63,pabloinsente,ieee-fraud-detailed-exploration-plots-all-var,pabloinsente_ieee-fraud-detailed-exploration-plots-all-var_m63
['**Plot XIII: C7 - C14 by Fraud status**'],64,pabloinsente,ieee-fraud-detailed-exploration-plots-all-var,pabloinsente_ieee-fraud-detailed-exploration-plots-all-var_m64
"['**Notes:**\n', ""- This is all supossed to be 'counting' data, yet, we get a bunch of negative values\n"", '- The main patter, is that not-fraud transactions have higher values, more tightly concentrated (high kurtosis), while fraud transactions are more evenly spread out (low kurtosis), which means more outliers']",65,pabloinsente,ieee-fraud-detailed-exploration-plots-all-var,pabloinsente_ieee-fraud-detailed-exploration-plots-all-var_m65
['**Plot XIV: D1 - D15 by Fraud status**'],66,pabloinsente,ieee-fraud-detailed-exploration-plots-all-var,pabloinsente_ieee-fraud-detailed-exploration-plots-all-var_m66
"['**Notes** \n', '- **Main insight**: Fraud transactions tend to be **more spread out over time**, while Not-Fraud transactions tend to be **more clustered around shorter time periods** (from the 0 time-point reference) ']",67,pabloinsente,ieee-fraud-detailed-exploration-plots-all-var,pabloinsente_ieee-fraud-detailed-exploration-plots-all-var_m67
['**Plot XV: id_01 - id_11 by fraud status**'],68,pabloinsente,ieee-fraud-detailed-exploration-plots-all-var,pabloinsente_ieee-fraud-detailed-exploration-plots-all-var_m68
['**Plot XV: id_01 - id_11 by Fraud status**'],69,pabloinsente,ieee-fraud-detailed-exploration-plots-all-var,pabloinsente_ieee-fraud-detailed-exploration-plots-all-var_m69
"['**Notes:**\n', '- In the cases where Fraud/Not-Fraud differ, the pattern is the same: **Fraud more clustered with a higher peak**, and **Not-Fraud more spread out with longer/heavier tails**']",70,pabloinsente,ieee-fraud-detailed-exploration-plots-all-var,pabloinsente_ieee-fraud-detailed-exploration-plots-all-var_m70
"[""**Explore 'V' Features**""]",71,pabloinsente,ieee-fraud-detailed-exploration-plots-all-var,pabloinsente_ieee-fraud-detailed-exploration-plots-all-var_m71
"['**Notes:**\n', '- There are so many features with no-identity info that it is hard to get a clear insight. It is clear though that there are A LOT features where Fraud transactions have higher means, which means that these variables are going to be variable for the model to learn to capture Fraud cases']",72,pabloinsente,ieee-fraud-detailed-exploration-plots-all-var,pabloinsente_ieee-fraud-detailed-exploration-plots-all-var_m72
"[""**This kernel is a bit long, so I'm continuining here with missing values analysis:**  \n"", 'https://www.kaggle.com/pabloinsente/ieee-missing-nan-values-analysis-and-imputation']",73,pabloinsente,ieee-fraud-detailed-exploration-plots-all-var,pabloinsente_ieee-fraud-detailed-exploration-plots-all-var_m73
"['# EDA, feature engineering, LightGBM baseline\n', '\n', '**Feature engineering:**\n', '* select ~280 features from 432 (-)\n', ""* count 'null' values per row (-)\n"", ""* fill 'null' values with constant (+)\n"", '* remove features that have more than 90% of same values (-)\n', ""* make 'os', 'browser', 'device' from 'id_30', 'id_31' (-)\n"", ""* transform 'TransactionAmt' (+)\n"", '* feature aggregates (-)\n', ""* 'card1', ... count (+)\n"", '* features interaction (+)\n', ""* make 'day', 'hour' features (+)\n"", ""* make 'vendor', 'suffix', 'us' from email features (-)\n"", '* remove timestamp (+)\n', '\n', '**Model:**\n', '* LightGBM\n', '* Optuna to get optimal parameters\n', '* 5 fold cross-validation']",0,pavelvpster,ieee-fraud-eda-lightgbm-baseline,pavelvpster_ieee-fraud-eda-lightgbm-baseline_m0
['> ## Load data'],1,pavelvpster,ieee-fraud-eda-lightgbm-baseline,pavelvpster_ieee-fraud-eda-lightgbm-baseline_m1
['## Keep selected features only'],2,pavelvpster,ieee-fraud-eda-lightgbm-baseline,pavelvpster_ieee-fraud-eda-lightgbm-baseline_m2
"[""## Count 'null' values per row""]",3,pavelvpster,ieee-fraud-eda-lightgbm-baseline,pavelvpster_ieee-fraud-eda-lightgbm-baseline_m3
"[""## Fill 'null' values with constant""]",4,pavelvpster,ieee-fraud-eda-lightgbm-baseline,pavelvpster_ieee-fraud-eda-lightgbm-baseline_m4
['## Remove features that have more than 90% of same values'],5,pavelvpster,ieee-fraud-eda-lightgbm-baseline,pavelvpster_ieee-fraud-eda-lightgbm-baseline_m5
"['## Transform id_30, id_31']",6,pavelvpster,ieee-fraud-eda-lightgbm-baseline,pavelvpster_ieee-fraud-eda-lightgbm-baseline_m6
"[""## Transform 'TransactionAmt'""]",7,pavelvpster,ieee-fraud-eda-lightgbm-baseline,pavelvpster_ieee-fraud-eda-lightgbm-baseline_m7
['## Feature aggregates'],8,pavelvpster,ieee-fraud-eda-lightgbm-baseline,pavelvpster_ieee-fraud-eda-lightgbm-baseline_m8
"[""## 'card1', ... count""]",9,pavelvpster,ieee-fraud-eda-lightgbm-baseline,pavelvpster_ieee-fraud-eda-lightgbm-baseline_m9
['## Features interaction'],10,pavelvpster,ieee-fraud-eda-lightgbm-baseline,pavelvpster_ieee-fraud-eda-lightgbm-baseline_m10
['## Make day and hour features'],11,pavelvpster,ieee-fraud-eda-lightgbm-baseline,pavelvpster_ieee-fraud-eda-lightgbm-baseline_m11
['## Transform emails'],12,pavelvpster,ieee-fraud-eda-lightgbm-baseline,pavelvpster_ieee-fraud-eda-lightgbm-baseline_m12
['## Remove timestamp'],13,pavelvpster,ieee-fraud-eda-lightgbm-baseline,pavelvpster_ieee-fraud-eda-lightgbm-baseline_m13
['## Encode categorial features'],14,pavelvpster,ieee-fraud-eda-lightgbm-baseline,pavelvpster_ieee-fraud-eda-lightgbm-baseline_m14
['## Free memory'],15,pavelvpster,ieee-fraud-eda-lightgbm-baseline,pavelvpster_ieee-fraud-eda-lightgbm-baseline_m15
['## Extract target variable'],16,pavelvpster,ieee-fraud-eda-lightgbm-baseline,pavelvpster_ieee-fraud-eda-lightgbm-baseline_m16
['> ## LightGBM'],17,pavelvpster,ieee-fraud-eda-lightgbm-baseline,pavelvpster_ieee-fraud-eda-lightgbm-baseline_m17
['## Submit predictions'],18,pavelvpster,ieee-fraud-eda-lightgbm-baseline,pavelvpster_ieee-fraud-eda-lightgbm-baseline_m18
['## Load and prepare data'],0,pavelvpster,ieee-fraud-features-interaction,pavelvpster_ieee-fraud-features-interaction_m0
['## LightGBM'],1,pavelvpster,ieee-fraud-features-interaction,pavelvpster_ieee-fraud-features-interaction_m1
['## Test features interaction'],2,pavelvpster,ieee-fraud-features-interaction,pavelvpster_ieee-fraud-features-interaction_m2
['## Select features'],3,pavelvpster,ieee-fraud-features-interaction,pavelvpster_ieee-fraud-features-interaction_m3
['## Test all new features together'],4,pavelvpster,ieee-fraud-features-interaction,pavelvpster_ieee-fraud-features-interaction_m4
['# Load data'],0,plasticgrammer,ieee-cis-fraud-detection-eda,plasticgrammer_ieee-cis-fraud-detection-eda_m0
['# Data analysis'],1,plasticgrammer,ieee-cis-fraud-detection-eda,plasticgrammer_ieee-cis-fraud-detection-eda_m1
"[""### what's target?""]",2,plasticgrammer,ieee-cis-fraud-detection-eda,plasticgrammer_ieee-cis-fraud-detection-eda_m2
['### isFraud'],3,plasticgrammer,ieee-cis-fraud-detection-eda,plasticgrammer_ieee-cis-fraud-detection-eda_m3
"['- fraud transaction rate by day, and week']",4,plasticgrammer,ieee-cis-fraud-detection-eda,plasticgrammer_ieee-cis-fraud-detection-eda_m4
"['- fraud transaction rate by weekday, hour, month-day, and year-month']",5,plasticgrammer,ieee-cis-fraud-detection-eda,plasticgrammer_ieee-cis-fraud-detection-eda_m5
['- fraud transaction rate by day'],6,plasticgrammer,ieee-cis-fraud-detection-eda,plasticgrammer_ieee-cis-fraud-detection-eda_m6
['- fraud transaction rate by weekday-hour'],7,plasticgrammer,ieee-cis-fraud-detection-eda,plasticgrammer_ieee-cis-fraud-detection-eda_m7
['- fraud rate by weekday '],8,plasticgrammer,ieee-cis-fraud-detection-eda,plasticgrammer_ieee-cis-fraud-detection-eda_m8
['- fraud rate by hour '],9,plasticgrammer,ieee-cis-fraud-detection-eda,plasticgrammer_ieee-cis-fraud-detection-eda_m9
['- fraud rate by weekday-hour'],10,plasticgrammer,ieee-cis-fraud-detection-eda,plasticgrammer_ieee-cis-fraud-detection-eda_m10
['- fraud rate by amount-bin'],11,plasticgrammer,ieee-cis-fraud-detection-eda,plasticgrammer_ieee-cis-fraud-detection-eda_m11
['### TransactionID'],12,plasticgrammer,ieee-cis-fraud-detection-eda,plasticgrammer_ieee-cis-fraud-detection-eda_m12
"['## Identity data\n', '\n', ""Variables in this table are identity information – network connection information (IP, ISP, Proxy, etc) and digital signature (UA/browser/os/version, etc) associated with transactions. They're collected by Vesta’s fraud protection system and digital security partners. (The field names are masked and pairwise dictionary will not be provided for privacy protection and contract agreement)\n"", '\n', 'Categorical Features:\n', '\n', '- DeviceType\n', '- DeviceInfo\n', '- id12 - id38']",13,plasticgrammer,ieee-cis-fraud-detection-eda,plasticgrammer_ieee-cis-fraud-detection-eda_m13
['### id_01 - id_11'],14,plasticgrammer,ieee-cis-fraud-detection-eda,plasticgrammer_ieee-cis-fraud-detection-eda_m14
['### id_12 - id_38'],15,plasticgrammer,ieee-cis-fraud-detection-eda,plasticgrammer_ieee-cis-fraud-detection-eda_m15
"['### DeviceType, DeviceInfo']",16,plasticgrammer,ieee-cis-fraud-detection-eda,plasticgrammer_ieee-cis-fraud-detection-eda_m16
"['## Transaction data\n', '\n', '- TransactionDT: timedelta from a given reference datetime (not an actual timestamp)\n', '- TransactionAMT: transaction payment amount in USD\n', '- ProductCD: product code, the product for each transaction\n', '- card1 - card6: payment card information, such as card type, card category, issue bank, country, etc.\n', '- addr: address\n', '- dist: distance\n', '- P_ and (R__) emaildomain: purchaser and recipient email domain\n', '- C1-C14: counting, such as how many addresses are found to be associated with the payment card, etc. The actual meaning is masked.\n', '- D1-D15: timedelta, such as days between previous transaction, etc.\n', '- M1-M9: match, such as names on card and address, etc.\n', '- Vxxx: Vesta engineered rich features, including ranking, counting, and other entity relations.']",17,plasticgrammer,ieee-cis-fraud-detection-eda,plasticgrammer_ieee-cis-fraud-detection-eda_m17
['### TransactionDT'],18,plasticgrammer,ieee-cis-fraud-detection-eda,plasticgrammer_ieee-cis-fraud-detection-eda_m18
['### TransactionAmt'],19,plasticgrammer,ieee-cis-fraud-detection-eda,plasticgrammer_ieee-cis-fraud-detection-eda_m19
['### ProductCD'],20,plasticgrammer,ieee-cis-fraud-detection-eda,plasticgrammer_ieee-cis-fraud-detection-eda_m20
['### card1 - card6'],21,plasticgrammer,ieee-cis-fraud-detection-eda,plasticgrammer_ieee-cis-fraud-detection-eda_m21
"['### addr1, addr2']",22,plasticgrammer,ieee-cis-fraud-detection-eda,plasticgrammer_ieee-cis-fraud-detection-eda_m22
"['### dist1, dist2']",23,plasticgrammer,ieee-cis-fraud-detection-eda,plasticgrammer_ieee-cis-fraud-detection-eda_m23
"['### P_emaildomain, R_emaildomain']",24,plasticgrammer,ieee-cis-fraud-detection-eda,plasticgrammer_ieee-cis-fraud-detection-eda_m24
['### C1 - C14'],25,plasticgrammer,ieee-cis-fraud-detection-eda,plasticgrammer_ieee-cis-fraud-detection-eda_m25
['- Cx & card'],26,plasticgrammer,ieee-cis-fraud-detection-eda,plasticgrammer_ieee-cis-fraud-detection-eda_m26
['### D1-D15'],27,plasticgrammer,ieee-cis-fraud-detection-eda,plasticgrammer_ieee-cis-fraud-detection-eda_m27
['- Dx & card'],28,plasticgrammer,ieee-cis-fraud-detection-eda,plasticgrammer_ieee-cis-fraud-detection-eda_m28
['### M1 - M9'],29,plasticgrammer,ieee-cis-fraud-detection-eda,plasticgrammer_ieee-cis-fraud-detection-eda_m29
['### Vxxx'],30,plasticgrammer,ieee-cis-fraud-detection-eda,plasticgrammer_ieee-cis-fraud-detection-eda_m30
['# Feature engineering'],31,plasticgrammer,ieee-cis-fraud-detection-eda,plasticgrammer_ieee-cis-fraud-detection-eda_m31
['# Predict'],32,plasticgrammer,ieee-cis-fraud-detection-eda,plasticgrammer_ieee-cis-fraud-detection-eda_m32
"['Forked from https://www.kaggle.com/tunguz/adversarial-ieee\n', '\n', 'My contribution  \n', '1) The data is time-split, so doing the validation without the date column makes sense.  \n', '2) Added feature-importance(gain)']",0,pocketsuteado,adversarial-ieee-without-date,pocketsuteado_adversarial-ieee-without-date_m0
"['Still a very high AUC.  \n', 'Perhaps another shaky competition?']",1,pocketsuteado,adversarial-ieee-without-date,pocketsuteado_adversarial-ieee-without-date_m1
"[""Let's take a look how similar train and test sets are.""]",0,raddar,adversarial-ieee,raddar_adversarial-ieee_m0
['> Understanding Null Values'],0,rajeshcv,features-based-on-nulls-and-lgbm-model,rajeshcv_features-based-on-nulls-and-lgbm-model_m0
"[""How missing values are handled in the data is a very important aspect of Machine learning problems. Some Data Scientists recoomend that if more than 70-75%  data in a feature/column are missing it's better to drop those features from the model. However if the entire data set has a high proportion of missing data then deleting individual features with higher proportion may work adversely in developing the model. The IEE-CIS Fraud Detection Competion data is one such case. As shown below 41%  of all the data entries in the Train_transaction file are null values.""]",1,rajeshcv,features-based-on-nulls-and-lgbm-model,rajeshcv_features-based-on-nulls-and-lgbm-model_m1
"[""Let's look at which columns have highest number of null values. It's interesting to observe ,from the plot of percentage of missing values against the Number of columns ,that many columns have exactly the same number of missing values. Columns with same percent of missing values can be grouped.""]",2,rajeshcv,features-based-on-nulls-and-lgbm-model,rajeshcv_features-based-on-nulls-and-lgbm-model_m2
"[""Shown below are  the column_groups with exactly the same number of missing values. It's interesting to note that these  each group is almost a continous series of column names.""]",3,rajeshcv,features-based-on-nulls-and-lgbm-model,rajeshcv_features-based-on-nulls-and-lgbm-model_m3
"[""Since all columns  within the  a column group have the same missing value percentage ,in a particular row of the dataframe all columns will have null values or all columns will have non null values. Let's do a value_count of data entries in all the rows for the the column group with 46 columns and missing value percentage  77.913435 to confirm this. ""]",4,rajeshcv,features-based-on-nulls-and-lgbm-model,rajeshcv_features-based-on-nulls-and-lgbm-model_m4
"['As expected in every row either none of the columns within a group have a  null value or all the columns within a group have null values.\n', '\n', 'Overall Fraud(isFraud=1) and non-Fraud(isFraud=0) plot is shown below.\n']",5,rajeshcv,features-based-on-nulls-and-lgbm-model,rajeshcv_features-based-on-nulls-and-lgbm-model_m5
['What would be interesting is to check within a particular group of columns whether there is any difference in Proportion of Fraud transactions between the rows with all values missing for the columns in that group and rows with values for columns in that group compared to the overall distribution. The graph below shows that distribution'],6,rajeshcv,features-based-on-nulls-and-lgbm-model,rajeshcv_features-based-on-nulls-and-lgbm-model_m6
"['It looks like incase of some column groups there appears to be a significant difference in the proportion of fraud cases between non null value rows and the null value rows.\n', '\n']",7,rajeshcv,features-based-on-nulls-and-lgbm-model,rajeshcv_features-based-on-nulls-and-lgbm-model_m7
['**Conclusion**'],8,rajeshcv,features-based-on-nulls-and-lgbm-model,rajeshcv_features-based-on-nulls-and-lgbm-model_m8
"['* The data set has multiple groups of related features/columns that possibly can be combined to create new features.\n', '* Presence or absence of data in these related feature/column groups may be an important feature in developing models.\n']",9,rajeshcv,features-based-on-nulls-and-lgbm-model,rajeshcv_features-based-on-nulls-and-lgbm-model_m9
['> Feature Engineering'],10,rajeshcv,features-based-on-nulls-and-lgbm-model,rajeshcv_features-based-on-nulls-and-lgbm-model_m10
['Let us create some new features based on the group of columns with  same percent of null values\n'],11,rajeshcv,features-based-on-nulls-and-lgbm-model,rajeshcv_features-based-on-nulls-and-lgbm-model_m11
['Among the C Columns some of the columns are Pseudo categorical in nature . Refer kernel https://www.kaggle.com/rajeshcv/exploring-c-columns  for details. A new feature is developed based on this. Based on transaction date transaction hour and weekday features are also created.'],12,rajeshcv,features-based-on-nulls-and-lgbm-model,rajeshcv_features-based-on-nulls-and-lgbm-model_m12
['> Label Encoding'],13,rajeshcv,features-based-on-nulls-and-lgbm-model,rajeshcv_features-based-on-nulls-and-lgbm-model_m13
"[""Let's encode the object columns to replace string values with numericals. Here the labe encoding id done without changing null values.\n"", '\n', 'A check is done to find if the values in the object type columns of train and test dataset are the same']",14,rajeshcv,features-based-on-nulls-and-lgbm-model,rajeshcv_features-based-on-nulls-and-lgbm-model_m14
"[""P_emaildomain has a value 'scranton.edu' in test dataset which is not in the train dataset. We will replace this with null value""]",15,rajeshcv,features-based-on-nulls-and-lgbm-model,rajeshcv_features-based-on-nulls-and-lgbm-model_m15
['> Model Building'],16,rajeshcv,features-based-on-nulls-and-lgbm-model,rajeshcv_features-based-on-nulls-and-lgbm-model_m16
['> Feature importance'],17,rajeshcv,features-based-on-nulls-and-lgbm-model,rajeshcv_features-based-on-nulls-and-lgbm-model_m17
['Feature importance by top 100 features is shown below'],18,rajeshcv,features-based-on-nulls-and-lgbm-model,rajeshcv_features-based-on-nulls-and-lgbm-model_m18
['> Results'],19,rajeshcv,features-based-on-nulls-and-lgbm-model,rajeshcv_features-based-on-nulls-and-lgbm-model_m19
"['Of the 394 columns in train_transaction file 339 columns start with V . In most of the models developed  Vcolumns individually have very low importance and gets removed during feature selection process.The kernel attempts to understand the similiarilties between the various V columns and the distribution of data in these columns inorder to understand various groupings that can be used to create useful features.\n', '\n', 'The first grouping is based on the percentage of missing values in the columns . The columns can be divided into 15 groups as below.']",0,rajeshcv,understanding-v-columns,rajeshcv_understanding-v-columns_m0
"[""Let's check whether similiar pattern exists in the test_transaction data.\n"", 'The same groups exist in test data also !!!.']",1,rajeshcv,understanding-v-columns,rajeshcv_understanding-v-columns_m1
"[""Let's explore the 15 groups of columns to understand how many unique values are there in each of the columns and how many values make 96.5% of the data in each of the columns. (96.5% is chosen as this is the percentage of transactions that is not Fraud.)""]",2,rajeshcv,understanding-v-columns,rajeshcv_understanding-v-columns_m2
"['Based on the  data distribution columns can be divided into 5 types.\n', '\n', '1. **Boolean** - columns  with only two unique values\n', '\n', '2. **Pseudo- Boolean**  - columns with  96.5% data covered by  maximum two unique values. Within this there are two types.\n', '        \n', '        Pseudo-Boolean-categorical - Columns with 15 or less unique values but 96.5% data covered by  maximum two unique values\n', '        Pseudo-Boolean-numerical - Columns with more than 15 unique values but 96.5% data covered by  maximum two unique values\n', '\n', '4. **Pseudo-Categorical**  - Columns with  96.5% data covered by  15 or less unique values\n', '\n', '5. **Numerical** - All Other columns\n', '\n']",3,rajeshcv,understanding-v-columns,rajeshcv_understanding-v-columns_m3
"['**Boolean Columns**\n', '\n']",4,rajeshcv,understanding-v-columns,rajeshcv_understanding-v-columns_m4
['In all these columns almost all values is 1. However except for V305 all values even though minimal which are not 1 are from fraud transactions.'],5,rajeshcv,understanding-v-columns,rajeshcv_understanding-v-columns_m5
['**Pseudo Booleans**'],6,rajeshcv,understanding-v-columns,rajeshcv_understanding-v-columns_m6
['221 Columns starting with V have 96.5% of their values covered by one or two values\n'],7,rajeshcv,understanding-v-columns,rajeshcv_understanding-v-columns_m7
"[""Of these 108 columns have only less than 15 uniques values . Let's look at data distribution in these columns.These are Pseudo Boolean categorical type columns""]",8,rajeshcv,understanding-v-columns,rajeshcv_understanding-v-columns_m8
"[""It's interesting to note that in many of the columns some of the unique values which fall in the 3.5% of column data the proportion of fraudulent transactions is in the range of 25% -50%""]",9,rajeshcv,understanding-v-columns,rajeshcv_understanding-v-columns_m9
"[""The remaining 113 have more than 15 unique values. Let's look at data distribution in these columns.These are Pseudo Boolean numerical type columns""]",10,rajeshcv,understanding-v-columns,rajeshcv_understanding-v-columns_m10
['The histograms of values less than 3.5% of the column data shows a higher proportion of fraud transactions.'],11,rajeshcv,understanding-v-columns,rajeshcv_understanding-v-columns_m11
['**Pseudo - Categorical**'],12,rajeshcv,understanding-v-columns,rajeshcv_understanding-v-columns_m12
['There are 44 Columns in this category'],13,rajeshcv,understanding-v-columns,rajeshcv_understanding-v-columns_m13
"[""Let's look at Data distribution in these columns""]",14,rajeshcv,understanding-v-columns,rajeshcv_understanding-v-columns_m14
['In some of these columns  a higher proportion of fraud cases are seen  for values which form less than 3.5% of the column data'],15,rajeshcv,understanding-v-columns,rajeshcv_understanding-v-columns_m15
['**Numerical**'],16,rajeshcv,understanding-v-columns,rajeshcv_understanding-v-columns_m16
"[""There are 67 columns in this category. Let's look at how data is distributed in these columns""]",17,rajeshcv,understanding-v-columns,rajeshcv_understanding-v-columns_m17
['In all these columns the more frequent values are in the lower range in both cases.'],18,rajeshcv,understanding-v-columns,rajeshcv_understanding-v-columns_m18
['**Conclusion**'],19,rajeshcv,understanding-v-columns,rajeshcv_understanding-v-columns_m19
['It looks like the Pseudo Boolean and Pseudo Categorical columns are important as in both tpes there is a higher proportion of fraud cases when the values fall with less than 3.5% of column data unique values'],20,rajeshcv,understanding-v-columns,rajeshcv_understanding-v-columns_m20
"['Kernel on Unique identifer based on C & D columns  https://www.kaggle.com/rajeshcv/curious-case-of-c-columns\n', '\n', 'Kernel on Null Values  https://www.kaggle.com/rajeshcv/tale-of-nulls']",21,rajeshcv,understanding-v-columns,rajeshcv_understanding-v-columns_m21
"['Of the 394 features/columns in the train_transaction data 15 columns begin in C .\n', 'The officaila explanation of these columns is.\n', '\n', '*C1-C14: counting, such as how many addresses are found to be associated with the payment card, etc. The actual meaning is masked.*\n', '\n', 'All C columns are of the numeric data type and summary is as below']",0,rajeshcv,unique-card-identifier-features-from-c-d-columns,rajeshcv_unique-card-identifier-features-from-c-d-columns_m0
"['The graph below shows number of unique values in each of the C Columns as blue bars.Orange bar shows the number of  unique values in 96.5% of the data in each of the columns. The difference between the two bars is a measure of how distributed the data is across the range of unique values in the column. \n', '\n', 'Red line is the percentage of missing values in  the columns.']",1,rajeshcv,unique-card-identifier-features-from-c-d-columns,rajeshcv_unique-card-identifier-features-from-c-d-columns_m1
"['**Interesting to  note that none of the C columns have missing values**.\n', '\n', 'Across the range of values in each of the column  few values make up 96.5% of data in each of the columns compared to total unique values.\n']",2,rajeshcv,unique-card-identifier-features-from-c-d-columns,rajeshcv_unique-card-identifier-features-from-c-d-columns_m2
"[""Let's also look at test_transaction data set to verify whether the distribution of values are similiar""]",3,rajeshcv,unique-card-identifier-features-from-c-d-columns,rajeshcv_unique-card-identifier-features-from-c-d-columns_m3
['Interestingly C13 has more than 90% missing values and C14 has 20% missing values.  These features are potentially candidates to be dropped while building models.'],4,rajeshcv,unique-card-identifier-features-from-c-d-columns,rajeshcv_unique-card-identifier-features-from-c-d-columns_m4
['**Unique Card Identifier**'],5,rajeshcv,unique-card-identifier-features-from-c-d-columns,rajeshcv_unique-card-identifier-features-from-c-d-columns_m5
"['It looks a combination of Card =features card1-card 6 and C columns will help us to identify the unique payment cards (cards with unique 15 or 16 digit card numbers).\n', '\n', ""After exploring various combiantion of features a combination  of ['card1' ,'card2','card3','card4','card5','card6', 'addr1','C1','C2' ,'C3', 'C4','C5','C6','C7','C8','C9','C10','C11']  shows some interesting patterns""]",6,rajeshcv,unique-card-identifier-features-from-c-d-columns,rajeshcv_unique-card-identifier-features-from-c-d-columns_m6
['Taking as sample details of transactions with card1 =9885'],7,rajeshcv,unique-card-identifier-features-from-c-d-columns,rajeshcv_unique-card-identifier-features-from-c-d-columns_m7
"['From the above data for card  with  card1 = 9885 and number of transactions during the 6 month period = 64 it can easily be seen that D3 column is the difference in number of days for  succesive transaction values of D1 and D2.\n', '\n', 'D5 values are almost same as D3 . But where D4 is null D5 is also null which means D5 is the difference in days of successive D4 values\n', '\n', ""D1,D2,D4,D11,D15 are days from some card events as their values increase with time. D4 ,D11 and D15 appears to be the same value but D11 has some nulls. D10 values don't follow the time series and need further analysis.\n"", '\n', 'The increase in the values of D1,D2 ,D4 ,D11 and D15 corresponds with increase in days of the TransactionDT values.\n', '\n', 'The difference between the value of D1 between the first and last transaction in this group is 174 days corresponding t a 6 month period.\n', '\n', 'from the above this set of values appear to be  transactions of a specific card during the 6 month period.\n', '\n', ""**Hence it's safe to assume that the combination of the features 'card1','card2','card3','card4','card5','card6', 'addr1', 'C1','C2','C3','C4','C5','C6','C7','C8','C9','C10'and 'C11' can  uniquely identify the credit card.**""]",8,rajeshcv,unique-card-identifier-features-from-c-d-columns,rajeshcv_unique-card-identifier-features-from-c-d-columns_m8
['There seems to be a few cases where the D3 value is noth edifference in days between succesive row. On case is where the d1 value is 236 and D2 value is 185 . D3 is 4 for this row . But the previous row D1& D2s value are 209 and 158. Hence instead of 4 the D3 value should have been 27 . So it looks like a row of values with D1=232 and D2=181 has missed out'],9,rajeshcv,unique-card-identifier-features-from-c-d-columns,rajeshcv_unique-card-identifier-features-from-c-d-columns_m9
"['The above row is the missing one and since it had null values for card2-card 6 it was not included in our grouping.\n', '\n', 'This throws up the possibility of imputing missing values in card2- card6 based on the feature groupings we identified as payment card identifier.\n']",10,rajeshcv,unique-card-identifier-features-from-c-d-columns,rajeshcv_unique-card-identifier-features-from-c-d-columns_m10
['**Exploring D Columns further**'],11,rajeshcv,unique-card-identifier-features-from-c-d-columns,rajeshcv_unique-card-identifier-features-from-c-d-columns_m11
['The plot below shows the data distribution in D columns '],12,rajeshcv,unique-card-identifier-features-from-c-d-columns,rajeshcv_unique-card-identifier-features-from-c-d-columns_m12
['From the above there is a fair uniform distribution of values in Dcolumns and these are truly numerical columns. histogram of data other than 0 and nulls are shown in the plot below.'],13,rajeshcv,unique-card-identifier-features-from-c-d-columns,rajeshcv_unique-card-identifier-features-from-c-d-columns_m13
"['From the above histogram D6,D7,D8,D12,D13,D14 seems to be number of days from some card event date .In any case thes columns have close to 90% null values. D9 is the day fraction of D8. This is confirmed by sample of data below.']",14,rajeshcv,unique-card-identifier-features-from-c-d-columns,rajeshcv_unique-card-identifier-features-from-c-d-columns_m14
['**Exploring C Columns further**'],15,rajeshcv,unique-card-identifier-features-from-c-d-columns,rajeshcv_unique-card-identifier-features-from-c-d-columns_m15
['The table below shows for each column the values that make 96.5% of data in each column(values_0.99) and values that make the remaining 1% data in (values_0.01)'],16,rajeshcv,unique-card-identifier-features-from-c-d-columns,rajeshcv_unique-card-identifier-features-from-c-d-columns_m16
"['###### For Columns C3,C4 ,C7 ,C8 ,C10 & C12 15 or less values make 96.5% of the column values . These are like categorical values in a sense. \n', '\n', 'The graph below shows a count plot of these categorical values which account for 96.5% values on the left and a histogram of remaining 3.5%  numeric values on the right.']",17,rajeshcv,unique-card-identifier-features-from-c-d-columns,rajeshcv_unique-card-identifier-features-from-c-d-columns_m17
['**Curiously almost 30% of values that fall in the 1% of data in these columns except C3 are part of Fraud transactions**'],18,rajeshcv,unique-card-identifier-features-from-c-d-columns,rajeshcv_unique-card-identifier-features-from-c-d-columns_m18
"[""###### Let's now examine the remaining columns which are numeric in nature.\n"", '\n', 'The graph below shows a histogram of  96.5% of column values on the left and a histogram of remaining 3.5%  values on the right.\n', '\n', '\n', '\n']",19,rajeshcv,unique-card-identifier-features-from-c-d-columns,rajeshcv_unique-card-identifier-features-from-c-d-columns_m19
"[""There seem to be no major differnce in proportion of fraud transactions between the two types. However it's interesting to note that the majority of values in these columns are in a narrow range of 0-20  in most of the cases except C13, even though maximum value in these most of columns exceed 3000.""]",20,rajeshcv,unique-card-identifier-features-from-c-d-columns,rajeshcv_unique-card-identifier-features-from-c-d-columns_m20
"['* Thanks to\n', '>* https://www.kaggle.com/jazivxt/safe-box\n', '>* https://www.kaggle.com/vaishvik25/ensemble\n', '>* https://www.kaggle.com/jolasa/stacking-higher-every-time-0-9427']",0,rajwardhanshinde,stackers-blend-top-4,rajwardhanshinde_stackers-blend-top-4_m0
['# Vote Early And Vote Often'],1,rajwardhanshinde,stackers-blend-top-4,rajwardhanshinde_stackers-blend-top-4_m1
['# Blending'],2,rajwardhanshinde,stackers-blend-top-4,rajwardhanshinde_stackers-blend-top-4_m2
['## [UpVote if this was helpful](http://)'],3,rajwardhanshinde,stackers-blend-top-4,rajwardhanshinde_stackers-blend-top-4_m3
"['>* Based on https://www.kaggle.com/lpachuong/statstack\n', '>* Thanks to <br>\n', 'https://www.kaggle.com/jazivxt/safe-box<br>\n', 'https://www.kaggle.com/artgor/eda-and-models<br>\n', 'https://www.kaggle.com/stocks/under-sample-with-multiple-runs<br>\n', 'https://www.kaggle.com/artkulak/ieee-fraud-simple-baseline-0-9383-lb']",0,rajwardhanshinde,stacking,rajwardhanshinde_stacking_m0
['## Upvote if this was helpful'],1,rajwardhanshinde,stacking,rajwardhanshinde_stacking_m1
['# Mean Stacking'],2,rajwardhanshinde,stacking,rajwardhanshinde_stacking_m2
['# Median Stacking'],3,rajwardhanshinde,stacking,rajwardhanshinde_stacking_m3
"['# Pushout + Median Stacking\n', '>* Pushout strategy is bit aggresive']",4,rajwardhanshinde,stacking,rajwardhanshinde_stacking_m4
"['# MinMax + Mean Stacking\n', '>* MinMax seems more gentle and it outperforms the previous one']",5,rajwardhanshinde,stacking,rajwardhanshinde_stacking_m5
['# MinMax + Median Stacking'],6,rajwardhanshinde,stacking,rajwardhanshinde_stacking_m6
"['# MinMax + BestBase Stacking\n', '>* loading submission with best score']",7,rajwardhanshinde,stacking,rajwardhanshinde_stacking_m7
['## Median stacking gives the best LB score'],8,rajwardhanshinde,stacking,rajwardhanshinde_stacking_m8
"['# IEEE Fraud Detection\n', '## Catboost Baseline Model\n', '![](https://miro.medium.com/max/1200/1*2p1GIUUcRSzyyJjSj4x7Iw.jpeg)']",0,robikscube,baseline-catboost-feat-importance,robikscube_baseline-catboost-feat-importance_m0
['## Read input data'],1,robikscube,baseline-catboost-feat-importance,robikscube_baseline-catboost-feat-importance_m1
"['# Creat X, y\n', '- Create Catboost Data Pools']",2,robikscube,baseline-catboost-feat-importance,robikscube_baseline-catboost-feat-importance_m2
"['## Train Model\n', '(I will update do KFold CV Later)']",3,robikscube,baseline-catboost-feat-importance,robikscube_baseline-catboost-feat-importance_m3
['# Predict'],4,robikscube,baseline-catboost-feat-importance,robikscube_baseline-catboost-feat-importance_m4
"['# IEEE Fraud Detection Competition\n', '![fraud](https://abcountrywide.com.au/wp-content/uploads/2018/04/fraud.jpg)\n', '\n', 'In this kernel I do some basic exploritory data analysis on the IEEE Fraud Detection dataset. Please upvote if you find this kernel helpful. I will continue to update as I find more discoveries. I suggest you also read the complete competition overview and data description found in the competition page.\n', '\n', 'I purposefully show all of my code. The intention is to not only show the results, but also have clear code that shows how similar analysis can be done on any dataset.\n', '\n', 'From the [competition overview](https://www.kaggle.com/c/ieee-fraud-detection/overview):\n', '\n', ""*In this competition, you’ll benchmark machine learning models on a challenging large-scale dataset. The data comes from Vesta's real-world e-commerce transactions and contains a wide range of features from device type to product features. You also have the opportunity to create new features to improve your results.*\n"", '   \n', '*If successful, you’ll improve the efficacy of fraudulent transaction alerts for millions of people around the world, helping hundreds of thousands of businesses reduce their fraud loss and increase their revenue. And of course, you will save party people just like you the hassle of false positives.*']",0,robikscube,ieee-fraud-detection-first-look-and-eda,robikscube_ieee-fraud-detection-first-look-and-eda_m0
"['# Data\n', '\n', 'In the competition you are predicting the probability that an online transaction is fraudulent, as denoted by the binary target isFraud.\n', '\n', 'The data is broken into two files identity and transaction, which are joined by TransactionID. Not all transactions have corresponding identity information.']",1,robikscube,ieee-fraud-detection-first-look-and-eda,robikscube_ieee-fraud-detection-first-look-and-eda_m1
"['- 24.4% of TransactionIDs in **train** (144233 / 590540) have an associated train_identity.\n', '- 28.0% of TransactionIDs in **test** (144233 / 590540) have an associated train_identity.']",2,robikscube,ieee-fraud-detection-first-look-and-eda,robikscube_ieee-fraud-detection-first-look-and-eda_m2
"['# Train vs Test are Time Series Split\n', '\n', 'The `TransactionDT` feature is a timedelta from a given reference datetime (not an actual timestamp). One early discovery about the data is that the train and test appear to be split by time. There is a slight gap inbetween, but otherwise the training set is from an earlier period of time and test is from a later period of time. This will impact which cross validation techniques should be used.\n', '\n', 'We will look into this more when reviewing differences in distribution of features between train and test.']",3,robikscube,ieee-fraud-detection-first-look-and-eda,robikscube_ieee-fraud-detection-first-look-and-eda_m3
"['# Distribution of Target in Training Set\n', '- 3.5% of transacations are fraud']",4,robikscube,ieee-fraud-detection-first-look-and-eda,robikscube_ieee-fraud-detection-first-look-and-eda_m4
"['## TransactionAmt\n', ""The ammount of transaction. I've taken a log transform in some of these plots to better show the distribution- otherwise the few, very large transactions skew the distribution. Because of the log transfrom, any values between 0 and 1 will appear to be negative.""]",5,robikscube,ieee-fraud-detection-first-look-and-eda,robikscube_ieee-fraud-detection-first-look-and-eda_m5
['- Fraudulent charges appear to have a higher average transaction ammount '],6,robikscube,ieee-fraud-detection-first-look-and-eda,robikscube_ieee-fraud-detection-first-look-and-eda_m6
"['## ProductCD\n', ""- For now we don't know exactly what these values represent.\n"", '- `W` has the most number of observations, `C` the least.\n', '- ProductCD `C` has the most fraud with >11%\n', '- ProductCD `W` has the least with ~2%']",7,robikscube,ieee-fraud-detection-first-look-and-eda,robikscube_ieee-fraud-detection-first-look-and-eda_m7
"['# Categorical Features - Transaction\n', 'We are told in the data description that the following transaction columns are categorical:\n', '- ProductCD\n', '- emaildomain\n', '- card1 - card6\n', '- addr1, addr2\n', '- P_emaildomain\n', '- R_emaildomain\n', '- M1 - M9']",8,robikscube,ieee-fraud-detection-first-look-and-eda,robikscube_ieee-fraud-detection-first-look-and-eda_m8
"['# card1 - card6\n', '- We are told these are all categorical, even though some appear numeric.']",9,robikscube,ieee-fraud-detection-first-look-and-eda,robikscube_ieee-fraud-detection-first-look-and-eda_m9
"['# addr1 & addr2\n', 'The data description states that these are categorical even though they look numeric. Could they be the address value?']",10,robikscube,ieee-fraud-detection-first-look-and-eda,robikscube_ieee-fraud-detection-first-look-and-eda_m10
"['# dist1 & dist2\n', ""Plotting with logx to better show the distribution. Possibly this could be the distance of the transaction vs. the card owner's home/work address. This is just a guess.""]",11,robikscube,ieee-fraud-detection-first-look-and-eda,robikscube_ieee-fraud-detection-first-look-and-eda_m11
"['# C1 - C14\n', 'Because we are provided many numerical columns, we can create a pairplot to plot feature interactions. I know these plots can be hard to read, but it is helpful for gaining intution about potential feature interactions and if certain features have more variance than others.']",12,robikscube,ieee-fraud-detection-first-look-and-eda,robikscube_ieee-fraud-detection-first-look-and-eda_m12
"['# D1-D9\n', 'Similarly for features D1-D9. In these plots we can see some linear and non-linear interactions between features. We may want to create additional features using these interactions if we think it would help our model better find relationship between fraud and non-fraud observations.']",13,robikscube,ieee-fraud-detection-first-look-and-eda,robikscube_ieee-fraud-detection-first-look-and-eda_m13
"['# M1-M9\n', '- Values are `T` `F` or `NaN`\n', '- Column `M4` appears to be different with values like `M2` and `M0`']",14,robikscube,ieee-fraud-detection-first-look-and-eda,robikscube_ieee-fraud-detection-first-look-and-eda_m14
"['# V1 - V339\n', 'Lots of 1s 0s and Nans, some larger values']",15,robikscube,ieee-fraud-detection-first-look-and-eda,robikscube_ieee-fraud-detection-first-look-and-eda_m15
"['# Identity Data\n', 'Next we will explore the identity data. These are provided for some, but not all `TransactionID`s. It contains information about the identity of the customer.\n', '- Categorical Features\n', '- `DeviceType`\n', '- `DeviceInfo`\n', '- `id_12` - `id_38`']",16,robikscube,ieee-fraud-detection-first-look-and-eda,robikscube_ieee-fraud-detection-first-look-and-eda_m16
['## DeviceType'],17,robikscube,ieee-fraud-detection-first-look-and-eda,robikscube_ieee-fraud-detection-first-look-and-eda_m17
['## Identity info as a function of time'],18,robikscube,ieee-fraud-detection-first-look-and-eda,robikscube_ieee-fraud-detection-first-look-and-eda_m18
"['## Compare Numeric Features in Train and Test\n', 'Similar to above but for the transaction data, specific examples that look interesting.']",19,robikscube,ieee-fraud-detection-first-look-and-eda,robikscube_ieee-fraud-detection-first-look-and-eda_m19
['# Lets track the Public LB Standings!'],0,robikscube,the-race-to-detect-fraud-final-lb,robikscube_the-race-to-detect-fraud-final-lb_m0
"['# Gold Medal Paths\n', '(Missing the team ""Young for you"" because their name must have changed from the data I have.\n', '\n', 'Updating to incluse the paths of the gold medal 🥇 teams on the private leaderboard!']",1,robikscube,the-race-to-detect-fraud-final-lb,robikscube_the-race-to-detect-fraud-final-lb_m1
['# Top Public LB Scores over time'],2,robikscube,the-race-to-detect-fraud-final-lb,robikscube_the-race-to-detect-fraud-final-lb_m2
"['# All competitors LB Position over Time\n', '(Kernel keeps breaking so I subsample to 1000 random teams)']",3,robikscube,the-race-to-detect-fraud-final-lb,robikscube_the-race-to-detect-fraud-final-lb_m3
['# Number of Teams by Date'],4,robikscube,the-race-to-detect-fraud-final-lb,robikscube_the-race-to-detect-fraud-final-lb_m4
['# Top LB Scores'],5,robikscube,the-race-to-detect-fraud-final-lb,robikscube_the-race-to-detect-fraud-final-lb_m5
['# Count of LB Submissions with Improved Score'],6,robikscube,the-race-to-detect-fraud-final-lb,robikscube_the-race-to-detect-fraud-final-lb_m6
['# Distribution of Scores over time'],7,robikscube,the-race-to-detect-fraud-final-lb,robikscube_the-race-to-detect-fraud-final-lb_m7
['# IEEE-CIS Fraud Detection'],0,rodgomrod,very-simple-nn-in-pytorch-with-sparsetensor,rodgomrod_very-simple-nn-in-pytorch-with-sparsetensor_m0
['In this notebook we show a very simple example of PyTorch on sparse data in the competition IEEE-CIS Fraud Detection.'],1,rodgomrod,very-simple-nn-in-pytorch-with-sparsetensor,rodgomrod_very-simple-nn-in-pytorch-with-sparsetensor_m1
['## Libraries'],2,rodgomrod,very-simple-nn-in-pytorch-with-sparsetensor,rodgomrod_very-simple-nn-in-pytorch-with-sparsetensor_m2
['Select cuda device and set a seed.'],3,rodgomrod,very-simple-nn-in-pytorch-with-sparsetensor,rodgomrod_very-simple-nn-in-pytorch-with-sparsetensor_m3
"['## Data\n', 'We load the data on npz compressed format (~20 MB train and test sets). For now, data is omitted.\n', '\n', 'Brief data description:\n', '\n', '- dummies on categorical data.\n', '- split in 512 bins continuous data and dummies.\n', '\n', 'Finally, we have about 13k columns.']",4,rodgomrod,very-simple-nn-in-pytorch-with-sparsetensor,rodgomrod_very-simple-nn-in-pytorch-with-sparsetensor_m4
['A function to transform data from csr_matrix format to PyTorch sparse tensor.'],5,rodgomrod,very-simple-nn-in-pytorch-with-sparsetensor,rodgomrod_very-simple-nn-in-pytorch-with-sparsetensor_m5
"['Split data in 80% train, 20% test, sorted by TransactionDT']",6,rodgomrod,very-simple-nn-in-pytorch-with-sparsetensor,rodgomrod_very-simple-nn-in-pytorch-with-sparsetensor_m6
['Transform valid data to sparse tensor. Fit data will be transformed into each mini-batch.'],7,rodgomrod,very-simple-nn-in-pytorch-with-sparsetensor,rodgomrod_very-simple-nn-in-pytorch-with-sparsetensor_m7
['## Neuronal Network'],8,rodgomrod,very-simple-nn-in-pytorch-with-sparsetensor,rodgomrod_very-simple-nn-in-pytorch-with-sparsetensor_m8
"['### Train\n', '\n', 'Train the NN with Time Series 80/20 split to determine which is the optimal epoch.']",9,rodgomrod,very-simple-nn-in-pytorch-with-sparsetensor,rodgomrod_very-simple-nn-in-pytorch-with-sparsetensor_m9
['Re-fitting with all train data and the best number of epochs.'],10,rodgomrod,very-simple-nn-in-pytorch-with-sparsetensor,rodgomrod_very-simple-nn-in-pytorch-with-sparsetensor_m10
"['### Test\n', '\n', 'Test set predictions in batches to avoid CUDA Memory Errors.']",11,rodgomrod,very-simple-nn-in-pytorch-with-sparsetensor,rodgomrod_very-simple-nn-in-pytorch-with-sparsetensor_m11
['Save final submission'],12,rodgomrod,very-simple-nn-in-pytorch-with-sparsetensor,rodgomrod_very-simple-nn-in-pytorch-with-sparsetensor_m12
"['While I navigated the Blending Kernels available for this competition. I always fear about overfitting on Private Leaderboard.  While I am still finalizing my kernel with EDA and Model, The inputs that use in this kernel are generated from the great contributions made by other Kernel GrandMasters, Masters or Experts. None of my inputs contains blends or stack results.']",0,roydatascience,ieee-fraud-detection-aggregating-lightgbm-models,roydatascience_ieee-fraud-detection-aggregating-lightgbm-models_m0
"['<pre><b>Credits to the Experts (Please like their kernels)\n', '1. Navaneetha Kernel : https://www.kaggle.com/krishonaveen/xtreme-boost-and-feature-engineering\n', '2. Shugen Kernel : https://www.kaggle.com/andrew60909/lgb-starter-r\n', '3. Khan HBK Kernel : https://www.kaggle.com/duykhanh99/hust-lgb-starter-with-r \n', '4. Konstantin Kernel : https://www.kaggle.com/kyakovlev/ieee-gb-2-make-amount-useful-again/output\n', '5. Avocado Kernel : https://www.kaggle.com/iasnobmatsu/xgb-model-with-feature-engineering?scriptVersionId=18686303\n', ""6. David's Kernel : https://www.kaggle.com/davidcairuz/feature-engineering-lightgbm-w-gpu\n"", ""7. Lyalikov's Kernel : https://www.kaggle.com/timon88/lgbm-baseline-small-fe-no-blend\n"", ""8. Yuanrong's Kernel : https://www.kaggle.com/yw6916/lgb-xgb-ensemble-stacking-based-on-fea-eng\n"", ""9. Steve's Kernel : https://www.kaggle.com/abednadir/best-r-score\n"", '</b></pre>']",1,roydatascience,ieee-fraud-detection-aggregating-lightgbm-models,roydatascience_ieee-fraud-detection-aggregating-lightgbm-models_m1
['# Stacking Approach using GMean and Median'],2,roydatascience,ieee-fraud-detection-aggregating-lightgbm-models,roydatascience_ieee-fraud-detection-aggregating-lightgbm-models_m2
"[""<pre><b>The idea of GMean is taken from Paulo's Kernel https://www.kaggle.com/paulorzp/gmean-of-light-gbm-models-lb-0-947x</b></pre>""]",3,roydatascience,ieee-fraud-detection-aggregating-lightgbm-models,roydatascience_ieee-fraud-detection-aggregating-lightgbm-models_m3
['# Geometric Mean Stacking'],4,roydatascience,ieee-fraud-detection-aggregating-lightgbm-models,roydatascience_ieee-fraud-detection-aggregating-lightgbm-models_m4
['# Median Stacking'],5,roydatascience,ieee-fraud-detection-aggregating-lightgbm-models,roydatascience_ieee-fraud-detection-aggregating-lightgbm-models_m5
['AggStacker.csv generates the best score (.9475)'],6,roydatascience,ieee-fraud-detection-aggregating-lightgbm-models,roydatascience_ieee-fraud-detection-aggregating-lightgbm-models_m6
['![https://cis.ieee.org/images/files/template/cis-logo.png](https://cis.ieee.org/images/files/template/cis-logo.png)'],0,roydatascience,light-gbm-with-complete-eda,roydatascience_light-gbm-with-complete-eda_m0
"['IEEE-CIS works across a variety of AI and machine learning areas, including deep neural networks, fuzzy systems, evolutionary computation, and swarm intelligence. Today they’re partnering with the world’s leading payment service company, Vesta Corporation, seeking the best solutions for fraud prevention industry']",1,roydatascience,light-gbm-with-complete-eda,roydatascience_light-gbm-with-complete-eda_m1
['# Import Libraries'],2,roydatascience,light-gbm-with-complete-eda,roydatascience_light-gbm-with-complete-eda_m2
['# Visualize the Dataset'],3,roydatascience,light-gbm-with-complete-eda,roydatascience_light-gbm-with-complete-eda_m3
"[""<pre><b>Credits to Leonardo's Kernel : \n"", 'https://www.kaggle.com/kabure/extensive-eda-and-modeling-xgb-hyperopt</b></pre>']",4,roydatascience,light-gbm-with-complete-eda,roydatascience_light-gbm-with-complete-eda_m4
['<pre><b>Ploting Transaction Amount Values Distribution</b></pre>'],5,roydatascience,light-gbm-with-complete-eda,roydatascience_light-gbm-with-complete-eda_m5
['<pre><b>The Product Feature</b></pre>'],6,roydatascience,light-gbm-with-complete-eda,roydatascience_light-gbm-with-complete-eda_m6
"['<pre><b>Visualizing Card 1, Card 2 and Card 3 Distributions</b></pre>']",7,roydatascience,light-gbm-with-complete-eda,roydatascience_light-gbm-with-complete-eda_m7
['<pre><b>Card 4 - Categorical Feature</b></pre>'],8,roydatascience,light-gbm-with-complete-eda,roydatascience_light-gbm-with-complete-eda_m8
['<pre><b>Card 6 - Categorical</b></pre>'],9,roydatascience,light-gbm-with-complete-eda,roydatascience_light-gbm-with-complete-eda_m9
['<pre><b>Exploring M1-M9 Features</b></pre>'],10,roydatascience,light-gbm-with-complete-eda,roydatascience_light-gbm-with-complete-eda_m10
['<pre><b>Addr1 Distributions</b></pre>'],11,roydatascience,light-gbm-with-complete-eda,roydatascience_light-gbm-with-complete-eda_m11
['<pre><b> ADDR2 Distributions </b></pre>'],12,roydatascience,light-gbm-with-complete-eda,roydatascience_light-gbm-with-complete-eda_m12
['<pre><b>Ploting P-Email Domain</b></pre>'],13,roydatascience,light-gbm-with-complete-eda,roydatascience_light-gbm-with-complete-eda_m13
['<pre><b>Exploring C1-C14 features</b></pre>'],14,roydatascience,light-gbm-with-complete-eda,roydatascience_light-gbm-with-complete-eda_m14
['TimeDelta Feature to check if frauds have some specific hour that has highest % of frauds</b></pre>'],15,roydatascience,light-gbm-with-complete-eda,roydatascience_light-gbm-with-complete-eda_m15
['<pre><b>Top Days with highest Total Transaction Amount</b></pre>'],16,roydatascience,light-gbm-with-complete-eda,roydatascience_light-gbm-with-complete-eda_m16
['<pre><b>Ploting WeekDays Distributions</b></pre>'],17,roydatascience,light-gbm-with-complete-eda,roydatascience_light-gbm-with-complete-eda_m17
['<pre><b>Ploting Hours Distributions</b></pre>'],18,roydatascience,light-gbm-with-complete-eda,roydatascience_light-gbm-with-complete-eda_m18
['<pre><b>Transactions and Total Amount per day</b></pre>'],19,roydatascience,light-gbm-with-complete-eda,roydatascience_light-gbm-with-complete-eda_m19
['<pre><b>Fraud Transactions by Date</b></pre>'],20,roydatascience,light-gbm-with-complete-eda,roydatascience_light-gbm-with-complete-eda_m20
['# Feature Engineering'],21,roydatascience,light-gbm-with-complete-eda,roydatascience_light-gbm-with-complete-eda_m21
"[""<pre><b>Credits to Konstantin Yakovlev's Kernel\n"", 'https://www.kaggle.com/kyakovlev/ieee-gb-2-make-amount-useful-again</b></pre> ']",22,roydatascience,light-gbm-with-complete-eda,roydatascience_light-gbm-with-complete-eda_m22
['## Keras NN Starter Kernel'],0,ryches,keras-nn-starter-w-time-series-split,ryches_keras-nn-starter-w-time-series-split_m0
"[""Not looking to seriously compete in this competition so figured I'd at least share the small experiments I was toying with. This kernel serves as just a starter to look at how to handle the various numerical and categorical variables in a NN with Keras. \n"", '\n', 'Core purpose of this kernel:\n', '* How to handle categorical and numerical variables in neural networks\n', '* Methods to normalize skewed numerical variables\n', '* Greedy feature selection via exclusion\n', '* Splitting based on time']",1,ryches,keras-nn-starter-w-time-series-split,ryches_keras-nn-starter-w-time-series-split_m1
"['## Data Loading\n', 'Just the standard loading of the data used in most other kernels. ']",2,ryches,keras-nn-starter-w-time-series-split,ryches_keras-nn-starter-w-time-series-split_m2
['Dropping time since this likely isnt something we want our model to directly learn from'],3,ryches,keras-nn-starter-w-time-series-split,ryches_keras-nn-starter-w-time-series-split_m3
"['Selecting just the first set of columns and excluding the synthetic ""v"" features and other very sparse categoricals like deviceinfo and deviceid']",4,ryches,keras-nn-starter-w-time-series-split,ryches_keras-nn-starter-w-time-series-split_m4
"['## Numerical and Categorical\n', 'Listing off and categorizing the various variables available to us. We have numerical and categoricals. We will treat both of these slightly differently later']",5,ryches,keras-nn-starter-w-time-series-split,ryches_keras-nn-starter-w-time-series-split_m5
['We already dropped a lot of these features because in some trial and error it was shown that these caused rapid overfitting for some reason or otherwise introduced unnecessary noise into the data. We will make sure we only list the features we actually have still in the df now'],6,ryches,keras-nn-starter-w-time-series-split,ryches_keras-nn-starter-w-time-series-split_m6
"[""NN doesn't like nans so we will fill numerical columns with 0's. Previous people have tried plugging in the column means, but upon inspection this didn't seem very reliable because the train and test means for a given column could sometimes be drastically different. Plugging in zeros likely isnt the best. Might be better to plug in the train mean to the test df, but for simplicity I will stick with 0's for now. ""]",7,ryches,keras-nn-starter-w-time-series-split,ryches_keras-nn-starter-w-time-series-split_m7
"['## Label Encoding\n', ""We will take our categorical features fill the nans and assign them an integer ID per category and write down the number of total categories per column. We'll use this later in an embedding layer of the NN""]",8,ryches,keras-nn-starter-w-time-series-split,ryches_keras-nn-starter-w-time-series-split_m8
"['## Numerical Scaling\n', '\n', 'Now we will do some scaling of the data so that it will be in a more NN friendly format. First we will do log1p for any values that are above 100 and not below 0. This is in order to scale down any numerical variables that might have some extremely high values that screws up the statistics of the standard scaler \n', '\n', 'After that we will pass them through the standard scaler so that the values have a normal mean and std. This makes the NN converge signficantly faster and prevents any blowouts. Feel free to try for yourself by commenting out this cell of code']",9,ryches,keras-nn-starter-w-time-series-split,ryches_keras-nn-starter-w-time-series-split_m9
['Grabbing the features we want to pass into the neural network'],10,ryches,keras-nn-starter-w-time-series-split,ryches_keras-nn-starter-w-time-series-split_m10
"['## Neural Network Model Details\n', '\n', 'Our neural network will be fairly standard. We will use the embedding layer for categoricals and the numericals will go through feed forward dense layers. \n', '\n', 'We create our embedding layers such that we have as many rows as we had categories and the dimension of the embedding is the log1p + 1 of the number of categories. So this means that categorical variables with very high cardinality will have more dimensions but not signficantly more so the information will still be compressed down to only about 13 dimensions and the smaller number of categories will be only 2-3.\n', '\n', 'We will then pass the embeddings through a spatial dropout layer which will drop dimensions within the embedding across batches and then flatten and concatenate. Then we will concatenate this to the numerical features and apply batch norm and then add some more dense layers after. ']",11,ryches,keras-nn-starter-w-time-series-split,ryches_keras-nn-starter-w-time-series-split_m11
['We will then extract the features we actually want to pass to the NN'],12,ryches,keras-nn-starter-w-time-series-split,ryches_keras-nn-starter-w-time-series-split_m12
['We will iterate through epochs of the model and save the model weights if the score is an improvement upon previous best roc_auc_scores since this is competition metric. If the NN does not improve upon previous best after 4 epochs we will skip the rest of the training steps to save time. '],13,ryches,keras-nn-starter-w-time-series-split,ryches_keras-nn-starter-w-time-series-split_m13
"['## Greedy Feature Selection\n', '\n', 'First we will train the NN with all categorical and numerical features in order to make a baseline\n', '\n', 'After that we will greedily drop one feature at a time and see if it increases or decreases performance. If it increases upon dropping the feature then we will drop the feature. If it decreases then it will stay. ']",14,ryches,keras-nn-starter-w-time-series-split,ryches_keras-nn-starter-w-time-series-split_m14
['Dropping categoricals'],15,ryches,keras-nn-starter-w-time-series-split,ryches_keras-nn-starter-w-time-series-split_m15
['Dropping numeric'],16,ryches,keras-nn-starter-w-time-series-split,ryches_keras-nn-starter-w-time-series-split_m16
"['## Sanity Checks\n', 'Now we will make sure we loaded in the correction  model that scored favorably and then we will compare prediction statistics between validation and test. These should be relatively close or else we know something might have been off between how we prepared the train and test data for inference']",17,ryches,keras-nn-starter-w-time-series-split,ryches_keras-nn-starter-w-time-series-split_m17
"[""Let's fine-tune on the validation data now since that is the most recent and will likely help generalization to the test set""]",18,ryches,keras-nn-starter-w-time-series-split,ryches_keras-nn-starter-w-time-series-split_m18
"[""Trying to find the minimum raw features to get a decent score.  I couldn't be bothered using Time Stuff which definitely improve things ;)""]",0,scirpus,50-features-with-target-encoding,scirpus_50-features-with-target-encoding_m0
['![](https://cdn-images-1.medium.com/max/853/1*DgFPLm5TKXuKnNUlYCE2DQ.jpeg)'],0,shahules,tackling-class-imbalance,shahules_tackling-class-imbalance_m0
"[""<div align='left'><font size='5' color='#5b2c6f '> Purpose of this notebook</font></div>""]",1,shahules,tackling-class-imbalance,shahules_tackling-class-imbalance_m1
"['- In this notebook we will discuss about class imbalance problem which is occus often more in problems like fraudulent transaction identification and\n', '  spam  identification .\n', '- Discuss and implement methods to solve this issue to an extend.\n', '- [Loading Libraries](#1)\n', '- [Loading Data ](#2)\n', '- [The metric trap](#3)\n', '- [Data preparating](#4)\n', '- [Resampling](#5)\n', '- [Resampling using sklearn](#6)\n', '- [Dimensionality Reduction and Clustering](#7)\n', '- [Python imbalanced-learn module](#8)\n', '- [Algorithmic Ensemble Techniques](#9)']",2,shahules,tackling-class-imbalance,shahules_tackling-class-imbalance_m2
"['### [Loading Required libraries](#1)<a id=""1""></a> <br>']",3,shahules,tackling-class-imbalance,shahules_tackling-class-imbalance_m3
"['## [Loading Data](#2)<a id=""2""></a> <br>']",4,shahules,tackling-class-imbalance,shahules_tackling-class-imbalance_m4
"[""<div align='left'><font size='4' color='#229954'>Getting basic Idea</font></div>""]",5,shahules,tackling-class-imbalance,shahules_tackling-class-imbalance_m5
"[""<div align='left'><font size='4' color='#229954'>Target variable</font></div>\n""]",6,shahules,tackling-class-imbalance,shahules_tackling-class-imbalance_m6
"['- There is clearly a class imbalace problem.\n', '- We will look into methods of solving this issue later in this notebook.']",7,shahules,tackling-class-imbalance,shahules_tackling-class-imbalance_m7
[' \n'],8,shahules,tackling-class-imbalance,shahules_tackling-class-imbalance_m8
"['## [The metric trap](#3)<a id=""3""></a> <br>\n', '\n', 'One of the major issues that novice users fall into when dealing with unbalanced datasets relates to the metrics used to evaluate their model. Using simpler metrics like accuracy_score can be misleading. In a dataset with highly unbalanced classes, if the classifier always ""predicts"" the most common class without performing any analysis of the features, it will still have a high accuracy rate, obviously illusory.\n', '\n', '\n', '    False Positive. Predict an event when there was no event.\n', '    False Negative. Predict no event when in fact there was an event.\n', '\n', '   In the overview of the problem statement the organizers has described a situation where you stand at the queue for a long time and when your chance arrives,the transaction gets denied because it was interpreted as a Fraudulent transaction which many of us have faced.\n', ' This is classical example of **False Negative** prediction.\n', ' \n', '\n', '\n', '**Change the performance metric**\n', '\n', 'As we saw above, accuracy is not the best metric to use when evaluating imbalanced datasets as it can be very misleading. Metrics that can provide better insight include:\n', '\n', '**Confusion Matrix**: a table showing correct predictions and types of incorrect predictions.\n', '    \n', '**Precision**: the number of true positives divided by all positive predictions. Precision is also called Positive Predictive Value. It is a measure of a classifier’s exactness. Low precision indicates a high number of false positives.\n', '    \n', '**Recall**: the number of true positives divided by the number of positive values in the test data. Recall is also called Sensitivity or the True Positive Rate. It is a measure of a classifier’s completeness. Low recall indicates a high number of false negatives.\n', '    \n', '**F1 Score**: the weighted average of precision and recall.\n', '    ']",9,shahules,tackling-class-imbalance,shahules_tackling-class-imbalance_m9
"[""I don't understand why the competition hosts selected ROC_AUC as evaluation metric,I think\n"", '- ROC curves should be used when there are roughly equal numbers of observations for each class.\n', '-  Precision-Recall curves should be used when there is a moderate to large class imbalance.\n']",10,shahules,tackling-class-imbalance,shahules_tackling-class-imbalance_m10
"['## [Merging transaction and identity dataset](#4)<a id=""4""></a> <br>\n', '\n', 'We will firt merge our **transactions** and **identity** datasets.']",11,shahules,tackling-class-imbalance,shahules_tackling-class-imbalance_m11
"[""<div align='left'><font size='4' color='#229954'>Reducing memory usage</font></div>\n"", '\n']",12,shahules,tackling-class-imbalance,shahules_tackling-class-imbalance_m12
"[""<div align='left'><font size='4' color='#229954'>Splitting to train and validation</font></div>\n""]",13,shahules,tackling-class-imbalance,shahules_tackling-class-imbalance_m13
"['- We will now split the train dataset into train and validation set.\n', '- We will keeep 20% of data for validation.']",14,shahules,tackling-class-imbalance,shahules_tackling-class-imbalance_m14
"['\n', '## [Resampling](#5)<a id=""5""></a> <br>\n', '\n', 'A widely adopted technique for dealing with highly unbalanced datasets is called resampling. It consists of removing samples from the majority class (under-sampling) and / or adding more examples from the minority class (over-sampling).\n', '\n', '![](https://raw.githubusercontent.com/rafjaa/machine_learning_fecib/master/src/static/img/resampling.png)']",15,shahules,tackling-class-imbalance,shahules_tackling-class-imbalance_m15
"['## [Resampling Techniques using sklearn](#6)<a id=""6""></a> <br>']",16,shahules,tackling-class-imbalance,shahules_tackling-class-imbalance_m16
"[""<div align='left'><font size='4' color=' #6c3483'> 1.Oversample minority class </font></div>\n""]",17,shahules,tackling-class-imbalance,shahules_tackling-class-imbalance_m17
"['Oversampling can be defined as adding more copies of the minority class. Oversampling can be a good choice when you don’t have a ton of data to work with.\n', '\n', 'We will use the resampling module from Scikit-Learn to randomly replicate samples from the minority class.']",18,shahules,tackling-class-imbalance,shahules_tackling-class-imbalance_m18
"[""<div align='left'><font size='4' color=' #6c3483'>  2. Undersample majority class </font></div>\n""]",19,shahules,tackling-class-imbalance,shahules_tackling-class-imbalance_m19
"['Undersampling can be defined as removing some observations of the majority class. Undersampling can be a good choice when you have a ton of data -think millions of rows. But a drawback is that we are removing information that may be valuable. This could lead to underfitting and poor generalization to the test set.\n', '\n', 'We will again use the resampling module from Scikit-Learn to randomly remove samples from the majority class.']",20,shahules,tackling-class-imbalance,shahules_tackling-class-imbalance_m20
['We will review other resampling techniques.'],21,shahules,tackling-class-imbalance,shahules_tackling-class-imbalance_m21
"['\n', '\n', ""For ease of visualization, let's create a small unbalanced sample dataset using the make_classification method:\n""]",22,shahules,tackling-class-imbalance,shahules_tackling-class-imbalance_m22
"['- We will do an experiment with this data without any resampling technique.\n', '- We will fit and predict the data on a Logistic regression model and observe the output scores.']",23,shahules,tackling-class-imbalance,shahules_tackling-class-imbalance_m23
['- We will define two functions to plot precision_recall curve and roc curve'],24,shahules,tackling-class-imbalance,shahules_tackling-class-imbalance_m24
"['\n', '\n', 'We will also create a 2-dimensional plot function, plot_2d_space, to see the data distribution:\n']",25,shahules,tackling-class-imbalance,shahules_tackling-class-imbalance_m25
"['\n', '\n', '\n', '\n', '## [Dimensionality Reduction and Clustering](#7)<a id=""7""></a> <br>\n', '\n', 'Understanding t-SNE:\n', 'In order to understand this algorithm you have to understand the following terms:\n', '\n', '    Euclidean Distance\n', '    Conditional Probability\n', '    Normal and T-Distribution Plots\n', '\n']",26,shahules,tackling-class-imbalance,shahules_tackling-class-imbalance_m26
"['In the below section we will implement three major dimensionality reduction algorithms\n', '- **T-sne**\n', '- **PCA**\n', '- **Truncated SVD**']",27,shahules,tackling-class-imbalance,shahules_tackling-class-imbalance_m27
['Now we will visualize the output of the above three algorithms in a 2D space.'],28,shahules,tackling-class-imbalance,shahules_tackling-class-imbalance_m28
"['\n', ""## [Python imbalanced-learn module](#8)<a id='8'></a></br>\n"", '\n', 'A number of more sophisticated resapling techniques have been proposed in the scientific literature.\n', '\n', 'For example, we can cluster the records of the majority class, and do the under-sampling by removing records from each cluster, thus seeking to preserve information. In over-sampling, instead of creating exact copies of the minority class records, we can introduce small variations into those copies, creating more diverse synthetic samples.\n', '\n', ""Let's apply some of these resampling techniques, using the Python library imbalanced-learn. It is compatible with scikit-learn and is part of scikit-learn-contrib projects.\n""]",29,shahules,tackling-class-imbalance,shahules_tackling-class-imbalance_m29
"[""<div align='left'><font size='4' color=' #6c3483'>  Random under-sampling  with imbalanced-learn </font></div>\n"", '\n']",30,shahules,tackling-class-imbalance,shahules_tackling-class-imbalance_m30
"[""- Let's try fit and predict on this data and observe the outcome.""]",31,shahules,tackling-class-imbalance,shahules_tackling-class-imbalance_m31
"[""<div align='left'><font size='4' color=' #6c3483'>  Random over-sampling  with imbalanced-learn </font></div>\n"", '\n']",32,shahules,tackling-class-imbalance,shahules_tackling-class-imbalance_m32
"[""Let's try fit and predict on this data and observe the outcome.""]",33,shahules,tackling-class-imbalance,shahules_tackling-class-imbalance_m33
"['\n', '## [Under-sampling: Tomek links](#9)\n', '\n', 'Tomek links are pairs of very close instances, but of opposite classes. Removing the instances of the majority class of each pair increases the space between the two classes, facilitating the classification process.\n', '\n', '![](https://raw.githubusercontent.com/rafjaa/machine_learning_fecib/master/src/static/img/tomek.png?v=2)']",34,shahules,tackling-class-imbalance,shahules_tackling-class-imbalance_m34
"[""<div align='left'><font size='4' color=' #6c3483'>  Over-sampling: SMOTE </font></div>\n"", '\n']",35,shahules,tackling-class-imbalance,shahules_tackling-class-imbalance_m35
"['\n', '\n', '\n', '\n', 'SMOTE (Synthetic Minority Oversampling TEchnique) consists of synthesizing elements for the minority class, based on those that already exist. It works randomly picingk a point from the minority class and computing the k-nearest neighbors for this point. The synthetic points are added between the chosen point and its neighbors.\n', '\n', '![](https://raw.githubusercontent.com/rafjaa/machine_learning_fecib/master/src/static/img/smote.png)\n']",36,shahules,tackling-class-imbalance,shahules_tackling-class-imbalance_m36
"[""Let's try fit and predict on this data and observe the outcome.""]",37,shahules,tackling-class-imbalance,shahules_tackling-class-imbalance_m37
"['## [Algorithmic Ensemble Techniques](#9)<a id=""1""></a> <br>\n', 'The above section, deals with handling imbalanced data by resampling original data to provide balanced classes. In this section, we are going to look at an alternate approach i.e.  Modifying existing classification algorithms to make them appropriate for imbalanced data sets.\n', '\n', 'The main objective of ensemble methodology is to improve the performance of single classifiers. The approach involves constructing several two stage classifiers from the original data and then aggregate their prediction\n', '\n', '![ Approach to Ensemble based Methodologies](https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/03/16142904/ICP4.png)']",38,shahules,tackling-class-imbalance,shahules_tackling-class-imbalance_m38
"[""<div align='left'><font size='4' color=' #6c3483'>  XGBoost </font></div>\n"", '\n', '\n', '\n', '![](https://miro.medium.com/max/1400/1*FLshv-wVDfu-i54OqvZdHg.png)\n', '\n', 'XGBoost (Extreme Gradient Boosting) is an advanced and more efficient implementation of Gradient Boosting Algorithm discussed in the previous section.\n', '\n', 'Advantages over Other Boosting Techniques\n', '\n', 'It is 10 times faster than the normal Gradient Boosting as it implements parallel processing. It is highly flexible as users can define custom optimization objectives and evaluation criteria, has an inbuilt mechanism to handle missing values.\n', 'Unlike gradient boosting which stops splitting a node as soon as it encounters a negative loss, XG Boost splits up to the maximum depth specified and prunes the tree backward and removes splits beyond which there is an only negative loss.\n', '\n', 'Extreme gradient boosting can be done using the XGBoost package in R and Python']",39,shahules,tackling-class-imbalance,shahules_tackling-class-imbalance_m39
"['### WORK IN PROGRESS\n', ""<div align='left'><font size='5' color=' #a93226 '>  If you like my work,please do upvote ^ </font></div>\n"", '\n']",40,shahules,tackling-class-imbalance,shahules_tackling-class-imbalance_m40
"[""<div align='left'><font size='4' color=' #6c3483'> References </font></div>\n"", '\n', '\n', '- [Dealing with Imbalanced Data](https://towardsdatascience.com/methods-for-dealing-with-imbalanced-data-5b761be45a18)\n', '- [Resampling strategies for imbalanced datasets](https://www.kaggle.com/rafjaa/resampling-strategies-for-imbalanced-datasets)']",41,shahules,tackling-class-imbalance,shahules_tackling-class-imbalance_m41
"['### XGBoost with Feature Interactions \n', 'Old but gold: [xgbfir](https://github.com/limexp/xgbfir)']",0,silverstone1903,simple-xgboost-with-feature-interactions,silverstone1903_simple-xgboost-with-feature-interactions_m0
"['#### Dropping columns and mapping emails. Thanks for the code the1owl!\n', 'https://www.kaggle.com/jazivxt/safe-box']",1,silverstone1903,simple-xgboost-with-feature-interactions,silverstone1903_simple-xgboost-with-feature-interactions_m1
"['### XGBoost feature importance types:\n', '* ""weight"" is the number of times a feature appears in a tree\n', '* ""gain"" is the average gain of splits which use the feature\n', '* ""cover"" is the average coverage of splits which use the feature where coverage is defined as the number of samples affected by the split']",2,silverstone1903,simple-xgboost-with-feature-interactions,silverstone1903_simple-xgboost-with-feature-interactions_m2
"[""#### Let's check the 4th fold feature interactions""]",3,silverstone1903,simple-xgboost-with-feature-interactions,silverstone1903_simple-xgboost-with-feature-interactions_m3
['##### Feature Interaction: https://christophm.github.io/interpretable-ml-book/interaction.html'],4,silverstone1903,simple-xgboost-with-feature-interactions,silverstone1903_simple-xgboost-with-feature-interactions_m4
['Based on these interactions you can create some new features...'],5,silverstone1903,simple-xgboost-with-feature-interactions,silverstone1903_simple-xgboost-with-feature-interactions_m5
"[""[Fork of 'Stacking?'](https://www.kaggle.com/rajwardhanshinde/stacking)\n"", '\n', '**Update for v.9:** again a slightly higher scoring Under_Over submission is added (nothing dramatic: 0.9398). You may have to remove and reattach ieeesubmissions5 dataset to get the latest version (`xgboost_under_over_blend_9398`)\n', '\n', ""**Update for v.8:** a tiny update with a fresh file from [Undersample with multiple runs kernel](https://www.kaggle.com/stocks/under-sample-with-multiple-runs). don't know the score yet\n"", '\n', '**Update for v.7**:\n', '\n', 'Added [IEEE - LGB + Bayesian opt.](https://www.kaggle.com/vincentlugat/ieee-lgb-bayesian-opt)\n', '\n', '**Update for v.5**:\n', '\n', ' * <font color=green>Median 0.9425</font>\n', ' * <font color=green>Mean 0.9420 </font>\n', ' * <font color=green>Median Rank 0.9413</font>\n', ' * <font color=green>Mean Rank 0.9413 </font>\n', '\n', '* `ieeesubmissions2` dataset was update replacing `safebox9416.csv` with `safebox9367.csv`. `safebox9416.csv` had an incorrect score linked to it (it is actually 0.9322). The reason for the mix-up is that it is unknown which submission generates which score in public kernels. The reason I am no longer using `blend1.csv` from Safebox kernel is because `blend1` itself was a blend with `ieee_9383` also used in this kernel.\n', '\n', '* added Median rank\n', '\n', '**Update for v.4**:\n', '\n', 'v.4 is the same as v.3, only reporting select scores for v.3\n', '\n', '**Update for v.3**:\n', '\n', '**<font color=green>Submissions tried in v.3:</font>**\n', '\n', ' * <font color=green>Median 0.9427</font>\n', ' * <font color=green>Mean 0.9420 </font>\n', '\n', 'I added a new dataset with submission files labelled by their scores (to the best of my knowledge). They all come from the same kernels as in v.2 but two are different  versions of those. Not sure if any them might improve the score.\n', '\n', '**Updates made by me in v.1 and v.2:**\n', '\n', '* use the higher scoring blend of oversample + undersample from [My kernel](https://www.kaggle.com/stocks/under-sample-with-multiple-runs). \n', ""I'm guessing that the stacking kernel that I've copied here used only my under-sampled model.\n"", 'At least one of the other models [EDA Kernel](https://www.kaggle.com/artgor/eda-and-models) is scoring higher.\n', 'Thus, the explanation for high score of this clone likely lies in higher scoring models used for stacking.\n', '* trying stacking based on ranks\n', '\n', '**<font color=green>Submission tried in v.2:</font>**\n', '\n', ' * <font color=green>Median 0.9429</font>\n', ' * <font color=green>Mean rank 0.9415 </font>\n', '\n', '#### Info from the parent of this clone:\n', '>* Based on https://www.kaggle.com/lpachuong/statstack\n', '>* Thanks to <br>\n', 'https://www.kaggle.com/jazivxt/safe-box<br>\n', 'https://www.kaggle.com/artgor/eda-and-models<br>\n', 'https://www.kaggle.com/stocks/under-sample-with-multiple-runs<br>\n', 'https://www.kaggle.com/artkulak/ieee-fraud-simple-baseline-0-9383-lb']",0,stocks,stacking-higher-and-higher,stocks_stacking-higher-and-higher_m0
['## <font color=blue>Vote early and vote often!</font>\n'],1,stocks,stacking-higher-and-higher,stocks_stacking-higher-and-higher_m1
['# Mean Stacking'],2,stocks,stacking-higher-and-higher,stocks_stacking-higher-and-higher_m2
['# Median Stacking'],3,stocks,stacking-higher-and-higher,stocks_stacking-higher-and-higher_m3
"['# Pushout + Median Stacking\n', '>* Pushout strategy is bit aggresive']",4,stocks,stacking-higher-and-higher,stocks_stacking-higher-and-higher_m4
"['# MinMax + Mean Stacking\n', '>* MinMax seems more gentle and it outperforms the previous one']",5,stocks,stacking-higher-and-higher,stocks_stacking-higher-and-higher_m5
['# MinMax + Median Stacking'],6,stocks,stacking-higher-and-higher,stocks_stacking-higher-and-higher_m6
['# Median Rank'],7,stocks,stacking-higher-and-higher,stocks_stacking-higher-and-higher_m7
['# Mean Rank'],8,stocks,stacking-higher-and-higher,stocks_stacking-higher-and-higher_m8
['**Note : The best result of this kernel is at V16.**'],0,sunilsj99,fraud-detection-ieee,sunilsj99_fraud-detection-ieee_m0
['**Merging the transactions and indentity data**'],1,sunilsj99,fraud-detection-ieee,sunilsj99_fraud-detection-ieee_m1
"[""**Let's reduce the memory of the dataframe**""]",2,sunilsj99,fraud-detection-ieee,sunilsj99_fraud-detection-ieee_m2
"['## Objective:\n', '\n', '**Predict fraud based on transaction information**\n']",0,suoires1,fraud-detection-eda-and-modeling,suoires1_fraud-detection-eda-and-modeling_m0
"['## Joining the transaction data:\n', '\n', '### Transaction Table:\n', '\n', 'Transaction table contain TransactionID, which we can use to join with the identity table.']",1,suoires1,fraud-detection-eda-and-modeling,suoires1_fraud-detection-eda-and-modeling_m1
"['The transaction ID for transaction table is also unique for each observation. Therefore, we have 1 to 1 join from identity table to transaction table:']",2,suoires1,fraud-detection-eda-and-modeling,suoires1_fraud-detection-eda-and-modeling_m2
"['However, it comes to my attention that the number of rows are different for each table, despite having unique TransactionID:']",3,suoires1,fraud-detection-eda-and-modeling,suoires1_fraud-detection-eda-and-modeling_m3
"[""This suggests that there are transactions that don't have identity. A quick research on the [discussion thread](https://www.kaggle.com/c/ieee-fraud-detection/discussion/101203#latest-605862) reveals that Vesta was unable to collect all identity information due to technical difficulty. Therefore, we will need to face two options:\n"", '\n', '1. Using identity + transaction to make predictions. This option results in fewer observations but more complete (more features).\n', '\n', '2. Using only transaction\n', '\n', '3. Using transaction but add identity when avaiable\n', '\n', 'For now, we only explore the identity + transaction joined table to do EDA and build model. ']",4,suoires1,fraud-detection-eda-and-modeling,suoires1_fraud-detection-eda-and-modeling_m4
"['# Data Quality Inspection\n', '\n', 'There are couple common issues that we need to watch out for:\n', '\n', '1. Attributes Formatting (data types)\n', '\n', '2. Missing Data\n', '\n', '3. Replacement or Drop\n', '\n', '4. Response Variable\n', '\n', ""First, let's transform our data into the types that we expected:\n"", '\n', '## 1. Attributes formatting (data types)']",5,suoires1,fraud-detection-eda-and-modeling,suoires1_fraud-detection-eda-and-modeling_m5
"['Most of the column names have been masked for privacy protection. Without accurate description of the fields meaning, it would be difficult to determine the type of data. Fortunately, Vesta have provided us with high-level summary of data.\n', '\n', ""Let's recall the avaiable groups of information that were provided for us:\n"", '\n', '1. Identity Table:\n', '\n', '    * id_01 - id_38: contains network connection information\n', '    \n', '    * DeviceType and DeviceInfo\n', '    \n', '2. Transactional Table:\n', '\n', '    * card1 - card6: card information\n', '    \n', '    * addr: address\n', '    \n', '    * dist: distance\n', '\n', '    * P_ and (R__) emaildomain: purchaser and recipient email domain\n', '    \n', '    * C1-C14: counting\n', '    \n', '    * D1-D15: timedelta, such as days between previous transaction, etc.\n', '    \n', '    * M1-M9: match, such as names on card and address, etc.\n', '    \n', '    * V1-V339: Vesta engineered features\n', '    \n', '    * ProductCD: product code, the product for each transaction\n', '    \n', '    * TransactionDT: timedelta from a given reference datetime\n', '    \n', '    * TransactionAMT: transaction payment amount in USD    \n', '    \n', '## Missing Data\n', '    \n', ""Let's take a look at the missing data for the **categorical variables** first:""]",6,suoires1,fraud-detection-eda-and-modeling,suoires1_fraud-detection-eda-and-modeling_m6
"['**Observation**: We can see that our data has a lot of missing values. White color presents missing values.\n', '\n', '1. Most M columns missing almost if not all data\n', '\n', '2. Id_07, 08 and id_21-27 missing most data\n', '\n', ""3. Id_01, id_12, card1, card2 contains mostly non-null. Perhaps, these columns contain unique ID information, and therefore, cannot be null. Let's double check the number of missing in these columns: ""]",7,suoires1,fraud-detection-eda-and-modeling,suoires1_fraud-detection-eda-and-modeling_m7
"['Yes, they are indeed complete, except for card 2. If I were to guess, card1 could be first name and card2 could be last name.\n', '\n', ""Now let's check out missing data for **numerical variables**:""]",8,suoires1,fraud-detection-eda-and-modeling,suoires1_fraud-detection-eda-and-modeling_m8
"['**Observation**: \n', '    \n', '    1. Basic information about transaction such as ID, DT, amount and type of product is complete \n', '    \n', '    2. Dist1 and dist2 is very sparse.\n', '    \n', '    3. C columns are complete\n', '    \n', '    4. Most D columns are sparse except D1\n', '    \n', ""Lastly, we want to check for data completeness of **Vesta's engineered features**:""]",9,suoires1,fraud-detection-eda-and-modeling,suoires1_fraud-detection-eda-and-modeling_m9
"['**Observation**: Ahh, she looks like a work of art. The repeated missing patterns in the V columns suggest that many V columns are related and perhaps trying to describe certain characteristics of a transaction. For example, columns V322-V399 have identical missing locations.\n', '\n', ""Let's verify our intuition with correlation heatmap measures nullity correlation: how strongly the presence or absence of one variable affects the presence of another:""]",10,suoires1,fraud-detection-eda-and-modeling,suoires1_fraud-detection-eda-and-modeling_m10
"['**Interpretation**: The dendrogram uses a hierarchical clustering algorithm to bin variables against one another by their nullity correlation. Each cluster of leaves explain how one variable might always be empty when another is empty, or filled when another variable is filled. This dendogram suggests that the position of missing/fill values are correlated. Perhaps similar columns were derived from the same feature or combinations of features.']",11,suoires1,fraud-detection-eda-and-modeling,suoires1_fraud-detection-eda-and-modeling_m11
"['## 3. Replacement or drop the missings\n', 'The idea of imputation is both *""seductive and dangerous""* in the words of R.J.A Little. \n', '\n', 'I truly believe that there is no best way to deal with missing, especially when having to deal with partial information. Knowing which columns could be imputed or dropped may alter the result of the final predictions by a non-trivial amount. The fact that certain value is missing could have been due to specific variation in the feature (missing not at random). This is one of the process that could have been much more useful if we were given the meaning of each columns. But when life gives you lemon, you turns it into sweet, sweet meachine learning input juice. \n', '\n', '### Understand that Train and Test data were splitted by time\n', '\n', 'This is a graceful finding from https://www.kaggle.com/robikscube/ieee-fraud-detection-first-look-and-eda.\n', '\n', '* The `TransactionDT` feature is defined as time delta from a chosen datetime. This gives us information about the relative time and the countinuity of each transaction. Ploting both test and train `TransactionDT` on the graph suggests that train and test dataset were splited by time, with a gap in between.']",12,suoires1,fraud-detection-eda-and-modeling,suoires1_fraud-detection-eda-and-modeling_m12
"['Some people suggest that if `TransactionDT` is measured in seconds, then the combined time period between test and train dataset could total to approximately 1 year, and the gap can account for ~ 1 month. \n', '\n', 'Lynn@Vesta commented in one of the discussion post:\n', '\n', '*""We define reported chargeback on card, user account, associated email address and transactions directly linked to these attributes as fraud transaction (isFraud=1); If none of above is found after 120 days, then we define as legit (isFraud=0)""*\n']",13,suoires1,fraud-detection-eda-and-modeling,suoires1_fraud-detection-eda-and-modeling_m13
['## 4. Response/ Target Variable'],14,suoires1,fraud-detection-eda-and-modeling,suoires1_fraud-detection-eda-and-modeling_m14
"['**Observation**:The fraud percentage is quite high: 7.85% for the complete observations (identity + transaction). We can see there is a class imbalance problem, where occurence of one class is significantly higher than another. This will lead to much a higher false negative - tendency of picking ""not fraud"". We can mitigate this issue by using two common methods:\n', '\n', '1. Cost function based approaches\n', '\n', '2. Sampling based approaches']",15,suoires1,fraud-detection-eda-and-modeling,suoires1_fraud-detection-eda-and-modeling_m15
"['# Explore Categorical Features\n', '**Categorical Features:**\n', '\n', '**1. Transactional Table:**\n', '    \n', '    ProductCD\n', '\n', '    card1 - card6\n', '\n', '    addr1, addr2\n', '\n', '    Pemaildomain Remaildomain\n', '\n', '    M1 - M9\n', '    \n', '    \n', '**2. Identity Table**\n', '\n', '    DeviceType\n', '\n', '    DeviceInfo\n', '    \n', '    id12 - id38\n', '\n', ""Let's take a quick look at these categorical features:""]",16,suoires1,fraud-detection-eda-and-modeling,suoires1_fraud-detection-eda-and-modeling_m16
['## Examine Product Code'],17,suoires1,fraud-detection-eda-and-modeling,suoires1_fraud-detection-eda-and-modeling_m17
['**Observations**: C is the most frequent product category. Product C also have the highest count of fraud. We can obtain the proportion of fraud for each product category:'],18,suoires1,fraud-detection-eda-and-modeling,suoires1_fraud-detection-eda-and-modeling_m18
"['**Conclusion**: Product C takes up 67.5% of fraud cases for transactions that have identity. And also have highest rate of fraud: 12%, more than double any other class of product.\n', '\n', '**Question**: Why product C? Is there any additional information that help us better understand product C high fraud rate?\n', '\n', 'We have 2 numerical variables that we can compare between groups of products:\n', '\n', 'TransactionDT: timedelta from a given reference datetime\n', '\n', 'TransactionAmt: transaction payment amount in USD']",19,suoires1,fraud-detection-eda-and-modeling,suoires1_fraud-detection-eda-and-modeling_m19
['**Observation**: Product C are items with low dollar value.'],20,suoires1,fraud-detection-eda-and-modeling,suoires1_fraud-detection-eda-and-modeling_m20
"['**Observation**: All products have same min and max timedelta range. \n', '\n', '**Conclusion**:The plot suggests little to no difference in timedelta accross all groups.\n', '\n', '## Examine Card 1,2,3,5\n', '\n', ""The card 1,2,3, and 5 was represented as numerical values, temping us to plot the histogram. However, we need to remember that card columns were classified as categorical variables. Meaning it's likely that these numerical variables were coded for categorical variables.""]",21,suoires1,fraud-detection-eda-and-modeling,suoires1_fraud-detection-eda-and-modeling_m21
"[""Card 1 contains 8499 unique values, suggesting card 1 may have been ID of the card. Card 2,3 and 5 have less unique values, so perhaps they could be expiration date, or combinations that generate card identity? Since we don't know how these information was scrammbled, we might pickup patterns generated by encryption algorithm instead of data. No further analysis should be done unless more infomation is given.\n"", '\n', 'Same goes for the addr1 and addr2.']",22,suoires1,fraud-detection-eda-and-modeling,suoires1_fraud-detection-eda-and-modeling_m22
"['## Examine Card 4 and 6\n', '\n', '### Card 4: Card Network']",23,suoires1,fraud-detection-eda-and-modeling,suoires1_fraud-detection-eda-and-modeling_m23
"['**Observation:** Visa card accounts for the highest instances of fraud, but this also because visa is the most popular card type. Again, we can only conclude after comparing the fraud propotion for each card type:']",24,suoires1,fraud-detection-eda-and-modeling,suoires1_fraud-detection-eda-and-modeling_m24
"['**Conclusion**: Visa accounts for 61% of all fraud occurences. However, when normalized by total number of each type, Visa have fraud rate of only 8%, lower than Mastercard and same as Discovercard. Only American Express have significantly lower fraud rate compare to others.']",25,suoires1,fraud-detection-eda-and-modeling,suoires1_fraud-detection-eda-and-modeling_m25
"['### Card 6: Card Type\n', '\n', 'Similarly, we can use the same method of data analysis on this variable:']",26,suoires1,fraud-detection-eda-and-modeling,suoires1_fraud-detection-eda-and-modeling_m26
"['**Observation:** The number of card type are fairly simiar, and so does the fraud cases. ']",27,suoires1,fraud-detection-eda-and-modeling,suoires1_fraud-detection-eda-and-modeling_m27
"['**Observation**: Not much difference in fraud rate between credit card and debit card\n', '\n', '## Examine Email Domain\n', '\n', '### 1. Purchaser Email']",28,suoires1,fraud-detection-eda-and-modeling,suoires1_fraud-detection-eda-and-modeling_m28
"['**Observation**: I see alot of domains came from the same distributors such as hotmail.com, hotmail.fr, yahoo.com, yahoo.fr, yahoo.de, etc. We can group these domains together under the parent distributors.\n', '\n', ""**Action:** Create P_parent_emaildomain field that remove the part after '.' ""]",29,suoires1,fraud-detection-eda-and-modeling,suoires1_fraud-detection-eda-and-modeling_m29
"[""Fewer email domains result in cleaner x tickers. Let's add the fraud rate like in the previous graphs, but this time we add the rate line on top of this graph:""]",30,suoires1,fraud-detection-eda-and-modeling,suoires1_fraud-detection-eda-and-modeling_m30
"[""Protonmail returns an exemely high fraud rate. Almost 80% of transactions from purchaser using protonmail.com were label fraud. Let's double check this result:""]",31,suoires1,fraud-detection-eda-and-modeling,suoires1_fraud-detection-eda-and-modeling_m31
"['### 2. Recipient Email\n', '\n', 'Similarly, we can perform the similar analysis on Recepient email domains']",32,suoires1,fraud-detection-eda-and-modeling,suoires1_fraud-detection-eda-and-modeling_m32
"['I enjoy this format of visualizing, so I should creat a function that help me explore the categorical format with regard to fraud rate:']",33,suoires1,fraud-detection-eda-and-modeling,suoires1_fraud-detection-eda-and-modeling_m33
"['## Examine M1-M9\n', '\n', ""The transaction data that has comple identity returns mostly NaN except for M4. Let's check it out:""]",34,suoires1,fraud-detection-eda-and-modeling,suoires1_fraud-detection-eda-and-modeling_m34
"['**Observartion**: Not much variation in fraud rate between M0, M1, and M2 in M4\n', '\n', 'We have gone through all categorical variables in the Transaction Table, now we check out the remaining categorical variables in the Identity Table.\n', '\n', '## Examine DeviceType']",35,suoires1,fraud-detection-eda-and-modeling,suoires1_fraud-detection-eda-and-modeling_m35
['**Observation**: Fraud rate is higher for mobile device compared to desktop'],36,suoires1,fraud-detection-eda-and-modeling,suoires1_fraud-detection-eda-and-modeling_m36
['## Examine DeviceInfo'],37,suoires1,fraud-detection-eda-and-modeling,suoires1_fraud-detection-eda-and-modeling_m37
"[""Since we have way too many devices, it makes more sense to select a few devices that has non-trivial count. Let's select categories that have more than 500 counts:""]",38,suoires1,fraud-detection-eda-and-modeling,suoires1_fraud-detection-eda-and-modeling_m38
"['**Observation**: We can see the fraud rate is higher for certain devices\n', '\n', '## Examine id12 - id38\n', '\n', 'We may generate all the graphs for id12 to id38. Depend on your preference, some graphs may be more informative than the other. The graphs below are selected based on:\n', '\n', '1. If the graph contains non-masked information (or categories have self-expalainatory meaning)\n', ""    * For example: 'Found' and 'NotFound' are two categories that by themselves, don't provide us with any helpful information in understanding their relationships with target variable. Perhaps our learner can pickup on the differences, but it's outside of our domain to understand these variables semantically. \n"", '\n', '2. If the graph contains not too many categories so that the xtickers can be plotted legibly\n', '\n', 'You can plots them all out and select for yourself. Here are some of my picks:\n', '\n', '### IP Proxy']",39,suoires1,fraud-detection-eda-and-modeling,suoires1_fraud-detection-eda-and-modeling_m39
"['**Obervation**: The first notable id plot is the IP status. It is interesting to see the anonymous IP_Proxy would have a higher fraud rate. If someone were to commit a fraudulent transaction, it makes sense that the person would want to protect his/her identity.\n', '\n', '### Operating Systems']",40,suoires1,fraud-detection-eda-and-modeling,suoires1_fraud-detection-eda-and-modeling_m40
['We can aggregate the operating system into a few major OSs. '],41,suoires1,fraud-detection-eda-and-modeling,suoires1_fraud-detection-eda-and-modeling_m41
"['**Observation**: The fraud rate across multiple well-known OSs seem fairly similar. ""Other"" operating systems have a much higher fraud rate.\n', '\n', ""However, it's strange that we see more IOS devices compared to Android, given that Android is the most popular mobile system. If I were to work for Vista, I would ask how the system collects more IOS instances. Could it be that Vista have given us an filtered dataset? Specific market segment? Systematic error or deficiency in collecting Android info?\n"", '\n', '### Browsers']",42,suoires1,fraud-detection-eda-and-modeling,suoires1_fraud-detection-eda-and-modeling_m42
"['Same as previous plot, we need to reduce the number of categories using aggregation:']",43,suoires1,fraud-detection-eda-and-modeling,suoires1_fraud-detection-eda-and-modeling_m43
"[""We have a few browers that have absurdly high fraud rate. This is likely to due the scarcity of those browsers. We can fix this by apply a minimum-instance-filter. Let's say 0.1 percent of data rows is our cut-off, then each category must have at least 144 instances to be included in our plot:""]",44,suoires1,fraud-detection-eda-and-modeling,suoires1_fraud-detection-eda-and-modeling_m44
['**Observation**: Opera and android browser have relatively high fraud rate'],45,suoires1,fraud-detection-eda-and-modeling,suoires1_fraud-detection-eda-and-modeling_m45
"['# Explore Numerical Features\n', '\n', 'I anticipate that most variables we will encounter would not follow a normal distribution. Therefore, for each variable, we will explore:\n', '\n', '1. Distribution\n', '\n', '2. Log of distribution\n', '\n', '3. Distribution by target variable\n', '\n', '4. Log of distribution by target variable\n', '\n', '5. Boxplot comparison between fraud and non-fraud\n', '\n', '## Examine Transaction Amount']",46,suoires1,fraud-detection-eda-and-modeling,suoires1_fraud-detection-eda-and-modeling_m46
"['**Observation**: \n', '    1. TransactionAmt has right-skewed distribution: most transactions are small (less than $200)\n', '    2. There is little difference between distribution and average amount for fraud and non-fraud']",47,suoires1,fraud-detection-eda-and-modeling,suoires1_fraud-detection-eda-and-modeling_m47
['## Examine Transaction DT'],48,suoires1,fraud-detection-eda-and-modeling,suoires1_fraud-detection-eda-and-modeling_m48
"['**Observation**: There is a large number of non-fraud transactions generated at a certain period . This discrepancy also causing the difference in our boxplot.\n', '\n', '**Possible Improvement**: I should try undersampling the period of non-fraud so that we have less imbalance issue for that particular period.']",49,suoires1,fraud-detection-eda-and-modeling,suoires1_fraud-detection-eda-and-modeling_m49
"['## Examine Distance 2\n', '\n', 'Dist1 contains no values. For dist2, we also running into two problems:\n', '\n', '1. Missing values:\n', '\n', '    Solution: keeping only the non-null rows in dist2.\n', '\n', '2. Zero values:\n', '\n', '    Zero values cause log transform to return infinity values\n', '\n', '    Solution: add small amount to 0s to avoid infinity\n', '    \n', '3. Negative values\n', '    \n', '    The logarithm is only defined for positive numbers. I could perhaps take the log(x+n), where n is the offset values that make the min negative value > 0. However, for such data 0 has a meaning (equality!) that should be respected. Unless I know the meaning of the data, I cannot make arbitrary transformation.\n', '    \n', '    Solution: no solution, omit the log-transformation graphs\n', '\n', ""Let's update our graphing function with this implementation\n""]",50,suoires1,fraud-detection-eda-and-modeling,suoires1_fraud-detection-eda-and-modeling_m50
"['**Observation**: Dist2 does not seem to varies between fraud and not-fraud\n', '\n', '## Examine C features\n', '\n', 'Same way of handling a large number of variables, I only choose the notable plots that reflect a large degree of variation. Trying to keep this kernel concised is one of my goals. In this case, I only consider C3 to have some significant patterns:']",51,suoires1,fraud-detection-eda-and-modeling,suoires1_fraud-detection-eda-and-modeling_m51
"['**Observation**: Higher values of C3 associated with no-fraud.\n', '\n', 'C5 and C9 are homogeneous columns.\n', '\n', '## Examine D features']",52,suoires1,fraud-detection-eda-and-modeling,suoires1_fraud-detection-eda-and-modeling_m52
"['## Conclusion for EDA:\n', '\n', '1. Target variable has class imbalance problem where instance of fraud is much lower than non-fraud\n', '\n', '2. Multiple columns contain too many missing values\n', '\n', '3. Several columns are homogeneous, therefore, prodvide no useful information in predicting the target variable (this may not be the case for transaction table since we are using a joined table)\n', '\n', '4. There is period of time where instances of non-fraud far exceed the usual proportion of non-fraud to fraud \n', '\n', '5. Basic understand of variables can help us do simple feature engineering\n', '\n', ""We will deal with each problem with the purpose of improving the prediction accuracy. But first, let's try a default XGBoost model provided by Vesta. We can use this model as a baseline to compare the improvement (or reduction) of each engineered feature, change, and alteration that  we made along the way.\n"", '\n', '## Brainstorm\n', 'Before treating this problem like a black box of ensemble learning, it\'s worthwhile to take our hands off the keyboards and think about the problem of fraud detection in a more ""open-box"" way. There are a lot of intersting questions worth investigating before diving into the madness of hyperparameters tuning. Insights that could lead to trivial and sometimes important questions. Questions that take us on a journey of curiosity and fulfilment. \n', '\n', 'For the data scientists whose minds love to wander. This section is dedicated for silly and serious questions alike.\n', '\n', ""**Scenario** :A Vesta executive storms in the office and excitedly tells everyone that an exciting project has fallen in their laps. It's the fraud detection problem. And he ask his people for some ideas of where to start, which features should be useful in prediciting fraud. He knows it is strange to ask the scientists before attempting any EDA or modeling. After all, they haven't seen a lick of relevant data. But he saids it would be great practive to dip the toe into the water before diving in without any direction. So let's start with the few things that were provided to us: transaction amount, time, card infor, identity, etc... Which information would give us a good start at cracking this problem?\n"", '\n', 'Let\'s define clearly what is a fraud transaction first. ""Fraud detection is a set of activities undertaken to prevent money or property from being obtained through false pretenses"" [Source](https://searchsecurity.techtarget.com/definition/fraud-detection). Most common type of frauds are forging checks or using stolen credit cards. If a person got of hold of your card info, what should he/she do with it? After browsing on Reddit, I found some crude scenarios:\n', '\n', ""1. If you drop your card, it's likely the person who found it by chance and commit a fraud would spend it on consumable and essential products like grocery and gas. The perp will likely go somewhere nearby and spend a larger amount than usual before the card get locked. So perhaps we should look at user's purchase history so that any activities or purchases that deviate from normal buying habit would stand out. But we don't have identifiable data, so we can't go on this route.\n"", '\n', ""2. If your information get hacked by careless purchases on some shady websites/gas stations, it's likely that your information will be sold to someone else who use your information for making fraud transaction. This person will make an online purchase and ship it to a distributor, who sells the good for cash and share the profit with the frauder. In this situation, the good is shipped to some far-away place from the user's home address. So the further the distance, the more a transaction looks like a fraud? No, of course not. People sends gifts all the times. But perhaps gifting 3 expensive laptops is slightly more suspicious than gifting a box of chocolate.\n"", '    \n', '    **Feature Engineering:** Combine transaction amount, type of good, and distance together.\n', '    \n', ""3. Fraud commited by someone close to you (family member: spouse, siblings, etc). It's rare, but it could happen.\n"", '\n', '4. Prefered tools for committing fraud. We have learned previously in the EDA that Protonmail has exceptionally high fraud rate >95%. A quick google reveal that Proton is a email service that provide free, anonymous, end-to-end encryption email accounts. Quote from Proton website: ""ProtonMail is incorporated in Switzerland and all our servers are located in Switzerland. This means all user data is protected by strict Swiss privacy laws"". Meaning fraud perpetrator not only protected by the full extend of the privacy law, but also doing it at no cost. Similarly, we have other tools that also have abnormally high fraud rate such as:\n', '\n', '    * Browser: Comodo IceDragon, Mozilla/Firefox?? (not firefox, but perhaps is Comodo IceDragon but recognized as another version of Firefox?)\n', '    \n', '    * Operating system: ""other"" category has fraud rate of 60%.\n', '    \n', '    * Phone (or browser?): Lanix Ilium\n', '    \n', '    **Feature Engineering:** New features that emphasize the importance of these tools\n', '    \n', '5. Time of operation. Just like any other jobs, frauders operate at routinely hours that perhaps different from the real users. It is strange, at least to me, to make purchase decision to buy an iphone at 3 in the morning. Again, without historical data, this approach is dead in the egg.\n', '\n', '\n', '# Baseline Model\n', '\n', 'Modeling section is being explore in another private notebook since only 1 GPU instance is allow in Kaggle Kernel...']",53,suoires1,fraud-detection-eda-and-modeling,suoires1_fraud-detection-eda-and-modeling_m53
[],54,suoires1,fraud-detection-eda-and-modeling,suoires1_fraud-detection-eda-and-modeling_m54
"['## Introduction\n', '\n', 'The aim of this competition is to predict whether a given transaction is fraudelent or not based on the information given regarding the transaction. Information such as the transaction amount, card type, product category, etc is given.\n', '\n', 'In this kernel, I will be visualizing and exploring these features in-depth using **seaborn**. I will attempt to make inferences and conlusions based on these graphs and plots.\n', '\n', 'Then, I will demonstrate how to build models to solve this problem. Specifically, I will be using LightGBM and Neural Network models. I will train these models on the training data, and then finally, I will show how to make predictions on the test data using these trained models.']",0,tarunpaparaju,ieee-cis-competition-fresh-eda-and-modeling,tarunpaparaju_ieee-cis-competition-fresh-eda-and-modeling_m0
"['<center><img src=""https://i.imgur.com/K3FE0NA.jpg"" width=""500px""></center>']",1,tarunpaparaju,ieee-cis-competition-fresh-eda-and-modeling,tarunpaparaju_ieee-cis-competition-fresh-eda-and-modeling_m1
"[""<font size=3 color='red'>Please upvote this kernel if you like it :)</font>""]",2,tarunpaparaju,ieee-cis-competition-fresh-eda-and-modeling,tarunpaparaju_ieee-cis-competition-fresh-eda-and-modeling_m2
['## Preparing the ground for analysis'],3,tarunpaparaju,ieee-cis-competition-fresh-eda-and-modeling,tarunpaparaju_ieee-cis-competition-fresh-eda-and-modeling_m3
['### Import necessary libraries'],4,tarunpaparaju,ieee-cis-competition-fresh-eda-and-modeling,tarunpaparaju_ieee-cis-competition-fresh-eda-and-modeling_m4
['### Check the files available in the dataset'],5,tarunpaparaju,ieee-cis-competition-fresh-eda-and-modeling,tarunpaparaju_ieee-cis-competition-fresh-eda-and-modeling_m5
['### Define the paths for the train and test data'],6,tarunpaparaju,ieee-cis-competition-fresh-eda-and-modeling,tarunpaparaju_ieee-cis-competition-fresh-eda-and-modeling_m6
['### Load the train and test datasets'],7,tarunpaparaju,ieee-cis-competition-fresh-eda-and-modeling,tarunpaparaju_ieee-cis-competition-fresh-eda-and-modeling_m7
['### Replace the NaNs with 0s and check the first few rows'],8,tarunpaparaju,ieee-cis-competition-fresh-eda-and-modeling,tarunpaparaju_ieee-cis-competition-fresh-eda-and-modeling_m8
['# EDA'],9,tarunpaparaju,ieee-cis-competition-fresh-eda-and-modeling,tarunpaparaju_ieee-cis-competition-fresh-eda-and-modeling_m9
"['## Transaction Amounts\n', '\n', 'As the name suggests, this is the amount of money transferred during the transaction, and this is clearly a continuous variable. I will visualize this feature in relation with the target, *isFraud*.\n', '\n']",10,tarunpaparaju,ieee-cis-competition-fresh-eda-and-modeling,tarunpaparaju_ieee-cis-competition-fresh-eda-and-modeling_m10
['### Distribution of transaction amounts for non-fraudelent and fraudulent cases'],11,tarunpaparaju,ieee-cis-competition-fresh-eda-and-modeling,tarunpaparaju_ieee-cis-competition-fresh-eda-and-modeling_m11
['### Violin Plot'],12,tarunpaparaju,ieee-cis-competition-fresh-eda-and-modeling,tarunpaparaju_ieee-cis-competition-fresh-eda-and-modeling_m12
"['In the violin plot above, the green violin is the distribution of transaction amounts for non-fraudulent samples and the red violin is that for fraudulent samples. It can be seen that both distributions have a strong positive (leftward) skew. But, the red violin has greater probability density towards the higher values of *TransactionAmt* as compared to the green distribution. \n', '\n', 'The green distribution has a very high probability denisty concentrated around the lower values of *TransactionAmt* and as a result, the probability density around the higher values of *TransactionAmt* is almost negligible. But, the red violin on the other hand, has lesser probability density concentrated around the lower values of *TransactionAmt* and thus, there is a considerable probability density around the higher values of *TransactionAmt*.\n', '\n', 'This happens because the green violin has multiple peaks (multimodal characteristic) around the lower values of *TransactionAmt*, whereas the red violin has only one clear peak in this region. This presence of only one peak in the red violin indicates that it has a much milder skew than the green violin. **Therefore, in general, the greater the transaction amount, the more likely it is for the transaction to be fraudulent.**\n', '\n', 'This makes intuitive sense, because very expensive transactions have a greater chance of being fraudulent than less expensive transactions.\n', '\n', '\n']",13,tarunpaparaju,ieee-cis-competition-fresh-eda-and-modeling,tarunpaparaju_ieee-cis-competition-fresh-eda-and-modeling_m13
['### Box Plot'],14,tarunpaparaju,ieee-cis-competition-fresh-eda-and-modeling,tarunpaparaju_ieee-cis-competition-fresh-eda-and-modeling_m14
"['The box plot above also suggests that more expensive transactions are more likely to be fraudulent. This can be inferred from the fact that the mean value of the red box is greater than that of the green box. Although the first quartiles of the two distributions are very similar, the third quartile of the red box is significantly greater than that of the green box, providing further evidence that higher transaction amounts are more likely to be fraudulent than not, *i.e.* **the greater the transaction amount, the more likely it is for the transaction to be fraudulent.** ']",15,tarunpaparaju,ieee-cis-competition-fresh-eda-and-modeling,tarunpaparaju_ieee-cis-competition-fresh-eda-and-modeling_m15
['## Product CD'],16,tarunpaparaju,ieee-cis-competition-fresh-eda-and-modeling,tarunpaparaju_ieee-cis-competition-fresh-eda-and-modeling_m16
"['I am not sure what Product CD exactly means, but it seems to be a categorical variable that provides some information regarding the category of a product. **Products in this dataset come under five broad categories: W, H, C, S, and R.** I will visualize this feature in relation with the target, *isFraud*.']",17,tarunpaparaju,ieee-cis-competition-fresh-eda-and-modeling,tarunpaparaju_ieee-cis-competition-fresh-eda-and-modeling_m17
['### Frequencies of the different product categories '],18,tarunpaparaju,ieee-cis-competition-fresh-eda-and-modeling,tarunpaparaju_ieee-cis-competition-fresh-eda-and-modeling_m18
"['From the above plot, we can clearly see that the most common *ProductCD* value is W. The other four product categories, H, C, S, and R are very rare compared to W. ']",19,tarunpaparaju,ieee-cis-competition-fresh-eda-and-modeling,tarunpaparaju_ieee-cis-competition-fresh-eda-and-modeling_m19
"['In the above proportion plot, the height of the dark green bar (at the top) represents the probability of a transaction involving a given product category being fraudulent. We can see that a product of category *C* is more likely to be involved in a fraudulent transaction as compared to any other product category. The next most fraudulence-prone product category is category *S*, and so on.\n', '\n']",20,tarunpaparaju,ieee-cis-competition-fresh-eda-and-modeling,tarunpaparaju_ieee-cis-competition-fresh-eda-and-modeling_m20
['### Distributions of transaction amounts for different *ProductCD* values (for non-fraudulent and fraudulent cases)'],21,tarunpaparaju,ieee-cis-competition-fresh-eda-and-modeling,tarunpaparaju_ieee-cis-competition-fresh-eda-and-modeling_m21
"['In the above violin plots, the light green sections represent the distribution for non-fraudulent cases and the dark green sections represent the distribution for fraudulent cases. We can see the same trend as before. Both the distributions types have strong positive (leftward) skews. But, the light green (non-fraudulent) distributions have a greater probability density concentrated around the lower values of transaction amounts, once again, because there are several peaks (unlike the dark green distributions).\n', '\n', 'But, there are a few exceptions to this trend in the above visualization. For example, the non-fraudulent and fraudulent distributions are very similar for the *C* product category. Both the distributions have only one peak and they look very similar in almost every way. Interestingly, the *C* product category also has the highest fraudulence rate, and this is probably the reason why the correlation between the transaction amount and the target is very low for this category.\n', '\n', 'But, for the rest of the product categories, the trend is roughly followed.\n', '\n', 'The distributions for the *S* product category seem to have the lowest means and the strongest skews. These distributions have a very high concentration of probability density around the lower values of *TransactionAmt*. Transactions of type *S* tend to have low transaction amounts. On the other side of the spectrum, the distributions for the *R* product category seem to have the highest means and the weakest skews. These distributions have a very even spread and almost no skew. Transactions of type *R* tend to have high transaction amounts.\n', '\n', 'Maybe, one can create a model that changes the weightage of the *TransactionAmt* feature based on the product category. For example, the weightage of the *TransactionAmt* feature can be reduced for the *C* product category, because the fraudulent and non-fraudulent distributions are very similar for this product category. ']",22,tarunpaparaju,ieee-cis-competition-fresh-eda-and-modeling,tarunpaparaju_ieee-cis-competition-fresh-eda-and-modeling_m22
"['The same trends and patterns can be seen in the box plots above. Both the distributions types have strong positive (leftward) skews. But, the light green (non-fraudulent) distributions clearly have lower means as compared to the dark green (fraudulent) distributions.\n', '\n', 'The same exceptions to this trend can also be seen in the above visualization. For example, the non-fraudulent and fraudulent distributions are very similar for the *C* product category. Both the distributions have similar means, first quartiles, and third quartiles.']",23,tarunpaparaju,ieee-cis-competition-fresh-eda-and-modeling,tarunpaparaju_ieee-cis-competition-fresh-eda-and-modeling_m23
['## P_emaildomain'],24,tarunpaparaju,ieee-cis-competition-fresh-eda-and-modeling,tarunpaparaju_ieee-cis-competition-fresh-eda-and-modeling_m24
"['The *P_emaildomain* represents the email domain through which the transaction was **payed**. The most common domains are *gmail.com, yahoo.com, hotmail.com, and anonymous.com*.']",25,tarunpaparaju,ieee-cis-competition-fresh-eda-and-modeling,tarunpaparaju_ieee-cis-competition-fresh-eda-and-modeling_m25
['### Frequencies of the different email domains (P)'],26,tarunpaparaju,ieee-cis-competition-fresh-eda-and-modeling,tarunpaparaju_ieee-cis-competition-fresh-eda-and-modeling_m26
"['From the above plot, we can clearly see that the most common *P_emaildomain* value is *gmail.com*. The next most common email domain (for payment) is *yahoo.com* and the rest are comparatively very rare. ']",27,tarunpaparaju,ieee-cis-competition-fresh-eda-and-modeling,tarunpaparaju_ieee-cis-competition-fresh-eda-and-modeling_m27
['### Fraudulence Proportion Plot'],28,tarunpaparaju,ieee-cis-competition-fresh-eda-and-modeling,tarunpaparaju_ieee-cis-competition-fresh-eda-and-modeling_m28
"['In the above proportion plot, the height of the dark blue bar (at the top) represents the probability of a transaction made through a given email domain being fraudulent. We can see that an email domain of *hotmail.com* is associated with the highest probability of a fraudulent transaction. The next most fraudulence-prone email domain is *gmail.com*. And, the least fraudulence-prone email domain is *yahoo.com*.  ']",29,tarunpaparaju,ieee-cis-competition-fresh-eda-and-modeling,tarunpaparaju_ieee-cis-competition-fresh-eda-and-modeling_m29
['### TransactionAmt vs. P_emaildomain Violin Plot'],30,tarunpaparaju,ieee-cis-competition-fresh-eda-and-modeling,tarunpaparaju_ieee-cis-competition-fresh-eda-and-modeling_m30
"['In the above violin plots, the light blue sections represent the distribution for non-fraudulent cases and the dark blue sections represent the distribution for fraudulent cases. We can see the same trend as before. Both the distributions types have strong positive (leftward) skews. But, the light blue (non-fraudulent) distributions have a greater probability density concentrated around the lower values of transaction amounts, once again, because there are several peaks (unlike the dark blue distributions).\n', '\n', 'There are no real exceptions to this trend except for *hotmail.com*. The distributions for transaction amount (for fraudulent and non-fraudulent cases) are very similar at *hotmail.com*. The other domains, namely, *gmail.com, yahoo.com, and anonymous.com* follow the general trend.\n', '\n', ""Each email domain's distributions are roughly concentrated around the same mean, but with differing variances. No single email domain has a much greater or lower mean than the other email domains.\n"", '\n', '**Interestingly, once again, the most fraudulence-prone email domain, namely, *hotmail.com* is also the one that does not follow the trend. This is similar to the *C* product category, which was the most fraudulent category and was also the category that did not follow the trend.** \n', ' \n', '\n', 'Maybe, one can create a model that changes the weightage of the *TransactionAmt* feature based on the email domain. For example, the weightage of the *TransactionAmt* feature can be reduced for the *hotmail.com* email domain, because the fraudulent and non-fraudulent distributions are very similar for this email domain. ']",31,tarunpaparaju,ieee-cis-competition-fresh-eda-and-modeling,tarunpaparaju_ieee-cis-competition-fresh-eda-and-modeling_m31
['### TransactionAmt vs. P_emaildomain Box Plot '],32,tarunpaparaju,ieee-cis-competition-fresh-eda-and-modeling,tarunpaparaju_ieee-cis-competition-fresh-eda-and-modeling_m32
"['The same trends and patterns can be seen in the box plots above. Both the distributions types have strong positive (leftward) skews. But, the light blue (non-fraudulent) distributions clearly have lower means as compared to the dark blue (fraudulent) distributions.\n', '\n', 'The same exceptions to this trend can also be seen in the above visualization. For example, the non-fraudulent and fraudulent distributions are very similar for the *hotmail.com* email domain. Both the distributions have similar means and first quartiles. Although the third quartiles for the *anonymous.com* distributions are very similar, their means are very different.']",33,tarunpaparaju,ieee-cis-competition-fresh-eda-and-modeling,tarunpaparaju_ieee-cis-competition-fresh-eda-and-modeling_m33
['## R_emaildomain'],34,tarunpaparaju,ieee-cis-competition-fresh-eda-and-modeling,tarunpaparaju_ieee-cis-competition-fresh-eda-and-modeling_m34
"['The *R_emaildomain* represents the email domain through which the transaction was **received**. The most common domains are *gmail.com, yahoo.com, hotmail.com, and anonymous.com*. ']",35,tarunpaparaju,ieee-cis-competition-fresh-eda-and-modeling,tarunpaparaju_ieee-cis-competition-fresh-eda-and-modeling_m35
['### Frequencies of the different email domains (R)'],36,tarunpaparaju,ieee-cis-competition-fresh-eda-and-modeling,tarunpaparaju_ieee-cis-competition-fresh-eda-and-modeling_m36
"['From the above plot, we can clearly see that the most common *R_emaildomain* value is *gmail.com*. The next most common email domain is *hotmail.com*. The least common receiver email domain is *yahoo.com*.']",37,tarunpaparaju,ieee-cis-competition-fresh-eda-and-modeling,tarunpaparaju_ieee-cis-competition-fresh-eda-and-modeling_m37
['### Fraudulence Proportion Plot'],38,tarunpaparaju,ieee-cis-competition-fresh-eda-and-modeling,tarunpaparaju_ieee-cis-competition-fresh-eda-and-modeling_m38
"['In the above proportion plot, the height of the dark red bar (at the top) represents the probability of a transaction received through a given email domain being fraudulent. We can see that an email domain of *gmail.com* is associated with the highest probability of a fraudulent transaction. The next most fraudulence-prone email domain is *hotmail.com*. And, the least fraudulence-prone email domain is *anonymous.com*.']",39,tarunpaparaju,ieee-cis-competition-fresh-eda-and-modeling,tarunpaparaju_ieee-cis-competition-fresh-eda-and-modeling_m39
['### TransactionAmt vs. R_emaildomain  Violin Plot'],40,tarunpaparaju,ieee-cis-competition-fresh-eda-and-modeling,tarunpaparaju_ieee-cis-competition-fresh-eda-and-modeling_m40
"['In the above violin plots, the pink sections represent the distribution for non-fraudulent cases and the red sections represent the distribution for fraudulent cases. We can see the same trend as before. Both the distributions types have strong positive (leftward) skews. But, the pink (non-fraudulent) distributions have a greater probability density concentrated around the lower values of transaction amounts, once again, because there are several peaks (unlike the red distributions).\n', '\n', 'The exception to this trend is *anonymous.com*. The distributions for transaction amount (at *anonymous.com*) is much more skewed in the fraudulent case as compared to the non-fraudulent case. The other domains, namely, *gmail.com, yahoo.com, and hotmail.com* follow the general trend.\n', '\n', ""Each email domain's distributions are roughly concentrated around the same mean, but with differing variances. No single email domain has a much greater or lower mean than the other email domains.\n"", '\n', 'Maybe, one can create a model that changes the weightage of the *TransactionAmt* feature based on the email domain. For example, the weightage of the *TransactionAmt* feature can be reduced for the *hotmail.com* email domain, because the fraudulent and non-fraudulent distributions are very similar for this email domain. ']",41,tarunpaparaju,ieee-cis-competition-fresh-eda-and-modeling,tarunpaparaju_ieee-cis-competition-fresh-eda-and-modeling_m41
['### TransactionAmt vs. R_emaildomain  Box Plot'],42,tarunpaparaju,ieee-cis-competition-fresh-eda-and-modeling,tarunpaparaju_ieee-cis-competition-fresh-eda-and-modeling_m42
"['The same trends and patterns can be seen in the box plots above. Both the distributions types have strong positive (leftward) skews. But, the light blue (non-fraudulent) distributions clearly have lower means as compared to the dark blue (fraudulent) distributions.\n', '\n', 'The same exceptions to this trend can also be seen in the above visualization. For example, at *anonymous.com*, the fraudulent distribution has a much lower mean than the non-fraudulent one (counterintuitively). Also, the non-fraudulent and fraudulent distributions are very similar for the *hotmail.com* email domain. Both the distributions have similar means, quartiles, minima, and maxima.']",43,tarunpaparaju,ieee-cis-competition-fresh-eda-and-modeling,tarunpaparaju_ieee-cis-competition-fresh-eda-and-modeling_m43
['## Card brand (card4)'],44,tarunpaparaju,ieee-cis-competition-fresh-eda-and-modeling,tarunpaparaju_ieee-cis-competition-fresh-eda-and-modeling_m44
"['The card4 feature represents the brand of the card through which the transaction was made. The card brands in this dataset are *discover, mastercard, visa,* and *american express*.']",45,tarunpaparaju,ieee-cis-competition-fresh-eda-and-modeling,tarunpaparaju_ieee-cis-competition-fresh-eda-and-modeling_m45
['### Frequencies of the different card brands'],46,tarunpaparaju,ieee-cis-competition-fresh-eda-and-modeling,tarunpaparaju_ieee-cis-competition-fresh-eda-and-modeling_m46
"['From the above plot, it is clear that the most common card brand used for transactions is *visa*. The second most common card brand is *mastercard*. The remaining two card brands, namely, *discover* and *american express* are very rare (comparatively).']",47,tarunpaparaju,ieee-cis-competition-fresh-eda-and-modeling,tarunpaparaju_ieee-cis-competition-fresh-eda-and-modeling_m47
['### Fraudulence Proportion Plot'],48,tarunpaparaju,ieee-cis-competition-fresh-eda-and-modeling,tarunpaparaju_ieee-cis-competition-fresh-eda-and-modeling_m48
"['In the above proportion plot, the height of the dark orange bar (at the top) represents the probability of a transaction received through a given card brand being fraudulent. We can see that *discover* cards are associated with the highest probability of a fraudulent transaction. The remaining card brands have a very similar fraudelence proportion.']",49,tarunpaparaju,ieee-cis-competition-fresh-eda-and-modeling,tarunpaparaju_ieee-cis-competition-fresh-eda-and-modeling_m49
['### TransactionAmt vs. card4 Violin Plot '],50,tarunpaparaju,ieee-cis-competition-fresh-eda-and-modeling,tarunpaparaju_ieee-cis-competition-fresh-eda-and-modeling_m50
"['In the above violin plots, the light orange sections represent the distribution for non-fraudulent cases and the dark orange sections represent the distribution for fraudulent cases. We can see the same trend as before. Both the distributions types have strong positive (leftward) skews. But, the light orange (non-fraudulent) distributions have a greater probability density concentrated around the lower values of transaction amounts, once again, because they have several peaks (unlike the dark orange distributions).\n', '\n', 'There do not seem to be any exceptions to this trend. The distributions for *american express* and *discover* cards seem to have higher means as compared to the other two card types. Therefore, more expensive transactions tend to take place through *american express* and *discover* cards.']",51,tarunpaparaju,ieee-cis-competition-fresh-eda-and-modeling,tarunpaparaju_ieee-cis-competition-fresh-eda-and-modeling_m51
['### TransactionAmt vs. card4 Box Plot '],52,tarunpaparaju,ieee-cis-competition-fresh-eda-and-modeling,tarunpaparaju_ieee-cis-competition-fresh-eda-and-modeling_m52
"['In the box plot above, we can say that, for *mastercard* and *visa* cards, the fraudulent and non-fraudulent distributions are very similar. They have very similar means. It is also clear in this box plot, that *discover* and *american express* cards tend to be associated with higher transaction amounts. This can be inferred from the fact that they have much higher means as compared to the other two card brands.']",53,tarunpaparaju,ieee-cis-competition-fresh-eda-and-modeling,tarunpaparaju_ieee-cis-competition-fresh-eda-and-modeling_m53
['## Card type (card6)'],54,tarunpaparaju,ieee-cis-competition-fresh-eda-and-modeling,tarunpaparaju_ieee-cis-competition-fresh-eda-and-modeling_m54
['The card6 feature represents the btype of the card through which the transaction was made. The major card types in this dataset are *credit* and *debit*.'],55,tarunpaparaju,ieee-cis-competition-fresh-eda-and-modeling,tarunpaparaju_ieee-cis-competition-fresh-eda-and-modeling_m55
['### Frequencies of the different card types'],56,tarunpaparaju,ieee-cis-competition-fresh-eda-and-modeling,tarunpaparaju_ieee-cis-competition-fresh-eda-and-modeling_m56
['The two most common card types in this dataset are credit and debit (the rest are comparatively negligible). Debit cards are more common than credit cards in this dataset.'],57,tarunpaparaju,ieee-cis-competition-fresh-eda-and-modeling,tarunpaparaju_ieee-cis-competition-fresh-eda-and-modeling_m57
['### Fraudulence Proportion Plot'],58,tarunpaparaju,ieee-cis-competition-fresh-eda-and-modeling,tarunpaparaju_ieee-cis-competition-fresh-eda-and-modeling_m58
"['In the above proportion plot, the height of the dark purple bar (at the top) represents the probability of a transaction received through a given card type being fraudulent. We can see that a given *credit* card is more likely involved in a fraudulent transaction as compared to a given *debit card*.']",59,tarunpaparaju,ieee-cis-competition-fresh-eda-and-modeling,tarunpaparaju_ieee-cis-competition-fresh-eda-and-modeling_m59
['### TransactionAmt vs. card6 Violin Plot'],60,tarunpaparaju,ieee-cis-competition-fresh-eda-and-modeling,tarunpaparaju_ieee-cis-competition-fresh-eda-and-modeling_m60
"['In the above violin plots, the light purple sections represent the distribution for non-fraudulent cases and the dark purple sections represent the distribution for fraudulent cases. We can see the same trend as before. Both the distributions types have strong positive (leftward) skews. But, the light purple (non-fraudulent) distributions have a greater probability density concentrated around the lower values of transaction amounts, once again, because they have several peaks (unlike the dark purple distributions).\n', '\n', 'In the above violin plot, it can be seen that debit cards cards are associated with lower transaction amounts as compared to credit cards because their distributions have stronger positive (leftward) skew. Also, the credit card distributions have higher means.']",61,tarunpaparaju,ieee-cis-competition-fresh-eda-and-modeling,tarunpaparaju_ieee-cis-competition-fresh-eda-and-modeling_m61
['### TransactionAmt vs. card6 Box Plot'],62,tarunpaparaju,ieee-cis-competition-fresh-eda-and-modeling,tarunpaparaju_ieee-cis-competition-fresh-eda-and-modeling_m62
"['In the box plot above, we can see that credit cards are associated with much higher mean transaction amounts as compared to debit cards. ']",63,tarunpaparaju,ieee-cis-competition-fresh-eda-and-modeling,tarunpaparaju_ieee-cis-competition-fresh-eda-and-modeling_m63
"['## ""M"" Features']",64,tarunpaparaju,ieee-cis-competition-fresh-eda-and-modeling,tarunpaparaju_ieee-cis-competition-fresh-eda-and-modeling_m64
"['I am not sure about the meanings of these features (M1, M2, and so on till M9), but the competition overview states that these are categorical features.']",65,tarunpaparaju,ieee-cis-competition-fresh-eda-and-modeling,tarunpaparaju_ieee-cis-competition-fresh-eda-and-modeling_m65
"['### Fraudulence Proportion Plots of ""M"" features']",66,tarunpaparaju,ieee-cis-competition-fresh-eda-and-modeling,tarunpaparaju_ieee-cis-competition-fresh-eda-and-modeling_m66
['### M1'],67,tarunpaparaju,ieee-cis-competition-fresh-eda-and-modeling,tarunpaparaju_ieee-cis-competition-fresh-eda-and-modeling_m67
"['From the above proportion plot, it can be seen that the most fraudulence prone M1 value is 0.0. This is because all the NaN values were converted to 0s in the beginning. Therefore, an undefined M1 value has the highest chance of being fraudulent. Besides this, a sample with an M1 value of T has a much higher chance of being fraudulent than one with an M1 value of F.']",68,tarunpaparaju,ieee-cis-competition-fresh-eda-and-modeling,tarunpaparaju_ieee-cis-competition-fresh-eda-and-modeling_m68
['### M2'],69,tarunpaparaju,ieee-cis-competition-fresh-eda-and-modeling,tarunpaparaju_ieee-cis-competition-fresh-eda-and-modeling_m69
"['From the above proportion plot, it can be seen that the most fraudulence prone M2 value is 0.0. This is because all the NaN values were converted to 0s in the beginning. Therefore, an undefined M2 value has the highest chance of being fraudulent. Besides this, a sample with an M2 value of F has a much higher chance of being fraudulent than one with an M2 value of T.']",70,tarunpaparaju,ieee-cis-competition-fresh-eda-and-modeling,tarunpaparaju_ieee-cis-competition-fresh-eda-and-modeling_m70
['### M3'],71,tarunpaparaju,ieee-cis-competition-fresh-eda-and-modeling,tarunpaparaju_ieee-cis-competition-fresh-eda-and-modeling_m71
"['From the above proportion plot, it can be seen that the most fraudulence prone M3 value is 0.0. This is because all the NaN values were converted to 0s in the beginning. Therefore, an undefined M3 value has the highest chance of being fraudulent. Besides this, a sample with an M3 value of F has a much higher chance of being fraudulent than one with an M3 value of T.']",72,tarunpaparaju,ieee-cis-competition-fresh-eda-and-modeling,tarunpaparaju_ieee-cis-competition-fresh-eda-and-modeling_m72
['### M4'],73,tarunpaparaju,ieee-cis-competition-fresh-eda-and-modeling,tarunpaparaju_ieee-cis-competition-fresh-eda-and-modeling_m73
"['From the above proportion plot, it can be seen that a that a sample with an M4 value of M2 has a much higher chance of being fraudulent than one with an M4 value of M0. Also, a sample with an M4 value of M0 has a higher chance of being fraudulent than one with an M4 value of M1. An M4 value of 0.0 (or NaN) has the lowest chance of being fraudulent.']",74,tarunpaparaju,ieee-cis-competition-fresh-eda-and-modeling,tarunpaparaju_ieee-cis-competition-fresh-eda-and-modeling_m74
['### M5'],75,tarunpaparaju,ieee-cis-competition-fresh-eda-and-modeling,tarunpaparaju_ieee-cis-competition-fresh-eda-and-modeling_m75
"['From the above proportion plot, it can be seen that the most fraudulence prone M5 value is 0.0. This is because all the NaN values were converted to 0s in the beginning. Therefore, an undefined M5 value has the highest chance of being fraudulent. Besides this, a sample with an M5 value of T has a much higher chance of being fraudulent than one with an M5 value of F.']",76,tarunpaparaju,ieee-cis-competition-fresh-eda-and-modeling,tarunpaparaju_ieee-cis-competition-fresh-eda-and-modeling_m76
['### M6'],77,tarunpaparaju,ieee-cis-competition-fresh-eda-and-modeling,tarunpaparaju_ieee-cis-competition-fresh-eda-and-modeling_m77
"['From the above proportion plot, it can be seen that the most fraudulence prone M6 value is 0.0. This is because all the NaN values were converted to 0s in the beginning. Therefore, an undefined M6 value has the highest chance of being fraudulent. Besides this, a sample with an M6 value of F has a much higher chance of being fraudulent than one with an M6 value of T.']",78,tarunpaparaju,ieee-cis-competition-fresh-eda-and-modeling,tarunpaparaju_ieee-cis-competition-fresh-eda-and-modeling_m78
['### M7'],79,tarunpaparaju,ieee-cis-competition-fresh-eda-and-modeling,tarunpaparaju_ieee-cis-competition-fresh-eda-and-modeling_m79
"['From the above proportion plot, it can be seen that the most fraudulence prone M7 value is 0.0. This is because all the NaN values were converted to 0s in the beginning. Therefore, an undefined M7 value has the highest chance of being fraudulent. Besides this, a sample with an M7 value of F has a very similar chance of being fraudulent to one with an M7 value of T.']",80,tarunpaparaju,ieee-cis-competition-fresh-eda-and-modeling,tarunpaparaju_ieee-cis-competition-fresh-eda-and-modeling_m80
['### M8'],81,tarunpaparaju,ieee-cis-competition-fresh-eda-and-modeling,tarunpaparaju_ieee-cis-competition-fresh-eda-and-modeling_m81
"['From the above proportion plot, it can be seen that the most fraudulence prone M8 value is 0.0. This is because all the NaN values were converted to 0s in the beginning. Therefore, an undefined M8 value has the highest chance of being fraudulent. Besides this, a sample with an M8 value of T has a very similar chance of being fraudulent to one with an M8 value of F.']",82,tarunpaparaju,ieee-cis-competition-fresh-eda-and-modeling,tarunpaparaju_ieee-cis-competition-fresh-eda-and-modeling_m82
['### M9'],83,tarunpaparaju,ieee-cis-competition-fresh-eda-and-modeling,tarunpaparaju_ieee-cis-competition-fresh-eda-and-modeling_m83
"['From the above proportion plot, it can be seen that the most fraudulence prone M9 value is 0.0. This is because all the NaN values were converted to 0s in the beginning. Therefore, an undefined M9 value has the highest chance of being fraudulent. Besides this, a sample with an M9 value of F has a much higher chance of being fraudulent than one with an M9 value of T.']",84,tarunpaparaju,ieee-cis-competition-fresh-eda-and-modeling,tarunpaparaju_ieee-cis-competition-fresh-eda-and-modeling_m84
['# Modeling'],85,tarunpaparaju,ieee-cis-competition-fresh-eda-and-modeling,tarunpaparaju_ieee-cis-competition-fresh-eda-and-modeling_m85
['## Preparing the training and testing data'],86,tarunpaparaju,ieee-cis-competition-fresh-eda-and-modeling,tarunpaparaju_ieee-cis-competition-fresh-eda-and-modeling_m86
['### Define the categorical columns'],87,tarunpaparaju,ieee-cis-competition-fresh-eda-and-modeling,tarunpaparaju_ieee-cis-competition-fresh-eda-and-modeling_m87
['### Convert categorical string data into numerical format'],88,tarunpaparaju,ieee-cis-competition-fresh-eda-and-modeling,tarunpaparaju_ieee-cis-competition-fresh-eda-and-modeling_m88
['### Create final train and validation arrays'],89,tarunpaparaju,ieee-cis-competition-fresh-eda-and-modeling,tarunpaparaju_ieee-cis-competition-fresh-eda-and-modeling_m89
['## LightGBM'],90,tarunpaparaju,ieee-cis-competition-fresh-eda-and-modeling,tarunpaparaju_ieee-cis-competition-fresh-eda-and-modeling_m90
['### Build and train LightGBM model'],91,tarunpaparaju,ieee-cis-competition-fresh-eda-and-modeling,tarunpaparaju_ieee-cis-competition-fresh-eda-and-modeling_m91
"['**This trained model can be used to make predictions on the training data using:**\n', '\n', '> model.predict(X_test)']",92,tarunpaparaju,ieee-cis-competition-fresh-eda-and-modeling,tarunpaparaju_ieee-cis-competition-fresh-eda-and-modeling_m92
['### Visualize feature importances'],93,tarunpaparaju,ieee-cis-competition-fresh-eda-and-modeling,tarunpaparaju_ieee-cis-competition-fresh-eda-and-modeling_m93
"['The above diagram ranks the features based on importance. The features given the most weightage during prediction are considered more ""important"". The features at the top are the most important and the ones at the bottom are the least important for this LightGBM model.']",94,tarunpaparaju,ieee-cis-competition-fresh-eda-and-modeling,tarunpaparaju_ieee-cis-competition-fresh-eda-and-modeling_m94
['## Neural Networks'],95,tarunpaparaju,ieee-cis-competition-fresh-eda-and-modeling,tarunpaparaju_ieee-cis-competition-fresh-eda-and-modeling_m95
['### Build Neural Network model'],96,tarunpaparaju,ieee-cis-competition-fresh-eda-and-modeling,tarunpaparaju_ieee-cis-competition-fresh-eda-and-modeling_m96
"['This is a simple neural network model that consists of two hidden layers with 10 neurons each. The activation function used is ReLU and the activation function for the last layer is sigmoid because a probability between 0 and 1 needs to given as output.\n', '\n', '**Steps:**\n', '\n', '* Pass the input vector through a dense layer with 10 neurons\n', '* Pass the output of that layer through another dense layer with 10 neurons\n', '* Finally, pass the output of the previous layer through a dense layer with one neuron with a sigmoid activation function (to output a probability between 0 and 1)']",97,tarunpaparaju,ieee-cis-competition-fresh-eda-and-modeling,tarunpaparaju_ieee-cis-competition-fresh-eda-and-modeling_m97
['### Check model summary'],98,tarunpaparaju,ieee-cis-competition-fresh-eda-and-modeling,tarunpaparaju_ieee-cis-competition-fresh-eda-and-modeling_m98
['### Visualize the model architecture'],99,tarunpaparaju,ieee-cis-competition-fresh-eda-and-modeling,tarunpaparaju_ieee-cis-competition-fresh-eda-and-modeling_m99
['### Train the Neural Network'],100,tarunpaparaju,ieee-cis-competition-fresh-eda-and-modeling,tarunpaparaju_ieee-cis-competition-fresh-eda-and-modeling_m100
"['**This trained model can be used to make predictions on the training data using:**\n', '\n', '> model.predict(X_test)']",101,tarunpaparaju,ieee-cis-competition-fresh-eda-and-modeling,tarunpaparaju_ieee-cis-competition-fresh-eda-and-modeling_m101
['### Visualize change in accuracy'],102,tarunpaparaju,ieee-cis-competition-fresh-eda-and-modeling,tarunpaparaju_ieee-cis-competition-fresh-eda-and-modeling_m102
['It can be clearly seen from the above plot that the training and validation accuracies are increasing as the training process proceeds.'],103,tarunpaparaju,ieee-cis-competition-fresh-eda-and-modeling,tarunpaparaju_ieee-cis-competition-fresh-eda-and-modeling_m103
['### Visualize change in loss'],104,tarunpaparaju,ieee-cis-competition-fresh-eda-and-modeling,tarunpaparaju_ieee-cis-competition-fresh-eda-and-modeling_m104
['It can be clearly seen from the above plot that the training and validation losses (binary cross-entropy) are decreasing as the training process proceeds.'],105,tarunpaparaju,ieee-cis-competition-fresh-eda-and-modeling,tarunpaparaju_ieee-cis-competition-fresh-eda-and-modeling_m105
['# Ending note'],106,tarunpaparaju,ieee-cis-competition-fresh-eda-and-modeling,tarunpaparaju_ieee-cis-competition-fresh-eda-and-modeling_m106
"['This concludes my EDA and modeling kernel. I hope that this kernel was helpful for you to understand the data better. Please post any feedback or comments regarding this kernel below in the comments section.\n', '\n', ""**Thanks for reading this kernel and please don't forget to upvote if you liked it :)**\n"", '\n']",107,tarunpaparaju,ieee-cis-competition-fresh-eda-and-modeling,tarunpaparaju_ieee-cis-competition-fresh-eda-and-modeling_m107
['# Introduction'],0,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m0
"['<center><img src=""https://i.imgur.com/Hh0CekB.jpg"" width=""500px""></center> ']",1,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m1
"['Many participants in Kaggle competitions often try to increase their scores on the public leaderboard (LB) again and again, so that they stay ahead in the race for a medal. Some of these participants succeed in their endeavour and manage to stay at the top of the public LB. But, suddenly, when the private LB is revealed, a lot of these ""table toppers"" drop to low positions on the new leaderboard and miss out on a medal. But, on the other hand, many people who were low on the public LB jump up to high positions on the private LB. This phenomenon is called a **shakeup**.\n', '\n', 'Shakeups are most often caused by something called **overfitting** in machine learning. So, in this kernel, I will be explaining the most effective methods to identify and reduce overfitting, so that you can ensure a high place on the private LB and possibly even win a medal! Additionally, I will  be suggesting articles, blogs, and videos that explain these methods in greater detail.\n', '\n']",2,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m2
"['<font size=""3"" color=""red""> Please upvote this kernel if you like it. It motivates me to produce more quality content :) </font>']",3,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m3
['# Acknowledgements'],4,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m4
['* [@tearth](https://www.kaggle.com/tearth) for the meme above :)'],5,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m5
"['# Import libraries and prepare the data\n', '\n', '#### (Click CODE on the right side)']",6,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m6
['# What is overfitting?'],7,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m7
"['<center><img src=""https://i.imgur.com/dJVkotI.jpg"" width=""500px""></center>']",8,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m8
"['Overfitting is the tendency of a machine learning model to perform really well on the known training data, and then perform significantly worse on the testing data. **This happens when the model memorizes the data instead of understanding the hidden patterns in the data.** This means that the model learns the training data so precisely, that it loses the ability to adapt and use its knowledge on new data.\n', '\n', 'This is one form of overfitting, but the main type of overfitting that causes shakeups is **test data overfitting**. This happens when the model performs very well on the public test data, but underperforms on the private test data. This is a result of **public LB probing**, where the participant tries to increase the the public LB score as much as possible, but the model performs poorly on the private LB because it is essentially memorizing the public test data.\n', '\n', 'On the other hand, underfitting is when the model is not even able to fit the training data properly. Therefore, both the training and testing accuracis are low in the case of underfitting.']",9,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m9
['## A video explaining overfitting'],10,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m10
['Here is a video by Udacity explaining what overfitting is:'],11,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m11
['## Resources for understanding overfitting'],12,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m12
"['* [Overfitting and Underfitting With Machine Learning Algorithms](https://machinelearningmastery.com/overfitting-and-underfitting-with-machine-learning-algorithms/) ~ by Machine Learning Mastery\n', '* [Overfitting vs. Underfitting](https://towardsdatascience.com/overfitting-vs-underfitting-a-complete-example-d05dd7e19765) ~ by Will Koehrsen\n', '* [Overfitting vs. Underfitting: A Conceptual Explanation](https://towardsdatascience.com/overfitting-vs-underfitting-a-conceptual-explanation-d94ee20ca7f9) ~ by Will Koehrsen']",13,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m13
['### Takeaways from this section'],14,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m14
"[""* Overfitting is when a model's accuracy is significantly higher on the training data than the testing data.\n"", '* Overfitting can also happen when one fits the private test data significantly worse than the public test data.\n', '* When a model overfits, it tends to memorize the data instead of understanding patterns in the data.']",15,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m15
['# How can we identify overfitting?'],16,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m16
"['Overfitting is a fairly simple concept to understand, but understanding how to identify and correct overfitting are very difficult tasks. A data scientist can get better at these things only by continuous practice. But, I can give some tips regarding how you can identify if your model is overfitting or not.']",17,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m17
['## Validation'],18,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m18
"[""Validation is a way of checking how your model performs on new, unseen data. In validation, one samples a small percentage of the data points in the training data (usually around 20%) and sets in aside. The remaining samples are used to train the model and the chosen samples are used to evaluate the model. After each iteration of training, the model's performance is evaluated on this validation data to check how the model performs on data that it has never seen before.\n"", '\n', 'Note that the validation samples need to be selected randomly so that the validation score actually reflects how well the model may do on unseen data (*i.e.* the public and private testing datasets).']",19,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m19
['### Demonstration'],20,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m20
"['Now, I will demonstrate how you can add validation to a neural network model.']",21,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m21
['### Build neural network'],22,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m22
['### Visualize the model architecture'],23,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m23
['### Split the data into training and validation sets'],24,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m24
['### Train the model'],25,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m25
['### Plot the loss and accuracy statistics '],26,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m26
['### Accuracy'],27,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m27
"['In the plot above, we can see how the validation and training accuracies are changing. The training accuracy ends up just above the validation accuracy at the last epoch. This shows that the model is learning well because the gap between the training and validation accuracies is small. This means that the data performs at almost the same level on unseen data. In this way, we can use validation to check whether our model is overfitting or not.']",28,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m28
"['We can see the same insight in the bar plot above. The training accuracy is initially lower than the validation accuracy, but it eventually catches up, and ends up just above the validation accuracy at the end. This is a sign of a model that is not overfitting (training accuracy only just above validation accuracy).']",29,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m29
['### Loss'],30,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m30
"['In the plot above, we can see how the validation and training losses are changing. The losses end up in close proximity towards the last few epochs. This shows that the training and validation losses are both decreasing together in sync with each other. Once again, it is an indication that there is no clear overfitting.']",31,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m31
['We can observe the same insight from the bar plot above. The losses get closer and closer to each other as the epochs increase. This is a sign of healthy training without overfitting.'],32,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m32
['### Cross validation'],33,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m33
"['Cross validation is a special type of validation that makes use of the entire training data to evaluate the model properly, so that as much randomness as possible can be introduced. In cross validation, the training dataset is divided into a certain number of **folds**. These folds are subsets of the original training data. Each fold has an equal number of randomly selected data samples. \n', '\n', 'One fold is selected and set aside. The remaining folds are used to train the model, and the selected fold is used to evaluate the performance of the model at each iteration. Now, the next fold is selected and the same process repeats. We repeat this process until each fold has acted as a validation set once. \n', '\n', 'This process is called **cross-validation**. Cross validation is helpful because the entire training dataset is being used to validate the model. This means that we are introducing more randomness and variety into our validation set instead of restricting ourselves to one fixed validation set. **This gives a more reliable idea of how the model would perform on real testing data.**']",34,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m34
['### Types of cross validation'],35,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m35
['There are many types of cross validation that are used for different types of data. Here are a few common types of cross validation:'],36,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m36
"['* **Time-series split** : For data with a time dimension\n', '* **Stratified K-Fold** : For data with a categorical target (like this competition)\n', '* **K-Fold** : For data with a continuous target']",37,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m37
"['Cross validation is a very good way to diagnose overfitting. In fact, the gap between your cross validation score and public LB score can be used as a way to determine whether your model is overfitting or not. So, **looking at only the public LB score or only the cross validation score may not be that useful, but looking at them together will give a clearer picture**.\n', '\n', 'Let me illustrate this point with a simple example. Assume that the statistics given below are the accuracy scores for two different models. The first row corresponds to the first and the second one to the second model.']",38,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m38
"['|        | Public LB Score   |  Cross Validation Score    |\n', '| ------ | ------ |------|\n', '|Model 1 | 97 %  |  97.5 %    |\n', '| Model 2 | 97.5 % | 99 %']",39,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m39
"['From the above table of scores, it may look like model 2 is a better choice for private LB submission because it has a higher CV and public LB score as compared to model 2. But, this is actually a misleading illusion.\n', '\n', 'If you look at the numbers closely, the error percentage in model 1 is 2.5% for cross validation and 3% for public LB. These error percentages are somewhat similar. But, if you look at the error percentages in model 2, they are 1% for cross validation and 2.5% for public LB. This is a comparatively massive gap between the CV and public LB scores. The public LB error percentage for model 2 is 2.5 times larger than the cross validation error. This shows that model 2 is overfitting to the cross validation data, and is not improving much on its test data performance. So, model 1 is more likely to perform better on a new private test dataset because it has very good CV and LB accuracies, and at the same, the gap between CV and LB is not very large. To summarize, **never go for a very small increase in LB score when the increase in CV score is very large, because this may lead to overfitting**.\n', '\n', 'So, look for an increase in CV score, but make sure that the gap between CV and LB remains small. This idea is excellently explained by [@cpmpml](https://kaggle.com/cpmpml) in his talk at Kaggle days:']",40,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m40
['### Resources for understanding validation'],41,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m41
"['* [About Train, Validation and Test Sets in Machine Learning](https://towardsdatascience.com/train-validation-and-test-sets-72cb40cba9e7) ~ by Tarang Shah\n', '* [What is the Difference Between Test and Validation Datasets?](https://machinelearningmastery.com/difference-test-validation-datasets/) ~ by Machine Learning Mastery\n', '* [Cross-Validation in Machine Learning](https://towardsdatascience.com/cross-validation-in-machine-learning-72924a69872f) ~ by Prashant Gupta\n', '* [A Gentle Introduction to k-fold Cross-Validation](https://machinelearningmastery.com/k-fold-cross-validation/) ~ by Machine Learning Mastery']",42,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m42
['### Takeaways from this section'],43,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m43
"['* Always create a robust and reliable validation system for your models.\n', '* When choosing which models to submit to the private LB, look at the CV and public LB scores together.\n', '* The gap between CV and public LB scores should be as small as possible, and never go far a change that shows a huge improvement in CV, but a relatively small improvement in public LB.']",44,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m44
['# How to reduce overfitting?'],45,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m45
"['Now, because we understand how validation can be used to detect overfitting, the next step is to understand how to fix overfitting. There are many ways to reduce overfitting, but the most most common ones are as follows:']",46,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m46
"['* **Reducing model complexity**\n', '* **Ensembling**\n', '* **Regularization**']",47,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m47
"['I will look at these differrent methods one-by-one, explain them, and then demonstrate how they work with a simple neural network model.']",48,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m48
['## Reducing model complexity'],49,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m49
"['In the above diagram, the data can be easily modeled using a simple linear function. This is because the general trend in the data seems to be linear. But, instead, a highe degree polynomial is used and the data is fit perfectly without any error. This model may achieve 100% accuracy on the training data, but it will surely fail badly when tested on new data. This is because the model has **fitted the noise** in the training data, and in the process, it memorizes the data instead of finding the required pattern. Therefore, an overly complex model may lead to overfitting. But, at the same time, an overly simple model may lead to underfitting.\n', '\n', 'So, when a model starts overfitting, one way to reduce the overfitting is to reduce the complexity of the model. In a neural network, this can be done by reducing the number of layers in the network or reducing the number of neurons in the each layer. This can also be done for a gradient boosting model like LightGBM by reducing paramters like *max_depth*. By doing this, we are intentionally, reducing the complexity of the model in order to prevent overfitting from taking place.']",50,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m50
"['<center><img src=""https://i.imgur.com/k6ZQvak.png"" width=""800px""></center>']",51,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m51
"['The idea that reducing model complexity can reduce overfitting can be understood using the concepts of **variance and bias**.\n', '\n', '**Variance** is the amount that the estimate of the model will change if different training data was used. This quantity measures the capacity of a model to fit different datasets, and is therefore also a measure of the complexity of the model. A high variance means that the model has a very complex target function and is able to fit the training data very accurately. On the other hand, a low variance means that the model has a low level of complexity and is therefore unable to fit the training very closely. It may look like the more the variance the better, but in fact, the variance of any model should be moderate, and the ""moderate"" level of variance depends on the nature of the data. Some datasets demand models with a high variance whereas other datasets require a much lower level of variance. For example, the MNIST dataset can be modeled easily using a simple CNN, but a self-driving car dataset involving complex video and sensor data may require a more complex (high variance) model.\n', '\n', 'This brings me to the concept of bias. **Bias** are the simplifying assumptions made by a model to make the target function easier to learn.  This quantity measures the simplicity of the model, because simpler models often tend to make more assumptions to fit the data. A high bias means that the model has a simple target function and is therefore unable to fit the training very closely. On the other hand, a low bias means that the model has a high level of complexity and is therefore able to fit the training very closely.\n', '\n', '**Models with a high variance tend to have a low bias and vice-versa, and therefore, there is a direct tradeoff between them**. Choosing the correct balance of variance and bias (for a particular dataset) is the key to avoiding overfitting.']",52,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m52
['### Demonstration'],53,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m53
"['Now, I will demonstrate how the reducing the model complexity may help prevent a model from overfitting. Once again, I will use neural network models to demonstrate this idea. Specifically, I will build and train a very complex neural network model (with several neurons). Then, I will show how reducing the complexity of the model (by removing neurons) can help reduce overfitting in the model.']",54,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m54
['### Build model #1'],55,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m55
"['First, we will build a model with two hidden layers (100 neurons each) and a single-neuron layer with a sigmoid activation at the end to output the probability.']",56,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m56
['### Visualize the model architecture'],57,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m57
['### Train the model and store training history'],58,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m58
['### Visualize the training and validation statistics for model #1'],59,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m59
['### Accuracy'],60,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m60
"['In the above plot, we can see that the training and validation accuracies for the 100-neuron model diverge towards the last few epochs. The training accuracy ends up significantly higher than the validation accuracy. This indicates that the model is not able to replicate its training performance when faced with the unseen validation data. This is a clear sign of overfitting.']",61,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m61
"['We can observe the same fact from the bar plot above. The training and validation accuracies diverge towards the end, with the training accuracy ending up significantly higher than the validation accuracy. This is, once again, a clear indication of overfitting.']",62,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m62
['### Loss'],63,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m63
"['In the above plot, we can see that the validation loss spikes clearly higher than the training loss towards the end of the training process. This indicates that the model is overfitting, as it is unable to replicate its training performance on the unseen validation data.']",64,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m64
"['We can once again observe the same insight from the above bar plot. Towards the last few epochs, the validation loss rises clearly higher above the training loss. Once, again, this indicates that the model is overfitting.']",65,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m65
"['In general, we can conclude that this 100-neuron model (model #1) is overfitting. Now, let us decrease the complexity of the model by decreasing the number of neurons in each hidden layer. This should help reduce the overfitting.']",66,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m66
['### Build model #2'],67,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m67
"['Now, we build another model. The architecture is the same as last time, but this time, there are 10 neurons in each hidden layer instead of 100.']",68,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m68
['### Visualize the model architecture'],69,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m69
['### Train the model and store training history'],70,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m70
['### Visualize the training and validation statistics for model #2'],71,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m71
['### Accuracy'],72,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m72
"['In the above plot, we can see that the training accuracy rises above the validation accuracy towards the end of the training process. But, the gap between the training and validation accuracies is very small compared to the 100-neuron model. This indicates that the model is training properly and is able to successfully avoid overfitting.']",73,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m73
"['In the bar plot above, we can observe the same insight. The training accuracy ends up just above the validation accuracy at the end, but the gap between the two accuracies is significantly smaller than it was in the case of the 100-neuron model.']",74,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m74
['### Loss'],75,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m75
"['In the above plot, we can see that the training loss ends up below the validation loss towards the end of the training process. But, the gap between the two losses is much smaller compared to the 100-neuron model. This shows that the model is learning properly. It is not overfitting.']",76,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m76
"['We can observe the same insight from the bar plot above. The validation loss ends up just above the training loss at the end of the training process, but the gap is much smaller compared to the 100-neuron model. This once again indicates that the model is not overfitting.']",77,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m77
['### Make predictions on training and validation data from the models '],78,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m78
['### Visualize training and validation accuracy for both the models'],79,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m79
"['We can see from the above plot that the training accuracy ends up higher than the validation accuracy in both cases. But, in the case of the second model, the gap is much smaller. Thus, we can conclude that we have been able to reduce overfitting by reducing the complexity of the model.']",80,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m80
['## Ensembling'],81,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m81
"['Ensembling is a very useful and common method used to reduce overfitting. In ensembling, the outputs from several models are combined (most commonly by averaging) to form one prediction vector. This final prediction usually has a greater accuracy than the other individual prediction vectors. This is because, when the predictions from different models are combined by averaging, the errors in the different prediction vectors are partially ""canceled out"" or reduced in magnitude. But, note that **this works only when the correlation between the different prediction vectors is considerably low, otherwise the ensemble would score similarly to the individual models**.\n', '\n', 'This implies that the **variety in the models is very important**. This means that one must ensemble models which are very different from each other in terms of architecture, hyperparameters, or algorithm in order to achieve maximum benefits from ensembling. For example, it might be a better idea to ensemble a LightGBM model with a Neural Network model instead of ensembling two LightGBM models. When more diverse models are selected, the diversity in predictions is also greater, and therefore, there is a higher chance of the ensemble to effectively reduce the errors in the predictions by ""canceling them out"".']",82,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m82
['### A video explaining ensembling'],83,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m83
['Here is another video by Udacity explaining what ensembling is:'],84,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m84
['### Demonstration'],85,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m85
"['Now, I will demonstrate how ensembling can be used to help a model generalize better and avoid overfitting. I will build and train two separate neural network models, and then show how ensembling them can increase the cross validation score.']",86,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m86
['### Build model #1'],87,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m87
"['First, we will build a model with two hidden layers (20 neurons each) and a single-neuron layer with a sigmoid activation at the end to output the probability.']",88,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m88
['### Visualize model architecture'],89,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m89
['### Train the model and store training history'],90,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m90
['### Visualize the training and validation statistics for model #1'],91,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m91
['### Accuracy'],92,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m92
"['In the above plot, we can see that the training and validation accuracies for the 10-neuron model slightly diverge towards the last few epochs. The training accuracy ends up slightly higher than the validation accuracy. This indicates that the model is overfitting, but not as much as the previous 100-neuron model.']",93,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m93
"['We can see the same insight from the above bar plot. The training accuracy ends up slightly higher than the validation accuracy towards the last few epochs. This indicates that the model is overfitting, but only slightly.']",94,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m94
['### Loss'],95,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m95
"['In the above plot, we can see that the validation loss ends up slightly higher than the training loss towards the end. This once again shows that the model is overfitting, but not very heavily.']",96,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m96
"['We can see the same insight from the above bar plot. The training accuracy ends up slightly higher than the validation accuracy towards the last few epochs. This indicates that the model is overfitting, but only slightly.']",97,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m97
"['We can conclude that this model is overfitting only slightly. So, now I will build another model with 25 neurons instead of 10 neurons in each layer.']",98,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m98
['### Build model #2'],99,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m99
"['Now, we will build a new model with the same architecture, but with 25 (instead of 20) neurons per hidden layer, and a single-neuron layer with a sigmoid activation at the end to output the probability.']",100,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m100
['### Visualize the model architecture'],101,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m101
['### Train the model and store training history'],102,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m102
['### Visualize the training and validation statistics for model #2'],103,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m103
['### Accuracy'],104,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m104
"['In the above plot, we can see that the training and validation accuracies for the 25-neuron model are similar towards the last few epochs. The training accuracy ends up only slightly higher than the validation accuracy. This indicates that the model is able to replicate its training performance when faced with the unseen validation data. This is not overfitting.']",105,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m105
"['We can observe the same fact from the bar plot above. The training and validation accuracies diverge by a very small margin, with the training accuracy ending up only slightly higher than the validation accuracy. This is, once again, an indication of no overfitting.']",106,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m106
['### Loss'],107,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m107
"['In the above plot, we can see that the validation loss ends up just above the training loss, but since the gap is very small, we can say that the model is not overfitting.']",108,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m108
"['We can once again observe the same insight from the above bar plot. Towards the last few epochs, the validation loss ends up near the training loss. Once, again, this indicates that the model is not overfitting.']",109,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m109
"['In general, we can conclude that this 25-neuron model (model #2) is now overfitting. Now, let us ensemble the two models and see what happens.']",110,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m110
['### Ensembling the two models'],111,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m111
"['Now, I will ensemble the two models by averaging their predictions on the validation data.']",112,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m112
['I give a higher weightage (75 %) to the first model than the second model (25 %) because the second model has a better CV score than the first model. **The model weightages in the ensemble can be decided based on the individual cross validation scores**.'],113,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m113
"['### Check the accuracy values for model #1, model #2, and their ensemble']",114,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m114
"[""The accuracy of models #1 and #2 are both very high. Therefore, the ensemble's accuracy was higher than the accuracies of the two models. Therefore, **ensembling is effective only when several models with strong generalization are combined together**. Even one weak model can bring down the accuracy of the ensemble by a lot. ""]",115,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m115
"['### Visualize the accuracy for model #1, model #2, and their ensemble']",116,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m116
['### Difference of the accuracies of the models above 96.9 %'],117,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m117
['We can once again see from the above plot that the ensemble accuracy is greater than the individual accuracies of model #1 and model #2.'],118,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m118
['### Resources for understading ensembling'],119,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m119
"['* [A Comprehensive Guide to Ensemble Learning (with Python codes)](https://www.analyticsvidhya.com/blog/2018/06/comprehensive-guide-for-ensemble-models/) ~ by Aishwarya Singh\n', '* [Ensemble Learning Methods for Deep Learning Neural Networks](https://machinelearningmastery.com/ensemble-methods-for-deep-learning-neural-networks/) ~ by Machine Learning Mastery']",120,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m120
['## Regularization'],121,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m121
"['Regularization is another great way to prevent models from overfitting. **Regularization is a group of methods aimed at improving the generalization of models**. Basically, regularization methods ensure that the model maintains the same performance level on the validation data (and testing data) as it does on the training data.\n', '\n', 'Regularization can take on many different types and forms based on the data and model under consideration. For example, a method called [Dropout](https://medium.com/@amarbudhiraja/https-medium-com-amarbudhiraja-learning-less-to-learn-better-dropout-in-deep-machine-learning-74334da4bfc5) can be used to improve the generalization of neural networks. This is a form of regularization. Similarly, one can also use the [L1 and L2 methods](https://towardsdatascience.com/l1-and-l2-regularization-methods-ce25e7fc831c) to reduce overfitting in a neural network. Once, this is a form of regularization.\n', '\n', 'Dropout is a regularization method that approximates training a large number of neural networks with different architectures in parallel.\n', '\n', 'During training, some number of layer outputs are randomly ignored or “dropped out.” This has the effect of making the layer look-like and be treated-like a layer with a different number of nodes and connectivity to the prior layer. In effect, each update to a layer during training is performed with a different “view” of the configured layer.\n', '\n', 'The probablity of dropping a random neuron is called the dropout rate. For the demonstration below, I will use a dropout rate 0f 0.2.']",122,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m122
"['<center><img src=""https://i.imgur.com/qFmZI6M.png"" width=""600px""></center>']",123,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m123
['### A video explaining regularization'],124,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m124
['Here is a video of Andrew Ng explaining the idea of regularization and how it can be used to combat overfitting:'],125,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m125
['### Demonstration'],126,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m126
"['Regularization can be done in many ways, but for this kernel, I will demonstrating how **dropout** can be used to regularize neural networks.']",127,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m127
['### Build model #1 (without dropout)'],128,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m128
"['First, we will build a model with two hidden layers (200 neurons each) and a single-neuron layer with a sigmoid activation at the end to output the probability.']",129,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m129
['### Visualize model architecture'],130,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m130
['### Train the model and store training history'],131,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m131
['### Visualize the training and validation statistics for model #1'],132,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m132
['### Accuracy'],133,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m133
"['In the above plot, we can see that the training and validation accuracies for the 200-neuron model diverge towards the last few epochs. The training accuracy ends up significantly higher than the validation accuracy. This indicates that the model is not able to replicate its training performance when faced with the unseen validation data. This is a clear sign of heavy overfitting.']",134,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m134
"['We can once again observe the same insight from the above bar plot. Towards the last few epochs, the validation loss rises significantly higher above the training loss. Once, again, this indicates that the model is heavily overfitting.']",135,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m135
['### Loss'],136,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m136
"['In the above plot, we can see that the validation loss spikes clearly higher than the training loss towards the end of the training process. This indicates that the model is overfitting, as it is unable to replicate its training performance on the unseen validation data. This time, the overfitting seems to be significant.']",137,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m137
"['We can once again observe the same insight from the above bar plot. Towards the last few epochs, the validation loss rises clearly higher above the training loss. Once, again, this indicates that the model is overfitting.']",138,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m138
['### Build model #2 (with dropout)'],139,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m139
"['Now, we will build a new model with the same architecture, **but with a dropout layer after each hidden layer**, and a single-neuron layer with a sigmoid activation at the end to output the probability.']",140,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m140
['### Visualize the model architecture'],141,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m141
['### Train the model and store training history'],142,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m142
['### Visualize the training and validation statistics for model #1'],143,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m143
['### Accuracy'],144,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m144
"['In the above plot, we can see that the validation accuracy rises above the training accuracy towards the end of the training process. But, the gap between the training and validation accuracies is very small. This indicates that the model is training properly and is able to successfully avoid overfitting.']",145,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m145
"['In the bar plot above, we can observe the same insight. The validation accuracy ends up just above the training accuracy at the end, but the gap between the two accuracies is significantly smaller than it was in the case of the no-dropout model.']",146,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m146
['### Loss'],147,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m147
"['In the above plot, we can see that the validation loss ends up below the training loss towards the end of the training process. But, the gap between the two losses is much smaller compared to the no-dropout model. This shows that the model is learning properly. It is not overfitting.']",148,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m148
"['We can observe the same insight from the bar plot above. The validation loss ends up just below the training loss at the end of the training process, but the gap is much smaller compared to the no-dropout model. This once again indicates that the model is not overfitting.']",149,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m149
"['We can easily conclude that adding dropout to the neural network acted as a regularizer. It helped reduce the gap between training and validation scores. Thus, adding dropout helped successfully remove overfitting from the model.']",150,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m150
['### Make predictions on training and validation data from the models '],151,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m151
['### Visualize the training and validation accuracy for both the models'],152,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m152
"['We can see from the above plot that the training accuracy ends up higher than the validation accuracy in both cases. But, in the case of the second model, the gap is much smaller. Thus, we can conclude that we have been able to reduce overfitting by adding dropout to the model.']",153,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m153
['### Resources for understanding regularization'],154,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m154
"['* [Dropout in (Deep) Machine Learning](https://medium.com/@amarbudhiraja/https-medium-com-amarbudhiraja-learning-less-to-learn-better-dropout-in-deep-machine-learning-74334da4bfc5) ~ by Amar Budhiraja\n', '* [L1 and L2 Regularization Methods](https://towardsdatascience.com/l1-and-l2-regularization-methods-ce25e7fc831c) ~ by Anuja Nagpal\n', '* [A Gentle Introduction to Dropout for Regularizing Deep Neural Networks](https://machinelearningmastery.com/dropout-for-regularizing-deep-neural-networks/) ~ by Machine Learning Mastery']",155,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m155
['# Conclusion'],156,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m156
"['From this kernel, I hope you understand that you have a certain degree of control over your public LB standings and it is not completely random. So, using these tips and tricks, you can survive a shakeup and end up with a medal :)']",157,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m157
['Thanks for reading this kernel. I hope you found it useful.'],158,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m158
"['<font size=""4"" color=""red""> Please upvote this kernel if you liked it. It motivates me to produce more quality content :) </font>']",159,tarunpaparaju,ieee-cis-competition-how-to-survive-the-shakeup,tarunpaparaju_ieee-cis-competition-how-to-survive-the-shakeup_m159
"[""# <font color='orange'>Introduction</font>\n"", ""**TransactionDT** is a column represents date and time of each transaction. Problem is, the values don't traditionally start from 1970/1/1 as usual but rather some random point in time. Figuring out when the start time is can help tremendously if one wants to use seasonality in their analyses and modeling.\n"", '\n', ""Let's find out""]",0,terrypham,transactiondt-timeframe-deduction,terrypham_transactiondt-timeframe-deduction_m0
"[""# <font color='orange'>Overview</font>""]",1,terrypham,transactiondt-timeframe-deduction,terrypham_transactiondt-timeframe-deduction_m1
"[""<font color='orange'>*Notice the minimum value of TransactionDT is 86400 that happens to be the number of seconds of 1 day so we can assume that the unit of the column is second*</font>\n"", '\n', ""* **TransactionDT** data of train and test set are from different distribution as the graphs don't overlap.\n"", '* The data spreads across 396 days which is about 13 months\n', ""* Looking at the peaks at both ends, they're likely to be associated with a festive season, Black Friday or Christmas maybe""]",2,terrypham,transactiondt-timeframe-deduction,terrypham_transactiondt-timeframe-deduction_m2
['# Year'],3,terrypham,transactiondt-timeframe-deduction,terrypham_transactiondt-timeframe-deduction_m3
"[""### Let's see which devices made the first transactions ""]",4,terrypham,transactiondt-timeframe-deduction,terrypham_transactiondt-timeframe-deduction_m4
"[""<font color='orange'>A quick check shows that the Samsung Galaxy S8 ***(SAMSUNG SM-G892A Build/NRD90M)*** was released in the US the earliest on 21st April 2017 (Wikipedia). Since there's transactions made by this phone the 2nd day of the data's time, this data apparently can only be as old as ***April of 2017***</font>""]",5,terrypham,transactiondt-timeframe-deduction,terrypham_transactiondt-timeframe-deduction_m5
['# Date'],6,terrypham,transactiondt-timeframe-deduction,terrypham_transactiondt-timeframe-deduction_m6
"[""<font color='white'>The data spreads over 395 days which is about 13 months. Also there're 2 peaks at the ends of the period. \n"", ""1. And I heard Black Friday is the peak shopping season in the US, no? <br/><br/> Let's find out!</font>""]",7,terrypham,transactiondt-timeframe-deduction,terrypham_transactiondt-timeframe-deduction_m7
"[""<font color='white'>Looks like it's the case. Google Trends shows high traffic associated with Shopping related keywords during Black Friday season.</font>\n"", '\n', 'https://trends.google.com/trends/explore?cat=18&date=2017-04-01%202018-12-31&geo=US']",8,terrypham,transactiondt-timeframe-deduction,terrypham_transactiondt-timeframe-deduction_m8
['### How about plotting the TransactionDT day wise?'],9,terrypham,transactiondt-timeframe-deduction,terrypham_transactiondt-timeframe-deduction_m9
"[""1. <font color='navy'>**Assuming we're right that the peaks are associated with the Black Friday season. \n"", ""    Let's see what are the dates we're looking at**</font>""]",10,terrypham,transactiondt-timeframe-deduction,terrypham_transactiondt-timeframe-deduction_m10
"[""Since 2017's Black Friday was 24th November that happens to be tally with the numbers above. Let us just assume that the data begins on November 1st of 2017, Let's have a look at the 2nd peak in the test data""]",11,terrypham,transactiondt-timeframe-deduction,terrypham_transactiondt-timeframe-deduction_m11
"[""<font color='navy'>**23rd November 2018 is Black Friday, and 26th November 2018 is Cyber Monday, how cool is that**</font>""]",12,terrypham,transactiondt-timeframe-deduction,terrypham_transactiondt-timeframe-deduction_m12
['# Conclusion'],13,terrypham,transactiondt-timeframe-deduction,terrypham_transactiondt-timeframe-deduction_m13
"[""*Based on the patterns found within the data, together with insights provided by Google Trends, the data's start date is likely 2017/11/1*""]",14,terrypham,transactiondt-timeframe-deduction,terrypham_transactiondt-timeframe-deduction_m14
['### Installing LightGBM GPU build'],0,tharug,ieee-fraud-detection,tharug_ieee-fraud-detection_m0
"['So the data description tells us that, we can join the two datasets by the `TransactionID` column. However, not all transactions will have corresponding identity information.\n']",1,tharug,ieee-fraud-detection,tharug_ieee-fraud-detection_m1
"[""Let's check to see if we have the same levels (categories) in the training and testing set. If we have new categories is the test set, the model may not be able to accuratly predict on those values.""]",2,tharug,ieee-fraud-detection,tharug_ieee-fraud-detection_m2
"[""We have values that apprear in the test set that is not available in the training set. To handle this we'll set the valid categories as the ones apprearing in the training set. This will force the catagories apprearing in the test set to be `nan`.""]",3,tharug,ieee-fraud-detection,tharug_ieee-fraud-detection_m3
['### Memory reduction'],4,tharug,ieee-fraud-detection,tharug_ieee-fraud-detection_m4
['### Card Information'],5,tharug,ieee-fraud-detection,tharug_ieee-fraud-detection_m5
"['**Transaction Date**  \n', 'The `TransactionDT` gives the timedelta from a reference date. We can calculate the days past the reference date by dividing the values by 86,400 ($60\\times60\\times24$). ']",6,tharug,ieee-fraud-detection,tharug_ieee-fraud-detection_m6
"[""If we look at a seven day period we can see that there is a cyclical movement within each day, but there isn't much difference between days. We would expect that the transaction volumns are lowest near midnight. We can calculate the hour of each day when the transaction occure by dividing the timedelta by 24 and taking the remainder and setting an offset. ""]",7,tharug,ieee-fraud-detection,tharug_ieee-fraud-detection_m7
"['From observing the histogram, we can estimate that 9/24 will be a good off-set.']",8,tharug,ieee-fraud-detection,tharug_ieee-fraud-detection_m8
"[""The day's 0, 1, 2 and 6 all have fraud rates higher than the average. If we were to guess we could consider day 6 to be friday, 0 as saturday, 1 as sunday and 2 as monday. ""]",9,tharug,ieee-fraud-detection,tharug_ieee-fraud-detection_m9
"['Again we see a trend that is somewhat expected, on average there is an increase in the fraud rates during the night times, and reduced number of frauds during the day time. ']",10,tharug,ieee-fraud-detection,tharug_ieee-fraud-detection_m10
['### Transaction Amount'],11,tharug,ieee-fraud-detection,tharug_ieee-fraud-detection_m11
['### Encoding Categorical Data'],12,tharug,ieee-fraud-detection,tharug_ieee-fraud-detection_m12
['### XGBoost'],13,tharug,ieee-fraud-detection,tharug_ieee-fraud-detection_m13
['### LightGBM'],14,tharug,ieee-fraud-detection,tharug_ieee-fraud-detection_m14
['### Using Hyperopt to find best parameters'],15,tharug,ieee-fraud-detection,tharug_ieee-fraud-detection_m15
"['<hr/>\n', '[**Tolgahan Cepel**](https://www.kaggle.com/tolgahancepel)\n', '<hr/>\n', '<font color=green>\n', '* 1. [Importing Libraries and Reading the Dataset](#1)\n', '* 2. [Feature Engineering](#2)\n', '    * [Add New Features](#3)\n', '    * [Handle Email Domains](#4) \n', '    * [Handle P Email Domain and R Email Domain](#5) \n', '    * [Set Time](#6) \n', '    * [Handle Browser Version](#7) \n', '    * [Handle Device Type](#8)\n', '    * [Set Frequency](#9)\n', '* 3. [Data Preprocessing](#10) \n', '* 4. [Models](#11)\n', '    * [LightGBM](#12) \n', '* 5. [Submission](#13)\n', '<hr/>']",0,tolgahancepel,lightgbm-single-model-and-feature-engineering,tolgahancepel_lightgbm-single-model-and-feature-engineering_m0
"['## <span id=""1""></span> ** 1. Importing Libraries and Reading the Dataset **']",1,tolgahancepel,lightgbm-single-model-and-feature-engineering,tolgahancepel_lightgbm-single-model-and-feature-engineering_m1
"['## <span id=""2""></span> ** 2. Feature Engineering **']",2,tolgahancepel,lightgbm-single-model-and-feature-engineering,tolgahancepel_lightgbm-single-model-and-feature-engineering_m2
"['### <span id=""3""></span> ** Add New Features **']",3,tolgahancepel,lightgbm-single-model-and-feature-engineering,tolgahancepel_lightgbm-single-model-and-feature-engineering_m3
"['### <span id=""4""></span> ** Handle Email Domains **']",4,tolgahancepel,lightgbm-single-model-and-feature-engineering,tolgahancepel_lightgbm-single-model-and-feature-engineering_m4
"['### <span id=""5""></span> ** Handle P Email Domain and R Email Domain **']",5,tolgahancepel,lightgbm-single-model-and-feature-engineering,tolgahancepel_lightgbm-single-model-and-feature-engineering_m5
"['### <span id=""6""></span> ** Set Time **']",6,tolgahancepel,lightgbm-single-model-and-feature-engineering,tolgahancepel_lightgbm-single-model-and-feature-engineering_m6
"['### <span id=""7""></span> ** Handle Browser Version **']",7,tolgahancepel,lightgbm-single-model-and-feature-engineering,tolgahancepel_lightgbm-single-model-and-feature-engineering_m7
"['### <span id=""8""></span> ** Handle Device Type **']",8,tolgahancepel,lightgbm-single-model-and-feature-engineering,tolgahancepel_lightgbm-single-model-and-feature-engineering_m8
"['### <span id=""9""></span> ** Set Frequency **']",9,tolgahancepel,lightgbm-single-model-and-feature-engineering,tolgahancepel_lightgbm-single-model-and-feature-engineering_m9
"['## <span id=""10""></span> ** 3. Data Preprocessing **']",10,tolgahancepel,lightgbm-single-model-and-feature-engineering,tolgahancepel_lightgbm-single-model-and-feature-engineering_m10
"['## <span id=""11""></span> ** 4. Model **']",11,tolgahancepel,lightgbm-single-model-and-feature-engineering,tolgahancepel_lightgbm-single-model-and-feature-engineering_m11
"['### <span id=""12""></span> ** LightGBM **']",12,tolgahancepel,lightgbm-single-model-and-feature-engineering,tolgahancepel_lightgbm-single-model-and-feature-engineering_m12
"['## <span id=""13""></span> ** 5. Submission **']",13,tolgahancepel,lightgbm-single-model-and-feature-engineering,tolgahancepel_lightgbm-single-model-and-feature-engineering_m13
"['<b><font color=""red"">Don\'t forget to </font></b> <b><font color=""green"">UPVOTE </font></b> if you liked this kernel, thank you. 🙂👍']",14,tolgahancepel,lightgbm-single-model-and-feature-engineering,tolgahancepel_lightgbm-single-model-and-feature-engineering_m14
"['In this kernel I want to make an univariate analysis of Vxxx features. looking ahead, I think that these features are very important for fraud detection, because they give us a lot of information which transaction is fraudent and which is not, and using these features we can, theoretically, reduce our false negatives and false positives rate. Also they give us a lot of insights about feeature selection.\n', '\n', 'We do not know, what these features means, only information we have is:\n', '\n', 'Vxxx: Vesta engineered rich features, including ranking, counting, and other entity relations.']",0,trolukovich,vxxx-features-eda,trolukovich_vxxx-features-eda_m0
"['Vast ammount of features in datasets have a lot of null values, up to 86%.']",1,trolukovich,vxxx-features-eda,trolukovich_vxxx-features-eda_m1
"['We can see something interesting here - our features have very small numbers in first, second and third quartiles, but maximum value on contrary is very high.\n', '\n', ""Also it seems like some features are binary (V1 for example), some looks like ordinal (V2) and some looks like numeric (V126), let's try to divide features by groups.""]",2,trolukovich,vxxx-features-eda,trolukovich_vxxx-features-eda_m2
"['Now we can start plotting.\n', '\n', ""First - binary features (bars in plots ordered by fraud rate), I'll create function to plot categorical features with fraud rate.""]",3,trolukovich,vxxx-features-eda,trolukovich_vxxx-features-eda_m3
"['Looking ahead, I want to plot fraud rates for different values of features.']",4,trolukovich,vxxx-features-eda,trolukovich_vxxx-features-eda_m4
"['We can see that values for our binary features have very similar fraud rates.\n', '\n', ""Let's return to plotting""]",5,trolukovich,vxxx-features-eda,trolukovich_vxxx-features-eda_m5
"[""Let's look at counts:""]",6,trolukovich,vxxx-features-eda,trolukovich_vxxx-features-eda_m6
['0 values (2 for V305 feature) of binary features have very small ammount of transactions - less than 0.1% and fraud rate of these transacions is less than 0.1% or equal to 0.'],7,trolukovich,vxxx-features-eda,trolukovich_vxxx-features-eda_m7
"['Next step - Ordinal features.\n', '\n', ""We have 257 ordinal features in dataset, i'll plot them by small groups and make some preparations for aesthetic purposes.\n"", '\n', 'First, I want to divide them by number of values, if feature have more than 20 unique values - it goes to long_ordinal list, else - to short_ordinal list.']",8,trolukovich,vxxx-features-eda,trolukovich_vxxx-features-eda_m8
"[""Let's look at fraud rates.""]",9,trolukovich,vxxx-features-eda,trolukovich_vxxx-features-eda_m9
"[""6, 3, 4, 2, 5, 7, 8, 9 values don't give us much information, but we can see that a lot of features have fraud rate close to 0 at Null values, as 1 and 0 values.\n"", '\n', 'On contrary: 15, 24, 23, 17, 16, 18, 19 values have features with fraud rate equal to 1.\n', '\n', ""It's a pity that we can't make such plot for long_ordinal, it's too computational expensive and even if we will have unlimited computational resourses, all we got is mess.\n"", '\n', 'So, I want to use different approach for these features.']",10,trolukovich,vxxx-features-eda,trolukovich_vxxx-features-eda_m10
"[""Now let's return to the routine and make plots for ordinal features.\n"", '\n', 'When I worked on this part, I faced with a problem, when such number of plots just crashed my kernel, so I decided to save plots in .png fomat and show them here as pictures.\n', '\n', ""Also, I'm including code for plots in comments.""]",11,trolukovich,vxxx-features-eda,trolukovich_vxxx-features-eda_m11
['We can see something interesting here. Almost every feature in short_ordinal have a fraud peaks on some value for example V17 and V18 have 6 values with 100% fraud rate. Also every feature have values which fraud rate close or equal to zero.'],12,trolukovich,vxxx-features-eda,trolukovich_vxxx-features-eda_m12
"['Similar situation here, we can easily see which walues give us 100% or 0% fraud rates.\n', '\n', ""Next - numeric features. I'll use log1p transformed distribution plots to see fraud and non fraud peaks.""]",13,trolukovich,vxxx-features-eda,trolukovich_vxxx-features-eda_m13
"[""Let's take a look how similar train and test sets are.""]",0,tunguz,adversarial-ieee,tunguz_adversarial-ieee_m0
"[""Whoa, that's a pretty significant AUC!  0.999996 adverserial AUC is the biggest one I've ever come across. I first thought I might be making a mistake, but re-run this script several times, and don't seem to find any bugs in it. But I am open to criticims/suggestions.\n"", '\n', 'Let\'s look now at the top 20 ""adversarial"" features.']",1,tunguz,adversarial-ieee,tunguz_adversarial-ieee_m1
"['Seems that transaction date is the main ""culprit"".\n', '\n', ""Let's see what happens when we remove time stamp.""]",2,tunguz,adversarial-ieee,tunguz_adversarial-ieee_m2
"[""At 0.90 the AUC has improved, but it's still really high.""]",3,tunguz,adversarial-ieee,tunguz_adversarial-ieee_m3
['To be continued ...'],4,tunguz,adversarial-ieee,tunguz_adversarial-ieee_m4
"['**THIS KERNAL IS BLEND OF **\n', 'So awesome kernels present Right now \n', '\n', '**vote if you love blend**\n', '\n', '1. https://www.kaggle.com/raghaw/ensemble-on-fire {already blended}\n', '2. https://www.kaggle.com/vincentlugat/ieee-lgb-bayesian-opt/output {lgb+bayesian}\n', '3. https://www.kaggle.com/ryches/keras-nn-starter-w-time-series-split/output {Keras NN}\n', '4. https://www.kaggle.com/timon88/lgbm-baseline-small-fe-no-blend/output {lgbm+fe}']",0,vaishvik25,ensemble,vaishvik25_ensemble_m0
"['## phase 1 [Ensemble]\n', '\n', '\n']",1,vaishvik25,ensemble,vaishvik25_ensemble_m1
['**Hist Graph of scores**'],2,vaishvik25,ensemble,vaishvik25_ensemble_m2
"['## phase 2 [Stacking]\n', '\n']",3,vaishvik25,ensemble,vaishvik25_ensemble_m3
"['**submission_p2_1.csv tops the chart**\n', '**vote if you love blend**']",4,vaishvik25,ensemble,vaishvik25_ensemble_m4
"[' ## <div style=""text-align: left"">Simple EDA IEEE : Vesta Fraud Data </div> \n', '\n', '![](https://cdn1.imggmi.com/uploads/2019/8/7/884ef7fecc277f2c58396ed766d3d569-full.png)\n', '\n', '<br>\n', '<div style=""text-align: left"">\n', 'Vesta Corporation provided the dataset for this competition. Vesta Corporation is the forerunner in guaranteed e-commerce payment solutions. Founded in 1995, Vesta pioneered the process of fully guaranteed card-not-present (CNP) payment transactions for the telecommunications industry. Since then, Vesta has firmly expanded data science and machine learning capabilities across the globe and solidified its position as the leader in guaranteed ecommerce payments. Today, Vesta guarantees more than $18B in transactions annually.\n', '\n', ""In this competition, you’ll benchmark machine learning models on a challenging large-scale dataset. The data comes from Vesta's real-world e-commerce transactions and contains a wide range of features from device type to product features. You also have the opportunity to create new features to improve your results.</div>""]",0,vaishvik25,ieee-exploratory-data-analysis,vaishvik25_ieee-exploratory-data-analysis_m0
"['# Distribution plots of features\n', '\n', '-999 value is injected for Nan values']",1,vaishvik25,ieee-exploratory-data-analysis,vaishvik25_ieee-exploratory-data-analysis_m1
['## Card 1-6'],2,vaishvik25,ieee-exploratory-data-analysis,vaishvik25_ieee-exploratory-data-analysis_m2
['## C 1-14'],3,vaishvik25,ieee-exploratory-data-analysis,vaishvik25_ieee-exploratory-data-analysis_m3
['## D 1-15'],4,vaishvik25,ieee-exploratory-data-analysis,vaishvik25_ieee-exploratory-data-analysis_m4
['## M 1-9'],5,vaishvik25,ieee-exploratory-data-analysis,vaishvik25_ieee-exploratory-data-analysis_m5
['## ID 1-38'],6,vaishvik25,ieee-exploratory-data-analysis,vaishvik25_ieee-exploratory-data-analysis_m6
['## Other '],7,vaishvik25,ieee-exploratory-data-analysis,vaishvik25_ieee-exploratory-data-analysis_m7
"[' # What is Correlation?\n', '\n', 'Variables within a dataset can be related for lots of reasons.\n', '\n', 'For example:\n', '\n', '* One variable could cause or depend on the values of another variable.\n', '* One variable could be lightly associated with another variable.\n', '* Two variables could depend on a third unknown variable.\n', '\n', 'It can be useful in data analysis and modeling to better understand the relationships between variables. The statistical relationship between two variables is referred to as their correlation.\n', '\n', 'A correlation could be positive, meaning both variables move in the same direction, or negative, meaning that when one variable’s value increases, the other variables’ values decrease. Correlation can also be neural or zero, meaning that the variables are unrelated.\n', '\n', '1. > **Positive Correlation:** both variables change in the same direction.\n', '2. > **Neutral Correlation:** No relationship in the change of the variables.\n', '3. > **Negative Correlation:** variables change in opposite directions.\n', '\n', 'The performance of some algorithms can deteriorate if two or more variables are tightly related, called multicollinearity. An example is linear regression, where one of the offending correlated variables should be removed in order to improve the skill of the model.\n', '\n', 'We may also be interested in the correlation between input variables with the output variable in order provide insight into which variables may or may not be relevant as input for developing a model.\n', '\n', 'The structure of the relationship may be known, e.g. it may be linear, or we may have no idea whether a relationship exists between two variables or what structure it may take. Depending what is known about the relationship and the distribution of the variables, different correlation scores can be calculated.\n', '\n']",8,vaishvik25,ieee-exploratory-data-analysis,vaishvik25_ieee-exploratory-data-analysis_m8
['## Card 1-6'],9,vaishvik25,ieee-exploratory-data-analysis,vaishvik25_ieee-exploratory-data-analysis_m9
['## C 1-14'],10,vaishvik25,ieee-exploratory-data-analysis,vaishvik25_ieee-exploratory-data-analysis_m10
['## D 1-15'],11,vaishvik25,ieee-exploratory-data-analysis,vaishvik25_ieee-exploratory-data-analysis_m11
['## M 1-9'],12,vaishvik25,ieee-exploratory-data-analysis,vaishvik25_ieee-exploratory-data-analysis_m12
['## V 1-50'],13,vaishvik25,ieee-exploratory-data-analysis,vaishvik25_ieee-exploratory-data-analysis_m13
['## V 51-100'],14,vaishvik25,ieee-exploratory-data-analysis,vaishvik25_ieee-exploratory-data-analysis_m14
['## V 101-150'],15,vaishvik25,ieee-exploratory-data-analysis,vaishvik25_ieee-exploratory-data-analysis_m15
['## V 151-200'],16,vaishvik25,ieee-exploratory-data-analysis,vaishvik25_ieee-exploratory-data-analysis_m16
['## V 201-250'],17,vaishvik25,ieee-exploratory-data-analysis,vaishvik25_ieee-exploratory-data-analysis_m17
['## V 251-300'],18,vaishvik25,ieee-exploratory-data-analysis,vaishvik25_ieee-exploratory-data-analysis_m18
['## V 301-339'],19,vaishvik25,ieee-exploratory-data-analysis,vaishvik25_ieee-exploratory-data-analysis_m19
['## iD 1-38'],20,vaishvik25,ieee-exploratory-data-analysis,vaishvik25_ieee-exploratory-data-analysis_m20
['## Other'],21,vaishvik25,ieee-exploratory-data-analysis,vaishvik25_ieee-exploratory-data-analysis_m21
"['# Heatmaps of Covariance\n', '\n', 'In probability, covariance is the measure of the joint probability for two random variables. It describes how the two variables change together.']",22,vaishvik25,ieee-exploratory-data-analysis,vaishvik25_ieee-exploratory-data-analysis_m22
['## Card 1-6'],23,vaishvik25,ieee-exploratory-data-analysis,vaishvik25_ieee-exploratory-data-analysis_m23
['## C 1-14'],24,vaishvik25,ieee-exploratory-data-analysis,vaishvik25_ieee-exploratory-data-analysis_m24
['## D 1-15'],25,vaishvik25,ieee-exploratory-data-analysis,vaishvik25_ieee-exploratory-data-analysis_m25
['## M 1-9'],26,vaishvik25,ieee-exploratory-data-analysis,vaishvik25_ieee-exploratory-data-analysis_m26
['## V 1-50'],27,vaishvik25,ieee-exploratory-data-analysis,vaishvik25_ieee-exploratory-data-analysis_m27
['## V 51-100'],28,vaishvik25,ieee-exploratory-data-analysis,vaishvik25_ieee-exploratory-data-analysis_m28
['## V 101-150'],29,vaishvik25,ieee-exploratory-data-analysis,vaishvik25_ieee-exploratory-data-analysis_m29
['## V 151-200'],30,vaishvik25,ieee-exploratory-data-analysis,vaishvik25_ieee-exploratory-data-analysis_m30
['## V 201-250'],31,vaishvik25,ieee-exploratory-data-analysis,vaishvik25_ieee-exploratory-data-analysis_m31
['## V 251-300'],32,vaishvik25,ieee-exploratory-data-analysis,vaishvik25_ieee-exploratory-data-analysis_m32
['## V 301-339'],33,vaishvik25,ieee-exploratory-data-analysis,vaishvik25_ieee-exploratory-data-analysis_m33
['## iD 1-38'],34,vaishvik25,ieee-exploratory-data-analysis,vaishvik25_ieee-exploratory-data-analysis_m34
['## Other'],35,vaishvik25,ieee-exploratory-data-analysis,vaishvik25_ieee-exploratory-data-analysis_m35
"['\n', '<div style=""text-align: center; color:gold""> **To be continue & UPVOTE IF YOU LIKE** </div> \n']",36,vaishvik25,ieee-exploratory-data-analysis,vaishvik25_ieee-exploratory-data-analysis_m36
"['----------\n', '**IEEE - Catboost GPU baseline(5 Kfold)**\n', '=====================================\n', '\n', '***Vincent Lugat***\n', '\n', '*July 2019*\n', '\n', '----------']",0,vincentlugat,ieee-catboost-gpu-baseline-5-kfold,vincentlugat_ieee-catboost-gpu-baseline-5-kfold_m0
['![](https://image.noelshack.com/fichiers/2019/29/2/1563297157-cis-logo.png)'],1,vincentlugat,ieee-catboost-gpu-baseline-5-kfold,vincentlugat_ieee-catboost-gpu-baseline-5-kfold_m1
"[""- <a href='#1'>1. Libraries and Data</a>  \n"", ""- <a href='#2'>2. Catboost GPU </a> \n"", ""- <a href='#3'>3. Features importance</a>\n"", ""- <a href='#4'>4. Submission</a>""]",2,vincentlugat,ieee-catboost-gpu-baseline-5-kfold,vincentlugat_ieee-catboost-gpu-baseline-5-kfold_m2
"[""# <a id='1'>1. Librairies and data</a> ""]",3,vincentlugat,ieee-catboost-gpu-baseline-5-kfold,vincentlugat_ieee-catboost-gpu-baseline-5-kfold_m3
['## DATASETS'],4,vincentlugat,ieee-catboost-gpu-baseline-5-kfold,vincentlugat_ieee-catboost-gpu-baseline-5-kfold_m4
"['## MERGE, MISSING VALUE, FILL NA']",5,vincentlugat,ieee-catboost-gpu-baseline-5-kfold,vincentlugat_ieee-catboost-gpu-baseline-5-kfold_m5
['## ENCODING'],6,vincentlugat,ieee-catboost-gpu-baseline-5-kfold,vincentlugat_ieee-catboost-gpu-baseline-5-kfold_m6
['## CONFUSION MATRIX'],7,vincentlugat,ieee-catboost-gpu-baseline-5-kfold,vincentlugat_ieee-catboost-gpu-baseline-5-kfold_m7
"[""# <a id='2'>2. Catboost</a> ""]",8,vincentlugat,ieee-catboost-gpu-baseline-5-kfold,vincentlugat_ieee-catboost-gpu-baseline-5-kfold_m8
['## PARAMS '],9,vincentlugat,ieee-catboost-gpu-baseline-5-kfold,vincentlugat_ieee-catboost-gpu-baseline-5-kfold_m9
['## CV 5 FOLDS AND METRICS'],10,vincentlugat,ieee-catboost-gpu-baseline-5-kfold,vincentlugat_ieee-catboost-gpu-baseline-5-kfold_m10
"[""# <a id='3'>3. Feature importance</a> ""]",11,vincentlugat,ieee-catboost-gpu-baseline-5-kfold,vincentlugat_ieee-catboost-gpu-baseline-5-kfold_m11
"[""# <a id='4'>4. Submission</a> ""]",12,vincentlugat,ieee-catboost-gpu-baseline-5-kfold,vincentlugat_ieee-catboost-gpu-baseline-5-kfold_m12
"['----------\n', '**IEEE Fraud Detection - Bayesian optimization - LGB**\n', '=====================================\n', '\n', '***Vincent Lugat***\n', '\n', '*July 2019*\n', '\n', '----------']",0,vincentlugat,ieee-lgb-bayesian-opt,vincentlugat_ieee-lgb-bayesian-opt_m0
['![](https://image.noelshack.com/fichiers/2019/29/2/1563297157-cis-logo.png)'],1,vincentlugat,ieee-lgb-bayesian-opt,vincentlugat_ieee-lgb-bayesian-opt_m1
"[""- <a href='#1'>1. Libraries and Data</a>  \n"", ""- <a href='#2'>2. Bayesian Optimisation </a> \n"", ""- <a href='#3'>3. LGB + best hyperparameters</a>\n"", ""- <a href='#4'>4. Features importance</a>\n"", ""- <a href='#4'>5. Submission</a>""]",2,vincentlugat,ieee-lgb-bayesian-opt,vincentlugat_ieee-lgb-bayesian-opt_m2
"[""# <a id='1'>1. Librairies and data</a> ""]",3,vincentlugat,ieee-lgb-bayesian-opt,vincentlugat_ieee-lgb-bayesian-opt_m3
['## DATASETS'],4,vincentlugat,ieee-lgb-bayesian-opt,vincentlugat_ieee-lgb-bayesian-opt_m4
"['## MERGE, MISSING VALUE, FILL NA']",5,vincentlugat,ieee-lgb-bayesian-opt,vincentlugat_ieee-lgb-bayesian-opt_m5
['Source : https://www.kaggle.com/vaishvik25/refine-ieee-data'],6,vincentlugat,ieee-lgb-bayesian-opt,vincentlugat_ieee-lgb-bayesian-opt_m6
['## ENCODING'],7,vincentlugat,ieee-lgb-bayesian-opt,vincentlugat_ieee-lgb-bayesian-opt_m7
"[""# <a id='2'>2. Bayesian Optimisation</a> ""]",8,vincentlugat,ieee-lgb-bayesian-opt,vincentlugat_ieee-lgb-bayesian-opt_m8
['## CONFUSION MATRIX'],9,vincentlugat,ieee-lgb-bayesian-opt,vincentlugat_ieee-lgb-bayesian-opt_m9
"[""# <a id='3'>3. LGB + best hyperparameters</a> ""]",10,vincentlugat,ieee-lgb-bayesian-opt,vincentlugat_ieee-lgb-bayesian-opt_m10
"[""# <a id='4'>4. Features importance</a> ""]",11,vincentlugat,ieee-lgb-bayesian-opt,vincentlugat_ieee-lgb-bayesian-opt_m11
"[""# <a id='5'>5. Submission</a> ""]",12,vincentlugat,ieee-lgb-bayesian-opt,vincentlugat_ieee-lgb-bayesian-opt_m12
['looks like merging the two data tables has created a lot of missing values. let us first try to identify good features from these.'],0,viswajithkn,fraud-detection,viswajithkn_fraud-detection_m0
"['Now that we have identified good numeric variables, let us use just the missing data as benchmark for the categorical variables']",1,viswajithkn,fraud-detection,viswajithkn_fraud-detection_m1
"['# Bounded region of parameter space\n', 'bounds_LGB = {\n', ""    'num_leaves': (31, 500), \n"", ""    'min_data_in_leaf': (20, 200),\n"", ""    'bagging_fraction' : (0.1, 0.9),\n"", ""    'feature_fraction' : (0.1, 0.9),\n"", ""    'learning_rate': (0.01, 0.3),\n"", ""    'min_child_weight': (1, 4),   \n"", ""    'reg_alpha': (0.2,2), \n"", ""    'reg_lambda': (0.2,2),\n"", ""    'max_depth':(-1,50),\n"", ""    'n_estimators':(750,7500),\n"", ""    'max_bin':(32,256)\n"", '}\n', '\n', 'from bayes_opt import BayesianOptimization\n', 'from sklearn.model_selection import train_test_split\n', 'X_train, X_test, y_train, y_test = train_test_split(train_data, target_data, test_size=0.25, random_state=42)\n', 'train_index = X_train.index\n', 'test_index = X_test.index\n', '\n', 'def LGB_bayesian(num_leaves,bagging_fraction,feature_fraction,min_child_weight,\n', '                 min_data_in_leaf,max_depth,n_estimators,reg_alpha,reg_lambda,learning_rate,max_bin):\n', '    num_leaves = int(num_leaves)\n', '    min_data_in_leaf = int(min_data_in_leaf)\n', '    max_depth = int(max_depth)\n', '    n_estimators = int(n_estimators)\n', '    min_child_weight = int(min_child_weight)\n', '    max_bin = int(max_bin)\n', '\n', '    assert type(num_leaves) == int\n', '    assert type(min_data_in_leaf) == int\n', '    assert type(max_depth) == int\n', '    assert type(n_estimators) == int\n', '    assert type(min_child_weight) == int\n', '    assert type(max_bin) == int\n', ""    param = {'num_leaves': num_leaves,'min_data_in_leaf': min_data_in_leaf,'min_child_weight': min_child_weight,'bagging_fraction' : bagging_fraction,\n"", ""             'feature_fraction' : feature_fraction,'max_depth': max_depth,'reg_alpha': reg_alpha,'reg_lambda': reg_lambda,\n"", ""              'objective': 'binary','boosting_type': 'gbdt','colsample_bytree':.8,'subsample':.9,'min_split_gain':.01,'max_bin':max_bin,\n"", ""             'bagging_freq':5,'learning_rate':learning_rate,'metric':'auc','n_estimators':n_estimators,'min_data_in_leaf':min_data_in_leaf,\n"", ""            'early_stopping_rounds':100} \n"", ""    lgb_bayes = LGBMClassifier(boosting = param['boosting_type'],n_estimators =  param['n_estimators'],\n"", ""                     learning_rate =  param['learning_rate'],num_leaves =  param['num_leaves'],\n"", ""                     colsample_bytree = param['colsample_bytree'],subsample =  param['subsample'],\n"", ""                     max_depth =  param['max_depth'],reg_alpha =  param['reg_alpha'],\n"", ""                     reg_lambda =  param['reg_lambda'],min_split_gain =  param['min_split_gain'],\n"", ""                     min_child_weight =  param['min_child_weight'],max_bin =  param['max_bin'],\n"", ""                     bagging_freq =  param['bagging_freq'],feature_fraction =  param['feature_fraction'],\n"", ""                     bagging_fraction =  param['bagging_fraction'],min_data_in_leaf = param['min_data_in_leaf'],\n"", ""                              early_stopping_rounds = param['early_stopping_rounds'])\n"", '    lgb_bayes.fit(train_data.iloc[train_index,:], target_data.iloc[train_index],\n', '                 eval_set = [(train_data.iloc[train_index,:], target_data.iloc[train_index]), \n', ""                             (train_data.iloc[test_index,:], target_data.iloc[test_index])],eval_metric='auc',verbose = 200)\n"", '    fpr, tpr, auc_score = compute_roc_auc(lgb_bayes,test_index)  \n', '    return auc_score\n', 'lightGBM_bo = BayesianOptimization(LGB_bayesian, bounds_LGB, random_state=42)\n', 'print(lightGBM_bo.space.keys)\n', 'init_points = 10\n', 'n_iter = 35\n', ""print('-' * 130)\n"", '\n', ""lightGBM_bo.maximize(init_points=init_points, n_iter=n_iter, acq='ucb', xi=0.0, alpha=1e-6)\n"", ""print(lightGBM_bo.max['target'])\n"", ""print(lightGBM_bo.max['params'])\n"", 'params = {\n', ""        'n_estimators': int(lightGBM_bo.max['params']['n_estimators']), \n"", ""        'num_leaves': int(lightGBM_bo.max['params']['num_leaves']), \n"", ""        'min_child_weight': lightGBM_bo.max['params']['min_child_weight'],\n"", ""        'min_data_in_leaf': int(lightGBM_bo.max['params']['min_data_in_leaf']),\n"", ""        'bagging_fraction': lightGBM_bo.max['params']['bagging_fraction'], \n"", ""        'feature_fraction': lightGBM_bo.max['params']['feature_fraction'],\n"", ""        'reg_lambda': lightGBM_bo.max['params']['reg_lambda'],\n"", ""        'reg_alpha': lightGBM_bo.max['params']['reg_alpha'],\n"", ""        'max_depth': int(lightGBM_bo.max['params']['max_depth']), \n"", ""        'metric':'auc',\n"", ""        'boosting_type': 'gbdt',\n"", ""        'colsample_bytree':.8,'subsample':.9,\n"", ""        'min_split_gain':.01,\n"", ""        'max_bin':int(lightGBM_bo.max['params']['max_bin']), #127,\n"", ""        'bagging_freq':5,\n"", ""        'learning_rate':lightGBM_bo.max['params']['learning_rate'],\n"", ""        #'learning_rate':0.01,\n"", ""        'early_stopping_rounds':100\n"", '    }']",2,viswajithkn,fraud-detection,viswajithkn_fraud-detection_m2
['![fraud%20detection.jpeg](attachment:fraud%20detection.jpeg)'],0,wti200,xgboost-with-binning-and-imputaion,wti200_xgboost-with-binning-and-imputaion_m0
"['The purpose of this notbook is to perform the following tasks:\n', ' \n', ' 1. [Load Packages and Data](#1)\n', ' 2. [Exploratory Data Analysis](#2)\n', ' 3. [Feature Engineering](#3)\n', ' 4. [Binning of variables and Imputation of missing values](#4)\n', ' 5. [Outlier Analysis and Feature Scaling](#5)\n', ' 6. [Modeling: xgboost](#6)\n', ' \n', 'Main Findings:\n', ' \n', ' * The feature set can be divided into 3 data types: \n', ' \n', '  1. Categorical Variables, \n', '  2. Numerical Variables and \n', '  3. Numeric Encoding Variables {C_X, D_X and V_X} \n', '  \n', '  \n', ' * 41% of the train_transaction is missing data.\n', ' * 36% of the test_transaction is missing data.\n', ' * 35% of train_identity is missing data.\n', ' * 36% of test_identity is missing data.\n', ' \n', 'At the end of the task the training and test datasets are imputed for missing values, scaled and fixed for outliers and features are binned to prevent overfitting. The results are saved as outputs of this notebook. The user can use the outputs of this notebook for model building.']",1,wti200,xgboost-with-binning-and-imputaion,wti200_xgboost-with-binning-and-imputaion_m1
"['### [Load Packages and Data](#1)<a id=""1""></a> <br>']",2,wti200,xgboost-with-binning-and-imputaion,wti200_xgboost-with-binning-and-imputaion_m2
"['### [Exploratory Data Analysis](#2)<a id=""2""></a> <br>\n', ' \n', ' * Variables C_X and D_X have only integers as values and most of V_X as well. \n', ' * I strongly believe that C_X, D_X and V_X are numeric encoding. This is important to know when imputing the missing values, performing outlier analysis and modelling.\n', ' * Varibles dist1, dist2 and TransactionAmt have a long tail distribution. Agian this is also important to realize when performing outlier analysis.\n', ' * 41% of the train_transaction is missing.\n', ' * 36% of the test_transaction is missing.\n', ' * 35% of train_identity is missing data.\n', ' * 36% of test_identity is missing data.']",3,wti200,xgboost-with-binning-and-imputaion,wti200_xgboost-with-binning-and-imputaion_m3
"['### [Feature Engineering](#3)<a id=""3""></a> <br>\n', '\n', '* Day of the week and hour are engineerd from TransactionDT.\n', '* The decimal part of the TransactionAmt is engineerd as an separate feature. \n']",4,wti200,xgboost-with-binning-and-imputaion,wti200_xgboost-with-binning-and-imputaion_m4
"['### [Binning of variables and Imputation of missing values](#4)<a id=""4""></a> <br>\n', ' \n', '  * Before we go into the missing value imputation I have explicitly changed the dataype of some of the categorical features into type boolean beacuse some of the categorical features are loaded as numerical.\n', '  * For numerical features dist1, dist2 and TransactionAmt the missing values are replaced by the mean of the column. The missing value of other features are replaced by -999\n', '  * Variables C_X and D_X have only integers as values and most of V_X as well. There missing values are replaced by -999.\n', '  * The rare values of all features other than dist1, dist2, TransactionAmt and TransactionDT are binned together. This is a very important step to perform to prevent overfitting when performing modelling.\n', '  * To reduce the size of the datasets the kernel published by [MJ Bahamani](https://www.kaggle.com/mjbahmani/reducing-memory-size-for-ieee) is applied to the datasets. It is a very helpfull function.']",5,wti200,xgboost-with-binning-and-imputaion,wti200_xgboost-with-binning-and-imputaion_m5
"['### [Outlier Analysis and Feature Scaling](#5)<a id=""5""></a> <br>\n', ' \n', '  * I have taken these two subjects under one section because I am using **RobustScaler()** to scale dist1, dist2 and TransactionAmt. **RobustScaler()** applies Inter Quartile Range(IQR) to scale the features when the argument **with_scaling=True**.\n', '  * Outlier Analysis is contentious territory. I have opted for Inter Quartile Range (IQR) to detect outliers since it does not assume any distribution for the features.\n', '  *  There are different ways to perform feature scaling. Here it has been opted for **RobustScaler()** because it is relatively more robust outliers. [Here](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html) you can find more information regarding this scaler. \n', '  *  It is applied only to dist1, dist2 and TransactionAmt features.']",6,wti200,xgboost-with-binning-and-imputaion,wti200_xgboost-with-binning-and-imputaion_m6
"['### [Modeling: Xgboost](#6)<a id=""6""></a> <br>\n', '\n', '* The parameters are not optimised.\n', '* The xgboost model is based on [this](copied from https://www.kaggle.com/xhlulu/ieee-fraud-xgboost-with-gpu-fit-in-40s) great kernel. ']",7,wti200,xgboost-with-binning-and-imputaion,wti200_xgboost-with-binning-and-imputaion_m7
"['Get feature importance of each feature. Importance type can be defined as:\n', '\n', '‘weight’: the number of times a feature is used to split the data across all trees.\n', '\n', '‘gain’: the average gain across all splits the feature is used in.\n', '\n', '‘cover’: the average coverage across all splits the feature is used in.\n', '\n', '‘total_gain’: the total gain across all splits the feature is used in.\n', '\n', '‘total_cover’: the total coverage across all splits the feature is used in.']",8,wti200,xgboost-with-binning-and-imputaion,wti200_xgboost-with-binning-and-imputaion_m8
"['# General\n', ""I'm very sorry that I changed the name of this kernel. This is becuase I often update this kernel and the content is changing.\n"", '\n', 'Thanks to VESTA, we can know about what the addr2 data is.\n', '\n', 'https://www.kaggle.com/c/ieee-fraud-detection/discussion/101203#latest-596566\n', '\n', 'In this discussion(the URL↑), VESTA person says ""addr2 as billing country"".\n', '\n', 'Therefore, I want to identify the country and use data more efficiently.\n', '\n', 'I wish that this sharing help other kagglers!']",0,yasagure,classify-country-of-addr2-addr2-usage,yasagure_classify-country-of-addr2-addr2-usage_m0
"['# what already is known\n', '\n', 'I consulted with this discussion.\n', '\n', 'https://www.kaggle.com/c/ieee-fraud-detection/discussion/100400#579001\n', '\n', 'We can know the hour data from transactionDT.\n', '\n', 'Therefore, the gap of standard time should be supeculated.']",1,yasagure,classify-country-of-addr2-addr2-usage,yasagure_classify-country-of-addr2-addr2-usage_m1
['## the appearance of addr2'],2,yasagure,classify-country-of-addr2-addr2-usage,yasagure_classify-country-of-addr2-addr2-usage_m2
"['Various country data is used.\n', '\n', 'I use only the countries which have more tha 50 transaction data, because I can not guess the precise standard time if the amount of data is small.\n', '\n', 'That is, 16.0 31.0 32.0 60.0 65.0 87.0 96.0']",3,yasagure,classify-country-of-addr2-addr2-usage,yasagure_classify-country-of-addr2-addr2-usage_m3
"['### make hour column from transactionDT\n', '\n', 'As I said, I consulted with the discussion. Thanks for people who excite the discussion. \n']",4,yasagure,classify-country-of-addr2-addr2-usage,yasagure_classify-country-of-addr2-addr2-usage_m4
"['## speculation of standard time\n', 'At first, the country ""87.0"" has 520481 transaction.\n', '\n', 'This must be the U.S.\n', '\n', 'I checked the gap between the U.S. and others.']",5,yasagure,classify-country-of-addr2-addr2-usage,yasagure_classify-country-of-addr2-addr2-usage_m5
"['# 16,65\n', 'One of them may be Russia, but it is uncertain.\n', '\n', 'The possibility of china also exist.\n', '\n', 'However, we can say that these two countries are similar place to Russia or China.\n', '\n', 'I name them Asia group.']",6,yasagure,classify-country-of-addr2-addr2-usage,yasagure_classify-country-of-addr2-addr2-usage_m6
"['# 31,32\n', 'The standard time is similar to America.\n', 'Juding from country size, 32 should be Canada. \n', '\n', 'However, it may be Mexico.']",7,yasagure,classify-country-of-addr2-addr2-usage,yasagure_classify-country-of-addr2-addr2-usage_m7
"['# 60,96\n', 'They are about -6 hour. \n', 'Threfore, it should be Europe country.']",8,yasagure,classify-country-of-addr2-addr2-usage,yasagure_classify-country-of-addr2-addr2-usage_m8
"['# making future\n', 'At first, I made Europe future.\n', '\n', ""Soon, I'll increase it.""]",9,yasagure,classify-country-of-addr2-addr2-usage,yasagure_classify-country-of-addr2-addr2-usage_m9
['There is a gap between them.'],10,yasagure,classify-country-of-addr2-addr2-usage,yasagure_classify-country-of-addr2-addr2-usage_m10
['We can find the difference between them.'],11,yasagure,classify-country-of-addr2-addr2-usage,yasagure_classify-country-of-addr2-addr2-usage_m11
"['# Conclusion\n', 'This identification is meningful to some degree, because we can find the gap between Europe data and other data.\n', '\n', 'The problem is that America has some standart time, so some error is appearing, for example, we can not boastfuly say that 16 is Russia.\n', '\n', 'I want to apply this result for futher data_engineering.']",12,yasagure,classify-country-of-addr2-addr2-usage,yasagure_classify-country-of-addr2-addr2-usage_m12
"['# Before going to main content\n', 'new feature was being created.\n', '\n', 'V1 V319-V320\n', '\n', 'V7 V109-V110\n', '\n', 'V12 V329-V330\n', '\n', 'V14 V316-V331\n', '\n', 'V18 V4-V5\n', ' \n', '\n', ""**Sorry, I want to update quickly this kernel, but I can't because these days kernel's commiting is too busy! **\n"", '\n', 'This kernel is using official kernel for judging whether my new feature is meaningful.\n', '\n', 'The official kernel is here(https://www.kaggle.com/inversion/ieee-simple-xgboost)\n', '\n', 'And that the feature which I made is explained in detail here ( https://www.kaggle.com/yasagure/how-do-we-treat-with-similar-columns-v319-v321/edit/run/18988983)']",0,yasagure,contrive-usage-of-v-features-in-official-xgboost,yasagure_contrive-usage-of-v-features-in-official-xgboost_m0
"['# Introduction - Do they match?\n', '**Some people throw away similar columns, but it is good thing?**\n', '\n', 'Everyone found that there is a lot of similar columns in this dataset.\n', '\n', 'V319-V320 and V109-V110 are good example of them.\n', '\n', 'How should we ""use"" it?\n', '\n', 'Most people throw away the data, but I think that I can get useful information from it.\n', '\n', '**My idea is paying attention to whether they match each other.**']",1,yasagure,contrive-usage-of-v-features-in-official-xgboost,yasagure_contrive-usage-of-v-features-in-official-xgboost_m1
['# load dataset'],2,yasagure,contrive-usage-of-v-features-in-official-xgboost,yasagure_contrive-usage-of-v-features-in-official-xgboost_m2
"['We want to both of data -""identity"" and ""transaction""-\n', '\n', 'The identity data should be added to transaction data. ']",3,yasagure,contrive-usage-of-v-features-in-official-xgboost,yasagure_contrive-usage-of-v-features-in-official-xgboost_m3
['# How V-feature is?'],4,yasagure,contrive-usage-of-v-features-in-official-xgboost,yasagure_contrive-usage-of-v-features-in-official-xgboost_m4
"['V features have a lot of similar columns.\n', '\n', ""Let's look at it.""]",5,yasagure,contrive-usage-of-v-features-in-official-xgboost,yasagure_contrive-usage-of-v-features-in-official-xgboost_m5
"['# Feature engineering\n', 'I picked up these 3 pairs of columns. ']",6,yasagure,contrive-usage-of-v-features-in-official-xgboost,yasagure_contrive-usage-of-v-features-in-official-xgboost_m6
['They seem to be similar.'],7,yasagure,contrive-usage-of-v-features-in-official-xgboost,yasagure_contrive-usage-of-v-features-in-official-xgboost_m7
['# look at appearance of this new feature '],8,yasagure,contrive-usage-of-v-features-in-official-xgboost,yasagure_contrive-usage-of-v-features-in-official-xgboost_m8
['## V319-V320'],9,yasagure,contrive-usage-of-v-features-in-official-xgboost,yasagure_contrive-usage-of-v-features-in-official-xgboost_m9
['## V109-V110'],10,yasagure,contrive-usage-of-v-features-in-official-xgboost,yasagure_contrive-usage-of-v-features-in-official-xgboost_m10
['## V329-V330'],11,yasagure,contrive-usage-of-v-features-in-official-xgboost,yasagure_contrive-usage-of-v-features-in-official-xgboost_m11
['## V316-V331'],12,yasagure,contrive-usage-of-v-features-in-official-xgboost,yasagure_contrive-usage-of-v-features-in-official-xgboost_m12
['## V4-V5'],13,yasagure,contrive-usage-of-v-features-in-official-xgboost,yasagure_contrive-usage-of-v-features-in-official-xgboost_m13
"['There seem to be difference, but the gap of ""diff_V109_V110"" is small.\n', '\n', 'In Version7, I found that ""diff_V109_V110"" is not meaningful. I deleted.\n', '\n', 'In Version13, I found that ""diff_V329_V330"" is not meaningful. I deleted.\n', '\n', 'In Version20, I found that ""diff_V4_V5"" is not meaningful. I deleted.\n', '\n', 'I feel that when the gap is big, the column is meaningful.']",14,yasagure,contrive-usage-of-v-features-in-official-xgboost,yasagure_contrive-usage-of-v-features-in-official-xgboost_m14
['# data cleaning'],15,yasagure,contrive-usage-of-v-features-in-official-xgboost,yasagure_contrive-usage-of-v-features-in-official-xgboost_m15
"['## Label Encoding\n', 'We cannot use literal features for XGB, so these features are changes.\n', '\n', 'For example, [H,G,W,A] →[0,1,2,3]\n', '\n', 'The number of the words is often related to the numeral([0,1,2,3]).']",16,yasagure,contrive-usage-of-v-features-in-official-xgboost,yasagure_contrive-usage-of-v-features-in-official-xgboost_m16
['# Model'],17,yasagure,contrive-usage-of-v-features-in-official-xgboost,yasagure_contrive-usage-of-v-features-in-official-xgboost_m17
"['# Is it useful?\n', 'The score of V5 is 0.9367.\n', '\n', 'The official score was 0.9366.\n', '\n', 'The score seem to be  improved.\n', '\n', 'The score of V8(V109-V110 added) is not unknown. Soon, I tell you it. ←the diff V109-V110 does not seem to be useful.\n', '\n', 'Also, the diff V329-V330 does not seem to be useful.']",18,yasagure,contrive-usage-of-v-features-in-official-xgboost,yasagure_contrive-usage-of-v-features-in-official-xgboost_m18
"['# Conclusion\n', 'Some people think this is useful, but others not.\n', '\n', 'If you interested in it, please use for your model and judge wheter these columns are useful for your model.']",19,yasagure,contrive-usage-of-v-features-in-official-xgboost,yasagure_contrive-usage-of-v-features-in-official-xgboost_m19
"['##Attention\n', '\n', 'Thanks to comments on this kernel, I found some problem.\n', '\n', '*In this kernel, the datetime of transaction is not seriously discussed.\n', '\n', '*Transactions which have the identity data are more doubtful.\n', '\n', 'In V3, I fixed about second problem, but still have the first problem\n', '\n', 'And that, I mainly focused on getting interesting information, not on getting useful information only for this competition. ']",0,yasagure,fraud-makers-may-be-earnest-people-about-browser,yasagure_fraud-makers-may-be-earnest-people-about-browser_m0
"['**General**\n', '\n', 'After provoking the discussion (https://www.kaggle.com/c/ieee-fraud-detection/discussion/103565), I want to examine it.\n', '\n', 'I wish that this kernel would help your research! \n']",1,yasagure,fraud-makers-may-be-earnest-people-about-browser,yasagure_fraud-makers-may-be-earnest-people-about-browser_m1
"['I focused on browser!\n', '\n', 'I am a very lazy person, so I rarely update my browser.\n', '\n', 'How swindlers are? ']",2,yasagure,fraud-makers-may-be-earnest-people-about-browser,yasagure_fraud-makers-may-be-earnest-people-about-browser_m2
"['There are a lot of data from which we cannot say that someone are try hard to update browser.\n', '\n', 'Therefore, most of them was NaN.']",3,yasagure,fraud-makers-may-be-earnest-people-about-browser,yasagure_fraud-makers-may-be-earnest-people-about-browser_m3
"['I consulted with wikipedia about chrome.\n', '\n', 'https://en.wikipedia.org/wiki/Google_Chrome_version_history\n', 'On December 5th in 2017, chrome 63 version was released.\n', '\n', 'I determined 63 was lastest.\n', '\n', 'It may be more useful if you use the date information.\n']",4,yasagure,fraud-makers-may-be-earnest-people-about-browser,yasagure_fraud-makers-may-be-earnest-people-about-browser_m4
"[""**Let's check!**""]",5,yasagure,fraud-makers-may-be-earnest-people-about-browser,yasagure_fraud-makers-may-be-earnest-people-about-browser_m5
['##Conclusion'],6,yasagure,fraud-makers-may-be-earnest-people-about-browser,yasagure_fraud-makers-may-be-earnest-people-about-browser_m6
"['This result suggests that people who update browser are more possibly fraud makers than people who are lazy about browser. \n', '\n', 'Fraud makers may be earnest people.\n']",7,yasagure,fraud-makers-may-be-earnest-people-about-browser,yasagure_fraud-makers-may-be-earnest-people-about-browser_m7
"[""# Beginner's Random Forests example\n"", '\n', 'This is a very simple Random Forests example meant for beginners. This is not meant to achieve a high score, merely a starting point on which to start, without complicated techniques.\n', '\n', ""It's recommended that you finish the Kaggle Learn courses (introduction and intermediate machine learning).""]",0,yoongkang,beginner-s-random-forest-example,yoongkang_beginner-s-random-forest-example_m0
"['## Files\n', '\n', 'As can be seen the training data contains two files, `train_transaction.csv` and `train_identity.csv`. These two tables are related to each other via the column `TransactionID`. ']",1,yoongkang,beginner-s-random-forest-example,yoongkang_beginner-s-random-forest-example_m1
"[""In order to use these files for training, we'll need to do what's sometimes called denormalising the data. We can do this by doing a left join on both tables using the DataFrame's `merge()` method.""]",2,yoongkang,beginner-s-random-forest-example,yoongkang_beginner-s-random-forest-example_m2
"[""Let's do a sanity check whenever we do something like this. Make sure the shape contains the same number of rows and the combined columns:""]",3,yoongkang,beginner-s-random-forest-example,yoongkang_beginner-s-random-forest-example_m3
"[""Looks like that worked! But we're now using a lot of RAM (look at the sidebar of your kernel). My kernel currently says I'm at 5 gigabytes, and we haven't even read the test set yet!\n"", '\n', ""While cleaning the data, it's possible we may need to make copies of (some sections) of the data, so this is obviously not ideal.""]",4,yoongkang,beginner-s-random-forest-example,yoongkang_beginner-s-random-forest-example_m4
"['## Memory reduction\n', '\n', 'As discussed in [my other kernel](https://www.kaggle.com/yoongkang/beginner-memory-reduction-techniques), parsing the training dataset with default settings could take up to 2GBs of memory unnecessarily. With a few techniques (also discussed in the linked kernel) we can cut this down by about a gigabyte. \n', '\n', ""In this kernel, we'll use similar techniques, if you want to see my reasoning for this please refer to the other kernel.\n"", '\n', 'First we need to determine which numeric columns we have so that we can downcast them (cast from float64 to another type that requires less memory).']",5,yoongkang,beginner-s-random-forest-example,yoongkang_beginner-s-random-forest-example_m5
"['As described in the other kernel, some of these columns are `float64` by default due to the presence of some `NaN` values.\n', '\n', ""The fact that they are `NaN` might be meaningful in this context, so completely replacing them (this is called imputation), doesn't sound like a great idea. However, we can add a boolean column to mark that the column has been replaced, and hopefully the training algorithm is smart enough to take care of it. There's no guarantee that it will, though! So you should always challenge your assumptions (e.g. maybe just dropping the columns could give you similar results, and train faster).\n"", '\n', ""Also bear in mind we'll need to use the same values we're using to impute on the training set on the test set. ""]",6,yoongkang,beginner-s-random-forest-example,yoongkang_beginner-s-random-forest-example_m6
"[""Now that we've removed all the `NaN` values, we can downcast the columns to the lowest precision.\n"", '\n', ""First we'll need to know which columns are integers, though! The following snippet does just that.""]",7,yoongkang,beginner-s-random-forest-example,yoongkang_beginner-s-random-forest-example_m7
"[""There will be some errors printed, but that's normal because some numeric columns are already integers. I'm too lazy to fix that right now.\n"", '\n', ""Let's look at some stats.""]",8,yoongkang,beginner-s-random-forest-example,yoongkang_beginner-s-random-forest-example_m8
"[""So we can see here that there are some very small ranges there -- not all of them will need to be `float64`. Let's downcast them.""]",9,yoongkang,beginner-s-random-forest-example,yoongkang_beginner-s-random-forest-example_m9
"['Looks like we shaved a whole gig.\n', '\n', ""We're not done yet, we need to make sure we do the same thing on the test set. Let's read it in now, merge the tables, and impute it the same way.""]",10,yoongkang,beginner-s-random-forest-example,yoongkang_beginner-s-random-forest-example_m10
"['We added some columns in our training set and replaced missing values with the medians. We need to add those same columns, and also add the median from the training set for those missing values (the same ones).']",11,yoongkang,beginner-s-random-forest-example,yoongkang_beginner-s-random-forest-example_m11
"[""Unfortunately, we're not done yet. We might have some missing values on other numeric columns we didn't anticipate. \n"", '\n', 'In practice, we don\'t always have a ""test set"". The test set might be new observations that come in the future, could be a list of observations or a single one, so we can\'t really use statistics from the test set to impute the missing values. So we need to use values from the training set.\n', '\n', ""We'll use the median as well.""]",12,yoongkang,beginner-s-random-forest-example,yoongkang_beginner-s-random-forest-example_m12
"['Now, we can downcast numeric columns in the same way']",13,yoongkang,beginner-s-random-forest-example,yoongkang_beginner-s-random-forest-example_m13
"['## Categorical values\n', '\n', ""Okay, we've reduced memory, but we still need to deal with categorical values. As machine learning algorithms don't understand things like strings, we need to convert them into numbers. This is called encoding.\n"", '\n', ""We could use either label encoding, which replaces each category into a numerical representation, or use one hot encoding which creates a separate column for each category. In general, one hot encoding performs better. However, in this case we have columns with very high cardinality -- and since we have a large dataset, it's probably more practical to use label encoding which we'll do.\n"", '\n', ""Two things we need to deal with for label encoding are missing values and unknown values. Missing values means the data is simply not there, whereas unknown values are values in the test set that we don't have in the training set.\n"", '\n', 'For missing values, we\'ll just replace them with a label, e.g. the string `""missing""`. That\'s pretty straightforward.\n', '\n', 'For unknown values, that requires a bit more thought. The main question is whether or not we have all the categories a priori. If we know all the possible categories beforehand (i.e. fixed categories like gender, state, postcodes) then we can go ahead and devise a mapping beforehand for all possible values. However, sometimes categories only come in the future, like mobile phone models. In the latter case, we have no way of knowing all the possible future values, and thus we can\'t map them -- so we\'ll need another strategy, i.e. replace them with a different label like the string `""unknown""`. We\'ll be doing that.']",14,yoongkang,beginner-s-random-forest-example,yoongkang_beginner-s-random-forest-example_m14
"['First, we\'ll replace missing values with the string `""missing""` (we actually don\'t need to do this since pandas does it automatically, but I like to give it an explicit label, makes it easier to see).']",15,yoongkang,beginner-s-random-forest-example,yoongkang_beginner-s-random-forest-example_m15
"[""Next we'll convert the columns in the training set to categorical.""]",16,yoongkang,beginner-s-random-forest-example,yoongkang_beginner-s-random-forest-example_m16
"[""Then we'll convert the test set.""]",17,yoongkang,beginner-s-random-forest-example,yoongkang_beginner-s-random-forest-example_m17
"[""Now we're more or less done with the minimum preprocessing required. Let's save our progress to a feather file, so that we don't have to go through it again!""]",18,yoongkang,beginner-s-random-forest-example,yoongkang_beginner-s-random-forest-example_m18
"['## Validation set\n', '\n', ""Now we can start training our model. But how do we know if a model is good or not? We commonly use something called a validation set, that is separate to the test set. The reason we have a holdout set is that we use the validation set to choose our model (even if we don't use it for training), otherwise our model will overfit. If you're unfamiliar with this, I suggest reading on overfitting and underfitting.\n"", '\n', ""The data description seems to indicate that the data is time ordered, so we don't really want a random split. So let's hold out a portion of the bottom rows to use as our validation set, and the rest as our training set.""]",19,yoongkang,beginner-s-random-forest-example,yoongkang_beginner-s-random-forest-example_m19
"['## Training the model\n', '\n', 'Now we can finally train a model. You can iterate on this part.']",20,yoongkang,beginner-s-random-forest-example,yoongkang_beginner-s-random-forest-example_m20
"['Using the whole training set is too time-consuming for quick iteration, so we can use a sample. \n', '\n', ""We could use a random sample, but since this is time-ordered, I'm guessing the more recent rows would give us better predictive value. So let's just grab the bottom rows.""]",21,yoongkang,beginner-s-random-forest-example,yoongkang_beginner-s-random-forest-example_m21
['## Submission'],22,yoongkang,beginner-s-random-forest-example,yoongkang_beginner-s-random-forest-example_m22
"['Now that we have a decent model, we can actually train on the whole dataset, including the validation set.']",23,yoongkang,beginner-s-random-forest-example,yoongkang_beginner-s-random-forest-example_m23
"['Thanks to  \n', 'https://www.kaggle.com/iasnobmatsu/xgb-model-with-feature-engineering']",0,yw6916,lgb-xgb-ensemble-stacking-based-on-fea-eng,yw6916_lgb-xgb-ensemble-stacking-based-on-fea-eng_m0
"['For StackNet stacking, thanks to   \n', 'https://www.kaggle.com/carlolepelaars/ensembling-with-stacknet']",1,yw6916,lgb-xgb-ensemble-stacking-based-on-fea-eng,yw6916_lgb-xgb-ensemble-stacking-based-on-fea-eng_m1
"['Ensemble only atm, stacking is on the way']",2,yw6916,lgb-xgb-ensemble-stacking-based-on-fea-eng,yw6916_lgb-xgb-ensemble-stacking-based-on-fea-eng_m2
"['### Import libraries and data, reduce memory usage']",3,yw6916,lgb-xgb-ensemble-stacking-based-on-fea-eng,yw6916_lgb-xgb-ensemble-stacking-based-on-fea-eng_m3
"['### Some Feature Engineering\n', '\n', 'drop columns, count encoding, aggregation, fillna']",4,yw6916,lgb-xgb-ensemble-stacking-based-on-fea-eng,yw6916_lgb-xgb-ensemble-stacking-based-on-fea-eng_m4
['### XGB LGB models and training'],5,yw6916,lgb-xgb-ensemble-stacking-based-on-fea-eng,yw6916_lgb-xgb-ensemble-stacking-based-on-fea-eng_m5
['### Stacking models and training'],6,yw6916,lgb-xgb-ensemble-stacking-based-on-fea-eng,yw6916_lgb-xgb-ensemble-stacking-based-on-fea-eng_m6
['cut down features to avoid the kernel die'],7,yw6916,lgb-xgb-ensemble-stacking-based-on-fea-eng,yw6916_lgb-xgb-ensemble-stacking-based-on-fea-eng_m7
# Home Credit Default Risk 2018,0,aantonova,797-lgbm-and-bayesian-optimization,aantonova_797-lgbm-and-bayesian-optimization_m0
__Warning!__ This kernel cannot run on Kaggle: not enough memory. But the code works fine and quickly on the local computer with the same amount of memory.,1,aantonova,797-lgbm-and-bayesian-optimization,aantonova_797-lgbm-and-bayesian-optimization_m1
"Based on kernels: 

- https://www.kaggle.com/jsaguiar/updated-0-792-lb-lightgbm-with-simple-features

- https://www.kaggle.com/poohtls/fork-of-fork-lightgbm-with-simple-features",2,aantonova,797-lgbm-and-bayesian-optimization,aantonova_797-lgbm-and-bayesian-optimization_m2
## Aggregating datasets,3,aantonova,797-lgbm-and-bayesian-optimization,aantonova_797-lgbm-and-bayesian-optimization_m3
### Service functions,4,aantonova,797-lgbm-and-bayesian-optimization,aantonova_797-lgbm-and-bayesian-optimization_m4
### Aggregating functions,5,aantonova,797-lgbm-and-bayesian-optimization,aantonova_797-lgbm-and-bayesian-optimization_m5
## Cleaning dataset,6,aantonova,797-lgbm-and-bayesian-optimization,aantonova_797-lgbm-and-bayesian-optimization_m6
## Optimization LGBM parameters,7,aantonova,797-lgbm-and-bayesian-optimization,aantonova_797-lgbm-and-bayesian-optimization_m7
### Optimization and visualisation functions,8,aantonova,797-lgbm-and-bayesian-optimization,aantonova_797-lgbm-and-bayesian-optimization_m8
### Table for scores,9,aantonova,797-lgbm-and-bayesian-optimization,aantonova_797-lgbm-and-bayesian-optimization_m9
### First scores with parameters from Tilii kernel,10,aantonova,797-lgbm-and-bayesian-optimization,aantonova_797-lgbm-and-bayesian-optimization_m10
### New Bayesian Optimization,11,aantonova,797-lgbm-and-bayesian-optimization,aantonova_797-lgbm-and-bayesian-optimization_m11
# Home Credit Default Risk 2018,0,aantonova,aggregating-all-tables-in-one-dataset,aantonova_aggregating-all-tables-in-one-dataset_m0
### Importing all libraries,1,aantonova,aggregating-all-tables-in-one-dataset,aantonova_aggregating-all-tables-in-one-dataset_m1
## Application train\test,2,aantonova,aggregating-all-tables-in-one-dataset,aantonova_aggregating-all-tables-in-one-dataset_m2
### loading,3,aantonova,aggregating-all-tables-in-one-dataset,aantonova_aggregating-all-tables-in-one-dataset_m3
### converting categorical features to numeric by frequencies,4,aantonova,aggregating-all-tables-in-one-dataset,aantonova_aggregating-all-tables-in-one-dataset_m4
### dropping features with small variance,5,aantonova,aggregating-all-tables-in-one-dataset,aantonova_aggregating-all-tables-in-one-dataset_m5
## bureau_balance -> bureau,6,aantonova,aggregating-all-tables-in-one-dataset,aantonova_aggregating-all-tables-in-one-dataset_m6
"### Bureau_balance: loading, converting to numeric, dropping",7,aantonova,aggregating-all-tables-in-one-dataset,aantonova_aggregating-all-tables-in-one-dataset_m7
"### Bureau: loading, converting to numeric, dropping",8,aantonova,aggregating-all-tables-in-one-dataset,aantonova_aggregating-all-tables-in-one-dataset_m8
### agregating Bureau_balance features into Bureau dataset,9,aantonova,aggregating-all-tables-in-one-dataset,aantonova_aggregating-all-tables-in-one-dataset_m9
"## installments_payments, credit_card_balance, POS_CASH_balance -> previous_application",10,aantonova,aggregating-all-tables-in-one-dataset,aantonova_aggregating-all-tables-in-one-dataset_m10
"### installments_payments: loading, converting to numeric, dropping",11,aantonova,aggregating-all-tables-in-one-dataset,aantonova_aggregating-all-tables-in-one-dataset_m11
"### credit_card_balance: loading, converting to numeric, dropping",12,aantonova,aggregating-all-tables-in-one-dataset,aantonova_aggregating-all-tables-in-one-dataset_m12
"### POS_CASH_balance: loading, converting to numeric, dropping",13,aantonova,aggregating-all-tables-in-one-dataset,aantonova_aggregating-all-tables-in-one-dataset_m13
"### previous_application: loading, converting to numeric, dropping",14,aantonova,aggregating-all-tables-in-one-dataset,aantonova_aggregating-all-tables-in-one-dataset_m14
### agregating installments_payments features into previous_application dataset,15,aantonova,aggregating-all-tables-in-one-dataset,aantonova_aggregating-all-tables-in-one-dataset_m15
### agregating credit_card_balance features into previous_application dataset,16,aantonova,aggregating-all-tables-in-one-dataset,aantonova_aggregating-all-tables-in-one-dataset_m16
### agregating POS_CASH_balance features into previous_application dataset,17,aantonova,aggregating-all-tables-in-one-dataset,aantonova_aggregating-all-tables-in-one-dataset_m17
"## bureau_agg,  previous_application_agg -> application train\test

This block of cells runs more than rest of one hour. Drop the comments before using.",18,aantonova,aggregating-all-tables-in-one-dataset,aantonova_aggregating-all-tables-in-one-dataset_m18
# A collection of useful (for me) functions,0,aantonova,collection-of-useful-functions-and-simple-baseline,aantonova_collection-of-useful-functions-and-simple-baseline_m0
"This is a collection of scripts which can be useful for this and next competitions, as I think.

There is an example of baseline at the end of this notebook.",1,aantonova,collection-of-useful-functions-and-simple-baseline,aantonova_collection-of-useful-functions-and-simple-baseline_m1
## Service functions,2,aantonova,collection-of-useful-functions-and-simple-baseline,aantonova_collection-of-useful-functions-and-simple-baseline_m2
## For EDA,3,aantonova,collection-of-useful-functions-and-simple-baseline,aantonova_collection-of-useful-functions-and-simple-baseline_m3
## For cross-validation,4,aantonova,collection-of-useful-functions-and-simple-baseline,aantonova_collection-of-useful-functions-and-simple-baseline_m4
## For blending predictions,5,aantonova,collection-of-useful-functions-and-simple-baseline,aantonova_collection-of-useful-functions-and-simple-baseline_m5
# Example of baseline,6,aantonova,collection-of-useful-functions-and-simple-baseline,aantonova_collection-of-useful-functions-and-simple-baseline_m6
This is just an example!,7,aantonova,collection-of-useful-functions-and-simple-baseline,aantonova_collection-of-useful-functions-and-simple-baseline_m7
### Loading datasets,8,aantonova,collection-of-useful-functions-and-simple-baseline,aantonova_collection-of-useful-functions-and-simple-baseline_m8
"### Convert categorical features

Only Label encoding for this example",9,aantonova,collection-of-useful-functions-and-simple-baseline,aantonova_collection-of-useful-functions-and-simple-baseline_m9
### Exploring missing values,10,aantonova,collection-of-useful-functions-and-simple-baseline,aantonova_collection-of-useful-functions-and-simple-baseline_m10
To drop `EXT_SOURCE_1` feature if it's not usefull in next explorations,11,aantonova,collection-of-useful-functions-and-simple-baseline,aantonova_collection-of-useful-functions-and-simple-baseline_m11
### Exploring correlation of features between the train set and target,12,aantonova,collection-of-useful-functions-and-simple-baseline,aantonova_collection-of-useful-functions-and-simple-baseline_m12
### Exploring correlation of features between the train and test sets,13,aantonova,collection-of-useful-functions-and-simple-baseline,aantonova_collection-of-useful-functions-and-simple-baseline_m13
### Selection the best classic model for this dataset,14,aantonova,collection-of-useful-functions-and-simple-baseline,aantonova_collection-of-useful-functions-and-simple-baseline_m14
"The most interesting model is LGBM with the first draft score .757. 

The least interesting one is KNearest.",15,aantonova,collection-of-useful-functions-and-simple-baseline,aantonova_collection-of-useful-functions-and-simple-baseline_m15
### Calculating the first metrics without Bayesian Optimization,16,aantonova,collection-of-useful-functions-and-simple-baseline,aantonova_collection-of-useful-functions-and-simple-baseline_m16
To submit `pred_test` prediction and manually add real LB score in the next cell.,17,aantonova,collection-of-useful-functions-and-simple-baseline,aantonova_collection-of-useful-functions-and-simple-baseline_m17
### Calculating the metrics with Bayesian Optimization (initial seeds),18,aantonova,collection-of-useful-functions-and-simple-baseline,aantonova_collection-of-useful-functions-and-simple-baseline_m18
You can select the best seed for Bayesian Optimization and for CV. I cannot do it in this example.,19,aantonova,collection-of-useful-functions-and-simple-baseline,aantonova_collection-of-useful-functions-and-simple-baseline_m19
### Blending predictions,20,aantonova,collection-of-useful-functions-and-simple-baseline,aantonova_collection-of-useful-functions-and-simple-baseline_m20
"Neural Network with convolution over previous applications and bureau credit reports.

This notebook attempts to predict the defaulters in the competition with a neural network without aggregating data. Appart from the main input with details on the current loan application the details of previous activity will be fed with two auxiliary inputs for both Bureau Credit Reports and Previous Applications.

Preprocessing is limited to standarization, categorical encodings and building the tensors that will feed the neural network. Other than those transformation steps there is no feature engeneering and no aggregation of input tables.

At this point only Current Application, Bureau and Previous Application data is included. This model does not include Installments, Credit Card Balance, Pos Cash Balance or Bureau Balance details.",0,aaraneo,nn-with-convolution-over-prev-app-and-bureau,aaraneo_nn-with-convolution-over-prev-app-and-bureau_m0
This function takes care of the categorical encoding and standarization for non categorical data. Setting combined to true and passing two DF to it allows for combined treatment of train/test data.,1,aaraneo,nn-with-convolution-over-prev-app-and-bureau,aaraneo_nn-with-convolution-over-prev-app-and-bureau_m1
"The network will have a main input (IE application_train.csv) and two additional inputs for previous products (Previous Application and Bureau Data). 

The main input will have an ordinary input matrix of the form (samples, features)

The two additional inputs have a many-to-one relationship with the main dataset, hence the format will be the following: (samples, products, features). The additional dimension, products, identifies the previous applications or bureau reports.

A similar approach can be used and extend this to the next level of detail (credit card balances, installment payments, etc. ). That last level is not implemented in this notebook.

The following function, generate_conv_tensor_simple, builds the additional input tensors as an ndarray. The number of products to consider for each sample is capped (at 24) because the tensor needs a predefined size for convolutions. 

After building the input matrices and tensors they will be stored in H5Py format which Keras can use. Using ndarrays directly was unfeasible in terms of memory (especially when attempting to implement the next level of detail and include data from the more detailed sources). Alternatively a generator/yield scheme could be used but resulted much slower than the h5py method.

The input data frames to the following function need to be pre sorted so as to iterate only once on the samples in the main data source and each detailed source (Previous Application and Bureau).",2,aaraneo,nn-with-convolution-over-prev-app-and-bureau,aaraneo_nn-with-convolution-over-prev-app-and-bureau_m2
"The following custom callback calculates the AUC at the end of each epoch, deals with early stopping and saves the weights to disk if the auc is the best so far. Next to that there is an auc_m custom metric that calculates the same within training but tends to differ a bit from the sklearn numbers. ",3,aaraneo,nn-with-convolution-over-prev-app-and-bureau,aaraneo_nn-with-convolution-over-prev-app-and-bureau_m3
"I would like to describe the main concept, how I am going to start working on analyzing and prediction. As a first stage it would be good to try to use the simplest way to analyze and train data, as you can spend plenty of time to developing new features and analyzing data and as result to get just overfitting model.


Any feedback will be nice",0,alena0604,a-little-start-for-a-big-journey,alena0604_a-little-start-for-a-big-journey_m0
"I will use data from competition Home Credit Defaul Risk. 
Steps:
1. add library
2. load data
3. look what kind of data we have",1,alena0604,a-little-start-for-a-big-journey,alena0604_a-little-start-for-a-big-journey_m1
"To get the first view of data we have, we can use the following commands:
- df.describe() 
- df.head() # shows several rows
- df.shape() 
- df.columns() # name of columns
- df.info() # data type",2,alena0604,a-little-start-for-a-big-journey,alena0604_a-little-start-for-a-big-journey_m2
"The structure of the data with dependencies is on the image from competition page
![](https://storage.googleapis.com/kaggle-media/competitions/home-credit/home_credit.png)",3,alena0604,a-little-start-for-a-big-journey,alena0604_a-little-start-for-a-big-journey_m3
Explore data using graph,4,alena0604,a-little-start-for-a-big-journey,alena0604_a-little-start-for-a-big-journey_m4
"Let's devide application train columns to several groups:
    1. Client personal information (everything connected with: gender, education, family, phones...)
    2. Client's registration and locations (registration ratings, dates, matches)
    3. Client's property (car, realty)
    4. Client credit information (everything about loan)
I think it would be usefull to see how much money people requested, depends on thier status/education/family size/work
* NAME_CONTRACT_TYPE
* CNT_CHILDREN
* NAME_FAMILY_STATUS
* NAME_INCOME_TYPE
* NAME_EDUCATION_TYPE
* NAME_FAMILY_STATUS
* OCCUPATION_TYPE",5,alena0604,a-little-start-for-a-big-journey,alena0604_a-little-start-for-a-big-journey_m5
"There are several methods to convert categorical variable: 
1. pandas.get_dummies(data, prefix=None, prefix_sep='_', dummy_na=False, columns=None, sparse=False, drop_first=False, dtype=None)

*Convert categorical variable into dummy/indicator variables*
2. pandas.factorize(values, sort=False, order=None, na_sentinel=-1, size_hint=None) 

*Encode the object as an enumerated type or categorical variable*

For memory efficient usage:
1.  Let free_raw_data=True (default is True) when constructing the Dataset
2.  Explicit set raw_data=None after the Dataset has been constructed
3.  Call gc",6,alena0604,a-little-start-for-a-big-journey,alena0604_a-little-start-for-a-big-journey_m6
"Machine learning models are often considered as black boxes - we set (or guess?) some parameters, give some input data and receive predictions. But in reality those boxes are mostly not completely black. 
Purpose of this notebook is to show some possibilities to view inside the black box and see little bit of what's inside...

The main questions addressed here:
* **Which features** are the most important in general and which contributed to some particular prediction most?
* **How** are features contributing to some particular predictions (lowering, pushing up)?
* **Why** some particular prediction has the value it has?

Let's take as an example this great kernel with currently highest public LB score: https://www.kaggle.com/poohtls/fork-of-fork-lightgbm-with-simple-features
(Kudos to it's author,  2 kudos to the kernel's author which that kernel was forked from, 4 kudos to the kernel's author even below, etc).

That kernel already has feature importances plot, which gives us some basic impression about, how model is transforming features into predictions. 
But it doesn't provide any possibilities to say something about some individual predictions. Let's try to obtain some intuition for this also.

First we run model to get trained LightGBM Booster...
",0,alijs1,explaining-model-s-predictions,alijs1_explaining-model-s-predictions_m0
"Once we have trained Booster, we can get feature importances. LightGBM provides 2 types of feature impotances:
* split - numbers of times the feature is used in a model;
* gain - total gains of splits which use the feature.

Let's look at both of them to see if there are any differences...",1,alijs1,explaining-model-s-predictions,alijs1_explaining-model-s-predictions_m1
"What we see is that ""NEW_EXT_SOURCES_MEAN"" feature is dominating in the importance of gain. Probably this feature typically has splits in lowest levels of trees. We can check this assumption by plotting some random trees. Below is plot of one tree splits and yes - ""NEW_EXT_SOURCES_MEAN"" is at the root of the tree. You can try viewing some other randomly chosen trees by changing tree_index parameter, and mostly you'll see something similar.

**Note:** in Notebook plotted tree is unreadable due to limited size of image... to see full graph, do right click on the image and view it from there.",2,alijs1,explaining-model-s-predictions,alijs1_explaining-model-s-predictions_m2
"I guess we already have obtained some high level intuition about how model is using features to make predictions. Now let's go deeper and take a look at some individual predictions. We'll use SHAP library (https://github.com/slundberg/shap) for that because of it's nice graphics.

Let's make force plot of first prediction and see what we can say from it...",3,alijs1,explaining-model-s-predictions,alijs1_explaining-model-s-predictions_m3
"What can we see:
* Predicted value for this item is high, so it is to be most likely a default (and it is, as we see truth is 1).
* Main features pushing this prediction closer to 1 (in red color) are ""NEW_EXT_SOURCES_MEAN"", ""EXT_SOURCES_3"" and ""DEF_30_CNT_SOCIAL_CIRCLE"".
* Main features pushing this prediction closer to 0 (in blue color) are ""DAYS_BIRTH"" and ""LIVINGAREA_MODE"".
    
So basically, from this we could even generate some automatic response to the client, which would tell something like - *""Even if your age is ok and you live in good area, your external scores are too low and risk to have credit default is too high - so sorry, we can't give you credit this time""*. 

Ok, let's plot some more cases...",4,alijs1,explaining-model-s-predictions,alijs1_explaining-model-s-predictions_m4
"Predicted value is low, we can give credit here. So, what could we say to this client? *""Great, you have good credit/annuity ratio, good total income and higher education, therefore risk of credit default is low enough to receive credit!""*. 

One more?",5,alijs1,explaining-model-s-predictions,alijs1_explaining-model-s-predictions_m5
"*""You have some problems with employment, but your external scores and annuity to income ratio is good, so we can give you a credit!""*. 

Ok, generating responses was fun. Let's look what more can we get out of this library. Let's make force plot for more data...",6,alijs1,explaining-model-s-predictions,alijs1_explaining-model-s-predictions_m6
"Graph is interactive - when moving mouse over it, for that specific point info is shown about what feature values are pushing predictions up, and which - down. Playing with the graph a bit helps to get some impression about how model is using features and to see if there are some strange decisions made by model (e.g. that some ID value is used to push prediction up/down, etc).

More to come, if I'll have time and motivation ;)",7,alijs1,explaining-model-s-predictions,alijs1_explaining-model-s-predictions_m7
...,8,alijs1,explaining-model-s-predictions,alijs1_explaining-model-s-predictions_m8
...,9,alijs1,explaining-model-s-predictions,alijs1_explaining-model-s-predictions_m9
...,10,alijs1,explaining-model-s-predictions,alijs1_explaining-model-s-predictions_m10
...,11,alijs1,explaining-model-s-predictions,alijs1_explaining-model-s-predictions_m11
...,12,alijs1,explaining-model-s-predictions,alijs1_explaining-model-s-predictions_m12
"# Home Credit Default Risk

This kernel will contain EDA, visualization, feature engineering and some modelling. Work currently in progress.",0,artgor,eda-basic-fe-and-lgb,artgor_eda-basic-fe-and-lgb_m0
"There are several files with data, let's go through them step by step.",1,artgor,eda-basic-fe-and-lgb,artgor_eda-basic-fe-and-lgb_m1
## Data Exploration,2,artgor,eda-basic-fe-and-lgb,artgor_eda-basic-fe-and-lgb_m2
"### application_train and application_test
These are main files with data and technically we can use only them to make predictions. Obviously using additional data is necessary to improve score.",3,artgor,eda-basic-fe-and-lgb,artgor_eda-basic-fe-and-lgb_m3
We have 122 columns in just main file! Let's take a look on some of them.,4,artgor,eda-basic-fe-and-lgb,artgor_eda-basic-fe-and-lgb_m4
#### Categorical features,5,artgor,eda-basic-fe-and-lgb,artgor_eda-basic-fe-and-lgb_m5
##### Target,6,artgor,eda-basic-fe-and-lgb,artgor_eda-basic-fe-and-lgb_m6
"We have disbalanced target, though disbalance isn't really serious.",7,artgor,eda-basic-fe-and-lgb,artgor_eda-basic-fe-and-lgb_m7
##### NAME_CONTRACT_TYPE,8,artgor,eda-basic-fe-and-lgb,artgor_eda-basic-fe-and-lgb_m8
We can see that there are two types of contract - cash loans and revolving loans. Most of the loans are cash loans which are defaulted.,9,artgor,eda-basic-fe-and-lgb,artgor_eda-basic-fe-and-lgb_m9
##### CODE_GENDER,10,artgor,eda-basic-fe-and-lgb,artgor_eda-basic-fe-and-lgb_m10
"We can see that women take more loans and higher percentage of them repays the loans. And there are 4 people with unindentified gender, who repayed their loans :)",11,artgor,eda-basic-fe-and-lgb,artgor_eda-basic-fe-and-lgb_m11
##### FLAG_OWN_CAR and FLAG_OWN_REALTY,12,artgor,eda-basic-fe-and-lgb,artgor_eda-basic-fe-and-lgb_m12
##### CNT_CHILDREN and NAME_FAMILY_STATUS,13,artgor,eda-basic-fe-and-lgb,artgor_eda-basic-fe-and-lgb_m13
We can see that most of the people are married and have zero children. In face we can divide people into two group based on their family status - living together with their partner or single.,14,artgor,eda-basic-fe-and-lgb,artgor_eda-basic-fe-and-lgb_m14
It isn't surprising that there are a lot of families consisting of two or one adults. Also there are families with two adults and 1-2 children.,15,artgor,eda-basic-fe-and-lgb,artgor_eda-basic-fe-and-lgb_m15
"##### NAME_TYPE_SUITE
This feature shows who was accompanying client when he was applying for the loan.",16,artgor,eda-basic-fe-and-lgb,artgor_eda-basic-fe-and-lgb_m16
"It is interesting to see that these two variables sometimes contradict each other. For example, separated, single or widowed applicants were sometimes accompanied by their partner. I suppose this means unofficial relationships? Also sometimes children accompanied the applicant. Maybe these were adult childred?",17,artgor,eda-basic-fe-and-lgb,artgor_eda-basic-fe-and-lgb_m17
##### NAME_INCOME_TYPE,18,artgor,eda-basic-fe-and-lgb,artgor_eda-basic-fe-and-lgb_m18
"We can see that there are 4 categories with little amount of people in them: several high-income businessmen, 4 women and 1 man on maternity leave, and some unemployed/students. It is quite interesting that unemployed/students have quite a high income.
And of course, most of the people work.",19,artgor,eda-basic-fe-and-lgb,artgor_eda-basic-fe-and-lgb_m19
"##### AMT_GOODS_PRICE
For consumer loans it is the price of the goods for which the loan is given",20,artgor,eda-basic-fe-and-lgb,artgor_eda-basic-fe-and-lgb_m20
So this means that only 278 loans have some other type. Let's fo deeper.,21,artgor,eda-basic-fe-and-lgb,artgor_eda-basic-fe-and-lgb_m21
"We can see that most of the loans have the amount which is similar to the goods price, but there are some outliers.",22,artgor,eda-basic-fe-and-lgb,artgor_eda-basic-fe-and-lgb_m22
##### NAME_HOUSING_TYPE,23,artgor,eda-basic-fe-and-lgb,artgor_eda-basic-fe-and-lgb_m23
"##### Contact information
There are 6 features showing that client provided some contact information, let's see how many ways of contact clients usually provide.",24,artgor,eda-basic-fe-and-lgb,artgor_eda-basic-fe-and-lgb_m24
"Most clients provide 3 ways to contact them and usually minimus is 2, if we don't consider several people who left only 1.",25,artgor,eda-basic-fe-and-lgb,artgor_eda-basic-fe-and-lgb_m25
"# deliquencies

It is very important to see how many times clients was late with payments or defaulted his loans. I suppose info about his social circle is also important. I'll divide values into 2 groups: 0, 1 and more than 1.",26,artgor,eda-basic-fe-and-lgb,artgor_eda-basic-fe-and-lgb_m26
#### Continuous variables,27,artgor,eda-basic-fe-and-lgb,artgor_eda-basic-fe-and-lgb_m27
##### AMT_INCOME_TOTAL,28,artgor,eda-basic-fe-and-lgb,artgor_eda-basic-fe-and-lgb_m28
"We can see following things from the information above:
- income feature has some huge outliers. This could be due to rich individuals or due to errors in data;
- average income is almost similar for those who repay the loans and those who don't;
- if we leave only data within 90 percentile, it is almost normally distributed;
- log transformation also helps;",29,artgor,eda-basic-fe-and-lgb,artgor_eda-basic-fe-and-lgb_m29
##### AMT_CREDIT,30,artgor,eda-basic-fe-and-lgb,artgor_eda-basic-fe-and-lgb_m30
"This feature shows the amount of the loan in question.
We can see following things from the information above:
- income feature has some outliers. Maybe mortgage?;
- average credit amoint is almost similar for those who repay the loans and those who don't;
- if we leave only data within 95 percentile, it is almost normally distributed;
- log transformation also helps;",31,artgor,eda-basic-fe-and-lgb,artgor_eda-basic-fe-and-lgb_m31
##### DAYS_BIRTH,32,artgor,eda-basic-fe-and-lgb,artgor_eda-basic-fe-and-lgb_m32
We can see that age distribution is almost normal and most of the people are between 30 and 40 years.,33,artgor,eda-basic-fe-and-lgb,artgor_eda-basic-fe-and-lgb_m33
##### DAYS_EMPLOYED,34,artgor,eda-basic-fe-and-lgb,artgor_eda-basic-fe-and-lgb_m34
"Ther was a strange value - 365243, it could mean empty values or some errors, so I replace it with zero.
A lot of people don't work, but let's look deeper into this.",35,artgor,eda-basic-fe-and-lgb,artgor_eda-basic-fe-and-lgb_m35
"Well, it seems that a lot of non-working people are pensioners, which is normal. As for working people - they seem to work for several years at one place.",36,artgor,eda-basic-fe-and-lgb,artgor_eda-basic-fe-and-lgb_m36
Ther are so many features and so many possible angles from which we can analyze them. Let's see this for example:,37,artgor,eda-basic-fe-and-lgb,artgor_eda-basic-fe-and-lgb_m37
We can see that most of the loans are taken by working people with secondary education.,38,artgor,eda-basic-fe-and-lgb,artgor_eda-basic-fe-and-lgb_m38
## Transforming and merging data,39,artgor,eda-basic-fe-and-lgb,artgor_eda-basic-fe-and-lgb_m39
"## Basic modelling, LGB",40,artgor,eda-basic-fe-and-lgb,artgor_eda-basic-fe-and-lgb_m40
"This was EDA and basic feature engineering. I know that feature engineering and modelling could be much better, but decided to make EDA the main focus of this kernel. I'll do better feature engineering and modelling in the next one.",41,artgor,eda-basic-fe-and-lgb,artgor_eda-basic-fe-and-lgb_m41
">[this kernel](https://www.kaggle.com/shivamb/homecreditrisk-extensive-eda-baseline-0-772) had provided with initial exploration ideas

>please suggest some of your methods to improve my exploration technique

> Successfully visualised all of the columns (220 in total).",0,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m0
## The idea of directory that I am working in,1,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m1
## The data files present in that directory,2,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m2
## Important imports,3,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m3
## The function to plot the distribution of the categorical values Horizontaly,4,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m4
## Function to get the distribution of the categories according to the target,5,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m5
## Function to explore the numeric data,6,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m6
## Reading the first data file,7,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m7
## Got the idea about its ,8,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m8
## The number of rows and columns we have in the *application_train.csv*,9,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m9
## Target ,10,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m10
"> Target variable (1 - client with payment difficulties: he/she had late payment more than X days on at least one of the first Y installments of the loan in our sample, 0 - all other cases)",11,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m11
"> We see that the 8.07% (24,825) of the clients have difficulties while repayment of the loan",12,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m12
## *NAME_CONTRACT_TYPE* :- IDENTIFICATION IF THE LOAN IS CASH OR REVOLVING,13,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m13
"> 9.52% of the loan is the revolving loan
>90.5% of the loan is the cash loan",14,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m14
## *CODE_GENDER* :- Gender of the Client,15,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m15
> We see that there are 3 gender categories and may be the XNA is the null value,16,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m16
## *FLAG_OWN_CAR*:- Flag if the client owns a car,17,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m17
">66% do not own car while only 34% own car

",18,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m18
> people with no cars have more repayment difficulties,19,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m19
## *FLAG_OWN_REALTY* :- Flag if client owns a house or a flat ,20,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m20
## *CNT_CHILDREN*:-Number of children the client has,21,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m21
## *AMT_INCOME_TOTAL*:- Income of the Client,22,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m22
## *AMT_CREDIT*:- Credit Amount of Loan ,23,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m23
## *AMT_ANNUITY*:-Loan Annuity,24,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m24
## *AMT_GOODS_PRICE*:-For consumer loans it is the price of the goods for which the loan is given,25,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m25
## *NAME_TYPE_SUITE*:-Who was accompanying client when he was applying for the loan,26,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m26
"## *NAME_INCOME_TYPE*:-Clients income type(Business, Maternity,...) ",27,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m27
## *NAME_EDUCATION_TYPE*:- Level of Highest Education the Client Achieved,28,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m28
## *NAME_FAMILY_STATUS*:-Family Status of the client,29,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m29
"## *NAME_HOUSING_TYPE*:-What is the Housing Situation of the Client (renting, living with parents,...)",30,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m30
## *REGION_POPULATION_RELATIVE*:-Normalised Population of the Regions where Clients live (higher number more populated region),31,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m31
## *DAYS_BIRTH*:-Client 's Age in days at the time of application,32,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m32
,33,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m33
## *DAYS_EMPLOYED*:-How many days before the application the client started current employement,34,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m34
## *DAYS_REGISTRATION*:- How many days before the application did the client change his registration ,35,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m35
## *OWN_AGE_CAR*:- Age of clients car,36,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m36
## *FLAG_MOBIL*:- Did client provide mobile phone,37,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m37
## *FLAG_EMP_PHONE*:- Did client provide Work phone,38,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m38
"## *FLAG_WORK_PHONE*:- Did client provide home phone
",39,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m39
"
## *FLAG_CONT_MOBILE*:-Was mobile phone reachable",40,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m40
## *FLAG_PHONE*:- Did client provide home phone,41,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m41
## *FLAG_EMAIL*:- Did client provide email,42,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m42
"
## *OCCUPATION_TYPE*:-What kind of occupation does the client have",43,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m43
## *CNT_FAM_MEMBERS*:- How many family member does the client have,44,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m44
## *REGION_RATING_CLIENT*:-Our(their) rating of the region where client lives,45,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m45
"
## *REGION_RATING_CLIENT_W_CITY*:-Our rating of the region where client lives with taking city into account (1,2,3)
",46,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m46
"## *REGION_RATING_CLIENT_W_CITY*:-On which day of the week did the client apply for the loan
",47,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m47
"## *HOUR_APPR_PROCESS_START*:-Approximately at what hour did the client apply for the loan
",48,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m48
"## *REG_REGION_NOT_LIVE_REGION*:- Flag if client's permanent address does not match contact address (1=different, 0=same, at region level)",49,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m49
"
## REG_REGION_NOT_WORK_REGION:- Flag if client's permanent address does not match work address (1=different, 0=same, at region level)",50,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m50
"## *LIVE_REGION_NOT_WORK_REGION*:-Flag if client's contact address does not match work address (1=different, 0=same, at region level)",51,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m51
"## *REG_CITY_NOT_LIVE_CITY* :- Flag if client's permanent address does not match contact address (1=different, 0=same, at city level)",52,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m52
"## *REG_CITY_NOT_LIVE_CITY* :- Flag if client's permanent address does not match work address (1=different, 0=same, at city level)",53,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m53
"
## *LIVE_CITY_NOT_WORK_CITY*:- Flag if client's contact address does not match work address (1=different, 0=same, at city level)",54,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m54
## *ORGANIZATION_TYPE*:- Type of organization where client works,55,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m55
"
## *EXT_SOURCE_1*:- Normalized score from external data source",56,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m56
# *EXT_SOURCE_2* :-Normalized score from external data source,57,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m57
## *EXT_SOURCE_3*:- Normalized score from external data source,58,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m58
"## *APARTMENTS_AVG*:-Normalized information about building where the client lives, What is average (_AVG suffix), modus (_MODE suffix), median (_MEDI suffix) apartment size, common area, living area, age of building, number of elevators, number of entrances, state of the building, number of floor",59,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m59
"## *BASEMENTAREA_AV*:-Normalized information about building where the client lives, What is average (_AVG suffix), modus (_MODE suffix), median (_MEDI suffix) apartment size, common area, living area, age of building, number of elevators, number of entrances, state of the building, number of floor",60,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m60
"## *YEARS_BEGINEXPLUATATION_AVG*:-Normalized information about building where the client lives, What is average (_AVG suffix), modus (_MODE suffix), median (_MEDI suffix) apartment size, common area, living area, age of building, number of elevators, number of entrances, state of the building, number of floor",61,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m61
"## *YEARS_BUILD_AVG*:-Normalized information about building where the client lives, What is average (_AVG suffix), modus (_MODE suffix), median (_MEDI suffix) apartment size, common area, living area, age of building, number of elevators, number of entrances, state of the building, number of floor",62,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m62
">Normalized information about building where the client lives, What is average (_AVG suffix), modus (_MODE suffix), median (_MEDI suffix) apartment size, common area, living area, age of building, number of elevators, number of entrances, state of the building, number of floor
",63,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m63
## *OBS_30_CNT_SOCIAL_CIRCLE*:-How many observation of client's social surroundings with observable 30 DPD (days past due) default,64,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m64
## *DEF_30_CNT_SOCIAL_CIRCLE*:-How many observation of client's social surroundings defaulted on 30 DPD (days past due) ,65,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m65
## *OBS_60_CNT_SOCIAL_CIRCLE*:-How many observation of client's social surroundings with observable 60 DPD (days past due) default,66,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m66
## *DEF_60_CNT_SOCIAL_CIRCLE*:-How many observation of client's social surroundings defaulted on 60 (days past due) DPD,67,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m67
## *DAYS_LAST_PHONE_CHANGE* = How many days before application did client change phone,68,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m68
"> Did the client provide the list of documents (Document no.2,...Document no. 21)",69,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m69
## *AMT_REQ_CREDIT_BUREAU_HOUR*:-Number of enquiries to Credit Bureau about the client one hour before application,70,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m70
## *AMT_REQ_CREDIT_BUREAU_DAY*:-Number of enquiries to Credit Bureau about the client one day before application (excluding one hour before application),71,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m71
## *AMT_REQ_CREDIT_BUREAU_WEEK*:-Number of enquiries to Credit Bureau about the client one week before application (excluding one day before application),72,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m72
 ## *AMT_REQ_CREDIT_BUREAU_MON*:-Number of enquiries to Credit Bureau about the client one month before application (excluding one week before application),73,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m73
## *AMT_REQ_CREDIT_BUREAU_QRT*:-Number of enquiries to Credit Bureau about the client 3 month before application (excluding one month before application),74,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m74
## *AMT_REQ_CREDIT_BUREAU_YEAR*:-Number of enquiries to Credit Bureau about the client one day year (excluding last 3 months before application),75,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m75
## *bureau.csv*,76,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m76
## getting a view of the schema,77,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m77
"## *SK_ID_CURR*:-ID of loan in our sample - one loan in our sample can have 0,1,2 or more related previous credits in credit bureau  ",78,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m78
## *SK_BUREAU_ID*:-Recoded ID of previous Credit Bureau credit related to our loan (unique coding for each loan application),79,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m79
## *CREDIT_ACTIVE*:-Status of the Credit Bureau (CB) reported credits,80,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m80
## *CREDIT_CURRENCY*:-Recoded currency of the Credit Bureau credit,81,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m81
## *DAYS_CREDIT*:-How many days before current application did client apply for Credit Bureau credit,82,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m82
## *CREDIT_DAY_OVERDUE*:-Number of days past due on CB credit at the time of application for related loan in our sample,83,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m83
## *DAYS_CREDIT_ENDDATE*:-Remaining duration of CB credit (in days) at the time of application in Home Credit,84,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m84
## *DAYS_ENDDATE_FACT*:-Days since CB credit ended at the time of application in Home Credit (only for closed credit),85,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m85
"## ""AMT_CREDIT_MAX_OVERDUE"":-Maximal amount overdue on the Credit Bureau credit so far (at application date of loan in our sample)",86,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m86
## *CNT_CREDIT_PROLONG*:-How many times was the Credit Bureau credit prolonged,87,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m87
## *AMT_CREDIT_SUM*:-Current credit amount for the Credit Bureau credit,88,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m88
## *AMT_CREDIT_SUM_DEBT*:-Current debt on Credit Bureau credit,89,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m89
## *AMT_CREDIT_SUM_LIMIT*:-Current credit limit of credit card reported in Credit Bureau,90,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m90
## *AMT_CREDIT_SUM_OVERDUE*:-Current amount overdue on Credit Bureau credit,91,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m91
## *AMT_CREDIT_SUM_OVERDUE*:-Current amount overdue on Credit Bureau credit,92,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m92
"## *CREDIT_TYPE*:-Type of Credit Bureau credit (Car, cash,...)",93,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m93
   ## *DAYS_CREDIT_UPDATE*:-How many days before loan application did last information about the Credit Bureau credit come,94,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m94
## *AMT_ANNUITY*:-Annuity of the Credit Bureau credit,95,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m95
   > *bureau_balance.csv*:-,96,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m96
## *SK_BUREAU_ID*:-Recoded ID of Credit Bureau credit (unique coding for each application) - use this to join to CREDIT_BUREAU table ,97,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m97
## *MONTHS_BALANCE*:-Month of balance relative to application date (-1 means the freshest balance date),98,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m98
"## *STATUS*:-Status of Credit Bureau loan during the month (active, closed, DPD0-30,… [C means closed, X means status unknown, 0 means no DPD, 1 means maximal did during month between 1-30, 2 means DPD 31-60,… 5 means DPD 120+ or sold or written off ] )",99,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m99
> *POS_CASH_balance.csv*:-,100,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m100
"## *SK_ID_PREV *:-ID of previous credit in Home Credit related to loan in our sample. (One loan in our sample can have 0,1,2 or more previous loans in Home Credit)",101,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m101
## *SK_ID_CURR*:-ID of loan in our sample,102,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m102
"## *MONTHS_BALANCE*:-Month of balance relative to application date (-1 means the information to the freshest monthly snapshot, 0 means the information at application - often it will be the same as -1 as many banks are not updating the information to Credit Bureau regularly )",103,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m103
## *CNT_INSTALMENT*:-Term of previous credit (can change over time),104,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m104
## *CNT_INSTALMENT_FUTURE*:-Installments left to pay on the previous credit,105,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m105
## *CNT_INSTALMENT_FUTURE*:-Installments left to pay on the previous credit,106,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m106
## *NAME_CONTRACT_STATUS*:-Contract status during the month,107,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m107
## *SK_DPD*:- DPD (days past due) during the month of previous credit,108,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m108
## *SK_DPD_DEF*:- DPD during the month with tolerance (debts with low loan amounts are ignored) of the previous credit,109,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m109
> credit_card_balance.csv,110,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m110
"## *SK_ID_PREV *:-ID of previous credit in Home credit related to loan in our sample. (One loan in our sample can have 0,1,2 or more previous loans in Home Credit)",111,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m111
## *SK_ID_CURR*:-ID of loan in our sample,112,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m112
## *MONTHS_BALANCE*:-Month of balance relative to application date (-1 means the freshest balance date),113,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m113
## *AMT_BALANCE*:-Balance during the month of previous credit,114,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m114
## *AMT_CREDIT_LIMIT_ACTUAL*:-Credit card limit during the month of the previous credit,115,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m115
## *AMT_DRAWINGS_ATM_CURRENT*:-Amount drawing at ATM during the month of the previous credit,116,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m116
## *AMT_DRAWINGS_CURRENT*:-Amount drawing during the month of the previous credit,117,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m117
   ## *AMT_DRAWINGS_OTHER_CURRENT*:-Amount of other drawings during the month of the previous credit ,118,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m118
## *AMT_DRAWINGS_POS_CURRENT*:-Amount drawing or buying goods during the month of the previous credit,119,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m119
## *AMT_INST_MIN_REGULARITY*:-Minimal installment for this month of the previous credit,120,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m120
## *AMT_PAYMENT_CURRENT*:-How much did the client pay during the month on the previous credit,121,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m121
## *AMT_PAYMENT_TOTAL_CURRENT*:-How much did the client pay during the month in total on the previous credit,122,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m122
## *AMT_RECEIVABLE_PRINCIPAL*:-Amount receivable for principal on the previous credit,123,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m123
## *AMT_RECIVABLE*:-Amount receivable on the previous credit,124,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m124
## *AMT_TOTAL_RECEIVABLE*:-Total amount receivable on the previous credit,125,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m125
## *CNT_DRAWINGS_ATM_CURRENT*:-Number of drawings at ATM during this month on the previous credit,126,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m126
## *CNT_DRAWINGS_CURRENT*:-Number of drawings during this month on the previous credit,127,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m127
## *CNT_DRAWINGS_OTHER_CURRENT*:-Number of other drawings during this month on the previous credit,128,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m128
## *CNT_DRAWINGS_POS_CURRENT*:-Number of drawings for goods during this month on the previous credit,129,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m129
## *CNT_INSTALMENT_MATURE_CUM*:-Number of paid installments on the previous credit,130,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m130
"## *NAME_CONTRACT_STATUS*:-Contract status (active signed,...) on the previous credit",131,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m131
   ## *SK_DPD*:-DPD (Days past due) during the month on the previous credit,132,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m132
## *SK_DPD_DEF*:-DPD (Days past due) during the month with tolerance (debts with low loan amounts are ignored) of the previous credit,133,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m133
> Exploration of *previous_application.csv*,134,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m134
"## *SK_ID_PREV *:- ID of previous credit in Home credit related to loan in our sample. (One loan in our sample can have 0,1,2 or more previous loan applications in Home Credit, previous application could, but not necessarily have to lead to credit) ",135,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m135
## *SK_ID_CURR*:- ID of loan in our sample,136,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m136
"   ## *NAME_CONTRACT_TYPE*:-Contract product type (Cash loan, consumer loan [POS] ,...) of the previous application ",137,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m137
## *AMT_ANNUITY*:-Annuity of previous application,138,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m138
## *AMT_APPLICATION*:-For how much credit did client ask on the previous application,139,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m139
"## *AMT_CREDIT*:-Final credit amount on the previous application. This differs from AMT_APPLICATION in a way that the AMT_APPLICATION is the amount for which the client initially applied for, but during our approval process he could have received different amount - AMT_CREDIT",140,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m140
   ## *AMT_DOWN_PAYMENT*:-Down payment on the previous application,141,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m141
## *AMT_GOODS_PRICE*:-Goods price of good that client asked for (if applicable) on the previous application,142,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m142
## *WEEKDAY_APPR_PROCESS_START*:-On which day of the week did the client apply for previous application,143,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m143
## *HOUR_APPR_PROCESS_START*:-Approximately at what day hour did the client apply for the previous application,144,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m144
## *FLAG_LAST_APPL_PER_CONTRACT*:-Flag if it was last application for the previous contract. Sometimes by mistake of client or our clerk there could be more applications for one single contract,145,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m145
   ## *NFLAG_LAST_APPL_IN_DAY*:-Flag if the application was the last application per day of the client. Sometimes clients apply for more applications a day. Rarely it could also be error in our system that one application is in the database twice,146,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m146
## *NFLAG_MICRO_CASH*:-Flag Micro finance loan,147,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m147
    - No column with name in the table,148,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m148
## *RATE_DOWN_PAYMENT*:-Down payment rate normalized on previous credit,149,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m149
## *RATE_INTEREST_PRIMARY*:-Interest rate normalized on previous credit,150,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m150
## *RATE_INTEREST_PRIVILEGED*:-Interest rate normalized on previous credit,151,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m151
## *NAME_CASH_LOAN_PURPOSE*:-Purpose of the cash loan,152,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m152
"## *NAME_CONTRACT_STATUS*:-Contract status (approved, cancelled, ...) of previous application",153,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m153
## *DAYS_DECISION*:-Relative to current application when was the decision about previous application made,154,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m154
## *NAME_PAYMENT_TYPE*:-Payment method that client chose to pay for the previous application,155,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m155
## *CODE_REJECT_REASON*:- Why was the previous application rejected,156,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m156
## *NAME_TYPE_SUITE*:- Who accompanied client when applying for the previous application,157,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m157
## *NAME_CLIENT_TYPE*:-Was the client old or new client when applying for the previous application,158,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m158
## *NAME_GOODS_CATEGORY*:- What kind of goods did the client apply for in the previous application,159,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m159
"## *NAME_PORTFOLIO*:- Was the previous application for CASH, POS, CAR, …",160,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m160
## *NAME_PRODUCT_TYPE*:-Was the previous application x-sell o walk-in,161,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m161
## *CHANNEL_TYPE*:-Through which channel we acquired the client on the previous application,162,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m162
## *SELLERPLACE_AREA*:-Selling area of seller place of the previous application,163,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m163
## *NAME_SELLER_INDUSTRY*:-The industry of the seller,164,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m164
## *CNT_PAYMENT*:-Term of previous credit at application of the previous application,165,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m165
## *NAME_YIELD_GROUP*:-Grouped interest rate into small medium and high of the previous application,166,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m166
   ## *PRODUCT_COMBINATION*:-Detailed product combination of the previous application,167,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m167
## *DAYS_FIRST_DRAWING*:-Relative to application date of current application when was the first disbursement of the previous application,168,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m168
## *DAYS_FIRST_DUE*:-Relative to application date of current application when was the first due supposed to be of the previous application,169,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m169
## *DAYS_LAST_DUE_1ST_VERSION*:-Relative to application date of current application when was the first due of the previous application,170,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m170
## *DAYS_LAST_DUE*:-Relative to application date of current application when was the last due date of the previous application,171,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m171
## *DAYS_TERMINATION*:-Relative to application date of current application when was the expected termination of the previous application,172,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m172
## *NFLAG_INSURED_ON_APPROVAL*:-Did the client requested insurance during the previous application,173,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m173
> installments_payments.csv,174,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m174
"## *SK_ID_PREV *:-ID of previous credit in Home credit related to loan in our sample. (One loan in our sample can have 0,1,2 or more previous loans in Home Credit)",175,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m175
## *SK_ID_CURR*:-ID of loan in our sample,176,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m176
## *NUM_INSTALMENT_VERSION*:-Version of installment calendar (0 is for credit card) of previous credit. Change of installment version from month to month signifies that some parameter of payment calendar has changed,177,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m177
## *NUM_INSTALMENT_NUMBER*:-On which installment we observe payment,178,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m178
## *DAYS_INSTALMENT*:-When the installment of previous credit was supposed to be paid (relative to application date of current loan),179,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m179
## *DAYS_ENTRY_PAYMENT*:-When was the installments of previous credit paid actually (relative to application date of current loan),180,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m180
## *AMT_INSTALMENT*:-What was the prescribed installment amount of previous credit on this installment,181,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m181
## *AMT_PAYMENT*:-What the client actually paid on previous credit on this installment,182,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m182
## *A brief visualisationof all the columns of this competitions* :),183,ashukr,exploration-of-220-columns-1,ashukr_exploration-of-220-columns-1_m183
"# Introduction: Deep Learning with Embedding Layers


This notebook is intended for those who want an introduction into Embedding Layers with Keras. I choosed not to focus on describing the preprocessing nor the different methods, to merge all the table, but rather to focus more specificaly on how to get started in Embedding.

Embedding is a technique used to encode categorical features like One-Hot encoding or target encoding, it is a bit more difficult to implement but keras allow us to create a model pretty easily.

Embeddings help to generalize better when the data is sparse and statistics is unknown. Thus, it is especially useful for datasets with lots of high cardinality features, where other methods tend to overfit.

Why should we use Entity Embedding instead of One-Hot Encoding ? There are mutiple reasons for that :

*  One-Hot encoded vectors are high-dimensional and sparse. In this dataset we have a feature that represent an organization type (denoted: ORGANIZATION_TYPE) of 58 distinct value . This means that, when using one-hot encoding, this feature will be represented by a vector containing 58 integers. And 57 of these integers are zeros. In a big dataset or in NLP ( Natural Language Processing) when you have more than 2000 outcomes for a feature, this approach is not computationally efficient.


* The vectors of each embedding get updated while training the neural network. This allows us to visualize relationships between words or more generally speaking categories, but also between everything that can be turned into a vector through an embedding layer. Please look at the image below  that show how similarities between categories can be found in a multi-dimensional space.

![](https://cdn-images-1.medium.com/max/1000/1*sXNXYfAqfLUeiDXPCo130w.png)",0,astrus,entity-embedding-neural-network-keras-lb-0-748,astrus_entity-embedding-neural-network-keras-lb-0-748_m0
# Prepare the data,1,astrus,entity-embedding-neural-network-keras-lb-0-748,astrus_entity-embedding-neural-network-keras-lb-0-748_m1
There is **307511** lines in the train file and **48744** lines in the test files. We have 121 differents features ( I'm deliberating excluding **SK_ID_CURR** which act as an ID and the **TARGET** variable),2,astrus,entity-embedding-neural-network-keras-lb-0-748,astrus_entity-embedding-neural-network-keras-lb-0-748_m2
# Variable Type,3,astrus,entity-embedding-neural-network-keras-lb-0-748,astrus_entity-embedding-neural-network-keras-lb-0-748_m3
# Label encode the categorical features,4,astrus,entity-embedding-neural-network-keras-lb-0-748,astrus_entity-embedding-neural-network-keras-lb-0-748_m4
# Create the network,5,astrus,entity-embedding-neural-network-keras-lb-0-748,astrus_entity-embedding-neural-network-keras-lb-0-748_m5
In order to create our embedding model we need to have a look at the spatiality of the cat features. We choose here to use Embedding only on cat features that present more than 2 outcomes otherwise it is count as a numeric value (0 or 1).,6,astrus,entity-embedding-neural-network-keras-lb-0-748,astrus_entity-embedding-neural-network-keras-lb-0-748_m6
"We are including 13 features **out of 16 categorical features** into our Embedding.

We can see that our features have a reatively small number of outcomes except for **OCCUPATION_TYPE** and **ORGANIZATION_TYPE** which will be represented in a high dimensional spaces in our Embedding.

The first layer of our network is the embedding layer with the size of 3 ""CODE_GENDER"". The embedding-size defines the dimensionality in which we map the categorical variables (in a 3D spaces for instance). One good rule of thumb to use for the output is : 

**embedding size = min(50, number of categories/2)**",7,astrus,entity-embedding-neural-network-keras-lb-0-748,astrus_entity-embedding-neural-network-keras-lb-0-748_m7
"In order for keras to know which features are going to be included into the Embedding layers we need to create a list containing for each feature the corresponding numpy array (**13** in total for us). The last element of the list will be our numerical features (**173**) and the categorical features that we decided not to include in the Embedding (**3**) for a total of **176** distinct features.
",8,astrus,entity-embedding-neural-network-keras-lb-0-748,astrus_entity-embedding-neural-network-keras-lb-0-748_m8
Let us go more specifically into this function : ,9,astrus,entity-embedding-neural-network-keras-lb-0-748,astrus_entity-embedding-neural-network-keras-lb-0-748_m9
This list will be passed into the network. It is composed of 14 numpy arrays containing our categorical features that are going throught the Embedding Layers (**13 layers**). The last element of the list is a numpy array composed of the **173 numerics features added to the 3 categorical features that have at most 2 distinct outcomes**. ,10,astrus,entity-embedding-neural-network-keras-lb-0-748,astrus_entity-embedding-neural-network-keras-lb-0-748_m10
"# Prepare the data

In neural networks, it is a best practice to scale input data before use. Data scaling
makes the training of the network faster, memory efficient and yield accurate
forecast results. Neural networks only work with data usually between a specified range (1 to 1 or 0 to 1), it makes it necessary then that data is scaled down and normalized. 

Scaling can be as simple as taking the ratios (reciprocal normalization), computing the differences
(range normalization) or multiplicative normalization.
Normalization ensures that data is roughly uniformly distributed between the network inputs
and the outputs.",11,astrus,entity-embedding-neural-network-keras-lb-0-748,astrus_entity-embedding-neural-network-keras-lb-0-748_m11
# Train the network,12,astrus,entity-embedding-neural-network-keras-lb-0-748,astrus_entity-embedding-neural-network-keras-lb-0-748_m12
"We can see that the model is performing well with an **AVG AUC of 0.75 on CV5 out-of-fold ** and an **AUC of 0.748 on LB**. 

However I'm having difficulties to perform as well as Boosted Trees like XGBoost, Lgbm and Catboost. If anyone have any hint on how to improve this kernel, please let me know.",13,astrus,entity-embedding-neural-network-keras-lb-0-748,astrus_entity-embedding-neural-network-keras-lb-0-748_m13
"# Introduction
Home Credit strives to broaden financial inclusion for the unbanked population by providing a positive and safe borrowing experience. In order to make sure this underserved population has a positive loan experience, Home Credit makes use of a variety of alternative data--including telco and transactional information--to predict their clients' repayment abilities.

While Home Credit is currently using various statistical and machine learning methods to make these predictions, they're challenging Kagglers to help them unlock the full potential of their data. Doing so will ensure that clients capable of repayment are not rejected and that loans are given with a principal, maturity, and repayment calendar that will empower their clients to be successful.",0,bsivavenu,simple-data-analysis-for-beginners,bsivavenu_simple-data-analysis-for-beginners_m0
Lets see what are the files we have to explore the data.,1,bsivavenu,simple-data-analysis-for-beginners,bsivavenu_simple-data-analysis-for-beginners_m1
"## How files are sturtured and link between each files and their content is given in the below image, which helps us to understand probelm well.
",2,bsivavenu,simple-data-analysis-for-beginners,bsivavenu_simple-data-analysis-for-beginners_m2
![](https://storage.googleapis.com/kaggle-media/competitions/home-credit/home_credit.png),3,bsivavenu,simple-data-analysis-for-beginners,bsivavenu_simple-data-analysis-for-beginners_m3
Lets read all the fiels and have a glimpse of data.,4,bsivavenu,simple-data-analysis-for-beginners,bsivavenu_simple-data-analysis-for-beginners_m4
## Lets check the missing values in each file.,5,bsivavenu,simple-data-analysis-for-beginners,bsivavenu_simple-data-analysis-for-beginners_m5
you can also check missing values like this without need of function.,6,bsivavenu,simple-data-analysis-for-beginners,bsivavenu_simple-data-analysis-for-beginners_m6
# Visuliazation,7,bsivavenu,simple-data-analysis-for-beginners,bsivavenu_simple-data-analysis-for-beginners_m7
"from the above image, most of the loans were repayed and less than 5000 were not payed.",8,bsivavenu,simple-data-analysis-for-beginners,bsivavenu_simple-data-analysis-for-beginners_m8
Little advanced visualization ,9,bsivavenu,simple-data-analysis-for-beginners,bsivavenu_simple-data-analysis-for-beginners_m9
"who accompanied client when appliying for the loan/application, and their repayment count given below.",10,bsivavenu,simple-data-analysis-for-beginners,bsivavenu_simple-data-analysis-for-beginners_m10
"more in pipe line, this is for beginners and all are basic level and self explanatory only.

if you like it, please upvote for me. 

Thank you : ) ",11,bsivavenu,simple-data-analysis-for-beginners,bsivavenu_simple-data-analysis-for-beginners_m11
" In this notebook we will try to gain insight into a tree model based on the shap package. To understand why current feature importances calculated by lightGBM, Xgboost and other tree based models have issues read this article:[ Interpretable Machine Learning with XGBoost]( Interpretable Machine Learning with XGBoost). The shap library [https://github.com/slundberg/shap](https://github.com/slundberg/shap) can be used by going to the settings of the notebook (upper right corner,) and  ""add a custom package"" in the settings tab.
 
 The most important plot is the summary plot (below in this notebook), that shows the 30 most important features. For each feature a distribution is plotted on how the train samples influence the model outcome. The more red the dots, the higher the feature value, the more blue the lower the feature value.

In this case, the feature EXT_SOURCE_2 is the feature that has the most impact on the model output. Train samples with low EXT_SOURCE_2 have higher probability upon obtaining a loan. If the client has a high EXT_SOURCE_2 value, the probability of getting a loan is low. For the red blob on the left, we see that a lot clients are in this case.

In the dependence plot of EXT_SOURCE_2, we see that if this value is between 0 and 0.2 the model output is higher especially when CODE_GENDER is zero (the blue dots). The feature CODE_GENDER is automatically chosen by the shap dependence plot function.

In the cc_bal_CNT_DRAWINGS_ATM_CURRENT dependence plot we see a few outliers. Maybe we should remove them ? The are training samples with CODE_GENDER equal to 2. Are that transgenders ? Also dependence plot of SK_DPD_DEF show that most samples are zero except a few exceptions. Maybe we need some feature engineering here. Chances that the model influence of this feature does not generalise to the test set is very high. In other words, overfitting is likely. I would not recommend including this feature. As we can derive from the depence plot, the impact of leaving this feature out will be low on the model performance.",0,cast42,lightgbm-model-explained-by-shap,cast42_lightgbm-model-explained-by-shap_m0
# Feature enginering based on [https://www.kaggle.com/ogrellier/good-fun-with-ligthgbm](https://www.kaggle.com/ogrellier/good-fun-with-ligthgbm),1,cast42,lightgbm-model-explained-by-shap,cast42_lightgbm-model-explained-by-shap_m1
"# Explaining the lightgbm model with shap
The advantage of using lightgbm over sklearn random forrest classifier is that lightGBM can deal with the Nan.",2,cast42,lightgbm-model-explained-by-shap,cast42_lightgbm-model-explained-by-shap_m2
There are CODER_GENDER = 2 ?,3,cast42,lightgbm-model-explained-by-shap,cast42_lightgbm-model-explained-by-shap_m3
"Since CODE_GENDER does not appear in the test set, we can drop them from the train samples ?",4,cast42,lightgbm-model-explained-by-shap,cast42_lightgbm-model-explained-by-shap_m4
If DAYS_EMPLOYED is a large positive number means the client is unemployed ? Maybe extraxt those with dummy variable ? It would applying for a loan unemployed lowers your approval from 8.6% downto 5.4%.,5,cast42,lightgbm-model-explained-by-shap,cast42_lightgbm-model-explained-by-shap_m5
"# Visualize many predictions
To keep the browser happy we only visualize 1,000 individuals.",6,cast42,lightgbm-model-explained-by-shap,cast42_lightgbm-model-explained-by-shap_m6
# Prepare for submission,7,cast42,lightgbm-model-explained-by-shap,cast42_lightgbm-model-explained-by-shap_m7
"# Feature selection
![](http://)Overlap (or misclassification rate) and ""probability of superiority"" have two good properties:
* As probabilities, they don't depend on units of measure, so they are comparable between studies.
* They are expressed in operational terms, so a reader has a sense of what practical effect the difference makes.

### Cohen's effect size
There is one other common way to express the difference between distributions.  Cohen's $d$ is the difference in means, standardized by dividing by the standard deviation.  Here's the math notation:
 
 $ d = \frac{\bar{x}_1 - \bar{x}_2} s $
 
where $s$ is the pooled standard deviation:

$s = \sqrt{\frac{n_1 s^2_1 + n_2 s^2_2}{n_1+n_2}}$

 Here's a function that computes it:",0,cast42,upsample-minority-class-and-ligthgbm,cast42_upsample-minority-class-and-ligthgbm_m0
# Data preparation based on [fork-of-good-fun-with-ligthgbm-more-features](https://www.kaggle.com/cttsai/fork-of-good-fun-with-ligthgbm-more-features),1,cast42,upsample-minority-class-and-ligthgbm,cast42_upsample-minority-class-and-ligthgbm_m1
# Explore the data,2,cast42,upsample-minority-class-and-ligthgbm,cast42_upsample-minority-class-and-ligthgbm_m2
# Impute the data,3,cast42,upsample-minority-class-and-ligthgbm,cast42_upsample-minority-class-and-ligthgbm_m3
# Upsample the minority class to match the majority class with SMOTE,4,cast42,upsample-minority-class-and-ligthgbm,cast42_upsample-minority-class-and-ligthgbm_m4
# Prepare for submission,5,cast42,upsample-minority-class-and-ligthgbm,cast42_upsample-minority-class-and-ligthgbm_m5
![](http://www.homecredit.net/~/media/Images/H/Home-Credit-Group/image-gallery/full/image-gallery-01-11-2016-b.png),0,codename007,home-credit-complete-eda-feature-importance,codename007_home-credit-complete-eda-feature-importance_m0
"- <a href='#1'>1. Introduction</a>  
- <a href='#2'>2. Retrieving the Data</a>
- <a href='#3'>3. Glimpse of Data</a>
- <a href='#4'> 4. Check for missing data</a>
- <a href='#5'>5. Data Exploration</a>
    - <a href='#5-1'>5.1 Distribution of AMT_CREDIT</a>
    - <a href='#5-2'>5.2 Distribution of AMT_INCOME_TOTAL</a>
    - <a href='#5-3'>5.3 Distribution of AMT_GOODS_PRICE</a>
    - <a href='#5-4'>5.4 Distribution of Name of type of the Suite</a>
    - <a href='#5-5'>5.5 Data is balanced or imbalanced</a>
    - <a href='#5-6'>5.6 Types of loan</a>
    - <a href='#5-7'>5.7 Purpose of loan</a>
    - <a href='#5-8'>5.8 Income sources of Applicant's who applied for loan</a>
    - <a href='#5-9'>5.9 Family Status of Applicant's who applied for loan</a>
    - <a href='#5-10'>5.10 Occupation of Applicant's who applied for loan</a>
    - <a href='#5-11'>5.11 Education of Applicant's who applied for loan</a>
    - <a href='#5-12'>5.12 For which types of house higher applicant's applied for loan ?</a>
    - <a href='#5-13'>5.13 Types of Organizations who applied for loan </a>
    - <a href='#5-14'>5.14 Exploration in terms of loan is repayed or not</a>
        - <a href='#5-14-1'>5.14.1 Income sources of Applicant's in terms of loan is repayed or not</a>
        - <a href='#5-14-2'>5.14.2 Family Status of Applicant's in terms of loan is repayed or not</a>
        - <a href='#5-14-3'>5.14.3 Occupation of Applicant's in terms of loan is repayed or not</a>
        - <a href='#5-14-4'>5.14.4 Education of Applicant's in terms of loan is repayed or not</a>
        - <a href='#5-14-5'>5.14.5 For which types of house higher applicant's applied for loan in terms of loan is repayed or not</a>
        - <a href='#5-14-6'>5.14.6 Types of Organizations in terms of loan is repayed or not</a>
        - <a href='#5-14-7'>5.14.7 Distribution of Name of type of the Suite in terms of loan is repayed or not</a>
    - <a href='#5-15'>5.15 Exploartion of previous application data</a>
        - <a href='#5-15-1'>5.15.1 Contract product type of previous application</a>
        - <a href='#5-15-2'>5.15.2 On which day highest number of clients applied in prevoies application</a>
        - <a href='#5-15-3'>5.15.3 Purpose of cash loan in previous application</a>
        - <a href='#5-15-4'>5.15.4 Contract was approved or not in previous application</a>
        - <a href='#5-15-5'>5.15.5 Payment method that client choose to pay for the previous application</a>
        - <a href='#5-15-6'>5.15.6 Why was the previous application rejected ?</a>
        - <a href='#5-15-7'>5.15.7 Who accompanied client when applying for the previous application</a>
        - <a href='#5-15-8'>5.15.8 Was the client old or new client when applying for the previous application</a>
        - <a href='#5-15-9'>5.15.9 What kind of goods did the client apply for in the previous application</a>
        - <a href='#5=15=10'>5.15.10 Was the previous application for CASH, POS, CAR, …</a>
        - <a href='#5-15-11'>5.15.11 Was the previous application x-sell or walk-in ?</a>
        - <a href='#5-15-12'>5.15.12 Top channels  through which they acquired the client on the previous application</a>
        - <a href='#5-15-13'>5.15.13 Top industry of the seller</a>
        - <a href='#5-15-14'>5.15.14 Grouped interest rate into small medium and high of the previous application</a>
        - <a href='#5-15-15'>5.15.15 Top Detailed product combination of the previous application</a>
        - <a href='#5-15-16'>5.15.16 Did the client requested insurance during the previous application</a>
- <a href='#6'>6. Pearson Correlation of features</a>
- <a href='#7'>7. Feature Importance using Random forest</a>",1,codename007,home-credit-complete-eda-feature-importance,codename007_home-credit-complete-eda-feature-importance_m1
# <a id='1'>1. Introduction</a>,2,codename007,home-credit-complete-eda-feature-importance,codename007_home-credit-complete-eda-feature-importance_m2
"Home Credit strives to broaden financial inclusion for the unbanked population by providing a positive and safe borrowing experience. In order to make sure this underserved population has a positive loan experience, Home Credit makes use of a variety of alternative data--including telco and transactional information--to predict their clients' repayment abilities.

While Home Credit is currently using various statistical and machine learning methods to make these predictions, they're challenging Kagglers to help them unlock the full potential of their data. Doing so will ensure that clients capable of repayment are not rejected and that loans are given with a principal, maturity, and repayment calendar that will empower their clients to be successful.",3,codename007,home-credit-complete-eda-feature-importance,codename007_home-credit-complete-eda-feature-importance_m3
 # <a id='2'>2. Retrieving the Data</a>,4,codename007,home-credit-complete-eda-feature-importance,codename007_home-credit-complete-eda-feature-importance_m4
# <a id='3'>3. Glimpse of Data</a>,5,codename007,home-credit-complete-eda-feature-importance,codename007_home-credit-complete-eda-feature-importance_m5
**application_train data**,6,codename007,home-credit-complete-eda-feature-importance,codename007_home-credit-complete-eda-feature-importance_m6
**POS_CASH_balance data**,7,codename007,home-credit-complete-eda-feature-importance,codename007_home-credit-complete-eda-feature-importance_m7
**bureau_balance data**,8,codename007,home-credit-complete-eda-feature-importance,codename007_home-credit-complete-eda-feature-importance_m8
**previous_application data**,9,codename007,home-credit-complete-eda-feature-importance,codename007_home-credit-complete-eda-feature-importance_m9
**installments_payments data**,10,codename007,home-credit-complete-eda-feature-importance,codename007_home-credit-complete-eda-feature-importance_m10
**credit_card_balance data**,11,codename007,home-credit-complete-eda-feature-importance,codename007_home-credit-complete-eda-feature-importance_m11
**bureau data**,12,codename007,home-credit-complete-eda-feature-importance,codename007_home-credit-complete-eda-feature-importance_m12
# <a id='4'> 4 Check for missing data</a>,13,codename007,home-credit-complete-eda-feature-importance,codename007_home-credit-complete-eda-feature-importance_m13
**checking missing data in application_train **,14,codename007,home-credit-complete-eda-feature-importance,codename007_home-credit-complete-eda-feature-importance_m14
**checking missing data in POS_CASH_balance **,15,codename007,home-credit-complete-eda-feature-importance,codename007_home-credit-complete-eda-feature-importance_m15
**checking missing data in bureau_balance **,16,codename007,home-credit-complete-eda-feature-importance,codename007_home-credit-complete-eda-feature-importance_m16
**checking missing data in previous_application **,17,codename007,home-credit-complete-eda-feature-importance,codename007_home-credit-complete-eda-feature-importance_m17
**checking missing data in installments_payments **,18,codename007,home-credit-complete-eda-feature-importance,codename007_home-credit-complete-eda-feature-importance_m18
**checking missing data in credit_card_balance **,19,codename007,home-credit-complete-eda-feature-importance,codename007_home-credit-complete-eda-feature-importance_m19
**checking missing data in bureau **,20,codename007,home-credit-complete-eda-feature-importance,codename007_home-credit-complete-eda-feature-importance_m20
# <a id='5'>5. Data Exploration</a>,21,codename007,home-credit-complete-eda-feature-importance,codename007_home-credit-complete-eda-feature-importance_m21
## <a id='5-1'>5.1 Distribution of AMT_CREDIT</a>,22,codename007,home-credit-complete-eda-feature-importance,codename007_home-credit-complete-eda-feature-importance_m22
## <a id='5-2'>5.2 Distribution of AMT_INCOME_TOTAL</a>,23,codename007,home-credit-complete-eda-feature-importance,codename007_home-credit-complete-eda-feature-importance_m23
## <a id='5-3'>5.3 Distribution of AMT_GOODS_PRICE</a>,24,codename007,home-credit-complete-eda-feature-importance,codename007_home-credit-complete-eda-feature-importance_m24
## <a id='5-4'>5.4 Who accompanied client when applying for the  application</a>,25,codename007,home-credit-complete-eda-feature-importance,codename007_home-credit-complete-eda-feature-importance_m25
## <a id='5-5'>5.5 Data is balanced or imbalanced</a>,26,codename007,home-credit-complete-eda-feature-importance,codename007_home-credit-complete-eda-feature-importance_m26
* As we can see data is highly imbalanced.,27,codename007,home-credit-complete-eda-feature-importance,codename007_home-credit-complete-eda-feature-importance_m27
## <a id='5-6'>5.6 Types of loan</a>,28,codename007,home-credit-complete-eda-feature-importance,codename007_home-credit-complete-eda-feature-importance_m28
"* **Rovolving loans :**  Arrangement which allows for the loan amount to be withdrawn, repaid, and redrawn again in any manner and any number of times, until the arrangement expires. Credit card loans and overdrafts are revolving loans. Also called evergreen loan",29,codename007,home-credit-complete-eda-feature-importance,codename007_home-credit-complete-eda-feature-importance_m29
* Most of the loans are Cash loans which were taken by applicants. **90.5 %** loans are Cash loans.,30,codename007,home-credit-complete-eda-feature-importance,codename007_home-credit-complete-eda-feature-importance_m30
## <a id='5-7'>5.7 Purpose of loan</a>,31,codename007,home-credit-complete-eda-feature-importance,codename007_home-credit-complete-eda-feature-importance_m31
## <a id='5-8'>5.8 Income sources of Applicant's who applied for loan</a>,32,codename007,home-credit-complete-eda-feature-importance,codename007_home-credit-complete-eda-feature-importance_m32
* 51.6 % Applicants mentioned that they are working.  23.3 % are Commercial Associate and 18 % are Pensioner etc. ,33,codename007,home-credit-complete-eda-feature-importance,codename007_home-credit-complete-eda-feature-importance_m33
## <a id='5-9'>5.9 Family Status of Applicant's who applied for loan</a>,34,codename007,home-credit-complete-eda-feature-importance,codename007_home-credit-complete-eda-feature-importance_m34
* 63.9 % applicants are married. 14.8 % are single etc.,35,codename007,home-credit-complete-eda-feature-importance,codename007_home-credit-complete-eda-feature-importance_m35
## <a id='5-10'>5.10 Occupation of Applicant's who applied for loan</a>,36,codename007,home-credit-complete-eda-feature-importance,codename007_home-credit-complete-eda-feature-importance_m36
"* **Top Applicant's who applied for loan :**
  * Laborers - Apprx. 55 K
  * Sales Staff - Approx. 32 K
  * Core staff - Approx. 28 K
  * Managers - Approx. 21 K
  * Drivers - Approx. 19 K",37,codename007,home-credit-complete-eda-feature-importance,codename007_home-credit-complete-eda-feature-importance_m37
## <a id='5-11'>5.11 Education of Applicant's who applied for loan</a>,38,codename007,home-credit-complete-eda-feature-importance,codename007_home-credit-complete-eda-feature-importance_m38
* 71 % applicants have secondary and 24.3 % having higher education.,39,codename007,home-credit-complete-eda-feature-importance,codename007_home-credit-complete-eda-feature-importance_m39
## <a id='5-12'>5.12 For which types of house higher applicant's applied for loan ?</a>,40,codename007,home-credit-complete-eda-feature-importance,codename007_home-credit-complete-eda-feature-importance_m40
"* Approx. 89 % peoples applied for loan, they mentioned type of house is **House / Appartment**.",41,codename007,home-credit-complete-eda-feature-importance,codename007_home-credit-complete-eda-feature-importance_m41
## <a id='5-13'>5.13 Types of Organizations who applied for loan </a>,42,codename007,home-credit-complete-eda-feature-importance,codename007_home-credit-complete-eda-feature-importance_m42
"* **Types of Organizations who applied for loan :**
  * Business Entity Type 3 - Approx. 68 K
  * XNA - Approx. 55 K
  * Self employed - Approx. 38 K
  * Others - Approx. 17 K
  * Medicine - Approx. 11 K
 ",43,codename007,home-credit-complete-eda-feature-importance,codename007_home-credit-complete-eda-feature-importance_m43
## <a id='5-14'>5.14 Exploration in terms of loan is repayed or not</a>,44,codename007,home-credit-complete-eda-feature-importance,codename007_home-credit-complete-eda-feature-importance_m44
## <a id='5-14-1'>5.14.1 Income sources of Applicant's in terms of loan is repayed or not in %</a>,45,codename007,home-credit-complete-eda-feature-importance,codename007_home-credit-complete-eda-feature-importance_m45
## <a id='5-14-2'>5.14.2 Family Status of Applicant's in terms of loan is repayed or not in %</a>,46,codename007,home-credit-complete-eda-feature-importance,codename007_home-credit-complete-eda-feature-importance_m46
## <a id='5-14-3'>5.14.3 Occupation of Applicant's in terms of loan is repayed or not in %</a>,47,codename007,home-credit-complete-eda-feature-importance,codename007_home-credit-complete-eda-feature-importance_m47
## <a id='5-14-4'>5.14.4 Education of Applicant's in terms of loan is repayed or not in %</a>,48,codename007,home-credit-complete-eda-feature-importance,codename007_home-credit-complete-eda-feature-importance_m48
## <a id='5-14-5'>5.14.5 For which types of house higher applicant's applied for loan in terms of loan is repayed or not in %</a>,49,codename007,home-credit-complete-eda-feature-importance,codename007_home-credit-complete-eda-feature-importance_m49
## <a id='5-14-6'>5.14.6 Types of Organizations in terms of loan is repayed or not in %</a>,50,codename007,home-credit-complete-eda-feature-importance,codename007_home-credit-complete-eda-feature-importance_m50
## <a id='5-14-7'>5.14.7 Distribution of Name of type of the Suite in terms of loan is repayed or not in %</a>,51,codename007,home-credit-complete-eda-feature-importance,codename007_home-credit-complete-eda-feature-importance_m51
# <a id='5-15'>5.15 Exploartion of previous application data</a>,52,codename007,home-credit-complete-eda-feature-importance,codename007_home-credit-complete-eda-feature-importance_m52
## <a id='5-15-1'>5.15.1 Contract product type of previous application</a>,53,codename007,home-credit-complete-eda-feature-importance,codename007_home-credit-complete-eda-feature-importance_m53
"* **Contract product type of previous application :**
  * Cash loans - 44.8 %
  * Consumer loans - 43.7 %
  * Rovolving loan - 11.6 %
  * XNA - 0.0207 %",54,codename007,home-credit-complete-eda-feature-importance,codename007_home-credit-complete-eda-feature-importance_m54
## <a id='5-15-2'>5.15.2 On which day highest number of clients applied in prevoies application</a>,55,codename007,home-credit-complete-eda-feature-importance,codename007_home-credit-complete-eda-feature-importance_m55
"* What a coincedence, Approximately 15 % clients applied in each 5 days a week i.e, Tuesday, Wednesday, Monday, Friday and Thrusday. ",56,codename007,home-credit-complete-eda-feature-importance,codename007_home-credit-complete-eda-feature-importance_m56
## <a id='5-15-3'>5.15.3 Purpose of cash loan in previous application</a>,57,codename007,home-credit-complete-eda-feature-importance,codename007_home-credit-complete-eda-feature-importance_m57
"* **Main purpose of the cash loan was  :**
  * XAP - 55 %
  * XNA - 41 %",58,codename007,home-credit-complete-eda-feature-importance,codename007_home-credit-complete-eda-feature-importance_m58
## <a id='5-15-4'>5.15.4 Contract was approved or not in previous application</a>,59,codename007,home-credit-complete-eda-feature-importance,codename007_home-credit-complete-eda-feature-importance_m59
"* **Contract was approved or not in previous application :**
  * Approved : 62.1 % times
  * Cancelled : 18.9 % times
  * Refused : 17.4 % times 
  * Unused offer : 1.58 % times",60,codename007,home-credit-complete-eda-feature-importance,codename007_home-credit-complete-eda-feature-importance_m60
## <a id='5-15-5'>5.15.5 Payment method that client choose to pay for the previous application</a>,61,codename007,home-credit-complete-eda-feature-importance,codename007_home-credit-complete-eda-feature-importance_m61
* As we can most of the payment(61.9 %) has done thorugh cash only. ,62,codename007,home-credit-complete-eda-feature-importance,codename007_home-credit-complete-eda-feature-importance_m62
## <a id='5-15-6'>5.15.6 Why was the previous application rejected ?</a>,63,codename007,home-credit-complete-eda-feature-importance,codename007_home-credit-complete-eda-feature-importance_m63
## <a id='5-15-7'>5.15.7 Who accompanied client when applying for the previous application</a>,64,codename007,home-credit-complete-eda-feature-importance,codename007_home-credit-complete-eda-feature-importance_m64
"* **Who accompanied client when applying for the previous application :**
  * Unaccompanied : Approx. 60 % times
  * Family : Approx. 25 % times
  * Spouse, Partner : Approx. 8 %
  * Childrens : Approx. 4 %",65,codename007,home-credit-complete-eda-feature-importance,codename007_home-credit-complete-eda-feature-importance_m65
## <a id='5-15-8'>5.15.8 Was the client old or new client when applying for the previous application</a>,66,codename007,home-credit-complete-eda-feature-importance,codename007_home-credit-complete-eda-feature-importance_m66
* Approximately 74 % was repeater clients who applied for previous application.,67,codename007,home-credit-complete-eda-feature-importance,codename007_home-credit-complete-eda-feature-importance_m67
## <a id='5-15-9'>5.15.9 What kind of goods did the client apply for in the previous application</a>,68,codename007,home-credit-complete-eda-feature-importance,codename007_home-credit-complete-eda-feature-importance_m68
"## <a id='5=15=10'>5.15.10 Was the previous application for CASH, POS, CAR, …</a>",69,codename007,home-credit-complete-eda-feature-importance,codename007_home-credit-complete-eda-feature-importance_m69
## <a id='5-15-11'>5.15.11 Was the previous application x-sell or walk-in ?</a>,70,codename007,home-credit-complete-eda-feature-importance,codename007_home-credit-complete-eda-feature-importance_m70
## <a id='5-15-12'>5.15.12 Top channels  through which they acquired the client on the previous application</a>,71,codename007,home-credit-complete-eda-feature-importance,codename007_home-credit-complete-eda-feature-importance_m71
"* **Top channels  through which they acquired the client on the previous application :**
  * Credidit and cash offices : 43 % times
  * Country_wide : 30 % times
  * Stone : 13 % times",72,codename007,home-credit-complete-eda-feature-importance,codename007_home-credit-complete-eda-feature-importance_m72
## <a id='5-15-13'>5.15.13 Top industry of the seller</a>,73,codename007,home-credit-complete-eda-feature-importance,codename007_home-credit-complete-eda-feature-importance_m73
## <a id='5-15-14'>5.15.14 Grouped interest rate into small medium and high of the previous application</a>,74,codename007,home-credit-complete-eda-feature-importance,codename007_home-credit-complete-eda-feature-importance_m74
## <a id='5-15-15'>5.15.15 Top Detailed product combination of the previous application</a>,75,codename007,home-credit-complete-eda-feature-importance,codename007_home-credit-complete-eda-feature-importance_m75
## <a id='5-15-16'>5.15.16 Did the client requested insurance during the previous application</a>,76,codename007,home-credit-complete-eda-feature-importance,codename007_home-credit-complete-eda-feature-importance_m76
# <a id='6'>6. Pearson Correlation of features</a>,77,codename007,home-credit-complete-eda-feature-importance,codename007_home-credit-complete-eda-feature-importance_m77
# <a id='7'>7. Feature Importance using Random forest</a>,78,codename007,home-credit-complete-eda-feature-importance,codename007_home-credit-complete-eda-feature-importance_m78
# More To Come. Stayed Tuned ,79,codename007,home-credit-complete-eda-feature-importance,codename007_home-credit-complete-eda-feature-importance_m79
"## latest updates
* created another crosstab function - this one for numeric variables
* crosstabs which have been suppressed are now included as a data source
* set up option to suppress particular crosstabs which have been created previously and slow down current kernel runs
* created a custom crosstab function
* the time it takes for sections to run is now recorded
* dataframe info gets outputed to csv files
* concatenated application train and test files
* just setting up my job at the moment",0,darryldias,data-exploration-dd3,darryldias_data-exploration-dd3_m0
## application train and test data files,1,darryldias,data-exploration-dd3,darryldias_data-exploration-dd3_m1
### application train,2,darryldias,data-exploration-dd3,darryldias_data-exploration-dd3_m2
### application test,3,darryldias,data-exploration-dd3,darryldias_data-exploration-dd3_m3
## concatenating application train and test files,4,darryldias,data-exploration-dd3,darryldias_data-exploration-dd3_m4
### testing and learning (please ignore I am not doing anything great),5,darryldias,data-exploration-dd3,darryldias_data-exploration-dd3_m5
[],0,fanzzz,understand-variables-in-chinese,fanzzz_understand-variables-in-chinese_m0
"['- **application_{train|test}.csv**\n', '- **bureau.csv**\n', '- **bureau_balance.csv**\n', '- **previous_application**\n', '- **POS_CASH_balance.csv**\n', '- **credit_card_balance.csv**\n', '- **installments_payments.csv**\n', '\n', '# application_{train|test}.csv  \n', '1. 数据量及描述\n', '\t- train (307511,122) \n', '\t- test (48744,121) \n', '\t- 描述：所有贷款申请的静态数据，一行代表我们数据样本中的一笔贷款。 \n', '2. SK_ID_CURR\n', '\t- 含义：样本中贷款ID\n', '\t- 字段：10002/10003...\n', '3. TARGET\n', '\t- 含义：目标变量 (1代表客户在贷款的前Y个分期内至少有一笔迟交天数超过X天)\n', '\t- 字段：1/0 (test数据集无该列)\n', '4. NAME_CONTRACT_TYPE\n', '\t- 含义: 贷款是现金还是循环的标识 (循环贷是指客户将商品住房抵押给银行，就可获得一定的贷款额度，在房产抵押期限内客户可分次提款、循环使用，不超过可用额度单笔用款时，只需客户填写提款申请表，不用专门再次审批，一般1小时便可提取现金，等于随身有了一个安全又方便的流动大“金库”。)\n', '\t- 字段：Cash loans/Revolving loans\t\n', '5. CODE_GENDER\n', '\t- 含义：客户性别\n', '\t- 字段: F/M/**XNA**\n', '\t- 处理方法: **不做处理**\n', '6. FLAG_OWN_CAR\n', '\t- 含义：客户是否拥有汽车标识\n', '\t- 字段：N/Y\n', '7. FLAG_OWN_REALTY\n', '\t- 含义：客户是否拥有房屋或公寓标识\n', '\t- 字段：N/Y\n', '8. CNT_CHILDREN\n', '\t- 含义：客户拥有的孩子数目\n', '\t- 字段：0-12/14/19/20\n', '9. AMT_INCOME_TOTAL\n', '\t- 含义：客户年收入\n', '\t- 字段：25,650至117,000,000\n', '\t- 处理方法: **年收入超过100000000值(1人)替换为np.nan**\n', '10. AMT_CREDIT\n', '\t- 含义：贷款金额\n', '\t- 字段：45,000至4,050,000\n', '11. AMT_ANNUITY\n', '\t- 含义：贷款年金\n', '\t- 字段：1,616至258,026\n', '12. AMT_GOODS_PRICE\n', '\t- 含义：对于消费贷款，它是给予贷款的商品的价格\n', '\t- 字段：40,500至4,050,000\n', '13. NAME_TYPE_SUITE\n', '\t- 含义：谁在申请贷款时陪同客户\n', '\t- 字段：Unaccompanied/Family/Spouse, partner/children/Other_B/Other_A/Group of people\n', '14. NAME_INCOME_TYPE\n', '\t- 含义：客户收入类型\n', '\t- 字段：Working/Commercial associate/Pensioner/State servant/Unemployed/Student/Businessman/Maternity leave\n', '15. NAME_EDUCATION_TYPE\n', '\t- 含义：客户最高学历\n', '\t- 字段：Secondary/Higher education/Incomplete higher/Lower secondary/Academic degree\n', '16. NAME_FAMILY_STATUS\n', '\t- 含义：客户的家庭状况\n', '\t- 字段：Married/Single/Civil marriage/Separated/Widow/Unknown (我的理解是civil marriage指世俗婚姻或者民事婚姻，与教会承认而合法的婚姻相对。在欧美婚姻被合法承认有两种途径，一种由政府民事机构承认，一种由教会牧师承认。世俗婚姻即指前者。中国是只有世俗婚姻，而欧美在历史上曾经只有教会承认的婚姻，发展到现在则是两种婚姻并存。)\n', '17. NAME_HOUSING_TYPE\n', '\t- 含义：客户的住房情况\n', '\t- 字段：House apartment/With parents/Municipal apartment(市政公寓)/Rented apartment/Office apartment/Co-op apartment\n', '18. REGION_POPULATION_RELATIVE\n', '\t- 含义：客户所居住的区域的正常化人口（数字越大意味着客户居住在人口更稠密的地区）\n', '\t- 字段：0.00至0.07\n', '19. DAYS_BIRTH\n', '\t- 含义：客户出生日距离申请日天数\n', '\t- 字段：-25,229至-7,338 (20岁至69岁)\n', '\t- 处理方法: **取绝对值**\n', '20. DAYS_EMPLOYED\n', '\t- 含义：客户当前工作开始日期距离申请日天数\n', '\t- 字段：-17,912至**365,243(异常值)** \n', '\t- 处理方法: **365243全部替换为np.nan/取绝对值**\n', '21. DAYS_REGISTRATION\n', '\t- 含义：申请前几天客户改变了他的注册 (**什么样的注册?**)\n', '\t- 字段：-24,672至0\n', '\t- 处理方法: **取绝对值**\n', '22. DAYS_ID_PUBLISH\n', '\t- 含义：客户在申请前几天更改了申请贷款的身份证明文件\n', '\t- 字段：-7,197至-1,717\n', '\t- 处理方法: **取绝对值**\n', '23. OWN_CAR_AGE\n', '\t- 含义：客户车辆年限\n', '\t- 字段：0-91\n', '24. FLAG_MOBIL\n', '\t- 含义：客户是否提供移动电话号\n', '\t- 字段：1/0\n', '25. FLAG_EMP_PHONE\n', '\t- 含义：客户是否提供工作电话号\n', '\t- 字段：1/0\n', '26. FLAG_WORK_PHONE\n', '\t- 含义：客户是否提供家庭电话号\n', '\t- 字段：1/0\n', '27. FLAG_CONT_MOBILE\n', '\t- 含义：移动电话号是否可以拨通\n', '\t- 字段：1/0\n', '28. FLAG_PHONE\n', '\t- 含义：客户是否提供家庭电话号 (**跟FLAG_WORK_PHONE重复？**)\n', '\t- 字段：1/0\n', '\t- 处理方法: **删除**\n', '29. FLAG_EMAIL\n', '\t- 含义：客户是否提供邮件地址\n', '\t- 字段：1/0\n', '30. OCCUPATION_TYPE\n', '\t- 含义：客户职业类型\n', '\t- 字段：Laborers/Sales staff/Core Staff/Managers/Drivers/High Skill Tech Staff/Accountants/Medicine Staff/Security Staff/Cooking Staff/Cleaning Staff/Private service staff/Low-skill Laborers/Waiters barmen staff/Secretaries/Realty agents/HR staff/IT staff \n', '31. CNT_FAM_MEMBERS\n', '\t- 含义：客户拥有多少家庭成员\n', '\t- 字段：1-21\n', '32. REGION_RATING_CLIENT\n', '\t- 含义：我们对客户居住地区的评分\n', '\t- 字段：1/2/3\n', '33. REGION_RATING_CLIENT_W_CITY\n', '\t- 含义：我们对客户居住地区的评分(将城市因素考虑进去)\n', '\t- 字段：1/2/3/**-1(异常值)**\n', '\t- 处理方法: **去除-1行**\n', '34. WEEKDAY_APPR_PROCESS_START\n', '\t- 含义：客户在一周的第几天申请的贷款\n', '\t- 字段：MONDAY至SUNDAY\n', '35. HOUR_APPR_PROCESS_START\n', '\t- 含义：客户大约在几点申请的贷款\n', '\t- 字段：0-23\n', '36. REG_REGION_NOT_LIVE_REGION\n', '\t- 含义：客户永久地址不匹配客户联系地址标识(地区层级)\n', '\t- 字段：1/0\n', '37. REG_REGION_NOT_WORK_REGION\n', '\t- 含义：客户永久地址不匹配客户工作地址标识(地区层级)\n', '\t- 字段：1/0\n', '38. LIVE_REGION_NOT_WORK_REGION\n', '\t- 含义：客户联系地址不匹配客户工作地址标识(地区层级)\n', '\t- 字段：1/0\n', '39. REG_CITY_NOT_LIVE_CITY\n', '\t- 含义：客户永久地址不匹配客户联系地址标识(城市层级)\n', '\t- 字段：1/0\n', '40. REG_CITY_NOT_WORK_CITY\n', '\t- 含义：客户永久地址不匹配客户工作地址标识(城市层级)\n', '\t- 字段：1/0\n', '41. LIVE_CITY_NOT_WORK_CITY\n', '\t- 含义：客户联系地址不匹配客户工作地址标识(城市层级)\n', '\t- 字段：1/0\n', '42. ORGANIZATION_TYPE\n', '\t- 含义：客户工作机构类型\n', '\t- 字段：Business Entity Type 3/**XNA**/Self-employed/Other...\n', '\t- 处理方法: **XNA不做处理,把它当做一类**\n', '43. EXT_SOURCE_1\n', '\t- 含义：来自外部数据源的标准化分数 (**什么分数？**)\n', '\t- 字段：0-1\n', '44. EXT_SOURCE_2\n', '\t- 含义：来自外部数据源的标准化分数\n', '\t- 字段：0-1\n', '45. EXT_SOURCE_3\n', '\t- 含义：来自外部数据源的标准化分数\n', '\t- 字段：0-1\n', '46. APARTMENTS_AVG\n', '\t- 含义：**关于客户居住的建筑物的标准化信息**，平均值（_AVG后缀），众数（_MODE后缀），中位数（_MEDI后缀）；房屋面积均值? (**字段意义不明**) \n', '\t- 字段：0-1\n', '47. BASEMENTAREA_AVG\n', '\t- 含义：地下室区域面积均值\n', '\t- 字段：0-1\n', '48. YEARS_BEGINEXPLUATATION_AVG\n', '\t- 含义：开始建造年限均值\n', '\t- 字段：0-1\n', '49. YEARS_BUILD_AVG\n', '\t- 含义：建造年限均值\n', '\t- 字段：0-1\n', '50. COMMONAREA_AVG\n', '\t- 含义：公共区域面积均值\n', '\t- 字段：0-1\n', '51. ELEVATORS_AVG\n', '\t- 含义：电梯数均值\n', '\t- 字段：0-1\n', '52. ENTRANCES_AVG\n', '\t- 含义：入口数均值\n', '\t- 字段：0-1\n', '53. FLOORSMAX_AVG\n', '\t- 含义：最大楼层数均值\n', '\t- 字段：0-1\n', '54. FLOORSMIN_AVG\n', '\t- 含义：最小楼层数均值\n', '\t- 字段：0-1\n', '55. LANDAREA_AVG\n', '\t- 含义：土地面积均值\n', '\t- 字段：0-1\n', '56. LIVINGAPARTMENTS_AVG\n', '\t- 含义：生活公寓面积均值? (**字段意义不明**)\n', '\t- 字段：0-1\n', '57. LIVINGAREA_AVG\n', '\t- 含义：生活区域面积均值\n', '\t- 字段：0-1\n', '58. NONLIVINGAPARTMENTS_AVG (**字段意义不明**)\n', '\t- 含义：非生活公寓面积均值?\n', '\t- 字段：0-1\n', '59. NONLIVINGAREA_AVG\n', '\t- 含义：非生活区域面积均值\n', '\t- 字段：0-1\n', '60. APARTMENTS_MODE\n', '\t- 含义：公寓面积众数? (**字段意义不明**)\n', '\t- 字段：0-1\n', '61. BASEMENTAREA_MODE\n', '\t- 含义：地下室区域面积众数\n', '\t- 字段：0-1\n', '62. YEARS_BEGINEXPLUATATION_MODE\n', '\t- 含义：开始建造年限众数\n', '\t- 字段：0-1\n', '63. YEARS_BUILD_MODE\n', '\t- 含义：建造年限众数\n', '\t- 字段：0-1\n', '64. COMMONAREA_MODE\n', '\t- 含义：公共区域面积众数\n', '\t- 字段：0-1\n', '65. ELEVATORS_MODE\n', '\t- 含义：电梯数众数\n', '\t- 字段：0-1\n', '66. ENTRANCES_MODE\n', '\t- 含义：入口数众数\n', '\t- 字段：0-1\n', '67. FLOORSMAX_MODE\n', '\t- 含义：最大楼层数众数\n', '\t- 字段：0-1\n', '68. FLOORSMIN_MODE\n', '\t- 含义：最小楼层数众数\n', '\t- 字段：0-1\n', '69. LANDAREA_MODE\n', '\t- 含义：土地面积众数\n', '\t- 字段：0-1\n', '70. LIVINGAPARTMENTS_MODE\n', '\t- 含义：生活公寓面积众数?\n', '\t- 字段：0-1\n', '71. LIVINGAREA_MODE\n', '\t- 含义：生活区域面积众数\n', '\t- 字段：0-1\n', '72. NONLIVINGAPARTMENTS_MODE\n', '\t- 含义：非生活公寓面积众数？\n', '\t- 字段：0-1\n', '73. NONLIVINGAREA_MODE\n', '\t- 含义：非生活区域面积众数\n', '\t- 字段：0-1\n', '74. APARTMENTS_MEDI\n', '\t- 含义：公寓中位数\n', '\t- 字段：0-1\n', '75. BASEMENTAREA_MEDI\n', '\t- 含义：地下室面积中位数\n', '\t- 字段：0-1\n', '76. YEARS_BEGINEXPLUATATION_MEDI\n', '\t- 含义：开始建造年限中位数\n', '\t- 字段：0-1\n', '77. YEARS_BUILD_MEDI\n', '\t- 含义：建造年限中位数\n', '\t- 字段：0-1\n', '78. COMMONAREA_MEDI\n', '\t- 含义：公共区域中位数\n', '\t- 字段：0-1\n', '79. ELEVATORS_MEDI\n', '\t- 含义：电梯数中位数\n', '\t- 字段：0-1\n', '80. ENTRANCES_MEDI\n', '\t- 含义：入口数中位数\n', '\t- 字段：0-1\n', '81. FLOORSMAX_MEDI\n', '\t- 含义：最大楼层数中位数\n', '\t- 字段：0-1\n', '82. FLOORSMIN_MEDI\n', '\t- 含义：最小楼层数中位数\n', '\t- 字段：0-1\n', '83. LANDAREA_MEDI\n', '\t- 含义：土地面积中位数\n', '\t- 字段：0-1\n', '84. LIVINGAPARTMENTS_MEDI\n', '\t- 含义：生活公寓面积中位数?\n', '\t- 字段：0-1\n', '85. LIVINGAREA_MEDI\n', '\t- 含义：生活区域面积中位数\n', '\t- 字段：0-1\n', '86. NONLIVINGAPARTMENTS_MEDI\n', '\t- 含义：非生活公寓面积中位数?\n', '\t- 字段：0-1\n', '87. NONLIVINGAREA_MEDI\n', '\t- 含义：非生活区域面积中位数\n', '\t- 字段：0-1\n', '88. FONDKAPREMONT_MODE\n', '\t- 含义：账号类型众数\n', '\t- 字段：reg oper account 注册账号/reg oper spec account 注册标注账号/not specified 未注明/org spec account 组织标注账号\n', '89. HOUSETYPE_MODE\n', '\t- 含义：房屋类型众数\n', '\t- 字段：block of flats 公寓楼/specific housing/terraced house 排屋\n', '90. TOTALAREA_MODE\n', '\t- 含义：房屋总面积众数\n', '\t- 字段：0-1\n', '91. WALLSMATERIAL_MODE\n', '\t- 含义：墙壁材料众数\n', '\t- 字段：Panel 面板/Stone, brick 石头/Block/Wooden 木质/Mixed 混合/Monolithic 独石/Others\n', '92. EMERGENCYSTATE_MODE\n', '\t- 含义：紧急状态众数\n', '\t- 字段：No/Yes\n', '93. OBS_30_CNT_SOCIAL_CIRCLE\n', '\t- 含义：客户周围关系圈内有多少人可观察到欠交贷款30天\n', '\t- 字段：0-354 (**354疑似异常值**)\n', '\t- 处理方法: **大于100值替换为np.nan**\n', '94. DEF_30_CNT_SOCIAL_CIRCLE\n', '\t- 含义：客户周围关系圈内有多少人已欠交贷款30天 (**与上个变量区别是什么？**)\n', '\t- 字段：0-34\n', '95. OBS_60_CNT_SOCIAL_CIRCLE\n', '\t- 含义：客户周围关系圈内有多少人可观察到欠交贷款60天\n', '\t- 字段：0-351\n', '\t- 处理方法: **大于100值替换为np.nan**\n', '96. DEF_60_CNT_SOCIAL_CIRCLE\n', '\t- 含义：客户周围关系圈内有多少人已欠交贷款60天\n', '\t- 字段：0-24\n', '97. DAYS_LAST_PHONE_CHANGE\n', '\t- 含义：客户距离申请贷款改变电话号码天数\n', '\t- 字段：-4361-0\n', '\t- 处理方法: **取绝对值**\n', '98. FLAG_DOCUMENT_2\n', '\t- 含义：客户是否提供文件2\n', '\t- 字段：0/1\n', '99. FLAG_DOCUMENT_3\n', '\t- 含义：客户是否提供文件3\n', '\t- 字段：0/1\n', '100. FLAG_DOCUMENT_4\n', '\t- 含义：客户是否提供文件4\n', '\t- 字段：0/1\n', '101. FLAG_DOCUMENT_5\n', '\t- 含义：客户是否提供文件5\n', '\t- 字段：0/1\n', '102. FLAG_DOCUMENT_6\n', '\t- 含义：客户是否提供文件6\n', '\t- 字段：0/1\n', '103. FLAG_DOCUMENT_7\n', '\t- 含义：客户是否提供文件7\n', '\t- 字段：0/1\n', '104. FLAG_DOCUMENT_8\n', '\t- 含义：客户是否提供文件8\n', '\t- 字段：0/1\n', '105. FLAG_DOCUMENT_9\n', '\t- 含义：客户是否提供文件9\n', '\t- 字段：0/1\n', '106. FLAG_DOCUMENT_10\n', '\t- 含义：客户是否提供文件10\n', '\t- 字段：0/1\n', '107. FLAG_DOCUMENT_11\n', '\t- 含义：客户是否提供文件11\n', '\t- 字段：0/1\n', '108. FLAG_DOCUMENT_12\n', '\t- 含义：客户是否提供文件12\n', '\t- 字段：0/1\n', '109. FLAG_DOCUMENT_13\n', '\t- 含义：客户是否提供文件13\n', '\t- 字段：0/1\n', '110. FLAG_DOCUMENT_14\n', '\t- 含义：客户是否提供文件14\n', '\t- 字段：0/1\n', '111. FLAG_DOCUMENT_15\n', '\t- 含义：客户是否提供文件15\n', '\t- 字段：0/1\n', '112. FLAG_DOCUMENT_16\n', '\t- 含义：客户是否提供文件16\n', '\t- 字段：0/1\n', '113. FLAG_DOCUMENT_17\n', '\t- 含义：客户是否提供文件17\n', '\t- 字段：0/1\n', '114. FLAG_DOCUMENT_18\n', '\t- 含义：客户是否提供文件18\n', '\t- 字段：0/1\n', '115. FLAG_DOCUMENT_19\n', '\t- 含义：客户是否提供文件19\n', '\t- 字段：0/1\n', '116. FLAG_DOCUMENT_20\n', '\t- 含义：客户是否提供文件20\n', '\t- 字段：0/1\n', '117. FLAG_DOCUMENT_21\n', '\t- 含义：客户是否提供文件21\n', '\t- 字段：0/1\n', '118. AMT_REQ_CREDIT_BUREAU_HOUR\n', '\t- 含义：申请前一小时向信用局询问客户的情况次数\n', '\t- 字段：0/1/2/3/4\n', '119. AMT_REQ_CREDIT_BUREAU_DAY\n', '\t- 含义：申请前一天(排除前一小时)向信用局询问客户的情况次数\n', '\t- 字段：0-8\n', '120. AMT_REQ_CREDIT_BUREAU_WEEK\n', '\t- 含义：申请前一天(排除前一天)向信用局询问客户的情况次数\n', '\t- 字段：0-8\n', '121. AMT_REQ_CREDIT_BUREAU_MON\n', '\t- 含义：申请前一月(排除前一周)向信用局询问客户的情况次数\n', '\t- 字段：0-26\n', '122. AMT_REQ_CREDIT_BUREAU_QRT\n', '\t- 含义：申请前三月(排除前一月)向信用局询问客户的情况次数\n', '\t- 字段：0-261 (**261疑似异常值**)\n', '\t- 处理方法: **大于100值替换为np.nan**\n', '123. AMT_REQ_CREDIT_BUREAU_YEAR\n', '\t- 含义：申请前一年(排除前三月)向信用局询问客户的情况次数\n', '\t- 字段：0-25\n', '\n', '# bureau.csv\n', '1. 数据量及描述\n', '\t- (1716428,17) \n', '\t- 描述：所有客户之前由其他金融机构提供给信用局的信用报告（对于我们样本中有贷款的客户）。对于我们样本中的每笔贷款，客户在申请日期之前在信用局拥有的信贷数量与行数一样多。\n', '2. SK_ID_CURR\n', '\t- 含义：样本中贷款ID (样本中贷款可以有一至多笔在信用局相关的信贷记录)\n', '\t- 字段：100002/100003...\n', '3. SK_ID_BUREAU\n', '\t- 含义：与我们贷款相关的以前信用局信贷ID\n', '\t- 字段：5714462/5714463...\n', '4. CREDIT_ACTIVE\n', '\t- 含义：信用局记录在案的信贷的当前状态\n', '\t- 字段：Closed/Active/Sold/Bad Debt\n', '5. CREDIT_CURRENCY\n', '\t- 含义：信用局信贷的货币单位\n', '\t- 字段：currency 1/currency 2/currency 3/currency 4\n', '6. DAYS_CREDIT\n', '\t- 含义：客户申请信用局信贷距离当前申请天数\n', '\t- 字段：-2922至0\n', '\t- 处理方法: **取绝对值**\n', '7. CREDIT_DAY_OVERDUE\n', '\t- 含义：根据客户申请贷款的时间点来说，信用局信贷违约天数\n', '\t- 字段：0至2792\n', '8. DAYS_CREDIT_ENDDATE\n', '\t- 含义：根据客户申请贷款的时间点来说，在信用局信贷剩余的期限\n', '\t- 字段：-42,060至31,199 (**正数代表信贷未到期，负数代表信贷已经到期**)\n', '\t- 处理方法: **小于-20000值替换为np.nan (不可能在115年前该客户有贷款)**\n', '9. DAYS_ENDDATE_FACT\n', '\t- 含义：根据客户申请贷款的时间点来说，信用局信贷已经结束天数(仅限已结束的信贷)\n', '\t- 字段：-42,023至0 (42023=115年，肯定是错误的)\n', '\t- 处理方法: **小于-4000值替换为np.nan**\n', '10. AMT_CREDIT_MAX_OVERDUE\n', '\t- 含义：迄今为止，信用局信贷最高过期额度\n', '\t- 字段：0-**115,987,185.00(疑似异常值)** (0居多) \n', '\t- 处理方法: **大于10000000值替换为np.nan**\n', '11. CNT_CREDIT_PROLONG\n', '\t- 含义：信用局信贷延长了多少次\n', '\t- 字段：0-9 (0居多)\n', '12. AMT_CREDIT_SUM\n', '\t- 含义：信用局信贷的当前信用额度\n', '\t- 字段：0-**585,000,000(疑似异常值)** (1350000为95分位数，超过10000000有2183人)\n', '\t- 处理方法: **大于10000000值替换为np.nan**\n', '13. AMT_CREDIT_SUM_DEBT\n', '\t- 含义：信用局信贷的当前债务\n', '\t- 字段：-4,705,600.32至170,100,000 (**负值代表什么? 8418人小于0**)\n', '\t- 处理方法: **负值替换为0/大于50000000值替换为np.nan** \n', '14. AMT_CREDIT_SUM_LIMIT\n', '\t- 含义：信用局目前信用卡的信用额度\n', '\t- 字段：-586,406.11至4,705,600.32\n', '15. AMT_CREDIT_SUM_OVERDUE\n', '\t- 含义：信用局信贷当前逾期金额\n', '\t- 字段：0-3,756,681 (0居多)\n', '16. CREDIT_TYPE\n', '\t- 含义：信用局信贷类型\n', '\t- 字段：Consumer credit/Credit Card/Car loan/Mortgage/Microloan/Loan for business development...\n', '17. DAYS_CREDIT_UPDATE\n', '\t- 含义：最后一次来源于信用局的信息距离当今申请贷款天数\n', '\t- 字段：-41,947至372\n', '18. AMT_ANNUITY\n', '\t- 含义：信用局年度信用额度\n', '\t- 字段：0至**118,453,423(明显异常值)**\n', '\t- 处理方法: **超过10000000替换np.nan(24人)**\n', '\n', '# bureau_balance.csv\n', '1. 数据量及描述\n', '\t- (27299925,3) \n', '\t- 描述：信贷局以往信贷的每月金额。该表一行对应每月向信用局报告的信贷记录\n', '\t- 逻辑梳理：当今样本中贷款数(train) * 当前贷款相关的之前信用局的信贷数(bureau) * 之前信用局单笔信贷有记录月度信息的月数(bureau_balance)\n', '2. SK_BUREAU_ID\n', '\t- 含义：与我们贷款相关的以前信用局信贷ID\n', '\t- 字段：5714462/5714463...\n', '3. MONTHS_BALANCE\n', '\t- 含义：相对于申请日来说的月份个数(-1指最新的日期)\n', '\t- 字段：-96至0\n', '\t- 处理方法: **取绝对值**\n', '4. STATUS\n', '\t- 含义：在该月期间信用局的贷款状态\n', '\t- 字段：C means closed, X means status unknown, 0 means no DPD, 1 means maximal did during month between 1-30, 2 means DPD 31-60, 5 means DPD 120+ or sold or written off\n', '\n', '# previous_application.csv\n', '1. 数据量及描述\n', '\t- (1670214, 37)\n', '\t- 描述：所有在我们样本中申请贷款客户的之前所有在Home Credit申请贷款的记录。一行代表之前的一笔申请\n', '2. SK_ID_PREV \n', '\t- 含义：与我们现在申请贷款相关的之前在Home Credit的申请 (一笔贷款可能会有之前的多笔贷款)\n', '\t- 字段：1000983/1026910...\n', '3. SK_ID_CURR\n', '\t- 含义：样本中贷款ID\n', '\t- 字段：100002/100003...\n', '4. NAME_CONTRACT_TYPE\n', '\t- 含义：上一个申请的合同产品类型\n', '\t- 字段：Cash loans/Consumer loans/Revolving loans/**XNA**\n', '5. AMT_ANNUITY\n', '\t- 含义：贷款年金\n', '\t- 字段：0至418,058\n', '6. AMT_APPLICATION\n', '\t- 含义：客户在之前申请中申请了多少额度\n', '\t- 字段：0至6,905,160\n', '7. AMT_CREDIT\n', '\t- 含义：客户之前申请里最终的信贷额度\n', '\t- 字段：0至6,905,160\n', '8. AMT_DOWN_PAYMENT\n', '\t- 含义：先前申请的预付金\n', '\t- 字段：-0.90至3,060,045 (**-0.9异常值**)\n', '\t- 处理方法: **小于0数值替换为0**\n', '9. AMT_GOODS_PRICE\n', '\t- 含义：先前申请中客户想要的商品价格(如果有的话)\n', '\t- 字段：0至6,905,160\n', '10. WEEKDAY_APPR_PROCESS_START\n', '\t- 含义：先前申请中客户是在第几天申请的\n', '\t- 字段：MONDAY至SUNDAY\n', '11. HOUR_APPR_PROCESS_START\n', '\t- 含义：先前申请中客户大约在几点申请的贷款\n', '\t- 字段：0至23\n', '12. FLAG_LAST_APPL_PER_CONTRACT\n', '\t- 含义：如果它是上次合同的最后一次申请，则标记。 有时候，由于客户或我们的职员的错误，可能会有单一合同的多份申请\n', '\t- 字段：Y/N\n', '13. NFLAG_LAST_APPL_IN_DAY\n', '\t- 含义：是否是当天申请的最后一次申请。有时候客户一天会有多次申请，也有很低可能会发生由于系统错误导致的一次申请在数据库中出现两次\n', '\t- 字段：1/0\n', '14. NFLAG_MICRO_CASH\n', '\t- 含义：是否是小额贷款\n', '\t- 字段：**数据未有该字段**\n', '15. RATE_DOWN_PAYMENT\n', '\t- 含义：标准化的预付金\n', '\t- 字段：0.0至1.0\n', '16. RATE_INTEREST_PRIMARY\n', '\t- 含义：标准化利率\n', '\t- 字段：0.03至1.00\n', '17. RATE_INTEREST_PRIVILEGED\n', '\t- 含义：标准化利率 (**区别？**)\n', '\t- 字段：0.37至1.00\n', '18. NAME_CASH_LOAN_PURPOSE\n', '\t- 含义：现金贷款目的\n', '\t- 字段：**XAP/XNA**/Repairs/Other/Urgent Needs/Buying a used car/Building a house or an annex...\n', '\t- 处理方法: **不做处理**\n', '19. NAME_CONTRACT_STATUS\n', '\t- 含义：先前申请的合同状态\n', '\t- 字段：Approved/Canceled/Refused/Unused offer\n', '20. DAYS_DECISION\n', '\t- 含义：相对于当前申请，上次申请何时做出\n', '\t- 字段：-2,922至-1\n', '\t- 处理方法: **取绝对值**\n', '21. NAME_PAYMENT_TYPE\n', '\t- 含义：客户选择的付款方式\n', '\t- 字段：Cash through the bank/**XNA**/Non-cash from your account/Cashless from the account of the employer\n', '\t- 处理方法: **不做处理**\n', '22. CODE_REJECT_REASON\n', '\t- 含义：先前申请被拒绝原因\n', '\t- 字段：**XAP**/NC/**XNA**/LIMIT...\n', '\t- 处理方法: **不做处理**\n', '23. NAME_TYPE_SUITE\n', '\t- 含义：谁陪同客户去做的申请\n', '\t- 字段：Unaccompanied/Family/Spouse, partner...\n', '24. NAME_CLIENT_TYPE\n', '\t- 含义：是否新老客户\n', '\t- 字段：Repeater/New/Refreshed/**XNA**\n', '\t- 处理方法: **不做处理**\n', '25. NAME_GOODS_CATEGORY\n', '\t- 含义：客户先前申请的商品种类\n', '\t- 字段：**XNA**/Mobile/Consumer Electronics/Computers...\n', '\t- 处理方法: **不做处理**\n', '26. NAME_PORTFOLIO\n', '\t- 含义：之前的申请是为了CASH，POS，CAR\n', '\t- 字段：CASH/POS/CAR/Cards/**XNA**\n', '\t- 处理方法: **不做处理**\n', '27. NAME_PRODUCT_TYPE\n', '\t- 含义：先前申请是电子还是现场申请？(**?**)\n', '\t- 字段：**XNA**/x-sell/walk-in\n', '\t- 处理方法: **不做处理**\n', '28. CHANNEL_TYPE\n', '\t- 含义：我们通过何种渠道获得的客户\n', '\t- 字段：Credit and cash offices/Country-wide/Stone...\n', '29. SELLERPLACE_AREA\n', '\t- 含义：先前申请卖方的销售面积\n', '\t- 字段：-1至4,000,000 (**异常值**)\n', '\t- 处理方法: **小于0的值替换为0**\n', '30. NAME_SELLER_INDUSTRY\n', '\t- 含义：卖方行业\n', '\t- 字段：**XNA**/Consumer electronics/Connectivity/Furniture...\n', '\t- 处理方法: **不做处理** \n', '31. CNT_PAYMENT\n', '\t- 含义：申请时的先前信用期限\n', '\t- 字段：0至84\n', '32. NAME_YIELD_GROUP\n', '\t- 含义：利率分级\n', '\t- 字段：**XNA**/middle/high/low_normal/low_action\n', '\t- 处理方法: **不做处理** \n', '33. PRODUCT_COMBINATION\n', '\t- 含义：产品组合细节\n', '\t- 字段：Cash/POS household with interest/POS mobile with interest...\n', '34. DAYS_FIRST_DRAWING\n', '\t- 含义：相对于当前申请的申请日期，先前申请第一次支付时间\n', '\t- 字段：-2,922至**365,243**\n', '\t- 处理方法: **365243替换为np.nan/取绝对值**\n', '35. DAYS_FIRST_DUE\n', '\t- 含义：相对于当前申请的申请日期，先前申请第一次应该到期时间\n', '\t- 字段：-2,892至**365,243**\n', '\t- 处理方法: **365243替换为np.nan/取绝对值**\n', '36. DAYS_LAST_DUE_1ST_VERSION\n', '\t- 含义：相对于当前申请的申请日期，先前申请第一次实际到期时间\n', '\t- 字段：-2,801至**365,243**\n', '\t- 处理方法: **365243替换为np.nan/取绝对值**\n', '37. DAYS_LAST_DUE\n', '\t- 含义：相对于当前申请的申请日期，先前申请最后一次实际到期时间\n', '\t- 字段：-2,889至**365,243**\n', '\t- 处理方法: **365243替换为np.nan/取绝对值**\n', '38. DAYS_TERMINATION\n', '\t- 含义：相对于当前申请的申请日期，先前申请期望的结束时间\n', '\t- 字段：-2,874至**365,243**\n', '\t- 处理方法: **365243替换为np.nan/取绝对值**\n', '39. NFLAG_INSURED_ON_APPROVAL\n', '\t- 含义：客户是否在先前申请里申请了保险\n', '\t- 字段：0/1\n', '\n', '# POS_CASH_balance.csv\n', '1. 数据量及描述\n', '\t- (10001358, 8) \n', '\t- 描述：申请人先前申请的POS贷和现金贷的月度余额快照 (POS贷含义：短期资金需求的小微企业主和个体工商户，缺乏传统银行所需要的担保、抵质押物等条件，融资困难，凭借POS机进行贷款，利用交易流水换贷款授信)\n', '2. SK_ID_PREV \n', '\t- 含义：与我们现在申请贷款相关的之前在Home Credit的申请 (一笔贷款可能会有之前的多笔贷款)\n', '\t- 字段：1000983/1026910...\n', '3. SK_ID_CURR \n', '\t- 含义：样本中贷款ID\n', '\t- 字段：10002/10003...\n', '4. MONTHS_BALANCE\n', '\t- 含义：相对于申请日期的月份余额（-1表示最新月度快照的信息，0表示申请时的信息 - 通常与-1相同，因为许多银行没有定期向信用局更新信息）\n', '\t- 字段：-96至-1\n', '\t- 处理方法: **取绝对值** \n', '5. CNT_INSTALMENT\n', '\t- 含义：以前的信用期限（可以随时间变化）\n', '\t- 字段：1至92\n', '6. CNT_INSTALMENT_FUTURE\n', '\t- 含义：待付的分期付款期数\n', '\t- 字段：0至85\n', '7. NAME_CONTRACT_STATUS\n', '\t- 含义：该月期间的合同状态\n', '\t- 字段：Active/Completed/Signed/Demand/**XNA**...\n', '\t- 处理方法: **不做处理**\n', '8. SK_DPD\t\n', '    - 含义：该月期间逾期天数\n', '\t- 字段：0至4231 (0居多)\n', '9. SK_DPD_DEF\n', '\t- 含义：该月期间逾期天数 (低贷债务被忽略)\n', '\t- 字段：0至3595\n', '\n', '# credit_card_balance.csv\n', '1. 数据量及描述\n', '\t- (3840312, 23) \n', '\t- 描述：申请人先前信用卡的月度余额快照\n', '2. SK_ID_PREV \n', '\t- 含义：与我们现在申请贷款相关的之前在Home Credit的申请 (一笔贷款可能会有之前的多笔贷款)\n', '\t- 字段：1000983/1026910...\n', '3. SK_ID_CURR \n', '\t- 含义：样本中贷款ID\n', '\t- 字段：10002/10003...\n', '4. MONTHS_BALANCE\n', '\t- 含义：相对于申请日期的月份分数余额（-1表示最新月度快照的信息)\n', '\t- 字段：-96至-1\n', '\t- 处理方法: **取绝对值**\n', '5. AMT_BALANCE\n', '\t- 含义：先前信贷的月度余额\n', '\t- 字段：-420,250.18至1,505,902.19\n', '\t- 处理方法: **不做处理**\n', '6. AMT_CREDIT_LIMIT_ACTUAL\n', '\t- 含义：信用卡月度限额\n', '\t- 字段：0至1,350,000\n', '7. AMT_DRAWINGS_ATM_CURRENT\n', '\t- 含义：月度从ATM提取的金额\n', '\t- 字段：-6,827.31至2,115,000 (**负数的意思？**)\n', '\t- 处理方法: **小于0的值替换为np.nan**\n', '8. AMT_DRAWINGS_CURRENT\n', '\t- 含义：月度提取的金额\n', '\t- 字段：-6,211.62至2,287,098.31 \n', '\t- 处理方法: **小于0的值替换为np.nan**\n', '9. AMT_DRAWINGS_OTHER_CURRENT\n', '\t- 含义：月度其它提取的金额\n', '\t- 字段：0至1,529,847\n', '10. AMT_DRAWINGS_POS_CURRENT\n', '\t- 含义：月度提取金额或购买商品金额\n', '\t- 字段：0至2,239,274.16\n', '11. AMT_INST_MIN_REGULARITY\n', '\t- 含义：本月最小分期付款金额\n', '\t- 字段：0至202,882.01\n', '12. AMT_PAYMENT_CURRENT\n', '\t- 含义：客户月度付款金额\n', '\t- 字段：0至4,289,207.45\n', '13. AMT_PAYMENT_TOTAL_CURRENT\n', '\t- 含义：客户月度付款总金额\n', '\t- 字段：0至4,278,315.69\n', '14. AMT_RECEIVABLE_PRINCIPAL\n', '\t- 含义：收款的本金金额\n', '\t- 字段：-423,305.82至1,472,316.79\n', '15. AMT_RECIVABLE\n', '\t- 含义：收款的金额\n', '\t- 字段：-420,250.18至1,493,338.19\n', '16. AMT_TOTAL_RECEIVABLE\n', '\t- 含义：总共收款的金额\n', '\t- 字段：-420,250.18至1,493,338.19\n', '17. CNT_DRAWINGS_ATM_CURRENT\n', '\t- 含义：月度从ATM提取的次数\n', '\t- 字段：0-51\n', '18. CNT_DRAWINGS_CURRENT\n', '\t- 含义：月度提取的次数\n', '\t- 字段：0-165\n', '19. CNT_DRAWINGS_OTHER_CURRENT\n', '\t- 含义：月度其它提取的次数\n', '\t- 字段：0-12\n', '20. CNT_DRAWINGS_POS_CURRENT\n', '\t- 含义：月度用于商品提取的次数\n', '\t- 字段：0-165\n', '21. CNT_INSTALMENT_MATURE_CUM\n', '\t- 含义：已付的分期次数\n', '\t- 字段：0-120\n', '22. NAME_CONTRACT_STATUS\n', '\t- 含义：合同状态\n', '\t- 字段：Active/Completed/Signed...\n', '23. SK_DPD\n', '    - 含义：该月期间逾期天数\n', '\t- 字段：0至3260 (0居多)\n', '24. SK_DPD_DEF\n', '\t- 含义：该月期间逾期天数 (低贷债务被忽略)\n', '\t- 字段：0至3260\n', '\n', '# installments_payments.csv\n', '1. 数据量及描述\n', '\t- (13605401, 8) \n', '\t- 描述：先前信贷的还款历史；一次分期付款相当于我们样本中与贷款相关的一个先前Home Credit信贷的一次付款。\n', '2. SK_ID_PREV \n', '\t- 含义：与我们现在申请贷款相关的之前在Home Credit的申请 (一笔贷款可能会有之前的多笔贷款)\n', '\t- 字段：1000983/1026910...\n', '3. SK_ID_CURR\n', '\t- 含义：样本中贷款ID\n', '\t- 字段：10002/10003...\n', '4. NUM_INSTALMENT_VERSION\n', ' \t- 含义：先前信贷的分期日历版本。月之间分期日历版本的更改意味着付款日历的一些参数已经改变了\n', '\t- 字段：0至178\n', '5. NUM_INSTALMENT_NUMBER\n', ' \t- 含义：在哪一期我们观察付款\n', '\t- 字段：1-277\n', '6. DAYS_INSTALMENT\n', '\t- 含义：分期应该何时付款\n', '\t- 字段：-2,922至-1\n', '\t- 处理方法: **取绝对值**\n', '7. DAYS_ENTRY_PAYMENT\n', '\t- 含义：分期实际在何时付款\n', '\t- 字段：-4,921至-1\n', '\t- 处理方法: **取绝对值**\n', '8. AMT_INSTALMENT\n', '\t- 含义：本期中规定的分期金额是多少？\n', '\t- 字段：0至3,771,487.85\n', '9. AMT_PAYMENT\n', '\t- 含义：客户实际在此次分期中付款多少\n', '\t- 字段：0至3,771,487.85\n']",1,fanzzz,understand-variables-in-chinese,fanzzz_understand-variables-in-chinese_m1
,0,frednavruzov,auto-feature-generation-featuretools-example,frednavruzov_auto-feature-generation-featuretools-example_m0
"**Hi Guys!**

For those, who want to play with **automated feature engineering** - please find below small example of how to do it with [featuretools](https://docs.featuretools.com/)

Featuretools is a framework to perform automated feature engineering. 
<br>It (at least it's stated that) excels at transforming transactional and relational datasets into feature matrices for machine learning.",1,frednavruzov,auto-feature-generation-featuretools-example,frednavruzov_auto-feature-generation-featuretools-example_m1
### Load Main table (Applications),2,frednavruzov,auto-feature-generation-featuretools-example,frednavruzov_auto-feature-generation-featuretools-example_m2
### Load previous applications table,3,frednavruzov,auto-feature-generation-featuretools-example,frednavruzov_auto-feature-generation-featuretools-example_m3
## TABLE JOINING (FEATURE TOOLS),4,frednavruzov,auto-feature-generation-featuretools-example,frednavruzov_auto-feature-generation-featuretools-example_m4
"An `EntitySet` is a collection of entities and the relationships between them. 

They are useful for preparing raw, structured datasets for feature engineering. 
<br>While many functions in Featuretools take `entities` and `relationships` as separate arguments,
<br>it is recommended to create an `EntitySet`, so you can more easily manipulate your data as needed.",5,frednavruzov,auto-feature-generation-featuretools-example,frednavruzov_auto-feature-generation-featuretools-example_m5
"In the call to `entity_from_dataframe`, we specified three important parameters

- The `index` parameter specifies the column that uniquely identifies rows in the dataframe
- The `time_index` parameter tells Featuretools when the data was ""created"".
- The `variable_types` parameter indicates that some columns should be interpreted as a Categorical variable, 
even though it just an integer in the underlying data.
",6,frednavruzov,auto-feature-generation-featuretools-example,frednavruzov_auto-feature-generation-featuretools-example_m6
"**Adding a Relationship**

With two entities in our entity set, we can add a relationship between them.

We want to relate these two entities by the columns called “SK_ID_CURR” in each entity. 
<br>Each application has multiple previous applications associated with it, 
<br>so it is called it the parent entity, while the previous applications  entity is known as the child entity. 
<br>When specifying relationships we list the variable in the parent entity first. Note that each ft.Relationship must denote a **one-to-many relationship** rather than a relationship which is one-to-one or many-to-many.",7,frednavruzov,auto-feature-generation-featuretools-example,frednavruzov_auto-feature-generation-featuretools-example_m7
"**Feature primitives**

Feature primitives are the building blocks of Featuretools. They define individual computations that can be applied to raw datasets to create new features. Because a primitive only constrains the input and output data types, they can be applied across datasets and can stack to create new calculations.

**Why primitives?**

The space of potential functions that humans use to create a feature is expansive. By breaking common feature engineering calculations down into primitive components, we are able to capture the underlying structure of the features humans create today.

See [documentation](https://docs.featuretools.com/automated_feature_engineering/primitives.html) for further details",8,frednavruzov,auto-feature-generation-featuretools-example,frednavruzov_auto-feature-generation-featuretools-example_m8
"**Handling time**

When performing feature engineering to learn a model to predict the future, 
<br>the value to predict will be associated with a time. 
In this case, it is paramount to only incorporate data prior to this `“cutoff time”` when calculating the feature values.

Featuretools is designed to take time into consideration when required. 
<br>By specifying a cutoff time, we can control what portions of the data are used when calculating features.

We can specify the time for each instance of the `target_entity` to calculate features. 
<br>The timestamp represents the last time data can be used for calculating features. This is specified using a dataframe of cutoff times. 

Read more [here](https://docs.featuretools.com/automated_feature_engineering/handling_time.html)",9,frednavruzov,auto-feature-generation-featuretools-example,frednavruzov_auto-feature-generation-featuretools-example_m9
"**Running DFS with training windows**

Training windows are an extension of cutoff times: starting from the cutoff time and moving backwards through time, only data within that window of time will be used to calculate features. We will use events **within 2 month time window**",10,frednavruzov,auto-feature-generation-featuretools-example,frednavruzov_auto-feature-generation-featuretools-example_m10
"**Hope this small example inspired you to try this approach yourself! 
<br>Have fun - add custom features, add more tables and relationships, gather hands on experience**

**Likes and comments are welcome :)**",11,frednavruzov,auto-feature-generation-featuretools-example,frednavruzov_auto-feature-generation-featuretools-example_m11
"Here's how I load the data and reduce the memory usage of each dataframe.  I can save from 60% to 75% of memory usage on each dataframe.  
This method is inspired from this [kernel](https://www.kaggle.com/arjanso/reducing-dataframe-memory-size-by-65). I don't handle NANs at this point.  
Hope it helps.",0,gemartin,load-data-reduce-memory-usage,gemartin_load-data-reduce-memory-usage_m0
"This kernel shows how one can try to explain the predictions of a given boosted tree model using the lib SHAP https://github.com/slundberg/shap

The model is then retrained only with the best features to avoid fitting to noise and improve our LB score ;)

It is based on the best performing public script and some other variants: i.e. https://www.kaggle.com/jsaguiar/updated-0-792-lb-lightgbm-with-simple-features et al.",0,hmendonca,lightgbm-predictions-explained-with-shap-0-796,hmendonca_lightgbm-predictions-explained-with-shap-0-796_m0
## Using SHAP (SHapley Additive exPlanations),1,hmendonca,lightgbm-predictions-explained-with-shap-0-796,hmendonca_lightgbm-predictions-explained-with-shap-0-796_m1
"## Select Best Features
To avoid fitting to noise and improve our LB score ;)",2,hmendonca,lightgbm-predictions-explained-with-shap-0-796,hmendonca_lightgbm-predictions-explained-with-shap-0-796_m2
**Thank you everyone for showing your appreciation and support. It's my first gold medal in kernels and I hope to publish far better kernels than this in near future.**,0,ishaan45,thank-you,ishaan45_thank-you_m0
## Blend with one rank weighted submission [0.8 LB],1,ishaan45,thank-you,ishaan45_thank-you_m1
"## Diversified blend [0.799 LB]


**The blending ingredients are taken from three different type of models.**",2,ishaan45,thank-you,ishaan45_thank-you_m2
## Blending lowest correlated models,3,ishaan45,thank-you,ishaan45_thank-you_m3
,4,ishaan45,thank-you,ishaan45_thank-you_m4
"# Home Credit Default Risk Competition

## Why I Wrote this Kernel

This was my first Kaggle competition, and through several months of practice, research, trial and error, as well as extensive exploration of forum advice and kernels written by expert Kagglers, I was able to build a featureset/model that earned a final private leaderboard score of `0.79506` as a solo submission, which translated to final rank of 561 out of 7,198 -- just inside the top 8%.

Along the way, I had to figure out from scratch several things that seasoned Kagglers probably take for granted. These included things like:
* Are my Pandas operations as efficient as can be?
* How do I keep my memory use from ballooning out of control?
* What kind of cross validation should I use? And how do I implement it?
* Choosing a learning rate and tuning hyperparameters.
* Deciding how many boosting rounds to use during training.
* Should I use only one, or multiple training rounds?
* If more than one, how do I blend predictions from each training round?
* And finally, exactly how do I create that CSV file that I'll need to submit?

I wanted to publish this kernel in order create an example (both for my future self and for other beginners) of what it looks like to implement all the steps (preprocessing --> feature engineering --> cross validation --> training --> prediction generation --> post processing) that are necessary to build a (somewhat) competitive prediction algorithm. In other words, I wanted to demonstrate what it looks like to *""put it all together.""*

This is just single model LightGBM kernel, without any stacking. I'm planning to dive much deeper into stacking/blending/etc. in my next competition : )

I am indebted to the generosity of the community at large, and in particular, to the wisdom and techniques shared by folks like  [Silogram](https://www.kaggle.com/psilogram), [olivier](https://www.kaggle.com/ogrellier), and [Laurae](https://www.kaggle.com/laurae2).

Although I still consider myself to be a beginner who has a lot more to learn, these folks and their generosity have inspired me to do what little I can to give back. I hope that certain aspects of my code below, such as how my cross-validation method includes target encoding in a way that (I believe) prevents data leak, will be useful to others in the community. 

Finally, I welcome any tips and or feedback on my approach and implementation that follows below. Having one's blindspots pointed out to them is the surest way to growth and improvement  : )",0,jamesdellinger,home-credit-putting-all-the-steps-together,jamesdellinger_home-credit-putting-all-the-steps-together_m0
"## Summary of My Kernel

Single model LightGBM with 1,275 features in total. Solo submission. This was my best performing kernel, though not my final submission (more later on as to why this was the case). `0.79581` local CV score. `0.79524` private LB score. `0.79831` public LB score:


* I incorporated features from all seven of the data tables. I one-hot encoded categorical features from the bureau and previous application data tables. All other categorical features were left as categorical, and were ultimately target encoded. Some categorical features in the bureau and previous application tables were also target encoded.


* I tried my best to isolate unhelpful features and drop them. This proved to be time-consuming and it was soon clear to me that the process of vetting features one-by-one would not scale to the size of the competition's featureset. I had tried using LightGBM and SelectKBest feature importances to guide me, but found that they were more of a red herring than anything else, in that those importances did not reliably predict how a feature's absence would affect my local CV's ROC AUC score.


* I experimented with various types of scaling and normalization of numerical features, such as log-normalization, replacing NaN entries with 0, -1, -999999, etc., but found that none of these tweaks helped the performance of my LightGBM model. (This makes sense since LGBM is a tree-based model.)


* Several of my engineered features were simple aggregations, using mean, sum, min, max, etc. At the same time, I created a handful of bespoke features that made intuitive sense to me. 


* Stratified 5-fold CV for model selection.


* In addition to trying to add/remove features one-by-one, at times I tried adding/removing features in bulk (in the interest of saving time). It's not clear to me if one approach is necessarily better than the other all the time.

  I would find that there were times I would add one feature, see my CV score drop, then add several more features, re-tune my model parameters, see the CV score improve, then try and remove that first feature that had originally lowered my CV score, only to see the CV score fall after the feature was removed. This experience only strengthened my hunch that successful data science has an element of artfulness and intuition.
  

* I used LightGBM's built-in CV for feature selection and hyperparameter tuning. It trains and makes predictions on each validation fold in parallel, so much time is saved over running serial K-fold CV. Unfortunately, lightgbm.cv doesn't currently support the kind of preprocessing inside CV folds that would be necessary to properly perform target encoding during CV without leakage. (Believe me, I tried.)


* I found that using target encoding added just under 0.001 to my local CV and public LB scores. (Because lightgbm.cv doesn't support target encoding preprocessing for each fold, I had to use standard serial K-fold CV to do an apples-apples comparison of the performance of target encoding vs. merely using lightgbm's default categorical feature handling.)

  Interestingly, it ultimately turned out that the private LB score of my model that used target encoding (with number of boosting rounds determined by serial CV) was only just under 0.0001 better than that of my model that used LightGBM's default handling of categorical features (and had its number of boosting rounds determined by lightgbm.cv). If I had it to do over again, I'm not sure I'd use target encoding.
  

* To generate test set predictions, I trained my final model five times, each time using a different random seed for my LightGBM parameters, and generated five sets of test predictions. I used mean ranking to blend the sets of predictions. The number of boosting rounds was equal to 110% of the average round of highest score across each of the CV folds.

  Had I not used target encoding, I could have used lightgbm.cv and found the number of the actual single round where the average CV score across all folds was the highest. As it was, I had to settle for finding the round number of highest score for each of my five CV folds, and then take the average of those five round numbers.


* I didn't experiment with any other sort of ensemble methods such as blending different model types, or stacking. I am saving that for my next competition :)


### A Lesson I Learned About Overfitting to the Training Set:
As mentioned above, although this was my best performing kernel, it was only my second-to-final submission to the competition. In the final four days of the competition, I went on a spree of feature engineering/aggregation that more than doubled my feature count, arriving at a grand total of 2,782 features. This of course substantially decreased the speed of my data preprocessing and model training, but it did increase my local CV score from `0.79581` to `0.79665`. I figured the higher score was worth it and tagged that set of predictions as my final submission.

Unfortunately, while doing this raised my public LB score from `0.79831` to `0.80003`, it turned out that it would eventually lower my private LB score from `0.79524` to `0.79506`. Thankfully, this wasn't too huge a drop, and I was still able to achieve a bronze medal and score within the top 8% of the competition with my `0.79506` submission. 

In retrospect, although my local CV score did increase after adding all those extra features, I should have been suspicious because the difference between training and validation scores on each fold of my CV increased by nearly 50%. This was likely evidence that my model was now doing some serious overfitting in order to achieve that slight bump in local CV score. ",1,jamesdellinger,home-credit-putting-all-the-steps-together,jamesdellinger_home-credit-putting-all-the-steps-together_m1
"## Characteristics of the Competition and its Dataset

This [competition](https://www.kaggle.com/c/home-credit-default-risk) ran for three months, from May 17 to August 29, 2018. The objective was to build an algorithm that could predict the likelihood that a loan applicant would eventually default on his or her loan. The training set contained various financial and personal information originally taken from the loan application profiles 307,511 previous Home Credit borrowers. The test set had 48,744 borrower records. The scoring metric was area under the ROC curve. Features were contained in seven different data tables. The largest table contained demographic information such as job type and gender, along with various numerical features that described a borrower's financial status, such as normalized credit rating scores. Each of the six supplementary data tables contained different kinds of detailed financial records. (e.g. credit card payment histories, loan payment histories recorded as recorded by the credit bureau, etc.)

As for the content of the dataset itself, several features were noticeably sparse, and it was clear to me early-on that I would need an algorithm that handled NaN entries as deftly as possible. Furthermore, some features had names and descriptions that were cryptic or vague at best. At times this made it tough to gain an intuition of how to best engineer new features. There were even two features that were incorrectly described as normalized even though they were clearly categorical (they contained word strings as entries). Finally, several features didn't always have missing values represented by np.nan. For some numerical features, the integer 365243 was equivalent to NaN. For certain categorical features, the strings 'XNA' or 'XAP' were used to denote missing entries. None of this information was included in the dataset's description, but was shared in the forum by Home Credit's liaison during the course of the competition.

These speedbumps added a certain element of challenge that helped to level the playing field. Namely, there seemed to be a larger than normal benefit to those teams taking the time to diligently explore and understand all the quirks and idiosyncrasies of the dataset. Simply having the most advanced stacking or ensembling methods would not be enough to guarantee victory in this competition.

Most importantly, however, the consensus amongst competitors was that Home Credit's team went above and beyond in curating the test set such that data leak was minimized as much as possible. Furthermore, Home Credit's representative was active and responsive on the forums throughout the duration of the competition, and was helpful in clearing up questions that competitors had regarding the dataset and its feature definitions.

At the time of competition's conclusion, 7,198 teams had submitted entries, making this the largest ever featured competition in Kaggle's history. The [top team](https://www.kaggle.com/c/home-credit-default-risk/discussion/64821) achieved a private leaderboard score of `0.80570`.",2,jamesdellinger,home-credit-putting-all-the-steps-together,jamesdellinger_home-credit-putting-all-the-steps-together_m2
"## My Journey Toward Participating in this Competition

I began this competition in early June of 2018, in order to complete my [final project](https://github.com/jamesdellinger/machine_learning_nanodegree_capstone_project) for Udacity's Machine Learning Engineer Nanodegree.

I spent about a month exploring the idiosyncracies of the main data table's (`application_{train|test}.csv`) 120 features, experimented with various single model predictors (Naive Bayes, Logistic Regression, AdaBoost, Multi-Layer Perceptron, and LightGBM), and alternately trained each of them on the table's full feature set, on just the top 30 features according to SelectKBest, and on a featureset where the dimensionality of the main table's numerical features was reduced using PCA. I compared the performance of each of these models using a simple 80%-20% train-validation set split. 

Ultimately, my LightGBM single model that was trained on all 120 main table features performed the best, with a local CV score (ROC AUC) of `0.76092`. This translated to a public leaderboard score of `0.74111`. 

At this point, I was pleased that I had put into practice several of the techniques and algorithms I had learned while completing the Machine Learning Engineer Nanodegree. However, I was by no means satisfied that I had done all I could do to build a kernel that was *competitive* on the Home Credit dataset. This spurred me to spend the remainder of the competition learning and applying new techniques and approaches.",3,jamesdellinger,home-credit-putting-all-the-steps-together,jamesdellinger_home-credit-putting-all-the-steps-together_m3
"## Credit Where Credit is Due

I believe that apprenticeship and discipleship are some of the most effective ways to learn, and I have certainly benefited from being an indirect ""apprentice"" of some very generous world-class Kagglers. They have initiated and participated in extended threads in the forums, where they patiently described the ins and outs of their approaches and implementations. They have also shared kernels that contain real, usable implementations of some of the most important techniques. These kernels are far more useful than the run-of-the-mill blog posts that tend to introduce and wax poetic about the philosophy behind some advanced technique (such as stacking), yet leave the uninitiated woefully unprepared for the nitty-gritty and the subtle, yet crucial, details of the technique's actual implementation. 

I am particularly indebted to:

* [Silogram](https://www.kaggle.com/psilogram), whose extensive comments and advice on this [thread](https://www.kaggle.com/c/home-credit-default-risk/discussion/58332#348689) helped me to learn about mean rank prediction blending, a heuristic for deciding the number of training rounds, the usefulness of LightGBM's built-in CV, and most importantly, underscored for me the importance of trusting my CV and not giving in to the temptation to overfit to the public leaderboard.


* [olivier](https://www.kaggle.com/ogrellier), whose three kernels showed me how to implement [target encoding](https://www.kaggle.com/ogrellier/python-target-encoding-for-categorical-features/notebook), the different ways I could create [aggregate](https://www.kaggle.com/ogrellier/home-credit-hyperopt-optimization) features from the various data tables, as well as the code for using Seaborn to plot LightGBM [feature importance](https://www.kaggle.com/ogrellier/good-fun-with-ligthgbm/code).


* [Laurae](https://www.kaggle.com/laurae2), whose [masterpiece of a website](https://sites.google.com/view/lauraepp/parameters) taught me more than I could have dreamed about LightGBM, its parameters, and how to tune them.


* [neptune-ml](https://neptune.ml/), whose [open solution](https://github.com/neptune-ml/open-solution-home-credit/blob/solution-5/notebooks/eda-application.ipynb) showed me several features that I could engineer and aggregate from the main data table's featureset.",4,jamesdellinger,home-credit-putting-all-the-steps-together,jamesdellinger_home-credit-putting-all-the-steps-together_m4
## I. Preprocessing and Feature Engineering,5,jamesdellinger,home-credit-putting-all-the-steps-together,jamesdellinger_home-credit-putting-all-the-steps-together_m5
### Preprocessing Helper Functions,6,jamesdellinger,home-credit-putting-all-the-steps-together,jamesdellinger_home-credit-putting-all-the-steps-together_m6
###  1. Main Data Table `application_{train|test}.csv`,7,jamesdellinger,home-credit-putting-all-the-steps-together,jamesdellinger_home-credit-putting-all-the-steps-together_m7
### 2. Bureau Data Table `bureau.csv`,8,jamesdellinger,home-credit-putting-all-the-steps-together,jamesdellinger_home-credit-putting-all-the-steps-together_m8
### 3. Bureau Balance Data Table `bureau_balance.csv`,9,jamesdellinger,home-credit-putting-all-the-steps-together,jamesdellinger_home-credit-putting-all-the-steps-together_m9
### 4. Previous Application Data Table `previous_application.csv`,10,jamesdellinger,home-credit-putting-all-the-steps-together,jamesdellinger_home-credit-putting-all-the-steps-together_m10
### 5. POS CASH Balance Data Table `POS_CASH_balance.csv`,11,jamesdellinger,home-credit-putting-all-the-steps-together,jamesdellinger_home-credit-putting-all-the-steps-together_m11
### 6. Installments Payments Data Table `installments_payments.csv`,12,jamesdellinger,home-credit-putting-all-the-steps-together,jamesdellinger_home-credit-putting-all-the-steps-together_m12
### 7. Credit Card Balance Data Table `credit_card_balance.csv`,13,jamesdellinger,home-credit-putting-all-the-steps-together,jamesdellinger_home-credit-putting-all-the-steps-together_m13
### Engineer Features by Combining Features From Various Tables,14,jamesdellinger,home-credit-putting-all-the-steps-together,jamesdellinger_home-credit-putting-all-the-steps-together_m14
### Drop Unhelpful Features,15,jamesdellinger,home-credit-putting-all-the-steps-together,jamesdellinger_home-credit-putting-all-the-steps-together_m15
### Executing All Preprocessing and Feature Engineering:,16,jamesdellinger,home-credit-putting-all-the-steps-together,jamesdellinger_home-credit-putting-all-the-steps-together_m16
## II. Training & Cross-Validation,17,jamesdellinger,home-credit-putting-all-the-steps-together,jamesdellinger_home-credit-putting-all-the-steps-together_m17
### Displaying Feature Importances,18,jamesdellinger,home-credit-putting-all-the-steps-together,jamesdellinger_home-credit-putting-all-the-steps-together_m18
### Target Encoding Helper Functions for Categorical Features,19,jamesdellinger,home-credit-putting-all-the-steps-together,jamesdellinger_home-credit-putting-all-the-steps-together_m19
"### Train LightGBM Model

My method for training a LightGBM model. It's called below when I run serial CV and when I generate test predictions.",20,jamesdellinger,home-credit-putting-all-the-steps-together,jamesdellinger_home-credit-putting-all-the-steps-together_m20
"### LightGBM Parameters

Commented-out hyperparameter values give an indication of the different values I tried on my journey to ending up at my best-performing combination. 

I first try to pick a combination of 'max_depth'/'num_leaves' that is deep/large enough for the dataset without overfitting. At the same time, I choose the highest possible learning rate (usually 0.2, 0.1, or 0.009) and then observe how the CV score changes as I adjust other hyperparameter values. 

""Highest possible learning rate"" means: a learning rate that gives the model the chance to run for enough boosting rounds so that I can confirm that the model is incrementally learning from round to round. If the learning rate is too high, the model's score won't steadily improve from round to round, and we won't be able to observe how tuning other hyperparameters affects overall performance. If the learning rate is too low, we'll be taking an unnecessarily long time to make these observations.

I like to tune hyperparameters one-by-one, and empirically observe how the CV score changes. Sometimes, but not that often, I will adjust pairs of hyperparameters together if I believe that the two hyperparameters have a unique interrelationship. I'm not a fan of computational tools like GridSearchCV. I find that simple, rote trial and error gives me a far stronger intuition about how the different hyperparameters are responding to my dataset and affecting CV score, in a much shorter amount of time than it takes GridSearchCV to finish churning through all the different hyperparamter combos while running on my laptop's CPU.

Learning the fundamentals of decision trees, LightGBM specifically, and what each of its hyperparameters purports to do helped me to begin to be able to make mental shortcuts of know what hyperparameters to tune, and when and how to tune them. The following two resources helped me immensely:

1. The paper on LightGBM: https://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree.pdf

2. Laurea's site that explains all hyperparameter values for LightGBM/XGBoost: https://sites.google.com/view/lauraepp/parameters

After tuning the various hyperparameters at the highest possible learning rate, such that I can confirm improvements in performance, I then lower my learning rate to the value that will maximize my model's performance. This will be the learning rate I use when training and making predictions.",21,jamesdellinger,home-credit-putting-all-the-steps-together,jamesdellinger_home-credit-putting-all-the-steps-together_m21
"### Perform Cross Validation Using LightGBM's Built-In CV (all folds run in parallel)

The advantages of using LightGBM's built-in CV are that it not only trains all folds in parallel, but that it also keeps track of the *average* ROC AUC score *across all folds* for each boosting round. This allows me to know the exact round when the average ROC AUC score across all folds was at its maximum.

Unfortunately, lightgbm.cv doesn't support the kind of preprocessing of data within its folds that would be necessary to perform target encoding in a way that doesn't lead to data leakage. Nonetheless, with enough tuning of the parameters related to LightGBM's default handling of categorical features (in particular, reducing 'max_cat_threshold' from 32 to 4), I was able to get the CV score of lightgbm.cv to within 0.0002 below the CV score of my serial CV that computed the average best score across all folds.

Due to its much more rapid training time, I found it helpful to use lightgbm.cv while adding/dropping/engineering new features and tuning hyperparameters. By enabling 'verbose_eval', lightgbm.cv gives me the clearest possible picture of how my choice of learning rate is affecting how my model learns overall. This was invaluable in helping me to decide on a good temporary learning rate to use when tuning all the other hyperparameters, as well as finding the optimal learning rate (low enough, but not too low) to use for final training and test prediction generation. 

For this competition, I ultimately chose to use target encoding, which required me to implement standard, serial stratified K-Fold CV to get my final local CV score. I explain more about that decision below. ",22,jamesdellinger,home-credit-putting-all-the-steps-together,jamesdellinger_home-credit-putting-all-the-steps-together_m22
"### Stratified 5-Fold Cross Validation (performed serially, fold-by-fold, using target encoding)

Categorical feature target encoding is performed five times (for each of the five validation folds). This is necessary in order to prevent data leakage.

### Why I decided to use target encoding:
Performing serial cross validation is obviously much slower than using LightGBM's built-in CV, which trains and predicts on all five folds in parallel. However, after making the apples-to-apples comparison of performing serial CV, first using target encoding, and then again using LightGBM's default handling of categorical features, I found that I got a higher (by `0.001`) CV score when using target encoding. This was enough for me to conclude that accepting the limitations of not using lightgbm.cv was worth it as a tradeoff for the higher score I got using target encoding vs. not doing target encoding, all other conditions being held constant.

The single biggest trade-off, of course, is that using serial CV probably does a slightly worse job of approximating the true ideal number of boosting rounds for training my model, in that I can only know the *average round* of best score, as opposed to *the single round* when the *average score across* all 5 folds was best.

### However, knowing what I know now:
As I mentioned in my comments at the beginning of this kernel, it would ultimately turn out that training a predictor using LightGBM's default categorical feature handling, where the number of boosting rounds was determined by the results of running lightgbm.cv, would result in a private LB score only just `0.0001` lower than the private LB score earned by my model that used target encoding and had its number of boosting rounds determined by the results of running serial CV. 

My conclusion is that with the proper hyperparameter tuning, there may not be much advantage to using target encoding with LightGBM, at least not for the Home Credit competition's dataset.",23,jamesdellinger,home-credit-putting-all-the-steps-together,jamesdellinger_home-credit-putting-all-the-steps-together_m23
"### List Features in Order of LightGBM Importance (importance_type='split')

I tried using this to get the top 20/50/100 features of lowest importance, and then remove them from my dataset and see if my local CV score increased. However, doing this never improved my CV score.",24,jamesdellinger,home-credit-putting-all-the-steps-together,jamesdellinger_home-credit-putting-all-the-steps-together_m24
## III. Generating Test Set Predictions,25,jamesdellinger,home-credit-putting-all-the-steps-together,jamesdellinger_home-credit-putting-all-the-steps-together_m25
"<h1 align=""center""> Default Classification </h1>
<img src=""https://storage.googleapis.com/kaggle-organizations/1536/thumbnail.png%3Fr=93"" width=400 height=200>

<h2> Introduction: </h2>
In this project we will analyze what factors affect whether an individual will be able to repay a loan or not. We will explore several statistical techniques to find better ways to to analyze the data and for our model to learn from those statistical techniques in a more effective way. First, we will focus on the Exploratory Data Analysis (EDA) aspect since I want to have a better understanding of what the data is telling us, then we will determine if the dataset is highly imbalanced and we will proceed with several techniques as of how to deal with these types of datasets. One note before we start this project, I will be giving much slower updates than usual because currently I am taking a statistical course so in the future I could come with more approachable techniques so as to how solve and analyze complex datasets. Since this statistics course is taking a bit of a toll of my time, updates for this project will be more slow than usual. Let's start with our analysis! <br> <br>

This project will be focus into three phases:
<ul>
    <li><b>Extensive Exploratory Analysis:</b> There is a vast amount of data so in this phase it will take me some time to dig down into all the important features that I consider to be important. </li>
    <li> <b>Preprocessing the Data: </b> This is the most important aspect of how accurate our models will be, using the right techniques to scale and transform some of the missing values is essential.</li>
    <li><b>Implementation of the Model:</b> Decide which predictive model will work best in this scenario.  </li>
    </ul>
    
  <br>  
 <b> A note from the author: </b> I am a bit busy with my studies but I will have some free time to dedicate to this project however, I want to take my time to provide to the Kaggle community a good quality work so this might take some time. Have a great day everyone and enjoy the parts of this analysis that are already published!

<h2> Oulline: (To be Updated) </h2>
",0,janiobachmann,loan-classification-detecting-credit-defaults,janiobachmann_loan-classification-detecting-credit-defaults_m0
"<h3> Several Distributions: </h3>
In this section we will analyze several distribution to see if those distributions are <b> right-skewed </b>, <b> symetric (normal distribution) </b> or <b> left-skewed </b>. We will use the <b>norm</b> function from scipy to determine whether those distributions can fit our model perfectly. The more normal the distribution the better for our model to handle ouliers or values that are more than two standard deviations away from the mean. So how do we know if the distribution is fitting perfectly our model? If the dots don't fit perfectly the line or if somehow it deviates in such an extreme way we could assume that our model is not fitting perfectly. <b> Is there a way to make the distribution fit more normally? </b> One alternative is to use a natural logarithm in order to reduce the distance for extreme values which will improve how values fit to our model. See how the values deviate less in the second norm subplots.",1,janiobachmann,loan-classification-detecting-credit-defaults,janiobachmann_loan-classification-detecting-credit-defaults_m1
"<h3> Gender Analysis: </h3>

<h4> Summary: </h4>
<ul>
<li> <b>Housing Type: </b> Most of the housing type by both genders are house/apartment.</li>
<li><b> Income Type: </b> The top income types for both genders are currently working and commercial associate. </li>
<li><b>Correlation between income and credit: </b> There is a slight positive correlation between between income and credit (the higher the income, the higher the loan).</li>
<li><b> Income distributions: </b> Males tend to have a slightly higher income distribution compared to female. </li>
</ul>",2,janiobachmann,loan-classification-detecting-credit-defaults,janiobachmann_loan-classification-detecting-credit-defaults_m2
"### Continue Here forhe Gender Analysis:
<ul>
<li> Look for more insightfu variables that we can use mainly in the y and x variables to evaluate the discrepancies between Gender and determine whether there is more risk for females to default on loans. </li>
<li> Find the distribution of genders and see how many of each gender defaulted on a loan (Only available with the training dataset. </li>
</ul>",3,janiobachmann,loan-classification-detecting-credit-defaults,janiobachmann_loan-classification-detecting-credit-defaults_m3
"### Material Analysis: (Next Phase of the Project)
---> Description Coming Soon",4,janiobachmann,loan-classification-detecting-credit-defaults,janiobachmann_loan-classification-detecting-credit-defaults_m4
"['![app](https://i.imgur.com/jKtMJgg.jpg)\n', '\n', 'The missing data within a dataset can often provide insight into the issue at hand. We can look at the structure of the missing values - which features are affected, which records are affected, and differences between groups. We can also use the missing data as a feature itself by counting missing values or transforming them. In this report I explore some patterns and suggest one way to improve your predictive.\n', '\n', '## Patterns\n', ""First let's look at the overall pattern of missing data. The [missingno](https://github.com/ResidentMario/missingno) package by [Aleksey Bilogur](https://www.kaggle.com/residentmario) is the perfect tool here. Looking at a sample of data for all columns we see a group of columns where the missing values appear correlated.\n"", '\n']",0,jpmiller,patterns-of-missing-data,jpmiller_patterns-of-missing-data_m0
"['Zooming in on the middle columns we see they deal mostly with information about the building where the client lives. It appears there are many applicants who leave blank the information for their housing. We can think about why that might be the case or how it might inform our model.\n', '\n', ""I'll sort the data this time to better see the proportions.""]",1,jpmiller,patterns-of-missing-data,jpmiller_patterns-of-missing-data_m1
['The dendrogram view shows how missing values are related across columns by using hierarchical clustering. Pretty cool! '],2,jpmiller,patterns-of-missing-data,jpmiller_patterns-of-missing-data_m2
"['## Comparison of Completed Applications\n', '\n', ""With an idea of the overall picture, let's now focus on the large group of applications with missing house data. Is there a difference in mean default rates between those with house information and those without?""]",3,jpmiller,patterns-of-missing-data,jpmiller_patterns-of-missing-data_m3
"[""There appears to be a difference. Viewed one way, borrowers with incomplete applications are ~30% more likely to default. You may want to include this information in your model. I found it helpful to add a binary feature called 'no_housing_info'. The application is flagged if it has more than 45 blanks. You could also create three classes to account for the applications with some housing data (which may denote apartment dwellers). \n"", '\n', '\n', '## Statistical Significance\n', ""To be thorough, I looked at statistical significance of the difference in default rates between groups. I used a [G-test](https://en.wikipedia.org/wiki/G-test) which is similar to Pearson's chi-squared test. Either one should work in this case, but I generally prefer the G-test.""]",4,jpmiller,patterns-of-missing-data,jpmiller_patterns-of-missing-data_m4
"['""If p is low, the null must go."" The p-value here is 1e-114 which is pretty much 0. So we can reject the null hypothesis with only a small probability of [Type 1 error](https://en.wikipedia.org/wiki/Type_I_and_type_II_errors). In other words, the difference in default ratios between the two groups is not due to random chance. \n', '\n', 'Good luck!']",5,jpmiller,patterns-of-missing-data,jpmiller_patterns-of-missing-data_m5
"<h3>Feature importance evaluation</h3>

In this notebook we will be using the [Shap library](https://github.com/slundberg/shap) to evaluate the importance of features from [LightGBM with selected features](https://www.kaggle.com/ogrellier/lighgbm-with-selected-features/code) on the <b>full dataset</b>. LightGBM standard feature importance will also be used to comparison.

Shap (Shapley Additive explanations) combine [game theory](https://en.wikipedia.org/wiki/Game_theory) with local explanations to create a consistent indicator for feature importance in complex models. Shap value measures how much each feature contributes, either positively or negatively, to a default. For more information:

* [Medium article](https://medium.com/civis-analytics/demystifying-black-box-models-with-shap-value-analysis-3e20b536fc80) by Peter Cooman 
* [Original paper](http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf)
* [Detailed paper on Tree ensembles](https://arxiv.org/pdf/1802.03888.pdf)",0,jsaguiar,feature-analysis-using-shap-full-data,jsaguiar_feature-analysis-using-shap-full-data_m0
"<b>LightGBM Model</b>

The original script used Sklearn to train the model in five folds and then averaged the predictions and feature importance. This notebook will use the lgb.cv method to perform cross-validation in nfolds and then train a model on full data using the optimal iteration (boost round) found. We will be using the same parameters and features.",1,jsaguiar,feature-analysis-using-shap-full-data,jsaguiar_feature-analysis-using-shap-full-data_m1
"<b>Running the model</b>

Just removed the timers and added a function to reduce dataframe's memory usage.",2,jsaguiar,feature-analysis-using-shap-full-data,jsaguiar_feature-analysis-using-shap-full-data_m2
"<h3>Shap analysis</h3>

Now we have all we need to start evaluating each feature performance. The first graph shows the 20 most important features according to the sum of shap magnitudes over all samples. The color represents the feature value (red high, blue low) and the x-axis the impact on model output.",3,jsaguiar,feature-analysis-using-shap-full-data,jsaguiar_feature-analysis-using-shap-full-data_m3
Let's plot the feature importance (by gain) to compare:,4,jsaguiar,feature-analysis-using-shap-full-data,jsaguiar_feature-analysis-using-shap-full-data_m4
"<b>Individual data points</b>

It's also possible to visualize individual data points and how features contribute to the model output:",5,jsaguiar,feature-analysis-using-shap-full-data,jsaguiar_feature-analysis-using-shap-full-data_m5
"<b>Dependence plot</b>

We can use the dependence_plot to compare the feature value (x axis) and the shap values for that feature (y axis). The coloring represents the value for an automatically selected feature so we can understand the iteraction effect. Let's plot some of the most important features:",6,jsaguiar,feature-analysis-using-shap-full-data,jsaguiar_feature-analysis-using-shap-full-data_m6
"<b>Bureau and bureau_balance</b>

Let's plot the 20 most important features from bureau.csv and bureau_balance.csv:",7,jsaguiar,feature-analysis-using-shap-full-data,jsaguiar_feature-analysis-using-shap-full-data_m7
"<b>Previous Applications</b>

Most important features from previous_application.csv",8,jsaguiar,feature-analysis-using-shap-full-data,jsaguiar_feature-analysis-using-shap-full-data_m8
"<b>Previous applications: balances and payments</b>

Most important features from credit_card_balance, POS_CASH_balance and installments_payments.",9,jsaguiar,feature-analysis-using-shap-full-data,jsaguiar_feature-analysis-using-shap-full-data_m9
"<h3>Shap values and feature importance for all features</h3>

To conclude, we will sum the shap magnitude for each feature and print it with the feature importance by gain and by number of splits.",10,jsaguiar,feature-analysis-using-shap-full-data,jsaguiar_feature-analysis-using-shap-full-data_m10
"Hi there, this notebook aims to provide visualization about different features distribution and observing abnormal values in the dataset. 

# Table of Contents
1. Target Distribution
2. application_{train|test}.csv
    * 2.1 NaN Count
    * 2.2 [Feature Generation] Adding IS_NAN features for each column.
    * 2.3 The importance of the missing values
    * 2.4. Features' Distribution
    * 2.5 Are the distributions making sense?
3. bureau.csv
4. ...pending

# 1. Target Distribution


",0,kingychiu,home-credit-eda-distributions-and-outliers,kingychiu_home-credit-eda-distributions-and-outliers_m0
"The target distribution is imbalanced, which indicates the company has already done a great job (some direct feature, like external scoring, in the dataset might not be that helpful)",1,kingychiu,home-credit-eda-distributions-and-outliers,kingychiu_home-credit-eda-distributions-and-outliers_m1
"# 2. application_{train|test}.csv> 
##  2.1 NaN Count
There are lots of NaN values in the dataset (also as discussed in the forum, the organizer also filled in some missing data with magic values). Need to handle them carefully.",2,kingychiu,home-credit-eda-distributions-and-outliers,kingychiu_home-credit-eda-distributions-and-outliers_m2
## 2.2 [Feature Generation] Adding IS_NAN features for each column.,3,kingychiu,home-credit-eda-distributions-and-outliers,kingychiu_home-credit-eda-distributions-and-outliers_m3
## 2.3 The importance of the missing values,4,kingychiu,home-credit-eda-distributions-and-outliers,kingychiu_home-credit-eda-distributions-and-outliers_m4
## 2.4 Features' Distribution,5,kingychiu,home-credit-eda-distributions-and-outliers,kingychiu_home-credit-eda-distributions-and-outliers_m5
"## 2.5 Are the distributions making sense?
### 2.5.1 DAYS_EMPLOYED
How many days before the application the person started current employment

Discussed in the forum, For DAYS_xxx columns, **365243 means missing value**.
* The original distribution:",6,kingychiu,home-credit-eda-distributions-and-outliers,kingychiu_home-credit-eda-distributions-and-outliers_m6
"If the magic number is removed, the distribution:",7,kingychiu,home-credit-eda-distributions-and-outliers,kingychiu_home-credit-eda-distributions-and-outliers_m7
"### 2.5.2 AMT_INCOME_TOTAL
Income of the client.

There are a huge number at the right of the plot (1.170000e+08):",8,kingychiu,home-credit-eda-distributions-and-outliers,kingychiu_home-credit-eda-distributions-and-outliers_m8
The plot makes more sense if we remove that data point:,9,kingychiu,home-credit-eda-distributions-and-outliers,kingychiu_home-credit-eda-distributions-and-outliers_m9
"### 2.5.3 AMT_REQ_CREDIT_BUREAU_QRT
Number of enquiries to Credit Bureau about the client 3 month before application (excluding one month before application)

Why were there 261 enquireies about a application within 2 months? 4 calls a day?",10,kingychiu,home-credit-eda-distributions-and-outliers,kingychiu_home-credit-eda-distributions-and-outliers_m10
Removing that data point:,11,kingychiu,home-credit-eda-distributions-and-outliers,kingychiu_home-credit-eda-distributions-and-outliers_m11
"### 2.5.4 Normalized information about building where the client lives

Why those Normalized information got many 0s and 1s? such as this one:",12,kingychiu,home-credit-eda-distributions-and-outliers,kingychiu_home-credit-eda-distributions-and-outliers_m12
"### 2.5.5 OBS_30_CNT_SOCIAL_CIRCLE
How many observation of client's social surroundings with observable 30 DPD (days past due) default

Is it normal to have over 350 social surroundings overations?",13,kingychiu,home-credit-eda-distributions-and-outliers,kingychiu_home-credit-eda-distributions-and-outliers_m13
"# 3 bureau.csv

Pending",14,kingychiu,home-credit-eda-distributions-and-outliers,kingychiu_home-credit-eda-distributions-and-outliers_m14
"Thanks for reading, this is my first attempt to try making a relatively complete EDA & visualization. Please let me know if you have any suggestions, I am desired to learn new things.",15,kingychiu,home-credit-eda-distributions-and-outliers,kingychiu_home-credit-eda-distributions-and-outliers_m15
"# Preprocessing
## Solution 3",0,kkaczmarek,features-application-data-open-solution,kkaczmarek_features-application-data-open-solution_m0
"[Martin Kotek (Competition Host): ""Value 365243 denotes infinity in DAYS variables in the datasets, therefore you can consider them NA values. Also XNA/XAP denote NA values.""](https://www.kaggle.com/c/home-credit-default-risk/discussion/57247)",1,kkaczmarek,features-application-data-open-solution,kkaczmarek_features-application-data-open-solution_m1
"# Feature Engineering
## Solution 3
### Hand crafted features",2,kkaczmarek,features-application-data-open-solution,kkaczmarek_features-application-data-open-solution_m2
### Aggregation features,3,kkaczmarek,features-application-data-open-solution,kkaczmarek_features-application-data-open-solution_m3
"## Solution 4 
### Hand crafted features
* diff features",4,kkaczmarek,features-application-data-open-solution,kkaczmarek_features-application-data-open-solution_m4
* unemployed,5,kkaczmarek,features-application-data-open-solution,kkaczmarek_features-application-data-open-solution_m5
* age binns,6,kkaczmarek,features-application-data-open-solution,kkaczmarek_features-application-data-open-solution_m6
"# Feature Engineering
## Solution 3",0,kkaczmarek,features-installments-data-open-solution,kkaczmarek_features-installments-data-open-solution_m0
## Aggregations,1,kkaczmarek,features-installments-data-open-solution,kkaczmarek_features-installments-data-open-solution_m1
# Solution 4,2,kkaczmarek,features-installments-data-open-solution,kkaczmarek_features-installments-data-open-solution_m2
## per id aggregations,3,kkaczmarek,features-installments-data-open-solution,kkaczmarek_features-installments-data-open-solution_m3
## Per id k last installment information,4,kkaczmarek,features-installments-data-open-solution,kkaczmarek_features-installments-data-open-solution_m4
## per id dynamic ,5,kkaczmarek,features-installments-data-open-solution,kkaczmarek_features-installments-data-open-solution_m5
"### helper functions
These functions are just helpers :)",0,kkaczmarek,features-pos-cash-balance-open-solution,kkaczmarek_features-pos-cash-balance-open-solution_m0
### Aggregations,1,kkaczmarek,features-pos-cash-balance-open-solution,kkaczmarek_features-pos-cash-balance-open-solution_m1
"## Solution 4
### Hand crafted features",2,kkaczmarek,features-pos-cash-balance-open-solution,kkaczmarek_features-pos-cash-balance-open-solution_m2
"## Solution 5

### Hand crafted features",3,kkaczmarek,features-pos-cash-balance-open-solution,kkaczmarek_features-pos-cash-balance-open-solution_m3
### Last loan features,4,kkaczmarek,features-pos-cash-balance-open-solution,kkaczmarek_features-pos-cash-balance-open-solution_m4
### Trend features,5,kkaczmarek,features-pos-cash-balance-open-solution,kkaczmarek_features-pos-cash-balance-open-solution_m5
"# Feature Engineering
## Solution 3",0,kkaczmarek,features-previous-application-data-open-solution,kkaczmarek_features-previous-application-data-open-solution_m0
### Aggregations,1,kkaczmarek,features-previous-application-data-open-solution,kkaczmarek_features-previous-application-data-open-solution_m1
"## Solution 4
### Hand crafted features",2,kkaczmarek,features-previous-application-data-open-solution,kkaczmarek_features-previous-application-data-open-solution_m2
Bayesian Optimzation code copied here for display purposes,0,ogrellier,home-credit-hyperopt-optimization,ogrellier_home-credit-hyperopt-optimization_m0
CSV files readers and enhancers,1,ogrellier,home-credit-hyperopt-optimization,ogrellier_home-credit-hyperopt-optimization_m1
Hyperopt helper object,2,ogrellier,home-credit-hyperopt-optimization,ogrellier_home-credit-hyperopt-optimization_m2
"Read files, create full dataset and optimize",3,ogrellier,home-credit-hyperopt-optimization,ogrellier_home-credit-hyperopt-optimization_m3
,0,osciiart,homecreditrisk-extensive-eda-baseline-model-jp,osciiart_homecreditrisk-extensive-eda-baseline-model-jp_m0
"this is folked kernel with japanese comment for education

# ホーム・クレジット社 債務不履行リスク - データ探索 + 基本モデル

融資履歴が少なかったり無かったりするために、多くの人々が融資を受けるのに苦労しています。  
そして、残念なことに、このような人々は、怪しげな金貸し屋によってしばしばカモにされます。  
ホームクレジット社は、ポジティブで安全な借入経験を提供することによって、  
銀行口座を持たない人々のためのファイナンシャル・インクルージョン (貧困層に正規の金融取引ができるように改善する解決策を提供すること) を広めるために努力しています。  
この金銭的に不利な人々がポジティブな借入経験を持つことを確実にするために、ホームクレジットは電話や取引情報を含むさまざまな代替データを利用しています。
そして顧客の返済能力を予測しています。

ホームクレジット社は現在、これらの予測を行うためにさまざまな統計的方法や機械学習方法を使用していますが、  
ホームクレジット社は社の持つデータの潜在能力を最大限に発揮するためにKagglersに挑戦を挑みました。  
このコンペにより、返済能力のある顧客が無事借入できること、そして顧客がより確実に返済完了できるような借入額、完済日、返済スケジュールを提供することが可能となるでしょう。

これはホーム・クレジット社債務不履行データについてのデータ探索と基本モデルについての簡単なノートブックです。  
**Contents**   
1. Dataset Preparation    
2. Exploration - Applications Train  
&nbsp;&nbsp;&nbsp;&nbsp; 2.1 Snapshot - Application Train    
&nbsp;&nbsp;&nbsp;&nbsp; 2.2 Distribution of Target Variable    
&nbsp;&nbsp;&nbsp;&nbsp; 2.3 Gender and Contract Type Distribution and Target Variable    
&nbsp;&nbsp;&nbsp;&nbsp; 2.4 Own Realty and Own Car  - Distribution with Target Variable  
&nbsp;&nbsp;&nbsp;&nbsp; 2.5 Suit Type and Income Type    
&nbsp;&nbsp;&nbsp;&nbsp; 2.6 Family Statue and Housing Type   
&nbsp;&nbsp;&nbsp;&nbsp; 2.7 Education Type and Income Type   
&nbsp;&nbsp;&nbsp;&nbsp; 2.8.1 Organization Type and Occupation Type   
&nbsp;&nbsp;&nbsp;&nbsp; 2.8.2 Walls Material, Foundation and House Type   
&nbsp;&nbsp;&nbsp;&nbsp; 2.9 Amount Credit Distribution    
&nbsp;&nbsp;&nbsp;&nbsp; 2.10 Amount Annuity Distribution  
&nbsp;&nbsp;&nbsp;&nbsp; 2.11 Amount Goods Price   
&nbsp;&nbsp;&nbsp;&nbsp; 2.12 Amount Region Population Relative    
&nbsp;&nbsp;&nbsp;&nbsp; 2.13 Days Birth   
&nbsp;&nbsp;&nbsp;&nbsp; 2.14 Days Employed    
&nbsp;&nbsp;&nbsp;&nbsp; 2.15 Num Days Registration  
&nbsp;&nbsp;&nbsp;&nbsp; 2.15 Count of Family Members  
3. Exploration - Bureau Data  
&nbsp;&nbsp;&nbsp;&nbsp; 3.1 Snapshot - Bureau Data    
4. Exploration - Bureau Balance Data  
&nbsp;&nbsp;&nbsp;&nbsp; 4.1 Snapshot - Bureau Balance Data     
5. Exploration - Credit Card Balance Data   
&nbsp;&nbsp;&nbsp;&nbsp; 5.1 Snapshot - Credit Card Balance Data   
6. Exploration - POS Cash Balance Data   
&nbsp;&nbsp;&nbsp;&nbsp; 6.1 Snapshot - POS Cash Balance Data   
7. Exploration - Previous Application Data   
&nbsp;&nbsp;&nbsp;&nbsp; 7.1 Snapshot - Previous Application Data  
&nbsp;&nbsp;&nbsp;&nbsp; 7.2 Contract Status Distribution - Previous Applications  
&nbsp;&nbsp;&nbsp;&nbsp; 7.3 Suite Type Distribution - Previous Application    
&nbsp;&nbsp;&nbsp;&nbsp; 7.4 Client Type Distribution  - Previous Application    
&nbsp;&nbsp;&nbsp;&nbsp; 7.5 Channel Type Distribution - Previous Applications  
7. Exploration - Installation Payments  
&nbsp;&nbsp;&nbsp;&nbsp; 8.1 Snapshot of Installation Payments  
9. Baseline Model  
&nbsp;&nbsp;&nbsp;&nbsp; 9.1 Dataset Preparation  
&nbsp;&nbsp;&nbsp;&nbsp; 9.2 Label Encoding     
&nbsp;&nbsp;&nbsp;&nbsp; 9.3 Validation Sets Preparation    
&nbsp;&nbsp;&nbsp;&nbsp; 9.4 Model Fitting    
&nbsp;&nbsp;&nbsp;&nbsp; 9.5 Feature Importance    
&nbsp;&nbsp;&nbsp;&nbsp; 9.6 Prediction 



## 1. Dataset Preparation ",1,osciiart,homecreditrisk-extensive-eda-baseline-model-jp,osciiart_homecreditrisk-extensive-eda-baseline-model-jp_m1
"## 2. データ探索: Application (ローン申込書)

## 2.1 Application Train の概観

Application データは全ローン申込書の統計情報からなり、各行が1つのローンを表す。",2,osciiart,homecreditrisk-extensive-eda-baseline-model-jp,osciiart_homecreditrisk-extensive-eda-baseline-model-jp_m2
"> 307,511件のローンのデータがあり、列数は122です。

## 2.2 目的変数の分布
目的変数
- 1: 支払が困難なクライアント = クライアントが最初のY回の分割払いの内に少なくとも一回でX日以上延滞していた場合
- 0: それ以外の場合",3,osciiart,homecreditrisk-extensive-eda-baseline-model-jp,osciiart_homecreditrisk-extensive-eda-baseline-model-jp_m3
"> - 目的変数は約282k (85%) が 0 で、わずか24kが 1 です。

## 2.3 どの性別、どの契約タイプがローンを申し込んでいるか
- 性別: クライアントの性別  
- 契約タイプ: ローンがキャッシュかリボ払いか  

### 2.3.1 性別・契約タイプの分布",4,osciiart,homecreditrisk-extensive-eda-baseline-model-jp,osciiart_homecreditrisk-extensive-eda-baseline-model-jp_m4
"> 性別については、女性が多く (202448) 男性は少ない (105059)。  
> 契約タイプについてはキャッシュが主でリボ払いは約29kとかなり少ない。

### 2.3.2 性別・契約タイプと目的変数との関係",5,osciiart,homecreditrisk-extensive-eda-baseline-model-jp,osciiart_homecreditrisk-extensive-eda-baseline-model-jp_m5
## 2.4. 土地所有・車所有,6,osciiart,homecreditrisk-extensive-eda-baseline-model-jp,osciiart_homecreditrisk-extensive-eda-baseline-model-jp_m6
"## 2.5 同伴者・収入形態
- 同伴者 (NAME_TYPE_SUITE): 借入申請書提出時に同伴した人物

### 2.5.1 同伴者・収入形態の値",7,osciiart,homecreditrisk-extensive-eda-baseline-model-jp,osciiart_homecreditrisk-extensive-eda-baseline-model-jp_m7
"> 同伴者のトップ3は同伴者なし (250k)、家族、夫婦である。
> 収入形態は8タイプがありトップは:  
    - Working Class労働階級 (158K)
    - Pensiner 年金受給者 (55K)同伴者

### 2.5.2 同伴者・収入形態と目的変数との関係",8,osciiart,homecreditrisk-extensive-eda-baseline-model-jp,osciiart_homecreditrisk-extensive-eda-baseline-model-jp_m8
"## 2.6. 婚姻状況・住居

### 2.6.1 婚姻状況・住居の値",9,osciiart,homecreditrisk-extensive-eda-baseline-model-jp,osciiart_homecreditrisk-extensive-eda-baseline-model-jp_m9
"> - 既婚の顧客が最も多く (約196k) 独身がそれに続く。
> - 住居は多くが ""一軒家/アパート"" で85%を占め、両親と同居、公営住宅が続く。

### 2.6.2 婚姻状況・住居と目的変数との関係",10,osciiart,homecreditrisk-extensive-eda-baseline-model-jp,osciiart_homecreditrisk-extensive-eda-baseline-model-jp_m10
"## 2.7. 教育

### 2.7.1 教育の分布",11,osciiart,homecreditrisk-extensive-eda-baseline-model-jp,osciiart_homecreditrisk-extensive-eda-baseline-model-jp_m11
"> 多くの割合が中等教育に占められ (218k)、高等教育 (75k) がそれに続く。

### 2.7.2 教育と目的変数との関係",12,osciiart,homecreditrisk-extensive-eda-baseline-model-jp,osciiart_homecreditrisk-extensive-eda-baseline-model-jp_m12
"## 2.8. 組織・業種
-  組織: クライアントが働いている組織
-  業種: クライアントの業種

### 2.8.1 組織・業種の分布",13,osciiart,homecreditrisk-extensive-eda-baseline-model-jp,osciiart_homecreditrisk-extensive-eda-baseline-model-jp_m13
"> 申請者の中で多い業種は労働者 (55k)、販売員 (32k)、コアスタッフ (28k)  
> 多い組織は第3種法人が最多で67kを占める

### 2.8.2 組織・業種と目的変数との関係",14,osciiart,homecreditrisk-extensive-eda-baseline-model-jp,osciiart_homecreditrisk-extensive-eda-baseline-model-jp_m14
"### 2.8.3. 壁の種類・基礎の種類・家屋の種類の分布
変数の意味がよくわかりません…",15,osciiart,homecreditrisk-extensive-eda-baseline-model-jp,osciiart_homecreditrisk-extensive-eda-baseline-model-jp_m15
"> - 平屋が150kでほとんどを占め、特殊家屋、テラスハウスは1500以下である。
> - 壁はパネル、石・レンガがほぼ同数で120k近くを占める。

### 2.8.4 壁の種類・基礎の種類・家屋の種類と目的変数との関係",16,osciiart,homecreditrisk-extensive-eda-baseline-model-jp,osciiart_homecreditrisk-extensive-eda-baseline-model-jp_m16
## 2.9. 借金額の分布,17,osciiart,homecreditrisk-extensive-eda-baseline-model-jp,osciiart_homecreditrisk-extensive-eda-baseline-model-jp_m17
"## 2.10 年金の分布
- 年金: ローン年金 (って何?)",18,osciiart,homecreditrisk-extensive-eda-baseline-model-jp,osciiart_homecreditrisk-extensive-eda-baseline-model-jp_m18
"## 2.11 商品価格の分布
- 商品価格: ローンを組む目的である商品の価格",19,osciiart,homecreditrisk-extensive-eda-baseline-model-jp,osciiart_homecreditrisk-extensive-eda-baseline-model-jp_m19
"## 2.12 相対地域人口分布の分布
- 相対地域人口: 正規化されたクライアントが住んでいる地域の人口 (クライアントがより人工の多い地域に住んでいることを表す) ",20,osciiart,homecreditrisk-extensive-eda-baseline-model-jp,osciiart_homecreditrisk-extensive-eda-baseline-model-jp_m20
"## 2.13 年齢の分布
- 年齢: クライアントが借入申請日の何日前に生まれたか",21,osciiart,homecreditrisk-extensive-eda-baseline-model-jp,osciiart_homecreditrisk-extensive-eda-baseline-model-jp_m21
"## 2.14 雇用日数の分布
- 雇用日数: クライアントが借入申請日の何日前から現在の仕事を始めたか",22,osciiart,homecreditrisk-extensive-eda-baseline-model-jp,osciiart_homecreditrisk-extensive-eda-baseline-model-jp_m22
"## 2.15 登録日の分布
- 登録日: クライアントが借入申請日の何日前に登録情報を更新したか",23,osciiart,homecreditrisk-extensive-eda-baseline-model-jp,osciiart_homecreditrisk-extensive-eda-baseline-model-jp_m23
## 2.16 家族人数,24,osciiart,homecreditrisk-extensive-eda-baseline-model-jp,osciiart_homecreditrisk-extensive-eda-baseline-model-jp_m24
"## 3. 信用情報機関データのデータ探索

信用情報機関によって報告されている顧客の過去の他の金融機関で借入履歴。  
顧客の借入申込日以前の借入回数と同じ行数の借入情報が含まれる。  

## 3.1 信用情報機関データの概観",25,osciiart,homecreditrisk-extensive-eda-baseline-model-jp,osciiart_homecreditrisk-extensive-eda-baseline-model-jp_m25
"## 4. 信用情報機関残高のデータ探索

信用情報機関の過去の借入の月間残高。   
このテーブルには、過去の借入についての各月の残高が1行ずつ記録されています。  
テーブルの各列には、ある借入のx月 (借入申請からxヶ月前) の債務状況の情報が含まれます。  

## 4.1 信用情報機関残高データの概観",26,osciiart,homecreditrisk-extensive-eda-baseline-model-jp,osciiart_homecreditrisk-extensive-eda-baseline-model-jp_m26
"## 5. クレジットカード残高のデータ探索

借入申請者の持つホーム・クレジット社製クレジットカードの各月の残高情報。  
このテーブルには、借入申請者の持つホーム・クレジット社製クレジットカード (消費者金融・キャッシュローン) の各月の残高が1行ずつ記録されています。  
テーブルの各列には、あるクレジットカードのx月 (借入申請からxヶ月前) の債務状況の情報が含まれます。 

## 5.1 クレジットカード残高データの概観",27,osciiart,homecreditrisk-extensive-eda-baseline-model-jp,osciiart_homecreditrisk-extensive-eda-baseline-model-jp_m27
"## 6. POSキャッシュ残高のデータ探索

借入申請者の持つホーム・クレジットに関する過去のPOSとキャッシュローンの各月の残高情報。  
テーブルの各列には、あるローンのx月 (借入申請からxヶ月前) の債務状況の情報が含まれます。 
このテーブルには、ローン の各月の残高が1行ずつ記録されています。  
(訳注: よくわかりませんでした)

## 6.1 POSキャッシュ残高データの概観",28,osciiart,homecreditrisk-extensive-eda-baseline-model-jp,osciiart_homecreditrisk-extensive-eda-baseline-model-jp_m28
"## 7. 過去の借入申請書のデータ探索

## 7.1  過去の借入申請書データの概観",29,osciiart,homecreditrisk-extensive-eda-baseline-model-jp,osciiart_homecreditrisk-extensive-eda-baseline-model-jp_m29
"## 7.2 過去の借入申請書の契約状況の分布
- 契約状況:  受理、拒否...",30,osciiart,homecreditrisk-extensive-eda-baseline-model-jp,osciiart_homecreditrisk-extensive-eda-baseline-model-jp_m30
">-  多くの人が過去に申請が受理されている (62%)。一方で19%がキャンセル、17%が拒否となっている。

## 7.3 過去の借入申請書の同伴者の分布",31,osciiart,homecreditrisk-extensive-eda-baseline-model-jp,osciiart_homecreditrisk-extensive-eda-baseline-model-jp_m31
">- 過去の申請書の同伴者の多くが同伴者なしであり (60%)、家族がそれに続く (25%)。

## 7.4 過去の借入申請書の顧客タイプ",32,osciiart,homecreditrisk-extensive-eda-baseline-model-jp,osciiart_homecreditrisk-extensive-eda-baseline-model-jp_m32
">- 過去の借入申請者の74%がリピーターで18%が新規、8%が再登録? (refreshed)である。

## 7.5 チャネルタイプ
- チャネルタイプ: どの方法で借入申請書を受け取ったか",33,osciiart,homecreditrisk-extensive-eda-baseline-model-jp,osciiart_homecreditrisk-extensive-eda-baseline-model-jp_m33
"## 8. 分割支払のデータ探索
## 8.1 分割支払データの概観",34,osciiart,homecreditrisk-extensive-eda-baseline-model-jp,osciiart_homecreditrisk-extensive-eda-baseline-model-jp_m34
"## 9. ベースライン・モデル

### 9.1 前処理",35,osciiart,homecreditrisk-extensive-eda-baseline-model-jp,osciiart_homecreditrisk-extensive-eda-baseline-model-jp_m35
"### 9.2 カテゴリ変数の処理

より良い処理をしたければOliverの素晴らしいkernelを見に行ってください: https://www.kaggle.com/ogrellier/good-fun-with-ligthgbm ",36,osciiart,homecreditrisk-extensive-eda-baseline-model-jp,osciiart_homecreditrisk-extensive-eda-baseline-model-jp_m36
### 9.3 データセットを一箇所にまとめる,37,osciiart,homecreditrisk-extensive-eda-baseline-model-jp,osciiart_homecreditrisk-extensive-eda-baseline-model-jp_m37
### 9.4 validationデータの作成,38,osciiart,homecreditrisk-extensive-eda-baseline-model-jp,osciiart_homecreditrisk-extensive-eda-baseline-model-jp_m38
### 9.5 モデル (Light GBM)の学習,39,osciiart,homecreditrisk-extensive-eda-baseline-model-jp,osciiart_homecreditrisk-extensive-eda-baseline-model-jp_m39
### 9.6 特徴量の重要度,40,osciiart,homecreditrisk-extensive-eda-baseline-model-jp,osciiart_homecreditrisk-extensive-eda-baseline-model-jp_m40
### 9.7 推定,41,osciiart,homecreditrisk-extensive-eda-baseline-model-jp,osciiart_homecreditrisk-extensive-eda-baseline-model-jp_m41
"[""There are many different method's to select the important features from a dataset. In this notebook I will show a quick way to select important features with the use of Boruta.\n"", '\n', 'Boruta tries to find all relevant features that carry information to make an accurate classification. You can read more about Boruta [here](http://danielhomola.com/2015/05/08/borutapy-an-all-relevant-feature-selection-method/)\n', '\n', ""Let's start by doing all necessary imports.""]",0,rsmits,feature-selection-with-boruta,rsmits_feature-selection-with-boruta_m0
"[""Next we load only the 'application_train' data as this is to demonstrate Boruta only. ""]",1,rsmits,feature-selection-with-boruta,rsmits_feature-selection-with-boruta_m1
['All categorical values will be one-hot encoded.'],2,rsmits,feature-selection-with-boruta,rsmits_feature-selection-with-boruta_m2
['Get all feature names from the dataset'],3,rsmits,feature-selection-with-boruta,rsmits_feature-selection-with-boruta_m3
['Replace all missing values with the Mean.'],4,rsmits,feature-selection-with-boruta,rsmits_feature-selection-with-boruta_m4
['Get the final dataset *X* and labels *Y*'],5,rsmits,feature-selection-with-boruta,rsmits_feature-selection-with-boruta_m5
['Next we setup the *RandomForrestClassifier* as the estimator to use for Boruta. The *max_depth* of the tree is advised on the Boruta Github page to be between 3 to 7.'],6,rsmits,feature-selection-with-boruta,rsmits_feature-selection-with-boruta_m6
"[""Next we setup Boruta. It uses the *scikit-learn* interface as much as possible so we can use *fit(X, y), transform(X), fit_transform(X, y)*. I'll let it run for a maximum of *max_iter = 50* iterations. With *perc = 90* a threshold is specified. The lower the threshold the more features will be selected. I usually use a percentage between 80 and 90. ""]",7,rsmits,feature-selection-with-boruta,rsmits_feature-selection-with-boruta_m7
['After Boruta has run we can transform our dataset.'],8,rsmits,feature-selection-with-boruta,rsmits_feature-selection-with-boruta_m8
['And we create a list of the feature names if we would like to use them at a later stage.'],9,rsmits,feature-selection-with-boruta,rsmits_feature-selection-with-boruta_m9
"['So I hope you enjoyed my very first Kaggle Kernel :-)\n', 'Let me know if you have any feedback or suggestions.']",10,rsmits,feature-selection-with-boruta,rsmits_feature-selection-with-boruta_m10
,0,scirpus,pure-gp-with-logloss,scirpus_pure-gp-with-logloss_m0
,0,scirpus,pure-gp-with-mean-squared-error,scirpus_pure-gp-with-mean-squared-error_m0
[' # Exploratory analysis on credit risk analysis datasets'],0,shailaja4247,tackle-any-credit-risk-analysis-problem-homecredit,shailaja4247_tackle-any-credit-risk-analysis-problem-homecredit_m0
"['In the below notebook you will learn how to do exploratory analysis for any credit risk analysis problem. The dataset that we have taken is from the famous competition problem of Home Credit.\n', '\n', '**About Home Credit**\n', 'Home Credit strives to broaden financial inclusion for the unbanked population by providing a positive and safe borrowing experience. They wish to find out which customer is likely to default so that they can accordingly decide the customers they wish to lend to']",1,shailaja4247,tackle-any-credit-risk-analysis-problem-homecredit,shailaja4247_tackle-any-credit-risk-analysis-problem-homecredit_m1
"[""- <a href='#1'>1. Importing necessary libraries and packages and reading files</a>  \n"", ""- <a href='#2'>2. Handling non-numerical variables</a>\n"", ""- <a href='#3'>3. Aligning Training and Testing Data</a>\n"", ""- <a href='#4'> 4. Handling missing values (using Iterative Imputer)</a>\n"", ""- <a href='#5'>5. Outlier Detection (using Isolation Forest)</a>\n"", ""  - <a href='#5-1'>5.1 Anomaly detection</a>\n"", ""- <a href='#6'>6. Missing data in application_train</a>\n"", ""- <a href='#7'>7. Duplicate data in application_train</a>\n"", ""- <a href='#8'>8. Checking for data imbalance</a>\n"", ""- <a href='#9'>9. Exploratory Data Analysis for application_train by visualisation</a>\n"", ""   - <a href='#9.1'>9.1. Distribution of income</a>\n"", ""   - <a href='#9.2'>9.2. Distribution of credit</a>\n"", ""   - <a href='#9.3'>9.3. Distribution of loan types</a>\n"", ""   - <a href='#9.4'>9.4. Distribution of NAME_INCOME_TYPE</a>\n"", ""   - <a href='#9.5'>9.5. Distribution of NAME_TYPE_SUITE</a>\n"", ""   - <a href='#9.6'>9.6. Distribution of NAME_EDUCATION_TYPE</a>\n"", ""   - <a href='#9.7'>9.7. Effect of marital status on ability to pay back loans</a>\n"", ""   - <a href='#9.8'>9.8. Distribution of NAME_HOUSING_TYPE</a>\n"", ""   - <a href='#9.9'>9.9. Distribution of Age</a>\n"", ""   - <a href='#9.10'>9.10. Effect of OCCUPATION_TYPE on default \n"", ""- <a href='#10'>10. Preparation of Data</a>\n"", ""   - <a href='#10.1'>10.1. Feature Engineering of Application data </a>\n"", ""   - <a href='#10.2'>10.2 Using Bureau Data</a>\n"", ""   - <a href='#10.3'>10.3. Using Previous Application Data</a>\n"", ""   - <a href='#10.4'>10.4. Using POS_CASH_balance data</a>\n"", ""   - <a href='#10.5'>10.5 Using installments_payments data</a>\n"", ""   - <a href='#10.6'>10.6. Using Credit card balance data </a>\n"", ""- <a href='#11'>11. Dividing data into train, valid and test   </a>\n"", ""- <a href='#12'>12. Feature Selection using Information Value and Weight of Evidence </a>\n"", ""- <a href='#13'> 13. Data Imputation before applying machine learning algorithms</a>\n"", ""- <a href='#14'>14. Applying Machine Learning Algorithms </a>\n"", ""  - <a href='#14.1'>14.1. Applying Logistic Regression</a>\n"", ""  - <a href='#14.2'>14.2. Applying XGBoost </a>\n"", ""  - <a href='#14.3'>14.3. Applying CATBOOST</a>\n"", ""  - <a href='#14.4'>14.4. Applying LightGBM</a>\n"", ""  - <a href='#14.5'>14.5. Applying RandomForest</a>\n"", ""- <a href='#15'> 15. Evaluating machine learning algorithms accuracy on training and testing sets </a>\n"", ""- <a href='#16'> 16. Optimising selected machine learning model further by choosing best hyperparameters </a>\n"", ""- <a href='#17'> 17. Final Predictions </a>""]",2,shailaja4247,tackle-any-credit-risk-analysis-problem-homecredit,shailaja4247_tackle-any-credit-risk-analysis-problem-homecredit_m2
"['## <a id=""1""> 1. Importing necessary libraries and packages and reading files</a>']",3,shailaja4247,tackle-any-credit-risk-analysis-problem-homecredit,shailaja4247_tackle-any-credit-risk-analysis-problem-homecredit_m3
"['## <a id=""2""> 2. Handling non-numerical variables</a>\n', 'Machines can understand only numbers. Hence let us convert all non-numeric columns into numbers. Categorical variables will be converted into dummy columns , ordinal variables are converted into numbers by mapping and variables which are non-numeric and cannot be converted into numbers will be dropped from the model.']",4,shailaja4247,tackle-any-credit-risk-analysis-problem-homecredit,shailaja4247_tackle-any-credit-risk-analysis-problem-homecredit_m4
"['### <a id=""3""> 3. Aligning Training and Testing Data</a>\n', '\n', 'There need to be the same features (columns) in both the training and testing data.\n', 'One-hot encoding has created more columns in the training data because there were some categorical variables\n', 'with categories not represented in the testing data. To remove the columns in the training data that are not in the testing\n', 'data, we need to `align` the dataframes. First we extract the target column from the training data (because this is not in\n', 'the testing data but we need to keep this information). When we do the align, we must make sure to set `axis = 1` \n', 'to align the dataframes based on the columns and not on the rows!\n']",5,shailaja4247,tackle-any-credit-risk-analysis-problem-homecredit,shailaja4247_tackle-any-credit-risk-analysis-problem-homecredit_m5
['The training and testing datasets now have the same features which is required for machine learning. The number of features has grown significantly due to one-hot encoding. At some point we probably will want to try [dimensionality reduction (removing features that are not relevant)](https://en.wikipedia.org/wiki/Dimensionality_reduction) to reduce the size of the datasets.'],6,shailaja4247,tackle-any-credit-risk-analysis-problem-homecredit,shailaja4247_tackle-any-credit-risk-analysis-problem-homecredit_m6
"['### <a id=""4""> 4. Handling missing values (using Iterative Imputer) prior to outlier detection </a>\n', '\n', 'We need to handle our missing values before we can do any kind of outlier detection.\n', ""There are many ways to handle missing values. We can use fillna() and replace missing values with data's mean, median or most frequent value. The approach that we shall use below will be Iterative Imputer. Iterative imputer will consider the missing variable to be the dependent variable and all the other features will be independent variables. So there will be a regression and the independent variables will be used gto determine the dependent variable (which is the missing feature).""]",7,shailaja4247,tackle-any-credit-risk-analysis-problem-homecredit,shailaja4247_tackle-any-credit-risk-analysis-problem-homecredit_m7
"['###  <a id=""5"">5. OUTLIERS DETECTION</a>\n', '\n', 'In statistics, an outlier is an observation point that is distant from other observations. There are many ways for outlier detection. \n', '\n', '**Visual methods to spot and remove outliers**\n', '1. Box-plot\n', '2. Scatter plots\n', '\n', '**Outliers detection and removal using mathematical function**\n', '1. Z-score: Threshold of -3 to 3 is taken, and any point with z score not in this range is removed as an outlier.\n', '2. IQR Score : This works similar to a box plot and z - score in the sense that a threshold IQR value is defined. IQR is the first quartile subtracted from the third quartile. Any point below the threshold IQR is removed. \n', '\n', '\n', '**Clustering methods for outlier detection**\n', '1. DBScan clustering (making clusters around data points). Minimum number of points are required to be in a cluster. There will be points that do not belong to any cluster or else points which are single in an entire cluster. So we can remove such noise points.  \n', '2. Isolation Forest: Isolation Forest will output the predictions for each data point in an array. If the result is -1, it means that this specific data point is an outlier. If the result is 1, then it means that the data point is not an outlier\n', '\n', 'Here we shall use Isolation Forest method because it can handle missing values well and does not require scaling of inputs. ']",8,shailaja4247,tackle-any-credit-risk-analysis-problem-homecredit,shailaja4247_tackle-any-credit-risk-analysis-problem-homecredit_m8
"[""### <a id='5.1'> 5.1. Anomaly detection </a>\n"", '\n', 'Though we have removed outliers using Isolation Forest we will still see the data once to check for any anomalies. Isolation Forest or any outlier detection method assumes that outlier is a point which is in minority and does not resemble the other majority points. However sometimes some abberation points are too many in numbers too. Let us see if there is any such anamoly that we find']",9,shailaja4247,tackle-any-credit-risk-analysis-problem-homecredit,shailaja4247_tackle-any-credit-risk-analysis-problem-homecredit_m9
"['**Univariate outliers detection**\n', '\n', '**Negative numbers:**\n', 'DAYS_BIRTH, DAYS_EMPLOYED, DAYS_REGISTRATION, DAYS_ID_PUBLISH, DAYS_LAST_PHONE_CHANGE\n', '\n', 'Numbers are negative since they are taken relative to the date of application. So we need to change them to positive.\n', '\n', '**Maximum value discrepancy**\n', 'DAYS_EMPLOYED: 365243 days(over 1000 years)\n', 'OWN_CAR_AGE: 91 Years\n']",10,shailaja4247,tackle-any-credit-risk-analysis-problem-homecredit,shailaja4247_tackle-any-credit-risk-analysis-problem-homecredit_m10
['As we can see there is anomaly in Days_employed as it is highly unlikely that a person will be employed for 1000 years.'],11,shailaja4247,tackle-any-credit-risk-analysis-problem-homecredit,shailaja4247_tackle-any-credit-risk-analysis-problem-homecredit_m11
['The data now looks nice and clean.'],12,shailaja4247,tackle-any-credit-risk-analysis-problem-homecredit,shailaja4247_tackle-any-credit-risk-analysis-problem-homecredit_m12
"['## <a id=""6"">6. Missing data in application_train</a>']",13,shailaja4247,tackle-any-credit-risk-analysis-problem-homecredit,shailaja4247_tackle-any-credit-risk-analysis-problem-homecredit_m13
"['## <a id=""7"">7. Duplicate data in application_train </a>']",14,shailaja4247,tackle-any-credit-risk-analysis-problem-homecredit,shailaja4247_tackle-any-credit-risk-analysis-problem-homecredit_m14
"['## <a id=""8"">8. Checking for data imbalance</a>']",15,shailaja4247,tackle-any-credit-risk-analysis-problem-homecredit,shailaja4247_tackle-any-credit-risk-analysis-problem-homecredit_m15
['We see that the class is clearly imbalanced with cases of default as very low compared to overall cases. So we need to balance the data when we use Machine learning models.'],16,shailaja4247,tackle-any-credit-risk-analysis-problem-homecredit,shailaja4247_tackle-any-credit-risk-analysis-problem-homecredit_m16
"['## <a id=""9"">  9. Exploratory Data Analysis for application_train by visualisation </a>']",17,shailaja4247,tackle-any-credit-risk-analysis-problem-homecredit,shailaja4247_tackle-any-credit-risk-analysis-problem-homecredit_m17
"['### <a id=""9.1""> 9.1. Distribution of income</a>\n']",18,shailaja4247,tackle-any-credit-risk-analysis-problem-homecredit,shailaja4247_tackle-any-credit-risk-analysis-problem-homecredit_m18
"['The distribution is right skewed and there are extreme values, we can apply log distribution.']",19,shailaja4247,tackle-any-credit-risk-analysis-problem-homecredit,shailaja4247_tackle-any-credit-risk-analysis-problem-homecredit_m19
['People with high income tend to not default'],20,shailaja4247,tackle-any-credit-risk-analysis-problem-homecredit,shailaja4247_tackle-any-credit-risk-analysis-problem-homecredit_m20
['We see that income variable gets normal distribution when it is log transformed. '],21,shailaja4247,tackle-any-credit-risk-analysis-problem-homecredit,shailaja4247_tackle-any-credit-risk-analysis-problem-homecredit_m21
"['### <a id=""9.2"">9.2. Distribution of credit</a>\n']",22,shailaja4247,tackle-any-credit-risk-analysis-problem-homecredit,shailaja4247_tackle-any-credit-risk-analysis-problem-homecredit_m22
['People who take more credit default less.'],23,shailaja4247,tackle-any-credit-risk-analysis-problem-homecredit,shailaja4247_tackle-any-credit-risk-analysis-problem-homecredit_m23
"['### <a id=""9.3""> 9.3. Distribution of loan types</a>']",24,shailaja4247,tackle-any-credit-risk-analysis-problem-homecredit,shailaja4247_tackle-any-credit-risk-analysis-problem-homecredit_m24
['More people are interested to take cash loans than revolving loans.'],25,shailaja4247,tackle-any-credit-risk-analysis-problem-homecredit,shailaja4247_tackle-any-credit-risk-analysis-problem-homecredit_m25
"['### <a id=""9.4""> 9.4. Distribution of NAME_INCOME_TYPE</a>\n']",26,shailaja4247,tackle-any-credit-risk-analysis-problem-homecredit,shailaja4247_tackle-any-credit-risk-analysis-problem-homecredit_m26
"['### <a id=""9.5""> 9.5. Distribution of NAME_TYPE_SUITE</a>']",27,shailaja4247,tackle-any-credit-risk-analysis-problem-homecredit,shailaja4247_tackle-any-credit-risk-analysis-problem-homecredit_m27
['Who accompanied the person while taking the loan? '],28,shailaja4247,tackle-any-credit-risk-analysis-problem-homecredit,shailaja4247_tackle-any-credit-risk-analysis-problem-homecredit_m28
['Most people are unaccompanied.'],29,shailaja4247,tackle-any-credit-risk-analysis-problem-homecredit,shailaja4247_tackle-any-credit-risk-analysis-problem-homecredit_m29
"['### <a id=""9.6"">9.6. Distribution of NAME_EDUCATION_TYPE</a>\n']",30,shailaja4247,tackle-any-credit-risk-analysis-problem-homecredit,shailaja4247_tackle-any-credit-risk-analysis-problem-homecredit_m30
['People with a degree are able to pay back mostly.'],31,shailaja4247,tackle-any-credit-risk-analysis-problem-homecredit,shailaja4247_tackle-any-credit-risk-analysis-problem-homecredit_m31
"['### <a id=""9.7""> 9.7. Effect of marital status on ability to pay back loans</a>']",32,shailaja4247,tackle-any-credit-risk-analysis-problem-homecredit,shailaja4247_tackle-any-credit-risk-analysis-problem-homecredit_m32
"['### <a id=""9.8""> 9.8. Distribution of NAME_HOUSING_TYPE</a>\n']",33,shailaja4247,tackle-any-credit-risk-analysis-problem-homecredit,shailaja4247_tackle-any-credit-risk-analysis-problem-homecredit_m33
"['People in office apartment, co-op apartment almost never default.']",34,shailaja4247,tackle-any-credit-risk-analysis-problem-homecredit,shailaja4247_tackle-any-credit-risk-analysis-problem-homecredit_m34
"['### <a id=""9.9""> 9.9. Distribution of AGE</a>\n']",35,shailaja4247,tackle-any-credit-risk-analysis-problem-homecredit,shailaja4247_tackle-any-credit-risk-analysis-problem-homecredit_m35
"['### <a id=""9.10"">9.10. Effect of OCCUPATION_TYPE on default </a>']",36,shailaja4247,tackle-any-credit-risk-analysis-problem-homecredit,shailaja4247_tackle-any-credit-risk-analysis-problem-homecredit_m36
['Highly skilled people more likely to pay back and low skilled not so likely to pay back loans'],37,shailaja4247,tackle-any-credit-risk-analysis-problem-homecredit,shailaja4247_tackle-any-credit-risk-analysis-problem-homecredit_m37
"['## <a id=""10""> 10. Combining other tables to extract their data </a> \n', '\n', 'There are many tables apart from application_train. Due to memory and space restrictions on Kaggle, I am unable to describe them here. But one can easily look up the description on the competition data page. We need to extract information from these ']",38,shailaja4247,tackle-any-credit-risk-analysis-problem-homecredit,shailaja4247_tackle-any-credit-risk-analysis-problem-homecredit_m38
"[""<a id='3.4.1'></a>\n"", '<h3> 10.1 Feature Engineering of Application data </h3>']",39,shailaja4247,tackle-any-credit-risk-analysis-problem-homecredit,shailaja4247_tackle-any-credit-risk-analysis-problem-homecredit_m39
"[""<a id='10.2'></a>\n"", '<h3> 10.2 Using Bureau Data </h3>']",40,shailaja4247,tackle-any-credit-risk-analysis-problem-homecredit,shailaja4247_tackle-any-credit-risk-analysis-problem-homecredit_m40
"[""<a id='10.2.1'></a>\n"", '<h3> 10.2.1. Feature Engineering of Bureau Data </h3>']",41,shailaja4247,tackle-any-credit-risk-analysis-problem-homecredit,shailaja4247_tackle-any-credit-risk-analysis-problem-homecredit_m41
"[""<a id='10.3'></a>\n"", '<h3> 10.3 Using Previous Application Data </h3>']",42,shailaja4247,tackle-any-credit-risk-analysis-problem-homecredit,shailaja4247_tackle-any-credit-risk-analysis-problem-homecredit_m42
"[""<a id='10.4'></a>\n"", '<h3> 10.4. Using POS_CASH_balance data </h3>']",43,shailaja4247,tackle-any-credit-risk-analysis-problem-homecredit,shailaja4247_tackle-any-credit-risk-analysis-problem-homecredit_m43
"[""<a id='10.5'></a>\n"", '<h3> 10.5. Using installments_payments data</h3>']",44,shailaja4247,tackle-any-credit-risk-analysis-problem-homecredit,shailaja4247_tackle-any-credit-risk-analysis-problem-homecredit_m44
"[""<a id='10.6'></a>\n"", '<h3> 10.6. Using Credit card balance data </h3>']",45,shailaja4247,tackle-any-credit-risk-analysis-problem-homecredit,shailaja4247_tackle-any-credit-risk-analysis-problem-homecredit_m45
"[""<h3> <a id='11'> 11. Dividing data into train, valid and test </a> </h3> ""]",46,shailaja4247,tackle-any-credit-risk-analysis-problem-homecredit,shailaja4247_tackle-any-credit-risk-analysis-problem-homecredit_m46
"[""`X=application_bureau_prev.drop(columns=['TARGET'])\n"", ""y=application_bureau_prev['TARGET']`\n"", '\n', '`from sklearn.model_selection import train_test_split\n', 'X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)`']",47,shailaja4247,tackle-any-credit-risk-analysis-problem-homecredit,shailaja4247_tackle-any-credit-risk-analysis-problem-homecredit_m47
"[""<h3> <a id='12'> 12. Feature Selection using Information Value and Weight of Evidence </a> </h3>""]",48,shailaja4247,tackle-any-credit-risk-analysis-problem-homecredit,shailaja4247_tackle-any-credit-risk-analysis-problem-homecredit_m48
"['There are many methods for feature selection. Some of them include feature importance using XGBoost and RandomForest. Other methods are forward or backward elimination and Boruta. Here we use one of the most common methods for feature selection, Information value and weight of evidence to determine the feature selection for credit risk analysis.\n', '\n', '\n', '**Code to calculate IV and WOE**\n', '\n', 'max_bin = 20\n', '\n', 'force_bin = 3\n', '\n', '#Define a binning function\n', '\n', 'def mono_bin(Y, X, n = max_bin):\n', '\n', '    df1 = pd.DataFrame({""X"": X, ""Y"": Y})\n', ""    justmiss = df1[['X','Y']][df1.X.isnull()]\n"", ""    notmiss = df1[['X','Y']][df1.X.notnull()]\n"", '    r = 0\n', '    while np.abs(r) < 1:\n', '        try:\n', '            d1 = pd.DataFrame({""X"": notmiss.X, ""Y"": notmiss.Y, ""Bucket"": pd.qcut(notmiss.X, n)})\n', ""            d2 = d1.groupby('Bucket', as_index=True)\n"", '            r, p = stats.spearmanr(d2.mean().X, d2.mean().Y)\n', '            n = n - 1 \n', '        except Exception as e:\n', '            n = n - 1\n', '\n', '    if len(d2) == 1:\n', '        n = force_bin         \n', '        bins = algos.quantile(notmiss.X, np.linspace(0, 1, n))\n', '        if len(np.unique(bins)) == 2:\n', '            bins = np.insert(bins, 0, 1)\n', '            bins[1] = bins[1]-(bins[1]/2)\n', '        d1 = pd.DataFrame({""X"": notmiss.X, ""Y"": notmiss.Y, ""Bucket"": pd.cut(notmiss.X, np.unique(bins),include_lowest=True)}) \n', ""        d2 = d1.groupby('Bucket', as_index=True)\n"", '    \n', '    d3 = pd.DataFrame({},index=[])\n', '    d3[""MIN_VALUE""] = d2.min().X\n', '    d3[""MAX_VALUE""] = d2.max().X\n', '    d3[""COUNT""] = d2.count().Y\n', '    d3[""EVENT""] = d2.sum().Y\n', '    d3[""NONEVENT""] = d2.count().Y - d2.sum().Y\n', '    d3=d3.reset_index(drop=True)\n', '    \n', '    if len(justmiss.index) > 0:\n', ""        d4 = pd.DataFrame({'MIN_VALUE':np.nan},index=[0])\n"", '        d4[""MAX_VALUE""] = np.nan\n', '        d4[""COUNT""] = justmiss.count().Y\n', '        d4[""EVENT""] = justmiss.sum().Y\n', '        d4[""NONEVENT""] = justmiss.count().Y - justmiss.sum().Y\n', '        d3 = d3.append(d4,ignore_index=True)\n', '    \n', '    d3[""EVENT_RATE""] = d3.EVENT/d3.COUNT\n', '    d3[""NON_EVENT_RATE""] = d3.NONEVENT/d3.COUNT\n', '    d3[""DIST_EVENT""] = d3.EVENT/d3.sum().EVENT\n', '    d3[""DIST_NON_EVENT""] = d3.NONEVENT/d3.sum().NONEVENT\n', '    d3[""WOE""] = np.log(d3.DIST_EVENT/d3.DIST_NON_EVENT)\n', '    d3[""IV""] = (d3.DIST_EVENT-d3.DIST_NON_EVENT)*np.log(d3.DIST_EVENT/d3.DIST_NON_EVENT)\n', '    d3[""VAR_NAME""] = ""VAR""\n', ""    d3 = d3[['VAR_NAME','MIN_VALUE', 'MAX_VALUE', 'COUNT', 'EVENT', 'EVENT_RATE', 'NONEVENT', 'NON_EVENT_RATE', 'DIST_EVENT','DIST_NON_EVENT','WOE', 'IV']]       \n"", '    d3 = d3.replace([np.inf, -np.inf], 0)\n', '    d3.IV = d3.IV.sum()\n', '    \n', '    return(d3)\n', '\n', '\n', 'def char_bin(Y, X):\n', '        \n', '    df1 = pd.DataFrame({""X"": X, ""Y"": Y})\n', ""    justmiss = df1[['X','Y']][df1.X.isnull()]\n"", ""    notmiss = df1[['X','Y']][df1.X.notnull()]    \n"", ""    df2 = notmiss.groupby('X',as_index=True)\n"", '    \n', '    d3 = pd.DataFrame({},index=[])\n', '    d3[""COUNT""] = df2.count().Y\n', '    d3[""MIN_VALUE""] = df2.sum().Y.index\n', '    d3[""MAX_VALUE""] = d3[""MIN_VALUE""]\n', '    d3[""EVENT""] = df2.sum().Y\n', '    d3[""NONEVENT""] = df2.count().Y - df2.sum().Y\n', '    \n', '    if len(justmiss.index) > 0:\n', ""        d4 = pd.DataFrame({'MIN_VALUE':np.nan},index=[0])\n"", '        d4[""MAX_VALUE""] = np.nan\n', '        d4[""COUNT""] = justmiss.count().Y\n', '        d4[""EVENT""] = justmiss.sum().Y\n', '        d4[""NONEVENT""] = justmiss.count().Y - justmiss.sum().Y\n', '        d3 = d3.append(d4,ignore_index=True)\n', '    \n', '    d3[""EVENT_RATE""] = d3.EVENT/d3.COUNT\n', '    d3[""NON_EVENT_RATE""] = d3.NONEVENT/d3.COUNT\n', '    d3[""DIST_EVENT""] = d3.EVENT/d3.sum().EVENT\n', '    d3[""DIST_NON_EVENT""] = d3.NONEVENT/d3.sum().NONEVENT\n', '    d3[""WOE""] = np.log(d3.DIST_EVENT/d3.DIST_NON_EVENT)\n', '    d3[""IV""] = (d3.DIST_EVENT-d3.DIST_NON_EVENT)*np.log(d3.DIST_EVENT/d3.DIST_NON_EVENT)\n', '    d3[""VAR_NAME""] = ""VAR""\n', ""    d3 = d3[['VAR_NAME','MIN_VALUE', 'MAX_VALUE', 'COUNT', 'EVENT', 'EVENT_RATE', 'NONEVENT', 'NON_EVENT_RATE', 'DIST_EVENT','DIST_NON_EVENT','WOE', 'IV']]      \n"", '    d3 = d3.replace([np.inf, -np.inf], 0)\n', '    d3.IV = d3.IV.sum()\n', '    d3 = d3.reset_index(drop=True)\n', '    \n', '    return(d3)\n', '\n', '-------------------------------------------------------------------------------------------------------------------\n', '\n', 'def data_vars(df1, target):\n', '    \n', '    stack = traceback.extract_stack()\n', '    filename, lineno, function_name, code = stack[-2]\n', ""    vars_name = re.compile(r'\\((.*?)\\).*$').search(code).groups()[0]\n"", '    final = (re.findall(r""[\\w\']+"", vars_name))[-1]\n', '    \n', '    x = df1.dtypes.index\n', '    count = -1\n', '    \n', '    for i in x:\n', '        if i.upper() not in (final.upper()):\n', '            if np.issubdtype(df1[i], np.number) and len(Series.unique(df1[i])) > 2:\n', '                conv = mono_bin(target, df1[i])\n', '                conv[""VAR_NAME""] = i\n', '                count = count + 1\n', '            else:\n', '                conv = char_bin(target, df1[i])\n', '                conv[""VAR_NAME""] = i            \n', '                count = count + 1\n', '                \n', '            if count == 0:\n', '                iv_df = conv\n', '            else:\n', '                iv_df = iv_df.append(conv,ignore_index=True)\n', '    \n', ""    iv = pd.DataFrame({'IV':iv_df.groupby('VAR_NAME').IV.max()})\n"", '    iv = iv.reset_index()\n', '    return(iv_df,iv)\n', '    \n', '-------------------------------------------------------------------------------------------------------------------\n', '\n', '`import pandas as pd\n', 'import numpy as np\n', 'import pandas.core.algorithms as algos\n', 'from pandas import Series\n', 'import scipy.stats.stats as stats\n', 'import re\n', 'import traceback\n', 'import string\n', 'final_iv, IV = data_vars(X_train, y_train)\n', 'IV`\n', '\n', '**Output of IV**\n', '\n', 'Index         | VAR_NAME | IV\n', '----------------------|-----------|---------------\n', ' Logistic Regression with Selected features  |    AMT_ANNUITY  | 4.050335e-04\n', ' Random Forest with Selected features  |    AMT_CREDIT |    2.415783e-03  \n', ' LightGBM with Selected features |    AMT_GOODS_PRICE |    3.591973e-02\n', ' CATBoost with Selected features |    AMT_INCOME_TOTAL |    2.504913e-03\n', ' XGBoost with Selected features |    AMT_REQ_CREDIT_BUREAU_DAY |    1.289777e-02\n', '\n', '\n', 'In case of Information value, predictions with information value < 0.02 are useless for predictions, so we will only consider columns with IV > 0.02.\n', '\n', ""`list_of_columns=IV[IV['IV'] > 0.02]['VAR_NAME'].to_list()\n"", 'print(len(list_of_columns))`\n', '\n', '63\n', '\n', 'We find that only 63 columns are efficient in predicting the default by a customer. Hence we shall only consider those columns \n', '\n', '`X_train_selected_features=X_train[list_of_columns]\n', 'X_test_selected_features=X_test[list_of_columns]\n', ""X_train_selected_features['SK_ID_CURR']=X_train['SK_ID_CURR']\n"", ""X_test_selected_features['SK_ID_CURR']=X_test['SK_ID_CURR']`\n"", '\n', '`application_bureau_prev_test_selected_features=application_bureau_prev_test[list_of_columns]\n', ""application_bureau_prev_test_selected_features['SK_ID_CURR']=application_bureau_prev_test['SK_ID_CURR']`""]",49,shailaja4247,tackle-any-credit-risk-analysis-problem-homecredit,shailaja4247_tackle-any-credit-risk-analysis-problem-homecredit_m49
"[""## <a id ='13'> 13. Data Imputation before applying machine learning algorithms </a>""]",50,shailaja4247,tackle-any-credit-risk-analysis-problem-homecredit,shailaja4247_tackle-any-credit-risk-analysis-problem-homecredit_m50
"[""There are many ways to handle missing values. We can use fillna() and replace missing values with data's mean, median or most frequent value. The approach that we shall use below will be Iterative Imputer. Iterative imputer will consider the missing variable to be the dependent varibale and all the other features will be independent variables. Then it will apply regression and the independent variables will be used to determine the dependent variable (which is the missing feature).\n"", '\n', '`imputer = IterativeImputer(BayesianRidge())\n', 'X_train_imputed = pd.DataFrame(imputer.fit_transform(X_train_selected_features))\n', 'X_train_imputed.columns = X_train_selected_features.columns`\n', '\n', '`imputer = IterativeImputer(BayesianRidge())\n', 'application_bureau_prev_test_selected_features_subset1=application_bureau_prev_test_selected_features.iloc[:, np.r_[62,0:30]]\n', 'app_bur_prev_test_imputed_subset1 = pd.DataFrame(imputer.fit_transform(application_bureau_prev_test_selected_features_subset1))\n', 'app_bur_prev_test_imputed_subset1.columns = application_bureau_prev_test_selected_features_subset1.columns`\n', '\n', '`application_bureau_prev_test_selected_features_subset2=application_bureau_prev_test_selected_features.iloc[:, np.r_[31:63]]\n', 'app_bur_prev_test_imputed_subset2 = pd.DataFrame(imputer.fit_transform(application_bureau_prev_test_selected_features_subset2))\n', 'app_bur_prev_test_imputed_subset2.columns = application_bureau_prev_test_selected_features_subset2.columns`\n', '\n', ""`app_bur_prev_test_imputed=pd.merge(app_bur_prev_test_imputed_subset1, app_bur_prev_test_imputed_subset2, on= 'SK_ID_CURR')`\n"", '\n', '`imputer = IterativeImputer(BayesianRidge())\n', 'X_test_imputed = pd.DataFrame(imputer.fit_transform(X_test_selected_features))\n', 'X_test_imputed.columns = X_test_selected_features.columns`\n', '\n', '`print(X_test_imputed.shape)\n', 'print(X_train_imputed.shape)\n', 'print(app_bur_prev_test_imputed.shape)`\n', 'Output\n', '\n', '(61503, 63)\n', '\n', '(246008, 63)\n', '\n', '(48744, 62)\n', '\n', '**Align the training and testing dataframes, keep only columns present in both dataframes**\n', '\n', 'We see above that the number of columns in test and training set are not same.\n', '\n', ""`X_train_imputed, app_bur_prev_test_imputed  = app_bur_prev_test_imputed.align(X_train_imputed, join = 'inner', axis = 1)\n"", ""X_train_imputed,X_test_imputed= X_train_imputed.align(X_test_imputed, join = 'inner', axis = 1)`\n""]",51,shailaja4247,tackle-any-credit-risk-analysis-problem-homecredit,shailaja4247_tackle-any-credit-risk-analysis-problem-homecredit_m51
"[""## <a id='14'> 14. Applying Machine Learning Algorithms (using cross validation) </a>""]",52,shailaja4247,tackle-any-credit-risk-analysis-problem-homecredit,shailaja4247_tackle-any-credit-risk-analysis-problem-homecredit_m52
"['**14.1. Applying Logistic Regression**\n', '\n', '`from sklearn.linear_model import LogisticRegression\n', ""lr_clf = LogisticRegression(random_state = 0, class_weight='balanced')\n"", 'lr_clf.fit(X_train_imputed, y_train)\n', 'from sklearn.model_selection import cross_val_predict\n', 'from sklearn.model_selection import cross_val_score\n', 'y_train_pred_lr=cross_val_predict(lr_clf, X_train_imputed, y_train, cv=3)\n', ""print('Accuracy on Training set:',cross_val_score(lr_clf, X_train_imputed,y_train, cv=3, scoring='accuracy'))\n"", ""print('Accuracy on Test set:',cross_val_score(lr_clf, X_test_imputed,y_test, cv=3, scoring='accuracy'))`\n"", '\n', '*Accuracy on Training set: [0.66357739 0.68726373 0.6783615 ],\n', 'Accuracy on Test set: [0.66164277 0.66167504 0.65356098]*\n', '\n', '-----------------------------------------------------------------------------------------------------------------------\n', '\n', '**14.2. Applying XGBoost**\n', '\n', '`#Scale_pos_weight if set to sum(negative instances)/ sum(negative instances) will take care of imbalanced data in the dataset\n', 'scale_pos_weight_value=y_train.value_counts().values.tolist()[0]/y_train.value_counts().values.tolist()[1]\n', 'from xgboost import XGBClassifier\n', 'XGB_clf = XGBClassifier(scale_pos_weight=scale_pos_weight_value)\n', 'XGB_clf.fit(X_train_imputed, y_train)\n', ""print('Accuracy on Training set:',cross_val_score(XGB_clf, X_train_imputed, y_train, cv=3, scoring='accuracy'))\n"", ""print('Accuracy on Test set:',cross_val_score(XGB_clf, X_test_imputed, y_test, cv=3, scoring='accuracy'))`\n"", '\n', '*Accuracy on Training set: [0.74863421 0.75014024 0.74932319],\n', 'Accuracy on Test set: [0.81938347 0.82391103 0.82102439]*\n', '\n', '-----------------------------------------------------------------------------------------------------------------------\n', '\n', '**14.3. Applying CATBOOST**\n', '\n', '`cols_numeric = X_train_imputed.select_dtypes([np.number]).columns\n', 'cols_categorical=X_train_imputed.columns.difference(cols_numeric)\n', '#We find that there are no categorical columns.\n', 'from catboost import CatBoostClassifier\n', 'CatBoost_clf=CatBoostClassifier(scale_pos_weight=scale_pos_weight_value)\n', ""#CatBoost_clf=CatBoostRegressor(iterations=50, depth=3, learning_rate=0.1, loss_function='RMSE')\n"", 'CatBoost_clf.fit(X_train_imputed, y_train)\n', ""print('Accuracy on Training set:',cross_val_score(CatBoost_clf, X_train_imputed, y_train, cv=3, scoring='accuracy'))\n"", ""print('Accuracy on Test set:',cross_val_score(CatBoost_clf, X_test_imputed, y_test, cv=3, scoring='accuracy'))`\n"", '\n', '*Accuracy on Training set: [0.76207258 0.76210336 0.75900588],\n', 'Accuracy on Test set: [0.81211589 0.8081557  0.81302439]*\n', '\n', '-----------------------------------------------------------------------------------------------------------------------\n', '\n', '**14.4. Applying LightGBM**\n', '\n', '`import lightgbm as lgb\n', 'LightGBM_clf=lgb.LGBMClassifier(scale_pos_weight=scale_pos_weight_value)\n', 'LightGBM_clf.fit(X_train_imputed, y_train)\n', ""print('Accuracy on Training set:',cross_val_score(LightGBM_clf, X_train_imputed, y_train, cv=3, scoring='accuracy'))\n"", ""print('Accuracy on Test set:',cross_val_score(LightGBM_clf, X_test_imputed, y_test, cv=3, scoring='accuracy'))`\n"", '\n', '*Accuracy on Training set: [0.70613629 0.70886076 0.70815346],\n', 'Accuracy on Test set: [0.75158521 0.74947564 0.75639024]*\n', '\n', '-----------------------------------------------------------------------------------------------------------------------\n', '\n', '**14.5. Applying RandomForest**\n', '\n', ""`#class_weight = 'balanced' ensures that RandomForest works well on imbalanced datasets.\n"", 'from sklearn.ensemble import RandomForestClassifier\n', 'rf_clf = RandomForestClassifier(n_estimators = 10, random_state = 0, n_jobs=-1, class_weight=""balanced"")\n', 'rf_clf.fit(X_train_imputed, y_train)\n', ""print('Accuracy on Training set:',cross_val_score(rf_clf, X_train_imputed, y_train, cv=3, scoring='accuracy'))\n"", ""print('Accuracy on Test set:',cross_val_score(rf_clf, X_test_imputed, y_test, cv=3, scoring='accuracy'))`\n"", '\n', '*Accuracy on Training set: [0.86872348 0.86855077 0.8684654 ],\n', 'Accuracy on Test set: [0.86878841 0.86829667 0.8684878 ]*']",53,shailaja4247,tackle-any-credit-risk-analysis-problem-homecredit,shailaja4247_tackle-any-credit-risk-analysis-problem-homecredit_m53
"[""## <a id='15'> 15. Evaluating machine learning algorithms accuracy on training and testing sets </a>\n"", 'This perfromance is without tuning any hyperparameters and without any optimisation']",54,shailaja4247,tackle-any-credit-risk-analysis-problem-homecredit,shailaja4247_tackle-any-credit-risk-analysis-problem-homecredit_m54
"['Model         | Train Accuracy | Test AUC\n', '----------------------|-----------|---------------\n', ' Logistic Regression with Selected features  |    0.66  | 0.66\n', ' Random Forest with Selected features  |    0.86 |    0.86  \n', ' LightGBM with Selected features |    0.71 |    0.75\n', ' CATBoost with Selected features |    0.76 |    0.81\n', ' XGBoost with Selected features |    0.75 |    0.82\n', ' \n', ' \n']",55,shailaja4247,tackle-any-credit-risk-analysis-problem-homecredit,shailaja4247_tackle-any-credit-risk-analysis-problem-homecredit_m55
"['Since we get the best performance in Random Forest, let us try to enhance Random Forest by tuning hyperparameters']",56,shailaja4247,tackle-any-credit-risk-analysis-problem-homecredit,shailaja4247_tackle-any-credit-risk-analysis-problem-homecredit_m56
"[""## <a id='16'> 16. Optimising selected machine learning model further by choosing best hyperparameters </a>""]",57,shailaja4247,tackle-any-credit-risk-analysis-problem-homecredit,shailaja4247_tackle-any-credit-risk-analysis-problem-homecredit_m57
"[""Since we got best performance with Random Forest, let's try to optimise it further by using the correct parameters. We shall use GridSearchCV or RandomizedSearchCV to choose the best parameters.\n"", '\n', '**First choose the range of hyperparameters using RandomizedSearchCV.**\n', '\n', '`from sklearn.model_selection import RandomizedSearchCV`\n', '\n', '`#Hyperparameters`\n', '\n', '`n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n', ""max_features = ['auto', 'sqrt']\n"", 'max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n', 'max_depth.append(None)\n', 'min_samples_split = [ 10, 25, 50, 100]\n', 'min_samples_leaf = [1, 2, 4,10,20,30]\n', 'bootstrap = [True, False]`\n', '\n', '`#Create the random grid`\n', '\n', ""`random_grid = {'n_estimators': n_estimators,\n"", ""               'max_features': max_features,\n"", ""               'max_depth': max_depth,\n"", ""               'min_samples_split': min_samples_split,\n"", ""               'min_samples_leaf': min_samples_leaf,\n"", ""               'bootstrap': bootstrap}`\n"", '\n', '`from sklearn.ensemble import RandomForestClassifier\n', 'rf = RandomForestClassifier(class_weight=""balanced"")\n', 'rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 10, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n', '#Fit the random search model\n', 'rf_random.fit(X_train_imputed, y_train)\n', 'rf_random.best_params_`\n', '\n', '\n', '**Random_grid Best Parameters output**\n', '\n', '`{\n', ""    'bootstrap': [True],\n"", ""    'max_depth': [100],\n"", ""    'max_features': [2],\n"", ""    'min_samples_leaf': [ 4],\n"", ""    'min_samples_split': [10],\n"", ""    'n_estimators': [200]\n"", '}`\n', '\n', '**GridSearchCV**\n', '\n', 'After getting the best parameters from RandomSearchCV , we have understood the range for hyperparameters. For example the number of trees\\estimators to be used should be in the range of 200, maximum features should be in range of 2 and so on.\n', 'Now let us test which hyperparameter to use by using GridSearchCV. We will include the parameters in the range as found by random_grid output.\n', '\n', '`from sklearn.model_selection import GridSearchCV`\n', '\n', '`#Create the parameter grid based on the results of random search`\n', '\n', '`param_grid = {\n', ""    'bootstrap': [True],\n"", ""    'max_depth': [80, 90, 100, 110],\n"", ""    'max_features': [2, 3],\n"", ""    'min_samples_leaf': [3, 4, 5],\n"", ""    'min_samples_split': [8, 10, 12],\n"", ""    'n_estimators': [100, 200, 300, 1000]\n"", '}`\n', '\n', '`#Create a RandomForets based model\n', 'rf = RandomForestRegressor()`\n', '\n', '`#Instantiate the grid search model\n', 'grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n', '                          cv = 3, n_jobs = -1, verbose = 2)\n', 'grid_search.best_params_`\n', '\n', '**Output of grid search best parameters**\n', '\n', '`max_depth= 20,\n', "" max_features= 'sqrt',\n"", ' min_samples_leaf= 5,\n', ' min_samples_split= 40,\n', ' n_estimators= 200`\n', ' \n', 'Fitting the parameters obtained by GridSearchCV on the RandomForest classification and rechecking the accuracy obtained on training and testing set.\n', '\n', '`from sklearn.ensemble import RandomForestClassifier\n', 'rf_clf_grid = RandomForestClassifier(random_state = 0, n_jobs=-1, class_weight=""balanced"",bootstrap= True,\n', ' max_depth= 20,\n', "" max_features= 'sqrt',\n"", ' min_samples_leaf= 5,\n', ' min_samples_split= 40,\n', ' n_estimators= 200)\n', 'rf_clf_grid.fit(X_train_imputed, y_train)`\n', '\n', ""`print('Accuracy on Training set:',cross_val_score(rf_clf_grid, X_train_imputed, y_train, cv=3, scoring='accuracy'))\n"", ""print('Accuracy on Test set:',cross_val_score(rf_clf_grid, X_test_imputed, y_test, cv=3, scoring='accuracy'))`\n"", '\n', '*Accuracy on Training set: [0.88676162 0.88575888 0.88496622]\n', 'Accuracy on Test set: [0.89396156 0.89868787 0.89497561]*\n', '\n', '\n', 'Note that the accuracy obtained via GridsearchCV and RandomizedSearchCV is more than the accuracy obtained without hyperparameters tuning.']",58,shailaja4247,tackle-any-credit-risk-analysis-problem-homecredit,shailaja4247_tackle-any-credit-risk-analysis-problem-homecredit_m58
"[""## <a id='17'> 17. Final predictions </a>""]",59,shailaja4247,tackle-any-credit-risk-analysis-problem-homecredit,shailaja4247_tackle-any-credit-risk-analysis-problem-homecredit_m59
"['Fitting the parameters found out by using GridSearchCV and predicting the outputs by fitting the best machine learning model.\n', '\n', '`rf_clf_grid.fit(X_train_imputed, y_train)\n', 'predictions_grid=rf_clf_grid.predict(app_bur_prev_test_imputed)`\n', '\n', 'Saving the results in csv files \n', '\n', '`predictions_grid_df=pd.DataFrame(data={""SK_ID_CURR"":app_bur_prev_test_imputed[""SK_ID_CURR""],""TARGET"":predictions_grid}) \n', ""predictions_grid_df['SK_ID_CURR'] = predictions_grid_df['SK_ID_CURR'].astype(int)\n"", 'predictions_grid_df.to_csv(path_or_buf=""predictions_grid_df.csv"",index=False)`']",60,shailaja4247,tackle-any-credit-risk-analysis-problem-homecredit,shailaja4247_tackle-any-credit-risk-analysis-problem-homecredit_m60
"# HOW TO INTERPRET BUREAU DATA

This table talks about the Loan data of each unique customer with all financial institutions other than Home Credit
For each unique SK_ID_CURR we have multiple SK_ID_BUREAU Id's, each being a unique loan transaction from other financial institutions availed by the same customer and reported to the bureau. ",0,shanth84,home-credit-bureau-data-feature-engineering,shanth84_home-credit-bureau-data-feature-engineering_m0
"# EXAMPLE OF BUREAU TRANSACTIONS 

- In the example below customer with SK_ID_CURR = 100001 had  7 credit transactions before the current application. ",1,shanth84,home-credit-bureau-data-feature-engineering,shanth84_home-credit-bureau-data-feature-engineering_m1
# UNDERSTANDING OF VARIABLES ,2,shanth84,home-credit-bureau-data-feature-engineering,shanth84_home-credit-bureau-data-feature-engineering_m2
"CREDIT_ACTIVE - Current status of a Loan - Closed/ Active (2 values)

CREDIT_CURRENCY - Currency in which the transaction was executed -  Currency1, Currency2, Currency3, Currency4 
                                        ( 4 values)
                                        
CREDIT_DAY_OVERDUE -  Number of overdue days 

CREDIT_TYPE -  Consumer Credit, Credit card, Mortgage, Car loan, Microloan, Loan for working capital replemishment, 
                             Loan for Business development, Real estate loan, Unkown type of laon, Another type of loan. 
                             Cash loan, Loan for the purchase of equipment, Mobile operator loan, Interbank credit, 
                             Loan for purchase of shares ( 15 values )

DAYS_CREDIT -   Number of days ELAPSED since customer applied for CB credit with respect to current application 
Interpretation - Are these loans evenly spaced time intervals? Are they concentrated within a same time frame?


DAYS_CREDIT_ENDDATE - Number of days the customer CREDIT is valid at the time of application 
CREDIT_DAY_OVERDUE - Number of days the customer CREDIT is past the end date at the time of application

AMT_CREDIT_SUM -  Total available credit for a customer 
AMT_CREDIT_SUM_DEBT -  Total amount yet to be repayed
AMT_CREDIT_SUM_LIMIT -   Current Credit that has been utilized 
AMT_CREDIT_SUM_OVERDUE - Current credit payment that is overdue 
CNT_CREDIT_PROLONG - How many times was the Credit date prolonged 

# NOTE: 
For a given loan transaction 
 'AMT_CREDIT_SUM' =  'AMT_CREDIT_SUM_DEBT' +'AMT_CREDIT_SUM_LIMIT'



AMT_ANNUITY -  Annuity of the Credit Bureau data
DAYS_CREDIT_UPDATE -  Number of days before current application when last CREDIT UPDATE was received 
DAYS_ENDDATE_FACT -    Days since CB credit ended at the time of application 
AMT_CREDIT_MAX_OVERDUE - Maximum Credit amount overdue at the time of application 
",3,shanth84,home-credit-bureau-data-feature-engineering,shanth84_home-credit-bureau-data-feature-engineering_m3
# FEATURE ENGINEERING WITH BUREAU CREDIT ,4,shanth84,home-credit-bureau-data-feature-engineering,shanth84_home-credit-bureau-data-feature-engineering_m4
# FEATURE 1 - NUMBER OF PAST LOANS PER CUSTOMER ,5,shanth84,home-credit-bureau-data-feature-engineering,shanth84_home-credit-bureau-data-feature-engineering_m5
# FEATURE 2 - NUMBER OF TYPES OF PAST LOANS PER CUSTOMER ,6,shanth84,home-credit-bureau-data-feature-engineering,shanth84_home-credit-bureau-data-feature-engineering_m6
"# FEATURE 3 - AVERAGE NUMBER OF PAST LOANS PER TYPE PER CUSTOMER

# Is the Customer diversified in taking multiple types of Loan or Focused on a single type of loan
",7,shanth84,home-credit-bureau-data-feature-engineering,shanth84_home-credit-bureau-data-feature-engineering_m7
# FEATURE 4 - % OF ACTIVE LOANS FROM BUREAU DATA ,8,shanth84,home-credit-bureau-data-feature-engineering,shanth84_home-credit-bureau-data-feature-engineering_m8
"# FEATURE 5

# AVERAGE NUMBER OF DAYS BETWEEN SUCCESSIVE PAST APPLICATIONS FOR EACH CUSTOMER 

# How often did the customer take credit in the past? Was it spaced out at regular time intervals - a signal of good financial planning OR were the loans concentrated around a smaller time frame - indicating potential financial trouble?
",9,shanth84,home-credit-bureau-data-feature-engineering,shanth84_home-credit-bureau-data-feature-engineering_m9
"# FEATURE 6  

# % of LOANS PER CUSTOMER WHERE END DATE FOR CREDIT IS PAST

 # INTERPRETING CREDIT_DAYS_ENDDATE 
 
 #  NEGATIVE VALUE - Credit date was in the past at time of application( Potential Red Flag !!! )
 
 # POSITIVE VALUE - Credit date is in the future at time of application ( Potential Good Sign !!!!)
 
 # NOTE : This is not the same as % of Active loans since Active loans 
 # can have Negative and Positive values for DAYS_CREDIT_ENDDATE",10,shanth84,home-credit-bureau-data-feature-engineering,shanth84_home-credit-bureau-data-feature-engineering_m10
"# FEATURE 7 

# AVERAGE NUMBER OF DAYS IN WHICH CREDIT EXPIRES IN FUTURE -INDICATION OF CUSTOMER DELINQUENCY IN FUTURE??",11,shanth84,home-credit-bureau-data-feature-engineering,shanth84_home-credit-bureau-data-feature-engineering_m11
"# FEATURE 8 - DEBT OVER CREDIT RATIO 
# The Ratio of Total Debt to Total Credit for each Customer 
# A High value may be a red flag indicative of potential default",12,shanth84,home-credit-bureau-data-feature-engineering,shanth84_home-credit-bureau-data-feature-engineering_m12
"# FEATURE 9 - OVERDUE OVER DEBT RATIO 
#  What fraction of total Debt is overdue per customer?
# A high value could indicate a potential DEFAULT ",13,shanth84,home-credit-bureau-data-feature-engineering,shanth84_home-credit-bureau-data-feature-engineering_m13
# FEATURE 10 - AVERAGE NUMBER OF LOANS PROLONGED ,14,shanth84,home-credit-bureau-data-feature-engineering,shanth84_home-credit-bureau-data-feature-engineering_m14
"![](https://www.homecredit.ph/files/copy-hc-logo.png)

# DNN classifier in Tensorflow

This kernel will build a DNN classifier for the Home Credit Default Risk competition. The challenge here (as always!) is to try and match the performance of the LightGBM/XGBoost classifiers which always seems tricky for NNs for this kind of problem.

A lot of the feature engineering going into the model is from my previous kernel [here](https://www.kaggle.com/shep312/lightgbm-with-weighted-averages-dropout-771), so I will focus more on the NN graph development here.

### Contents

1. [Load and process data](#load)
    1. [Check nulls](#nulls)
    2. [Identify categoricals](#cats)
    3. [Scaling](#scale)
2. [Building the graph](#graph)
3. [Training the NN](#train)
4. [Analysis and submission](#submit)",0,shep312,deep-learning-in-tf-with-upsampling-lb-758,shep312_deep-learning-in-tf-with-upsampling-lb-758_m0
"## 1. Load and process data <a name=""load""></a>

First step is to load all the different .csvs into memory.",1,shep312,deep-learning-in-tf-with-upsampling-lb-758,shep312_deep-learning-in-tf-with-upsampling-lb-758_m1
Since they are in disparate .csv's next I need to merge them into a single use-able dataframe:,2,shep312,deep-learning-in-tf-with-upsampling-lb-758,shep312_deep-learning-in-tf-with-upsampling-lb-758_m2
"Before I do any futher processing, extract the target variable for training later.",3,shep312,deep-learning-in-tf-with-upsampling-lb-758,shep312_deep-learning-in-tf-with-upsampling-lb-758_m3
"### 1.1 Check nulls <a name=""nulls""></a>

The data set has a a few variables containing a lot of nulls. Drop any features that are over x% null, then fill with 0. This obviously isn't a great method and provides room for improvement later on.",4,shep312,deep-learning-in-tf-with-upsampling-lb-758,shep312_deep-learning-in-tf-with-upsampling-lb-758_m4
"### 1.2 Identify categorical variables <a name=""cats""></a>

Categorical variables will be important in this model - there are a lot of them and a few that have high cardinality. 

I will be creating embeddings to encode them for use in the model later, but for now just make a note of them and their positions.",5,shep312,deep-learning-in-tf-with-upsampling-lb-758,shep312_deep-learning-in-tf-with-upsampling-lb-758_m5
"### 1.3 Scaling <a name=""scale""></a>

Next data processing step is to scale the features so they don't get unfairly weighted against each other.",6,shep312,deep-learning-in-tf-with-upsampling-lb-758,shep312_deep-learning-in-tf-with-upsampling-lb-758_m6
"## 2. Building the graph <a name=""graph""></a>

Now the data is in decent shape, build the NN. 

First step, however, is to re-separate the competition train and test sets. I'll then further split the training set down to provide a hold out validation set.",7,shep312,deep-learning-in-tf-with-upsampling-lb-758,shep312_deep-learning-in-tf-with-upsampling-lb-758_m7
Set the parameters for the network:,8,shep312,deep-learning-in-tf-with-upsampling-lb-758,shep312_deep-learning-in-tf-with-upsampling-lb-758_m8
Graph itself. Note that there is an embedding step first where each categorical feature is embedded and attached to the continuous input features.,9,shep312,deep-learning-in-tf-with-upsampling-lb-758,shep312_deep-learning-in-tf-with-upsampling-lb-758_m9
"## 3. Training the NN <a name=""train""></a>

Time to train the network. We know that only 8% of the targets are positive, so I will be upsampling positives for the gradient descent batches. I'm going to go with even representation in the training batches, but this isn't necessarily going to get the best score so may need optimising.",10,shep312,deep-learning-in-tf-with-upsampling-lb-758,shep312_deep-learning-in-tf-with-upsampling-lb-758_m10
"So an OK performance, but not matching the gradient boosted results yet. There's a lot of avenues to improve, however, including but not limited to:

- **Optimising number of nodes / width**: Currently I've only used a small number of nodes in the hidden layers, limiting the complexity of the model. More nodes should lead to better performance, but I found that it started to overfit. There is probably a way to regularise the network to allow it to get more complex
- **Number of layers**: Currently at 3 hidden layers deep, could be a better number
- **Upsampling of positives**: Reduction in the upsampling of postive samples could be tuned
- **Other hyperparameters**: Learning rate, batch size etc. could be tuned",11,shep312,deep-learning-in-tf-with-upsampling-lb-758,shep312_deep-learning-in-tf-with-upsampling-lb-758_m11
"## 4. Analysis and Submission <a name=""submit""></a>

Lets have a look at some summarising plots, then submit the results. 

Training curves & the ROC curve:",12,shep312,deep-learning-in-tf-with-upsampling-lb-758,shep312_deep-learning-in-tf-with-upsampling-lb-758_m12
...and a precision-recall curve and the confusion matrix:,13,shep312,deep-learning-in-tf-with-upsampling-lb-758,shep312_deep-learning-in-tf-with-upsampling-lb-758_m13
"# Home Credit Default Risk - Exploration + Baseline Model

Many people struggle to get loans due to insufficient or non-existent credit histories. And, unfortunately, this population is often taken advantage of by untrustworthy lenders. Home Credit strives to broaden financial inclusion for the unbanked population by providing a positive and safe borrowing experience. In order to make sure this underserved population has a positive loan experience, Home Credit makes use of a variety of alternative data--including telco and transactional information--to predict their clients' repayment abilities.

While Home Credit is currently using various statistical and machine learning methods to make these predictions, they're challenging Kagglers to help them unlock the full potential of their data. Doing so will ensure that clients capable of repayment are not rejected and that loans are given with a principal, maturity, and repayment calendar that will empower their clients to be successful.

This is a simple notebook on exploration and baseline model of home credit default risk data 

**Contents**   
[1. Dataset Preparation](#1)    
[2. Exploration - Applications Train](#2)  
&nbsp;&nbsp;&nbsp;&nbsp; [2.1 Snapshot - Application Train](#2.1)    
&nbsp;&nbsp;&nbsp;&nbsp; [2.2 Distribution of Target Variable](#2.2)    
&nbsp;&nbsp;&nbsp;&nbsp; [2.3 Applicant's Gender Type](#2.3)    
&nbsp;&nbsp;&nbsp;&nbsp; [2.4 Family Status of Applicants who takes the loan](#2.4)  
&nbsp;&nbsp;&nbsp;&nbsp; [2.5 Does applicants own Real Estate or Car](#2.5)    
&nbsp;&nbsp;&nbsp;&nbsp; [2.6 Suite Type and Income Type of Applicants](#2.6)   
&nbsp;&nbsp;&nbsp;&nbsp; [2.7 Applicants Contract Type](#2.7)   
&nbsp;&nbsp;&nbsp;&nbsp; [2.8 Education Type and Occupation Type](#2.8)   
&nbsp;&nbsp;&nbsp;&nbsp; [2.9 Organization Type and Occupation Type](#2.9)   
&nbsp;&nbsp;&nbsp;&nbsp; [2.10 Walls Material, Foundation and House Type](#2.10)   
&nbsp;&nbsp;&nbsp;&nbsp; [2.11 Amount Credit Distribution](#2.11)    
&nbsp;&nbsp;&nbsp;&nbsp; [2.12 Amount Annuity Distribution - Distribution](#2.12)  
&nbsp;&nbsp;&nbsp;&nbsp; [2.13 Amount Goods Price - Distribution](#2.13)   
&nbsp;&nbsp;&nbsp;&nbsp; [2.14 Amount Region Population Relative](#2.14)    
&nbsp;&nbsp;&nbsp;&nbsp; [2.15 Days Birth - Distribution](#2.15)   
&nbsp;&nbsp;&nbsp;&nbsp; [2.16 Days Employed - Distribution](#2.16)    
&nbsp;&nbsp;&nbsp;&nbsp; [2.17 Distribution of Num Days Registration](#2.17)  
&nbsp;&nbsp;&nbsp;&nbsp; [2.18 Applicants Number of Family Members](#2.18)  
&nbsp;&nbsp;&nbsp;&nbsp; [2.19 Applicants Number of Children](#2.19)  
[3. Exploration - Bureau Data](#3)  
&nbsp;&nbsp;&nbsp;&nbsp; [3.1 Snapshot - Bureau Data](#3)    
[4. Exploration - Bureau Balance Data](#4)  
&nbsp;&nbsp;&nbsp;&nbsp; [4.1 Snapshot - Bureau Balance Data](#3)     
[5. Exploration - Credit Card Balance Data](#5)   
&nbsp;&nbsp;&nbsp;&nbsp; [5.1 Snapshot - Credit Card Balance Data](#3)   
[6. Exploration - POS Cash Balance Data](#6)   
&nbsp;&nbsp;&nbsp;&nbsp; [6.1 Snapshot - POS Cash Balance Data](#3)   
[7. Exploration - Previous Application Data](#7)   
&nbsp;&nbsp;&nbsp;&nbsp; [7.1 Snapshot - Previous Application Data](#7.1)  
&nbsp;&nbsp;&nbsp;&nbsp; [7.2 Contract Status Distribution - Previous Applications](#7.2)  
&nbsp;&nbsp;&nbsp;&nbsp; [7.3 Suite Type Distribution - Previous Application](#7.3)    
&nbsp;&nbsp;&nbsp;&nbsp; [7.4 Client Type Distribution  - Previous Application](#7.4)    
&nbsp;&nbsp;&nbsp;&nbsp; [7.5 Channel Type Distribution - Previous Applications](#7.5)  
[8. Exploration - Installation Payments](#8)  
&nbsp;&nbsp;&nbsp;&nbsp; [8.1 Snapshot of Installation Payments](#3)  
[9. Baseline Model](#9)  
&nbsp;&nbsp;&nbsp;&nbsp; [9.1 Dataset Preparation](#9.1)  
&nbsp;&nbsp;&nbsp;&nbsp; [9.2 Handelling Categorical Features](#9.2)     
&nbsp;&nbsp;&nbsp;&nbsp; [9.3 Create Flat Dataset](#9.3)     
&nbsp;&nbsp;&nbsp;&nbsp; [9.4 Validation Sets Preparation](#9.4)    
&nbsp;&nbsp;&nbsp;&nbsp; [9.5 Model Fitting](#9.5)    
&nbsp;&nbsp;&nbsp;&nbsp; [9.6 Feature Importance](#9.6)    
&nbsp;&nbsp;&nbsp;&nbsp; [9.7 Prediction](#9.7)   



## <a id=""1"">1. Dataset Preparation </a>",0,shivamb,homecreditrisk-extensive-eda-baseline-0-772,shivamb_homecreditrisk-extensive-eda-baseline-0-772_m0
"## <a href=""2"">2. Exploration of Applications Data </a>

### <a href=""2.1"">2.1 Snapshot of Application Train</a>

Application data consists of static data for all applications and every row represents one loan.",1,shivamb,homecreditrisk-extensive-eda-baseline-0-772,shivamb_homecreditrisk-extensive-eda-baseline-0-772_m1
"> There are total 307,511 rows which contains the information of loans and there are 122 variables. 

> The target variable defines if the client had payment difficulties meaning he/she had late payment more than X days on at least one of the first Y installments of the loan. Such case is marked as 1 while other all other cases as 0.

### <a href=""2.2"">2.2 Distribution of Target Variable </a>

The target variable defines weather the loan was repayed or not. Let us look at what is the distribution of loan repayment in the training dataset. ",2,shivamb,homecreditrisk-extensive-eda-baseline-0-772,shivamb_homecreditrisk-extensive-eda-baseline-0-772_m2
"> The target variable is slightly imbalance with the majority of loans has the target equals to 0 which indicates that individuals did not had any problems in paying installments in given time. There are about 91% loans which is equal to about 282K with target = 0, While only 9% of the total loans (about 24K applicants) in this dataset involved the applicants having problems in repaying the loan / making installments.  

### <a href=""2.3"">2.3 Gender Type of Applicants </a>",3,shivamb,homecreditrisk-extensive-eda-baseline-0-772,shivamb_homecreditrisk-extensive-eda-baseline-0-772_m3
"> In the applicant's data women have applied for a larger majority of loans which is almost the double as the men. In total, there are about 202,448 loan applications filed by females in contrast to about 105,059 applications filed by males. However, a larger percentage (about 10% of the total) of men had the problems in paying the loan or making installments within time as compared to women applicants (about 7%). 

### <a href=""2.4"">2.4 Family Status of Applicants </a>",4,shivamb,homecreditrisk-extensive-eda-baseline-0-772,shivamb_homecreditrisk-extensive-eda-baseline-0-772_m4
"> Married people have applied for a larger number of loan applications about 196K, However, people having Civil Marriage has the highest percentage (about 10%) of loan problems and challenges. 

### <a href=""2.5"">2.5. Does applicants own Real Estate or Car ?</a>",5,shivamb,homecreditrisk-extensive-eda-baseline-0-772,shivamb_homecreditrisk-extensive-eda-baseline-0-772_m5
"> About 70% of the applicants own Real Estate, while only 34% of applicants own Car who had applied for the loan in the past years. However, a higher percentage of people having payment difficulties was observed with applicants which did not owned Car or which did not owned Real Estate. 

### <a href=""2.6"">2.6 Suite Type and Income Type of Applicants </a>",6,shivamb,homecreditrisk-extensive-eda-baseline-0-772,shivamb_homecreditrisk-extensive-eda-baseline-0-772_m6
"> Top 3 Type Suites which applies for loan are the houses which are:  
    - Unaccompanined (about 248K applicants) 
    - Family (about 40K applicants)  
    - Spouse, partner (about 11K applicants)    
> The income type of people who applies for loan include about 8 categroes, top ones are : 
    - Working Class (158K)
    - Commercial Associate (71K)
    - Pensiner (55K)

### <a id=""2.6.1"">2.6.1 How does Target Varies with Suite and Income Type of Applicants </a>",7,shivamb,homecreditrisk-extensive-eda-baseline-0-772,shivamb_homecreditrisk-extensive-eda-baseline-0-772_m7
"> We see that Applicants having Income Types : Maternity Leaves and UnEmployed has the highest percentage (about 40% and 36% approx) of Target = 1 ie. having more payment problems, while Pensioners have the least (about 5.3%). 

### <a id=""2.7"">2.7. Applicant's Contract Type</a>",8,shivamb,homecreditrisk-extensive-eda-baseline-0-772,shivamb_homecreditrisk-extensive-eda-baseline-0-772_m8
"> Cash loans with about 278K loans contributes to a majorty of total lonas in this dataset. Revolving loans has significantly lesser number equal to about 29K as compared to Cash loans. 

### <a id=""2.8"">2.8 Education Type and Housing Type </a>",9,shivamb,homecreditrisk-extensive-eda-baseline-0-772,shivamb_homecreditrisk-extensive-eda-baseline-0-772_m9
"> A large number of applications (218K) are filed by people having secondary education followed by people with Higher Education with 75K applications. Applicants living in House / apartments has the highest number of loan apllications equal to 272K. While we see that the applicants with Lower Secondary education status has the highest percentage of payment related problems. Also, Applicants living in apartments or living with parents also shows the same trend. ",10,shivamb,homecreditrisk-extensive-eda-baseline-0-772,shivamb_homecreditrisk-extensive-eda-baseline-0-772_m10
"### <a id=""2.9"">2.9. Which Organization and Occupation Type applies for loan and which repays </a>",11,shivamb,homecreditrisk-extensive-eda-baseline-0-772,shivamb_homecreditrisk-extensive-eda-baseline-0-772_m11
"> Top Applicant's who applied for loan : Laborers - Approx 55 K, Sales Staff - Approx 32 K, Core staff - Approx 28 K. Entity Type 3 type organizations have filed maximum number of loans equal to approx 67K

### <a id=""2.9.1"">2.9.1 Target Variable with respect to Organization and Occupation Type </a>",12,shivamb,homecreditrisk-extensive-eda-baseline-0-772,shivamb_homecreditrisk-extensive-eda-baseline-0-772_m12
"### <a id=""2.10"">2.10 Walls Material, Foundation, and House Type </a>",13,shivamb,homecreditrisk-extensive-eda-baseline-0-772,shivamb_homecreditrisk-extensive-eda-baseline-0-772_m13
"> - ""Blocks and Flats"" related house types have filed the largest number of loan applications equal to about 150K, rest of the other categories : Specific Housing and Terraced house have less than 1500 applications. Similarly houses having Panel and Stone Brick type walls material have filed the largest applciations close to 120K combined. 

### <a id=""2.10.1"">2.10.1 Target Variable with respect to Walls Material, Fondkappremont, House Type </a>",14,shivamb,homecreditrisk-extensive-eda-baseline-0-772,shivamb_homecreditrisk-extensive-eda-baseline-0-772_m14
"### <a id=""2.11"">2.11. Distribution of Amount Credit </a>",15,shivamb,homecreditrisk-extensive-eda-baseline-0-772,shivamb_homecreditrisk-extensive-eda-baseline-0-772_m15
"### <a id=""2.12"">2.12 Distribution of Amount AMT_ANNUITY </a>",16,shivamb,homecreditrisk-extensive-eda-baseline-0-772,shivamb_homecreditrisk-extensive-eda-baseline-0-772_m16
"### <a id=""2.13"">2.13 Distribution of Amount AMT_GOODS_PRICE </a>",17,shivamb,homecreditrisk-extensive-eda-baseline-0-772,shivamb_homecreditrisk-extensive-eda-baseline-0-772_m17
"### <a id=""2.14"">2.14 Distribution of Amount REGION_POPULATION_RELATIVE </a>",18,shivamb,homecreditrisk-extensive-eda-baseline-0-772,shivamb_homecreditrisk-extensive-eda-baseline-0-772_m18
"### <a id=""2.15"">2.15 Distribution of Amount DAYS_BIRTH </a>",19,shivamb,homecreditrisk-extensive-eda-baseline-0-772,shivamb_homecreditrisk-extensive-eda-baseline-0-772_m19
"### <a id=""2.16"">2.16 Distribution of Amount DAYS_EMPLOYED </a>",20,shivamb,homecreditrisk-extensive-eda-baseline-0-772,shivamb_homecreditrisk-extensive-eda-baseline-0-772_m20
"### <a id=""2.17"">2.17 Distribution of Number of Days for Registration</a>",21,shivamb,homecreditrisk-extensive-eda-baseline-0-772,shivamb_homecreditrisk-extensive-eda-baseline-0-772_m21
"### <a id=""2.18"">2.18 How many Family Members does the applicants has </a>",22,shivamb,homecreditrisk-extensive-eda-baseline-0-772,shivamb_homecreditrisk-extensive-eda-baseline-0-772_m22
"> Most of the applicants who applied for loan had 2 family members in total

### <a id=""2.19""> 2.19 How many Children does the applicants have </a>",23,shivamb,homecreditrisk-extensive-eda-baseline-0-772,shivamb_homecreditrisk-extensive-eda-baseline-0-772_m23
"> A large majority of applicants did not had children when they applied for loan

## <a id=""3"">3. Exploration of Bureau Data</a>

All client's previous credits provided by other financial institutions that were reported to Credit Bureau (for clients who have a loan in our sample). For every loan in our sample, there are as many rows as number of credits the client had in Credit Bureau before the application date.

### <a id=""3.1"">3.1 Snapshot of Bureau Data</a>",24,shivamb,homecreditrisk-extensive-eda-baseline-0-772,shivamb_homecreditrisk-extensive-eda-baseline-0-772_m24
"## <a id=""4"">4. Exploration of Bureau Balance Data</a>

Monthly balances of previous credits in Credit Bureau. This table has one row for each month of history of every previous credit reported to Credit Bureau – i.e the table has (#loans in sample * # of relative previous credits * # of months where we have some history observable for the previous credits) rows.

### <a id=""4.1"">4.1 Snapshot of Bureau Balance Data</a>",25,shivamb,homecreditrisk-extensive-eda-baseline-0-772,shivamb_homecreditrisk-extensive-eda-baseline-0-772_m25
"## <a id=""5"">5. Exploration of Credit Card Balance</a>

Monthly balance snapshots of previous credit cards that the applicant has with Home Credit.This table has one row for each month of history of every previous credit in Home Credit (consumer credit and cash loans) related to loans in our sample – i.e. the table has (#loans in sample * # of relative previous credit cards * # of months where we have some history observable for the previous credit card) rows.

### <a id=""5.1"">5.1 Snapshot of Credit Card Balance</a>",26,shivamb,homecreditrisk-extensive-eda-baseline-0-772,shivamb_homecreditrisk-extensive-eda-baseline-0-772_m26
"## <a id=""6"">6. Exploration of POS CASH Balance Data</a>

Monthly balance snapshots of previous POS (point of sales) and cash loans that the applicant had with Home Credit. This table has one row for each month of history of every previous credit in Home Credit (consumer credit and cash loans) related to loans in our sample – i.e. the table has (#loans in sample * # of relative previous credits * # of months in which we have some history observable for the previous credits) rows.

### <a id=""6.1"">6.1 Snapshot of POS CASH Balance Data</a>",27,shivamb,homecreditrisk-extensive-eda-baseline-0-772,shivamb_homecreditrisk-extensive-eda-baseline-0-772_m27
"## <a id=""7"">7. Exploration of Prev Application</a>

### <a id=""7.1"">7.1 Snapshot of Prev Application</a>",28,shivamb,homecreditrisk-extensive-eda-baseline-0-772,shivamb_homecreditrisk-extensive-eda-baseline-0-772_m28
"### <a id=""7.2"">7.2 Contract Status Distribution in Previously Filed Applications</a>",29,shivamb,homecreditrisk-extensive-eda-baseline-0-772,shivamb_homecreditrisk-extensive-eda-baseline-0-772_m29
"> - A large number of people (about 62%) had their previous applications approved, while about 19% of them had cancelled and other 17% were resued. 

### <a id=""7.3"">7.3 Suite Type Distribution of Previous Applications</a>",30,shivamb,homecreditrisk-extensive-eda-baseline-0-772,shivamb_homecreditrisk-extensive-eda-baseline-0-772_m30
"> - A majority of applicants had previous applications having Unaccompanied Suite Type (about 60%) followed by Family related suite type (about 25%)

### <a id=""7.4"">7.4 Client Type of Previous Applications</a>",31,shivamb,homecreditrisk-extensive-eda-baseline-0-772,shivamb_homecreditrisk-extensive-eda-baseline-0-772_m31
"> - About 74% of the previous applications were Repeater Clients, while only 18% are new. About 8% are refreshed. 

### <a id=""7.5"">7.5 Channel Type - Previous Applications </a>",32,shivamb,homecreditrisk-extensive-eda-baseline-0-772,shivamb_homecreditrisk-extensive-eda-baseline-0-772_m32
"## <a id=""8"">8. Exploration of Installation Payments </a>
### <a id=""8.1"">8.1 Snapshot of Installation Payments </a>",33,shivamb,homecreditrisk-extensive-eda-baseline-0-772,shivamb_homecreditrisk-extensive-eda-baseline-0-772_m33
"## <a id=""9"">9. Baseline Model </a>

### <a id=""9.1"">9.1 Dataset Preparation</a>",34,shivamb,homecreditrisk-extensive-eda-baseline-0-772,shivamb_homecreditrisk-extensive-eda-baseline-0-772_m34
"### <a id=""9.2"">9.2 Handelling Categorical Features</a>",35,shivamb,homecreditrisk-extensive-eda-baseline-0-772,shivamb_homecreditrisk-extensive-eda-baseline-0-772_m35
"### <a id=""9.3"">9.3 Feature Engineering</a>",36,shivamb,homecreditrisk-extensive-eda-baseline-0-772,shivamb_homecreditrisk-extensive-eda-baseline-0-772_m36
"### <a id=""9.3.1"">9.3.1 Feature Engineering - Previous Applications</a>

Credits to excellent kernel shared by Olivier for more ideas: https://www.kaggle.com/ogrellier/good-fun-with-ligthgbm 
",37,shivamb,homecreditrisk-extensive-eda-baseline-0-772,shivamb_homecreditrisk-extensive-eda-baseline-0-772_m37
"### <a id=""9.3.2"">9.3.2 Feature Engineering - Bureau Data</a>",38,shivamb,homecreditrisk-extensive-eda-baseline-0-772,shivamb_homecreditrisk-extensive-eda-baseline-0-772_m38
"### <a id=""9.3.3"">9.3.3 Feature Engineering - Previous Installments</a>",39,shivamb,homecreditrisk-extensive-eda-baseline-0-772,shivamb_homecreditrisk-extensive-eda-baseline-0-772_m39
"### <a id=""9.3.4"">9.3.4 Feature Engineering - Pos Cash Balance</a>",40,shivamb,homecreditrisk-extensive-eda-baseline-0-772,shivamb_homecreditrisk-extensive-eda-baseline-0-772_m40
"### <a id=""9.3.5"">9.3.5 Feature Engineering - Credit Card Balance </a>",41,shivamb,homecreditrisk-extensive-eda-baseline-0-772,shivamb_homecreditrisk-extensive-eda-baseline-0-772_m41
"### <a id=""9.3.6"">9.3.6 Prepare Final Train and Test data</a>",42,shivamb,homecreditrisk-extensive-eda-baseline-0-772,shivamb_homecreditrisk-extensive-eda-baseline-0-772_m42
"### <a id=""9.4"">9.4 Create Validation Sets</a>",43,shivamb,homecreditrisk-extensive-eda-baseline-0-772,shivamb_homecreditrisk-extensive-eda-baseline-0-772_m43
"### <a id=""9.5"">9.5 Fit the Model</a>",44,shivamb,homecreditrisk-extensive-eda-baseline-0-772,shivamb_homecreditrisk-extensive-eda-baseline-0-772_m44
"### <a id=""9.6"">9.6 Feature Importance </a>",45,shivamb,homecreditrisk-extensive-eda-baseline-0-772,shivamb_homecreditrisk-extensive-eda-baseline-0-772_m45
"### <a id=""9.7"">9.7 Predict</a>",46,shivamb,homecreditrisk-extensive-eda-baseline-0-772,shivamb_homecreditrisk-extensive-eda-baseline-0-772_m46
Thanks for viewing. ,47,shivamb,homecreditrisk-extensive-eda-baseline-0-772,shivamb_homecreditrisk-extensive-eda-baseline-0-772_m47
"# Universal Blender
## Part 2. Potions 101.

Inspired by Henk van Veen 
https://www.kdnuggets.com/2015/06/ensembles-kaggle-data-science-competition-p3.html

In this part I go by the book and try all the blending techniques mentioned by Henk. This part shows that stacking and blending offers a lot of ways to be creative and overfit. :) I think I might have made a mistake when chosing holdout, so I have not achieved a higher score on test, despite the good results on cross validation. Try it on your own, maybe you will be luckier than me. If you spot a mistake in the code, I will much appreciate if you point it out in the comments.",0,sibmike,blender-part-2-potions-101,sibmike_blender-part-2-potions-101_m0
"### Data load
Note we won't touch train set at all. Only holdout and test sets.",1,sibmike,blender-part-2-potions-101,sibmike_blender-part-2-potions-101_m1
First check and drop any overcorrelated results:,2,sibmike,blender-part-2-potions-101,sibmike_blender-part-2-potions-101_m2
Let's drop lightGBM:,3,sibmike,blender-part-2-potions-101,sibmike_blender-part-2-potions-101_m3
"## Avergaing
Simply average all probabilities, in this case a single confident classifier may overrun several non-confident classifiers.",4,sibmike,blender-part-2-potions-101,sibmike_blender-part-2-potions-101_m4
That has not gone well; pass.,5,sibmike,blender-part-2-potions-101,sibmike_blender-part-2-potions-101_m5
"## Weighted vote
Same as linear regression with constraint on weights >0.",6,sibmike,blender-part-2-potions-101,sibmike_blender-part-2-potions-101_m6
"Better than sole xgb, but not for the leader board.",7,sibmike,blender-part-2-potions-101,sibmike_blender-part-2-potions-101_m7
"## Ranked logistic regression
Here we do two things: 
1. substitute rank for probability in every column
2. train simple logistic regression based on the ranks",8,sibmike,blender-part-2-potions-101,sibmike_blender-part-2-potions-101_m8
"**Once again we'll snoop the test, to get a better score on LB.** Clean solution would rank holdout set, train on it. Then match values from test set to ranks in holdout and use them to predict target. ",9,sibmike,blender-part-2-potions-101,sibmike_blender-part-2-potions-101_m9
Worse than xgb.,10,sibmike,blender-part-2-potions-101,sibmike_blender-part-2-potions-101_m10
"## Quadratic linear stacking
Same as above, but we add quadratic features of the ranked dataset:",11,sibmike,blender-part-2-potions-101,sibmike_blender-part-2-potions-101_m11
Awful!,12,sibmike,blender-part-2-potions-101,sibmike_blender-part-2-potions-101_m12
"## Logistic regression with ranks and PCA
1. Use PCA to reduce dimension of original holdout and test data
2. add the reduced dimensions to the ranked datesets,
3. train simple logistic regression based on the ranks + reduced dimensions.",13,sibmike,blender-part-2-potions-101,sibmike_blender-part-2-potions-101_m13
**Lorgistic regression with linear stretch on pca + rank features:**,14,sibmike,blender-part-2-potions-101,sibmike_blender-part-2-potions-101_m14
"**That is somewhat promising, but can we do better?
**Actually scored 0.788**",15,sibmike,blender-part-2-potions-101,sibmike_blender-part-2-potions-101_m15
"## Feature-Weighted Linear Stacking
https://arxiv.org/pdf/0911.0460.pdf

As far as I understand it is what it sounds like. Generate meta features for the dataset, then multiply them by predictions. And build linear regression on top of it.",16,sibmike,blender-part-2-potions-101,sibmike_blender-part-2-potions-101_m16
Nope.,17,sibmike,blender-part-2-potions-101,sibmike_blender-part-2-potions-101_m17
## LightGBM with ranks and PCA,18,sibmike,blender-part-2-potions-101,sibmike_blender-part-2-potions-101_m18
"That is very promising, but actually it just overfits and scores 0.788. Ok, as we deal with overfit, lets try to train a bunch of classifiers and then train logistic regreesion on their outcome. As we can only use holdout set, we will have to find weights with CV.",19,sibmike,blender-part-2-potions-101,sibmike_blender-part-2-potions-101_m19
## Dragons,20,sibmike,blender-part-2-potions-101,sibmike_blender-part-2-potions-101_m20
"""Here be dragons. With 7 heads. Standing on top of 30 other dragons."" Henk van Veen ",21,sibmike,blender-part-2-potions-101,sibmike_blender-part-2-potions-101_m21
"Let’s say you want to do 2-fold stacking:

* Split the train set in 2 parts: train_a and train_b
* Fit a first-stage model on train_a and create predictions for train_b
* Fit the same model on train_b and create predictions for train_a
* Finally fit the model on the entire train set and create predictions for the test set.
Now train a second-stage stacker model on the probabilities from the first-stage model(s) (using CV).",22,sibmike,blender-part-2-potions-101,sibmike_blender-part-2-potions-101_m22
"**Yet again it scored 0.788.** That is dissapointing. Perhaps, there is a flaw in implementation. For instance, I finish by using lgbm only. Maybe, that is not the best choise and a bunch of weaker estimators have to be blended/stacked instead. I also suspect that the hold out is not representative of the test set. The fact that the score is stuck at 0.788 hints that there is a simple mistake somewhere. Anyway, I hope this will serve as starter guide for creative ensamblimg and blending. Have fun and good luck! I'll go and retrain the whole damn thing, yet again; with different holdout.

PS: One other thing, that I is worth trying is to train a blender on the validation sets in the models training loop. This will make the code more cumbersome, but allow to train models on the whole set without need for holdout.",23,sibmike,blender-part-2-potions-101,sibmike_blender-part-2-potions-101_m23
"# Universal Blender: XGB+CatB+LGB
## Part 1. Staging.
This is an attempt to build a universal blender frame, that collects holdout, crossval and train predictions, so that more advanced stacking and blending techinques can be used. 

This realisation prepares stage to blend Ridge, DNN, XGB, CatBoost and LGBM, but you can add or drop any models you want. The code could have been more elegant with a estimator class, but it is not :). If you know how to make it better, please share.

It also includes Data Builder function with memory optimisation that decreases memory usage by 70%. The optimized pickle dump can be used instead of building the dataframe from scratch.
The kernel is submitted in debug mode. State debag = False in oof_regression_stacker to get real results. Chose number of folds carefully as XGBoost and CatBoost take forever to train.

**Note:** Dataloader applies MinMaxscaler on the dataset that includes train and test data, therefore a leak from test to train occures. It is beneficial for leaderboard score, but should be avoided in real projects. To get a real life example how it can mess up your results please watch Caltech lecture, Puzzle 4, 52:00 : https://www.youtube.com/watch?v=EZBUDG12Nr0&index=17&list=PLD63A284B7615313A",0,sibmike,blender-xgb-catb-lgb,sibmike_blender-xgb-catb-lgb_m0
"## Blending:
### Estimators:
#### Ridge regression",1,sibmike,blender-xgb-catb-lgb,sibmike_blender-xgb-catb-lgb_m1
"#### Simple DNN
Please thank its author:
https://www.kaggle.com/tottenham/10-fold-simple-dnn-with-rank-gauss

works surprisingly fast.",2,sibmike,blender-xgb-catb-lgb,sibmike_blender-xgb-catb-lgb_m2
"#### LightGBM
the best of the batch, fast and convenient to use.",3,sibmike,blender-xgb-catb-lgb,sibmike_blender-xgb-catb-lgb_m3
"#### XGBoost 
has a nasty feature that it takes only DMatrix as arguments therefore predict method has to be wrapend into a function.",4,sibmike,blender-xgb-catb-lgb,sibmike_blender-xgb-catb-lgb_m4
"#### Catboost
watch out, it has predict and predict_proba methods. predict_proba should be used; it returns 2d array, that has to be flattened.",5,sibmike,blender-xgb-catb-lgb,sibmike_blender-xgb-catb-lgb_m5
"##  Data part
### Data loader:",6,sibmike,blender-xgb-catb-lgb,sibmike_blender-xgb-catb-lgb_m6
"## Data Builder:
### Service functions:",7,sibmike,blender-xgb-catb-lgb,sibmike_blender-xgb-catb-lgb_m7
"### Memory reducer:
Thanks **You Guillaume Martin** for the Awesome Memory Optimizer!
https://www.kaggle.com/gemartin/load-data-reduce-memory-usage",8,sibmike,blender-xgb-catb-lgb,sibmike_blender-xgb-catb-lgb_m8
"### Data builder function:
From lighgbm-with-selected-features",9,sibmike,blender-xgb-catb-lgb,sibmike_blender-xgb-catb-lgb_m9
## Collecting data:,10,sibmike,blender-xgb-catb-lgb,sibmike_blender-xgb-catb-lgb_m10
# Blender call:,11,sibmike,blender-xgb-catb-lgb,sibmike_blender-xgb-catb-lgb_m11
"# Interpreting a LightGBM model

This notebook explores the patterns identified by a straightforward LightGBM model. Many thanks to Olivier and the Good_fun_with_LigthGBM kernel where all the LightGBM training code is from. This notebook is just meant to extend that kernel and examine it using individualized feature importances. You will need to scroll a bit to get to the pretty pictures :)",0,slundberg,interpreting-a-lightgbm-model,slundberg_interpreting-a-lightgbm-model_m0
## Build the dataset,1,slundberg,interpreting-a-lightgbm-model,slundberg_interpreting-a-lightgbm-model_m1
"## Train the LightGBM model

Here we just use a simple train/validation split.",2,slundberg,interpreting-a-lightgbm-model,slundberg_interpreting-a-lightgbm-model_m2
"# Explain the model

SHAP values are fair allocation of credit among the features and have theoretical garuntees about consistency from game theory (so you can trust them). There is a high speed algorithm to compute SHAP values for LightGBM (and XGBoost and CatBoost), so they are particularly helpful when interpreting predictions from gradient boosting tree models. While typical feature importances are for the whole dataset (and often [have consistency problems](https://towardsdatascience.com/interpretable-machine-learning-with-xgboost-9ec80d148d27) these values are computed for every single prediction, which opens up new ways to understand the model. For more details check out the [docs](https://github.com/slundberg/shap).**",3,slundberg,interpreting-a-lightgbm-model,slundberg_interpreting-a-lightgbm-model_m3
### Summarize the feature imporances with a bar chart,4,slundberg,interpreting-a-lightgbm-model,slundberg_interpreting-a-lightgbm-model_m4
"### Summarize the feature importances with a density scatter plot

Just looking a single number hides a lot of information about the model. So instead we plot the SHAP values for each feature for every sample on the x-axis and then let them pil up when there is not space to show density. If we then color each dot by the value of the original feature we can see how a low feature value or high feature value effects the model output. For example EXT_SOURCE_2 is the most important feature (because they are sorted by mean abolute SHAP value) and the red colored dots (samples) have low SHAP values with the blue colored dots have high SHAP values. Since for a logistic regression model the SHAP values are in log odds, this color spread means low values of EXT_SOURCE_2 raise the log odds output of the model significantly.

Note that some features like SK_DPD_DEF are not important for most people, but have a very large impact for a subset of the people in the data set. This highlights how a globally important feature is not nessecarrily the most importance feature for each person.",5,slundberg,interpreting-a-lightgbm-model,slundberg_interpreting-a-lightgbm-model_m5
"### Investigate the dependence of the model on each feature

The summary plot above gives a lot of information. But we can learn more by examining a single feature and how it impacts the model's predicition across all the samples. To do this we plot the SHAP value for a feature on the y-axis and the original value of the feature on the x-axis. Doing this for all samples for EXT_SOURCE_2 shows a fairly linear relationship that flattens near high values. Note that the impact of EXT_SOURCE_2 on the model output is different for different people that have the same value of EXT_SOURCE_2, as shown by the vertical spread of the dots at a single point on the x axis. This is because of interaction effects (and would disappear if the trees were all depth 1 in the LightGBM model). To highlight what interactions may be driving this vertical spread shap colors the dots by another feature that seems to explain some of the dispersion (see the docs for more details on getting the exact interaction effects). For EXT_SOURCE_2 we see gender seems to have some effect.",6,slundberg,interpreting-a-lightgbm-model,slundberg_interpreting-a-lightgbm-model_m6
"## Plot the SHAP dependence plots for the top 20 features

Here you can scroll through the top 20 features and see lots of interesting patterns and interactions. Like how having high EXT_SOURCE_1 is even more important if you also have a small DAYS_BIRTH magnitude (see the EXT_SOURCE_1 plot).",7,slundberg,interpreting-a-lightgbm-model,slundberg_interpreting-a-lightgbm-model_m7
"**Before you begin please upvote the original authors. Its all there effort not mine.**
**Links to original kernels-->**
1.[Lightgbm with simple features by jsaguiar](http://www.kaggle.com/jsaguiar/lightgbm-with-simple-features-0-785-lb)
2.[tidy_xgb -all tables by kxx](http://www.kaggle.com/kailex/tidy-xgb-all-tables-0-782/code)",0,stardust0,simple-blending-788-lb,stardust0_simple-blending-788-lb_m0
"- <a href='#1'>Prepare</a>  
- <a href='#2'>Feature Selection</a>
    - <a href='#2-1'>1. Filter</a>
        - <a href='#2-1-1'>1.1 Pearson Correlation</a>
        - <a href='#2-1-2'>1.2 Chi-2</a>
    - <a href='#2-2'>2. Wrapper</a>
    - <a href='#2-3'>3. Embeded</a>
        - <a href='#2-3-1'>3.1 Logistics Regression L1</a>
        - <a href='#2-3-2'>3.2 Random Forest</a>
        - <a href='#2-3-3'>3.3 LightGBM</a>
- <a href='#3'>Summary</a>",0,sz8416,6-ways-for-feature-selection,sz8416_6-ways-for-feature-selection_m0
# <a id='1'>Prepare</a>,1,sz8416,6-ways-for-feature-selection,sz8416_6-ways-for-feature-selection_m1
### Stratified Sampling (ratio = 0.1),2,sz8416,6-ways-for-feature-selection,sz8416_6-ways-for-feature-selection_m2
### Impute missing values,3,sz8416,6-ways-for-feature-selection,sz8416_6-ways-for-feature-selection_m3
### Deal with Categorical features: OneHotEncoding,4,sz8416,6-ways-for-feature-selection,sz8416_6-ways-for-feature-selection_m4
### Feature matrix and target,5,sz8416,6-ways-for-feature-selection,sz8416_6-ways-for-feature-selection_m5
"# <a id='2'>Feature Selection</a>
- select **100** features from 226
- **xxx_support**: list to represent select this feature or not
- **xxx_feature**: the name of selected features",6,sz8416,6-ways-for-feature-selection,sz8416_6-ways-for-feature-selection_m6
"## <a id='2-1'>1 Filter</a>
- documentation for **SelectKBest**: http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html

###  <a id='2-1-1'>1.1 Pearson Correlation</a>
**Note**
- Normalization: no
- Impute missing values: yes",7,sz8416,6-ways-for-feature-selection,sz8416_6-ways-for-feature-selection_m7
"###  <a id='2-1-2'>1.2 Chi-2</a>

**Note**
- Normalization: MinMaxScaler (values should be bigger than 0)
- Impute missing values: yes",8,sz8416,6-ways-for-feature-selection,sz8416_6-ways-for-feature-selection_m8
"## <a id='2-2'>2. Wrapper</a>
- documentation for **RFE**: http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html

**Note**
- Normalization: depend on the used model; yes for LR
- Impute missing values: depend on the used model; yes for LR
",9,sz8416,6-ways-for-feature-selection,sz8416_6-ways-for-feature-selection_m9
"## <a id='2-3'>3. Embeded</a>
- documentation for **SelectFromModel**: http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel.html
###  <a id='2-3-1'>3.1 Logistics Regression L1</a>
**Note**
- Normalization: Yes
- Impute missing values: Yes",10,sz8416,6-ways-for-feature-selection,sz8416_6-ways-for-feature-selection_m10
"###  <a id='2-3-2'>3.2 Random Forest</a>
**Note**
- Normalization: No
- Impute missing values: Yes",11,sz8416,6-ways-for-feature-selection,sz8416_6-ways-for-feature-selection_m11
"###  <a id='2-3-3'>3.3 LightGBM</a>
**Note**
- Normalization: No
- Impute missing values: No",12,sz8416,6-ways-for-feature-selection,sz8416_6-ways-for-feature-selection_m12
# <a id='3'>Summary</a>,13,sz8416,6-ways-for-feature-selection,sz8416_6-ways-for-feature-selection_m13
"- <a href='#0'>0. Introduction</a>  
- <a href='#1'>1. Get the Data</a>
- <a href='#2'>2. Check the Data</a>
- <a href='#3'> 3. Explore the data</a>
    - <a href='#3-1'>3.1 Categorical features</a>
    - <a href='#3-2'>3.2 Numerical features</a>
    - <a href='#3-3'>3.3 Categorical features by label</a>
    - <a href='#3-4'>3.4 Numerical features by label</a>
    - <a href='#3-5'>3.5 Correlation Matrix</a>
- <a href='#4'> 4. A further exploration on application table</a>
    - <a href='#4-1'>4.1 Impute missing values</a>
    - <a href='#4-2'>4.2 Create more feature</a>
    - <a href='#4-3'>4.3 Train model</a>
    - <a href='#4-4'>4.4 Feature importance</a>
    - <a href='#4-5'>4.5 Prediction</a>",0,sz8416,eda-baseline-model-using-application,sz8416_eda-baseline-model-using-application_m0
## <a id='0'>0. Introduction</a>,1,sz8416,eda-baseline-model-using-application,sz8416_eda-baseline-model-using-application_m1
" [Home Credit](http://www.homecredit.net/[](http://) is an international non-bank financial institution founded in 1997 in the Czech Republic. The company operates in 14 countries and focuses on lending primarily to people with little or no credit history. 

Home Credit strives to broaden financial inclusion for the unbanked population by providing a positive and safe borrowing experience. In order to make sure this underserved population has a positive loan experience, Home Credit makes use of a variety of alternative data--including telco and transactional information--**to predict their clients' repayment abilities.**

While Home Credit is currently using various statistical and machine learning methods to make these predictions, **they're challenging Kagglers to help them unlock the full potential of their data**. Doing so will ensure that clients capable of repayment are not rejected and that loans are given with a principal, maturity, and repayment calendar that will empower their clients to be successful.
![](http://www.homecredit.net/~/media/Images/H/Home-Credit-Group/image-gallery/full/image-gallery-01-11-2016-b.png)",2,sz8416,eda-baseline-model-using-application,sz8416_eda-baseline-model-using-application_m2
> ## <a id='1'>1. Get the data</a>,3,sz8416,eda-baseline-model-using-application,sz8416_eda-baseline-model-using-application_m3
"This file contains descriptions for the columns in the various data files.

<img src=""https://storage.googleapis.com/kaggle-media/competitions/home-credit/home_credit.png"" width=""800""></img>",4,sz8416,eda-baseline-model-using-application,sz8416_eda-baseline-model-using-application_m4
"## <a id='2'>2. Check the data</a>
### 2.1 application train / test",5,sz8416,eda-baseline-model-using-application,sz8416_eda-baseline-model-using-application_m5
### 2.2 POS_CASH_balance,6,sz8416,eda-baseline-model-using-application,sz8416_eda-baseline-model-using-application_m6
### 2.3 bureau,7,sz8416,eda-baseline-model-using-application,sz8416_eda-baseline-model-using-application_m7
### 2.4 bureau_balance,8,sz8416,eda-baseline-model-using-application,sz8416_eda-baseline-model-using-application_m8
### 2.5 credit_card_balance,9,sz8416,eda-baseline-model-using-application,sz8416_eda-baseline-model-using-application_m9
### 2.6 previous_application,10,sz8416,eda-baseline-model-using-application,sz8416_eda-baseline-model-using-application_m10
### 2.7 installments_payments,11,sz8416,eda-baseline-model-using-application,sz8416_eda-baseline-model-using-application_m11
"## <a id='3'>3. Explore the data</a>

### <a id='3-1'>3.1 Categorical features</a>
#### Label",12,sz8416,eda-baseline-model-using-application,sz8416_eda-baseline-model-using-application_m12
### Occupation Type,13,sz8416,eda-baseline-model-using-application,sz8416_eda-baseline-model-using-application_m13
#### Gender,14,sz8416,eda-baseline-model-using-application,sz8416_eda-baseline-model-using-application_m14
### Income Type,15,sz8416,eda-baseline-model-using-application,sz8416_eda-baseline-model-using-application_m15
### House Type,16,sz8416,eda-baseline-model-using-application,sz8416_eda-baseline-model-using-application_m16
"### <a id='3-2'>3.2 Numerical features</a>
#### Credit Amount",17,sz8416,eda-baseline-model-using-application,sz8416_eda-baseline-model-using-application_m17
#### Annuity Amount,18,sz8416,eda-baseline-model-using-application,sz8416_eda-baseline-model-using-application_m18
### Days employed,19,sz8416,eda-baseline-model-using-application,sz8416_eda-baseline-model-using-application_m19
"### <a id='3-3'>3.3 Categorical features by label</a>
#### Gender",20,sz8416,eda-baseline-model-using-application,sz8416_eda-baseline-model-using-application_m20
#### Education Type,21,sz8416,eda-baseline-model-using-application,sz8416_eda-baseline-model-using-application_m21
"### <a id='3-4'>3.4 Numerical features by label</a>
#### EXT_SOURCE_1",22,sz8416,eda-baseline-model-using-application,sz8416_eda-baseline-model-using-application_m22
#### EXT_SOURCE_2,23,sz8416,eda-baseline-model-using-application,sz8416_eda-baseline-model-using-application_m23
#### EXT_SOURCE_3,24,sz8416,eda-baseline-model-using-application,sz8416_eda-baseline-model-using-application_m24
 ### <a id='3-5'>3.5 Correlation Matrix</a>,25,sz8416,eda-baseline-model-using-application,sz8416_eda-baseline-model-using-application_m25
" ## <a id='4'>4 A further exploration on application table</a>
 ### <a id='4-1'>4.1 Impute missing values</a>",26,sz8416,eda-baseline-model-using-application,sz8416_eda-baseline-model-using-application_m26
"### split categorical, discrete and numerical features",27,sz8416,eda-baseline-model-using-application,sz8416_eda-baseline-model-using-application_m27
### convert categorical using LabelEncoder,28,sz8416,eda-baseline-model-using-application,sz8416_eda-baseline-model-using-application_m28
"### impute missing values
- for categorical and discrete features: use **'mode'** in SimpleImputer
- for continuous features: use [MICEImputer](http://scikit-learn.org/dev/modules/generated/sklearn.impute.MICEImputer.html) with **median** as initial strategy ",29,sz8416,eda-baseline-model-using-application,sz8416_eda-baseline-model-using-application_m29
 ### <a id='4-2'>4.2 Create more features</a>,30,sz8416,eda-baseline-model-using-application,sz8416_eda-baseline-model-using-application_m30
### Term: total credit / annuity,31,sz8416,eda-baseline-model-using-application,sz8416_eda-baseline-model-using-application_m31
### OVER_EXPECT_CREDIT: actual credit larger than goods price,32,sz8416,eda-baseline-model-using-application,sz8416_eda-baseline-model-using-application_m32
### MEAN_BUILDING_SCORE_TOTAL: the sum of all building AVG score,33,sz8416,eda-baseline-model-using-application,sz8416_eda-baseline-model-using-application_m33
### FLAG_DOCUMENT_TOTAL: the total number of provided document,34,sz8416,eda-baseline-model-using-application,sz8416_eda-baseline-model-using-application_m34
### AMT_REQ_CREDIT_BUREAU_TOTAL: the total number of enquiries,35,sz8416,eda-baseline-model-using-application,sz8416_eda-baseline-model-using-application_m35
### BIRTH_EMPLOTED_INTERVEL: the days between born and employed,36,sz8416,eda-baseline-model-using-application,sz8416_eda-baseline-model-using-application_m36
"### <a id='4-3'>4.3 Train model</a>

**application**: 'binary' for binary classification

**num_iterations**: number of boosting iterations/trees, **n_estimators** in sklearn

**learning_rate**

**num_leaves**: number of leaves in one tree

**feature_fraction**: part of features used for each iteration

**bagging_fraction**: part of data used for each iteration

**lambda_l1/lambda_l2**: L1/L2 regularization

**min_split_gain**: the minimun gain to perform a split

**early_stopping_round**: if the validation metric can't improve for n rounds, stop iteration

**categorical_feature**: LightGBM API can deal with categorical feature automatically, **but we need transform string into integer**",37,sz8416,eda-baseline-model-using-application,sz8416_eda-baseline-model-using-application_m37
### <a id='4-4'>4.4 Feature importance</a>,38,sz8416,eda-baseline-model-using-application,sz8416_eda-baseline-model-using-application_m38
### <a id='4-5'>4.5 Prediction</a>,39,sz8416,eda-baseline-model-using-application,sz8416_eda-baseline-model-using-application_m39
,0,sz8416,simple-bayesian-optimization-for-lightgbm,sz8416_simple-bayesian-optimization-for-lightgbm_m0
"### Step 1: parameters to be tuned
**Note**: values for parameters should make sense, e.g.: 'num_leaves' needs to be a integer and 'feature_fraction' should between 0 and 1",1,sz8416,simple-bayesian-optimization-for-lightgbm,sz8416_simple-bayesian-optimization-for-lightgbm_m1
"### Step 2: Set the range for each parameter
**Gentle reminder**: try to make the range as narrow as possible",2,sz8416,simple-bayesian-optimization-for-lightgbm,sz8416_simple-bayesian-optimization-for-lightgbm_m2
### Step 3: Bayesian Optimization: Maximize,3,sz8416,simple-bayesian-optimization-for-lightgbm,sz8416_simple-bayesian-optimization-for-lightgbm_m3
### Step 4: Get the parameters,4,sz8416,simple-bayesian-optimization-for-lightgbm,sz8416_simple-bayesian-optimization-for-lightgbm_m4
"### Put all together
**Note**: It is just a demo. To get a better result, you should increase initial rounds, optimization rounds and n_estimators",5,sz8416,simple-bayesian-optimization-for-lightgbm,sz8416_simple-bayesian-optimization-for-lightgbm_m5
"**Introduction**
Real world datasets commonly show the particularity to have a number of samples of a given class under-represented compared to other classes. This imbalance gives rise to the “class imbalance” problem (or “curse of imbalanced datasets”) which is the problem of learning a concept from the class that has a small number of samples.
The class imbalance problem has been encountered in multiple areas such as telecommu- nication managements, bioinformatics, fraud detection, and medical diagnosis, and has been considered one of the top 10 problems in data mining and pattern recognition. Imbalanced data substantially compromises the learning process, since most of the standard machine learning algorithms expect balanced class dis- tribution or an equal misclassification cost. Related Inferential Statistics post showing bootstrap permutation ECDF plots: https://www.kaggle.com/tini9911/data-wrangling-eda-inferential-statistics-ml-model",0,tini9911,imbalance-in-data,tini9911_imbalance-in-data_m0
"Here I will work on some techniques to handle highly unbalanced datasets, with a focus on resampling. The Home Credit Risk Prediction competition, is a classic problem of unbalanced classes, since Credit Loan in risk can be considered unusual cases when considering all clients. Other classic examples of unbalanced classes are the detection of financial fraud and attacks on computer networks.
Let's see how unbalanced the dataset is:",1,tini9911,imbalance-in-data,tini9911_imbalance-in-data_m1
<h1> Resampling,2,tini9911,imbalance-in-data,tini9911_imbalance-in-data_m2
"A widely adopted technique for dealing with highly unbalanced datasets is called resampling. It consists of removing samples from the majority class (under-sampling) and / or adding more examples from the minority class (over-sampling).
Despite the advantage of balancing classes, these techniques also have their weaknesses (there is no free lunch). The simplest implementation of over-sampling is to duplicate random records from the minority class, which can cause overfitting. In under-sampling, the simplest technique involves removing random records from the majority class, which can cause loss of information.
Let's implement a basic example, which uses the DataFrame.sample method to get random samples each class:",3,tini9911,imbalance-in-data,tini9911_imbalance-in-data_m3
**Random under-sampling**,4,tini9911,imbalance-in-data,tini9911_imbalance-in-data_m4
**Random over-sampling**,5,tini9911,imbalance-in-data,tini9911_imbalance-in-data_m5
**Python imbalanced-learn module **,6,tini9911,imbalance-in-data,tini9911_imbalance-in-data_m6
"A number of more sophisticated resapling techniques have been proposed in the scientific literature.
For example, we can cluster the records of the majority class, and do the under-sampling by removing records from each cluster, thus seeking to preserve information. In over-sampling, instead of creating exact copies of the minority class records, we can introduce small variations into those copies, creating more diverse synthetic samples.
Let's apply some of these resampling techniques, using the Python library imbalanced-learn. It is compatible with scikit-learn and is part of scikit-learn-contrib projects.",7,tini9911,imbalance-in-data,tini9911_imbalance-in-data_m7
"For ease of visualization, let's create a small unbalanced sample dataset using the make_classification method:",8,tini9911,imbalance-in-data,tini9911_imbalance-in-data_m8
"We will also create a 2-dimensional plot function, plot_2d_space, to see the data distribution:",9,tini9911,imbalance-in-data,tini9911_imbalance-in-data_m9
"Because the dataset has many dimensions (features) and our graphs will be 2D, we will reduce the size of the dataset using Principal Component Analysis (PCA):",10,tini9911,imbalance-in-data,tini9911_imbalance-in-data_m10
**Random under-sampling and over-sampling with imbalanced-learn **,11,tini9911,imbalance-in-data,tini9911_imbalance-in-data_m11
**Under-sampling: Tomek links **¶,12,tini9911,imbalance-in-data,tini9911_imbalance-in-data_m12
"Tomek links are pairs of very close instances, but of opposite classes. Removing the instances of the majority class of each pair increases the space between the two classes, facilitating the classification process.
In the code below, we'll use ratio='majority' to resample the majority class.",13,tini9911,imbalance-in-data,tini9911_imbalance-in-data_m13
**Under-sampling: Cluster Centroids** ,14,tini9911,imbalance-in-data,tini9911_imbalance-in-data_m14
"This technique performs under-sampling by generating centroids based on clustering methods. The data will be previously grouped by similarity, in order to preserve information.
In this example we will pass the {0: 10} dict for the parameter ratio, to preserve 10 elements from the majority class (0), and all minority class (1) .",15,tini9911,imbalance-in-data,tini9911_imbalance-in-data_m15
**Over-sampling: SMOTE **,16,tini9911,imbalance-in-data,tini9911_imbalance-in-data_m16
"SMOTE (Synthetic Minority Oversampling TEchnique) consists of synthesizing elements for the minority class, based on those that already exist. It works randomly picingk a point from the minority class and computing the k-nearest neighbors for this point. The synthetic points are added between the chosen point and its neighbors.
We'll use ratio='minority' to resample the minority class.",17,tini9911,imbalance-in-data,tini9911_imbalance-in-data_m17
**Over-sampling followed by under-sampling **,18,tini9911,imbalance-in-data,tini9911_imbalance-in-data_m18
"Now, we will do a combination of over-sampling and under-sampling, using the SMOTE and Tomek links techniques:",19,tini9911,imbalance-in-data,tini9911_imbalance-in-data_m19
<h1>Deploying Machine Learning Model over Resampled Dataset ,20,tini9911,imbalance-in-data,tini9911_imbalance-in-data_m20
<h1> Random Forest Classifier ,21,tini9911,imbalance-in-data,tini9911_imbalance-in-data_m21
" ##  **Introducation**

Home Credit strives to broaden financial inclusion for the unbanked population by providing a positive and safe borrowing experience. In order to make sure this underserved population has a positive loan experience, Home Credit makes use of a variety of alternative data--including telco and transactional information--to predict their clients' repayment abilities.

While Home Credit is currently using various statistical and machine learning methods to make these predictions, they're challenging Kagglers to help them unlock the full potential of their data. Doing so will ensure that clients capable of repayment are not rejected and that loans are given with a principal, maturity, and repayment calendar that will empower their clients to be successful.
",0,vinothan,prefect-eda-for-house-credit-default-risk,vinothan_prefect-eda-for-house-credit-default-risk_m0
 ## Importing The Dataset,1,vinothan,prefect-eda-for-house-credit-default-risk,vinothan_prefect-eda-for-house-credit-default-risk_m1
## Data Description,2,vinothan,prefect-eda-for-house-credit-default-risk,vinothan_prefect-eda-for-house-credit-default-risk_m2
"appliction_train & application_test

This is the main table, broken into two files for Train (with TARGET) and Test (without TARGET).
Static data for all applications. One row represents one loan in our data sample.

bureau

All client's previous credits provided by other financial institutions that were reported to Credit Bureau (for clients who have a loan in our sample).
For every loan in our sample, there are as many rows as number of credits the client had in Credit Bureau before the application date.

bureau_balance

Monthly balances of previous credits in Credit Bureau.
This table has one row for each month of history of every previous credit reported to Credit Bureau – i.e the table has (#loans in sample * # of relative previous credits * # of months where we have some history observable for the previous credits) rows.

POS_CASH_balance

Monthly balance snapshots of previous POS (point of sales) and cash loans that the applicant had with Home Credit.
This table has one row for each month of history of every previous credit in Home Credit (consumer credit and cash loans) related to loans in our sample – i.e. the table has (#loans in sample * # of relative previous credits * # of months in which we have some history observable for the previous credits) rows.

credit_card_balance

Monthly balance snapshots of previous credit cards that the applicant has with Home Credit.
This table has one row for each month of history of every previous credit in Home Credit (consumer credit and cash loans) related to loans in our sample – i.e. the table has (#loans in sample * # of relative previous credit cards * # of months where we have some history observable for the previous credit card) rows.

previous_application

All previous applications for Home Credit loans of clients who have loans in our sample.
There is one row for each previous application related to loans in our data sample.

installments_payments

Repayment history for the previously disbursed credits in Home Credit related to the loans in our sample.
There is a) one row for every payment that was made plus b) one row each for missed payment.
One row is equivalent to one payment of one installment OR one installment corresponding to one payment of one previous Home Credit credit related to loans in our sample.

",3,vinothan,prefect-eda-for-house-credit-default-risk,vinothan_prefect-eda-for-house-credit-default-risk_m3
![image.png](attachment:image.png),4,vinothan,prefect-eda-for-house-credit-default-risk,vinothan_prefect-eda-for-house-credit-default-risk_m4
"2. ### Number of records and Features in the datasets
2.1. ** Let Examine the Applaication Train DataSet**",5,vinothan,prefect-eda-for-house-credit-default-risk,vinothan_prefect-eda-for-house-credit-default-risk_m5
### Identifying Numerical and Categorical Features,6,vinothan,prefect-eda-for-house-credit-default-risk,vinothan_prefect-eda-for-house-credit-default-risk_m6
###  Function for  find out Numerical and categeical Variables,7,vinothan,prefect-eda-for-house-credit-default-risk,vinothan_prefect-eda-for-house-credit-default-risk_m7
### Identifying Missing Value Present in Application Train Dataset,8,vinothan,prefect-eda-for-house-credit-default-risk,vinothan_prefect-eda-for-house-credit-default-risk_m8
### The above given figure which clear say about the which features has missing value and precentage of missing value avaiable in application_train DataSet,9,vinothan,prefect-eda-for-house-credit-default-risk,vinothan_prefect-eda-for-house-credit-default-risk_m9
### Let us Examine application_test data set,10,vinothan,prefect-eda-for-house-credit-default-risk,vinothan_prefect-eda-for-house-credit-default-risk_m10
### identifying the Catergical and numberical variables ,11,vinothan,prefect-eda-for-house-credit-default-risk,vinothan_prefect-eda-for-house-credit-default-risk_m11
### identifying the missing values,12,vinothan,prefect-eda-for-house-credit-default-risk,vinothan_prefect-eda-for-house-credit-default-risk_m12
### Let Exmaine bureau dataset,13,vinothan,prefect-eda-for-house-credit-default-risk,vinothan_prefect-eda-for-house-credit-default-risk_m13
### identifying the catergical and numnerical features  ,14,vinothan,prefect-eda-for-house-credit-default-risk,vinothan_prefect-eda-for-house-credit-default-risk_m14
### identying the missing data,15,vinothan,prefect-eda-for-house-credit-default-risk,vinothan_prefect-eda-for-house-credit-default-risk_m15
### Exmaine the bureau_balance DataSet,16,vinothan,prefect-eda-for-house-credit-default-risk,vinothan_prefect-eda-for-house-credit-default-risk_m16
## identifying Catergical and numerical variables,17,vinothan,prefect-eda-for-house-credit-default-risk,vinothan_prefect-eda-for-house-credit-default-risk_m17
### identifying the missing value in bureau_balance,18,vinothan,prefect-eda-for-house-credit-default-risk,vinothan_prefect-eda-for-house-credit-default-risk_m18
### No missing data in Bureau Balance Dataset,19,vinothan,prefect-eda-for-house-credit-default-risk,vinothan_prefect-eda-for-house-credit-default-risk_m19
## Exmaine the POS_CASH_balance DataSet,20,vinothan,prefect-eda-for-house-credit-default-risk,vinothan_prefect-eda-for-house-credit-default-risk_m20
### identifying the Catergical and numerical variables ,21,vinothan,prefect-eda-for-house-credit-default-risk,vinothan_prefect-eda-for-house-credit-default-risk_m21
### identifying the missing values,22,vinothan,prefect-eda-for-house-credit-default-risk,vinothan_prefect-eda-for-house-credit-default-risk_m22
### Examine the credit_card_balance dataset,23,vinothan,prefect-eda-for-house-credit-default-risk,vinothan_prefect-eda-for-house-credit-default-risk_m23
### identifying the Categerical and numerical Variable ,24,vinothan,prefect-eda-for-house-credit-default-risk,vinothan_prefect-eda-for-house-credit-default-risk_m24
### identifying the missing value in credit_card_balance dataset,25,vinothan,prefect-eda-for-house-credit-default-risk,vinothan_prefect-eda-for-house-credit-default-risk_m25
### Exmaine the previous_application Dataset,26,vinothan,prefect-eda-for-house-credit-default-risk,vinothan_prefect-eda-for-house-credit-default-risk_m26
 ### identifying the catergical and numerical variable in previous application Data set.,27,vinothan,prefect-eda-for-house-credit-default-risk,vinothan_prefect-eda-for-house-credit-default-risk_m27
### identifying the missing value in previous_application,28,vinothan,prefect-eda-for-house-credit-default-risk,vinothan_prefect-eda-for-house-credit-default-risk_m28
### Exmaine the installments_payments dataset,29,vinothan,prefect-eda-for-house-credit-default-risk,vinothan_prefect-eda-for-house-credit-default-risk_m29
### identifying the categerical and numerical Variable ,30,vinothan,prefect-eda-for-house-credit-default-risk,vinothan_prefect-eda-for-house-credit-default-risk_m30
### identifying the missing value in installments_payments,31,vinothan,prefect-eda-for-house-credit-default-risk,vinothan_prefect-eda-for-house-credit-default-risk_m31
## Checking the Imbalance of Target Variable,32,vinothan,prefect-eda-for-house-credit-default-risk,vinothan_prefect-eda-for-house-credit-default-risk_m32
"It is evident that  many customer are able to pay the loan back i.e Only 91.9% of the total customer are repaying the loan.
We need to drill down more to get better insights from the data and see which categories of the customer are not able to pay back loan.

We will try to check the repayer and defualter rate by using the different features of the dataset. 
Some of the features being Gender,Education,Employment_type,etc. First let us understand the different types of features.",33,vinothan,prefect-eda-for-house-credit-default-risk,vinothan_prefect-eda-for-house-credit-default-risk_m33
"## Types Of Features

### Categorical Features:
A categorical variable is one that has two or more categories and each value in that feature can be categorised by them.For example, 
Name Education Type is a categorical variable having Five categories. Now we cannot sort or give any ordering to such variables. They are also known as Nominal Variables.

Categorical Features in the dataset: Education_type,occupation_type,Contract_type

### Analysing The Features
***Eduaction-----> Categorical Feature***",34,vinothan,prefect-eda-for-house-credit-default-risk,vinothan_prefect-eda-for-house-credit-default-risk_m34
### the proof is  edvined by looking at the above given plot and groupby function is clearly customer with education of secondary/secondary special has high  count where not able to pay loan back.,35,vinothan,prefect-eda-for-house-credit-default-risk,vinothan_prefect-eda-for-house-credit-default-risk_m35
## Analysis based on Code  Gender ,36,vinothan,prefect-eda-for-house-credit-default-risk,vinothan_prefect-eda-for-house-credit-default-risk_m36
### its clear that  by looking at the above given plot and groupby function is clearly customer based on code gender type female  has high  count where not able to pay loan back compare to male.,37,vinothan,prefect-eda-for-house-credit-default-risk,vinothan_prefect-eda-for-house-credit-default-risk_m37
### Analysis based on INCOME TYPE,38,vinothan,prefect-eda-for-house-credit-default-risk,vinothan_prefect-eda-for-house-credit-default-risk_m38
### its clear that  by looking at the above given plot and groupby function is clearly customer based on code income type and the working  has high  count where not able to pay loan back compare to all other,39,vinothan,prefect-eda-for-house-credit-default-risk,vinothan_prefect-eda-for-house-credit-default-risk_m39
### Analysis based on OCCUPATION TYPE,40,vinothan,prefect-eda-for-house-credit-default-risk,vinothan_prefect-eda-for-house-credit-default-risk_m40
###  its clear that  by looking at the above given plot and groupby function is clearly customer based on occupation  type. the working  has high  count where not able to pay loan back compare to all other.,41,vinothan,prefect-eda-for-house-credit-default-risk,vinothan_prefect-eda-for-house-credit-default-risk_m41
### Analysis Based on FAMILY STATUS,42,vinothan,prefect-eda-for-house-credit-default-risk,vinothan_prefect-eda-for-house-credit-default-risk_m42
### its clear that  by looking at the above given plot and groupby function is clearly customer based on code Family type and the Married customer  has high  count where not able to pay loan back compare to all other.,43,vinothan,prefect-eda-for-house-credit-default-risk,vinothan_prefect-eda-for-house-credit-default-risk_m43
"
### Analysis based HOUSING TYPE",44,vinothan,prefect-eda-for-house-credit-default-risk,vinothan_prefect-eda-for-house-credit-default-risk_m44
### its clear that  by looking at the above given plot and groupby function is clearly customer based on Housing type and the house type customer  has high  count where not able to pay loan back compare to all other.,45,vinothan,prefect-eda-for-house-credit-default-risk,vinothan_prefect-eda-for-house-credit-default-risk_m45
### Analysis based on TYPE SUITE,46,vinothan,prefect-eda-for-house-credit-default-risk,vinothan_prefect-eda-for-house-credit-default-risk_m46
### its clear that  by looking at the above given plot and groupby function is clearly customer based on Suite type and the unaccompanied customer  has high  count where not able to pay loan back compare to all other.,47,vinothan,prefect-eda-for-house-credit-default-risk,vinothan_prefect-eda-for-house-credit-default-risk_m47
### Analysis Based on ORGANIZATION TYPE,48,vinothan,prefect-eda-for-house-credit-default-risk,vinothan_prefect-eda-for-house-credit-default-risk_m48
### its clear that  by looking at the above given plot and groupby function is clearly customer based on Organization type and the Business type 3 customer  has high  count where not able to pay loan back compare to all other.,49,vinothan,prefect-eda-for-house-credit-default-risk,vinothan_prefect-eda-for-house-credit-default-risk_m49
### Analysis based on FLAG OWN CAR,50,vinothan,prefect-eda-for-house-credit-default-risk,vinothan_prefect-eda-for-house-credit-default-risk_m50
### its clear that  by looking at the above given plot and groupby function is clearly customer based on owning car type and the customer with no car  has high  count where not able to pay loan back compare to all other.,51,vinothan,prefect-eda-for-house-credit-default-risk,vinothan_prefect-eda-for-house-credit-default-risk_m51
### Analysis based on FLAG_OWN_REALTY,52,vinothan,prefect-eda-for-house-credit-default-risk,vinothan_prefect-eda-for-house-credit-default-risk_m52
### its clear that  by looking at the above given plot and groupby function is clearly customer based on owning Reality type and the customer has Reality  has high  count where not able to pay loan back compare to all other.,53,vinothan,prefect-eda-for-house-credit-default-risk,vinothan_prefect-eda-for-house-credit-default-risk_m53
### Analysis based on NAME_CONTRACT_TYPE,54,vinothan,prefect-eda-for-house-credit-default-risk,vinothan_prefect-eda-for-house-credit-default-risk_m54
### its clear that  by looking at the above given plot and groupby function is clearly customer based on Contract type and the customer with cash loans  has high  count where not able to pay loan back compare to all other.,55,vinothan,prefect-eda-for-house-credit-default-risk,vinothan_prefect-eda-for-house-credit-default-risk_m55
### Analysis based on WEEKDAY_APPR_PROCESS_START,56,vinothan,prefect-eda-for-house-credit-default-risk,vinothan_prefect-eda-for-house-credit-default-risk_m56
"### its clear that  by looking at the above given plot and groupby function is clearly customer based application start on days and the customer registed  on tuesday  has high  count were not able to pay loan back compare to all other.

### Analysis based House Type Mode",57,vinothan,prefect-eda-for-house-credit-default-risk,vinothan_prefect-eda-for-house-credit-default-risk_m57
### its clear that  by looking at the above given plot and groupby function is clearly customer based Housetype Mode and the customer registed  on Block of flats has high  count where not able to pay loan back compare to all other.,58,vinothan,prefect-eda-for-house-credit-default-risk,vinothan_prefect-eda-for-house-credit-default-risk_m58
### Analysis Based on EMERGENCYSTATE MODE ,59,vinothan,prefect-eda-for-house-credit-default-risk,vinothan_prefect-eda-for-house-credit-default-risk_m59
### its clear that  by looking at the above given plot and groupby function is clearly customer based Emergency state Mode and the customer has No  has high  count where not able to pay loan back compare to all other.,60,vinothan,prefect-eda-for-house-credit-default-risk,vinothan_prefect-eda-for-house-credit-default-risk_m60
"### ANALYSIS THE NUMBERICAL FEATURES 


#### ANALYZSIS BASED ON  COUNT CHILDREN ",61,vinothan,prefect-eda-for-house-credit-default-risk,vinothan_prefect-eda-for-house-credit-default-risk_m61
its clear that  by looking at the above given plots and groupby function is clearly customer based count of children and the customer with   has No  children has high  count and customer with more than  8   50% of non payer based on it own count where not able to pay loan back compare to all other.,62,vinothan,prefect-eda-for-house-credit-default-risk,vinothan_prefect-eda-for-house-credit-default-risk_m62
### if we see  the above given distrbution plot it clear say major distrbuiton  for amouth annuity is from 0 to 75000 and amount of anual income is from 0 to 1000000,63,vinothan,prefect-eda-for-house-credit-default-risk,vinothan_prefect-eda-for-house-credit-default-risk_m63
"#### Analysis based on   REGION_RATING_CLIENT , REGION_RATING_CLIENT_W_CITY', HOUR_APPR_PROCESS_STAR',
  REG_REGION_NOT_LIVE_REGION', 'REG_REGION_NOT_WORK_REGION',
  LIVE_REGION_NOT_WORK_REGION', 'REG_CITY_NOT_LIVE_CITY
 REG_CITY_NOT_WORK_CITY', 'LIVE_CITY_NOT_WORK_CITY'",64,vinothan,prefect-eda-for-house-credit-default-risk,vinothan_prefect-eda-for-house-credit-default-risk_m64
if we look at the above plot it is clear that customer register in city but not work in city based analysis we know that customer not city has high count.,65,vinothan,prefect-eda-for-house-credit-default-risk,vinothan_prefect-eda-for-house-credit-default-risk_m65
### By looking at the above given we can clear get id of which loan Repayer vs NOn Payer  all features list in above plot,66,vinothan,prefect-eda-for-house-credit-default-risk,vinothan_prefect-eda-for-house-credit-default-risk_m66
1. 1. ### Analysis Based on EXter Source Types,67,vinothan,prefect-eda-for-house-credit-default-risk,vinothan_prefect-eda-for-house-credit-default-risk_m67
### Analysis based Averages values,68,vinothan,prefect-eda-for-house-credit-default-risk,vinothan_prefect-eda-for-house-credit-default-risk_m68
### Checking the  Correlation Between The Features for Application Train Dataset,69,vinothan,prefect-eda-for-house-credit-default-risk,vinothan_prefect-eda-for-house-credit-default-risk_m69
### If look at the above given plot is clear that all AVG featuers are high correleted values by seeing this plot we can easy find out the coorelated features,70,vinothan,prefect-eda-for-house-credit-default-risk,vinothan_prefect-eda-for-house-credit-default-risk_m70
#### **By see above given two  corelation plot  we can easy find out  the most corelated featuers along with they corelated values.  and all kind of analysis are done for the application train dataset.,71,vinothan,prefect-eda-for-house-credit-default-risk,vinothan_prefect-eda-for-house-credit-default-risk_m71
"## EDA of Bureau Data

All client's previous credits provided by other financial institutions that were reported to Credit Bureau (for clients who have a loan in our sample). For every loan in our sample, there are as many rows as number of credits the client had in Credit Bureau before the application date.",72,vinothan,prefect-eda-for-house-credit-default-risk,vinothan_prefect-eda-for-house-credit-default-risk_m72
### Merging the bureau dataset along with application train dataset to do more analysis,73,vinothan,prefect-eda-for-house-credit-default-risk,vinothan_prefect-eda-for-house-credit-default-risk_m73
"### Analysis Based on CREDIT ACTIVE, CREDIT_CURRENCY, CREDIT TYPE",74,vinothan,prefect-eda-for-house-credit-default-risk,vinothan_prefect-eda-for-house-credit-default-risk_m74
### BASED on above  given plot it clear that the Credit type consumer credit has high count of non payer of loan and credit currency customer with currency 1 has high count of non payer and based credit active customer with in group of closed customer has high count of non payer,75,vinothan,prefect-eda-for-house-credit-default-risk,vinothan_prefect-eda-for-house-credit-default-risk_m75
### ABOVE given plots show distributions,76,vinothan,prefect-eda-for-house-credit-default-risk,vinothan_prefect-eda-for-house-credit-default-risk_m76
### EAD for bureau balance Dataset,77,vinothan,prefect-eda-for-house-credit-default-risk,vinothan_prefect-eda-for-house-credit-default-risk_m77
"### based  type its clear that status c, o,x has high count in bureau balnace dataset",78,vinothan,prefect-eda-for-house-credit-default-risk,vinothan_prefect-eda-for-house-credit-default-risk_m78
### EDA for previous application dataset,79,vinothan,prefect-eda-for-house-credit-default-risk,vinothan_prefect-eda-for-house-credit-default-risk_m79
### By looking at the above given plot its clear that which types has high count in each feature so we can easy out the root cause of the problem.,80,vinothan,prefect-eda-for-house-credit-default-risk,vinothan_prefect-eda-for-house-credit-default-risk_m80
### analyzing the numerical features disturbion in previous application dataset,81,vinothan,prefect-eda-for-house-credit-default-risk,vinothan_prefect-eda-for-house-credit-default-risk_m81
## those are few EAD i have done please  vote for me which help my movitvation to increase to do a lot of work if there any imporvment can be done means please say in comments References This notebook has been created based on great work done solving the House Credit defult Risk competition and other sources.,82,vinothan,prefect-eda-for-house-credit-default-risk,vinothan_prefect-eda-for-house-credit-default-risk_m82
"# Introduction: Automated Feature Engineering Basics

In this notebook, we will walk through applying automated feature engineering to the [Home Credit Default Risk dataset](https://www.kaggle.com/c/home-credit-default-risk) using the featuretools library. [Featuretools](https://docs.featuretools.com/) is an open-source Python package for automatically creating new features from multiple tables of structured, related data. It is ideal tool for problems such as the Home Credit Default Risk competition where there are several related tables that need to be combined into a single dataframe for training (and one for testing). 

## Feature Engineering

The objective of [feature engineering](https://en.wikipedia.org/wiki/Feature_engineering) is to create new features (alos called explantory variables or predictors) to represent as much information from an entire dataset in one table.  Typically, this process is done by hand using pandas operations such as `groupby`, `agg`, or `merge` and can be very tedious. Moreover, manual feature engineering is limited both by human time constraints and imagination: we simply cannot conceive of every possible feature that will be useful. (For an example of using manual feature engineering, check out [part one](https://www.kaggle.com/willkoehrsen/introduction-to-manual-feature-engineering) and [part two](https://www.kaggle.com/willkoehrsen/introduction-to-manual-feature-engineering-p2) applied to this competition). The importance of creating the proper features cannot be overstated because a machine learning model can only learn from the data we give to it. Extracting as much information as possible from the available datasets is crucial to creating an effective solution.

[Automated feature engineering](https://towardsdatascience.com/automated-feature-engineering-in-python-99baf11cc219) aims to help the data scientist with the problem of feature creation by automatically building hundreds or thousands of new features from a dataset. Featuretools - the only library for automated feature engineering at the moment - will not replace the data scientist, but it will allow her to focus on more valuable parts of the machine learning pipeline, such as delivering robust models into production. 

Here we will touch on the concepts of automated feature engineering with featuretools and show how to implement it for the Home Credit Default Risk competition. We will stick to the basics so we can get the ideas down and then build upon this foundation in later work when we customize featuretools. We will work with a subset of the data because this is a computationally intensive job that is outside the capabilities of the Kaggle kernels. I took the work done in this notebook and ran the methods on the entire dataset with the results [available here](https://www.kaggle.com/willkoehrsen/home-credit-default-risk-feature-tools). At the end of this notebook, we'll look at the features themselves, as well as the results of modeling with different combinations of hand designed and automatically built features. 

If you are new to this competition, I suggest checking out [this post to get started](https://www.kaggle.com/willkoehrsen/start-here-a-gentle-introduction). For a good take on why features are so important, here's a [blog post](https://www.featurelabs.com/blog/secret-to-data-science-success/) by one of the developers of Featuretools. ",0,willkoehrsen,automated-feature-engineering-basics,willkoehrsen_automated-feature-engineering-basics_m0
"# Problem

The Home Credit Default Risk competition is a supervised classification machine learning task. The objective is to use historical financial and socioeconomic data to predict whether or not an applicant will be able to repay a loan. This is a standard supervised classification task:

* __Supervised__: The labels are included in the training data and the goal is to train a model to learn to predict the labels from the features
* __Classification__: The label is a binary variable, 0 (will repay loan on time), 1 (will have difficulty repaying loan)

## Dataset

The data is provided by [Home Credit](http://www.homecredit.net/about-us.aspx), a service dedicated to provided lines of credit (loans) to the unbanked population. 

There are 7 different data files:

* __application_train/application_test__: the main training and testing data with information about each loan application at Home Credit. Every loan has its own row and is identified by the `SK_ID_CURR`. The training application data comes with the `TARGET` with indicating 0: the loan was repaid and 1: the loan was not repaid. 
* __bureau__: data concerning client's previous credits from other financial institutions. Each previous credit has its own row in bureau and is identified by the `SK_ID_BUREAU`, Each loan in the application data can have multiple previous credits.
* __bureau_balance__: monthly data about the previous credits in bureau. Each row is one month of a previous credit, and a single previous credit can have multiple rows, one for each month of the credit length. 
* __previous_application__: previous applications for loans at Home Credit of clients who have loans in the application data. Each current loan in the application data can have multiple previous loans. Each previous application has one row and is identified by the feature `SK_ID_PREV`. 
* __POS_CASH_BALANCE__: monthly data about previous point of sale or cash loans clients have had with Home Credit. Each row is one month of a previous point of sale or cash loan, and a single previous loan can have many rows.
* __credit_card_balance__: monthly data about previous credit cards clients have had with Home Credit. Each row is one month of a credit card balance, and a single credit card can have many rows.
* __installments_payment__: payment history for previous loans at Home Credit. There is one row for every made payment and one row for every missed payment. 

The diagram below (provided by Home Credit) shows how the tables are related. This will be very useful when we need to define relationships in featuretools. 

![image](https://storage.googleapis.com/kaggle-media/competitions/home-credit/home_credit.png)

### Read in Data and Create Small Datasets

We will read in the full dataset, sort by the `SK_ID_CURR` and keep only the first 1000 rows to make the calculations feasible. Later we can convert to a script and run with the entire datasets.",1,willkoehrsen,automated-feature-engineering-basics,willkoehrsen_automated-feature-engineering-basics_m1
"We'll join the train and test set together but add a separate column identifying the set. This is important because we are going to want to apply the same exact procedures to each dataset. It's safest to just join them together and treat them as a single dataframe. 

(I'm not sure if this is allowing data leakage into the train set and if these feature creation operations should be applied separately. Any thoughts would be much appreciated!)",2,willkoehrsen,automated-feature-engineering-basics,willkoehrsen_automated-feature-engineering-basics_m2
"# Featuretools Basics

[Featuretools](https://docs.featuretools.com/#minute-quick-start) is an open-source Python library for automatically creating features out of a set of related tables using a technique called [deep feature synthesis](http://www.jmaxkanter.com/static/papers/DSAA_DSM_2015.pdf). Automated feature engineering, like many topics in machine learning, is a complex subject built upon a foundation of simpler ideas. By going through these ideas one at a time, we can build up our understanding of how featuretools which will later allow for us to get the most out of it.

There are a few concepts that we will cover along the way:

* [Entities and EntitySets](https://docs.featuretools.com/loading_data/using_entitysets.html)
* [Relationships between tables](https://docs.featuretools.com/loading_data/using_entitysets.html#adding-a-relationship)
* [Feature primitives](https://docs.featuretools.com/automated_feature_engineering/primitives.html): aggregations and transformations
* [Deep feature synthesis](https://docs.featuretools.com/automated_feature_engineering/afe.html)

# Entities and Entitysets

An entity is simply a table or in Pandas, a `dataframe`. The observations are in the rows and the features in the columns. An entity in featuretools must have a unique index where none of the elements are duplicated.  Currently, only `app`, `bureau`, and `previous` have unique indices (`SK_ID_CURR`, `SK_ID_BUREAU`, and `SK_ID_PREV` respectively). For the other dataframes, we must pass in `make_index = True` and then specify the name of the index. Entities can also have time indices where each entry is identified by a unique time. (There are not datetimes in any of the data, but there are relative times, given in months or days, that we could consider treating as time variables).

An [EntitySet](https://docs.featuretools.com/loading_data/using_entitysets.html) is a collection of tables and the relationships between them. This can be thought of a data structute with its own methods and attributes. Using an EntitySet allows us to group together multiple tables and manipulate them much quicker than individual tables. 

First we'll make an empty entityset named clients to keep track of all the data.",3,willkoehrsen,automated-feature-engineering-basics,willkoehrsen_automated-feature-engineering-basics_m3
"Now we define each entity, or table of data. We need to pass in an index if the data has one or `make_index = True` if not. Featuretools will automatically infer the types of variables, but we can also change them if needed. For intstance, if we have a categorical variable that is represented as an integer we might want to let featuretools know the right type.",4,willkoehrsen,automated-feature-engineering-basics,willkoehrsen_automated-feature-engineering-basics_m4
"# Relationships

Relationships are a fundamental concept not only in featuretools, but in any relational database. The best way to think of a one-to-many relationship is with the analogy of parent-to-child. A parent is a single individual, but can have mutliple children. The children can then have multiple children of their own. In a _parent table_, each individual has a single row. Each individual in the parent table can have multiple rows in the _child table_. 

As an example, the `app` dataframe has one row for each client  (`SK_ID_CURR`) while the `bureau` dataframe has multiple previous loans (`SK_ID_PREV`) for each parent (`SK_ID_CURR`). Therefore, the `bureau` dataframe is the child of the `app` dataframe. The `bureau` dataframe in turn is the parent of `bureau_balance` because each loan has one row in `bureau` but multiple monthly records in `bureau_balance`. ",5,willkoehrsen,automated-feature-engineering-basics,willkoehrsen_automated-feature-engineering-basics_m5
"The `SK_ID_CURR` ""100002"" has one row in the parent table and multiple rows in the child. 

Two tables are linked via a shared variable. The `app` and `bureau` dataframe are linked by the `SK_ID_CURR` variable while the `bureau` and `bureau_balance` dataframes are linked with the `SK_ID_BUREAU`. Defining the relationships is relatively straightforward, and the diagram provided by the competition is helpful for seeing the relationships. For each relationship, we need to specify the parent variable and the child variable. Altogether, there are a total of 6 relationships between the tables. Below we specify all six relationships and then add them to the EntitySet.",6,willkoehrsen,automated-feature-engineering-basics,willkoehrsen_automated-feature-engineering-basics_m6
"Slightly advanced note: we need to be careful to not create a [diamond graph](https://en.wikipedia.org/wiki/Diamond_graph) where there are multiple paths from a parent to a child. If we directly link `app` and `cash` via `SK_ID_CURR`; `previous` and `cash` via `SK_ID_PREV`; and `app` and `previous` via `SK_ID_CURR`, then we have created two paths from `app` to `cash`. This results in ambiguity, so the approach we have to take instead is to link `app` to `cash` through `previous`. We establish a relationship between `previous` (the parent) and `cash` (the child) using `SK_ID_PREV`. Then we establish a relationship between `app` (the parent) and `previous` (now the child) using `SK_ID_CURR`. Then featuretools will be able to create features on `app` derived from both `previous` and `cash` by stacking multiple primitives. ",7,willkoehrsen,automated-feature-engineering-basics,willkoehrsen_automated-feature-engineering-basics_m7
"All entities in the entity can be related to each other. In theory this allows us to calculate features for any of the entities, but in practice, we will only calculate features for the `app` dataframe since that will be used for training/testing. ",8,willkoehrsen,automated-feature-engineering-basics,willkoehrsen_automated-feature-engineering-basics_m8
"# Feature Primitives

A [feature primitive](https://docs.featuretools.com/automated_feature_engineering/primitives.html) is an operation applied to a table or a set of tables to create a feature. These represent simple calculations, many of which we already use in manual feature engineering, that can be stacked on top of each other to create complex features. Feature primitives fall into two categories:

* __Aggregation__: function that groups together child datapoints for each parent and then calculates a statistic such as mean, min, max, or standard deviation. An example is calculating the maximum previous loan amount for each client. An aggregation works across multiple tables using relationships between tables.
* __Transformation__: an operation applied to one or more columns in a single table. An example would be taking the absolute value of a column, or finding the difference between two columns in one table.

A list of the available features primitives in featuretools can be viewed below.",9,willkoehrsen,automated-feature-engineering-basics,willkoehrsen_automated-feature-engineering-basics_m9
"# Deep Feature Synthesis

Deep Feature Synthesis (DFS) is the process featuretools uses to make new features. DFS stacks feature primitives to form features with a ""depth"" equal to the number of primitives. For example, if we take the maximum value of a client's previous loans (say `MAX(previous.loan_amount)`), that is a ""deep feature"" with a depth of 1. To create a feature with a depth of two, we could stack primitives by taking the maximum value of a client's average montly payments per previous loan (such as `MAX(previous(MEAN(installments.payment)))`). The [original paper on automated feature engineering using deep feature synthesis](https://dai.lids.mit.edu/wp-content/uploads/2017/10/DSAA_DSM_2015.pdf) is worth a read. 

To perform DFS in featuretools, we use the `dfs`  function passing it an `entityset`, the `target_entity` (where we want to make the features), the `agg_primitives` to use, the `trans_primitives` to use and the `max_depth` of the features. Here we will use the default aggregation and transformation primitives,  a max depth of 2, and calculate primitives for the `app` entity. Because this process is computationally expensive, we can run the function using `features_only = True` to return only a list of the features and not calculate the features themselves. This can be useful to look at the resulting features before starting an extended computation.",10,willkoehrsen,automated-feature-engineering-basics,willkoehrsen_automated-feature-engineering-basics_m10
### DFS with Default Primitives,11,willkoehrsen,automated-feature-engineering-basics,willkoehrsen_automated-feature-engineering-basics_m11
"If you are interested in running this call on the entire dataset and making the features, I wrote a script [for that here](https://www.kaggle.com/willkoehrsen/feature-engineering-using-feature-tools). Unfortunately, this will not run in a Kaggle kernel due to the computational expense of the operation. Using a computer with 64GB of ram, this function call took around 24 hours (I don't think I'm technically breaking the rules of my university's high powered computing center). I have made the entire dataset available [here](https://www.kaggle.com/willkoehrsen/home-credit-default-risk-feature-tools/data) in the file called `feature_matrix.csv`. 

To generate a subset of the features, run the code cell below.",12,willkoehrsen,automated-feature-engineering-basics,willkoehrsen_automated-feature-engineering-basics_m12
"### DFS with Selected Aggregation Primitives

With featuretools, we were able to go from 121 original features to almost 1700 in a few lines of code.  When I did feature engineering by hand, it took about 12 hours to create a comparable size dataset. However, while we get a lot of features in featuretools, this function call is not very well-informed. We simply used the default aggregations without thinking about which ones are ""important"" for the problem. We end up with a lot of features, but they are probably not all relevant to the problem. Too many irrelevant features can decrease performance by drowning out the important features (related to the [curse of dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality))

The next call we make will specify a smaller set of features. We still are not using much domain knowledge, but this feature set will be more manageable. The next step from here is improving the features we actually build and performing feature selection.",13,willkoehrsen,automated-feature-engineering-basics,willkoehrsen_automated-feature-engineering-basics_m13
"That ""only"" gives us 884 features (and takes about 12 hours to run on the complete dataset). ",14,willkoehrsen,automated-feature-engineering-basics,willkoehrsen_automated-feature-engineering-basics_m14
"## Notes on Basic Implementation

These calls represent only a [small fraction of the ability of featuretools](https://docs.featuretools.com/guides/tuning_dfs.html). We did not specify the variable types when creating entities, did not use the relative time variables, and didn't touch on [custom primitives](https://docs.featuretools.com/guides/advanced_custom_primitives.html) or seed features or interesting values! Nonetheless, in this notebook, we were able to learn the basic foundations which will allow us to more effective use the tool as we learn how it works.  Now, let's take a look at some of the features we have built and modeling results.",15,willkoehrsen,automated-feature-engineering-basics,willkoehrsen_automated-feature-engineering-basics_m15
"# Results

To determine whether our basic implementation of featuretools was useful, we can look at several results:

* Cross validation scores and public leaderboard scores using several different sets of features.
* Correlations: both between the features and the `TARGET`, and between features themselves
* Feature importances: determined by a gradient boosting machine model
",16,willkoehrsen,automated-feature-engineering-basics,willkoehrsen_automated-feature-engineering-basics_m16
"## Feature Performance Experiments

To compare a number of different feature sets for the machine learning task, I set up several experiments.. In order to isolate the effect of the features, the same model was used to test a number of different feature sets. The model (which can be viewed in the appendix) is a basic LightGBM algorithm using 5-fold cross validation for training and evaluation. First, we establish a control dataset, and then we carry out a series of experiments and present the results.

* Control: using only data from the `application` dataset
* Test One: manual feature engineering using only the `application`, `bureau` and `bureau_balance` data
* Test Two: manual feature engineering using all datasets
* Test Three: featuretools default features (in the `feature_matrix`)
* Test Four: featuretools specified features (in the `feature_matrix_spec`)
* Test Five: featuretools specified features combined with manual feature engineering 

The number of features is after one-hot encoding, the validation receiver operating characteristic area under the curve (ROC AUC) is calculated using 5-fold cross validation, the test ROC AUC is from the public leaderboard, and the time spent designing is my best estimate of how long it took to make the dataset! 

| Test    | Number of Features | Validation ROC AUC | Test ROC AUC | Time Spent |
|---------|--------------------|--------------------|--------------|--------|
| Control | 241                |           0.760         |     0.745         |       0.25 hours  |
| One     | 421                |       0.766             |      0.757        |        8 hours        |
| Two     |      1465             |          0.785          |         0.783     |                 12 hours |
| Three   | 1803               |      0.784              |       0.777       |               1 hour
| Four    | 1156               |         0.786           |        0.779      |                 1.25 hours |
| Five    |  1624                  |           0. 787        |      0.782        |                    13.25 hours |


It's hard to say which set is exactly the best (although I trust the cross validation scores more than the public leaderboard) but there are huge discrepancies is the time for development. The specified featuretools dataset was able to achieve nearly the same performance as the hand engineered features on the test set with 8% of the time invested. It's clear that featuretools delivered value on this problem, but it still did not leave us without a job. The vital role of the data scientist now comes down to choosing the correct set of primitives and selecting the best features from among all the candidates. ",17,willkoehrsen,automated-feature-engineering-basics,willkoehrsen_automated-feature-engineering-basics_m17
"## Correlations

Next we can look at correlations within the data. When we look at correlations with the target, we need to be careful about the [multiple comparisons problem](https://towardsdatascience.com/the-multiple-comparisons-problem-e5573e8b9578): if we make a ton of features, some are likely to be correlated with the target simply because of random noise. Using correlations is fine as a first approximation for identifying ""good features"", but it is not a rigorous feature selection method.  

Also, based on examining some of the features, it seems there might be issues with [collinearity between features](https://en.wikipedia.org/wiki/Multicollinearity) made by featuretools. Features that are highly correlated with one another can diminish interpretability and generalization performance on the test set. In an ideal scenario, we would have a set of independent features, but that rarely occurs in practice. If there are very highly correlated varibables, we might want to think about removing some of them.

For the correlations, we will focus on the `feature_matrix_spec`, the features we made by specifying the primitives. The same analysis could be applied to the default feature set. These correlations were calculated using the entire training section of the feature matrix.",18,willkoehrsen,automated-feature-engineering-basics,willkoehrsen_automated-feature-engineering-basics_m18
### Correlations with the Target,19,willkoehrsen,automated-feature-engineering-basics,willkoehrsen_automated-feature-engineering-basics_m19
"Several of the features created by featuretools are among the most correlated with the `TARGET` (in terms of absolute magnitude). However, that does not mean they are necessarily ""important"". 

",20,willkoehrsen,automated-feature-engineering-basics,willkoehrsen_automated-feature-engineering-basics_m20
"### Visualize Distribution of Correlated Variables

One way we can look at the resulting features and their relation to the target is with a kernel density estimate plot. This shows the distribution of a single variable, and can be thought of as a smoothed histogram. To show the effect of a categorical variable on the distribution of a numeric variable, we can color the plot by th value of the categorical variable. In the plot below, we show the distribution of two of the newly created features, colored by the value of the target. 

First, we read in some of the feature matrix using the `nrows` argument of pandas `read_csv` function. This ensures we will not read in the entire 2 GB file. ",21,willkoehrsen,automated-feature-engineering-basics,willkoehrsen_automated-feature-engineering-basics_m21
"The correlation between this feature and the target is extremely weak and could be only noise. Trying to interpret this feature is difficult, but my best guess is: a client's maximum value of average number of atm drawings per month on previous credit card loans. (We are only using a sample of the features, so this might not be representative of the entire dataset).",22,willkoehrsen,automated-feature-engineering-basics,willkoehrsen_automated-feature-engineering-basics_m22
"Another area to investigate is highly correlated features, known as collinear features. We can look for pairs of correlated features and potentially remove any above a threshold.",23,willkoehrsen,automated-feature-engineering-basics,willkoehrsen_automated-feature-engineering-basics_m23
#### Collinear Features,24,willkoehrsen,automated-feature-engineering-basics,willkoehrsen_automated-feature-engineering-basics_m24
These variables all have a 0.99 correlation with each other which is nearly perfectly positively linear. Including them all in the model is unnecessary because it would be encoding redundant information. We would probably want to remove some of these highly correlated variables in order to help the model learn and generalize better. ,25,willkoehrsen,automated-feature-engineering-basics,willkoehrsen_automated-feature-engineering-basics_m25
"## Feature Importances

The feature importances returned by a tree-based model [represent the reduction in impurity](https://stackoverflow.com/questions/15810339/how-are-feature-importances-in-randomforestclassifier-determined) from including the feature in the model. While the absolute value of the importances can be difficult to interpret, looking at the relative value of the importances allows us to compare the relevance of features. Although we want to be careful about placing too much value on the feature importances, they can be a useful method for dimensionality reduction and understanding the model.",26,willkoehrsen,automated-feature-engineering-basics,willkoehrsen_automated-feature-engineering-basics_m26
"The most important feature created by featuretools was the maximum number of days before current application that the client applied for a loan at another institution. (This feature is originally recorded as negative, so the maximum value would be closest to zero).",27,willkoehrsen,automated-feature-engineering-basics,willkoehrsen_automated-feature-engineering-basics_m27
We can calculate the number of top 100 features that were made by featuretools. ,28,willkoehrsen,automated-feature-engineering-basics,willkoehrsen_automated-feature-engineering-basics_m28
Let's write a short function to visualize the 15 most important features. ,29,willkoehrsen,automated-feature-engineering-basics,willkoehrsen_automated-feature-engineering-basics_m29
"The most important feature created by featuretools was `MAX(bureau.DAYS_CREDIT)`. `DAYS_CREDIT` represents the number of days before the current application at Home Credit that the applicant applied for a loan at another credit institution. The maximum of this value (over the previous loans) is therefore represented by this feature. We also see several important features with a depth of two such as `MEAN(previous_app.MIN(installments.AMT_PAYMENT))` which is the average over a client's loans of the minimum value of previous credit application installment payments. 

Feature importances can be used for dimensionality reduction. They can also be used to help us better understand a problem. For example, we could use the most important features in order to concentrate on these aspects of a client when evaluating a potential loan. Let's look at the number of features with 0 importance which almost certainly can be removed from the featureset. ",30,willkoehrsen,automated-feature-engineering-basics,willkoehrsen_automated-feature-engineering-basics_m30
"## Remove Low Importance Features

Feature selection is an entire topic by itself, but one thing we can do is remove any features that have only a single unique value or are all null. Featuretools has a default method for doing this available in the `selection` module.",31,willkoehrsen,automated-feature-engineering-basics,willkoehrsen_automated-feature-engineering-basics_m31
"## Align Train and Test Sets

We also want to make sure the train and test sets have the same exact features. We can first one-hot encode the data (we'll have to do this anyway for our model) and then align the dataframes on the columns.",32,willkoehrsen,automated-feature-engineering-basics,willkoehrsen_automated-feature-engineering-basics_m32
Removing the low information features and aligning the dataframes has left us with 1689 features! Feature selection will certainly play an important role when using featuretools. ,33,willkoehrsen,automated-feature-engineering-basics,willkoehrsen_automated-feature-engineering-basics_m33
" # Conclusions

In this notebook we went through a basic implementation of using automated feature engineering with featuretools for the Home Credit Default Risk dataset. Although we did not use the advanced functionality of featuretools, we still were able to create useful features that improved the model's performance in cross validation and on the test set. Moreover, automated feature engineering took a fraction of the time spent manual feature engineering while delivering comparable results. 

__Even the default set of features in featuretools was able to achieve similar performance to hand-engineered features in less than 10% of the time.__
__Featuretools demonstrably adds value when included in a data scientist's toolbox.__

The next steps are to take advantage of the advanced functionality in featuretools combined with domain knowledge to create a more useful set of features. We will look explore [tuning featuretools in an upcoming notebook](https://www.kaggle.com/willkoehrsen/intro-to-tuning-automated-feature-engineering)!",34,willkoehrsen,automated-feature-engineering-basics,willkoehrsen_automated-feature-engineering-basics_m34
"## Appendix: GBM Model (Used Across Feature Sets)
```python
def model(features, test_features, encoding = 'ohe', n_folds = 5):
    
    """"""Train and test a light gradient boosting model using
    cross validation. 
    
    Parameters
    --------
        features (pd.DataFrame): 
            dataframe of training features to use 
            for training a model. Must include the TARGET column.
        test_features (pd.DataFrame): 
            dataframe of testing features to use
            for making predictions with the model. 
        encoding (str, default = 'ohe'): 
            method for encoding categorical variables. Either 'ohe' for one-hot encoding or 'le' for integer label encoding
            n_folds (int, default = 5): number of folds to use for cross validation
        
    Return
    --------
        submission (pd.DataFrame): 
            dataframe with `SK_ID_CURR` and `TARGET` probabilities
            predicted by the model.
        feature_importances (pd.DataFrame): 
            dataframe with the feature importances from the model.
        valid_metrics (pd.DataFrame): 
            dataframe with training and validation metrics (ROC AUC) for each fold and overall.
        
    """"""
    
    # Extract the ids
    train_ids = features['SK_ID_CURR']
    test_ids = test_features['SK_ID_CURR']
    
    # Extract the labels for training
    labels = features['TARGET']
    
    # Remove the ids and target
    features = features.drop(columns = ['SK_ID_CURR', 'TARGET'])
    test_features = test_features.drop(columns = ['SK_ID_CURR'])
    
    
    # One Hot Encoding
    if encoding == 'ohe':
        features = pd.get_dummies(features)
        test_features = pd.get_dummies(test_features)
        
        # Align the dataframes by the columns
        features, test_features = features.align(test_features, join = 'inner', axis = 1)
        
        # No categorical indices to record
        cat_indices = 'auto'
    
    # Integer label encoding
    elif encoding == 'le':
        
        # Create a label encoder
        label_encoder = LabelEncoder()
        
        # List for storing categorical indices
        cat_indices = []
        
        # Iterate through each column
        for i, col in enumerate(features):
            if features[col].dtype == 'object':
                # Map the categorical features to integers
                features[col] = label_encoder.fit_transform(np.array(features[col].astype(str)).reshape((-1,)))
                test_features[col] = label_encoder.transform(np.array(test_features[col].astype(str)).reshape((-1,)))

                # Record the categorical indices
                cat_indices.append(i)
    
    # Catch error if label encoding scheme is not valid
    else:
        raise ValueError(""Encoding must be either 'ohe' or 'le'"")
        
    print('Training Data Shape: ', features.shape)
    print('Testing Data Shape: ', test_features.shape)
    
    # Extract feature names
    feature_names = list(features.columns)
    
    # Convert to np arrays
    features = np.array(features)
    test_features = np.array(test_features)
    
    # Create the kfold object
    k_fold = KFold(n_splits = n_folds, shuffle = False, random_state = 50)
    
    # Empty array for feature importances
    feature_importance_values = np.zeros(len(feature_names))
    
    # Empty array for test predictions
    test_predictions = np.zeros(test_features.shape[0])
    
    # Empty array for out of fold validation predictions
    out_of_fold = np.zeros(features.shape[0])
    
    # Lists for recording validation and training scores
    valid_scores = []
    train_scores = []
    
    # Iterate through each fold
    for train_indices, valid_indices in k_fold.split(features):
        
        # Training data for the fold
        train_features, train_labels = features[train_indices], labels[train_indices]
        # Validation data for the fold
        valid_features, valid_labels = features[valid_indices], labels[valid_indices]
        
        # Create the model
        model = lgb.LGBMClassifier(n_estimators=10000, boosting_type = 'goss',
				   objective = 'binary', 
                                   class_weight = 'balanced', learning_rate = 0.05, 
                                   reg_alpha = 0.1, reg_lambda = 0.1, n_jobs = -1, random_state = 50)
        
        # Train the model
        model.fit(train_features, train_labels, eval_metric = 'auc',
                  eval_set = [(valid_features, valid_labels), (train_features, train_labels)],
                  eval_names = ['valid', 'train'], categorical_feature = cat_indices,
                  early_stopping_rounds = 100, verbose = 200)
        
        # Record the best iteration
        best_iteration = model.best_iteration_
        
        # Record the feature importances
        feature_importance_values += model.feature_importances_ / k_fold.n_splits
        
        # Make predictions
        test_predictions += model.predict_proba(test_features, num_iteration = best_iteration)[:, 1] / k_fold.n_splits
        
        # Record the out of fold predictions
        out_of_fold[valid_indices] = model.predict_proba(valid_features, num_iteration = best_iteration)[:, 1]
        
        # Record the best score
        valid_score = model.best_score_['valid']['auc']
        train_score = model.best_score_['train']['auc']
        
        valid_scores.append(valid_score)
        train_scores.append(train_score)
        
        # Clean up memory
        gc.enable()
        del model, train_features, valid_features
        gc.collect()
        
    # Make the submission dataframe
    submission = pd.DataFrame({'SK_ID_CURR': test_ids, 'TARGET': test_predictions})
    
    # Make the feature importance dataframe
    feature_importances = pd.DataFrame({'feature': feature_names, 'importance': feature_importance_values})
    
    # Overall validation score
    valid_auc = roc_auc_score(labels, out_of_fold)
    
    # Add the overall scores to the metrics
    valid_scores.append(valid_auc)
    train_scores.append(np.mean(train_scores))
    
    # Needed for creating dataframe of validation scores
    fold_names = list(range(n_folds))
    fold_names.append('overall')
    
    # Dataframe of validation scores
    metrics = pd.DataFrame({'fold': fold_names,
                            'train': train_scores,
                            'valid': valid_scores}) 
    
    return submission, feature_importances, metrics
```",35,willkoehrsen,automated-feature-engineering-basics,willkoehrsen_automated-feature-engineering-basics_m35
"# Introduction: Automated Hyperparameter Tuning

In this notebook, we will talk through a complete example of using automated hyperparameter tuning to optimize a machine learning model. In particular, we will use Bayesian Optimization and the Hyperopt library to tune the hyperparameters of a gradient boosting machine. 

__Additional Notebooks__ 

If you haven't checked out my other work on this problem, here is a complete list of the notebooks I have completed so far:

* [A Gentle Introduction](https://www.kaggle.com/willkoehrsen/start-here-a-gentle-introduction)
* [Manual Feature Engineering Part One](https://www.kaggle.com/willkoehrsen/introduction-to-manual-feature-engineering)
* [Manual Feature Engineering Part Two](https://www.kaggle.com/willkoehrsen/introduction-to-manual-feature-engineering-p2)
* [Introduction to Automated Feature Engineering](https://www.kaggle.com/willkoehrsen/automated-feature-engineering-basics)
* [Advanced Automated Feature Engineering](https://www.kaggle.com/willkoehrsen/tuning-automated-feature-engineering-exploratory)
* [Feature Selection](https://www.kaggle.com/willkoehrsen/introduction-to-feature-selection)
* [Intro to Model Tuning: Grid and Random Search](https://www.kaggle.com/willkoehrsen/intro-to-model-tuning-grid-and-random-search)
* [Automated Model Tuning](https://www.kaggle.com/willkoehrsen/automated-model-tuning)

There are four approaches to tuning the hyperparameters of a machine learning model

1. __Manual__: select hyperparameters based on intuition/experience/guessing, train the model with the hyperparameters, and score on the validation data. Repeat process until you run out of patience or are satisfied with the results.
2. __Grid Search__: set up a grid of hyperparameter values and for each combination, train a model and score on the validation data. In this approach, every single combination of hyperparameters values is tried which can be very inefficient!
3. __Random search__: set up a grid of hyperparameter values and select random combinations to train the model and score. The number of search iterations is set based on time/resources.
4. __Automated Hyperparameter Tuning__: use methods such as gradient descent, Bayesian Optimization, or evolutionary algorithms to conduct a guided search for the best hyperparameters.

These are listed in general order of least to most efficient. While we already conquered 2 and 3 [in this notebook](https://www.kaggle.com/willkoehrsen/intro-to-model-tuning-grid-and-random-search) (we didn't even try method 1), we have yet to take on automated hyperparameter tuning. There are a number of methods to do this including genetic programming, Bayesian optimization, and gradient based methods. Here we will focus only on Bayesian optimization, using the Tree Parzen Esimator (don't worry, you don't need to understand this in detail) in the [Hyperopt open-source Python library](https://hyperopt.github.io/hyperopt/).

For a little more background (we'll cover everything you need below), [here is an introductory article](https://towardsdatascience.com/an-introductory-example-of-bayesian-optimization-in-python-with-hyperopt-aae40fff4ff0) on Bayesian optimization, and [here is an article on automated hyperparameter tuning](https://towardsdatascience.com/automated-machine-learning-hyperparameter-tuning-in-python-dfda59b72f8a) using Bayesian optimization. Here we'll get right into automated hyperparameter tuning, so for the necessary background on model tuning, refer to [this kernel](https://www.kaggle.com/willkoehrsen/intro-to-model-tuning-grid-and-random-search)

## Bayesian Optimization Primer

The problem with grid and random search is that these are __uninformed methods__ because they do not use the past results from different values of hyperparameters in the objective function (remember the objective function takes in the hyperparameters and returns the model cross validation score). We record the results of the objective function for each set of hyperparameters, but the algorithms do not select the next hyperparameter values from this information. Intuitively, if we have the past results, we should  use them to reason about what hyperparameter values work the best and choose the next values wisely to try and spend more iterations evaluating promising values. Evaluating hyperparameters in the objective function is very time-consuming, and the __concept of Bayesian optimization is to limit calls to the evaluation function by choosing the next hyperparameter values based on the previous results.__ This allows the algorithm to spend __more time evaluating promising hyperparameter values and less time in low-scoring regions of the hyperparameter space__. For example, consider the image below:

![](https://raw.githubusercontent.com/WillKoehrsen/hyperparameter-optimization/master/images/random_forest_hypothetical.png)

If you were choosing the next number of trees to try for the random forest, where would you concentrate your search? Probably around 100 trees because that is where the lowest errors have tended to occur (imagine this is a problem where we want to minimize the error). In effect, you have just done Bayesian hyperparameter optimization in your head! You formed a probability model of the error as a function of the hyperparameters and then selected the next hyperparameter values by maximizing the probability of a low error. Bayesian optimization works by building a surrogate function (in the form of a probability model) of the objective function $P(\text{score} | \text{hyperparameters}$. The surrogate function is much cheaper to evaluate than the objective, so the algorithm chooses the next values to try in the objective based on maximizing a criterion on the surrogate (usually expected improvement), exactly what you would have done with respect to the image above. 

The surrogate function is based on past evaluation results - pairs of (score, hyperparameter) records - and is continually updated with each objective function evaluation. Bayesian optimization therefore uses Bayesian reasoning: form an initial model (called a prior) and then update it with more evidence. The idea is that as the data accumulates, the surrogate function gets closer and closer to the objective function, and the hyperparameter values that are the best in the surrogate function will also do the best in the objective function. Bayesian optimization methods differ in the algorithm used to build the surrogate function and choose the next hyperparameter values to try. Some of the common choices are Gaussian Process (implemented in Spearmint), Random Forest Regression (in SMAC), and the Tree Parzen Estimator (TPE) in Hyperopt (technical details can be found in this article, although they won't be necessary to use the methods).

### Four Part of Bayesian Optimization

Bayesian hyperparameter optimization requires the same four parts as we implemented in grid and random search:

1. __Objective Function__: takes in an input (hyperparameters) and returns a score to minimize or maximize (the cross validation score)
2. __Domain space__: the range of input values (hyperparameters) to evaluate
3. __Optimization Algorithm__: the method used to construct the surrogate function and choose the next values to evaluate
4. __Results__: score, value pairs that the algorithm uses to build the surrogate function

The only differences are that now our objective function will return a score to minimize (this is just convention in the field of optimization), our domain space will be probability distributions rather than a hyperparameter grid, and the optimization algorithm will be an __informed method__ that uses past results to choose the next hyperparameter values to evaluate. 

## Hyperopt

Hyperopt is an open-source Python library the implements Bayesian Optimization using the Tree Parzen Estimator algorithm to construct the surrogate function and select the next hyperparameter values to evaluate in the objective function. There are a number of other libraries such as Spearmint (Guassian process surrogate function) and SMAC (random forest regression surrogate function) sharing the same problem structure. The four parts of an optimization problem that we develop here will apply to all the libraries with only a change in syntax. Morevoer, the optimization methods as applied to the Gradient Boosting Machine will translate to other machine learning models or any problem where we have to minimize a function.

### Gradient Boosting Machine

We will use the gradient booosting machine (GBM) as our model to tune in the LightGBM library. The GBM is our choice of model because it performs extremely well for these types of problems (as shown on the leaderboard) and because the performance is heavily dependent on the choice of hyperparameter values. 

For more details of the Gradient Boosting Machine (GBM), check out this high-level blog post, or this in depth technical article.

### Cross Validation with Early Stopping

As with random and grid search, we will evaluate each set of hyperparameters using 5 fold cross validation on the training data. The GBM model will be trained with early stopping, where estimators are added to the ensemble until the validation score has not decrease for 100 iterations (estimators added). 

Cross validation and early stopping will be implemented using the LightGBM `cv` function. We will use 5 folds and 100 early stopping rounds. 

#### Dataset and Approach

As before, we will work with a limited section of the data - 10000 observations for training and 6000 observations for testing. This will allow the optimization within the notebook to finish in a reasonable amount of time. Later in the notebook, I'll present results from 1000 iterations of Bayesian hyperparameter optimization on the reduced dataset and we then will see if these results translate to a full dataset (from [this kernel](https://www.kaggle.com/jsaguiar/updated-0-792-lb-lightgbm-with-simple-features)). The functions developed here can be taken and run on any dataset, or used with any machine learning model (just with minor changes in the details) and working with a smaller dataset will allow us to learn all of the concepts. I am currently running 500 iterations of Bayesian hyperparameter optimization on a complete dataset and will make the results available when the search is completed. ",0,willkoehrsen,automated-model-tuning,willkoehrsen_automated-model-tuning_m0
"With the background details out of the way, let's get started with Bayesian optimization applied to automated hyperparameter tuning! ",1,willkoehrsen,automated-model-tuning,willkoehrsen_automated-model-tuning_m1
The code below reads in the data and creates a smaller version for training and a set for testing. We can only use the training data __a single time__ when we evaluate the final model. Hyperparameter tuning must be done on the training data using cross validation!,2,willkoehrsen,automated-model-tuning,willkoehrsen_automated-model-tuning_m2
"### Baseline Model 

First we can create a model with the default value of hyperparameters and score it using cross validation with early stopping. Using the `cv` LightGBM function requires creating a `Dataset`.",3,willkoehrsen,automated-model-tuning,willkoehrsen_automated-model-tuning_m3
Now we can evaluate the baseline model on the testing data.,4,willkoehrsen,automated-model-tuning,willkoehrsen_automated-model-tuning_m4
"# Objective Function

The first part to write is the objective function which takes in a set of hyperparameter values and returns the cross validation score on the training data. An objective function in Hyperopt must return either a single real value to minimize, or a dictionary with a key ""loss"" with the score to minimize (and a key ""status"" indicating if the run was successful or not). 

Optimization is typically about minimizing a value, and because our metric is Receiver Operating Characteristic Area Under the Curve (ROC AUC) where higher is better, the objective function will return $1 - \text{ROC AUC Cross Validation}$. The algorithm will try to drive this value as low as possible (raising the ROC AUC) by choosing the next hyperparameters based on the past results. 

The complete objective function is shown below. As with random and grid search, we write to a `csv` file on each call of the function in order to track results as the search progress and so we have a saved record of the search. (The `subsample` and `boosting_type` logic will be explained when we get to the domain). ",5,willkoehrsen,automated-model-tuning,willkoehrsen_automated-model-tuning_m5
"# Domain 

Specifying the domain (called the space in Hyperopt) is a little trickier than in grid search. In Hyperopt, and other Bayesian optimization frameworks, the domian is not a discrete grid but instead has probability distributions for each hyperparameter. For each hyperparameter, we will use the same limits as with the grid, but instead of being defined at each point, the domain represents probabilities for each hyperparameter. This will probably become clearer in the code and the images!",6,willkoehrsen,automated-model-tuning,willkoehrsen_automated-model-tuning_m6
"First we will go through an example of the learning rate. We are using a log-uniform space for the learning rate defined from 0.005 to 0.5. The log - uniform distribution has the values evenly placed in logarithmic space rather than linear space. This is useful for variables that differ over several orders of magnitude such as the learning rate. For example, with a log-uniform distribution, there will be an equal chance of drawing a value from 0.005 to 0.05 and from 0.05 to 0.5 (in linear space far more values would be drawn from the later since the linear distance is much larger. The logarithmic space is exactly the same - a factor of 10).",7,willkoehrsen,automated-model-tuning,willkoehrsen_automated-model-tuning_m7
We can visualize the learning rate by drawing 10000 samples from the distribution.,8,willkoehrsen,automated-model-tuning,willkoehrsen_automated-model-tuning_m8
The number of leaves on the other hand is a discrete uniform distribution.,9,willkoehrsen,automated-model-tuning,willkoehrsen_automated-model-tuning_m9
"### Conditional Domain

In Hyperopt, we can use nested conditional statements to indicate hyperparameters that depend on other hyperparameters. For example, the ""goss"" `boosting_type` cannot use subsampling, so when we set up the `boosting_type` categorical variable, we have to set the subsample to 1.0 while for the other boosting types it's a float between 0.5 and 1.0.",10,willkoehrsen,automated-model-tuning,willkoehrsen_automated-model-tuning_m10
"We need to set both the boosting_type and subsample as top-level keys in the parameter dictionary. We can use the Python dict.get method with a default value of 1.0. This means that if the key is not present in the dictionary, the value returned will be the default (1.0).",11,willkoehrsen,automated-model-tuning,willkoehrsen_automated-model-tuning_m11
"The gbm cannot use the nested dictionary so we need to set the `boosting_type` and `subsample` as top level keys. 

Nested conditionals allow us to use a different set of hyperparameters depending on other hyperparameters. For example, we can explore different models with completely different sets of hyperparameters by using nested conditionals. The only requirement is that the first nested statement must be based on a choice hyperparameter (the choice could be the type of model).",12,willkoehrsen,automated-model-tuning,willkoehrsen_automated-model-tuning_m12
"## Complete Bayesian Domain

Now we can define the entire domain. Each variable needs to have a label and a few parameters specifying the type and extent of the distribution. For the variables such as boosting type that are categorical, we use the choice variable. Other variables types include quniform, loguniform, and uniform. For the complete list, check out the documentation for Hyperopt. Altogether there are 10 hyperparameters to optimize. ",13,willkoehrsen,automated-model-tuning,willkoehrsen_automated-model-tuning_m13
"### Example of Sampling from the Domain

Let's sample from the domain (using the conditional logic) to see the result of each draw. Every time we run this code, the results will change. (Again notice that we need to assign the top level keys to the keywords understood by the GBM).",14,willkoehrsen,automated-model-tuning,willkoehrsen_automated-model-tuning_m14
"Let's test the objective function with the domain to make sure it works. (Every time the `of_connection` line is run, the `outfile` will be overwritten, so use a different name for each trial to save the results.)",15,willkoehrsen,automated-model-tuning,willkoehrsen_automated-model-tuning_m15
"# Optimization Algorithm

The optimization algorithm is the method for constructing the surrogate function (probability model) and selecting the next set of hyperparameters to evaluate in the objective function. Hyperopt has two choices: random search and Tree Parzen Estimator. 

The technical details of TPE can be found in [this article](https://papers.nips.cc/paper/4443-algorithms-for-hyper-parameter-optimization.pdf) and a conceptual explanation is in [this article](https://towardsdatascience.com/a-conceptual-explanation-of-bayesian-model-based-hyperparameter-optimization-for-machine-learning-b8172278050f). Although this is the most technical part of Bayesian hyperparameter optimization, defining the algorithm in Hyperopt is simple. ",16,willkoehrsen,automated-model-tuning,willkoehrsen_automated-model-tuning_m16
"# Results History
The final part is the history of objective function evaluations. Although Hyperopt internally keeps track of the results for the algorithm to use, if we want to monitor the results and have a saved copy of the search, we need to store the results ourselves. Here, we are using two methods to make sure we capture all the results:

1. A `Trials` object that stores the dictionary returned from the objective function
2. Adding a line to a csv file every iteration.

The csv file option also lets us monitor the results of an on-going experiment. Although do not use Excel to open the file while training is on-going. Instead check the results using `tail results/out_file.csv` from bash or open the file in Sublime Text or Notepad.",17,willkoehrsen,automated-model-tuning,willkoehrsen_automated-model-tuning_m17
"The `Trials` object will hold everything returned from the objective function in the `.results` attribute. We can use this after the search is complete to inspect the results, but an easier method is to read in the `csv` file because that will already be in a dataframe.",18,willkoehrsen,automated-model-tuning,willkoehrsen_automated-model-tuning_m18
"# Automated Hyperparameter Optimization in Practice

We have all four parts we need to run the optimization. To run Bayesian optimization we use the `fmin` function (a good reminder that we need a metric to minimize!) ",19,willkoehrsen,automated-model-tuning,willkoehrsen_automated-model-tuning_m19
`fmin` takes the four parts defined above as well as the maximum number of iterations `max_evals`. ,20,willkoehrsen,automated-model-tuning,willkoehrsen_automated-model-tuning_m20
"The `best` object holds only the hyperparameters that returned the lowest loss in the objective function. Although this is ultimately what we are after, if we want to understand how the search progresses, we need to inspect the `Trials` object or the `csv` file. For example, we can sort the `results` returned from the objective function by the lowest loss:",21,willkoehrsen,automated-model-tuning,willkoehrsen_automated-model-tuning_m21
An easier method is to read in the csv file since this will be a dataframe. ,22,willkoehrsen,automated-model-tuning,willkoehrsen_automated-model-tuning_m22
"The function below takes in the results, trains a model on the training data, and evalutes on the testing data. It returns a dataframe of hyperparameters from the search. 

Saving the results to a csv file converts the dictionary of hyperparameters to a string. We need to map this back to a dictionary using `ast.literal_eval`. ",23,willkoehrsen,automated-model-tuning,willkoehrsen_automated-model-tuning_m23
"## Continue Optimization

Hyperopt can continue searching where a previous search left off if we pass in a `Trials` object that already has results. The algorithms used in Bayesian optimization are black-box optimizers because they have no internal state. All they need is the previous results of objective function evaluations (the input values and loss) and they can build up the surrogate function and select the next values to evaluate in the objective function. This means that any search can be continued as long as we have the history in a `Trials` object. ",24,willkoehrsen,automated-model-tuning,willkoehrsen_automated-model-tuning_m24
"To save the `Trials` object so it can be read in later for more training, we can use the `json` format. ",25,willkoehrsen,automated-model-tuning,willkoehrsen_automated-model-tuning_m25
"To start the training from where it left off, simply load in the `Trials` object and pass it to an instance of `fmin`. (You might even be able to tweak the hyperparameter distribution and continue searching with the `Trials` object because the algorithm does not maintain an internal state. Someone should check this and let me know in the comments!).",26,willkoehrsen,automated-model-tuning,willkoehrsen_automated-model-tuning_m26
"## Next Steps

Now that we have developed all the necessary parts for automated hyperparameter tuning using Bayesian optimization, we can apply these to any dataset or any machine learning method. The functions taken here can be put in a script and run a full dataset. Next, we will go through results from 1000 evaluations on a reduced size dataset to see how the search progresses. We can then compare these results to random search to see how a method that uses __reasoning__ about past results differs from a method that does not. 

After examining the tuning results from the reduced dataset, we will take the best performing hyperparameters and see if these translate to a full dataset, the features from the `[Updated 0.792 LB] LightGBM with Simple Features`(https://www.kaggle.com/jsaguiar/updated-0-792-lb-lightgbm-with-simple-features) kernel (I did not develop these features and want to give credit to the numerous people, including [Aguiar](https://www.kaggle.com/jsaguiar) and [olivier](https://www.kaggle.com/ogrellier),  who have worked on these features. Please check out their [kernels](https://www.kaggle.com/ogrellier/lighgbm-with-selected-features)!). We saw in the random and grid search notebook that the best hyperparameter values from the small datasets do not necessarily perform well on the full datasets. I am currently running the Bayesian Hyperparameter optimization for 500 iterations on the features referenced above and will make the results publicly available when the search is finished. For now, we will turn to the 1000 trials from the smaller dataset. These results can be generated by running the cell below, but I can't guarantee if this will finish within the kernel time limit! ",27,willkoehrsen,automated-model-tuning,willkoehrsen_automated-model-tuning_m27
"# Search Results 

Next we will go through the results from 1000 search iterations on the reduced dataset. We will look at the scores, the distribution of hyperparameter values tried, the evolution of values over time, and compare the hyperparameters values to those from random search.

After examining the search results, we will use the optimized hyperparameters (at least optimized for the smaller dataset) to make predictions on a full dataset. These can then be submitted to the competition to see how well the methods do on a small sample of the data.",28,willkoehrsen,automated-model-tuning,willkoehrsen_automated-model-tuning_m28
## Learning Rate Distribution ,29,willkoehrsen,automated-model-tuning,willkoehrsen_automated-model-tuning_m29
"We can see that the Bayesian search did worse in cross validation but then found hyperparameter values that did better on the test set! We will have to see if these results translate to the acutal competition data. First though, we can get all the scores in a dataframe in order to plot them over the course of training.",30,willkoehrsen,automated-model-tuning,willkoehrsen_automated-model-tuning_m30
We can also find the best scores for plotting the best hyperparameter values.,31,willkoehrsen,automated-model-tuning,willkoehrsen_automated-model-tuning_m31
"Below is the code showing the progress of scores versus the iteration. For random search we do not expect to see a pattern, but for Bayesian optimization, we expect to see the scores increasing with the search as more promising hyperparameter values are tried.",32,willkoehrsen,automated-model-tuning,willkoehrsen_automated-model-tuning_m32
"Sure enough, we see that the Bayesian hyperparameter optimization scores increase as the search continues. This shows that more promising values (at least on the cross validation reduced dataset) were tried as the search progressed. Random search does record a better score, but the results do not improve over the course of the search. In this case, it looks like if we were to continue searching with Bayesian optimization, we would eventually reach higher scores on the cross vadidation data. 

For fun, we can make the same plot in Altair.",33,willkoehrsen,automated-model-tuning,willkoehrsen_automated-model-tuning_m33
"Same chart, just in a different library for practice! 

## Learning Rate Distribution

Next we can start plotting the distributions of hyperparameter values searched. We expect random search to align with the search domain, while the Bayesian hyperparameter optimization should tend to focus on more promising values, wherever those happen to be in the search domain.

The dashed vertical lines indicate the ""optimal"" value of the hyperparameter.",34,willkoehrsen,automated-model-tuning,willkoehrsen_automated-model-tuning_m34
"## Distribution of all Numeric Hyperparameters

We can make the same chart now for all of the hyperparameters. For each setting, we plot the values tried by random search and bayesian optimization, as well as the sampling distirbution.",35,willkoehrsen,automated-model-tuning,willkoehrsen_automated-model-tuning_m35
"## Evolution of Search

An interesting series of plots to make is the evolution of the hyperparameters over the search. This can show us what values the Bayesian optimization tended to focus on. The average cross validation score continued to improve throughout Bayesian optimization, indicating that ""more promising"" values of the hyperparameters were being evaluated and maybe a longer search would prove useful (or there could be a plateau in the validation scores with a longer search).",36,willkoehrsen,automated-model-tuning,willkoehrsen_automated-model-tuning_m36
The final plot is just a bar chart of the `boosting_type`. ,37,willkoehrsen,automated-model-tuning,willkoehrsen_automated-model-tuning_m37
"The Bayes optimization spent many more iterations using the `dart` boosting type than would be expected from a uniform distribution. We can use information such as this in further hyperparameter tuning. For example, we could use the distributions from Bayesian hyperparameter optimization to make a more focused hyperparameter grid for grid or even random search. 

![](http://)For this chart, we can also make it in Altair for the practice.",38,willkoehrsen,automated-model-tuning,willkoehrsen_automated-model-tuning_m38
"## Applied to Full Dataset

Now, we can take the best hyperparameters found from 1000 iterations of Bayesian hyperparameter optimization on the smaller dataset and apply these to a full dataset of features from the `[Updated 0.792 LB] LightGBM with Simple Features`(https://www.kaggle.com/jsaguiar/updated-0-792-lb-lightgbm-with-simple-features) kernel. The best hyperparameters from the smaller dataset will not necessarily be the best on the full dataset (because the small dataset does nto perfectly represent the entire data), but we can at least try them out. We will train a model using the optimal hyperparameters from Bayesian optimization using early stopping to determine the number of estimators. ",39,willkoehrsen,automated-model-tuning,willkoehrsen_automated-model-tuning_m39
### Random Search on the Full Dataset,40,willkoehrsen,automated-model-tuning,willkoehrsen_automated-model-tuning_m40
Then we can make predictions on the test data. The predictions are saved to a csv file that can be submitted to the competition.,41,willkoehrsen,automated-model-tuning,willkoehrsen_automated-model-tuning_m41
Submitting these to the competition results in a score of __0.787__ which compares to the original score from the kernel of __0.792__,42,willkoehrsen,automated-model-tuning,willkoehrsen_automated-model-tuning_m42
### Bayesian Optimization on the Full Dataset,43,willkoehrsen,automated-model-tuning,willkoehrsen_automated-model-tuning_m43
"Submitting these to the competition results in a score of __0.792__. So the Bayesian optimization outperforms the random search based on 1000 iterations on a reduced sized dataset. I wouldn't put too much weight into these results because we saw that random search actually yielded a higher cross validation score. Nonetheless, Bayesian hyperparameter optimization can be an effective technique for automated machine learning model tuning.",44,willkoehrsen,automated-model-tuning,willkoehrsen_automated-model-tuning_m44
"# Conclusions

Bayesian optimization is one method for automated hyperparameter tuning. Automated hyperparameter tuning aims to find the best hyperparameter values for a machine learning model on a given dataset with no input from the data scientist beyond initial set-up required. Bayesian optimization uses Bayesian reasoning to build a probability model of the objecitve function $P(\text{score} | \text{hyperparameters})$ which is then used to select the next hyperparameter values to evaluate. The concept is to use more search iterations evaluating promising hyperparameter values by reasoning from the past results. This is an intuitive method of hyperparamter optimization that works in much the same way a human does to get better at any situation: learn from past experiences! If everything works as expected, Bayesian hyperparameter optimization can result in:

* Better generalization performance on the test set
* Fewer iterations than random or grid search require

Even if Bayesian optimization (or other automated hyperparameter tuning methods) does not deliver on the above points in all situations, it is a useful skill to master as a data scientist. In the future, data scientists are not going to spend valuable time tweaking model hyperparameters, and knowing methods for accomplishing this automatically will go a long way in your career or studies! Feel free to take this code and apply it to any dataset or to a different machine learning model. I am currently running these methods on a full dataset and will share the results when they are completed. I look forward to the next notebook! 

As always, I welcome feedback and constructive criticism. I write for Towards Data Science at https://medium.com/@williamkoehrsen/ and can be reached on Twitter at https://twitter.com/koehrsen_will. Thanks for reading to the end and keep making the data science community such a wonderful place for learning and sharing, 

Will",45,willkoehrsen,automated-model-tuning,willkoehrsen_automated-model-tuning_m45
"# Clean Manual Feature Engineering

The purpose of this notebook is to clean up the manual feature engineering I had scattered over several other kernels. We will implement the complete manual feature engineering and then test the results.

Update August 7: __After some modifications, this can now run in a kernel!__ The features themselves are available at https://www.kaggle.com/willkoehrsen/home-credit-manual-engineered-features under `clean_manual.csv`. The feature importances for these features in a gradient boosting model are also available at the same link with the name `fi_clean_manual.csv`. 

### Roadmap

Our plan of action is as follows.We have to be very careful about memory usage in the kernels, which affects the order of operations:

1. Define functions:
    * `agg_numeric`
    * `agg_categorical`
    * `agg_child` 
    * `agg_grandchild`
 2. Add in domain knowledge features to `app`
 3. Work through the `bureau` and `bureau_balance` data
     * Add in hand built features
     * Aggregate both using the appropriate functions
     * Merge with `app` and delete the dataframes
4. Work through `previous`, `installments`, `cash`, and `credit`
    * Add in hand built features
    * Aggregate using the appropriate functions
    * Merge with `app` and delete the dataframes
5. Modeling using a Gradient Boosting Machine
    * Train model on training data using best hyperparameters from random search notebook
    * Make predictions and submit


",0,willkoehrsen,clean-manual-feature-engineering,willkoehrsen_clean-manual-feature-engineering_m0
"# Numeric Aggregation Function

The following function aggregates all the numeric variables in a child dataframe at the parent level. That is, for each parent, gather together (group) all of their children, and calculate the aggregations statistics across the children. The function also removes any columns that share the exact same values (which might happen using `count`). ",1,willkoehrsen,clean-manual-feature-engineering,willkoehrsen_clean-manual-feature-engineering_m1
"# Categorical Aggregation Function

Much like the numerical aggregation function, the `agg_categorical` function works on a child dataframe to aggregate statistics at the parent level. This can work with any child of `app` and might even be extensible to other problems with only minor changes in syntax.",2,willkoehrsen,clean-manual-feature-engineering,willkoehrsen_clean-manual-feature-engineering_m2
"# Combined Aggregation Function

We can put these steps together into a function that will handle a child dataframe. The function will take care of both the numeric and categorical variables and will return the result of merging the two dataframes. ",3,willkoehrsen,clean-manual-feature-engineering,willkoehrsen_clean-manual-feature-engineering_m3
"This function can be applied to both `bureau` and `previous` because these are direct children of `app`. For the children of the children, we will need to take an additional aggregation step. ",4,willkoehrsen,clean-manual-feature-engineering,willkoehrsen_clean-manual-feature-engineering_m4
"# Aggregate Grandchild Data Tables

Several of the tables (`bureau_balance, cash, credit_card`, and `installments`) are children of the child dataframes. In other words, these are grandchildren of the main `app` data table. To aggregate these tables, they must first be aggregated at the parent level (which is on a per loan basis) and then at the grandparent level (which is on the client basis). For example, in the `bureau_balance` dataframe, there is monthly information on the loans in `bureau`. To get this data into the `app` dataframe will first require grouping the monthly information for each loan and then grouping the loans for each client. 

Hopefully, the nomenclature does not get too confusing, but here's a rounddown:

* __grandchild__: the child of a child data table, for instance, `bureau_balance`. For every row in the child table, there can be multiple rows in the grandchild. 
* __parent__: the parent table of the grandchild that links the grandchild to the grandparent. For example, the `bureau` dataframe is the parent of the `bureau_balance` dataframe in this situation. `bureau` is in turn the child of the `app` dataframe. `bureau_balance` can be connected to `app` through `bureau`.
* __grandparent__: the parent of the parent of the grandchild, in this problem the `app` dataframe. The end goal is to aggregate the information in the grandchild into the grandparent. This will be done in two stages: first at the parent (loan) level and then at the grandparent (client) level
* __parent variable__: the variable linking the grandchild to the parent. For the `bureau` and `bureau_balance` data this is `SK_ID_BUREAU` which uniquely identifies each previous loan
* __grandparent variable__: the variable linking the parent to the grandparent. This is `SK_ID_CURR` which uniquely identifies each client in `app`.

### Aggregating Grandchildren Function

We can take the individual steps required for aggregating a grandchild dataframe at the grandparent level in a function. These are:

1. Aggregate the numeric variables at the parent (the loan, `SK_ID_BUREAU` or `SK_ID_PREV`) level.
2. Merge with the parent of the grandchild to get the grandparent variable in the data (for example `SK_ID_CURR`)
3. Aggregate the numeric variables at the grandparent (the client, `SK_ID_CURR`) level. 
4. Aggregate the categorical variables at the parent level.
5. Merge the aggregated data with the parent to get the grandparent variable
6. Aggregate the categorical variables at the grandparent level
7. Merge the numeric and categorical dataframes on the grandparent varible
8. Remove the columns with all duplicated values.
9. The resulting dataframe should now have one row for every grandparent (client) observation
10. Merge with the main dataframe (`app`) on the grandparent variable (`SK_ID_CURR`). 

This function can be applied to __all 4 grandchildren__ without the need for hard-coding in specific variables. ",5,willkoehrsen,clean-manual-feature-engineering,willkoehrsen_clean-manual-feature-engineering_m5
"# Putting it Together

Now that we have the individual pieces of semi-automated feature engineering, we need to put them together. There are two functions that can handle the children and the grandchildren data tables:

1. `agg_child(df, parent_var, df_name)`: aggregate the numeric and categorical variables of a child dataframe at the parent level. For example, the `previous` dataframe is a child of the `app` dataframe that must be aggregated for each client. 
2. `agg_grandchild(df, parent_df, parent_var, grandparent_var, df_name)`: aggregate the numeric and categorical variables of a grandchild dataframe at the grandparent level. For example, the `bureau_balance` dataframe is the grandchild of the `app` dataframe with `bureau` as the parent. 

For each of the children dataframes of `app`, (`previous` and `bureau`), we will use the first function and merge the result into the `app` on the parent variable, `SK_ID_CURR`. For the four grandchild dataframes, we will use the second function, which returns a single dataframe that can then be merged into app on `SK_ID_CURR`. ",6,willkoehrsen,clean-manual-feature-engineering,willkoehrsen_clean-manual-feature-engineering_m6
"## Hand-Built Features

Along the way, we will add in hand-built features to the datasets. These have come from my own ideas (probably not very optimal) and from the community.

First we will add in ""domain knowledge"" features to the `app` dataframe. These were developed based on work done in other kernels (both from the community and my own work)",7,willkoehrsen,clean-manual-feature-engineering,willkoehrsen_clean-manual-feature-engineering_m7
"### Hand-Built Features for other Dataframes

We can also add in hand built features for the other dataframes. Since these are not the main dataframe, these features will end up being aggregated in different ways. These will be added as we go through the tables.",8,willkoehrsen,clean-manual-feature-engineering,willkoehrsen_clean-manual-feature-engineering_m8
"#### Aggregate the bureau data

First add the loan rate for previous loans at other institutions.",9,willkoehrsen,clean-manual-feature-engineering,willkoehrsen_clean-manual-feature-engineering_m9
"#### Aggregate the bureau balance

Now we turn to the `bureau_balance` dataframe. We will make a column indicating whether a loan was past due for the month or whether the payment was on time.",10,willkoehrsen,clean-manual-feature-engineering,willkoehrsen_clean-manual-feature-engineering_m10
"## Merge with the main dataframe

The individual dataframes can all be merged into the main `app` dataframe. Merging is much quicker if done on any index, so it's good practice to first set the index to the variable on which we will merge. In each case, we use a `left` join so that all the observations in `app` are kept even if they are not present in the other dataframes (which occurs because not every client has previous records at Home Bureau or other credit institutions). After each step of mergning, we remove the dataframe from memory in order to hopefully let the kernel continue to run.

The final result is one dataframe with a single row for each client that can be used for training a machine learning model. ",11,willkoehrsen,clean-manual-feature-engineering,willkoehrsen_clean-manual-feature-engineering_m11
"#### Aggregate previous loans at Home Credit

We will add in two domain features, first the loan rate and then the difference between the amount applied for and the amount awarded.",12,willkoehrsen,clean-manual-feature-engineering,willkoehrsen_clean-manual-feature-engineering_m12
`AMT_DIFFERENCE` is the difference between what was given to the client and what the client requested on previous loans at Home Credit.,13,willkoehrsen,clean-manual-feature-engineering,willkoehrsen_clean-manual-feature-engineering_m13
"#### Aggregate Installments Data

The installments table has each installment (payment) for previous loans at Home Credit. We can create a column indicating whether or not a loan was late.",14,willkoehrsen,clean-manual-feature-engineering,willkoehrsen_clean-manual-feature-engineering_m14
`LOW_PAYMENT` represents a payment that was less than the prescribed amount. ,15,willkoehrsen,clean-manual-feature-engineering,willkoehrsen_clean-manual-feature-engineering_m15
"#### Aggregate Cash previous loans

The next dataframe is the `cash` which has monthly information on previous cash loans at Home Credit. We can create a column indicating if the loan was overdue for the month. ",16,willkoehrsen,clean-manual-feature-engineering,willkoehrsen_clean-manual-feature-engineering_m16
`INSTALLMENTS_PAID` is meant to represent the number of already paid (or I guess missed) installments by subtracting the future installments from the total installments.,17,willkoehrsen,clean-manual-feature-engineering,willkoehrsen_clean-manual-feature-engineering_m17
"#### Aggregate Credit previous loans

The last dataframe is `credit` which has previous credit card loans at Home Credit. We can make a column indicating whether the balance is greater than the credit limit, a column showing whether or not the balance was cleared (equal to 0), whether or not the payment was below the prescribed amount, and whether or not the payment was behind. Then we aggregate as with the other grandchildren.",18,willkoehrsen,clean-manual-feature-engineering,willkoehrsen_clean-manual-feature-engineering_m18
"__This is usually the point at which the kernel fails.__ To try and alleviate the problem, I have added a pause of 10 minutes.",19,willkoehrsen,clean-manual-feature-engineering,willkoehrsen_clean-manual-feature-engineering_m19
__Update August 7__: The kernel can now run!,20,willkoehrsen,clean-manual-feature-engineering,willkoehrsen_clean-manual-feature-engineering_m20
"# Modeling

After all the hard work, now we get to test our features! We will use a model with the hyperparameters from random search that are documented in another notebook. 

The final model scores __0.792__ when uploaded to the competition.",21,willkoehrsen,clean-manual-feature-engineering,willkoehrsen_clean-manual-feature-engineering_m21
"## Feature Importances

Now we can see if all that time was worth it! In the code below, we find the most important features and show them in a plot and dataframe.",22,willkoehrsen,clean-manual-feature-engineering,willkoehrsen_clean-manual-feature-engineering_m22
"# Conclusions

This code is a little too much to run in the Kaggle kernels. However, the features themselves are available at https://www.kaggle.com/willkoehrsen/home-credit-manual-engineered-features under `clean_manual.csv`. The feature importances for these features in a gradient boosting model are also available at the same link with the name `fi_clean_manual.csv`. 

This notebook is meant to serve as a clean version of the manual feature engineering I had scattered across several other notebooks. We were able to build a complete set of __ features that scored 0.792 on the public leaderboard__. Further hyperparameter tuning might improve the performance. For additional feature engineering, we will probably want to turn to more technical operations such as treating this as a time-series problem. Since we have relative time information (relative to the current loan at Home Credit), it's possible to find the most recent information and also trends over time. These can be useful because changes in behavior might inform us as to whether or not a client will be able to repay a loan! 

Thanks for reading and as always, I welcome feedback and constructive criticism. I'll see you in the next notebook.

Best,

Will",23,willkoehrsen,clean-manual-feature-engineering,willkoehrsen_clean-manual-feature-engineering_m23
"# Introduction: Hyperparameter Tuning using Grid and Random Search

In this notebook, we will explore two methods for hyperparameter tuning a machine learning model. [In contrast](https://machinelearningmastery.com/difference-between-a-parameter-and-a-hyperparameter/) to model __parameters__ which are learned during training, model __hyperparameters__ are set by the data scientist ahead of training and control implementation aspects of the model. The weights learned during training of a linear regression model are parameters while the number of trees in a random forest is a model hyperparameter because this is set by the data scientist. Hyperparameters can be thought of as model settings. These settings need to be tuned for each problem because the best model hyperparameters for one particular dataset will __not be__ the best across all datasets. The process of [hyperparameter tuning (also called hyperparameter optimization)](https://en.wikipedia.org/wiki/Hyperparameter_optimization) means finding the combination of hyperparameter values for a machine learning model that performs the best - as measured on a validation dataset - for a problem. 

(__Quick Note__: a lot of data scientists use the terms _parameters_ and _hyperparameters_ interchangeably to refer to the model settings. While this is technically incorrect, it's pretty common practice and it's usually possible to tell when they are referring to parameters learned during training versus hyperparameters. I'll try to stick to using model hyperparameters or model settings and I'll  point out when I'm talking about a parameter that is learned during training. If you're still confused, [this article](https://machinelearningmastery.com/difference-between-a-parameter-and-a-hyperparameter/) may help you out!)

__Additional Notebooks__ 

If you haven't checked out my other work on this problem, here is a complete list of the notebooks I have completed so far:

* [A Gentle Introduction](https://www.kaggle.com/willkoehrsen/start-here-a-gentle-introduction)
* [Manual Feature Engineering Part One](https://www.kaggle.com/willkoehrsen/introduction-to-manual-feature-engineering)
* [Manual Feature Engineering Part Two](https://www.kaggle.com/willkoehrsen/introduction-to-manual-feature-engineering-p2)
* [Introduction to Automated Feature Engineering](https://www.kaggle.com/willkoehrsen/automated-feature-engineering-basics)
* [Advanced Automated Feature Engineering](https://www.kaggle.com/willkoehrsen/tuning-automated-feature-engineering-exploratory)
* [Feature Selection](https://www.kaggle.com/willkoehrsen/introduction-to-feature-selection)
* [Intro to Model Tuning: Grid and Random Search](https://www.kaggle.com/willkoehrsen/intro-to-model-tuning-grid-and-random-search)
* [Automated Model Tuning](https://www.kaggle.com/willkoehrsen/automated-model-tuning)

There are several approaches to hyperparameter tuning

1. __Manual__: select hyperparameters based on intuition/experience/guessing, train the model with the hyperparameters, and score on the validation data. Repeat process until you run out of patience or are satisfied with the results. 
2. __Grid Search__: set up a grid of hyperparameter values and for each combination, train a model and score on the validation data. In this approach, every single combination of hyperparameters values is tried which can be very inefficient!
3. __Random search__: set up a grid of hyperparameter values and select _random_ combinations to train the model and score. The number of search iterations is set based on time/resources. 
4. __Automated Hyperparameter Tuning__: use methods such as gradient descent, Bayesian Optimization, or evolutionary algorithms to conduct a guided search for the best hyperparameters.

(This [Wikipedia Article](https://en.wikipedia.org/wiki/Hyperparameter_optimization) provides a good high-level overview of tuning options with links for more details)

In this notebook, we will implement approaches 2 and 3 for a Gradient Boosting Machine Learning Model. In a future notebook, we will implement automated hyperparameter tuning using Bayesian optimization, specifically the Hyperopt library. If you want to get an idea of how automated hyperparameter tuning is done, check out [this article](https://towardsdatascience.com/automated-machine-learning-hyperparameter-tuning-in-python-dfda59b72f8a). 

## Model: Gradient Boosting Machine 

The [Gradient Boosting Machine (GBM)](https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/) has recently emerged as one of the top machine learning models. The GBM is extremely effective on structured data - where the information is in rows and columns - and medium sized datasets - where there are at most a few million observations. We will focus on this model because it is currently the top performing method for most competitions on Kaggle and because the performance is highly dependent on the hyperparameter choices. The basics you need to know about the GBM are that it is an ensemble method that works by training many individual learners, almost always decision trees. However, unlike in a random forest where the trees are trained in __parallel__, in a GBM, the trees are trained __sequentially__ with each tree learning from the mistakes of the previous ones. The hundreds or thousands of weak learners are combined to make a single strong ensemble learner with the contributions of each individual learned during training using Gradient Descent (the weights of the individual trees would therefore be a model _parameter_). 

The GBM [has many hyperparameters to tune](http://lightgbm.readthedocs.io/en/latest/Parameters.html) that control both the overall ensemble (such as the learning rate) and the individual decision trees (such as the number of leaves in the tree or the maximum depth of the tree). It is difficult to know which combination of hyperparameters will work best based only on theory because there are complex interactions between hyperparameters. Hence the need for hyperparameter tuning: the only way to find the optimal hyperparameter values is to try many different combinations on a dataset!

We will use the implementation of the Gradient Boosting Machine in the [LightGBM library](http://lightgbm.readthedocs.io/en/latest/). This is a much faster (and some say more accurate) implementation than that available in Scikit-Learn.

For more details of the Gradient Boosting Machine (GBM), check out this [high-level blog post](http://blog.kaggle.com/2017/01/23/a-kaggle-master-explains-gradient-boosting/), or this [in depth technical article.](https://brage.bibsys.no/xmlui/bitstream/handle/11250/2433761/16128_FULLTEXT.pdf) 

### Getting Started

With the necessary background out of the way, let's get started. For this notebook, we will work with a subset of the data consisting of 10000 rows. Hyperparameter tuning is extremely computationally expensive and working with the full dataset in a Kaggle Kernel would not be feasible for more than a few search iterations. However, the same ideas that we will implement here can be applied to the full dataset and while this notebook is specifically aimed at the GBM, the methods can be applied for any machine learning model. 

To ""test"" the tuning results, we will save some of the training data, 6000 rows, as a separate testing set. When we do hyperparameter tuning, it's crucial to __not tune the hyperparameters on the testing data__. We can only use the testing data __a single time__ when we evaluate the final model that has been tuned on the validation data. To actually test our methods from this notebook, we would need to train the best model on all of the training data, make predictions on the actual testing data, and then submit our answers to the competition. ",0,willkoehrsen,intro-to-model-tuning-grid-and-random-search,willkoehrsen_intro-to-model-tuning-grid-and-random-search_m0
"Below we read in the data and separate into a training set of 10000 observations and a ""testing set"" of 6000 observations. After creating the testing set, we cannot do any hyperparameter tuning with it! ",1,willkoehrsen,intro-to-model-tuning-grid-and-random-search,willkoehrsen_intro-to-model-tuning-grid-and-random-search_m1
"We will also use only the numeric features to reduce the number of dimensions which will help speed up the hyperparameter search. Again, this is something we would not want to do on a real problem, but for demonstration purposes, it will allow us to see the concepts in practice (rather than waiting days/months for the search to finish).",2,willkoehrsen,intro-to-model-tuning-grid-and-random-search,willkoehrsen_intro-to-model-tuning-grid-and-random-search_m2
"# Cross Validation

To evaluate each combination of hyperparameter values, we need to score them on a validation set. The hyperparameters __can not be tuned on the testing data__. We can only use the testing data __once__ when we evaluate the final model. The testing data is meant to serve as an estimate of the model performance when deployed on real data, and therefore we do not want to optimize our model to the testing data because that will not give us a fair estimate of the actual performance. The correct approach is therefore to use a validation set. However, instead of splitting the valuable training data into a separate training and validation set, we use [KFold cross validation](https://www.youtube.com/watch?v=TIgfjmp-4BA). In addition to preserving training data, this should give us a better estimate of generalization performance on the test set than using a single validation set (since then we are probably overfitting to that validation set). The performance of each set of hyperparameters is determined by Receiver Operating Characteristic Area Under the Curve (ROC AUC) from the cross-validation.

In this example, we will use 5-fold cross validation which means training and testing the model with each set of hyperparameter values 5 times to assess performance. Part of the reason why hyperparameter tuning is so time-consuming is because of the use of cross validation. If we have a [large enough training set, we can probably get away with just using a single separate validation set](https://www.coursera.org/lecture/deep-neural-network/train-dev-test-sets-cxG1s), but cross validation is a safer method to avoid overfitting. 

To implement KFold cross validation, we will use the LightGBM cross validation function, `cv`, because this allows us to use a critical technique for training a GBM, early stopping. (For other machine learning models where we do not need to use early stopping, we can use the Scikit-Learn functions `RandomizedSearchCV` or `GridSearchCV`.)

## Early Stopping

One of the most important hyperparameters in a Gradient Boosting Machine is the number of estimators (the number of decision trees trained sequentially). We could set this as another hyperparameter in our search, but there's a better method: [early stopping](https://en.wikipedia.org/wiki/Early_stopping). Early stopping means training until the validation error does not decrease for a specified number of iterations. In the case of the GBM, this means training more decision trees, and in this example, we will use early stopping with 100 rounds, meaning that the training will continue until validation error has not decreased for 100 rounds. Then, the number of estimators that yielded the best score on the validation data will be chosen as the number of estimators to use in the final model.

The concept of early stopping is commonly applied to the GBM and to deep neural networks so it's a great technique to understand. This is one of many forms of regularization that aims to improve generalization performance on the testing set by not overfitting to the training data. If we keep adding estimators, the training error will always decrease because the capacity of the model increases. Although this might seem positive, it means that the model will start to memorize the training data and then will not perform well on new testing data. The __variance__ of the model increases as we continue adding estimators because the model starts to rely too heavily on the training data (high variance means overfitting).

Early stopping is simple to implement with the LightGBM library in the cross validation function. We simply need to pass in the number of early stopping rounds.

### Example of Cross Validation and Early Stopping 

To use the `cv` function, we first need to make a LightGBM `dataset`. ",3,willkoehrsen,intro-to-model-tuning-grid-and-random-search,willkoehrsen_intro-to-model-tuning-grid-and-random-search_m3
"We have to pass in a set of hyperparameters to the cross validation, so we will use the default hyperparameters in LightGBM. In the `cv` call, the `num_boost_round` is set to 10,000 (`num_boost_round` is the same as `n_estimators`), but this number won't actually be reached because we are using early stopping. As a reminder, the metric we are using is Receiver Operating Characteristic Area Under the Curve (ROC AUC).

The code below carries out both cross validation with 5 folds and early stopping with 100 early stopping rounds. ",4,willkoehrsen,intro-to-model-tuning-grid-and-random-search,willkoehrsen_intro-to-model-tuning-grid-and-random-search_m4
"The `cv_results` is a dictionary with lists for the `metric` mean and the `metric` standard deviation. The last entry (index of -1) contains the best performing score. The length of each list in the dictionary will be the ""optimal"" number of estimators to train.",5,willkoehrsen,intro-to-model-tuning-grid-and-random-search,willkoehrsen_intro-to-model-tuning-grid-and-random-search_m5
"We can use this result as a baseline model to beat. To find out how well the model does on our ""test"" data, we will retrain it on all the training data with the best number of estimators found during cross validation with early stopping.",6,willkoehrsen,intro-to-model-tuning-grid-and-random-search,willkoehrsen_intro-to-model-tuning-grid-and-random-search_m6
This is the baseline score _before hyperparameter tuning_. The only difference we made from the default model was using early stopping to set the number of estimators (which by default is 100). ,7,willkoehrsen,intro-to-model-tuning-grid-and-random-search,willkoehrsen_intro-to-model-tuning-grid-and-random-search_m7
"## Hyperparameter Tuning Implementation

Now we have the basic framework in place: we will use cross validation to determine the performance of model hyperparameters and early stopping with the GBM so we do not have to tune the number of estimators. The basic strategy for both grid and random search is simple: for each hyperparameter value combination, evaluate the cross validation score and record the results along with the hyperparameters. Then, at the end of searching, choose the hyperparameters that yielded the highest cross-validation score, train the model on all the training data, and make predictions on the test data.

# Four parts of Hyperparameter tuning

It's helpful to think of hyperparameter tuning as having four parts (these four parts also will form the basis of Bayesian Optimization):

1. Objective function: a function that takes in hyperparameters and returns a score we are trying to minimize or maximize
2. Domain: the set of hyperparameter values over which we want to search. 
3. Algorithm: method for selecting the next set of hyperparameters to evaluate in the objective function.
4. Results history: data structure containing each set of hyperparameters and the resulting score from the objective function.

Switching from grid to random search to Bayesian optimization will only require making minor modifications to these four parts. 

## Objective Function

The objective function takes in hyperparameters and outputs a value representing a score. Traditionally in optimization, this is a score to minimize, but here our score will be the ROC AUC which of course we want to maximize. Later, when we get to Bayesian Optimization, we will have to use a value to minimize, so we can take $1 - \text{ROC AUC}$ as the score. What occurs in the middle of the objective function will vary according to the problem, but for this problem, we will use cross validation with the specified model hyperparameters to get the cross-validation ROC AUC. This score will then be used to select the best model hyperparameter values. 

In addition to returning the value to maximize, our objective function will return the hyperparameters and the iteration of the search. These results will let us go back and inspect what occurred during a search. The code below implements a simple objective function which we can use for both grid and random search.",8,willkoehrsen,intro-to-model-tuning-grid-and-random-search,willkoehrsen_intro-to-model-tuning-grid-and-random-search_m8
"# Domain

The domain, or search space, is all the possible values for all the hyperparameters that we want to search over. For random and grid search, the domain is a hyperparameter grid and usually takes the form of a dictionary with the keys being the hyperparameters and the values lists of values for each hyperparameter.

## Hyperparameters for GBM

To see which settings we can tune, let's make a model and print it out. You can also refer to the [LightGBM documentation](http://lightgbm.readthedocs.io/en/latest/Parameters.html) for the description of all the hyperparameters.",9,willkoehrsen,intro-to-model-tuning-grid-and-random-search,willkoehrsen_intro-to-model-tuning-grid-and-random-search_m9
"Some of these we do not need to tune such as `silent`, `objective`, `random_state`, and `n_jobs`, and we will use early stopping to determine perhaps the most important hyperparameter, the number of individual learners trained, `n_estimators` (also referred to as `num_boost_rounds` or the number of iterations). Some of the hyperparameters do not need to be tuned if others are: for example, `min_child_samples` and `min_child_weight` both limit the complexity of individual decision trees by adjusting the minimum leaf observation requirements and therefore we will only adjust one. However, there are still many hyperparameters to optimize, and we will choose 10 to tune. 

Choosing a hyperparameter grid is probably the most difficult part of hyperparameter tuning: it's nearly impossible ahead of time to say which values of hyperparameters will work well and the optimal settings will depend on the dataset. Moreover, the hyperparameters have complex interactions with each other which means that just tuning one at a time doesn't work because when we start changing other hyperparameters that will affect the one we just tuned! 

If we have prior experience with a model, we might know where the best values for the hyperparameters typically lie, or what a good search space is. However, if we don't have much experience, we can simply define a large search space and hope that the best values are in there somewhere. Typically, when first using a method, I define a wide search space centered around the default values. Then, if I see that some values of hyperparameters tend to work better, I can concentrate the search around those values. 

A complete grid for the 10 hyperparameter is defined below. Each of the values in the dicionary must be a list, so we use `list` combined with `range`, `np.linspace`, and `np.logspace` to define the range of values for each hyperparameter. ",10,willkoehrsen,intro-to-model-tuning-grid-and-random-search,willkoehrsen_intro-to-model-tuning-grid-and-random-search_m10
"One aspect to note is that if `boosting_type` is `goss`, then we cannot use `subsample` (which refers to training on only a fraction of the rows in the training data, a technique known as [stochastic gradient boosting](https://en.wikipedia.org/wiki/Gradient_boosting#Stochastic_gradient_boosting)). Therefore, we will need a line of logic in our algorithm that sets the `subsample` to 1.0 (which means use all the rows) if `boosting_type=goss`. As an example below, if we randomly select a set of hyperparameters, and the boosting type is ""goss"", then we set the `subsample` to 1.0.",11,willkoehrsen,intro-to-model-tuning-grid-and-random-search,willkoehrsen_intro-to-model-tuning-grid-and-random-search_m11
"The `boosting_type` and `is_unbalance` domains are pretty simple because these are categorical variables. For the hyperparameters that must be integers (`num_leaves`, `min_child_samples`), we use `range(start, stop, [step])` which returns a range of numbers from start to stop spaced by step (or 1 if not specified). `range` always returns integers, which means that if we want evenly spaced values that can be fractions, we need to use `np.linspace(start, stop, [num])`.  This works the same way except the third argument is the number of values (by default 100).

Finally, `np.logspace(start, stop, [num = 100], [base = 10.0])` returns values evenly spaced on a logarithmic scale. According to the [the docs](https://docs.scipy.org/doc/numpy/reference/generated/numpy.logspace.html) ""In linear space, the sequence starts at $base^{start}$ (base to the power of start) and ends with $base ^{stop}$ "" This is useful for values that differ over several orders of magnitude such as the learning rate.",12,willkoehrsen,intro-to-model-tuning-grid-and-random-search,willkoehrsen_intro-to-model-tuning-grid-and-random-search_m12
"### Learning Rate Domain

The learning rate domain is from 0.005 to 0.5. Using a logarithmic uniform distribution allows us to create a domain where there are as many values from 0.005 to 0.05 as from 0.05 to 0.5. In a linear space, there would be far more values from 0.05 to 0.5 because this represents a larger distance in linear space but in logarithmic space each of these two intervals is the same width because they are multiples of 10 of each other. (Think about going from 1 to 10 and then from 10 to 100. On a logarithmic scale, these intervals are the same size, but on a linear scale the latter is 10 times the size of the former). In other words, a logarithmic uniform distribution lets us sample more evenly from a domain that varies over several orders of magnitude. 

If that's a little confusing, perhaps the graph above makes it clearer. We can also do a sanity check to make sure the spacing is correct by counting the number of values in each interval.",13,willkoehrsen,intro-to-model-tuning-grid-and-random-search,willkoehrsen_intro-to-model-tuning-grid-and-random-search_m13
"As an example of a simple domain, the `num_leaves` is a uniform distribution. This means values are evenly spaced on a linear scale.",14,willkoehrsen,intro-to-model-tuning-grid-and-random-search,willkoehrsen_intro-to-model-tuning-grid-and-random-search_m14
"# Algorithm for selecting next values

Although we don't generally think of them as such, both grid and random search are algorithms. In the case of grid search, we input the domain and the algorithm selects the next value for each hyperparameter in an ordered sequence. The only requirement of grid search is that it tries every combination in a grid once (and only once). For random search, we input the domain and each time the algorithm gives us a random combination of hyperparameter values to try. There are no requirements for random search other than that the next values are selected at random. 

We will implement these algorithms very shortly, as soon as we cover the final part of hyperparameter tuning.",15,willkoehrsen,intro-to-model-tuning-grid-and-random-search,willkoehrsen_intro-to-model-tuning-grid-and-random-search_m15
"# Results History

The results history is a data structure that contains the hyperparameter combinations and the resulting score on the objective function. When we get to Bayesian Optimization, the model actually _uses the past results to decide on the next hyperparmeters_ to evaluate. Random and grid search are _uninformed_ methods that do not use the past history, but we still need the history so we can find out which hyperparameters worked the best! 

A dataframe is a useful data structure to hold the results.",16,willkoehrsen,intro-to-model-tuning-grid-and-random-search,willkoehrsen_intro-to-model-tuning-grid-and-random-search_m16
"# Grid Search Implementation

Grid search is best described as exhuastive guess and check. We have a problem: find the hyperparameters that result in the best cross validation score, and a set of values to try in the hyperparameter grid - the domain. The grid search method for finding the answer is to try all combinations of values in the domain and hope that the best combination is  in the grid (in reality, we will never know if we found the best settings unless we have an infinite hyperparameter grid which would then require an infinite amount of time to run).

Grid search suffers from one limiting problem: it is extremely computationally expensive because we have to perform cross validation with every single combination of hyperparameters in the grid! Let's see how many total hyperparameter settings there are in our simple little grid we developed.",17,willkoehrsen,intro-to-model-tuning-grid-and-random-search,willkoehrsen_intro-to-model-tuning-grid-and-random-search_m17
"Until Kaggle upgrades the kernels to quantum computers, we are not going to be able to run evan a fraction of the combinations! Let's assume 100 seconds per evaluation and see how many years this would take:",18,willkoehrsen,intro-to-model-tuning-grid-and-random-search,willkoehrsen_intro-to-model-tuning-grid-and-random-search_m18
"I think we're going to need a better approach! Before we discuss alternatives, let's walk through how we would actually use this grid and evaluate all the hyperparameters.

The code below shows the ""algorithm"" for grid search. First, we [unpack the values](https://www.geeksforgeeks.org/packing-and-unpacking-arguments-in-python/) in the hyperparameter grid (which is a Python dictionary) using the line `keys, values = zip(*param_grid.items())`.  The key line is `for v in itertools.product(*values)` where we iterate through all the possible combinations of values in the hyperparameter grid one at a time.  For each combination of values, we create a dictionary `hyperparameters = dict(zip(keys, v))` and then pass these to the objective function defined earlier. The objective function returns the cross validation score from the hyperparameters which we record in the dataframe. This process is repeated for each and every combination of hyperparameter values. By using `itertools.product` (from [this Stack Overflow Question and Answer](https://codereview.stackexchange.com/questions/171173/list-all-possible-permutations-from-a-python-dictionary-of-lists)), we create a [generator](http://book.pythontips.com/en/latest/generators.html) rather than allocating a list of all possible combinations which would be far too large to hold in memory. ",19,willkoehrsen,intro-to-model-tuning-grid-and-random-search,willkoehrsen_intro-to-model-tuning-grid-and-random-search_m19
"Normally, in grid search, we do not limit the number of evaluations. The number of evaluations is set by the total combinations in the hyperparameter grid (or the number of years we are willing to wait!). So the lines 

```
        if i > MAX_EVALS:
            break
```

would not be used in actual grid search. Here we will run grid search for 5 iterations just as an example. The results returned will show us the validation score (ROC AUC), the hyperparameters, and the iteration sorted by best performing combination of hyperparameter values.",20,willkoehrsen,intro-to-model-tuning-grid-and-random-search,willkoehrsen_intro-to-model-tuning-grid-and-random-search_m20
"Now, since we have the best hyperparameters, we can evaluate them on our ""test"" data (remember not the real test data)!",21,willkoehrsen,intro-to-model-tuning-grid-and-random-search,willkoehrsen_intro-to-model-tuning-grid-and-random-search_m21
"It's interesting that the model scores better on the test set than in cross validation. Usually the opposite happens (higher on cross validation than on test) because the model is tuned to the validation data. In this case, the better performance is probably due to small size of the test data and we get very lucky (although this probably does not translate to the actual competition data). ",22,willkoehrsen,intro-to-model-tuning-grid-and-random-search,willkoehrsen_intro-to-model-tuning-grid-and-random-search_m22
"To get a sense of how grid search works, we can look at the progression of hyperparameters that were evaluated.",23,willkoehrsen,intro-to-model-tuning-grid-and-random-search,willkoehrsen_intro-to-model-tuning-grid-and-random-search_m23
"Look at the `subsample` and the `is_unbalance` because these are the only hyperparameters that change. In fact, the effect of  changing these values is so small that validation scores literally did not change across runs (indicating this small of a change has no effect on the model). This is grid search trying every single value in the grid! No matter how small the increment between subsequent values of a hyperparameter, it will try them all. Clearly, we are going to need a more efficient approach if we want to find better hyperparameters in a reasonable amount of time. ",24,willkoehrsen,intro-to-model-tuning-grid-and-random-search,willkoehrsen_intro-to-model-tuning-grid-and-random-search_m24
"#### Application

If you want to run this on the entire dataset feel free to take these functions and put them in a script. However, I would advise against using grid search unless you have a very small hyperparameter grid because this is such as exhaustive method! 
Later, we will look at results from 1000 iterations of grid and random search run on the same small subset of data as we used above. I have not tried to run any form of grid search on the full data (and probably will not try this method).",25,willkoehrsen,intro-to-model-tuning-grid-and-random-search,willkoehrsen_intro-to-model-tuning-grid-and-random-search_m25
"# Random Search

Random search is surprisingly efficient compared to grid search. Although grid search will find the optimal value of hyperparameters (assuming they are in your grid) eventually, random search will usually find a ""close-enough"" value in far fewer iterations. [This great paper explains why this is so](http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf): grid search spends too much time evaluating unpromising regions of the hyperparameter search space because it has to evaluate every single combination in the grid. Random search in contrast, does a better job of exploring the search space and therefore can usually find a good combination of hyperparameters in far fewer iterations. 

As [this article](https://medium.com/rants-on-machine-learning/smarter-parameter-sweeps-or-why-grid-search-is-plain-stupid-c17d97a0e881) lays out, random search should probably be the first hyperparameter optimization method tried because of its effectiveness. Even though it's an _uninformed_ method (meaning it does not rely on past evaluation results), random search can still usually find better values than the default and is simple to run.

Random search can also be thought of as an algorithm: randomly select the next set of hyperparameters from the grid! We can build a dictionary of hyperparameters by selecting one random value for each hyperparameter as follows (again accounting for subsampling):",26,willkoehrsen,intro-to-model-tuning-grid-and-random-search,willkoehrsen_intro-to-model-tuning-grid-and-random-search_m26
"Next, we define the `random_search` function. This takes the same general structure as `grid_search` except for the method used to select the next hyperparameter values. Moreover, random search is always run with a limit on the number of search iterations.",27,willkoehrsen,intro-to-model-tuning-grid-and-random-search,willkoehrsen_intro-to-model-tuning-grid-and-random-search_m27
"We can also evaluate the best random search model on the ""test"" data.",28,willkoehrsen,intro-to-model-tuning-grid-and-random-search,willkoehrsen_intro-to-model-tuning-grid-and-random-search_m28
"Finally, we can view the random search sequence of hyperparameters.",29,willkoehrsen,intro-to-model-tuning-grid-and-random-search,willkoehrsen_intro-to-model-tuning-grid-and-random-search_m29
"This time we see hyperparameter values that are all over the place, almost as if they had been selected at random! Random search will do a much better job than grid search of exploring the search domain (for the same number of iterations). If we have a limited time to evaluate hyperparameters, random search is a better option than grid search for exactly this reason.

### Stacking Random and Grid Search

One option for a smarter implementation of hyperparameter tuning is to combine random search and grid search: 

1. Use random search with  a large hyperparameter grid 
2. Use the results of random search to build a focused hyperparameter grid around the best performing hyperparameter values.
3. Run grid search on the reduced hyperparameter grid. 
4. Repeat grid search on more focused grids until maximum computational/time budget is exceeded.

In a later notebook (upcoming), we will look at methods that use the past evaluation results to pick the next hyperparameter values to try in the objective function. These methods (including [Bayesian optimization](https://sigopt.com/static/pdf/SigOpt_Bayesian_Optimization_Primer.pdf)) are essentially doing what we would do in the strategy outlined above: adjust the next values tried in the search from the previous results. The overall objective of these _informed methods_ is to limit evaluations of the objective function by reasoning about the next values to try based on past evaluation results. These algorithms are therefore able to save time by evaluating more promising values of hyperparameters. This is a really cool topic and [Bayesian optimization](http://gpss.cc/gpmc17/slides/LancasterMasterclass_1.pdf) is fascinating so stay tuned for this upcoming notebook. ",30,willkoehrsen,intro-to-model-tuning-grid-and-random-search,willkoehrsen_intro-to-model-tuning-grid-and-random-search_m30
"## Next Steps

We can now take these random and grid search functions and use them on the complete dataset or any dataset of our choosing. These search methods are very expensive, so expect the hyperparameter tuning to take a while.(I am currently running this script on a full set of features for 500 iterations and will make the results public when they are available. )

For now, we will turn to implementing random and grid search on the reduced dataset for 1000 iterations just to compare the results (I took the code below and already ran it because even with the small dataset, it takes a very long time. The results are available as part of the data in this kernel). 

## Writing to File to Monitor Progress

When we run these searches for a long time, it's natural to want to track the performance while the search is going on. We can print information to the command prompt, but this will grow cluttered after 1000 iterations and the results will be gone if we close the command prompt. A better solution (although not perfect) is to write a line to a csv (comma separated value) file on each iteration. Then, we can look at the file to track progress while the searching is running, and eventually, have the entire results saved when the search is complete.

### Extremely Important Note about Checking Files

When you want to check the csv file, __do not open it in Excel while the search is ongoing__. This will cause a permission error in Python and the search will be terminated. Instead, you can view the end of the file by typing `tail out_file.csv` from Bash where `out_file.csv` is the name of the file being written to. There are also some text editors, such as notepad or Sublime Text, where you can open the results safely while the search is occurring. However, __do not use Excel to open a file that is being written to in Python__. This is a mistake I've made several times so you do not have to! ",31,willkoehrsen,intro-to-model-tuning-grid-and-random-search,willkoehrsen_intro-to-model-tuning-grid-and-random-search_m31
"Below is the code we need to run before the search. This creates the csv file, opens a connection, writes the header (column names), and then closes the connection. This will overwrite any information currently in the `out_file`, so change to a new file name every time you want to start a new search.",32,willkoehrsen,intro-to-model-tuning-grid-and-random-search,willkoehrsen_intro-to-model-tuning-grid-and-random-search_m32
"Now we must slightly modify `random_search` and `grid_search` to write to this file every time. We do this by opening a connection, this time using the `""a""` option for append (the first time we used the `""w""` option for write) and writing a line with the desired information (which in this case is the cross validation score, the hyperparameters, and the number of the iteration). Then we close the connection until the function is called again.",33,willkoehrsen,intro-to-model-tuning-grid-and-random-search,willkoehrsen_intro-to-model-tuning-grid-and-random-search_m33
"To run these functions for 1000 iterations (or however many you choose) uncomment the cell below. Otherwise, I have run these functions on the reduced dataset and attached the results to this kernel.",34,willkoehrsen,intro-to-model-tuning-grid-and-random-search,willkoehrsen_intro-to-model-tuning-grid-and-random-search_m34
"# Results on Limited Data

We can examine 1000 search iterations of the above functions on the reduced dataset. Later, we can try the hyperparameters that worked the best for the small versions of the data on a complete dataset to see if the best hyperparameters translate when increasing the size of the data 30 times! The 1000 search iterations were not run in a kernel, although they might be able to finish (no guarantees) in the 12 hour time limit. 

First we can find out which method returned the best results. ",35,willkoehrsen,intro-to-model-tuning-grid-and-random-search,willkoehrsen_intro-to-model-tuning-grid-and-random-search_m35
"When we save the results to a csv, for some reason the dictionaries are saved as strings. Therefore we need to convert them back to dictionaries after reading in the results using the `ast.literal_eval` function.",36,willkoehrsen,intro-to-model-tuning-grid-and-random-search,willkoehrsen_intro-to-model-tuning-grid-and-random-search_m36
Now let's make a function to parse the results from the hyperparameter searches. This returns a dataframe where each column is a hyperparameter and each row has one search result (so taking the dictionary of hyperparameters and mapping it into a row in a dataframe).,37,willkoehrsen,intro-to-model-tuning-grid-and-random-search,willkoehrsen_intro-to-model-tuning-grid-and-random-search_m37
"# Visualizations

Visualizations are both enjoyable to make, and can give us an intuitive look into a technique. Here we will make a few simple plots using matplotlib, seaborn, and Altair! __Unfortunately, the Altair visualizations do not show up when the notebook is rendered. To view the Altair figures, you'll have to run the notebook yourself!__",38,willkoehrsen,intro-to-model-tuning-grid-and-random-search,willkoehrsen_intro-to-model-tuning-grid-and-random-search_m38
"First we can plot the validation scores versus the iteration. Here we will use the [Altair](https://altair-viz.github.io/) visualization library to make some plots! First, we need to put our data into a long format dataframe.",39,willkoehrsen,intro-to-model-tuning-grid-and-random-search,willkoehrsen_intro-to-model-tuning-grid-and-random-search_m39
"Below, we make the same plot using seaborn because the Altair visualizations do not show up in the rendered notebook. ",40,willkoehrsen,intro-to-model-tuning-grid-and-random-search,willkoehrsen_intro-to-model-tuning-grid-and-random-search_m40
"The grid cross validation score increases over time. This indicates that whatever hyperparameters are changing in grid search are gradually increasing the score. The random cross validation scores on the other hand are all over the place as expected. This grid search appears to be stuck in a relatively low-performing region of the search space, and because it is constrained to try all the values in the grid, it is not able to try significantly different hyperparameter values that would perform better (as occurs in random search). The random search method does a very good job of exploring the search space as we will see when we look at the hyperparameter values searched. ",41,willkoehrsen,intro-to-model-tuning-grid-and-random-search,willkoehrsen_intro-to-model-tuning-grid-and-random-search_m41
"## Distribution of Search Values

We can show the distribution of search values for random search (grid search is very uninteresting). Even though we expect these to be _random_, it's always a good idea to check our code both quantitatively and visually. ",42,willkoehrsen,intro-to-model-tuning-grid-and-random-search,willkoehrsen_intro-to-model-tuning-grid-and-random-search_m42
"The boosting type should be evenly distributed for random search. 

Again, we have to remake this chart in seaborn to have the visualization appear in the rendered notebook (if anyone knows how to address this issue, please tell me in the comments!)",43,willkoehrsen,intro-to-model-tuning-grid-and-random-search,willkoehrsen_intro-to-model-tuning-grid-and-random-search_m43
"Next, for the numeric hyperparameters, we will plot both the sampling distribution (the hyperparameter grid) and the results from random search in a kernel density estimate (KDE) plot. (The grid search results are completely uninteresting). As random search is just drawing random values, we would expect the random search distribution to align with the sampling grid (although it won't be perfectly aligned because of the limited number of searches). 

As an example, below we plot the distribution of learning rates from both the sampling distribution and the random search results. The vertical dashed line indicates the optimal value found from random search.",44,willkoehrsen,intro-to-model-tuning-grid-and-random-search,willkoehrsen_intro-to-model-tuning-grid-and-random-search_m44
The following code repeats this plot for all the of the numeric hyperparameters. ,45,willkoehrsen,intro-to-model-tuning-grid-and-random-search,willkoehrsen_intro-to-model-tuning-grid-and-random-search_m45
"## Sequence of Search Values

Finally, we can plot the sequence of search values against the iteration for random search. Clearly there will not be any order, but this can let us visualize what happens in a random search!

The star indicates the best value of the hyperparameter that was found.",46,willkoehrsen,intro-to-model-tuning-grid-and-random-search,willkoehrsen_intro-to-model-tuning-grid-and-random-search_m46
## Score versus Hyperparameters,47,willkoehrsen,intro-to-model-tuning-grid-and-random-search,willkoehrsen_intro-to-model-tuning-grid-and-random-search_m47
"As a final plot, we can show the score versus the value of each hyperparameter. We need to keep in mind that the hyperparameters are not changed one at a time, so if there are relationships between the values and the score, they do not mean that particular hyperparameter is influencing the score. However, we might be able to identify values of hyperparameters that seem more promising. Mostly these plots are for my own interest, to see if there are any trends! ",48,willkoehrsen,intro-to-model-tuning-grid-and-random-search,willkoehrsen_intro-to-model-tuning-grid-and-random-search_m48
"We want to avoid placing too much emphasis on any of these relationships because we were not changing one hyperparameter at a time (although we could carry out experiments where we only change one hyperparameter and observes the effects on the score) and so the trends are not due solely to the single hyperparameter we show. If we could plot this in higher dimensions, it might be interesting to see if there are more promising regions of the search space but here we are limited to one dimension (a single hyperparameter versus the score).  If we want to observe the effects of one hyperparameter on the cross validation score, we could alter only that hyperparameter while holding all the others constant. However, the hyperparameters do not act by themselves and there are complex interactions between the model settings.",49,willkoehrsen,intro-to-model-tuning-grid-and-random-search,willkoehrsen_intro-to-model-tuning-grid-and-random-search_m49
"# Testing Results on Full Data

We can take the best hyperparameters found from the 1000 iterations of random search on the reduced training data and try these on an entire training dataset. Here, we will use the features from the `[Updated 0.792 LB] LightGBM with Simple Features
`(https://www.kaggle.com/jsaguiar/updated-0-792-lb-lightgbm-with-simple-features) kernel (I did not develop these features and want to give credit to the numerous people, including [Aguiar](https://www.kaggle.com/jsaguiar) and [olivier](https://www.kaggle.com/ogrellier),  who have worked on these features. Please check out their [kernels](https://www.kaggle.com/ogrellier/lighgbm-with-selected-features)!). 

The code below uses the best random search hyperparameters to build a model, train on the full features from `[Updated 0.792 LB] LightGBM with Simple Features
`(https://www.kaggle.com/jsaguiar/updated-0-792-lb-lightgbm-with-simple-features), and test on the testing features. The test data is the actual competition data, so we can then submit these and see how well the score translates to a full dataset! ",50,willkoehrsen,intro-to-model-tuning-grid-and-random-search,willkoehrsen_intro-to-model-tuning-grid-and-random-search_m50
First we will test the cross validation score using the best model hyperparameter values from random search. This can give us an idea of the generalization error on the test set. We will delete the number of estimators found from the smaller dataset and use early stopping to find the best number of decision trees to train. ,51,willkoehrsen,intro-to-model-tuning-grid-and-random-search,willkoehrsen_intro-to-model-tuning-grid-and-random-search_m51
"The public leaderboard score is only calculated on 10% of the test data, so the cross validation score might actually give us a better idea of how the model will perform on the full test set. Usually we expect the cross validation score to be higher than on the testing data, but because of the small size of the testing data, this might be reversed for this problem.

Next, we will make predictions on the test data that can be submitted to the competition. ",52,willkoehrsen,intro-to-model-tuning-grid-and-random-search,willkoehrsen_intro-to-model-tuning-grid-and-random-search_m52
"The score when submitting to the test competition is __0.782__. The original score from the kernel where I got these features was 0.792, so we can conclude that the results from random search on the smaller dataset to not translate to a full dataset. I currently am running random search with 500 iterations on the full dataset, and will make those results publicly available when the search is complete! ",53,willkoehrsen,intro-to-model-tuning-grid-and-random-search,willkoehrsen_intro-to-model-tuning-grid-and-random-search_m53
"## Model Tuning Next Steps

From here, we might want to take the functions we wrote and apply them to a complete dataset. The results are likely to be different because we were only using a random subset of the training data. However, this will take much longer (300000+ observations instead of 10000). I'm currently running the random search on the full dataset from the Kernel referenced above, and will see how the results turn out. (Sampling some of the observations is not inherently negative, and it can help us get reasonable answers in a much shorter time frame. However, if we are using such a small portion of the data that is not representative of the entire dataset, then we should not expect the tuning to translate to the full dataset.)

In an upcoming notebook, we will turn to automated hyperparameter tuning, in particular, Bayesian Optimization. We will implement automated optimization of machine learning hyperparameters step-by-step using the Hyperopt open-source Python library. I'll provide the link here as soon as this notebook is finished, but if you want to get an idea of Bayesian optimization, you can check out [this introductory article](https://towardsdatascience.com/an-introductory-example-of-bayesian-optimization-in-python-with-hyperopt-aae40fff4ff0), or [this article on automated hyperparameter tuning](https://towardsdatascience.com/automated-machine-learning-hyperparameter-tuning-in-python-dfda59b72f8a). These topics are pretty neat and it's incredible that they are available in an easy-to-use format for anyone to take advantage of. I'll see you in the next notebook! ",54,willkoehrsen,intro-to-model-tuning-grid-and-random-search,willkoehrsen_intro-to-model-tuning-grid-and-random-search_m54
"# Conclusions

Model tuning is the process of finding the best machine learning model hyperparameters for a particular problem. Random and grid search are two uniformed methods for hyperparameter tuning that search by selecting hyperparameter values from a grid domain. 
The four parts of hyperparameter tuning are:

1. Objective function: takes in hyperparameters and returns the cross validation score we want to maximize or minimize
2. Domain of hyperparameters: values over which we want to search
3. Algorithm: method for selecting the next hyperparameter values to evaluate in the objective function
4. Results: history of hyperparameters and cross validation scores

These four parts apply to grid and random search as well as to Bayesian optimization, a form of automated hyperparameter tuning. In this notebook, we implemented both random and grid search on a reduced dataset, inspected the results, and tried to translate the optimal hyperparameters to a full dataset (from [this kernel](https://www.kaggle.com/jsaguiar/updated-0-792-lb-lightgbm-with-simple-features)). As a small note, it's important to remember that we tune the hyperparameters to the training data - using cross validation - so the hyperparameter values we find are only optimal for the training data. Although the best hyperparameters from the smaller dataset did not work that well on the full dataset, we were still able to see the ideas behind these two tuning methods. Moreover, we can take the functions developed here and apply them to any dataset or to any machine learning model, not just the gradient boosting machine. 

Random search turns out to work pretty well in practice (because it is good at exploring the search domain), but it still is not a reasoning method because it does not use past evaluation results to choose the next hyperparameter values. A better approach would be to use the past results to reason about the best values to try next in the objective function, especially because as we saw, evaluating the objective function is time-consuming! In future work, we will look at [implementing automated hyperparameter tuning](https://towardsdatascience.com/automated-machine-learning-hyperparameter-tuning-in-python-dfda59b72f8a) using Bayesian optimization. 

Hyperparameter tuning is a crucial part of the machine learning pipeline because the performance of a model can depend strongly on the choices of the hyperparameter values. Random and grid search are two decent methods to start tuning a model (at least they are better than manual tuning) and are important tools to have in the data science skillset. Thanks for reading and I'll see you in the next notebook!

As always, I welcome feedback and constructive criticism. I write for Towards Data Science at https://medium.com/@williamkoehrsen/ and can be reached on Twitter at https://twitter.com/koehrsen_will

Will",55,willkoehrsen,intro-to-model-tuning-grid-and-random-search,willkoehrsen_intro-to-model-tuning-grid-and-random-search_m55
"# Introduction: Feature Selection

In this notebook we will apply feature engineering to the manual engineered features built in two previous kernels. We will reduce the number of features using several methods and then we will test the performance of the features using a fairly basic gradient boosting machine model. 

The main takeaways from this notebook are:

* Going from 1465 total features to 536 and an AUC ROC of 0.783 on the public leaderboard
* A further optional step to go to 342 features and an AUC ROC of 0.782

The full set of features was built in [Part One](https://www.kaggle.com/willkoehrsen/introduction-to-manual-feature-engineering) and [Part Two](https://www.kaggle.com/willkoehrsen/introduction-to-manual-feature-engineering-p2) of Manual Feature Engineering

We will use three methods for feature selection:

1. Remove collinear features
2. Remove features with greater than a threshold percentage of missing values
3. Keep only the most relevant features using feature importances from a model

We will also take a look at an example of applying PCA although we will not use this method for feature reduction. ",0,willkoehrsen,introduction-to-feature-selection,willkoehrsen_introduction-to-feature-selection_m0
Standard imports for data science work. The LightGBM library is used for the gradient boosting machine.,1,willkoehrsen,introduction-to-feature-selection,willkoehrsen_introduction-to-feature-selection_m1
"* `train_bureau` is the training features built manually using the `bureau` and `bureau_balance` data
* `train_previous` is the training features built manually using the `previous`, `cash`, `credit`, and `installments` data

We first will see how many features we built over the manual engineering process. Here we use a couple of set operations to find the columns that are only in the `bureau`, only in the `previous`, and in both dataframes, indicating that there are `original` features from the `application` dataframe. Here we are working with a small subset of the data in order to not overwhelm the kernel. This code has also been run on the full dataset (we will take a look at some of the results).",2,willkoehrsen,introduction-to-feature-selection,willkoehrsen_introduction-to-feature-selection_m2
That gives us the number of features in each dataframe. Now we want to combine the data without creating any duplicate rows. ,3,willkoehrsen,introduction-to-feature-selection,willkoehrsen_introduction-to-feature-selection_m3
"Next we want to one-hot encode the dataframes. This doesn't give the full features since we are only working with a sample of the data and this will not create as many columns as one-hot encoding the entire dataset would. Doing this to the full dataset results in 1465 features.

An important note in the code cell is where we __align the dataframes by the columns.__ This ensures we have the same columns in the training and testing datasets.",4,willkoehrsen,introduction-to-feature-selection,willkoehrsen_introduction-to-feature-selection_m4
"When we do this to the full dataset, we get __1465__ features. ",5,willkoehrsen,introduction-to-feature-selection,willkoehrsen_introduction-to-feature-selection_m5
"### Admit and Correct Mistakes!

When doing manual feature engineering, I accidentally created some columns derived from the client id, `SK_ID_CURR`. As this is a unique identifier for each client, it should not have any predictive power, and we would not want to build a model trained on this ""feature"". Let's remove any columns built on the `SK_ID_CURR`.",6,willkoehrsen,introduction-to-feature-selection,willkoehrsen_introduction-to-feature-selection_m6
"After applying this to the full dataset, we end up with __1416 __ features. More features might seem like a good thing, and they can be if they help our model learn. However, irrelevant features, highly correlated features, and missing values can prevent the model from learning and decrease generalization performance on the testing data. Therefore, we perform feature selection to keep only the most useful variables.

We will start feature selection by focusing on collinear variables.",7,willkoehrsen,introduction-to-feature-selection,willkoehrsen_introduction-to-feature-selection_m7
"# Remove Collinear Variables

Collinear variables are those which are highly correlated with one another. These can decrease the model's availablility to learn, decrease model interpretability, and decrease generalization performance on the test set. Clearly, these are three things we want to increase, so removing collinear variables is a useful step. We will establish an admittedly arbitrary threshold for removing collinear variables, and then remove one out of any pair of variables that is above that threshold. 

The code below identifies the highly correlated variables based on the absolute magnitude of the Pearson correlation coefficient being greater than 0.9. Again, this is not entirely accurate since we are dealing with such a limited section of the data. This code is for illustration purposes, but if we read in the entire dataset, it would work (if the kernels allowed it)! 

This code is adapted from [work by Chris Albon](https://chrisalbon.com/machine_learning/feature_selection/drop_highly_correlated_features/).",8,willkoehrsen,introduction-to-feature-selection,willkoehrsen_introduction-to-feature-selection_m8
### Identify Correlated Variables,9,willkoehrsen,introduction-to-feature-selection,willkoehrsen_introduction-to-feature-selection_m9
#### Drop Correlated Variables,10,willkoehrsen,introduction-to-feature-selection,willkoehrsen_introduction-to-feature-selection_m10
"Applying this on the entire dataset __results in 538  collinear features__ removed.  

This has reduced the number of features singificantly, but it is likely still too many. At this point, we'll read in the full dataset after removing correlated variables for further feature selection.

The full datasets (after removing correlated variables) are available in `m_train_combined.csv` and `m_test_combined.csv`.",11,willkoehrsen,introduction-to-feature-selection,willkoehrsen_introduction-to-feature-selection_m11
"### Read in Full Dataset

Now we are ready to move on to the full set of features. These were built by applying the above steps to the entire `train_bureau` and `train_previous` files (you can do the same if you want and have the computational resources)!",12,willkoehrsen,introduction-to-feature-selection,willkoehrsen_introduction-to-feature-selection_m12
"# Remove Missing Values

A relatively simple choice of feature selection is removing missing values. Well, it seems simple, at least until we have to decide what percentage of missing values is the minimum threshold for removing a column. Like many choices in machine learning, there is no right answer, and not even a general rule of thumb for making this choice. In this implementation, if any columns have greater than 75% missing values, they will be removed. 

Most models (including those in Sk-Learn) cannot handle missing values, so we will have to fill these in before machine learning. The Gradient Boosting Machine ([at least in LightGBM](https://github.com/Microsoft/LightGBM/blob/master/docs/Advanced-Topics.rst)) can handle missing values. Imputing missing values always makes me a little uncomfortable because we are adding information that actually isn't in the dataset. Since we are going to be evaluating several models (in a later notebook), we will have to use some form of imputation. For now, we will focus on removing columns above the threshold.",13,willkoehrsen,introduction-to-feature-selection,willkoehrsen_introduction-to-feature-selection_m13
"Let's drop the columns, one-hot encode the dataframes, and then align the columns of the dataframes.",14,willkoehrsen,introduction-to-feature-selection,willkoehrsen_introduction-to-feature-selection_m14
"# Feature Selection through Feature Importances

The next method we can employ for feature selection is to use the feature importances of a model. Tree-based models (and consequently ensembles of trees) can determine an ""importance"" for each feature by measuring the reduction in impurity for including the feature in the model. I'm not really sure what that means (any explanations would be welcome) and the absolute value of the importance can be difficult to interpret. However, the relative value of the importances can be used as an approximation of the ""relevance"" of different features in a model. Moreover, we can use the feature importances to remove features that the model does not consider important. 

One method for doing this automatically is the [Recursive Feature Elimination method](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html) in Scikit-Learn. This accepts an estimator (one that either returns feature weights such as a linear regression, or feature importances such as a random forest) and a desired number of features. In then fits the model repeatedly on the data and iteratively removes the lowest importance features until the desired number of features is left. This means we have another arbitrary hyperparameter to use in out pipeline: the number of features to keep! 

Instead of doing this automatically, we can perform our own feature removal by first removing all zero importance features from the model. If this leaves too many features, then we can consider removing the features with the lowest importance. We will use a Gradient Boosted Model from the LightGBM library to assess feature importances. If you're used to the Scikit-Learn library, the LightGBM library has an API that makes deploying the model very similar to using a Scikit-Learn model. ",15,willkoehrsen,introduction-to-feature-selection,willkoehrsen_introduction-to-feature-selection_m15
"Since the LightGBM model does not need missing values to be imputed, we can directly `fit` on the training data. We will use Early Stopping to determine the optimal number of iterations and run the model twice, averaging the feature importances to try and avoid overfitting to a certain set of features.",16,willkoehrsen,introduction-to-feature-selection,willkoehrsen_introduction-to-feature-selection_m16
"We see that one of our features made it into the top 5 most important! That's a good sign for all of our hard work making the features. It also looks like many of the features we made have literally 0 importance. For the gradient boosting machine, features with 0 importance are not used at all to make any splits. Therefore, we can remove these features from the model with no effect on performance (except for faster training). ",17,willkoehrsen,introduction-to-feature-selection,willkoehrsen_introduction-to-feature-selection_m17
Let's remove the features that have zero importance.,18,willkoehrsen,introduction-to-feature-selection,willkoehrsen_introduction-to-feature-selection_m18
"At this point, we can re-run the model to see if it identifies any more features with zero importance. In a way, we are implementing our own form of recursive feature elimination. Since we are repeating work, we should probably put the zero feature importance identification code in a function.",19,willkoehrsen,introduction-to-feature-selection,willkoehrsen_introduction-to-feature-selection_m19
"There are now no 0 importance features left (I guess we should have expected this). If we want to remove more features, we will have to start with features that have a non-zero importance. One way we could do this is by retaining enough features to account for a threshold percentage of importance, such as 95%. At this point, let's keep enough features to account for 95% of the importance. Again, this is an arbitrary decision! ",20,willkoehrsen,introduction-to-feature-selection,willkoehrsen_introduction-to-feature-selection_m20
"We can keep only the features needed for 95% importance. This step seems to me to have the greatest chance of harming the model's learning ability, so rather than changing the original dataset, we will make smaller copies. Then, we can test both versions of the data to see if the extra feature removal step is worthwhile. ",21,willkoehrsen,introduction-to-feature-selection,willkoehrsen_introduction-to-feature-selection_m21
"# Test New Featuresets

The last step of feature removal we did seems like it may have the potential to hurt the model the most. Therefore we want to test the effect of this removal. To do that, we can use a standard model and change the features. 

We will use a fairly standard LightGBM model, similar to the one we used for feature selection. The main difference is this model uses five-fold cross validation for training and we  use it to make predictions. There's a lot of code here, but that's because I included documentation and a few extras (such as feature importances) that aren't strictly necessary. For now, understanding the entire model isn't critical, just know that we are using the same model with two different datasets to see which one performs the best.",22,willkoehrsen,introduction-to-feature-selection,willkoehrsen_introduction-to-feature-selection_m22
"### Test ""Full"" Dataset

This is the expanded dataset. To recap the process to make this dataset we:

* Removed collinear features as measured by the correlation coefficient greater than 0.9
* Removed any columns with greater than 80% missing values in the train or test set
* Removed all features with non-zero feature importances",23,willkoehrsen,introduction-to-feature-selection,willkoehrsen_introduction-to-feature-selection_m23
The full features after feature selection score __0.783__ when submitted to the public leaderboard. ,24,willkoehrsen,introduction-to-feature-selection,willkoehrsen_introduction-to-feature-selection_m24
"### Test ""Small"" Dataset

The small dataset requires one additional step over the ful l dataset:

* Keep only features needed to reach 95% cumulative importance in the gradient boosting machine",25,willkoehrsen,introduction-to-feature-selection,willkoehrsen_introduction-to-feature-selection_m25
The smaller featureset scores __0.782__ when submitted to the public leaderboard.,26,willkoehrsen,introduction-to-feature-selection,willkoehrsen_introduction-to-feature-selection_m26
"# Other Options for Dimensionality Reduction

We only covered a small portion of the techniques used for feature selection/dimensionality reduction. There are many other methods such as:

* PCA: Principle Components Analysis (PCA)
* ICA: Independent Components Analysis (ICA)
* Manifold learning: [also called non-linear dimensionality reduction](https://stats.stackexchange.com/questions/247907/what-is-the-difference-between-manifold-learning-and-non-linear-dimensionality-r)

PCA is a great method for reducing the number of features provided that you do not care about model interpretability. It projects the original set of features onto a lower dimension, in the process, eliminating any physical representation behind the features. Here's a pretty thorough introduction to the math for anyone interested. PCA also assumes that the data is Gaussian distributed, which may not be the case, especially when dealing with real-world human generated data. 

ICA representations also obscure any physical meaning behind the variables and presevere the most ""independent"" dimensions of the data (which is different than the dimensions with the most variance). 

Manifold learning is more often used for low-dimensional visualizations (such as with T-SNE or LLE) rather than for dimensionality reduction for a classifier. These methods are heavily dependent on several hyperparameters and are not deterministic which means that there is no way to apply it to new data (in other words you cannot `fit` it to the training data and then separately `transform` the testing data). The learned representation of a dataset will change every time you apply manifold learning so it is not generally a stable method for feature selection.",27,willkoehrsen,introduction-to-feature-selection,willkoehrsen_introduction-to-feature-selection_m27
"## PCA Example

We can go through a quick example to show how PCA is implemented. Without going through too many details, PCA finds a new set of axis (the principal components) that maximize the amount of variance captured in the data. The original data is then projected down onto these principal components. The idea is that we can use fewer principal components than the original number of features while still capturing most of the variance. PCA is implemented in Scikit-Learn in the same way as preprocessing methods. We can either select the number of new components, or the fraction of variance we want explained in the data. If we pass in no argument, the number of principal components will be the same as the number of original features. We can then use the `variance_explained_ratio_` to determine the number of components needed for different threshold of variance retained.",28,willkoehrsen,introduction-to-feature-selection,willkoehrsen_introduction-to-feature-selection_m28
We only need a few prinicipal components to account for the majority of variance in the data. We can use the first two principal components to visualize the entire dataset. We will color the datapoints by the value of the target to see if using two principal components clearly separates the classes.,29,willkoehrsen,introduction-to-feature-selection,willkoehrsen_introduction-to-feature-selection_m29
"Even though we have accounted for most of the variance, that does not mean the pca decomposition makes the problem of identifying loans repaid vs not repaid any easier. PCA does not consider the value of the label when projecting the features to a lower dimension. Feel free to try a classifier on top of this data, but when I have done so, I noticed that it was not very accurate. ",30,willkoehrsen,introduction-to-feature-selection,willkoehrsen_introduction-to-feature-selection_m30
"# Conclusions

In this notebook we employed a number of feature selection methods. These methods are necessary to reduce the number of features to increase model interpretability, decrease model runtime, and increase generalization performance on the test set. The methods of feature selection we used are:

1. Remove highly collinear variables as measured by a correlation coefficient greater than 0.9
2. Remove any columns with more than 75% missing values.
3. Remove any features with a zero importance as determined by a gradient boosting machine.
4. (Optional) keep only enough features to account for 95% of the importance in the gradient boosting machine.

Using the first three methods, we reduced the number of features from __1465__ to __536__ with a 5-fold cv AUC ROC score of 0.7838 and a public leaderboard score of 0.783.

After applying the fourth method, we end up with 342 features with a 5-fold cv AUC SCORE of 0.7482 and a public leaderboard score of 0.782. 

Going forward, we might actually want to add _more_ features except this time, instead of naively applying aggregations, think about what features are actually important from a domain point of view. There are a number of kernels that have created useful features that we can add to our set here to improve performance. The process of feature engineering - feature selection is iterative, and it may require several more passes before we get it completely right! ",31,willkoehrsen,introduction-to-feature-selection,willkoehrsen_introduction-to-feature-selection_m31
"# Introduction: Manual Feature Engineering (part two)

In this notebook we will expand on the [Introduction to Manual Feature Engineering](https://www.kaggle.com/willkoehrsen/introduction-to-manual-feature-engineering/output) notebook. We will use the aggregation and value counting functions developed in that notebook in order to incorporate information from the `previous_application`, `POS_CASH_balance`, `installments_payments`, and `credit_card_balance` data files. We already used the information from the `bureau` and `bureau_balance` in the previous notebook and were able to improve our competition score compared to using only the `application` data. After running a model with the features included here, performance does increase, but we run into issues with an explosion in the number of features! I'm working on a notebook of feature selection, but for this notebook we will continue building up a rich set of data for our model. 

The definitions of the four additional data files are:

* previous_application (called `previous`): previous applications for loans at Home Credit of clients who have loans in the application data. Each current loan in the application data can have multiple previous loans. Each previous application has one row and is identified by the feature SK_ID_PREV.
* POS_CASH_BALANCE (called `cash`): monthly data about previous point of sale or cash loans clients have had with Home Credit. Each row is one month of a previous point of sale or cash loan, and a single previous loan can have many rows.
* credit_card_balance (called `credit`): monthly data about previous credit cards clients have had with Home Credit. Each row is one month of a credit card balance, and a single credit card can have many rows.
* installments_payment (called `installments`): payment history for previous loans at Home Credit. There is one row for every made payment and one row for every missed payment.",0,willkoehrsen,introduction-to-manual-feature-engineering-p2,willkoehrsen_introduction-to-manual-feature-engineering-p2_m0
"# Functions 

We spent quite a bit of time developing two functions in the previous notebook:

* `agg_numeric`: calculate aggregation statistics (`mean`, `count`, `max`, `min`) for numeric variables.
* `agg_categorical`: compute counts and normalized counts of each category in a categorical variable.

Together, these two functions can extract information about both the numeric and categorical data in a dataframe. Our general approach will be to apply both of these functions to the dataframes, grouping by the client id, `SK_ID_CURR`. For the `POS_CASH_balance`, `credit_card_balance`, and `installment_payments`, we can first group by the `SK_ID_PREV`, the unique id for the previous loan. Then we will group the resulting dataframe by the `SK_ID_CURR` to calculate the aggregation statistics for each client across all of their previous loans. If that's a little confusing, I'd suggest heading back to the [first feature engineering notebook](https://www.kaggle.com/willkoehrsen/introduction-to-manual-feature-engineering/output).**",1,willkoehrsen,introduction-to-manual-feature-engineering-p2,willkoehrsen_introduction-to-manual-feature-engineering-p2_m1
"## Function to Aggregate Numeric Data

This groups data by the `group_var` and calculates `mean`, `max`, `min`, and `sum`. It will only be applied to numeric data by default in pandas.",2,willkoehrsen,introduction-to-manual-feature-engineering-p2,willkoehrsen_introduction-to-manual-feature-engineering-p2_m2
"### Function to Calculate Categorical Counts

This function calculates the occurrences (counts) of each category in a categorical variable for each client. It also calculates the normed count, which is the count for a category divided by the total counts for all categories in a categorical variable. ",3,willkoehrsen,introduction-to-manual-feature-engineering-p2,willkoehrsen_introduction-to-manual-feature-engineering-p2_m3
"### Function for KDE Plots of Variable

We also made a function that plots the distribution of variable colored by the value of `TARGET` (either 1 for did not repay the loan or 0 for did repay the loan). We can use this function to visually examine any new variables we create. This also calculates the correlation cofficient of the variable with the target which can be used as an approximation of whether or not the created variable will be useful. ",4,willkoehrsen,introduction-to-manual-feature-engineering-p2,willkoehrsen_introduction-to-manual-feature-engineering-p2_m4
"# Function to Convert Data Types

This will help reduce memory usage by using more efficient types for the variables. For example `category` is often a better type than `object` (unless the number of unique categories is close to the number of rows in the dataframe).",5,willkoehrsen,introduction-to-manual-feature-engineering-p2,willkoehrsen_introduction-to-manual-feature-engineering-p2_m5
Let's deal with one dataframe at a time. First up is the `previous_applications`. This has one row for every previous loan a client had at Home Credit. A client can have multiple previous loans which is why we need to aggregate statistics for each client.,6,willkoehrsen,introduction-to-manual-feature-engineering-p2,willkoehrsen_introduction-to-manual-feature-engineering-p2_m6
### previous_application,7,willkoehrsen,introduction-to-manual-feature-engineering-p2,willkoehrsen_introduction-to-manual-feature-engineering-p2_m7
We can join the calculated dataframe to the main training dataframe using a merge. Then we should delete the calculated dataframes to avoid using too much of the kernel memory.,8,willkoehrsen,introduction-to-manual-feature-engineering-p2,willkoehrsen_introduction-to-manual-feature-engineering-p2_m8
"We are going to have to be careful about calculating too many features. We don't want to overwhelm the model with too many irrelevant features or features with too many missing values. In the previous notebook, we removed any features with more than 75% missing values. To be consistent, we will apply that same logic here. ",9,willkoehrsen,introduction-to-manual-feature-engineering-p2,willkoehrsen_introduction-to-manual-feature-engineering-p2_m9
## Function to Calculate Missing Values,10,willkoehrsen,introduction-to-manual-feature-engineering-p2,willkoehrsen_introduction-to-manual-feature-engineering-p2_m10
# Applying to More Data,11,willkoehrsen,introduction-to-manual-feature-engineering-p2,willkoehrsen_introduction-to-manual-feature-engineering-p2_m11
### Function to Aggregate Stats at the Client Level,12,willkoehrsen,introduction-to-manual-feature-engineering-p2,willkoehrsen_introduction-to-manual-feature-engineering-p2_m12
## Monthly Cash Data,13,willkoehrsen,introduction-to-manual-feature-engineering-p2,willkoehrsen_introduction-to-manual-feature-engineering-p2_m13
## Monthly Credit Data,14,willkoehrsen,introduction-to-manual-feature-engineering-p2,willkoehrsen_introduction-to-manual-feature-engineering-p2_m14
### Installment Payments,15,willkoehrsen,introduction-to-manual-feature-engineering-p2,willkoehrsen_introduction-to-manual-feature-engineering-p2_m15
" #### Save All Newly Calculated Features
 
 Unfortunately, saving all the created features does not work in a Kaggle notebook. You will have to run the code on your personal machine. I have run the code and uploaded the [entire datasets here](https://www.kaggle.com/willkoehrsen/home-credit-manual-engineered-features). I plan on doing some feature selection and uploading reduced versions of the datasets. Right now, they are slightly to big to handle in Kaggle notebooks or scripts. .",16,willkoehrsen,introduction-to-manual-feature-engineering-p2,willkoehrsen_introduction-to-manual-feature-engineering-p2_m16
## Modeling,17,willkoehrsen,introduction-to-manual-feature-engineering-p2,willkoehrsen_introduction-to-manual-feature-engineering-p2_m17
"# Introduction: Manual Feature Engineering

If you are new to this competition, I highly suggest checking out [this notebook](https://www.kaggle.com/willkoehrsen/start-here-a-gentle-introduction/) to get started.

In this notebook, we will explore making features by hand for the Home Credit Default Risk competition. In an earlier notebook, we used only the `application` data in order to build a model. The best model we made from this data achieved a score on the leaderboard around 0.74. In order to better this score, we will have to include more information from the other dataframes. Here, we will look at using information from the `bureau` and `bureau_balance` data. The definitions of these data files are:

* bureau: information about client's previous loans with other financial institutions reported to Home Credit. Each previous loan has its own row.
* bureau_balance: monthly information about the previous loans. Each month has its own row.

Manual feature engineering can be a tedious process (which is why we use automated feature engineering with featuretools!) and often relies on domain expertise. Since I have limited domain knowledge of loans and what makes a person likely to default, I will instead concentrate of getting as much info as possible into the final training dataframe. The idea is that the model will then pick up on which features are important rather than us having to decide that. Basically, our approach is to make as many features as possible and then give them all to the model to use! Later, we can perform feature reduction using the feature importances from the model or other techniques such as PCA. 

The process of manual feature engineering will involve plenty of Pandas code, a little patience, and a lot of great practice manipulation data. Even though automated feature engineering tools are starting to be made available, feature engineering will still have to be done using plenty of data wrangling for a little while longer. ",0,willkoehrsen,introduction-to-manual-feature-engineering,willkoehrsen_introduction-to-manual-feature-engineering_m0
"## Example: Counts of a client's previous loans

To illustrate the general process of manual feature engineering, we will first simply get the count of a client's previous loans at other financial institutions. This requires a number of Pandas operations we will make heavy use of throughout the notebook:

* `groupby`: group a dataframe by a column. In this case we will group by the unique client, the `SK_ID_CURR` column
* `agg`: perform a calculation on the grouped data such as taking the mean of columns. We can either call the function directly (`grouped_df.mean()`) or use the `agg` function together with a list of transforms (`grouped_df.agg([mean, max, min, sum])`)
* `merge`: match the aggregated statistics to the appropriate client. We need to merge the original training data with the calculated stats on the `SK_ID_CURR` column which will insert `NaN` in any cell for which the client does not have the corresponding statistic

We also use the (`rename`) function quite a bit specifying the columns to be renamed as a dictionary. This is useful in order to keep track of the new variables we create.

This might seem like a lot, which is why we'll eventually write a function to do this process for us. Let's take a look at implementing this by hand first. ",1,willkoehrsen,introduction-to-manual-feature-engineering,willkoehrsen_introduction-to-manual-feature-engineering_m1
Scroll all the way to the right to see the new column. ,2,willkoehrsen,introduction-to-manual-feature-engineering,willkoehrsen_introduction-to-manual-feature-engineering_m2
"## Assessing Usefulness of New Variable with r value

To determine if the new variable is useful, we can calculate the Pearson Correlation Coefficient (r-value) between this variable and the target. This measures the strength of a linear relationship between two variables and ranges from -1 (perfectly negatively linear) to +1 (perfectly positively linear). The r-value is not best measure of the ""usefulness"" of a new variable, but it can give a first approximation of whether a variable will be helpful to a machine learning model. The larger the r-value of a variable with respect to the target, the more a change in this variable is likely to affect the value of the target. Therefore, we look for the variables with the greatest absolute value r-value relative to the target.

We can also visually inspect a relationship with the target using the Kernel Density Estimate (KDE) plot. ",3,willkoehrsen,introduction-to-manual-feature-engineering,willkoehrsen_introduction-to-manual-feature-engineering_m3
"### Kernel Density Estimate Plots

The kernel density estimate plot shows the distribution of a single variable (think of it as a smoothed histogram). To see the different in distributions dependent on the value of a categorical variable, we can color the distributions differently according to the category. For example, we can show the kernel density estimate of the `previous_loan_count` colored by whether the `TARGET` = 1 or 0. The resulting KDE will show any significant differences in the distribution of the variable between people who did not repay their loan (`TARGET == 1`) and the people who did (`TARGET == 0`). This can serve as an indicator of whether a variable will be 'relevant' to a machine learning model. 

We will put this plotting functionality in a function to re-use for any variable. ",4,willkoehrsen,introduction-to-manual-feature-engineering,willkoehrsen_introduction-to-manual-feature-engineering_m4
We can test this function using the `EXT_SOURCE_3` variable which we [found to be one of the most important variables ](https://www.kaggle.com/willkoehrsen/start-here-a-gentle-introduction) according to a Random Forest and Gradient Boosting Machine. ,5,willkoehrsen,introduction-to-manual-feature-engineering,willkoehrsen_introduction-to-manual-feature-engineering_m5
"Now for the new variable we just made, the number of previous loans at other institutions.",6,willkoehrsen,introduction-to-manual-feature-engineering,willkoehrsen_introduction-to-manual-feature-engineering_m6
"From this it's difficult to tell if this variable will be important. The correlation coefficient is extremely weak and there is almost no noticeable difference in the distributions. 

Let's move on to make a few more variables from the bureau dataframe. We will take the mean, min, and max of every numeric column in the bureau dataframe.",7,willkoehrsen,introduction-to-manual-feature-engineering,willkoehrsen_introduction-to-manual-feature-engineering_m7
"## Aggregating Numeric Columns

To account for the numeric information in the `bureau` dataframe, we can compute statistics for all the numeric columns. To do so, we `groupby` the client id, `agg` the grouped dataframe, and merge the result back into the training data. The `agg` function will only calculate the values for the numeric columns where the operation is considered valid. We will stick to using `'mean', 'max', 'min', 'sum'` but any function can be passed in here. We can even write our own function and use it in an `agg` call. ",8,willkoehrsen,introduction-to-manual-feature-engineering,willkoehrsen_introduction-to-manual-feature-engineering_m8
"We need to create new names for each of these columns. The following code makes new names by appending the stat to the name. Here we have to deal with the fact that the dataframe has a multi-level index. I find these confusing and hard to work with, so I try to reduce to a single level index as quickly as possible.",9,willkoehrsen,introduction-to-manual-feature-engineering,willkoehrsen_introduction-to-manual-feature-engineering_m9
Now we simply merge with the training data as we did before.,10,willkoehrsen,introduction-to-manual-feature-engineering,willkoehrsen_introduction-to-manual-feature-engineering_m10
"### Correlations of Aggregated Values with Target

We can calculate the correlation of all new values with the target. Again, we can use these as an approximation of the variables which may be important for modeling. ",11,willkoehrsen,introduction-to-manual-feature-engineering,willkoehrsen_introduction-to-manual-feature-engineering_m11
"In the code below, we sort the correlations by the magnitude (absolute value) using the `sorted` Python function. We also make use of an anonymous `lambda` function, another important Python operation that is good to know. ",12,willkoehrsen,introduction-to-manual-feature-engineering,willkoehrsen_introduction-to-manual-feature-engineering_m12
"None of the new variables have a significant correlation with the TARGET. We can look at the KDE plot of the highest correlated variable, `bureau_DAYS_CREDIT_mean`, with the target in  in terms of absolute magnitude correlation. ",13,willkoehrsen,introduction-to-manual-feature-engineering,willkoehrsen_introduction-to-manual-feature-engineering_m13
"The definition of this column is: ""How many days before current application did client apply for Credit Bureau credit"". My interpretation is this is the number of days that the previous loan was applied for before the application for a loan at Home Credit. Therefore, a larger negative number indicates the loan was further before the current loan application. We see an extremely weak positive relationship between the average of this variable and the target meaning that clients who applied for loans further in the past potentially are more likely to repay loans at Home Credit. With a correlation this weak though, it is just as likely to be noise as a signal. 

#### The Multiple Comparisons Problem

When we have lots of variables, we expect some of them to be correlated just by pure chance, a [problem known as multiple comparisons](https://towardsdatascience.com/the-multiple-comparisons-problem-e5573e8b9578). We can make hundreds of features, and some will turn out to be corelated with the target simply because of random noise in the data. Then, when our model trains, it may overfit to these variables because it thinks they have a relationship with the target in the training set, but this does not necessarily generalize to the test set. There are many considerations that we have to take into account when making features! ",14,willkoehrsen,introduction-to-manual-feature-engineering,willkoehrsen_introduction-to-manual-feature-engineering_m14
"## Function for Numeric Aggregations

Let's encapsulate all of the previous work into a function. This will allow us to compute aggregate stats for numeric columns across any dataframe. We will re-use this function when we want to apply the same operations for other dataframes.",15,willkoehrsen,introduction-to-manual-feature-engineering,willkoehrsen_introduction-to-manual-feature-engineering_m15
"To make sure the function worked as intended, we should compare with the aggregated dataframe we constructed by hand. ",16,willkoehrsen,introduction-to-manual-feature-engineering,willkoehrsen_introduction-to-manual-feature-engineering_m16
"If we go through and inspect the values, we do find that they are equivalent. We will be able to reuse this function for calculating numeric stats for other dataframes. Using functions allows for consistent results and decreases the amount of work we have to do in the future! 

### Correlation Function

Before we move on, we can also make the code to calculate correlations with the target into a function.",17,willkoehrsen,introduction-to-manual-feature-engineering,willkoehrsen_introduction-to-manual-feature-engineering_m17
"## Categorical Variables

Now we move from the numeric columns to the categorical columns. These are discrete string variables, so we cannot just calculate statistics such as mean 
and max which only work with numeric variables. Instead, we will rely on calculating value counts of each category within each categorical variable. As an example, if we have the following dataframe:

| SK_ID_CURR | Loan type |
|------------|-----------|
| 1          | home      |
| 1          | home      |
| 1          | home      |
| 1          | credit    |
| 2          | credit    |
| 3          | credit    |
| 3          | cash      |
| 3          | cash      |
| 4          | credit    |
| 4          | home      |
| 4          | home      |

we will use this information counting the number of loans in each category for each client. 

| SK_ID_CURR | credit count | cash count | home count | total count |
|------------|--------------|------------|------------|-------------|
| 1          | 1            | 0          | 3          | 4           |
| 2          | 1            | 0          | 0          | 1           |
| 3          | 1            | 2          | 0          | 3           |
| 4          | 1            | 0          | 2          | 3           |


Then we can normalize these value counts by the total number of occurences of that categorical variable for that observation (meaning that the normalized counts must sum to 1.0 for each observation).

| SK_ID_CURR | credit count | cash count | home count | total count | credit count norm | cash count norm | home count norm |
|------------|--------------|------------|------------|-------------|-------------------|-----------------|-----------------|
| 1          | 1            | 0          | 3          | 4           | 0.25              | 0               | 0.75            |
| 2          | 1            | 0          | 0          | 1           | 1.00              | 0               | 0               |
| 3          | 1            | 2          | 0          | 3           | 0.33              | 0.66            | 0               |
| 4          | 1            | 0          | 2          | 3           | 0.33              | 0               | 0.66            |

Hopefully, encoding the categorical variables this way will allow us to capture the information they contain. If anyone has a better idea for this process, please let me know in the comments!
We will now go through this process step-by-step. At the end, we will wrap up all the code into one function to be re-used for many dataframes.",18,willkoehrsen,introduction-to-manual-feature-engineering,willkoehrsen_introduction-to-manual-feature-engineering_m18
First we one-hot encode a dataframe with only the categorical columns (`dtype == 'object'`).,19,willkoehrsen,introduction-to-manual-feature-engineering,willkoehrsen_introduction-to-manual-feature-engineering_m19
"The `sum` columns represent the count of that category for the associated client and the `mean` represents the normalized count. One-hot encoding makes the process of calculating these figures very easy!

We can use a similar function as before to rename the columns. Again, we have to deal with the multi-level index for the columns. We iterate through the first level (level 0) which is the name of the categorical variable appended with the value of the category (from one-hot encoding). Then we iterate  stats we calculated for each client. We will rename the column with the level 0 name appended with the stat. As an example, the column with `CREDIT_ACTIVE_Active` as level 0 and `sum` as level 1 will become `CREDIT_ACTIVE_Active_count`. ",20,willkoehrsen,introduction-to-manual-feature-engineering,willkoehrsen_introduction-to-manual-feature-engineering_m20
"The sum column records the counts and the mean column records the normalized count. 

We can merge this dataframe into the training data.",21,willkoehrsen,introduction-to-manual-feature-engineering,willkoehrsen_introduction-to-manual-feature-engineering_m21
"### Function to Handle Categorical Variables

To make the code more efficient, we can now write a function to handle the categorical variables for us. This will take the same form as the `agg_numeric` function in that it accepts a dataframe and a grouping variable. Then it will calculate the counts and normalized counts of each category for all categorical variables in the dataframe.",22,willkoehrsen,introduction-to-manual-feature-engineering,willkoehrsen_introduction-to-manual-feature-engineering_m22
"### Applying Operations to another dataframe

We will now turn to the bureau balance dataframe. This dataframe has monthly information about each client's previous loan(s) with other financial institutions. Instead of grouping this dataframe by the `SK_ID_CURR` which is the client id, we will first group the dataframe by the `SK_ID_BUREAU` which is the id of the previous loan. This will give us one row of the dataframe for each loan. Then, we can group by the `SK_ID_CURR` and calculate the aggregations across the loans of each client. The final result will be a dataframe with one row for each client, with stats calculated for their loans.",23,willkoehrsen,introduction-to-manual-feature-engineering,willkoehrsen_introduction-to-manual-feature-engineering_m23
"First, we can calculate the value counts of each status for each loan. Fortunately, we already have a function that does this for us! ",24,willkoehrsen,introduction-to-manual-feature-engineering,willkoehrsen_introduction-to-manual-feature-engineering_m24
"Now we can handle the one numeric column. The `MONTHS_BALANCE` column has the ""months of balance relative to application date."" This might not necessarily be that important as a numeric variable, and in future work we might want to consider this as a time variable. For now, we can just calculate the same aggregation statistics as previously. ",25,willkoehrsen,introduction-to-manual-feature-engineering,willkoehrsen_introduction-to-manual-feature-engineering_m25
"The above dataframes have the calculations done on each _loan_. Now we need to aggregate these for each _client_. We can do this by merging the dataframes together first and then since all the variables are numeric, we just need to aggregate the statistics again, this time grouping by the `SK_ID_CURR`. ",26,willkoehrsen,introduction-to-manual-feature-engineering,willkoehrsen_introduction-to-manual-feature-engineering_m26
"To recap, for the `bureau_balance` dataframe we:

1. Calculated numeric stats grouping by each loan
2. Made value counts of each categorical variable grouping by loan
3. Merged the stats and the value counts on the loans
4. Calculated numeric stats for the resulting dataframe grouping by the client id

The final resulting dataframe has one row for each client, with statistics calculated for all of their loans with monthly balance information. 

Some of these variables are a little confusing, so let's try to explain a few:

* `client_bureau_balance_MONTHS_BALANCE_mean_mean`: For each loan calculate the mean value of `MONTHS_BALANCE`. Then for each client, calculate the mean of this value for all of their loans. 
* `client_bureau_balance_STATUS_X_count_norm_sum`: For each loan, calculate the number of occurences of `STATUS` == X divided by the number of total `STATUS` values for the loan. Then, for each client, add up the values for each loan. ",27,willkoehrsen,introduction-to-manual-feature-engineering,willkoehrsen_introduction-to-manual-feature-engineering_m27
We will hold off on calculating the correlations until we have all the variables together in one dataframe. ,28,willkoehrsen,introduction-to-manual-feature-engineering,willkoehrsen_introduction-to-manual-feature-engineering_m28
"# Putting the Functions Together

We now have all the pieces in place to take the information from the previous loans at other institutions and the monthly payments information about these loans and put them into the main training dataframe. Let's do a reset of all the variables and then use the functions we built to do this from the ground up. This demonstrate the benefit of using functions for repeatable workflows! ",29,willkoehrsen,introduction-to-manual-feature-engineering,willkoehrsen_introduction-to-manual-feature-engineering_m29
### Counts of Bureau Dataframe,30,willkoehrsen,introduction-to-manual-feature-engineering,willkoehrsen_introduction-to-manual-feature-engineering_m30
### Aggregated Stats of Bureau Dataframe,31,willkoehrsen,introduction-to-manual-feature-engineering,willkoehrsen_introduction-to-manual-feature-engineering_m31
### Value counts of Bureau Balance dataframe by loan,32,willkoehrsen,introduction-to-manual-feature-engineering,willkoehrsen_introduction-to-manual-feature-engineering_m32
### Aggregated stats of Bureau Balance dataframe by loan,33,willkoehrsen,introduction-to-manual-feature-engineering,willkoehrsen_introduction-to-manual-feature-engineering_m33
### Aggregated Stats of Bureau Balance by Client,34,willkoehrsen,introduction-to-manual-feature-engineering,willkoehrsen_introduction-to-manual-feature-engineering_m34
## Insert Computed Features into Training Data,35,willkoehrsen,introduction-to-manual-feature-engineering,willkoehrsen_introduction-to-manual-feature-engineering_m35
"# Feature Engineering Outcomes

After all that work, now we want to take a look at the variables we have created. We can look at the percentage of missing values, the correlations of variables with the target, and also the correlation of variables with the other variables. The correlations between variables can show if we have collinear varibles, that is, variables that are highly correlated with one another. Often, we want to remove one in a pair of collinear variables because having both variables would be redundant. We can also use the percentage of missing values to remove features with a substantial majority of values that are not present. __Feature selection__ will be an important focus going forward, because reducing the number of features can help the model learn during training and also generalize better to the testing data. The ""curse of dimensionality"" is the name given to the issues caused by having too many features (too high of a dimension). As the number of variables increases, the number of datapoints needed to learn the relationship between these variables and the target value increases exponentially. 

Feature selection is the process of removing variables to help our model to learn and generalize better to the testing set. The objective is to remove useless/redundant variables while preserving those that are useful. There are a number of tools we can use for this process, but in this notebook we will stick to removing columns with a high percentage of missing values and variables that have a high correlation with one another. Later we can look at using the feature importances returned from models such as the `Gradient Boosting Machine` or `Random Forest` to perform feature selection.",36,willkoehrsen,introduction-to-manual-feature-engineering,willkoehrsen_introduction-to-manual-feature-engineering_m36
"## Missing Values

An important consideration is the missing values in the dataframe. Columns with too many missing values might have to be dropped. ",37,willkoehrsen,introduction-to-manual-feature-engineering,willkoehrsen_introduction-to-manual-feature-engineering_m37
"We see there are a number of columns with a high percentage of missing values. There is no well-established threshold for removing missing values, and the best course of action depends on the problem. Here, to reduce the number of features, we will remove any columns in either the training or the testing data that have greater than 90% missing values.",38,willkoehrsen,introduction-to-manual-feature-engineering,willkoehrsen_introduction-to-manual-feature-engineering_m38
"Before we remove the missing values, we will find the missing value percentages in the testing data. We'll then remove any columns with greater than 90% missing values in either the training or testing data.
Let's now read in the testing data, perform the same operations, and look at the missing values in the testing data. We already have calculated all the counts and aggregation statistics, so we only need to merge the testing data with the appropriate data. ",39,willkoehrsen,introduction-to-manual-feature-engineering,willkoehrsen_introduction-to-manual-feature-engineering_m39
## Calculate Information for Testing Data,40,willkoehrsen,introduction-to-manual-feature-engineering,willkoehrsen_introduction-to-manual-feature-engineering_m40
"We need to align the testing and training dataframes, which means matching up the columns so they have the exact same columns. This shouldn't be an issue here, but when we one-hot encode variables, we need to align the dataframes to make sure they have the same columns.",41,willkoehrsen,introduction-to-manual-feature-engineering,willkoehrsen_introduction-to-manual-feature-engineering_m41
"The dataframes now have the same columns (with the exception of the `TARGET` column in the training data). This means we can use them in a machine learning model which needs to see the same columns in both the training and testing dataframes.

Let's now look at the percentage of missing values in the testing data so we can figure out the columns that should be dropped.",42,willkoehrsen,introduction-to-manual-feature-engineering,willkoehrsen_introduction-to-manual-feature-engineering_m42
We ended up removing no columns in this round because there are no columns with more than 90% missing values. We might have to apply another feature selection method to reduce the dimensionality. ,43,willkoehrsen,introduction-to-manual-feature-engineering,willkoehrsen_introduction-to-manual-feature-engineering_m43
At this point we will save both the training and testing data. I encourage anyone to try different percentages for dropping the missing columns and compare the outcomes. ,44,willkoehrsen,introduction-to-manual-feature-engineering,willkoehrsen_introduction-to-manual-feature-engineering_m44
"## Correlations

First let's look at the correlations of the variables with the target. We can see in any of the variables we created have a greater correlation than those already present in the training data (from `application`). ",45,willkoehrsen,introduction-to-manual-feature-engineering,willkoehrsen_introduction-to-manual-feature-engineering_m45
"The highest correlated variable with the target (other than the `TARGET` which of course has a correlation of 1), is a variable we created. However, just because the variable is correlated does not mean that it will be useful, and we have to remember that if we generate hundreds of new variables, some are going to be correlated with the target simply because of random noise. 

Viewing the correlations skeptically, it does appear that several of the newly created variables may be useful. To assess the ""usefulness"" of variables, we will look at the feature importances returned by the model. For curiousity's sake (and because we already wrote the function) we can make a kde plot of two of the newly created variables.",46,willkoehrsen,introduction-to-manual-feature-engineering,willkoehrsen_introduction-to-manual-feature-engineering_m46
"This variable represents the average number of monthly records per loan for each client. For example, if a client had three previous loans with 3, 4, and 5 records in the monthly data, the value of this variable for them would be 4. Based on the distribution, clients with a greater number of average monthly records per loan were more likely to repay their loans with Home Credit. Let's not read too much into this value, but it could indicate that clients who have had more previous credit history are generally more likely to repay a loan.",47,willkoehrsen,introduction-to-manual-feature-engineering,willkoehrsen_introduction-to-manual-feature-engineering_m47
Well this distribution is all over the place. This variable represents the number of previous loans with a `CREDIT_ACTIVE` value of `Active` divided by the total number of previous loans for a client. The correlation here is so weak that I do not think we should draw any conclusions! ,48,willkoehrsen,introduction-to-manual-feature-engineering,willkoehrsen_introduction-to-manual-feature-engineering_m48
"### Collinear Variables

We can calculate not only the correlations of the variables with the target, but also the correlation of each variable with every other variable. This will allow us to see if there are highly collinear variables that should perhaps be removed from the data. 

Let's look for any variables that have a greather than 0.8 correlation with other variables.",49,willkoehrsen,introduction-to-manual-feature-engineering,willkoehrsen_introduction-to-manual-feature-engineering_m49
"For each of these pairs of highly correlated variables, we only want to remove one of the variables. The following code creates a set of variables to remove by only adding one of each pair. ",50,willkoehrsen,introduction-to-manual-feature-engineering,willkoehrsen_introduction-to-manual-feature-engineering_m50
We can remove these columns from both the training and the testing datasets. We will have to compare performance after removing these variables with performance keeping these variables (the raw csv files we saved earlier).,51,willkoehrsen,introduction-to-manual-feature-engineering,willkoehrsen_introduction-to-manual-feature-engineering_m51
"# Modeling 

To actually test the performance of these new datasets, we will try using them for machine learning! Here we will use a function I developed in another notebook to compare the features (the raw version with the highly correlated variables removed). We can run this kind of like an experiment, and the control will be the performance of just the `application` data in this function when submitted to the competition. I've already recorded that performance, 
so we can list out our control and our two test conditions:

__For all datasets, use the model shown below (with the exact hyperparameters).__

* control: only the data in the `application` files. 
* test one: the data in the `application` files with all of the data recorded from the `bureau` and `bureau_balance` files
* test two: the data in the `application` files with all of the data recorded from the `bureau` and `bureau_balance` files with highly correlated variables removed. ",52,willkoehrsen,introduction-to-manual-feature-engineering,willkoehrsen_introduction-to-manual-feature-engineering_m52
"### Control

The first step in any experiment is establishing a control. For this we will use the function defined above (that implements a Gradient Boosting Machine model) and the single main data source (`application`). ",53,willkoehrsen,introduction-to-manual-feature-engineering,willkoehrsen_introduction-to-manual-feature-engineering_m53
"Fortunately, once we have taken the time to write a function, using it is simple (if there's a central theme in this notebook, it's use functions to make things simpler and reproducible!). The function above returns a `submission` dataframe we can upload to the competition, a `fi` dataframe of feature importances, and a `metrics` dataframe with validation and test performance. ",54,willkoehrsen,introduction-to-manual-feature-engineering,willkoehrsen_introduction-to-manual-feature-engineering_m54
"The control slightly overfits because the training score is higher than the validation score. We can address this in later notebooks when we look at regularization (we already perform some regularization in this model by using `reg_lambda` and `reg_alpha` as well as early stopping). 

We can visualize the feature importance with another function, `plot_feature_importances`. The feature importances may be useful when it's time for feature selection. ",55,willkoehrsen,introduction-to-manual-feature-engineering,willkoehrsen_introduction-to-manual-feature-engineering_m55
__The control scores 0.745 when submitted to the competition.__,56,willkoehrsen,introduction-to-manual-feature-engineering,willkoehrsen_introduction-to-manual-feature-engineering_m56
"### Test One

Let's conduct the first test. We will just need to pass in the data to the function, which does most of the work for us.",57,willkoehrsen,introduction-to-manual-feature-engineering,willkoehrsen_introduction-to-manual-feature-engineering_m57
"Based on these numbers, the engineered features perform better than the control case. However, we will have to submit the predictions to the leaderboard before we can say if this better validation performance transfers to the testing data. ",58,willkoehrsen,introduction-to-manual-feature-engineering,willkoehrsen_introduction-to-manual-feature-engineering_m58
"Examining the feature improtances, it looks as if a few of the feature we constructed are among the most important. Let's find the percentage of the top 100 most important features that we made in this notebook. However, rather than just compare to the original features, we need to compare to the _one-hot encoded_ original features. These are already recorded for us in `fi` (from the original data). ",59,willkoehrsen,introduction-to-manual-feature-engineering,willkoehrsen_introduction-to-manual-feature-engineering_m59
Over half of the top 100 features were made by us! That should give us confidence that all the hard work we did was worthwhile. ,60,willkoehrsen,introduction-to-manual-feature-engineering,willkoehrsen_introduction-to-manual-feature-engineering_m60
__Test one scores 0.759 when submitted to the competition.__,61,willkoehrsen,introduction-to-manual-feature-engineering,willkoehrsen_introduction-to-manual-feature-engineering_m61
"### Test Two

That was easy, so let's do another run! Same as before but with the highly collinear variables removed. ",62,willkoehrsen,introduction-to-manual-feature-engineering,willkoehrsen_introduction-to-manual-feature-engineering_m62
"These results are better than the control, but slightly lower than the raw features. ",63,willkoehrsen,introduction-to-manual-feature-engineering,willkoehrsen_introduction-to-manual-feature-engineering_m63
__Test Two scores 0.753 when submitted to the competition.__,64,willkoehrsen,introduction-to-manual-feature-engineering,willkoehrsen_introduction-to-manual-feature-engineering_m64
"# Results

After all that work, we can say that including the extra information did improve performance! The model is definitely not optimized to our data, but we still had a noticeable improvement over the original dataset when using the calculated features. Let's officially summarize the performances:

| __Experiment__ | __Train AUC__ | __Validation AUC__ | __Test AUC__  |
|------------|-------|------------|-------|
| __Control__    | 0.815 | 0.760      | 0.745 |
| __Test One__   | 0.837 | 0.767      | 0.759 |
| __Test Two__   | 0.826 | 0.765      | 0.753 |


(Note that these scores may change from run to run of the notebook. I have not observed that the general ordering changes however.)

All of our hard work translates to a small improvement of 0.014 ROC AUC over the original testing data. Removing the highly collinear variables slightly decreases performance so we will want to consider a different method for feature selection. Moreover, we can say that some of the features we built are among the most important as judged by the model. 

In a competition such as this, even an improvement of this size is enough to move us up 100s of spots on the leaderboard. By making numerous small improvements such as in this notebook, we can gradually achieve better and better performance. I encourage others to use the results here to make their own improvements, and I will continue to document the steps I take to help others. 

## Next Steps

Going forward, we can now use the functions we developed in this notebook on the other datasets. There are still 4 other data files to use in our model! In the next notebook, we will incorporate the information from these other data files (which contain information on previous loans at Home Credit) into our training data. Then we can build the same model and run more experiments to determine the effect of our feature engineering. There is plenty more work to be done in this competition, and plenty more gains in performance to be had! I'll see you in the next notebook.",65,willkoehrsen,introduction-to-manual-feature-engineering,willkoehrsen_introduction-to-manual-feature-engineering_m65
"# Introduction: Random vs Bayesian Optimization Model Tuning

In this notebook, we will compare random search and Bayesian optimization hyperparameter tuning methods implemented in two previous notebooks.

* [Intro to Model Tuning: Grid and Random Search](https://www.kaggle.com/willkoehrsen/intro-to-model-tuning-grid-and-random-search)
* [Automated Model Tuning](https://www.kaggle.com/willkoehrsen/automated-model-tuning)

In those notebooks we saw results of the methods applied to a limited dataset (10000 observations) but here we will explore results on a complete dataset with 700 + features.  The results in this notebook are from 500 iterations of random search and 400 iterations of Bayesian Optimization (these took about 5 days to run each). We will thoroughly explore the results both visually and statistically, and then implement the best hyperparameter values on a full set of features. After all the hard work in the random search and Bayesian optimization notebooks, now we get to have some fun! 

# Roadmap

Our plan of action is as follows:

1. High Level Overview
    * Which method did best? 
2. Examine distribution of scores
    * Are there trends over the course of the search?
3. Explore hyperparameter values
    * Look at values over the course of the search
    * Identify correlations between hyperparameters and the score
4. Perform ""meta"" machine learning using these results
    * Fit a linear regression to results and look at coefficients
5. Train a model on the full set of features using the best performing values
    * Try best results from both random search and bayesian optimization
6.  Lay out next steps
    * How can we use these results for this _and other_ problems? 
    * Are there better methods for hyperparameter optimization
    
At each step, we will use plenty of figures and statistics to explore the data. This will be a fun notebook (even though it may not land you at the top of the leaderboard)! 

## Recap 

In the respective notebooks, we examined we performed 1000 iterations of random search and Bayesian optimization on a reduced sample of the dataset (10000 rows). We compared the cross-validation ROC AUC on the training data, the score on a ""testing set"" (6000 observations) and the score on the real test set when submitted to the competition leaderboard. Results are below:

| Method                               | Cross Validation Score | Test Score (on 6000 Rows) | Submission to Leaderboard | Iterations to best score |
|--------------------------------------|------------------------|---------------------------|---------------------------|--------------------------|
| Random Search                        | 0.73110                | 0.73274                   | 0.782                     | 996                      |
| Bayesian Hyperparameter Optimization | 0.73448                | 0.73069                   | 0.792                     | 596                      ",0,willkoehrsen,model-tuning-results-random-vs-bayesian-opt,willkoehrsen_model-tuning-results-random-vs-bayesian-opt_m0
"__Take these with some skepticism because they were performed on a very small subset of the data!__ 

For more rigorous results, we will turn to the evaluation metrics from running __500 iterations (with random search)__ and __400+ iterations (with Bayesian Optimization)__ on a full training dataset with about 700 features (the features are from [this notebook](https://www.kaggle.com/jsaguiar/updated-0-792-lb-lightgbm-with-simple-features) by [Aguiar](https://www.kaggle.com/jsaguiar)). These iterations took around 6 days on a machine with 128 GB of RAM so they will not run in a kernel! The Bayesian Optimization method is still running and I will update the results as they finish.

__In this notebook  we will focus only on the results and building the best model, so for the explanations of the methods, refer to the previous notebooks! __

# Overall Results

First, let's start with the most basic question: which model produced the highest cross validation ROC AUC score (using 5 folds) on the training dataset?",1,willkoehrsen,model-tuning-results-random-vs-bayesian-opt,willkoehrsen_model-tuning-results-random-vs-bayesian-opt_m1
"Well, there you go! __Random search slightly outperformed  Bayesian optimization and found a higher cross validation model in far fewer iterations.__ However, as we will shortly see, this does not mean random search is the better hyperparameter optimization method. 

When submitted to the competition (at the end of this notebook):

* __Random search results scored 0.790__
* __Bayesian optimization results scored 0.791__

What were the best model hyperparameters from both methods?

####  Random Search best Hyperparameters",2,willkoehrsen,model-tuning-results-random-vs-bayesian-opt,willkoehrsen_model-tuning-results-random-vs-bayesian-opt_m2
#### Bayesian Optimization best Hyperparameters,3,willkoehrsen,model-tuning-results-random-vs-bayesian-opt,willkoehrsen_model-tuning-results-random-vs-bayesian-opt_m3
"If we compare the individual values, we actually see that they are fairly close together when we consider the entire search grid! 

## Distribution of Scores

Let's plot the distribution of scores for both models in a kernel density estimate plot.",4,willkoehrsen,model-tuning-results-random-vs-bayesian-opt,willkoehrsen_model-tuning-results-random-vs-bayesian-opt_m4
"Bayesian optimization did not produce the highest individual score, but it did tend to spend more time evaluating ""better"" values of hyperparameters. __Random search got lucky and found the best values but Bayesian optimization tended to ""concentrate"" on better-scoring values__. That's pretty much what we expect: random search does a good job of exploring the search space which means it will probably happen upon a high-scoring set of values (if the space is not extremely high-dimensional) while Bayesian optimization will tend to focus on a set of values that yield higher scores. __If all you wanted was the conclusion, then you're probably good to go. If you really enjoy making plots and doing exploratory data analysis and want to gain a better understanding of how these methods work, then read on!__ In the next few sections, we will thoroughly explore these results.

Our plan for going through the results is as follows:

* Distribution of scores
    * Overall distribution
    * Score versus the iteration (did scores improve as search progressed)
* Distribution of hyperparameters
    * Overall distribution including the hyperparameter grid for a reference
    * Hyperparameters versus iteration to look at _evolution_ of values
* Hyperparameter values versus the score
    * Do scores improve with certain values of hyperparameters (correlations)
    * 3D plots looking at effects of 2 hyperparameters at a time on the score
* Additional Plots
    * Time to run each evaluation for Bayesian optimization
    * Correlation heatmaps of hyperparameters with score
    
There will be all sorts of plots: heatmaps, 3D scatterplots, density plots, bar charts (hey even bar charts can be helpful!)

After going through the results, we will do a little meta-machine learning, and implement the best model on the full set of features.",5,willkoehrsen,model-tuning-results-random-vs-bayesian-opt,willkoehrsen_model-tuning-results-random-vs-bayesian-opt_m5
"# Distribution of Scores

We already saw the kernel density estimate plot, so let's go on to a bar plot. First we'll get the data in a long format.",6,willkoehrsen,model-tuning-results-random-vs-bayesian-opt,willkoehrsen_model-tuning-results-random-vs-bayesian-opt_m6
"Keep in mind that random search ran for more iterations (as of now). Even so, we can see that Bayesian Optimization tended to produce much more higher cross validation scores. Let's look at the statistical averages:",7,willkoehrsen,model-tuning-results-random-vs-bayesian-opt,willkoehrsen_model-tuning-results-random-vs-bayesian-opt_m7
"If we are going by mean, then Bayesian optimization is the clear winner. If we go by high score, then random search just wins out. ",8,willkoehrsen,model-tuning-results-random-vs-bayesian-opt,willkoehrsen_model-tuning-results-random-vs-bayesian-opt_m8
"## Score versus Iteration

Now, to see if either method improves over the course of the search, we need to plot the score as a function of the iteration. ",9,willkoehrsen,model-tuning-results-random-vs-bayesian-opt,willkoehrsen_model-tuning-results-random-vs-bayesian-opt_m9
"Again keeping in mind that Bayesian optimization has not yet finished, we can see a clear upward trend for this method and no trend whatsoever for random search. 

### Linear Regression of Scores versus Iteration

To show that Bayesian optimization improves over time, we can regress the score by the iteration. Then, we can use this to extrapolate into the future, __a wildly inappropriate technique in this case, but fun nonetheless!__

Here we use `np.polyfit` with a degree of 1 for the linear regression (you can compare the results with `LinearRegression`  from `sklearn.linear_model`.",10,willkoehrsen,model-tuning-results-random-vs-bayesian-opt,willkoehrsen_model-tuning-results-random-vs-bayesian-opt_m10
The random search slope is basically zero. ,11,willkoehrsen,model-tuning-results-random-vs-bayesian-opt,willkoehrsen_model-tuning-results-random-vs-bayesian-opt_m11
"The Bayesian slope is about 15 times greater than that of random search! What happens if we say run these methods for 10,000 iterations?",12,willkoehrsen,model-tuning-results-random-vs-bayesian-opt,willkoehrsen_model-tuning-results-random-vs-bayesian-opt_m12
"Incredible! I told you this was wildly inappropriate. Nonetheless, the slope does indicate that Bayesian optimization ""learns"" the hyperparameter values that do better over time. It then concentrates on evaluating these rather than spending time exploring other values as does random search. This means it can get stuck in a local optimum and can tend to __exploit__ values rather than continue to __explore__.

Now we will move on to the actual values of the hyperparameters.",13,willkoehrsen,model-tuning-results-random-vs-bayesian-opt,willkoehrsen_model-tuning-results-random-vs-bayesian-opt_m13
"# Hyperparameter Values

For each hyperparameter, we will plot the values tried by both searches as well as the reference distribution (which was the same in both cases, just a grid for random and distributions for Bayesian). We would expect the random search to almost exactly match the reference - it will converge on the reference given enough iterations.

First, we will process the results into a dataframe where each column is one hyperparameter. Saving the file converted the dictionary into a string, so we use `ast.literal_eval` to convert back to a dictionary before adding as a row in the dataframe.",14,willkoehrsen,model-tuning-results-random-vs-bayesian-opt,willkoehrsen_model-tuning-results-random-vs-bayesian-opt_m14
Next we define the hyperparameter grid that was used (the same ranges applied in both searches).,15,willkoehrsen,model-tuning-results-random-vs-bayesian-opt,willkoehrsen_model-tuning-results-random-vs-bayesian-opt_m15
"# Distributions of Search Values

Below are the kernel density estimate plots for each hyperparameter. The dashed vertical lines indicate the ""optimal"" value found in the respective searches. 

We start with the learning rate:",16,willkoehrsen,model-tuning-results-random-vs-bayesian-opt,willkoehrsen_model-tuning-results-random-vs-bayesian-opt_m16
"Even though the search domain extended from 0.005 to 0.2, both optimal values clustered around a lower value. Perhaps this tells us we should concentrate further searches in this area below 0.02?

That code was a little tedious, so let's write a function that makes the same code for any hyperparameter (feel free to pick your own colors!).",17,willkoehrsen,model-tuning-results-random-vs-bayesian-opt,willkoehrsen_model-tuning-results-random-vs-bayesian-opt_m17
"We can do this for all of the hyperparameters. These results can be used to inform further searches. They can even be used to define a grid search over a concentrated region. The problem with grid search is the insane compuational and time costs involved, and a smaller hyperparameter grid will help immensely! ",18,willkoehrsen,model-tuning-results-random-vs-bayesian-opt,willkoehrsen_model-tuning-results-random-vs-bayesian-opt_m18
"The `reg_alpha` and `reg_lambda` best scores seem to complement one another for Bayesian optimization. In other words, if either `reg_lambda` or `reg_alpha` is high (say greater than 0.5), then the other should be low (below 0.5). These hyperparameters control a penalty placed on the weights of the trees and thus are meant to control overfitting. It might make sense if only one needs to be high then.",19,willkoehrsen,model-tuning-results-random-vs-bayesian-opt,willkoehrsen_model-tuning-results-random-vs-bayesian-opt_m19
"### Boosting Type

The boosting type deserves its own section because it is a categorical variable, and because as we will see, it has an outsized effect on model performance. First, let's calculate statistics grouped by boosting type for each search method.",20,willkoehrsen,model-tuning-results-random-vs-bayesian-opt,willkoehrsen_model-tuning-results-random-vs-bayesian-opt_m20
"In both search methods, the `gbdt` (gradient boosted decision tree) and `dart` (dropout meets additive regression tree) do much better than `goss` (gradient based one-sided sampling). `gbdt` does the best on average (and for the max), so it might make sense to use that method in the future! Let's view the results as a barchart:",21,willkoehrsen,model-tuning-results-random-vs-bayesian-opt,willkoehrsen_model-tuning-results-random-vs-bayesian-opt_m21
"__`gbdt` (or `dart`) it should be! Notice that random search tried `gbdt` about the same number of times as the other two (since it selected with no reasoning) while Bayesian optimization tried `gbdt` much more often. __

Since `gbdt` supports `subsample` (using on a sample of the observations to train on in every tree) we can plot the distribution of `subsample` where `boosting_type=='gbdt'`. We also show the reference distribution.",22,willkoehrsen,model-tuning-results-random-vs-bayesian-opt,willkoehrsen_model-tuning-results-random-vs-bayesian-opt_m22
There is a significant disagreement between the two methods on the optimal value for `subsample`. Perhaps we would want to leave this as a wide distribution in any further searches (although some subsampling does look to be beneficial).,23,willkoehrsen,model-tuning-results-random-vs-bayesian-opt,willkoehrsen_model-tuning-results-random-vs-bayesian-opt_m23
"Finally, we can look at the instance of `is_unbalance`, a hyperparameter that tells LightGBM whether or not to treat the problem as unbalance classification.",24,willkoehrsen,model-tuning-results-random-vs-bayesian-opt,willkoehrsen_model-tuning-results-random-vs-bayesian-opt_m24
"__According to the average score, it pretty much does not matter if this hyperparameter is `True` or `False`.__ To be honest, I'm not sure what difference this is supposed to make, so anyone who wants can fill me in!",25,willkoehrsen,model-tuning-results-random-vs-bayesian-opt,willkoehrsen_model-tuning-results-random-vs-bayesian-opt_m25
"# Hyperparameters versus Iteration

Next we will take a look at the __evolution__ of the Bayesian search (random search shows no pattern as expected) by graphing the values versus the iteration. This can inform us the direction in which the search was heading in terms of where the values tended to cluster. Given these graphs, we might then be able to extrapolate values that lead to even higher scores (or maybe not, _extrapolation is dangerous_!)

The black star in the plots below signifies the best scoring value.",26,willkoehrsen,model-tuning-results-random-vs-bayesian-opt,willkoehrsen_model-tuning-results-random-vs-bayesian-opt_m26
"We want to be careful about placing too much value in these results, because remember, the Bayesian optimization could have found a local minimum of the cross validation loss that it is exploting. Moreover, the trends here are generally pretty small. It is encouraging that the best value was found close to the end of the search indicating cross validation scores were continuing to improve. 

Next, we can look at the values of the score as a function of the hyperparameter values. This is again a dangerous area! ",27,willkoehrsen,model-tuning-results-random-vs-bayesian-opt,willkoehrsen_model-tuning-results-random-vs-bayesian-opt_m27
"# Plots of Hyperparameters vs Score

![](http://)These next plots show the value of a single hyperparameter versus the score. We want to avoid placing too much emphasis on these graphs, because we are not changing one hyperparameter at a time. Therefore, if there are trends, it might not be solely due to the single hyperparameter we show. A truly accurate grid would be 10-dimensional and show the values of __all__ hyperparameters and the resulting score. If we could understand a __10-dimensional__ graph, then we might be able to figure out the optimal combination of hyperparameters! ",28,willkoehrsen,model-tuning-results-random-vs-bayesian-opt,willkoehrsen_model-tuning-results-random-vs-bayesian-opt_m28
"__The only clear distinction is that the score decreases as the learning rate increases.__ Of course, we cannot say whether that is due to the learning rate itself, or some other factor (we will look at the interplay between the learning rate and the number of esimators shortly). The learning rate domain was on a logarithmic scale, so it's most accurate for the plot to be as well (unfortunately I cannot get this to work yet).",29,willkoehrsen,model-tuning-results-random-vs-bayesian-opt,willkoehrsen_model-tuning-results-random-vs-bayesian-opt_m29
Now for the next four hyperparameters versus the score.,30,willkoehrsen,model-tuning-results-random-vs-bayesian-opt,willkoehrsen_model-tuning-results-random-vs-bayesian-opt_m30
"There are not any strong trends here. Next we will try to look at two hyperparameters simultaneously versus the score in a 3-dimensional plot. This makes sense for hyperparameters that work in concert, such as the learning rate and the number of esimators or the two regularization values.",31,willkoehrsen,model-tuning-results-random-vs-bayesian-opt,willkoehrsen_model-tuning-results-random-vs-bayesian-opt_m31
"## 3D Plots 

To try and examine the simultaneous effects of hyperparameters, we can make 3D plots with 2 hyperparameters and the score. A truly accurate plot would be 10-D (one for each hyperparameter) but in this case we will stick to 3 dimensions. 3D plots can be made in matplotlib by import `Axes3D` and specifying the `3d` projection in a call to `.add_subplot`",32,willkoehrsen,model-tuning-results-random-vs-bayesian-opt,willkoehrsen_model-tuning-results-random-vs-bayesian-opt_m32
First up is `reg_alpha` and `reg_lambda`. These control the amount of regularization on each decision tree and help to prevent overfitting to the training data.,33,willkoehrsen,model-tuning-results-random-vs-bayesian-opt,willkoehrsen_model-tuning-results-random-vs-bayesian-opt_m33
"It's a little difficult to tell much from this plot. If we look at the best values and then look at the plot, we can see that scores do tend to be higher around 0.9 for `reg_alpha`and 0.2 for `reg_lambda`.  Later, we'll make the same plot for the Bayesian Optimization for comparison.",34,willkoehrsen,model-tuning-results-random-vs-bayesian-opt,willkoehrsen_model-tuning-results-random-vs-bayesian-opt_m34
The next plot is learning rate and number of estimators versus the score. __Remember that the number of estimators was selected using early stopping for 100 rounds with 5-fold cross validation__. The number of estimators __was not__ a hyperparameter in the grid that we searched over. Early stopping is a more efficient method of finding the best number of estimators than including it in a search (based on my limited experience)!,35,willkoehrsen,model-tuning-results-random-vs-bayesian-opt,willkoehrsen_model-tuning-results-random-vs-bayesian-opt_m35
Here there appears to be a clear trend: a lower learning rate leads to higher values! What does the plot of just learning rate versus number of estimators look like?,36,willkoehrsen,model-tuning-results-random-vs-bayesian-opt,willkoehrsen_model-tuning-results-random-vs-bayesian-opt_m36
"This plot is very easy to interpret: the lower the learning rate, the more estimators that will be trained. From our knowledge of the model, this makes sense: each individual decision trees contribution is lessened as the learning rate is decreased leading to a need for more decision trees in the ensemble. Moreover, from the previous graphs, it appears that decreasing the learning rate increases the model score.",37,willkoehrsen,model-tuning-results-random-vs-bayesian-opt,willkoehrsen_model-tuning-results-random-vs-bayesian-opt_m37
"### Function for 3D plotting

Any time you write code more than twice, it should be encoded into a function! That's what the next code block is for: putting this code into a function that we can use many times! This function can be used for __any__ 3d plotting needs.",38,willkoehrsen,model-tuning-results-random-vs-bayesian-opt,willkoehrsen_model-tuning-results-random-vs-bayesian-opt_m38
The bayesian optimization results are close in trend to those from random search: lower learning rate leads to higher cross validation scores.,39,willkoehrsen,model-tuning-results-random-vs-bayesian-opt,willkoehrsen_model-tuning-results-random-vs-bayesian-opt_m39
"Again, we probably want one of the regularization values to be high and the other to be low. This must help to ""balance"" the model between bias and variance. ",40,willkoehrsen,model-tuning-results-random-vs-bayesian-opt,willkoehrsen_model-tuning-results-random-vs-bayesian-opt_m40
"# Correlations between Hyperparameters and Score

Time for another dangerous act: finding correlations between the hyperparameters and the score. These are not going to be accurate because again, we are not varying one value at a time! Nonetheless, we may discover useful insight about the Gradient Boosting Machine model.",41,willkoehrsen,model-tuning-results-random-vs-bayesian-opt,willkoehrsen_model-tuning-results-random-vs-bayesian-opt_m41
### Correlations for Random Search,42,willkoehrsen,model-tuning-results-random-vs-bayesian-opt,willkoehrsen_model-tuning-results-random-vs-bayesian-opt_m42
"As expected, the `learning_rate` has one of the greatest correlations with the score. The `subsample` rate might be affected by the fact that 1/3 of the time this was set to 1.0.",43,willkoehrsen,model-tuning-results-random-vs-bayesian-opt,willkoehrsen_model-tuning-results-random-vs-bayesian-opt_m43
### Correlations for Bayesian Optimization,44,willkoehrsen,model-tuning-results-random-vs-bayesian-opt,willkoehrsen_model-tuning-results-random-vs-bayesian-opt_m44
"The `learning_rate` again appears to be moderately correlated with the score. This should tell us again that a lower learning rate tends to co-occur with a higher cross-validation score, but not that this is nexessarily the cause of the higher score. ",45,willkoehrsen,model-tuning-results-random-vs-bayesian-opt,willkoehrsen_model-tuning-results-random-vs-bayesian-opt_m45
"## Correlation Heatmap

Now we can make a heatmap of the correlations. I enjoy heatmaps and thankfully, they are not very difficult to make in `seaborn`.",46,willkoehrsen,model-tuning-results-random-vs-bayesian-opt,willkoehrsen_model-tuning-results-random-vs-bayesian-opt_m46
That's a lot of plot for not very much code! We can see that the number of estimators and the learning rate have the greatest magnitude correlation (ignoring subsample which is influenced by the boosting type).,47,willkoehrsen,model-tuning-results-random-vs-bayesian-opt,willkoehrsen_model-tuning-results-random-vs-bayesian-opt_m47
Feel free to use this code for your own heatmaps! (Also send me color recommendations because I am not great at picking out a palette).,48,willkoehrsen,model-tuning-results-random-vs-bayesian-opt,willkoehrsen_model-tuning-results-random-vs-bayesian-opt_m48
"# Meta-Machine Learning

So we have a labeled set of data: the hyperparameter values and the resulting score. Clearly, the next step is to use these for machine learning? Yes, here we will perform _meta-machine learning_ by fitting an estimator on top of the hyperparameter values and the scores. This is a supervised regression problem, and although we can use any method for learning the data, here we will stick to a linear regression. This will let us examine the coefficients on each hyperparameter and will help reduce overfitting. ",49,willkoehrsen,model-tuning-results-random-vs-bayesian-opt,willkoehrsen_model-tuning-results-random-vs-bayesian-opt_m49
"If we wanted, we could treat this as _another optimization problem_ and try to maximize the linear regression in terms of the score! However, for now I think we have done enough optimization. 

It's time to move on to implementing the best hyperparameter values from random and Bayesian optimization on the full dataset.",50,willkoehrsen,model-tuning-results-random-vs-bayesian-opt,willkoehrsen_model-tuning-results-random-vs-bayesian-opt_m50
"# Implementation

The full set of features on which these results come are from [this notebook](https://www.kaggle.com/jsaguiar/updated-0-792-lb-lightgbm-with-simple-features) by [Aguiar](https://www.kaggle.com/jsaguiar)). Here, we will load in the same features, train on the full training features and make predictions on the testing data. These can then be uploaded to the competition.",51,willkoehrsen,model-tuning-results-random-vs-bayesian-opt,willkoehrsen_model-tuning-results-random-vs-bayesian-opt_m51
First we need to format the data and extract the labels.,52,willkoehrsen,model-tuning-results-random-vs-bayesian-opt,willkoehrsen_model-tuning-results-random-vs-bayesian-opt_m52
We can also save the features to later use for plotting feature importances.,53,willkoehrsen,model-tuning-results-random-vs-bayesian-opt,willkoehrsen_model-tuning-results-random-vs-bayesian-opt_m53
### Random Search,54,willkoehrsen,model-tuning-results-random-vs-bayesian-opt,willkoehrsen_model-tuning-results-random-vs-bayesian-opt_m54
### Bayesian Optimization,55,willkoehrsen,model-tuning-results-random-vs-bayesian-opt,willkoehrsen_model-tuning-results-random-vs-bayesian-opt_m55
"### Competition Results

* __Random search results scored 0.790__
* __Bayesian optimization results scored 0.791__

If we go by best score on the public leaderboard, Bayesian Optimization wins! However, the public leaderboard is based only on 10% of the test data, so it's possible this is a result of overfitting to this particular subset of the testing data. Overall, I would say the complete results suggest that both methods produce similar outcomes especially when run for enough iterations. Either method is better than hand-tuning! ",56,willkoehrsen,model-tuning-results-random-vs-bayesian-opt,willkoehrsen_model-tuning-results-random-vs-bayesian-opt_m56
"#### Feature Importances

As a final step, we can compare the feature importances between the models from the best hyperparameters. It would be interesting to see if the hyperparameter values has an effect on the feature importances.",57,willkoehrsen,model-tuning-results-random-vs-bayesian-opt,willkoehrsen_model-tuning-results-random-vs-bayesian-opt_m57
"The feature importances look to be relatively stable across hyperparameter values. This is what I expected, but at the same time, we can see that the _absolute magnitude_ of the importances differs significantly but not the _relative ordering_.",58,willkoehrsen,model-tuning-results-random-vs-bayesian-opt,willkoehrsen_model-tuning-results-random-vs-bayesian-opt_m58
"# Conclusions

Random search narrowly beat out Bayesian optimization in terms of finding the hyperparameter values that resulted in the highest cross validation ROC AUC. That single number does not tell the whole story though as the Bayesian method average ROC AUC was much higher than that of random search. We expect this to be the case because Bayesian optimization should focus on higher scoring values based on the surrogate model of the objective function it constructs. Morevoer, this tells us Bayesian optimization is a valuable technique, but random search can still happen upon better values in fewer search iterations if we are lucky. 

* Random search slightly outperformed Bayesian optimization in terms of cv ROC AUC 
* Bayesian optimization average scores were much higher than random search indicating it spends more time evaluating ""better"" hyperparameters
* Bayesian scored 0.791 when submitted and random search scored 0.790 indicating that with enough iterations, the methods deliver similar results
* Boosting type ""gdbt"" did much better than ""goss"" with ""dart"" nearly as good
* A lower learning rate resulted in higher model scores: lower than 0.02 looks to be optimal
* `reg_alpha` and `reg_lambda` should complement one another: if one is high (above 0.5), than the other should be lower (below 0.5)
* Some subsampling appears to increase the model scores
* The other hyperparameters either did not have a significant effect, or their effects are intertwined and hence could not be disentangled in this study

Feel free to build upon these results! I'm curious if the best hyperparameters for this dataset will translate to other datasets, either for this problem, or for vastly different data science problems. The best way to find out is to try them! 

If you're looking for more work on this problem, I have a series of notebooks documenting my work:

__Additional Notebooks__ 

* [A Gentle Introduction](https://www.kaggle.com/willkoehrsen/start-here-a-gentle-introduction)
* [Manual Feature Engineering Part One](https://www.kaggle.com/willkoehrsen/introduction-to-manual-feature-engineering)
* [Manual Feature Engineering Part Two](https://www.kaggle.com/willkoehrsen/introduction-to-manual-feature-engineering-p2)
* [Introduction to Automated Feature Engineering](https://www.kaggle.com/willkoehrsen/automated-feature-engineering-basics)
* [Advanced Automated Feature Engineering](https://www.kaggle.com/willkoehrsen/tuning-automated-feature-engineering-exploratory)
* [Feature Selection](https://www.kaggle.com/willkoehrsen/introduction-to-feature-selection)
* [Intro to Model Tuning: Grid and Random Search](https://www.kaggle.com/willkoehrsen/intro-to-model-tuning-grid-and-random-search)
* [Automated Model Tuning](https://www.kaggle.com/willkoehrsen/automated-model-tuning)

Thanks for reading and feel free to share any constructive criticism or feedback. 

Best,

Will",59,willkoehrsen,model-tuning-results-random-vs-bayesian-opt,willkoehrsen_model-tuning-results-random-vs-bayesian-opt_m59
"# Introduction: Tuning Automated Feature Engineering

In this notebook we will expand upon the [basic automated feature engineering](https://www.kaggle.com/willkoehrsen/applied-automated-feature-engineering-basics) applied to the Home Credit Default Risk competition. We will explore a few different methods for improving the set of features and incorporating domain knowledge into the final dataset. These methods include:

* Properly representing variable types
* Creating and using time variables
* Setting interesting values of variables
* Creating seed features
* Building custom primitives

Reading through the discussion around this competition and working through some of the top kernels, intricate feature engineering is a must. Using the default feature primitives in the basic notebook did improve our score, but to do better we will need some more advanced methods. 

This will be more as an exploration of the capabilities of featuretools than a complete implementation. I'm still working on figuring out the most useful features to build by reading through other kernels, finding features, and figuring out how to recreate and build upon those in featuretools. Any ideas would be much appreciated! 

This work draws heavily on the [featuretools documentation](https://docs.featuretools.com/) and the [featuretools GitHub repository](https://github.com/Featuretools/featuretools). ",0,willkoehrsen,tuning-automated-feature-engineering-exploratory,willkoehrsen_tuning-automated-feature-engineering-exploratory_m0
"### Read in Data and Create Smaller Datasets

We will limit the data to 1000 rows because automated feature engineering is computationally intensive work. Later we can refactor this code into functions and put it in a script to run on a more powerful machine. ",1,willkoehrsen,tuning-automated-feature-engineering-exploratory,willkoehrsen_tuning-automated-feature-engineering-exploratory_m1
"# Properly Representing Variable Types

There are a number of columns in the `app` dataframe that are represented as integers but are really discrete variables that can only take on a limited number of features. Some of these are Boolean flags (only 1 or 0) and two columns are ordinal (ordered discrete). To tell featuretools to treat these as Boolean variables, we need to pass in the correct datatype using a dictionary mapping {`variable_name`: `variable_type`}. ",2,willkoehrsen,tuning-automated-feature-engineering-exploratory,willkoehrsen_tuning-automated-feature-engineering-exploratory_m2
There are also two ordinal variables in the `app` data: the rating of the region with and without the city. ,3,willkoehrsen,tuning-automated-feature-engineering-exploratory,willkoehrsen_tuning-automated-feature-engineering-exploratory_m3
The previous data also has two Boolean variables. ,4,willkoehrsen,tuning-automated-feature-engineering-exploratory,willkoehrsen_tuning-automated-feature-engineering-exploratory_m4
"# Time Variables

Time can be a crucial factor in many datasets because behaviors change over time and therefore we want to make features to reflect this. For example, a client might be taking out larger and larger loans over time which could be an indicator that they are about to default or they could have a run of missed payments but then get back on track.

There are no explicit datetimes in the data, but there are relative time offsets. All the time offset are measured from the current application at Home Credit and are measured in months or days. For example, in `bureau`, the `DAYS_CREDIT` column represents ""How many days before current application did client apply for Credit Bureau credit"". (Credit Bureau refers to any other credit organization besides Home Credit). Although we do not know the actual application date, if we assume a starting application date that is the same for all clients, then we can convert the `MONTHS_BALANCE` into a datetime. This can then be treated as a relative time that we can use to find trends or identify the most recent value of a variable. ",5,willkoehrsen,tuning-automated-feature-engineering-exploratory,willkoehrsen_tuning-automated-feature-engineering-exploratory_m5
"### Replace Outliers

There are a number of day offsets that are recorded as 365243. Reading through discussions, others replaced this number with `np.nan`. If we don't do this, Pandas will not be able to convert into a timedelta and throws an error that the number is too large. The following code has been adapted from a script on [GitHub](https://github.com/JYLFamily/Home_Credit_Default_Risk/blob/master/20180603/FeaturesV2/ApplicationTestFeatures.py).",6,willkoehrsen,tuning-automated-feature-engineering-exploratory,willkoehrsen_tuning-automated-feature-engineering-exploratory_m6
First we can establish an arbitrary date and then convert the time offset in months into a Pandas `timedelta` object. ,7,willkoehrsen,tuning-automated-feature-engineering-exploratory,willkoehrsen_tuning-automated-feature-engineering-exploratory_m7
" These four columns represent different offsets:

* `DAYS_CREDIT`: Number of days before current application at Home Credit client applied for loan at other financial institution. We will call this the application date, `bureau_credit_application_date` and make it the `time_index` of the entity. 
* `DAYS_CREDIT_ENDDATE`: Number of days of credit remaining at time of client's application at Home Credit. We will call this the ending date, `bureau_credit_end_date`
* `DAYS_ENDDATE_FACT`: For closed credits, the number of days before current application at Home Credit that credit at other financial institution ended. We will call this the closing date, `bureau_credit_close_date`. 
* `DAYS_CREDIT_UPDATE`: Number of days before current application at Home Credit that the most recent information about the previous credit arrived. We will call this the update date, `bureau_credit_update_date`. 

If we were doing manual feature engineering, we might want to create new columns such as by subtracting `DAYS_CREDIT_ENDDATE` from `DAYS_CREDIT` to get the planned length of the loan in days, or subtracting `DAYS_CREDIT_ENDDATE` from `DAYS_ENDDATE_FACT` to find the number of days the client paid off the loan early. However, in this notebook we will not make any features by hand, but rather let featuretools develop useful features for us.

To make date columns from the `timedelta`, we simply add the offset to the start date. ",8,willkoehrsen,tuning-automated-feature-engineering-exploratory,willkoehrsen_tuning-automated-feature-engineering-exploratory_m8
"### Plot for a sanity check

To make sure the conversion went as planned, let's make a plot showing the distribution of loan lengths.",9,willkoehrsen,tuning-automated-feature-engineering-exploratory,willkoehrsen_tuning-automated-feature-engineering-exploratory_m9
"It looks as if there are a number of loans that are unreasonably long. Reading through the discussions, other people had noticed this as well. At this point, we will just leave in the outliers. We also will drop the time offset columns.",10,willkoehrsen,tuning-automated-feature-engineering-exploratory,willkoehrsen_tuning-automated-feature-engineering-exploratory_m10
"#### Bureau Balance

The bureau balance dataframe has a `MONTHS_BALANCE` column that we can use as a months offset. The resulting column of dates can be used as a `time_index`.",11,willkoehrsen,tuning-automated-feature-engineering-exploratory,willkoehrsen_tuning-automated-feature-engineering-exploratory_m11
"#### Previous Applications

The `previous` dataframe holds previous applications at Home Credit. There are a number of time offset columns in this dataset:

* `DAYS_DECISION`: number of days before current application at Home Credit that decision was made about previous application. This will be the `time_index` of the data.
* `DAYS_FIRST_DRAWING`: number of days before current application at Home Credit that first disbursement was made
* `DAYS_FIRST_DUE`: number of days before current application at Home Credit that first due was suppoed to be
* `DAYS_LAST_DUE_1ST_VERSION`: number of days before current application at Home Credit that first was??
* `DAYS_LAST_DUE`: number of days before current application at Home Credit of last due date of previous application
* `DAYS_TERMINATION`: number of days before current application at Home Credit of expected termination

Let's convert all these into timedeltas in a loop and then make time columns.",12,willkoehrsen,tuning-automated-feature-engineering-exploratory,willkoehrsen_tuning-automated-feature-engineering-exploratory_m12
"#### Previous Credit and Cash

The `credit_card_balance` and `POS_CASH_balance` each have a `MONTHS_BALANCE` column with the month offset. This is the number of months before the current application at Home Credit of the previous application record. These will represent the `time_index` of the data. ",13,willkoehrsen,tuning-automated-feature-engineering-exploratory,willkoehrsen_tuning-automated-feature-engineering-exploratory_m13
"#### Installments Payments 

The `installments_payments` data contains information on each payment made on the previous loans at Home Credit. It has two date offset columns:

* `DAYS_INSTALMENT`: number of days before current application at Home Credit that previous installment was supposed to be paid
* `DAYS_ENTRY_PAYMENT`: number of days before current application at Home Credit that previous installment was actually paid

By now the process should be familiar: convert to timedeltas and then make time columns. The DAYS_INSTALMENT will serve as the `time_index`. ",14,willkoehrsen,tuning-automated-feature-engineering-exploratory,willkoehrsen_tuning-automated-feature-engineering-exploratory_m14
"# Applying Featuretools

We can now start making features using the time columns. We will create an entityset named clients much as before, but now we have time variables that we can use. ",15,willkoehrsen,tuning-automated-feature-engineering-exploratory,willkoehrsen_tuning-automated-feature-engineering-exploratory_m15
"### Entities

When creating the entities, we specify the `index`, the `time_index` (if present), and the `variable_types` (if they need to be specified). ",16,willkoehrsen,tuning-automated-feature-engineering-exploratory,willkoehrsen_tuning-automated-feature-engineering-exploratory_m16
"### Relationships

Not surprisingly, the relationships between tables has not changed since the previous implementation. ",17,willkoehrsen,tuning-automated-feature-engineering-exploratory,willkoehrsen_tuning-automated-feature-engineering-exploratory_m17
"## Time Features

Let's look at some of the time features we can make from the new time variables. Because these times are relative and not absolute, we are only interested in values that show change over time, such as trend or cumulative sum. We would not want to calculate values like the year or month since we choose an arbitrary starting date. 

Throughout this notebook, we will pass in a `chunk_size` to the `dfs` call which specifies the number of rows (if an integer) or the fraction or rows to use in each chunk (if a float). This can help to optimize the `dfs` procedure, and the `chunk_size` can have a [significant effect on the run time](https://docs.featuretools.com/guides/performance.html). Here we will use a chunk size equal to the number of rows in the data so all the results will be calculated in one pass. We also want to avoid making any features with the testing data, so we pass in `ignore_entities = [app_test]`.",18,willkoehrsen,tuning-automated-feature-engineering-exploratory,willkoehrsen_tuning-automated-feature-engineering-exploratory_m18
Let's visualize one of these new variables. We can look at the trend in credit size over time. A positive value indicates that the loan size for the client is increasing over time. ,19,willkoehrsen,tuning-automated-feature-engineering-exploratory,willkoehrsen_tuning-automated-feature-engineering-exploratory_m19
"# Interesting Values

Another method we can use in featuretools is ""interesting values."" Specifying interesting values will calculate new features conditioned on values of existing features. For example, we can create new features that are conditioned on the value of `NAME_CONTRACT_STATUS` in the `previous` dataframe. Each stat will be calculated for the specified interesting values which can be useful when we know that there are certain indicators that are of greater importance in the data.  ",20,willkoehrsen,tuning-automated-feature-engineering-exploratory,willkoehrsen_tuning-automated-feature-engineering-exploratory_m20
"To use interesting values, we assign them to the variable and then specify the `where_primitives` in the `dfs` call. ",21,willkoehrsen,tuning-automated-feature-engineering-exploratory,willkoehrsen_tuning-automated-feature-engineering-exploratory_m21
"One of the features is `MEAN(previous.CNT_PAYMENT WHERE NAME_CONTRACT_STATUS = Approved)`. This shows the average ""term of previous credit"" on previous loans conditioned on the previous loan being approved. We can compare the distribution of this feature to the `MEAN(previous.CNT_PAYMENT WHERE NAME_CONTRACT_STATUS = Canceled)` to see how these loans differ.",22,willkoehrsen,tuning-automated-feature-engineering-exploratory,willkoehrsen_tuning-automated-feature-engineering-exploratory_m22
"Based on the most important features returned by a model, we can create new interesting features. This is one area where we can apply domain knowledge to feature creation.",23,willkoehrsen,tuning-automated-feature-engineering-exploratory,willkoehrsen_tuning-automated-feature-engineering-exploratory_m23
"# Seed Features

An additional extension to the default aggregations and transformations is to use [seed features](https://docs.featuretools.com/automated_feature_engineering/dfs_usage_tips.html#specifying-list-of-aggregation-functions). These are user defined features that we provide to deep feature synthesis that can then be built on top of where possible. 

As an example, we can create a seed feature that determines whether or not a payment was late. This time when we make the `dfs` function call, we need to pass in the `seed_features` argument.",24,willkoehrsen,tuning-automated-feature-engineering-exploratory,willkoehrsen_tuning-automated-feature-engineering-exploratory_m24
Another seed feature we can use is whether or not a previous loan at another institution was past due. ,25,willkoehrsen,tuning-automated-feature-engineering-exploratory,willkoehrsen_tuning-automated-feature-engineering-exploratory_m25
"# Create Custom Feature Primitives

If we are not satisfied with the existing primitives in featuretools, we [can write our own](https://docs.featuretools.com/automated_feature_engineering/primitives.html#defining-custom-primitives). This is an extremely powerful method that lets us expand the capabilities of featuretools. 

### NormalizedModeCount and LongestSeq

As an example, we will make three features, building on code from the [featuretools GitHub](https://github.com/Featuretools/featuretools). These will be aggregation primitives, where the function takes in an array of values and returns a single value.  The first, `NormalizedModeCount`, builds upon the `Mode` function by returning the fraction of total observations in a categorical feature that the model makes up. In other words, for a client with 5 total `bureau_balance` observations where 4 of the `STATUS` were `X`, the value of the `NormalizedModeCount` would be 0.8. The idea is to record not only the most common value, but also the relative frequency of the most common value compared to all observations.  

The second custom feature will record the longest consecutive run of a discrete variable. `LongestSeq` takes in an array of discrete values and returns the element that appears the most consecutive times. Because entities in the entityset are sorted by the `time_index`, this will return the value that occurs the most number of times in a row with respect to time. 



",26,willkoehrsen,tuning-automated-feature-engineering-exploratory,willkoehrsen_tuning-automated-feature-engineering-exploratory_m26
"These features could be completely useless, or they may be helpful. Only building a model and training it with the features will help us determine the answer. 

### MostRecent

The final custom feature will be `MOSTRECENT`. This simply returns the most recent value of a discrete variable with respect to time columns in a dataframe. When we create an entity, featuretools will [sort the entity](https://github.com/Featuretools/featuretools/blob/master/featuretools/entityset/entity.py) by the `time_index`. Therefore, the built-in aggregation primitive `LAST` calculates the most recent value based on the time index. However, in cases where there are multiple different time columns, it might be useful to know the most recent value with respect to all of the times. To build the custom feature primitive, I adapted the existing `TREND` primitive ([code here](https://github.com/Featuretools/featuretools/blob/master/featuretools/primitives/aggregation_primitives.py)). ",27,willkoehrsen,tuning-automated-feature-engineering-exploratory,willkoehrsen_tuning-automated-feature-engineering-exploratory_m27
"To test whether this function works as intended, we can compare the most recent variable of `CREDIT_TYPE` ordered by two different dates. ",28,willkoehrsen,tuning-automated-feature-engineering-exploratory,willkoehrsen_tuning-automated-feature-engineering-exploratory_m28
"For client 100002, the most recent type of credit was `Credit card` if we order by the application date, but `Consumer credit` if we order by the end date of the loan. Whether this is actually useful knowledge is hard to say! 

",29,willkoehrsen,tuning-automated-feature-engineering-exploratory,willkoehrsen_tuning-automated-feature-engineering-exploratory_m29
"# Putting it all Together

Finally, we can run deep feature synthesis with the time variables, with the correct specified categorical variables, with the interesting features, with the seed features, and with the custom features. To actually run this on the entire dataset, we can take the code here, put it in a script, and then use more computational resources. ",30,willkoehrsen,tuning-automated-feature-engineering-exploratory,willkoehrsen_tuning-automated-feature-engineering-exploratory_m30
We will now do the same operation applied to the test set. Doing the calculations separately should prevent leakage from the testing data into the training data.,31,willkoehrsen,tuning-automated-feature-engineering-exploratory,willkoehrsen_tuning-automated-feature-engineering-exploratory_m31
"## Remove Features

[Feature selection](https://en.wikipedia.org/wiki/Feature_selection) is an entire topic to itself. However, one thing we can do is use the built-in featuretools [selection function to remove](https://docs.featuretools.com/generated/featuretools.selection.remove_low_information_features.html) columns that only have one unique value or have all null values. ",32,willkoehrsen,tuning-automated-feature-engineering-exploratory,willkoehrsen_tuning-automated-feature-engineering-exploratory_m32
"When we're done, we probably want to save the results to a csv. We want to be careful because the index of the dataframe is the identifying column, so we should keep the index. We also should align the training and testing dataframes to make sure they have the same columns.",33,willkoehrsen,tuning-automated-feature-engineering-exploratory,willkoehrsen_tuning-automated-feature-engineering-exploratory_m33
"# Conclusions 

In this notebook we explored some of the advanced functionality in featuretools including:

* Time Variables: allow us to track trends over time 
* Interesting Variables: condition new features on values of existing features
* Seed Features: define new features manually that featuretools will then build on top of
* Custom feature primitives: design any transformation or aggregation feature that can incorporate domain knowledge

We can use these methods to encode domain knowledge about a problem into our features or create features based on what others have found useful. The next step from here would be to run the script on the entire dataset, then use the features for modeling. We could use the feature importances from the model to determine the most relevant features, perform feature selection, and then go through another round of feature synthesis with a new set of of primitives, seed features, and interesting features. As with many aspects of machine learning, feature creation is largely an empirical and iterative procedure. ",34,willkoehrsen,tuning-automated-feature-engineering-exploratory,willkoehrsen_tuning-automated-feature-engineering-exploratory_m34
"# Home Credit Default Risk: Random Forest & K-Fold Cross Validation
This notebook shows a simple random forest approach to the Home Credit Default Risk problem. A K-Fold cross validation is used to avoid overfitting.",0,ynouri,random-forest-k-fold-cross-validation,ynouri_random-forest-k-fold-cross-validation_m0
"## Loans data model

It's good to keep in mind Home Credit loans data model to know how to join the different tables.",1,ynouri,random-forest-k-fold-cross-validation,ynouri_random-forest-k-fold-cross-validation_m1
![Home Credit loans data model](https://storage.googleapis.com/kaggle-media/competitions/home-credit/home_credit.png),2,ynouri,random-forest-k-fold-cross-validation,ynouri_random-forest-k-fold-cross-validation_m2
## Read data,3,ynouri,random-forest-k-fold-cross-validation,ynouri_random-forest-k-fold-cross-validation_m3
"
## Features selection

The feature selection has been done using the most important features resulting of Will Koehrsen [automated features generation](https://www.kaggle.com/willkoehrsen/applied-automated-feature-engineering-basics/notebook). Thanks to him for his awesome notebook and introduction to [Featuretools](https://www.featuretools.com/) and [Deep Feature Synthesis](http://www.jmaxkanter.com/static/papers/DSAA_DSM_2015.pdf) method.

First, let's aggregate the features coming from the credit bureau file in the main loans table.",4,ynouri,random-forest-k-fold-cross-validation,ynouri_random-forest-k-fold-cross-validation_m4
"Next, let's generate the features coming from the installments data. To do so:
* We compute first the minimum of installment payments in the previous applications table. The rational is to catch the loans where one payment has been missed or was very low.
* Then we merge the aggregates of all minimum installment payments across all previous loans.",5,ynouri,random-forest-k-fold-cross-validation,ynouri_random-forest-k-fold-cross-validation_m5
"Finally, let's add all the base features from the main loan table which don't need aggregation. We fill the missing values with their average.",6,ynouri,random-forest-k-fold-cross-validation,ynouri_random-forest-k-fold-cross-validation_m6
## Random Forest model,7,ynouri,random-forest-k-fold-cross-validation,ynouri_random-forest-k-fold-cross-validation_m7
## K-Fold cross validation,8,ynouri,random-forest-k-fold-cross-validation,ynouri_random-forest-k-fold-cross-validation_m8
"Contents
- <a href='#1'>1. Read dataset</a>
    - <a href='#1_1'>1.1. Read dataset</a>
    - <a href='#1_2'>1.2. Check null data</a>
    - <a href='#1_2'>1.3. Make meta dataframe</a>
- <a href='#2'>2. EDA - application train</a>
    - <a href='#2_1'>2.1. Object feature</a>
        - <a href='#2_1_1'>2.1.1 Contract type</a>
        - <a href='#2_1_2'>2.1.2. Gender</a>
        - <a href='#2_1_3'>2.1.3. Do you have an own car?</a>
        - <a href='#2_1_4'>2.1.4. Do you have own realty?</a>
        - <a href='#2_1_5'>2.1.5. Suite type</a>
        - <a href='#2_1_6'>2.1.6. Income type</a>
        - <a href='#2_1_7'>2.1.7 Contract type </a>
        - <a href='#2_1_8'>2.1.8. 2.8 Family status</a>
        - <a href='#2_1_9'>2.1.9. Housing type</a>
        - <a href='#2_1_10'>2.1.10. Occupation type</a>
        - <a href='#2_1_11'>2.1.11. Process start (weekday)</a>
        - <a href='#2_1_12'>2.1.12. Organization type</a>
        - <a href='#2_1_13'>2.1.13. FONDKAPREMONT </a>
        - <a href='#2_1_14'>2.1.14. House type</a>
        - <a href='#2_1_15'>2.1.15. Wall material</a>
        - <a href='#2_1_16'>2.1.16. Emergency</a>
    - <a href='#2_2'>2.2. Int feature</a>
        - <a href='#2_2_1'>2.2.1 Count of children</a>
        - <a href='#2_2_2'>2.2.2. Mobil</a>
        - <a href='#2_2_3'>2.2.3. EMP Phone</a>
        - <a href='#2_2_4'>2.2.4. Work phone</a>
        - <a href='#2_2_5'>2.2.5. Cont mobile</a>
        - <a href='#2_2_6'>2.2.6. Phone</a>
        - <a href='#2_2_7'>2.2.7 Region Rating Client</a>
        - <a href='#2_2_8'>2.2.8. Region Rating Client With City</a>
        - <a href='#2_2_9'>2.2.9. Hour Appr Process Start</a>
        - <a href='#2_2_10'>2.2.10. Register region and not live region</a>
        - <a href='#2_2_11'>2.2.11. Register region and not work region</a>
        - <a href='#2_2_12'>2.2.12. Live region and not work region</a>
        - <a href='#2_2_13'>2.2.13. Register city and not live city</a>
        - <a href='#2_2_14'>2.2.14. Register city and not work city</a>
        - <a href='#2_2_15'>2.2.15. Live city and not work city</a>
        - <a href='#2_2_16'>2.2.16. Heatmap for int features</a>
        - <a href='#2_2_17'>2.2.17. More analysis for int features which have correlation with target</a>
        - <a href='#2_2_18'>2.2.18. linear regression analysis on the high correlated feature combinations</a> 
- <a href='#3'>3. EDA - Bureau</a>
    - <a href='#3_1'>3.1. Read and check data</a>
    - <a href='#3_2'>3.2. Merge with application_train</a>
    - <a href='#3_3'>3.3. Analysis on object feature</a>
        - <a href='#3_3_1'>3.3.1. Credit active</a>
        - <a href='#3_3_2'>3.3.2. Credit currency</a>
        - <a href='#3_3_3'>3.3.3. Credit type</a>
    - <a href='#3_4'>3.4. Analysis on int feature</a>
        - <a href='#3_4_1'>3.4.1. Credit day</a>
        - <a href='#3_4_2'>3.4.2. Credit day overdue</a>
        - <a href='#3_4_3'>3.4.3. Credit day prolong</a>
    - <a href='#3_5'>3.5. Analysis on float feature</a>
        - <a href='#3_5_1'>3.5.1 Amount credit sum</a>
        - <a href='#3_5_2'>3.5.2 Amount credit sum debt</a>
        - <a href='#3_5_3'>3.5.3 Amount credit sum limit</a>
        - <a href='#3_5_4'>3.5.4 Amount credit sum overdue</a>",0,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m0
# <a id='1'>1. Read dataset</a>,1,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m1
## <a id='1_1'>1.1. Read dataset</a>,2,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m2
## <a id='1_2'>1.2. Check null data</a>,3,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m3
"- With msno library, we could see the blanks in the dataset. Check null data in application train.",4,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m4
## <a id='1_3'>1.3. Make meta dataframe</a>,5,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m5
"- There are 3 data types(float64, int64, object) in application_train dataframe.",6,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m6
"- Before starting EDA, It would be useful to make meta dataframe which include the information of dtype, level, response rate and role of each features. ",7,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m7
## <a id='1_4'>1.4. Check imbalance of target</a>,8,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m8
"- Checking the imbalance of dataset is important. If imbalanced, we need to select more technical strategy to make a model.",9,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m9
"- As you can see, target is imbalanced.
- This fact makes this competition diffcult to solve. But, no pain, no gain. After this competition, we could learn many things! Enjoy!",10,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m10
# <a id='2'>2. EDA - application_train </a>,11,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m11
## <a id='2_1'>2.1. Object feature</a>,12,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m12
- I want to draw two count bar plot for each object and int features. One contain the each count of responses and other contain the percent on target.,13,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m13
"- Sometimes, null data itself can be important feature. So, I want to compare the change when using null data as feature and not using nulll data as feature.",14,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m14
### <a id='2_1_1'>2.1.1. Contract type</a>,15,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m15
**REMIND:  repay == 0 and not repay == 1**,16,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m16
"- Most contract type of clients is Cash loans. 
- Not repayment rate is higher in cash loans (~8%) than in revolving loans(~5%).",17,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m17
### <a id='2_1_2'>2.1.2. Gender</a>,18,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m18
"- The number of female clients is almoist double the number of male clients.
- Males have a higher chance of not returning their loans (~10%), comparing with women(~7%).",19,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m19
### <a id='2_1_3'>2.1.3. Do you have an own car?</a>,20,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m20
"- The clients that owns a car are higher than no-car clients by a factor of two times. 
- The Not-repayment percent is similar. (Own: ~7%, Not-own: ~8%)",21,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m21
### <a id='2_1_4'>2.1.4. Do you have own realty?</a>,22,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m22
"- T he clients that owns a realty almost a half of the ones that doesn't own realty. 
- Both categories have not-repayment rate, about ~8%.",23,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m23
### <a id='2_1_5'>2.1.5. Suite type</a>,24,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m24
"- Most suite type of clients are 'Unaccompanied', followed by Family, Spouse, children.
- When considering null data, there is no change the order.
- Other_B and Other_A have higher not-repayment rate than others.",25,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m25
### <a id='2_1_6'>2.1.6. Income type</a>,26,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m26
"- Most of the clients get income from working. 
- The number of Student, Unemployed, Bussnessman and Maternity leave are very few.
- When unemployed and maternity leave, there is  high probability of not-repayment.",27,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m27
### <a id='2_1_7'>2.1.7. Education type</a>,28,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m28
"- Clients with secondary education type are most numerous, followed by higher education, incomplete higher.
- Clients with Lower secondary have the highest not-repayment rate(~10%).",29,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m29
### <a id='2_1_8'>2.1.8. Family status</a>,30,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m30
"- Most of clients  for loans are married followed by single/not married, civial marriage.
- Civil marriage have almost 10% ratio of not returning loans followed by single/notmarried(9.9%), separate(8%).",31,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m31
### <a id='2_1_9'>2.1.9. Housing type</a>,32,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m32
"- Clients with house/apartment are most numerous, followed by With parents, Municipal apartment.
- When Rented apartment and live with parents, clients have somewhat high not-repayment ratio. (~12%)
",33,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m33
### <a id='2_1_10'>2.1.10. Occupation type</a>,34,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m34
"- When not considering null data, Majority of clients are laborers, sales staff, core staff, drivers. But with considering null data, null data(I think it would be 'not want to repond' or 'no job', 'not in category') are most numerous.
- However, not-repayment rate is low for null data. Low-skill labor is the most high not-repayment rate (~17%) in both plot.",35,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m35
### <a id='2_1_11'>2.1.11. Process start (weekday)</a>,36,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m36
"- The number of process for weekend is less than other days. That's because Weekend is weekend.
- There are no big changes between not-repayment rate of all days.
- Day is not important factor for repayment.",37,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m37
### <a id='2_1_12'>2.1.12. Organization type</a>,38,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m38
"- The most frequent case of organization is Bussiness Entity Type 3 followed XNA and self-employ.
- The Transport: type 3 has the highest not repayment rate(~16%), Industry: type 13(~13.5%).",39,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m39
### <a id='2_1_13'>2.1.13. FONDKAPREMONT</a>,40,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m40
"- Actually, I don't know exact meaning of this feature FONDKAPREMONT_MODE.
- Anyway, when considering null data, nul data has the highest count and not-repayment rate.",41,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m41
### <a id='2_1_14'>2.1.14. House type</a>,42,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m42
"- When considering null data, null data and block of flats are two-top. 
- But, specific housing and terraced house have higher not-repayment rate than block of flats. 
- null data has the highest not-repayment rate(~9%).",43,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m43
### <a id='2_1_15'>2.1.15. Wall material</a>,44,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m44
"- There are over 150,000 null data for WALLSMATERIAL_MODE. 
- Clients with Wooden have higher than 9% not repayment rate.",45,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m45
### <a id='2_1_16'>2.1.16. Emergency</a>,46,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m46
"- For emergency state, there is also many null data. 
- If clients is in an emergency state, not-repayment rate(~10%) is higher than not in an emergency state.
- null is also high not-repayment rate(~-10%).",47,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m47
## <a id='2_2'>2.2. Int feature</a>,48,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m48
- Let's do similar analysis for int features.,49,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m49
### <a id='2_2_1'>2.2.1. Count of children</a>,50,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m50
"- Most clients with no children requested loan. 
- Clients with 9, 11 have 100% not-repayment rate. the each count of those cases is 2 and 1.
- Except 9, 11, Clients with 6 children has high not-repayment rate.",51,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m51
### <a id='2_2_2'>2.2.2. Mobil</a>,52,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m52
- There are no clients without mobil(maybe mobile).,53,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m53
### <a id='2_2_3'>2.2.3. EMP Phone</a>,54,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m54
"- Most clients(82%) have EPM Phone.
- The gap between the not-repayment percent is about 3%.",55,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m55
### <a id='2_2_4'>2.2.3. Work Phone</a>,56,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m56
- Most clients(80%) don't have work phone.,57,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m57
### <a id='2_2_5'>2.2.5. Cont mobile</a>,58,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m58
- Clients who chose 'no' for CONT_MOBILE FALG is very few.(574),59,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m59
### <a id='2_2_6'>2.2.6. Phone</a>,60,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m60
- Most clients(72%) don't have work phone.,61,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m61
### <a id='2_2_7'>2.2.7. Region Rating Client</a>,62,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m62
"- Clients who chose 2 for REGION_RATING_CLIENT is numerous, followed by 3, 1.
- For not-repayment, the order is 3, 2, 1.",63,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m63
### <a id='2_2_8'>2.2.8. Region Rating Client With City</a>,64,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m64
"- Clients who chose 2 for REGION_RATING_CLIENT with city is numerous, followed by 3, 1.
- For not-repayment, the order is 3, 2, 1.",65,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m65
### <a id='2_2_9'>2.2.9. Hour Appr Process Start</a>,66,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m66
- The most busy hour for Appr Process Start is a range from 10:00 to 13:00.,67,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m67
### <a id='2_2_10'>2.2.10. Register region and not live region</a>,68,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m68
- 98.5% of clients registered their region but not live in the region.,69,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m69
### <a id='2_2_11'>2.2.11. Register region and not work region</a>,70,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m70
- 95% of clients registered their region but not work in the region.,71,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m71
### <a id='2_2_12'>2.2.12. Live region and not work region</a>,72,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m72
- 95.9% of clients lives in their region but don't work in the region.,73,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m73
"- For 3 questions about region(10, 11, 12), the not-repayment percent is similar for each case.",74,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m74
### <a id='2_2_13'>2.2.13. Register city and not live city</a>,75,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m75
"- 92.1% of clients registered city and don't live in the city.
- Unlike region, city could be good information. Because the difference of the not-repayment percent between 'yes' and 'no' is higher than region case(2.2.10, 2.2.11, 2.2.12)",76,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m76
### <a id='2_2_14'>2.2.14. Register city and not work city</a>,77,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m77
"- 78% of clients registered city and don't work in the city.
- If client is this case, the not-repayment rate is about 10%.",78,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m78
### <a id='2_2_15'>2.2.15. Live city and not work city</a>,79,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m79
"- 82% of clients registered city and don't work in the city.
- If client is this case, the not-repayment rate is about 10%.",80,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m80
### <a id='2_2_16'>2.2.16. Flag document</a>,81,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m81
"- Document 2: 13 clients chose 1 and not-repayment rate is high, about 30%.
- Document 4: 25 clients chose 1 and not-repayment rate is 0. all the clients who chose 1 repaid.
- Document 10: 7 clients chose 1 and not-repayment rate is 0. all the clients who chose 1 repaid.
- Document 12: 2 clients chose 1 and not-repayment rate is 0. all the clients who chose 1 repaid.",82,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m82
### <a id='2_2_16'>2.2.16. Heatmap for int features</a>,83,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m83
- Let's see the correlations between the int features. Heatmap helps us to see this easily.,84,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m84
"- There are some combinations with high correlation coefficient.
- FLAG_DOCUMENT_6 and FLAG_EMP_PHONE
- DAYS_BIRTH and FLAG_EMP_PHONE
- DAYS_EMPLOYED and FLAG_EMP_PHONE
- In follow section, we will look those features more deeply using linear regression plot with seaborn.",85,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m85
### <a id='2_2_17'>2.2.17. More analysis for int features which have correlation with target</a>,86,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m86
"- At first, find the int features which have high correlation with target.",87,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m87
"- DAYS_BIRTH is some high correlation with target.
- With dividing 365(year) and applying abs(), we can see DAYS_BIRTH in the unit of year(AGE).",88,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m88
"- As you can see, The younger, The higher not-repayment probability.
- The older, The lower not-repayment probability.",89,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m89
### <a id='2_2_18'>2.2.18. linear regression analysis on the high correlated feature combinations</a>,90,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m90
"- With lmplot from seaborn, we can draw linear regression plot very easily. Thanks!",91,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m91
"- With gaussian kde density represented by color and linear regression plot, we can see that there are many clients who have EMP Phone and chose document 6.",92,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m92
"- With gaussian kde density represented by color and linear regression plot, we can see that the younger people tend to have EMP phone.",93,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m93
"- With gaussian kde density represented by color and linear regression plot, we can see that clients with shorter employed tend to have EMP phone. (simiar result compared to FLAG_EMP_PHONE vs DAYS_BIRTH)",94,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m94
## <a id='2_3'>2.3. float feature</a>,95,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m95
- Let's move on float features.,96,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m96
### <a id='2_3_1'>2.3.1. Heatmap for float features</a>,97,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m97
- Let us draw the heatmap of float features.,98,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m98
"- There are some features which have  some high correlation with target. In follow section, we will find them and analyze them.
- There are many feature combinations which have high correlation value(larger than 0.9).
- Let's find the combinations.",99,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m99
### <a id='2_3_2'>2.3.2. More analysis for int features which have correlation with target</a>,100,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m100
- Let's find the float features which are highly correlated with target.,101,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m101
"- The simple kde plot(kernel density estimation plot) shows that the distribution of repay and not-repay is different for EXT_SOURCE_1.
- EXT_SOURCE_1 can be good feature.",102,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m102
"- Not as much as EXT_SOURCE_1 do, EXT_SOURCE_2 shows different distribution for each repay and not-repay.",103,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m103
"- EXX_SOUCE_3 has similar trend with EXT_SOURCE_1.
- EXT_SOURCE_3 can be good feature.",104,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m104
### <a id='2_3_3'>2.3.3. linear regression analysis on the high correlated feature combinations</a>,105,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m105
"- Using corr() and numpy boolean technique with triu(), we could obtain the correlation matrix without replicates.",106,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m106
"- There are 60 combinations which have larger correlation values than 0.95.
- Let's draw the regplot for all combinations with splitting the target.",107,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m107
"- After looking these 56 plots, I found som combinations in which the distribution for repay and not-repay is a bit different.
- Let's see this with single and multi variable kde plot.
- It is nice to use log-operation on features. With log-operation, we can analyze the distribution more easily.",108,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m108
"- The mutivariate kde plot of not-repay is broader than one of repay.
- For both CNT_60_SOCIAL_CIRCLE and OBS_30_CNT_SOCIAL_CIRCLE, the distribution of each repay and not-repay is a bit different. Log-operation helps us to see them easily.",109,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m109
"- This case is similar with previous case.
- The mutivariate kde plot of not-repay is broader than one of repay.
- For both CNT_60_SOCIAL_CIRCLE and OBS_30_CNT_SOCIAL_CIRCLE, the distribution of each repay and not-repay is a bit different. Log-operation helps us to see them easily.",110,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m110
# <a id='3'>3. EDA - Bureau </a>,111,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m111
- Bureau data contains the information of previous loan history of clients from other company.,112,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m112
## <a id='3_1'>3.1. Read and check data</a>,113,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m113
## <a id='3_2'>3.2. Merge with application_train</a>,114,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m114
- A client can have several loans so that merge with bureau data can explode the row of application train.,115,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m115
## <a id='3_3'>3.3. Analysis on object feature</a>,116,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m116
### <a id='3_3_1'>3.3.1 Credit active</a>,117,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m117
"- Most credit type of clients is 'Closed', 'Active'. 
- If credit type is finished in the state of bad dept, the not-repayment rate is some high.(20%)",118,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m118
### <a id='3_3_2'>3.3.2 Credit currency</a>,119,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m119
"- 99.9% of clients chose currency 1.
- By the way, the not-repayment rate is high at currency 3.",120,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m120
### <a id='3_3_3'>3.3.3 Credit type</a>,121,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m121
"- Clients with consumer credit is most numerous, followed by credit card.
- If clients requested loan for the purchase of equipment, the not-repayment rate is high.(23.5%). Next is microloan(20.6%).",122,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m122
## <a id='3_4'>3.4. Analysis on int feature</a>,123,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m123
### <a id='3_4_1'>3.4.1 Credit day</a>,124,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m124
"- There are 2 general(not linear) trends we can see.
- The shorter credit days, the more not-repayment.
- The larger credit days, the more repayment.",125,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m125
### <a id='3_4_2'>3.4.2 Credit day overdue</a>,126,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m126
- It is hard to see the trend for now. Let's remove the samples. (overdue < 200),127,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m127
"- As you can see, repay have a litter more right-skewed distribution.
- To see more deeply, Let's divide the overdue feature into several groups. ",128,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m128
"- The clients with short overdue days(<30) is most numerous.
- B group has the highest not-repayment rate (19%), followed by C, D, E. A group is the lowest.",129,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m129
- KDE plot with samples which have overdue larger than 30 shows that the distribution of clients who repaid is larger than that of not-repay clients.,130,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m130
### <a id='3_4_3'>3.4.3 Credit day prolong</a>,131,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m131
- There are no clients who have prolong larger than 3.,132,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m132
## <a id='3_5'>3.5. Analysis on float feature</a>,133,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m133
### <a id='3_5_1'>3.5.1 Amount credit sum</a>,134,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m134
"- As you can see, if credit is lower than 2,000,000, the distribution of each repay and not-repay is similar.
- But, if credit is larger than 2,000,000, the distribution of each repay and not-repay is different. Many clients who have very high(> 10,000,000) credit repaid.",135,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m135
### <a id='3_5_2'>3.5.2 Amount credit sum debt</a>,136,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m136
"- AMT_CREDIT_SUM_DEBT shows similar trend compared to AMT_CREDIT_SUM.
- Many clients with high dept(> 50,000,000) repaid.",137,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m137
### <a id='3_5_3'>3.5.3 Amount credit sum limit</a>,138,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m138
"- In rough way, the repay clients have high CREDIT_SUM_LIMIT.
- Is it possible to have minus credit sum limit??",139,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m139
### <a id='3_5_4'>3.5.4 Amount credit sum overdue</a>,140,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m140
"- Likewise, there are many repay-clients who have large credit sum overdue.",141,youhanlee,extensive-eda-for-application-and-bureau-data,youhanlee_extensive-eda-for-application-and-bureau-data_m141
"**Want to find out how to do the feature enigeering step automatically in a concise and compact tutorial, applying it on a binary classification problem using lightGBM?---look no further **

After finding out that data scientists created a tool that ""replaces"" data-scientists I had to try it out. Thank you [https://docs.featuretools.com/](http://)! Feature-engineering is tiresome, and takes the biggest amount of time do it. What if we can make it a one liner. Well now it seems we can. Even more appropriately we will be working on Home Credit Default Risk. A set of datasets where all of them are in a relationship with one-another and from all of them some information should be extracted. Featuretools makes it easy! Our goal in the end is simple. Predict whether the customer will default or not.",0,zikazika,lightgbm-automated-feature-engineering-easy,zikazika_lightgbm-automated-feature-engineering-easy_m0
"Load in the data, NOTE: datasets are huge, working on them will be computationally costly. In order to avoid it we can introduce some limited sample size.",1,zikazika,lightgbm-automated-feature-engineering-easy,zikazika_lightgbm-automated-feature-engineering-easy_m1
If we merge datasets now we can perofrm neccesary operations and seperate them later.,2,zikazika,lightgbm-automated-feature-engineering-easy,zikazika_lightgbm-automated-feature-engineering-easy_m2
**NOTE** This NaN handling is just for the sake of it. It is by no-means complete and there are lot of them underneath (function is built that shows us percentage). But there is a specific way that GBM (light and xBGM) handle missing values. So even tough it would be better we want to focus on algortihm and automatic feature engineering!,3,zikazika,lightgbm-automated-feature-engineering-easy,zikazika_lightgbm-automated-feature-engineering-easy_m3
"**NOTE** Even tough it is automatic, we can incorporate some manual features. IF we know some domain specific information.",4,zikazika,lightgbm-automated-feature-engineering-easy,zikazika_lightgbm-automated-feature-engineering-easy_m4
"***Featuretools*** is an open-source Python library for automatically creating features out of a set of related tables using a technique called deep feature synthesis. Automated feature engineering, like many topics in machine learning, is a complex subject built upon a foundation of simpler ideas. By going through these ideas one at a time, we can build up our understanding of how featuretools which will later allow for us to get the most out of it.

There are a few concepts that we will cover along the way:

1.  Entities and EntitySets
2. Relationships between tables
3. Feature primitives: aggregations and transformations
4. Deep feature synthesis",5,zikazika,lightgbm-automated-feature-engineering-easy,zikazika_lightgbm-automated-feature-engineering-easy_m5
**2. Relationships betweeen the sets**,6,zikazika,lightgbm-automated-feature-engineering-easy,zikazika_lightgbm-automated-feature-engineering-easy_m6
**Feature primitives** Basically which functions are we going to use to create features. Since we did not specify it we will be using standard ones (check doc) There is a option to define own ones or to just select some of the standards.,7,zikazika,lightgbm-automated-feature-engineering-easy,zikazika_lightgbm-automated-feature-engineering-easy_m7
**Label encoding** Making it machine readable,8,zikazika,lightgbm-automated-feature-engineering-easy,zikazika_lightgbm-automated-feature-engineering-easy_m8
**NaN imputation** will be skipped in this tutorial.,9,zikazika,lightgbm-automated-feature-engineering-easy,zikazika_lightgbm-automated-feature-engineering-easy_m9
Let us split the variables one more time.,10,zikazika,lightgbm-automated-feature-engineering-easy,zikazika_lightgbm-automated-feature-engineering-easy_m10
"**Train** the model, predict, etc.",11,zikazika,lightgbm-automated-feature-engineering-easy,zikazika_lightgbm-automated-feature-engineering-easy_m11
