ORIGINAL CODE:
---------------------------------------------------------------------------------------------------
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
%matplotlib inline
================================================================
RECOMMENDED CODE:
----------------------------------------------------
[]
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

ORIGINAL CODE:
---------------------------------------------------------------------------------------------------
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from sklearn.ensemble import RandomForestRegressor
%matplotlib inline
import seaborn as sns
================================================================
RECOMMENDED CODE:
----------------------------------------------------
[]
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

ORIGINAL CODE:
---------------------------------------------------------------------------------------------------
import pandas as pd
import numpy as np
from scipy.optimize import curve_fit

%matplotlib inline
%config InlineBackend.figure_format = 'retina'
import matplotlib.pyplot as plt

from sklearn.linear_model import LinearRegression
from sklearn.metrics import log_loss
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.ensemble import GradientBoostingRegressor
from lightgbm import LGBMRegressor


import xgboost as xgb

from tqdm.notebook import tqdm

def exponential(x, a, k, b):
    return a*np.exp(x*k) + b

def funclog(x, a, b,c):
    return a*np.log(b+x)+c

def rmse( yt, yp ):
    return np.sqrt( np.mean( (yt-yp)**2 ) )

def pinball(y_true, y_pred, tao=0.5 ):
    return np.max( [(y_true - y_pred)*tao, (y_pred - y_true)*(1 - tao) ], axis=0 ) 

def calc_metric( df ):
    tmp = df.copy()
    tmp['m0'] = pinball( tmp['TargetValue'].values, tmp['q05'].values , 0.05 )
    tmp['m1'] = pinball( tmp['TargetValue'].values, tmp['q50'].values , 0.50 )
    tmp['m2'] = pinball( tmp['TargetValue'].values, tmp['q95'].values , 0.95 )
    tmp['q'] = tmp['Weight']*(tmp['m0']+tmp['m1']+tmp['m2']) / 3
    return tmp['q'].mean()
train = pd.read_csv('../input/covid19-global-forecasting-week-5/train.csv')

train['Date'] = pd.to_datetime( train['Date'] )
mindate  = str(train['Date'].min())[:10]
maxdate  = str(train['Date'].max())[:10]
testdate = str( train['Date'].max() + pd.Timedelta(days=1) )[:10]
print( mindate, maxdate, testdate )

train['County'] = train['County'].fillna('N')
train['Province_State'] = train['Province_State'].fillna('N')
train['Country_Region'] = train['Country_Region'].fillna('N')
train['geo'] = train['Country_Region'] + '-' + train['Province_State'] + '-' + train['County']

print(train.shape)
train['dedup'] = pd.factorize( train['geo'] + '-' + train['Target'] + '-' + train['Date'].apply(str) + '-' + train['Population'].apply(str) )[0]
train.drop_duplicates(subset ="dedup", keep = 'first', inplace = True)
del train['dedup']
print(train.shape)

train.sort_values( ['geo','Date'], inplace=True )

train.head(5)
test = pd.read_csv('../input/covid19-global-forecasting-week-5/test.csv')

test['Date'] = pd.to_datetime( test['Date'] )
#testdate = str( test['Date'].max() + pd.Timedelta(days=1) )[:10]
#print( maxdate, testdate )

test['County'] = test['County'].fillna('N')
test['Province_State'] = test['Province_State'].fillna('N')
test['Country_Region'] = test['Country_Region'].fillna('N')
test['geo'] = test['Country_Region'] + '-' + test['Province_State'] + '-' + test['County']

print(test.shape)
test.sort_values( ['geo','Date'], inplace=True )

print(test.head())
train = pd.concat( (train, test.loc[test.Date>=testdate]) , sort=False )
train.sort_values( ['geo','Date'], inplace=True )

train.loc[ (train.Date=='2020-04-24')&(train.geo=='Spain-N-N')&(train.Target=='ConfirmedCases'), 'TargetValue' ] = 6000
train.loc[ (train.Date=='2020-04-29')&(train.geo=='France-N-N')&(train.Target=='ConfirmedCases'), 'TargetValue' ] = 1843.5
train.loc[ (train.Date=='2020-04-22')&(train.geo=='France-N-N')&(train.Target=='ConfirmedCases'), 'TargetValue' ] = 2522
train0 = train.loc[ train.Target == 'ConfirmedCases' ].copy()
train1 = train.loc[ train.Target == 'Fatalities' ].copy()
for t in [train0,train1]:
    t['q05'] = 0
    t['q50'] = 0
    t['q95'] = 0

train0 = train0.loc[ (train0.Date  >='2020-03-01') ].copy()
train1 = train1.loc[ (train1.Date  >='2020-03-01') ].copy()
    
test0 = test.loc[ test.Target == 'ConfirmedCases' ].copy()
test1 = test.loc[ test.Target == 'Fatalities' ].copy()
train0.shape, test0.shape
!ls -l ../input/covid-w5-worldometer-scraper/train_oldformat.csv
DF = pd.read_csv('../input/covid-w5-worldometer-scraper/train_oldformat.csv')
DF['Date'] = pd.to_datetime( DF['Date'] )
#testdate = str( test['Date'].max() + pd.Timedelta(days=1) )[:10]
#print( maxdate, testdate )

DF['County'] = DF['County'].fillna('N')
DF['Province_State'] = DF['Province_State'].fillna('N')
DF['Country_Region'] = DF['Country_Region'].fillna('N')
DF['geo'] = DF['Country_Region'] + '-' + DF['Province_State'] + '-' + DF['County']
DF
DF0 = train0.loc[ train0.Date == '2020-05-11' ].copy()
DF0['ypred'] = DF0[['geo','Date']].merge( DF.loc[DF.Target=='ConfirmedCases'], on=['geo','Date'] , how='left' )['TargetValue'].values

DF1 = train1.loc[ train1.Date == '2020-05-11' ].copy()
DF1['ypred'] = DF1[['geo','Date']].merge( DF.loc[DF.Target=='Fatalities'], on=['geo','Date'] , how='left' )['TargetValue'].values


train0['ypred'] = train0['TargetValue'].values
train0['mpred'] = train0.groupby('geo')['TargetValue'].rolling(7).mean().values
train0['Hstd']  = np.clip(train0['ypred'] - train0['mpred'], 0, 9999999999)
train0['Lstd']  = np.clip(train0['ypred'] - train0['mpred'], -9999999999, 0)

train0.loc[ train0.Date>='2020-04-27' ,'ypred'] = np.nan
train0.loc[ train0.Date>='2020-04-27' ,'mpred'] = np.nan
train0.loc[ train0.Date>='2020-04-27' ,'Hstd']  = np.nan
train0.loc[ train0.Date>='2020-04-27' ,'Lstd']  = np.nan

train0['Hstd']  = train0.groupby('geo')['Hstd'].rolling(28).std().values
train0['Lstd']  = train0.groupby('geo')['Lstd'].rolling(28).std().values

train0['Lstd']  = train0.groupby('geo')['Lstd'].fillna( method='ffill' )
train0['Hstd']  = train0.groupby('geo')['Hstd'].fillna( method='ffill' )
train0['ypred'] = train0.groupby('geo')['ypred'].fillna( method='ffill' )
train0['mpred'] = train0.groupby('geo')['mpred'].fillna( method='ffill' )

train0['q50'] = train0['TargetValue'].values
train0.loc[ train0.Date>='2020-04-27' ,'q50']  = np.nan
train0['q05'] = train0['q50']
train0['q95'] = train0['q50']
import statsmodels.api as sm

count = 1
for valday in [
    '2020-04-27',
    '2020-04-28',
    '2020-04-29',
    '2020-04-30',
    '2020-05-01',
    '2020-05-02',
    '2020-05-03',
    '2020-05-04',
    '2020-05-05',
    '2020-05-06',
    '2020-05-07',
    '2020-05-08',
    '2020-05-09',
    '2020-05-10',
    ]:
    
    for i in np.arange(1,13,1):
        train0['lag1'+str(i)] = train0.groupby('geo')['q50'].shift(i)
    train0['std1']= train0.groupby('geo')['q50'].shift(1).rolling(7).std()
    train0['std2']= train0.groupby('geo')['q50'].shift(1).rolling(14).std()
    train0['std2']= train0.groupby('geo')['q50'].shift(1).rolling(21).std()
    train0['std2']= train0.groupby('geo')['q50'].shift(1).rolling(28).std()
    
    
    TRAIN = train0.loc[ (train0.Date  <'2020-04-27')&(train0.Date >='2020-04-01') ].copy()
    VALID = train0.loc[ (train0.Date ==valday) ].copy()
    
    features = TRAIN.columns[9:]
    features = [f for f in features if f not in ['geo','ForecastId','q05','q50','q95','ypred','qstd','Lstd','Hstd','mpred','flag']  ]

    if valday == '2020-04-27':        
        model05 = sm.QuantReg(TRAIN['q50'], TRAIN[features]).fit(q=0.05)
        model50 = sm.QuantReg(TRAIN['q50'], TRAIN[features]).fit(q=0.50)
        model95 = sm.QuantReg(TRAIN['q50'], TRAIN[features]).fit(q=0.95)
        
    #break
    VALID['q05'] = model05.predict( VALID[features] ) - VALID['Lstd']*np.clip(0.25*count,0,3.5)
    VALID['q50'] = model50.predict( VALID[features] )
    VALID['q95'] = model95.predict( VALID[features] ) + VALID['Hstd']*np.clip(0.25*count,0,3.5)
    
    VALID.loc[ VALID.q50<0  ,'q50'] = 0
    VALID.loc[ VALID.q50<VALID.q05 ,'q05'] = VALID.loc[ VALID.q50<VALID.q05 ,'q50']
    VALID.loc[ VALID.q50>VALID.q95 ,'q95'] = VALID.loc[ VALID.q50>VALID.q95 ,'q50']

    VALID['q05'] = VALID['q05']/(1.02**count)
    VALID['q50'] = VALID['q50']
    VALID['q95'] = VALID['q95']*(1.02**count)
    
    VALID.loc[ VALID.q05<0  ,'q05'] = 0
    VALID.loc[ VALID.q50<0  ,'q50'] = 0
    VALID.loc[ VALID.q95<0  ,'q95'] = 0
    
    train0.loc[ (train0.Date ==valday),'q05'] = VALID['q05']
    train0.loc[ (train0.Date ==valday),'q50'] = VALID['q50']
    train0.loc[ (train0.Date ==valday),'q95'] = VALID['q95']
   
    print( calc_metric( VALID ), valday )
    count+=1

TMP0 = train0.loc[ (train0.Date>='2020-04-27')&(train0.Date<='2020-05-10') ].copy()
print( calc_metric( TMP0 ) )  

train0['ypred'] = train0['TargetValue'].values
train0['mpred'] = train0.groupby('geo')['TargetValue'].rolling(7).mean().values
train0['Hstd']  = np.clip(train0['ypred'] - train0['mpred'], 0, 9999999999)
train0['Lstd']  = np.clip(train0['ypred'] - train0['mpred'], -9999999999, 0)

# train0.loc[ train0.Date>='2020-04-27' ,'ypred'] = np.nan
# train0.loc[ train0.Date>='2020-04-27' ,'mpred'] = np.nan
# train0.loc[ train0.Date>='2020-04-27' ,'Hstd']  = np.nan
# train0.loc[ train0.Date>='2020-04-27' ,'Lstd']  = np.nan

train0['Hstd']  = train0.groupby('geo')['Hstd'].rolling(28).std().values
train0['Lstd']  = train0.groupby('geo')['Lstd'].rolling(28).std().values

train0['Lstd']  = train0.groupby('geo')['Lstd'].fillna( method='ffill' )
train0['Hstd']  = train0.groupby('geo')['Hstd'].fillna( method='ffill' )
train0['ypred'] = train0.groupby('geo')['ypred'].fillna( method='ffill' )
train0['mpred'] = train0.groupby('geo')['mpred'].fillna( method='ffill' )

train0['q50'] = train0['TargetValue'].values
#train0.loc[ train0.Date>='2020-04-27' ,'q50']  = np.nan
train0['q05'] = train0['q50']
train0['q95'] = train0['q50']


count = 1
for valday in [
    '2020-05-11',
    '2020-05-12',
    '2020-05-13',
    '2020-05-14',
    '2020-05-15',
    '2020-05-16',
    '2020-05-17',
    '2020-05-18',
    '2020-05-19',
    '2020-05-20',
    '2020-05-21',
    '2020-05-22',
    '2020-05-23',
    '2020-05-24',
    '2020-05-25',
    '2020-05-26',
    '2020-05-27',
    '2020-05-28',
    '2020-05-29',
    '2020-05-30',
    '2020-05-31',
    '2020-06-01',
    '2020-06-02',
    '2020-06-03',
    '2020-06-04',
    '2020-06-05',
    '2020-06-06',
    '2020-06-07',
    '2020-06-08',
    '2020-06-09',
    '2020-06-10',
]:
    for i in np.arange(1,13,1):
        train0['lag1'+str(i)] = train0.groupby('geo')['q50'].shift(i)#.fillna(0)
    train0['std1']= train0.groupby('geo')['q50'].shift(1).rolling(7).std()
    train0['std2']= train0.groupby('geo')['q50'].shift(1).rolling(14).std()
    train0['std2']= train0.groupby('geo')['q50'].shift(1).rolling(21).std()
    train0['std2']= train0.groupby('geo')['q50'].shift(1).rolling(28).std()
    TRAIN = train0.loc[ (train0.Date  <'2020-05-11')&(train0.Date >='2020-04-01') ].copy()
    VALID = train0.loc[ (train0.Date ==valday) ].copy()
    
    features = TRAIN.columns[9:]
    features = [f for f in features if f not in ['geo','ForecastId','q05','q50','q95','ypred','qstd','Lstd','Hstd','mpred','flag']  ]

    if valday == '2020-05-11':        
        model05 = sm.QuantReg(TRAIN['q50'], TRAIN[features]).fit(q=0.05)
        model50 = sm.QuantReg(TRAIN['q50'], TRAIN[features]).fit(q=0.50)
        model95 = sm.QuantReg(TRAIN['q50'], TRAIN[features]).fit(q=0.95)
        
    #break
    VALID['q05'] = model05.predict( VALID[features] ) - VALID['Lstd']*np.clip(0.25*count,0,3.5)
    VALID['q50'] = model50.predict( VALID[features] )
    VALID['q95'] = model95.predict( VALID[features] ) + VALID['Hstd']*np.clip(0.25*count,0,3.5)
    
    VALID.loc[ VALID.q50<0  ,'q50'] = 0
    VALID.loc[ VALID.q50<VALID.q05 ,'q05'] = VALID.loc[ VALID.q50<VALID.q05 ,'q50']
    VALID.loc[ VALID.q50>VALID.q95 ,'q95'] = VALID.loc[ VALID.q50>VALID.q95 ,'q50']

    VALID['q05'] = VALID['q05']/(1.02**count)
    VALID['q50'] = VALID['q50']
    VALID['q95'] = VALID['q95']*(1.02**count)
    
    VALID.loc[ VALID.q05<0  ,'q05'] = 0
    VALID.loc[ VALID.q50<0  ,'q50'] = 0
    VALID.loc[ VALID.q95<0  ,'q95'] = 0
    
    train0.loc[ (train0.Date ==valday),'q05'] = VALID['q05']
    train0.loc[ (train0.Date ==valday),'q50'] = VALID['q50']
    train0.loc[ (train0.Date ==valday),'q95'] = VALID['q95']
   
    print( calc_metric( VALID ), valday )
    count+=1
TMP0B = train0.loc[ (train0.Date>='2020-05-11') ].copy()
TMP0B.shape

train1['ypred'] = train1['TargetValue'].values
train1['mpred'] = train1.groupby('geo')['TargetValue'].rolling(7).mean().values
train1['Hstd']  = np.clip(train1['ypred'] - train1['mpred'], 0, 9999999999)
train1['Lstd']  = np.clip(train1['ypred'] - train1['mpred'], -9999999999, 0)

train1.loc[ train1.Date>='2020-04-27' ,'ypred'] = np.nan
train1.loc[ train1.Date>='2020-04-27' ,'mpred'] = np.nan
train1.loc[ train1.Date>='2020-04-27' ,'Hstd']  = np.nan
train1.loc[ train1.Date>='2020-04-27' ,'Lstd']  = np.nan

train1['Hstd']  = train1.groupby('geo')['Hstd'].rolling(28).std().values
train1['Lstd']  = train1.groupby('geo')['Lstd'].rolling(28).std().values

train1['Lstd']  = train1.groupby('geo')['Lstd'].fillna( method='ffill' )
train1['Hstd']  = train1.groupby('geo')['Hstd'].fillna( method='ffill' )
train1['ypred'] = train1.groupby('geo')['ypred'].fillna( method='ffill' )
train1['mpred'] = train1.groupby('geo')['mpred'].fillna( method='ffill' )

train1['q50'] = train1['TargetValue'].values
train1.loc[ train1.Date>='2020-04-27' ,'q50']  = np.nan
train1['q05'] = train1['q50']
train1['q95'] = train1['q50']
train1['q50_cases'] = train1[['geo','Date']].merge( train0[['geo','Date','q50']], on=['geo','Date'], how='left' )['q50'].values
train1.iloc[-60:,5:25]
count = 1
for valday in [
    '2020-04-27',
    '2020-04-28',
    '2020-04-29',
    '2020-04-30',
    '2020-05-01',
    '2020-05-02',
    '2020-05-03',
    '2020-05-04',
    '2020-05-05',
    '2020-05-06',
    '2020-05-07',
    '2020-05-08',
    '2020-05-09',
    '2020-05-10',
    ]:
    
    for i in np.arange(1,13,1):
        train1['lag1'+str(i)] = train1.groupby('geo')['q50'].shift(i)
        train1['lagCases1'+str(i)] = train1.groupby('geo')['q50_cases'].shift(i)
    train1['std1']= train1.groupby('geo')['q50'].shift(1).rolling(7).std()
    train1['std2']= train1.groupby('geo')['q50'].shift(1).rolling(14).std()
    train1['std2']= train1.groupby('geo')['q50'].shift(1).rolling(21).std()
    train1['std2']= train1.groupby('geo')['q50'].shift(1).rolling(28).std()
    
    TRAIN = train1.loc[ (train1.Date  <'2020-04-27')&(train1.Date >='2020-04-01') ].copy()
    VALID = train1.loc[ (train1.Date ==valday) ].copy()
    
    features = TRAIN.columns[9:]
    features = [f for f in features if f not in ['geo','ForecastId','q05','q50','q95','ypred','qstd','Lstd','Hstd','mpred','flag']  ]

    if valday == '2020-04-27':        
        model05 = sm.QuantReg(TRAIN['q50'], TRAIN[features]).fit(q=0.05)
        model50 = sm.QuantReg(TRAIN['q50'], TRAIN[features]).fit(q=0.50)
        model95 = sm.QuantReg(TRAIN['q50'], TRAIN[features]).fit(q=0.95)
        
    #break
    VALID['q05'] = model05.predict( VALID[features] ) - VALID['Lstd']*np.clip(0.01*count,0,3.5)
    VALID['q50'] = model50.predict( VALID[features] )
    VALID['q95'] = model95.predict( VALID[features] ) + VALID['Hstd']*np.clip(0.01*count,0,3.5)
    
    VALID.loc[ VALID.q50<0  ,'q50'] = 0
    VALID.loc[ VALID.q50<VALID.q05 ,'q05'] = VALID.loc[ VALID.q50<VALID.q05 ,'q50']
    VALID.loc[ VALID.q50>VALID.q95 ,'q95'] = VALID.loc[ VALID.q50>VALID.q95 ,'q50']

    VALID['q05'] = VALID['q05']/(1.001**count)
    VALID['q50'] = VALID['q50']
    VALID['q95'] = VALID['q95']*(1.001**count)
    
    VALID.loc[ VALID.q05<0  ,'q05'] = 0
    VALID.loc[ VALID.q50<0  ,'q50'] = 0
    VALID.loc[ VALID.q95<0  ,'q95'] = 0
    
    train1.loc[ (train1.Date ==valday),'q05'] = VALID['q05']
    train1.loc[ (train1.Date ==valday),'q50'] = VALID['q50']
    train1.loc[ (train1.Date ==valday),'q95'] = VALID['q95']
   
    print( calc_metric( VALID ), valday )
    count+=1

TMP1 = train1.loc[ (train1.Date>='2020-04-27')&(train1.Date<='2020-05-10') ].copy()
print( calc_metric( TMP1 ) )
train1.iloc[-60:,5:25]
train1['ypred'] = train1['TargetValue'].values
train1['mpred'] = train1.groupby('geo')['TargetValue'].rolling(7).mean().values
train1['Hstd']  = np.clip(train1['ypred'] - train1['mpred'], 0, 9999999999)
train1['Lstd']  = np.clip(train1['ypred'] - train1['mpred'], -9999999999, 0)

# train1.loc[ train1.Date>='2020-04-27' ,'ypred'] = np.nan
# train1.loc[ train1.Date>='2020-04-27' ,'mpred'] = np.nan
# train1.loc[ train1.Date>='2020-04-27' ,'Hstd']  = np.nan
# train1.loc[ train1.Date>='2020-04-27' ,'Lstd']  = np.nan

train1['Hstd']  = train1.groupby('geo')['Hstd'].rolling(28).std().values
train1['Lstd']  = train1.groupby('geo')['Lstd'].rolling(28).std().values

train1['Lstd']  = train1.groupby('geo')['Lstd'].fillna( method='ffill' )
train1['Hstd']  = train1.groupby('geo')['Hstd'].fillna( method='ffill' )
train1['ypred'] = train1.groupby('geo')['ypred'].fillna( method='ffill' )
train1['mpred'] = train1.groupby('geo')['mpred'].fillna( method='ffill' )

train1['q50'] = train1['TargetValue'].values
#train1.loc[ train1.Date>='2020-04-27' ,'q50']  = np.nan
train1['q05'] = train1['q50']
train1['q95'] = train1['q50']

count = 1
for valday in [
    '2020-05-11',
    '2020-05-12',
    '2020-05-13',
    '2020-05-14',
    '2020-05-15',
    '2020-05-16',
    '2020-05-17',
    '2020-05-18',
    '2020-05-19',
    '2020-05-20',
    '2020-05-21',
    '2020-05-22',
    '2020-05-23',
    '2020-05-24',
    '2020-05-25',
    '2020-05-26',
    '2020-05-27',
    '2020-05-28',
    '2020-05-29',
    '2020-05-30',
    '2020-05-31',
    '2020-06-01',
    '2020-06-02',
    '2020-06-03',
    '2020-06-04',
    '2020-06-05',
    '2020-06-06',
    '2020-06-07',
    '2020-06-08',
    '2020-06-09',
    '2020-06-10',
    ]:
    
    for i in np.arange(1,13,1):
        train1['lag1'+str(i)] = train1.groupby('geo')['q50'].shift(i)
        train1['lagCases1'+str(i)] = train1.groupby('geo')['q50_cases'].shift(i)
    train1['std1']= train1.groupby('geo')['q50'].shift(1).rolling(7).std()
    train1['std2']= train1.groupby('geo')['q50'].shift(1).rolling(14).std()
    train1['std2']= train1.groupby('geo')['q50'].shift(1).rolling(21).std()
    train1['std2']= train1.groupby('geo')['q50'].shift(1).rolling(28).std()
    
    TRAIN = train1.loc[ (train1.Date  <'2020-05-11')&(train1.Date >='2020-04-01') ].copy()
    VALID = train1.loc[ (train1.Date ==valday) ].copy()
    
    features = TRAIN.columns[9:]
    features = [f for f in features if f not in ['geo','ForecastId','q05','q50','q95','ypred','qstd','Lstd','Hstd','mpred','flag']  ]

    if valday == '2020-05-11':        
        model05 = sm.QuantReg(TRAIN['q50'], TRAIN[features]).fit(q=0.05)
        model50 = sm.QuantReg(TRAIN['q50'], TRAIN[features]).fit(q=0.50)
        model95 = sm.QuantReg(TRAIN['q50'], TRAIN[features]).fit(q=0.95)
        
    #break
    VALID['q05'] = model05.predict( VALID[features] ) - VALID['Lstd']*np.clip(0.01*count,0,3.5)
    VALID['q50'] = model50.predict( VALID[features] )
    VALID['q95'] = model95.predict( VALID[features] ) + VALID['Hstd']*np.clip(0.01*count,0,3.5)
    
    VALID.loc[ VALID.q50<0  ,'q50'] = 0
    VALID.loc[ VALID.q50<VALID.q05 ,'q05'] = VALID.loc[ VALID.q50<VALID.q05 ,'q50']
    VALID.loc[ VALID.q50>VALID.q95 ,'q95'] = VALID.loc[ VALID.q50>VALID.q95 ,'q50']

    VALID['q05'] = VALID['q05']/(1.001**count)
    VALID['q50'] = VALID['q50']
    VALID['q95'] = VALID['q95']*(1.001**count)
    
    VALID.loc[ VALID.q05<0  ,'q05'] = 0
    VALID.loc[ VALID.q50<0  ,'q50'] = 0
    VALID.loc[ VALID.q95<0  ,'q95'] = 0
    
    train1.loc[ (train1.Date ==valday),'q05'] = VALID['q05']
    train1.loc[ (train1.Date ==valday),'q50'] = VALID['q50']
    train1.loc[ (train1.Date ==valday),'q95'] = VALID['q95']
   
    print( calc_metric( VALID ), valday )
    count+=1
    
TMP1B = train1.loc[ (train1.Date>='2020-05-11') ].copy()
TMP1B.shape    
================================================================
RECOMMENDED CODE:
----------------------------------------------------
[]
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

ORIGINAL CODE:
---------------------------------------------------------------------------------------------------
# Reference: https://www.kaggle.com/code/jsaguiar/complete-eda-time-series-with-plotly

item_sum = df.groupby(['item', 'date'])['sales'].sum()
traces = []

for i in range(1, 51):
    s = item_sum[i].to_frame().reset_index()
    trace = go.Box(y= s.sales, name= 'Item {}'.format(i), jitter=0.8, whiskerwidth=0.2, marker=dict(size=2), line=dict(width=1))
    traces.append(trace)

layout = go.Layout(
    title='Sales BoxPlot for each item',
    yaxis=dict(
        autorange=True, showgrid=True, zeroline=True,
        gridcolor='rgb(233,233,233)', zerolinecolor='rgb(255, 255, 255)',
        zerolinewidth=2, gridwidth=1
    ),
    margin=dict(l=40, r=30, b=80, t=100), showlegend=False,
)

fig = go.Figure(data=traces, layout=layout)
iplot(fig)
================================================================
RECOMMENDED CODE:
----------------------------------------------------
# Reference: https://www.kaggle.com/code/jsaguiar/complete-eda-time-series-with-plotly

store_sum = df.groupby(['store', 'date'])['sales'].sum()
traces = []

for i in range(1, 11):
    s = store_sum[i].to_frame().reset_index()
    trace = go.Box(y= s.sales, name= 'Store {}'.format(i), jitter=0.8, whiskerwidth=0.2, marker=dict(size=2), line=dict(width=1))
    traces.append(trace)

layout = go.Layout(
    title='Sales BoxPlot for each store',
    yaxis=dict(
        autorange=True, showgrid=True, zeroline=True,
        gridcolor='rgb(233,233,233)', zerolinecolor='rgb(255, 255, 255)',
        zerolinewidth=2, gridwidth=1
    ),
    margin=dict(l=40, r=30, b=80, t=100), showlegend=False,
)

fig = go.Figure(data=traces, layout=layout)
iplot(fig)
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

ORIGINAL CODE:
---------------------------------------------------------------------------------------------------
# Reference: https://www.kaggle.com/code/hmendonca/aptos19-regressor-fastai-oversampling-tta

learn.lr_find(end_lr=0.5)
learn.recorder.plot(suggestion=True)
# train head first
learn.fit_one_cycle(2, max_lr=5e-3, div_factor=15)
learn.save('stage-1')
learn.recorder.plot_losses()
# unfreeze and search appropriate learning rate for full training
learn.unfreeze()
learn.lr_find(start_lr=1e-10, wd=1e-3)
learn.recorder.plot(suggestion=True)
# train all layers
learn.fit_one_cycle(6, max_lr=slice(1e-5, 1e-3), div_factor=10, wd=1e-3)
learn.save('stage-2')
learn.recorder.plot_losses()
# schedule of the lr (left) and momentum (right) that the 1cycle policy uses
learn.recorder.plot_lr(show_moms=True)
# kappa scores
learn.recorder.plot_metrics()
# reload best model so far and look for a new learning rate
learn.load('bestmodel')
learn.lr_find(start_lr=1e-10)
learn.recorder.plot(suggestion=True)
# train all layers, now with some weight decay
learn.fit_one_cycle(12, max_lr=slice(2e-6, 2e-4), div_factor=20)
learn.save('stage-3')
learn.recorder.plot_losses()
# # schedule of the lr (left) and momentum (right) that the 1cycle policy uses
# learn.recorder.plot_lr(show_moms=True)
================================================================
RECOMMENDED CODE:
----------------------------------------------------
# Reference: https://www.kaggle.com/code/kashnitsky/amazon-product-reviews-classification-with-ulmfit

%%time
learn = language_model_learner(data_lm, drop_mult=0.3, arch=AWD_LSTM)
%%time
learn.lr_find(start_lr = slice(10e-7, 10e-5), end_lr=slice(0.1, 10))
learn.recorder.plot(skip_end=10, suggestion=True)
best_lm_lr = 3e-3 #learn.recorder.min_grad_lr
# best_lm_lr
%%time
learn.fit_one_cycle(1, best_lm_lr)
learn.unfreeze()
%%time
learn.fit_one_cycle(5, best_lm_lr)
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

ORIGINAL CODE:
---------------------------------------------------------------------------------------------------
from PIL import Image, ImageDraw, ImageFont
from os import listdir
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline
================================================================
RECOMMENDED CODE:
----------------------------------------------------
[]
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

ORIGINAL CODE:
---------------------------------------------------------------------------------------------------
# Reference: https://www.kaggle.com/code/bulentsiyah/dogs-vs-cats-classification-vgg16-fine-tuning

sample = random.choice(filenames)
image = load_img("../input/train/train/"+sample)
plt.imshow(image)
================================================================
RECOMMENDED CODE:
----------------------------------------------------
# Reference: https://www.kaggle.com/code/kmader/qbi-illumination-correction

np.random.seed(2019)
xx = np.stack([np.arange(5)]*5, -1)
yy = xx.T
bins_sample_8bit = np.linspace(0, 255, 8)
sample_img = (25*(xx+yy)+np.random.uniform(-10, 10, size=(5, 5))).astype('uint8')
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(6,3))
sns.heatmap(sample_img, annot=True,fmt='02d', ax=ax1, cmap='viridis')
ax2.hist(sample_img.ravel(), bins_sample_8bit, label='Original', alpha=1)
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

ORIGINAL CODE:
---------------------------------------------------------------------------------------------------
# Reference: https://www.kaggle.com/code/bulentsiyah/dogs-vs-cats-classification-vgg16-fine-tuning

loss, accuracy = model.evaluate_generator(validation_generator, total_validate//batch_size, workers=12)
print("Test: accuracy = %f  ;  loss = %f " % (accuracy, loss))
def plot_model_history(model_history, acc='acc', val_acc='val_acc'):
    fig, axs = plt.subplots(1,2,figsize=(15,5))
    axs[0].plot(range(1,len(model_history.history[acc])+1),model_history.history[acc])
    axs[0].plot(range(1,len(model_history.history[val_acc])+1),model_history.history[val_acc])
    axs[0].set_title('Model Accuracy')
    axs[0].set_ylabel('Accuracy')
    axs[0].set_xlabel('Epoch')
    axs[0].set_xticks(np.arange(1,len(model_history.history[acc])+1),len(model_history.history[acc])/10)
    axs[0].legend(['train', 'val'], loc='best')
    axs[1].plot(range(1,len(model_history.history['loss'])+1),model_history.history['loss'])
    axs[1].plot(range(1,len(model_history.history['val_loss'])+1),model_history.history['val_loss'])
    axs[1].set_title('Model Loss')
    axs[1].set_ylabel('Loss')
    axs[1].set_xlabel('Epoch')
    axs[1].set_xticks(np.arange(1,len(model_history.history['loss'])+1),len(model_history.history['loss'])/10)
    axs[1].legend(['train', 'val'], loc='best')
    plt.show()
    
plot_model_history(history)
Y_val = validate_df['category']
y_pred =  model.predict_generator(validation_generator)
threshold = 0.5
y_final = np.where(y_pred > threshold, 1,0)
y_final.size
import seaborn as sns
from sklearn.metrics import confusion_matrix
# Predict the values from the validation dataset

# compute the confusion matrix
confusion_mtx = confusion_matrix(Y_val, y_final) 
# plot the confusion matrix
f,ax = plt.subplots(figsize=(8, 8))
sns.heatmap(confusion_mtx, annot=True, linewidths=0.01,cmap="Greens",linecolor="gray", fmt= '.1f',ax=ax)
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.title("Confusion Matrix")
plt.show()
from sklearn.metrics import classification_report

# Generate a classification report
report = classification_report(Y_val, y_final, target_names=['0','1'])

print(report)
================================================================
RECOMMENDED CODE:
----------------------------------------------------
[]
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

ORIGINAL CODE:
---------------------------------------------------------------------------------------------------
# Reference: https://www.kaggle.com/code/ryches/turbo-charging-andrew-s-pytorch

runner.train(
    model=model,
    criterion=criterion,
    optimizer=optimizer,
    scheduler=scheduler,
    loaders=loaders,
    callbacks=[DiceCallback(), EarlyStoppingCallback(patience=5, min_delta=0.001)],
    logdir=logdir,
    num_epochs=num_epochs,
    verbose=True
)
utils.plot_metrics(
    logdir=logdir, 
    # specify which metrics we want to plot
    metrics=["loss", "dice", 'lr', '_base/lr']
)
encoded_pixels = []
loaders = {"infer": valid_loader}
runner.infer(
    model=model,
    loaders=loaders,
    callbacks=[
        CheckpointCallback(
            resume=f"{logdir}/checkpoints/best.pth"),
        InferCallback()
    ],
)
valid_masks = []
probabilities = np.zeros((2220, 350, 525), dtype = np.float32)
for i, (batch, output) in enumerate(tqdm.tqdm(zip(
        valid_dataset, runner.callbacks[0].predictions["logits"]))):
    image, mask = batch
    for m in mask:
        if m.shape != (350, 525):
            m = cv2.resize(m, dsize=(525, 350), interpolation=cv2.INTER_LINEAR)
        valid_masks.append(m)

    for j, probability in enumerate(output):
        if probability.shape != (350, 525):
            probability = cv2.resize(probability, dsize=(525, 350), interpolation=cv2.INTER_LINEAR)
        probabilities[i * 4 + j, :, :] = probability
import gc
torch.cuda.empty_cache()
gc.collect()
================================================================
RECOMMENDED CODE:
----------------------------------------------------
# Reference: https://www.kaggle.com/code/gogo827jz/deeper-efficientnet-b7

# Need this line so Google will recite some incantations
# for Turing to magically load the model onto the TPU
with strategy.scope():
    enet = efn.EfficientNetB7(
        input_shape=(512, 512, 3),
        weights='imagenet',
        include_top=False
    )
    x = enet.output
    x1 = tf.keras.layers.GlobalMaxPooling2D()(x)
    x2 = tf.keras.layers.GlobalAveragePooling2D()(x)
    x = tf.keras.layers.Concatenate()([x1, x2])
    x = tf.keras.layers.BatchNormalization()(x)
    x = tf.keras.layers.Dropout(0.5)(x)
    x = tf.keras.layers.Dense(512, activation='elu')(x)
    y = tf.keras.layers.Dense(len(CLASSES), activation='softmax')(x)
    
    model = tf.keras.Model(inputs=enet.input, outputs=y)
        
model.compile(
    optimizer=tf.keras.optimizers.Adam(lr=0.0001),
    loss = 'sparse_categorical_crossentropy',
    metrics=['sparse_categorical_accuracy']
)
model.summary()
tf.keras.utils.plot_model(
    model,
    to_file='DeepEfficientNetB7.png',
    show_shapes=False,
    show_layer_names=True,
    rankdir='TB',
    expand_nested=False,
    dpi=96
)
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

ORIGINAL CODE:
---------------------------------------------------------------------------------------------------
%matplotlib inline
import pandas as pd
from IPython.core.interactiveshell import InteractiveShell
InteractiveShell.ast_node_interactivity = 'all'
pd.set_option('display.max_columns', 99)
pd.set_option('display.max_rows', 99)
import os
import numpy as np
from tqdm import tqdm
import datetime as dt
import matplotlib.pyplot as plt
plt.rcParams['figure.figsize'] = [16, 10]
plt.rcParams['font.size'] = 14
import seaborn as sns
sns.set_palette(sns.color_palette('tab20', 20))

import plotly.express as px
import plotly.graph_objects as go
def to_log(x):
    return np.log(x + 1)


def to_exp(x):
    return np.exp(x) - 1

start = dt.datetime.now()

lb_periods = {
    1: ('2020-03-26', '2020-04-23'),
    2: ('2020-04-02', '2020-04-30'),
    3: ('2020-04-09', '2020-05-07'),
    4: ('2020-04-16', '2020-05-14')
}
def get_competition_data(week):
    train = pd.read_csv(f'../input/covid19-global-forecasting-week-{week}/train.csv')
    test = pd.read_csv(f'../input/covid19-global-forecasting-week-{week}/test.csv')
    
    if 'Province/State' in test.columns:
        test = test.rename(columns={'Province/State': 'Province_State', 'Country/Region': 'Country_Region'})
        train = train.rename(columns={'Province/State': 'Province_State', 'Country/Region': 'Country_Region'})
    train['Location'] = train['Country_Region'] + '-' + train['Province_State'].fillna('')
    test['Location'] = test['Country_Region'] + '-' + test['Province_State'].fillna('')
    train = train[['Date', 'Location', 'ConfirmedCases', 'Fatalities']]
    return train, test
def get_actual(skip_nova_scotia=False):
    #     actual, _ = get_competition_data(week=4)
    actual = pd.read_csv(f'../input/covid-19-forecasting-ongoing-data-updates/train.csv')
    actual.ConfirmedCases = actual.ConfirmedCases.clip(0, None)
    actual.Fatalities = actual.Fatalities.clip(0, None)
    actual['Location'] = actual['Country_Region'] + '-' + actual['Province_State'].fillna('')
    actual = actual[['Date', 'Location', 'ConfirmedCases', 'Fatalities']]
    if skip_nova_scotia:
        actual = actual[actual.Location != 'Canada-Nova Scotia']
    return actual
actual = get_actual(skip_nova_scotia=False)
actual[actual.Location=='Canada-Nova Scotia'].tail(10)
actual[actual.Location=='US-New York'].tail(40)
def get_submissions(week):
    submission_path = f'../input/covid19-global-forecasting-submissions/week_{week}'
    submission_files = os.listdir(submission_path)
    submissions_list = []

    for f in tqdm(submission_files):
        submission = pd.read_csv(os.path.join(submission_path, f))
        submission.insert(0, 'SubmissionId', int(f[:-4]))
        submissions_list.append(submission)

    submissions = pd.concat(submissions_list, ignore_index=True, sort=False)
    
    submissions = submissions[['SubmissionId', 'ForecastId', 'ConfirmedCases', 'Fatalities']]
    
    submissions.ConfirmedCases = submissions.ConfirmedCases.clip(0, None)
    submissions.Fatalities = submissions.Fatalities.clip(0, None)
    
    _, test = get_competition_data(week)
    submissions = submissions.merge(test, on='ForecastId', how='left')
    
    submissions = submissions.loc[submissions.Date >= lb_periods[week][0]]
    
    actual = get_actual()
    submissions = submissions.merge(actual, how='left', on=['Date', 'Location'], suffixes=['', 'Actual'])
    
    return submissions
# actual, _ = get_competition_data(week=4)
actual = get_actual()
print(f'Actual last day: {actual.Date.max()}')
actual.describe()
week = 1
submissions = get_submissions(week)


submissions.head()
submissions.shape
def add_errors(submissions):
    submissions.loc[:,'FatalitiesSLE'] = (to_log(submissions.Fatalities) - to_log(submissions.FatalitiesActual)) ** 2
    submissions.loc[:,'ConfirmedCasesSLE'] = (to_log(submissions.ConfirmedCases) - to_log(submissions.ConfirmedCasesActual)) ** 2
    return submissions

def calculate_lb(submissions):
    lb = submissions[['SubmissionId', 'FatalitiesSLE', 'ConfirmedCasesSLE']].groupby('SubmissionId').mean().reset_index()
    lb.loc[:, 'FatalatiesRMSLE'] = np.sqrt(lb['FatalitiesSLE'])
    lb.loc[:, 'ConfirmedCasesRMSLE'] = np.sqrt(lb['ConfirmedCasesSLE'])
    lb.loc[:, 'RMSLE'] = (lb['FatalatiesRMSLE'] + lb['ConfirmedCasesRMSLE']) / 2.0
    lb = lb.sort_values(by='RMSLE')
    lb['Rank'] = np.arange(len(lb))
    return lb
submissions = add_errors(submissions)

lb = calculate_lb(submissions)
submissions = submissions.merge(lb[['SubmissionId', 'RMSLE', 'Rank']], on='SubmissionId')
submissions.head()
lb.head()
def get_ensemble(submissions, k=10):
    submissions['LogCC'] = to_log(submissions.ConfirmedCases)
    submissions['LogF'] = to_log(submissions.Fatalities)

    ensemble = submissions[submissions.Rank < k].groupby(['Date', 'Location'])[['LogCC', 'LogF']].mean()
    ensemble['ConfirmedCases'] = to_exp(ensemble.LogCC)
    ensemble['Fatalities'] = to_exp(ensemble.LogF)
    ensemble = ensemble.reset_index()

    ensemble = ensemble.merge(actual, how='left', on=['Date', 'Location'], suffixes=['', 'Actual'])
    ensemble = add_errors(ensemble)
    return ensemble
def calculate_lb_and_ensemble(week, top_ranks=10):
    submissions = get_submissions(week)
    submissions = add_errors(submissions)

    lb = calculate_lb(submissions)
    submissions = submissions.merge(lb[['SubmissionId', 'RMSLE', 'Rank']], on='SubmissionId')

    ens = get_ensemble(submissions, k=10)
    np.sqrt((ens.FatalitiesSLE.mean() + ens.ConfirmedCasesSLE.mean() ) / 2.0)

    daily_error = submissions[submissions.Rank < top_ranks].groupby(['SubmissionId', 'Date']).mean().reset_index()
    daily_error['Daily RMSLE'] = np.sqrt(0.5 * daily_error.FatalitiesSLE + 0.5 * daily_error.ConfirmedCasesSLE)
    daily_error['LB Score'] = '#' + daily_error.Rank.astype(str) + ' - ' + daily_error.RMSLE.round(5).astype(str) + ' - ' + daily_error.SubmissionId.astype(str)
    daily_error = daily_error.sort_values(by=['Rank', 'Date'])
    fig = px.line(daily_error, x='Date', y='Daily RMSLE', color='LB Score')
    _ = fig.update_layout(
        title_text=f'COVID-19 Daily Prediction Error (Week {week})'
    )

    return submissions, lb, ens, daily_error, fig
    
================================================================
RECOMMENDED CODE:
----------------------------------------------------
[]
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

