,markdown_content,preprocessed_markdown
0,"['## Our goal\n', '\n', 'In this competition we are asked to predict if a customer will make a transaction or not regardless of the amount of money transacted. Hence our goal is to solve a binary classification problem. In the data description you can see that the features given are numeric and anonymized. Furthermore the data seems to be artificial as they state that ""the data has the same structure as our real data"". \n', '\n', '### Table of contents\n', '\n', '1. [Loading packages](#load) (complete)\n', '2. [Sneak a peek at the data](#data) (complete)\n', '2. [What can we say about the target?](#target) (complete)\n', '3. [Can we find relationships between features?](#correlation) (complete)\n', '4. [Baseline submission](#baselines) (complete)\n', '5. [Basic feature engineering](#engineering) (complete)\n', '6. [Gaussian Mixture Clustering](#clustering) (complete)']",goal competit ask predict custom make transact regardless amount money transact henc goal solv binari classif problem data descript see featur given numer anonym furthermor data seem artifici state data structur real data tabl content 1 load packag load complet 2 sneak peek data data complet 2 say target target complet 3 find relationship featur correl complet 4 baselin submiss baselin complet 5 basic featur engin engin complet 6 gaussian mixtur cluster cluster complet
1,['## Kernel settings'],kernel set
2,"['## Loading packages <a class=""anchor"" id=""load""></a>']",load packag class anchor id load
3,"['## Sneak a peek at the data <a class=""anchor"" id=""data""></a>']",sneak peek data class anchor id data
4,['### Train'],train
5,"['Ok, 200.000 rows and 202 features. ']",ok 200 000 row 202 featur
6,"[""The target as well as the ID-Code of a sample are 2 special variables. Consequently 200 features are left. Browsing through the columns we can see that they look really numeric. It seems that there are no counter or integer variables. In addition it looks like if there are no missing values. Let's check it out:""]",target well id code sampl 2 special variabl consequ 200 featur left brow column see look realli numer seem counter integ variabl addit look like miss valu let check
7,['### Test'],test
8,['At a first glance this looks similar to train except from the missing target.'],first glanc look similar train except miss target
9,"['### Submission\n', '\n', ""Before we start, let's look at the sample submission as well:""]",submiss start let look sampl submiss well
10,"['Ok, not much to say about it.']",ok much say
11,"['## What can we say about the target? <a class=""anchor"" id=""target""></a>']",say target class anchor id target
12,"['### Take Away\n', '\n', '* We have to solve an imbalanced class problem. The number of customers that will not make a transaction is much higher than those that will. \n', '* It seem that there is no relationship of the target with the index of the train dataframe. This is more empressend by the zero targets than for the ones. \n', '* Take a look at the jitter plots within the violinplots. We can see that the targets look uniformly distributed over the indexes. It seems that the competitors were careful during the process of ordering the data. Once more this indicates that the data is simulated.']",take away solv imbalanc class problem number custom make transact much higher seem relationship target index train datafram empressend zero target one take look jitter plot within violinplot see target look uniformli distribut index seem competitor care process order data indic data simul
13,"['## Can we find relationships between features? <a class=""anchor"" id=""correlation""></a>']",find relationship featur class anchor id correl
14,"['### Linear correlations\n', '\n', ""I have already seen some correlation heatmaps in public kernels and it seems as if there is almost no correlation between features. Let's check this out by computing all correlation values and plotting the overall distribution:""]",linear correl alreadi seen correl heatmap public kernel seem almost correl featur let check comput correl valu plot overal distribut
15,['Woooow! :-O All features seem to have no linear correlation!!! Neither in train nor in test. Very strange. We know that they are anonymized and perhaps they are decorrelated by some transformation as well. '],woooow featur seem linear correl neither train test strang know anonym perhap decorrel transform well
16,"['### Random Forest Top Features\n', '\n', ""To start easy, let's use a random forest to select top 10 features. They can serve as a starting point to discover their nature and for trying to understand the data. In addition they may yield some ideas on how to generate new features. I am going to use stratified KFold as a cross validation strategy. It's somehow arbitrary to use KFold as we don't know if we have time series data given, but it may serve as a good starting point. \n"", '\n', ""To start simple I like to use a random forest that helps us to select important features. As there are no linear correlations it's a good idea to start with a nonlinear model that allows us to discover features, their importances as well as interactions. Let's start! :-)""]",random forest top featur start easi let use random forest select top 10 featur serv start point discov natur tri understand data addit may yield idea gener new featur go use stratifi kfold cross valid strategi somehow arbitrari use kfold know time seri data given may serv good start point start simpl like use random forest help u select import featur linear correl good idea start nonlinear model allow u discov featur import well interact let start
17,['You can see that the score is not as good as some other scores of public kernels but nontheless my attempt is to understand the data by improving this score. We can use more powerful models later on.'],see score good score public kernel nontheless attempt understand data improv score use power model later
18,"[""Let's take a look at n_top features of your choice:""]",let take look n top featur choic
19,"['Ok, that\'s enough to start with the ""data-understanding-journey"".']",ok enough start data understand journey
20,"['### Exploring top features\n', '\n', 'First of all: How do the distributions of the variables look like with respect to the targets in train? Can we observe discrepancies between train and test features for selected top features?']",explor top featur first distribut variabl look like respect target train observ discrep train test featur select top featur
21,"['### Take Away\n', '\n', '* Interestingly there are some peeks inside the distributions, especially for variables 81, 12 and 53. Why do these data points accumulate on these values?\n', '* We can observe that the accumulations are less dense in the test data. \n', '* Variable 174 seem to miss the bulb on the right hand side of the distribution in the test data. ']",take away interestingli peek insid distribut especi variabl 81 12 53 data point accumul valu observ accumul le den test data variabl 174 seem miss bulb right hand side distribut test data
22,['### How do the scatter plots look like?'],scatter plot look like
23,['Crazy! Can you see the sharp limits of several variables where the samples with target 1 suddenly accumulate and seldomly pass over. Look at var 81 and 12 for example. You can see that there are limits close to 10 (var 81) and 13.5 (var 12). This finding could be a nice entry point for further feature engineering.'],crazi see sharp limit sever variabl sampl target 1 suddenli accumul seldomli pas look var 81 12 exampl see limit close 10 var 81 13 5 var 12 find could nice entri point featur engin
24,"['## Baseline submissions \n', '\n', '### What score does the forest yield on public LB?']",baselin submiss score forest yield public lb
25,['Yields 0.662 on public LB.'],yield 0 662 public lb
26,"['## Feature engineering \n', '\n', ""Let's do some basic feature engineering. Perhaps it helps to improve:""]",featur engin let basic featur engin perhap help improv
27,['### Rounding & quantile based binning'],round quantil base bin
28,['### New feature importances'],new featur import
29,"['## Gaussian Mixture Clustering  <a class=""anchor"" id=""clustering""></a>\n', '\n', ""The majority of the data looks like a big gaussian distribution. Besides that there seems to be at least one or two more gaussians that could explain the second and third mode that we can find for important features. Let's motivate this even further by looking at scatter and kde-plots of some top-features:""]",gaussian mixtur cluster class anchor id cluster major data look like big gaussian distribut besid seem least one two gaussian could explain second third mode find import featur let motiv even look scatter kde plot top featur
30,"['* At least one big gaussians with one or two small, very thin but long gaussians.\n', ""* It's very interesting that we can still find outliers beside sharp lines. \n"", '\n', ""Let's assume now that the data was generated using a mixture of gaussians and let's try to cluster them. Perhaps we can see that some clusters occupy more hot targets than others.""]",least one big gaussian one two small thin long gaussian interest still find outlier besid sharp line let assum data gener use mixtur gaussian let tri cluster perhap see cluster occupi hot target other
31,"['### Take-Away\n', '\n', ""* By fitting the gaussian mixture model we are maximizing the log likelihood. The higher, the better the gaussians suite to our data. As it's difficult to choose the right number of components (gaussians) I decided to use a stratified k fold of the train data. This way we can fit gaussians to a train subset, and test how big the log likelihood is on the test subset. By doing so three times for each selected component, we gain some more information about the stability of our solution. **We can see that 3 gaussians seem to be sufficient as the log likelihood values decrease with more components**. \n"", '* This need not be true as the **solution depends on the initialization of the gaussians (the seeds I used) and with more data, the result may be different**. \n', '* But we can say: There are **at least 3 gaussians**. This is what we have already found by visual exploration of the data. \n', ""* The individual score per data spot can be understood as a measure of density. If it's low, the data spot lives in a region with other data points far away. If it's high, it should have a lot of neighbors. Consequently the individual logL-score can tell us something about outliers in the data.""]",take away fit gaussian mixtur model maxim log likelihood higher better gaussian suit data difficult choos right number compon gaussian decid use stratifi k fold train data way fit gaussian train subset test big log likelihood test subset three time select compon gain inform stabil solut see 3 gaussian seem suffici log likelihood valu decreas compon need true solut depend initi gaussian seed use data result may differ say least 3 gaussian alreadi found visual explor data individu score per data spot understood measur densiti low data spot live region data point far away high lot neighbor consequ individu logl score tell u someth outlier data
32,"[""* As we have much more cold-targets (zero) that hot (ones), I'm not surprised that hot targets occupy only a small part of the data per cluster. Nonetheless we can see that cluster 1 has significantly more hot targets than the others.\n"", '* The second plot shows that most hot targets are located in cluster 1 followed by cluster 2. This confirms our assumption that the big gaussian in the middle (cluster 0) has the smallest amount of hot targets and that the small, thin side distributions are more likely to have hot targets. ']",much cold target zero hot one surpris hot target occupi small part data per cluster nonetheless see cluster 1 significantli hot target other second plot show hot target locat cluster 1 follow cluster 2 confirm assumpt big gaussian middl cluster 0 smallest amount hot target small thin side distribut like hot target
33,['Only some features are important to separate the structure of the data. '],featur import separ structur data
34,['Good luck for the last days! :-)'],good luck last day
35,['![Imgur](https://i.imgur.com/L2Jaqm8.png)'],imgur http imgur com l2jaqm8 png
36,"['What is in this tutorial?\n', 'in this tutorial I am trying to illustrate how ensembling techniques work manually and by python code to make a good intuition about why is it  useful and why do we use it.\n']",tutori tutori tri illustr ensembl techniqu work manual python code make good intuit use use
37,"['**Introduction to ensembling**\n', '\n', '**Types of ensembling :**\n', '\n', '**Basic Ensemble Techniques**\n', '\n', '*     Max Voting\n', '*     Averaging\n', '*     Weighted Average\n', '\n', '**Advanced Ensemble Techniques**\n', '\n', '* Stacking\n', '* Blending\n', '* Bagging\n', '* Boosting\n', '\n', '**Algorithms based on Bagging and Boosting**\n', '\n', '> * Bagging meta-estimator\n', '* Random Forest\n', '* AdaBoost\n', '* GBM\n', '* XGB\n', '* Light GBM\n', '* CatBoost\n']",introduct ensembl type ensembl basic ensembl techniqu max vote averag weight averag advanc ensembl techniqu stack blend bag boost algorithm base bag boost bag meta estim random forest adaboost gbm xgb light gbm catboost
38,"['What is in this tutorial?\n', 'in thi tutorial I am trying to illustrate how ensembling techniques work manually and by python code to make a good intuition about why it is useful and why we use it.\n', '\n']",tutori thi tutori tri illustr ensembl techniqu work manual python code make good intuit use use
39,['At first there is a rational we must stabilize that : combination between models increase accuracy and in machine learning combination is **Ensembling** \n'],first ration must stabil combin model increas accuraci machin learn combin ensembl
40,['**Introduction to ensembling :**'],introduct ensembl
41,['**Errors**'],error
42,['The error emerging from any model can be broken down into three components mathematically. Following are these component :'],error emerg model broken three compon mathemat follow compon
43,['![Imgur](https://i.imgur.com/LmeI08b.png)'],imgur http imgur com lmei08b png
44,"['**Why is this important in the current context?**\n', 'To understand what really goes behind an ensemble model, we need to first understand what causes error in the model. We will briefly introduce you to these errors and give an insight to each ensemble learner in this regards.']",import current context understand realli goe behind ensembl model need first understand caus error model briefli introduc error give insight ensembl learner regard
45,"['**Bias error **\n', '\n', 'is useful to quantify how much on an average are the predicted values different from the actual value. A high bias error means we have a under-performing model which keeps on missing important trends.\n', '\n', '**Variance**\n', '\n', 'on the other side quantifies how are the prediction made on same observation different from each other. A high variance model will over-fit on your training population and perform badly on any observation beyond training. Following diagram will give you more clarity (Assume that red spot is the real value and blue dots are predictions) :\n']",bia error use quantifi much averag predict valu differ actual valu high bia error mean perform model keep miss import trend varianc side quantifi predict made observ differ high varianc model fit train popul perform badli observ beyond train follow diagram give clariti assum red spot real valu blue dot predict
46,['![Imgur](https://i.imgur.com/jFfarvo.png)'],imgur http imgur com jffarvo png
47,['model should maintain a balance between these two types of errors. This is known as the trade-off management of bias-variance errors. **Ensemble learning is one way to execute this trade off analysis.**'],model maintain balanc two type error known trade manag bia varianc error ensembl learn one way execut trade analysi
48,['![Imgur](https://i.imgur.com/ZDZsSr1.png)'],imgur http imgur com zdzssr1 png
49,['![Imgur](https://i.imgur.com/Xm5sKxD.png)'],imgur http imgur com xm5skxd png
50,['![Imgur](https://i.imgur.com/GEG80ni.png)'],imgur http imgur com geg80ni png
51,"['**A group of predictors is called an ensemble**; thus, this technique is called Ensemble Learning, and an Ensemble Learning algorithm is called an** Ensemble method.**\n', '\n', ""Suppose you ask a complex question to thousands of random people, then aggregate their answers. In many cases you will find that this aggregated answer is better than an expert's answer. This is called **the wisdom of the crowd**\n"", '\n', 'Likewise, if you aggregate the predictions of a group of predictors (e.g. decision tree classifer, SVM, logistic regression), you will often get better predictions than with the best individual predictor.']",group predictor call ensembl thu techniqu call ensembl learn ensembl learn algorithm call ensembl method suppos ask complex question thousand random peopl aggreg answer mani case find aggreg answer better expert answer call wisdom crowd likewis aggreg predict group predictor e g decis tree classif svm logist regress often get better predict best individu predictor
52,['**Types of ensembling :****'],type ensembl
53,"[' **Basic Ensemble Techniques**\n', '\n', '*     Max Voting\n', '*     Averaging\n', '*     Weighted Average\n', '\n', '**Advanced Ensemble Techniques**\n', '\n', '* Stacking\n', '* Blending\n', '* Bagging\n', '* Boosting\n', '\n', '**Algorithms based on Bagging and Boosting**\n', '\n', '> * Bagging meta-estimator\n', '* Random Forest\n', '* AdaBoost\n', '* GBM\n', '* XGB\n', '* Light GBM\n', '* CatBoost\n', '\n']",basic ensembl techniqu max vote averag weight averag advanc ensembl techniqu stack blend bag boost algorithm base bag boost bag meta estim random forest adaboost gbm xgb light gbm catboost
54,['lets talk first about Max voting'],let talk first max vote
55,"['**Max Voting\n', '**\n']",max vote
56,"['The max voting method is generally used for classification problems. In this technique, multiple models are used to make predictions for each data point. The predictions by each model are considered as a ‘vote’. The predictions which we get from the majority of the models are used as the final prediction.\n', '\n', 'For example, when you asked 5 of your colleagues to rate your movie (out of 5); we’ll assume three of them rated it as 4 while two of them gave it a 5. Since the majority gave a rating of 4, the final rating will be taken as 4. You can consider this as taking the mode of all the predictions.\n', '\n', 'The result of max voting would be something like this:\n', '\n', 'Colleague 1-5\n', '\n', 'Colleague 2-4\n', '\n', 'Colleague 3-5\n', '\n', 'Colleague 4-4\n', '\n', 'Colleague 5-4\n', '\n', 'Finalrating-4']",max vote method gener use classif problem techniqu multipl model use make predict data point predict model consid vote predict get major model use final predict exampl ask 5 colleagu rate movi 5 well assum three rate 4 two gave 5 sinc major gave rate 4 final rate taken 4 consid take mode predict result max vote would someth like colleagu 1 5 colleagu 2 4 colleagu 3 5 colleagu 4 4 colleagu 5 4 finalr 4
57,['**Code in python**'],code python
58,"['there are 2 methods :\n', '\n', '1-Mode\n', '\n', '2-Voting classifier']",2 method 1 mode 2 vote classifi
59,"['**Majority Voting / Hard Voting\n', '**\n']",major vote hard vote
60,"['**Hard voting **\n', '\n', 'is the simplest case of majority voting. Here, we predict the class label y^ via majority (plurality) voting of each classifier\n', '\n', 'y^=mode{C1(x),C2(x),...,Cm(x)}\n', '\n', 'Assuming that we combine three classifiers that classify a training sample as follows:\n', '\n', 'classifier 1 -> class 0 classifier 2 -> class 0 classifier 3 -> class 1\n', '\n', 'y^=mode{0,0,1}=0\n', '\n', 'Via majority vote, we would we would classify the sample as ""class 0.""']",hard vote simplest case major vote predict class label via major plural vote classifi mode c1 x c2 x cm x assum combin three classifi classifi train sampl follow classifi 1 class 0 classifi 2 class 0 classifi 3 class 1 mode 0 0 1 0 via major vote would would classifi sampl class 0
61,"['**Soft Voting\n', '**\n']",soft vote
62,"['If all classifiers are able to estimate class probabilities (i.e. they have a predict_proba() method), then you can tell Scikit-Learn to predict the class with the highest class probability, averaged over all the individual classifiers.\n', '\n', 'This is called **soft voting** and it often achieves higher performance than hard voting because *it gives more weight to highly confident votes*.\n', '\n', ""To perform soft voting, all you need to do is **replace** voting='hard' with voting='soft' **and ensure that all classifiers can estimate class probabilities.\n"", '\n', ""**The SVC class can't estimate class probabilities by default**, so you'll need to set its probability hyperparameter to **True**, as this will make the SVC class use cross-validation to estimate class probabilities (which slows training down), and it will add a predict_proba() method.\n"", '\n', '**In soft voting**, we predict the class labels based on the predicted probabilities p for classifier -- this approach is only recommended if the classifiers are **well-calibrated**.\n', '\n', '*y^=argmaxi∑j=1mwjpij,* where **wj** is the weight that can be assigned to the **jth** classifier.\n', '\n', 'Assuming the example in the previous section was a *binary classification* task with class labels i∈{0,1}, our ensemble could make the following prediction:\n', '\n', 'C1(x)→[0.9,0.1]\n', '\n', 'C2(x)→[0.8,0.2]\n', '\n', 'C3(x)→[0.4,0.6]\n', '\n', 'Using uniform weights, we compute the average probabilities:\n', '\n', 'p(i0∣x)=0.9+0.8+0.43=0.7p(i1∣x)=0.1+0.2+0.63=0.3\n', '\n', 'y^=argmaxi[p(i0∣x),p(i1∣x)]=0\n', '\n', 'However, assigning the weights {0.1, 0.1, 0.8} would yield a prediction y^=1:\n', '\n', 'p(i0∣x)=0.1×0.9+0.1×0.8+0.8×0.4=0.49p(i1∣x)=0.1×0.1+0.2×0.1+0.8×0.6=0.51\n', '\n', 'y^=argmaxi[p(i0∣x),p(i1∣x)]=1']",classifi abl estim class probabl e predict proba method tell scikit learn predict class highest class probabl averag individu classifi call soft vote often achiev higher perform hard vote give weight highli confid vote perform soft vote need replac vote hard vote soft ensur classifi estim class probabl svc class estim class probabl default need set probabl hyperparamet true make svc class use cross valid estim class probabl slow train add predict proba method soft vote predict class label base predict probabl p classifi approach recommend classifi well calibr argmaxij 1mwjpij wj weight assign jth classifi assum exampl previou section binari classif task class label 0 1 ensembl could make follow predict c1 x 0 9 0 1 c2 x 0 8 0 2 c3 x 0 4 0 6 use uniform weight comput averag probabl p i0x 0 9 0 8 0 43 0 7p i1x 0 1 0 2 0 63 0 3 argmaxi p i0x p i1x 0 howev assign weight 0 1 0 1 0 8 would yield predict 1 p i0x 0 10 9 0 10 8 0 80 4 0 49p i1x 0 10 1 0 20 1 0 80 6 0 51 argmaxi p i0x p i1x 1
63,"['**Averaging**\n', '\n', 'Similar to the max voting technique, multiple predictions are made for each data point in averaging. In this method, we take an **average** of predictions from all the models and use it to make the final prediction.Averaging can be used for making predictions in regression problems or while calculating probabilities for classification problems.For example, in the below case, the averaging method would take the average of all the values.\n', '\n', '*i.e. (5+4+5+4+4)/5 = 4.4*']",averag similar max vote techniqu multipl predict made data point averag method take averag predict model use make final predict averag use make predict regress problem calcul probabl classif problem exampl case averag method would take averag valu e 5 4 5 4 4 5 4 4
64,['**Code in python**\n'],code python
65,"['**Weighted Average\n', '**']",weight averag
66,"['**This is an extension of the averaging method.** All models are assigned different weights defining the importance of each model for prediction. For instance, if two of your colleagues are critics, while others have no prior experience in this field, then the answers by these two friends are given more importance as compared to the other people.\n', '\n', 'The result is calculated as\n', '\n', '*[(50.23) + (40.23) + (50.18) + (40.18) + (4*0.18)] = 4.41.*']",extens averag method model assign differ weight defin import model predict instanc two colleagu critic other prior experi field answer two friend given import compar peopl result calcul 50 23 40 23 50 18 40 18 4 0 18 4 41
67,['![Imgur](https://i.imgur.com/J9drqs1.png)'],imgur http imgur com j9drqs1 png
68,['**Code in python**'],code python
69,['**Advanced Ensemble techniques**\n'],advanc ensembl techniqu
70,['**Bagging**'],bag
71,"['**Bagging** is very common in competitions. I don’t think I have ever seen anybody win without using it. But, in order for this to work, your data must have *variance*, otherwise you’re just adding levels after levels of additional iterations with **little benefit** to your score and a big headache for those maintaining your modeling pipeline in production. Even when it does improve things, you have to asked yourself if its worth all that extra work…']",bag common competit dont think ever seen anybodi win without use order work data must varianc otherwis your ad level level addit iter littl benefit score big headach maintain model pipelin product even improv thing ask worth extra work
72,"['In simple terms, **bagging irons out variance from a data set** . If, after splitting your data into multiple chunks and training them, you find that your predictions are *different*, then your data has *variance*. Bagging can turn a bad thing into a competitive advantage. For more theory behind the magic, check out *Bootstrap Aggregating on Wikipedia.* Bagging was invented by *Leo Breiman* at the University of California. He is also one of the grandfathers of Boosting and Random Forests.']",simpl term bag iron varianc data set split data multipl chunk train find predict differ data varianc bag turn bad thing competit advantag theori behind magic check bootstrap aggreg wikipedia bag invent leo breiman univers california also one grandfath boost random forest
73,['**Stability and Accuracy**\n'],stabil accuraci
74,"['By saving each prediction set and averaging them together, you not only lower variance without affecting bias, but your accuracy may be **improved**! In essence, you are creating many slightly different models and ensembling them together; **this avoids over-fitting**, **stabilizes your predictions and increases your accuracy**. Mind you, this assumes your data has variance, if it doesn’t,**bagging won’t help.**']",save predict set averag togeth lower varianc without affect bia accuraci may improv essenc creat mani slightli differ model ensembl togeth avoid fit stabil predict increas accuraci mind assum data varianc doesnt bag wont help
75,"['Bagging is based on the *statistical method of bootstrapping*, Bagging actually refers to (Bootstrap Aggregators). Most any paper or post that references using bagging algorithms will also reference Leo Breiman who wrote a paper in 1996 called “*Bagging Predictors*”.']",bag base statist method bootstrap bag actual refer bootstrap aggreg paper post refer use bag algorithm also refer leo breiman wrote paper 1996 call bag predictor
76,"['1-we make subsets with replacement: that means every item may appears in different subsets.\n', '\n', '2-apply model for every subset of the sample.\n', '\n', '3-The models run in parallel and are independent of each other.\n', '\n', '4-predict x-text by using each model\n', '\n', '5-then aggregate their predictions (either by voting or by averaging) to form a final prediction.']",1 make subset replac mean everi item may appear differ subset 2 appli model everi subset sampl 3 model run parallel independ 4 predict x text use model 5 aggreg predict either vote averag form final predict
77,['![Imgur](https://i.imgur.com/eu95V9N.png)'],imgur http imgur com eu95v9n png
78,['**Bagging algorithms:**\n'],bag algorithm
79,"['\n', '* Bagging meta-estimator\n', '* Random forest']",bag meta estim random forest
80,['**Bagging meta-estimator**'],bag meta estim
81,"['**Bagging meta-estimator** is an ensembling algorithm that can be used for **both** classification (BaggingClassifier) and regression (BaggingRegressor) problems. It follows the typical bagging technique to make predictions. Following are the steps for the bagging meta-estimator algorithm:\n', '\n', '1-Random subsets are created from the original dataset (Bootstrapping).\n', '\n', '2-The subset of the dataset includes all features.\n', '\n', '3-A user-specified base estimator is fitted on each of these smaller sets.\n', '\n', '4-Predictions from each model are combined to get the final result.']",bag meta estim ensembl algorithm use classif baggingclassifi regress baggingregressor problem follow typic bag techniqu make predict follow step bag meta estim algorithm 1 random subset creat origin dataset bootstrap 2 subset dataset includ featur 3 user specifi base estim fit smaller set 4 predict model combin get final result
82,['**Code in python**'],code python
83,"['where :\n', '\n', '**base_estimator:**\n', 'It defines the base estimator to fit on random subsets of the dataset. When nothing is specified, the base estimator is a decision tree.\n', '\n', '**n_estimators:**\n', 'It is the number of base estimators to be created. The number of estimators should be carefully tuned as a large number would take a very long time to run, while a very small number might not provide the best results.\n', '\n', '**max_samples:**\n', 'This parameter controls the size of the subsets. It is the maximum number of samples to train each base estimator.\n', '\n', '**max_features:**\n', 'Controls the number of features to draw from the whole dataset. It defines the maximum number of features required to train each base estimator.\n', '\n', '**n_jobs:**\n', 'The number of jobs to run in parallel. Set this value equal to the cores in your system. If -1, the number of jobs is set to the number of cores.\n', '\n', '**random_state:**\n', 'It specifies the method of random split. When random state value is same for two models, the random selection is same for both models. This parameter is useful when you want to compare different models.']",base estim defin base estim fit random subset dataset noth specifi base estim decis tree n estim number base estim creat number estim care tune larg number would take long time run small number might provid best result max sampl paramet control size subset maximum number sampl train base estim max featur control number featur draw whole dataset defin maximum number featur requir train base estim n job number job run parallel set valu equal core system 1 number job set number core random state specifi method random split random state valu two model random select model paramet use want compar differ model
84,['**Random Forest**\n'],random forest
85,"['**Random Forest** is another ensemble machine learning algorithm that follows the bagging technique. It is an extension of the bagging estimator algorithm. The base estimators in random forest are decision trees. Unlike bagging meta estimator, random forest **randomly** selects a set of features which are used to decide the best split at each node of the decision tree.']",random forest anoth ensembl machin learn algorithm follow bag techniqu extens bag estim algorithm base estim random forest decis tree unlik bag meta estim random forest randomli select set featur use decid best split node decis tree
86,"['step-by-step, this is what a random forest model does:\n', '\n', '1-Random subsets are created from the original dataset (bootstrapping).\n', '\n', '2-At each node in the decision tree, only a random set of features are considered to decide the best split.\n', '\n', '3-A decision tree model is fitted on each of the subsets. The final prediction is calculated by averaging the predictions from all decision trees.\n', '\n', '**Note:** The decision trees in random forest can be built on a subset of data and features. Particularly, the sklearn model of random forest uses all features for decision tree and a subset of features are randomly selected for splitting at each node.\n', '\n', '**To sum up, Random forest randomly selects data points and features, and builds multiple trees (Forest) .**']",step step random forest model 1 random subset creat origin dataset bootstrap 2 node decis tree random set featur consid decid best split 3 decis tree model fit subset final predict calcul averag predict decis tree note decis tree random forest built subset data featur particularli sklearn model random forest use featur decis tree subset featur randomli select split node sum random forest randomli select data point featur build multipl tree forest
87,['**Code in python**'],code python
88,"['You can see feature importance by using **model.featureimportances** in random forest.\n', '\n']",see featur import use model featureimport random forest
89,['**Sample code for regression problem:**\n'],sampl code regress problem
90,"['Parameters :\n', '\n', '**n_estimators:**\n', 'It defines the number of decision trees to be created in a random forest. Generally, a higher number makes the predictions stronger and more stable, but a very large number can result in higher training time.\n', '\n', '**criterion:**\n', 'It defines the function that is to be used for splitting. The function measures the quality of a split for each feature and chooses the best split.\n', '\n', '**max_features :**\n', 'It defines the maximum number of features allowed for the split in each decision tree. Increasing max features usually improve performance but a very high number can decrease the diversity of each tree.\n', '\n', '**max_depth:**\n', 'Random forest has multiple decision trees. This parameter defines the maximum depth of the trees. min_samples_split: Used to define the minimum number of samples required in a leaf node before a split is attempted. If the number of samples is less than the required number, the node is not split.\n', '\n', '**min_samples_leaf:**\n', 'This defines the minimum number of samples required to be at a leaf node. Smaller leaf size makes the model more prone to capturing noise in train data.\n', '\n', '**max_leaf_nodes:**\n', 'This parameter specifies the maximum number of leaf nodes for each tree. The tree stops splitting when the number of leaf nodes becomes equal to the max leaf node.\n', '\n', '**n_jobs:**\n', 'This indicates the number of jobs to run in parallel. S']",paramet n estim defin number decis tree creat random forest gener higher number make predict stronger stabl larg number result higher train time criterion defin function use split function measur qualiti split featur choos best split max featur defin maximum number featur allow split decis tree increas max featur usual improv perform high number decreas diver tree max depth random forest multipl decis tree paramet defin maximum depth tree min sampl split use defin minimum number sampl requir leaf node split attempt number sampl le requir number node split min sampl leaf defin minimum number sampl requir leaf node smaller leaf size make model prone captur nois train data max leaf node paramet specifi maximum number leaf node tree tree stop split number leaf node becom equal max leaf node n job indic number job run parallel
91,['**Boosting**\n'],boost
92,"['The term ‘Boosting’ refers to a family of algorithms which **converts weak learner to strong learners**. Boosting is an ensemble method for improving the model predictions of any given learning algorithm. The idea of boosting **is to train weak learners sequentially, each trying to correct its predecessor**.\n', '\n', 'Boosting is all about “*teamwork*”. Each model that runs, dictates what features the next model will focus on.']",term boost refer famili algorithm convert weak learner strong learner boost ensembl method improv model predict given learn algorithm idea boost train weak learner sequenti tri correct predecessor boost teamwork model run dictat featur next model focu
93,['**AdaBoost**\n'],adaboost
94,"['**Adaptive boosting or AdaBoost** is one of the simplest boosting algorithms. Usually, decision trees are used for modelling. Multiple sequential models are created, each correcting the errors from the last model. AdaBoost assigns weights to the observations which are incorrectly predicted and the subsequent model works to predict these values correctly.\n', '\n', '**steps:**\n', '\n', '1-all observations in the dataset are given equal weights.\n', '\n', '2-A model is built on a subset of data.\n', '\n', '3-Using this model, predictions are made on the whole dataset.\n', '\n', '4-Errors are calculated by comparing the predictions and actual values.\n', '\n', '5-While creating the next model, higher weights are given to the data points which were predicted incorrectly.\n', '\n', '6-Weights can be determined using the error value. For instance, higher the error more is the weight assigned to the observation.\n', '\n', '7-This process is repeated until the error function does not change, or the maximum limit of the number of estimators is reached.']",adapt boost adaboost one simplest boost algorithm usual decis tree use model multipl sequenti model creat correct error last model adaboost assign weight observ incorrectli predict subsequ model work predict valu correctli step 1 observ dataset given equal weight 2 model built subset data 3 use model predict made whole dataset 4 error calcul compar predict actual valu 5 creat next model higher weight given data point predict incorrectli 6 weight determin use error valu instanc higher error weight assign observ 7 process repeat error function chang maximum limit number estim reach
95,['**Code in python**'],code python
96,"['**Parameters**\n', '\n', '**base_estimators:**\n', 'It helps to specify the type of base estimator, that is, the machine learning algorithm to be used as base learner.\n', '\n', '**n_estimators:**\n', 'It defines the number of base estimators.\n', 'The default value is 10, but you should keep a higher value to get better performance.\n', '\n', '**learning_rate:**\n', 'This parameter controls the contribution of the estimators in the final combination.\n', 'There is a trade-off between learning_rate and n_estimators.\n', '\n', '**max_depth:**\n', 'Defines the maximum depth of the individual estimator.\n', 'Tune this parameter for best performance.\n', '\n', '**n_jobs:**\n', 'Specifies the number of processors it is allowed to use.\n', 'Set value to -1 for maximum processors allowed.\n', '\n', '**random_state :**\n', 'An integer value to specify the random data split.\n', 'A definite value of random_state will always produce same results if given with same parameters and training data.']",paramet base estim help specifi type base estim machin learn algorithm use base learner n estim defin number base estim default valu 10 keep higher valu get better perform learn rate paramet control contribut estim final combin trade learn rate n estim max depth defin maximum depth individu estim tune paramet best perform n job specifi number processor allow use set valu 1 maximum processor allow random state integ valu specifi random data split definit valu random state alway produc result given paramet train data
97,"['**stacking\n', '**']",stack
98,"['\n', 'Stacking is a similar to boosting:\n', '\n', ""you also apply several models to your original data. The difference here is, however, that you don't have just an empirical formula for your weight function, rather you introduce a meta-level and use another model/approach to estimate the input together with outputs of every model to estimate the weights or, in other words, to determine what models perform well and what badly given these input data. and finally I get its true illustration.""]",stack similar boost also appli sever model origin data differ howev empir formula weight function rather introduc meta level use anoth model approach estim input togeth output everi model estim weight word determin model perform well badli given input data final get true illustr
99,"['consider we have a dataset we splite our data set into 3 parts : training, validation , test']",consid dataset splite data set 3 part train valid test
100,['![Imgur](https://i.imgur.com/EQYa8C8.png)'],imgur http imgur com eqya8c8 png
101,['then make this step'],make step
102,['![Imgur](https://i.imgur.com/fVazCYe.png)'],imgur http imgur com fvazcy png
103,['![Imgur](https://i.imgur.com/2zjxVBC.png)'],imgur http imgur com 2zjxvbc png
104,['![Imgur](https://i.imgur.com/lpDhGd1.png)'],imgur http imgur com lpdhgd1 png
105,"['train algorythm 0 on **A** and make predictions for **B** and **C** and save to **B1,C1**\n', '\n', 'train algorythm 1 on **A** and make predictions for **B** and **C** and save to **B1,C1**']",train algorythm 0 make predict b c save b1 c1 train algorythm 1 make predict b c save b1 c1
106,['![Imgur](https://i.imgur.com/10slay8.png)'],imgur http imgur com 10slay8 png
107,['**At this moment we stacked predictions to each others thats where stacking name comes from** and then'],moment stack predict other that stack name come
108,"['train algorythm 2 on **A** and make predictions for **B** and **C** and save to **B1,C1**']",train algorythm 2 make predict b c save b1 c1
109,['![Imgur](https://i.imgur.com/hfp6JGP.png)'],imgur http imgur com hfp6jgp png
110,"['then we take the data from the validation set which we already knew and we are going to feed a new model .\n', '\n', 'train algorythm 3 on **B1** and make predictions for **C1**']",take data valid set alreadi knew go feed new model train algorythm 3 b1 make predict c1
111,['![Imgur](https://i.imgur.com/md3L8yB.png)'],imgur http imgur com md3l8yb png
112,['**Code in python**'],code python
113,['![Imgur](https://i.imgur.com/bfb2qlr.png)'],imgur http imgur com bfb2qlr png
114,['![Imgur](https://i.imgur.com/Bo4KItc.png)'],imgur http imgur com bo4kitc png
115,['![Imgur](https://i.imgur.com/hGkZd9T.png)'],imgur http imgur com hgkzd9t png
116,['I hope that I give you a piece of introduction of ensembling methods and this is not the end of my tutorial but this is only the first episode and I will continue soon illustrating the remaining methods of ensemlbing techniques.\n'],hope give piec introduct ensembl method end tutori first episod continu soon illustr remain method ensemlb techniqu
117,"['resources :\n', '\n', '\n', '[Google](https://www.google.com/webhp?hl=en&sa=X&ved=0ahUKEwiU0c_cgOLhAhUjQxUIHfetDCwQPAgH)\n', '    \n', '\n', '[Analytics videa](https://www.analyticsvidhya.com/)\n', '    \n', '\n', '[youtube](https://www.youtube.com/)\n', '    \n', '\n', '[wikipedia](https://www.wikipedia.org/)\n', '    \n', '  \n', '  \n', '  and a lot of other resources .\n', '    thanks a lot.']",resourc googl http www googl com webhp hl en sa x ved 0ahukewiu0c cgolhahujqxuihfetdcwqpagh analyt videa http www analyticsvidhya com youtub http www youtub com wikipedia http www wikipedia org lot resourc thank lot
118,"[""The data looks like it has had Principal Component Analysis applied to it, since each feature looks near Gaussian. Let's generate some data to see what PCA looks like on generated continuous and categorical data, and compare some plots to the Santander data.\n"", '\n', ""**Update**: CPMP suggested Truncated SVD, so we'll explore that too (https://www.kaggle.com/c/santander-customer-transaction-prediction/discussion/87301#503701)""]",data look like princip compon analysi appli sinc featur look near gaussian let gener data see pca look like gener continu categor data compar plot santand data updat cpmp suggest truncat svd explor http www kaggl com c santand custom transact predict discus 87301 503701
119,"['# Create random data\n', ""Let's create 1000 random variables which are correlated""]",creat random data let creat 1000 random variabl correl
120,['# PCA on continuous variables'],pca continu variabl
121,"[""Let's have a look at the distributions and cross plots""]",let look distribut cross plot
122,['# Truncated SVD on continuous variables'],truncat svd continu variabl
123,"['# PCA on categorical variables\n', ""Let's convert our continuous dataset to a discrete/categorical dataset by rounding to the nearest 100. Then see what the transformed data looks like""]",pca categor variabl let convert continu dataset discret categor dataset round nearest 100 see transform data look like
124,['So now there should only be a handful of categories per feature'],hand categori per featur
125,"['Even with a handful of categories per feature, after performing PCA, the cross plots still look like circular/spherical data clouds\n', '\n', '# Truncated SVD on categorical data']",even hand categori per featur perform pca cross plot still look like circular spheric data cloud truncat svd categor data
126,['# Comparing to the Santander dataset'],compar santand dataset
127,"['# Discussion\n', ""* By visually comparing the pair plots above with our data for both continuous and categorical features, they look pretty similar. Although we can't prove that PCA or SVD has been applied, we can't rule it out either\n"", '* Applying PCA or SVD on categorical data makes it hard to see that the data was originally categorical after the transform. For example, there are no bands/stripes in the cross plots to suggest discretisation. It looks pretty difficult to recover any categories from the transformed data\n', ""* PCA features typically have a mean close to zero. The Santander features do not. This might be a clue hinting that PCA wasn't applied to the data, or deliberate obfuscation by the organisers. SVD on the otherhand creates features with non-zero mean""]",discus visual compar pair plot data continu categor featur look pretti similar although prove pca svd appli rule either appli pca svd categor data make hard see data origin categor transform exampl band stripe cross plot suggest discretis look pretti difficult recov categori transform data pca featur typic mean close zero santand featur might clue hint pca appli data deliber obfusc organis svd otherhand creat featur non zero mean
128,"['# General information\n', '\n', 'In Santander Customer Transaction Prediction competition we have a binary classification task. Train and test data have 200k samples each and we have 200 anonimyzed numerical columns. It would be interesting to try good models without overfitting and knowing the meaning of the features.\n', ""In fact this competition seems to be similar to another current competition: don't overfit II, so I'll use a lot of ideas from my [kernel](https://www.kaggle.com/artgor/how-to-not-overfit).\n"", '\n', ""In this kernel I'll write the following things:\n"", '\n', '* EDA on the features and trying to get some insights;\n', '* Using permutation importance to select most impactful features;\n', '* Comparing various models: linear models, tree based models and others;\n', '* Trying various approaches to feature selection including taking top features from eli5;\n', '* Hyperparameter optimization for models;\n', '* Feature generation;\n', '* Other things;\n', '\n', '![](https://i.imgur.com/e5vPHpJ.png)\n', '\n', '*Work still in progress*']",gener inform santand custom transact predict competit binari classif task train test data 200k sampl 200 anonimyz numer column would interest tri good model without overfit know mean featur fact competit seem similar anoth current competit overfit ii use lot idea kernel http www kaggl com artgor overfit kernel write follow thing eda featur tri get insight use permut import select impact featur compar variou model linear model tree base model other tri variou approach featur select includ take top featur eli5 hyperparamet optim model featur gener thing http imgur com e5vphpj png work still progress
129,['## Data exploration'],data explor
130,"['From this overview we can see the following things:\n', '* target is binary and has disbalance: 10% of samples belong to 1 class;\n', '* values in columns are more or less similar;\n', '* columns have high std (up to 20)\n', '* columns have a high range of means;']",overview see follow thing target binari disbal 10 sampl belong 1 class valu column le similar column high std 20 column high rang mean
131,"[""Let's have a look at correlations now!""]",let look correl
132,"['We can see that all features have a low correlation with target. So we have no highly correlated features which we could drop, on the other hand we could drop some columns with have little correlation with the target.']",see featur low correl target highli correl featur could drop hand could drop column littl correl target
133,['## Basic modelling'],basic model
134,['## ELI5'],eli5
135,"[""ELI5 didn't help up to eliminate features, but let's at least try to take top-100 and see how it helps.""]",eli5 help elimin featur let least tri take top 100 see help
136,['## Feature generation'],featur gener
137,"['### Feature interaction\n', '\n', ""Didn't improve score""]",featur interact improv score
138,"['### Scaling\n', '\n', '! **Notice** scaling severely decreases score']",scale notic scale sever decreas score
139,['### Statistics'],statist
140,['Training with these features gives the same score on LB: 0.899'],train featur give score lb 0 899
141,"['### NN features\n', '\n', 'Takes several hours.']",nn featur take sever hour
142,['## Blend'],blend
143,['### Rounding data'],round data
144,"['----\n', '\n', '![](data:image/png;base64,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)\n', '\n', '-----\n', '\n', '# ***OUTLINE OF THE NOTEBOOK***\n', '\n', '------\n', '\n', '* **Step:** [**1.Load Libraries**](#1.Load-Libraries)\n', '* **Step:** [**2.Read Dataset**](#2.Read-Dataset)\n', '* **Step:** [**3. Display Dataset**](#3.-Display-Dataset*)\n', '* **Step:** [**4.Remove unwanted columns**](#4.Remove-unwanted-columns)\n', '* **Step:** [**5.Create Instance of Feature Selector**](#5.Create-Instance-of-Feature-Selector)\n', '* **Step:** [**6.Missing Value**](#6.Missing-Value)\n', '* **Step:** [**7.Single Unique Value**](#7.Single-Unique-Value)\n', '* **Step:** [**8.Plot Feature Importances**](#8.Plot-Feature-Importances)\n', '* **Step:** [**9.Low Importance Features**](#9.Low-Importance-Features)\n', '* **Step:** [**10.Removing Features**](#10.Removing-Features)\n', '* **Step:** [**11.Handling One-Hot Features**](#11.Handling-One-Hot-Features)\n', '* **Step:** [**12.Model Training**](#12.Model-Training)\n', '\n', '------\n', '\n', '## **1.Load Libraries**']",data imag png base64 ivborw0kggoaaaansuheugaaaeqaaabocamaaaaxdxpcaaaakfbmvexsaad tfbt pz5wmd94ul 9pt83t7untxzbm796enzenr6y8vtaadxu1p 9fxycxhukyv1j4 xyml71dx60ndxagjvozvtiyp4trbumth96oj0hob 7 xx1 1lzx3pkttdq35urrvret6ymj0iynwvlbyehj1kplthbz3rkz3qkjwtu3vquh2nz3tjyeb5 gvaaajv0leqvr4no2c6zqiohcgcqgotkltgguiu61j6 3f3sfvcssiac8 tp56f8yajdhmq2xvoq2diaiciaiciaiciaiciaiciaiciaiciaiciaicieoym4 uaxfvpppxo6ta3js46dy6cssdivhapfnvwtf2ehqdidvdgn ug7efwmmisfdr9ecuctczhz8dc2i 8jftsjh14k4kzzx e3rtsduyoylzkjhv pxwe6tr1fht9btp 8cqorg ejk c7dn4vvhl2jv0di3vl pvdzfnrtfphtguygetngbpa1 68birprv7j trsp0 vfdih29gh ihizxmmctd7y16bl8dg1maul3cxmr22y28fqfzdiyjqxplyeqribu2xckwcczoz071jjb3dagyzpiiortb0wnjggwu5n2wp8am83 evmafn8 9xxuzrizmrn22gbl2lafr8h10yzhqlfiv9pumgmxv3dgfw nmnkj2lljemxrp7t7hldehg9tm5cyxnr ciuqil xwbxwv7hzra78u5knhzppplndg6j idzyu d4h3066vhkpcbfmofjx5b5p1vb0hizmkkdlxqcdmhj1h sha tgzt7apgz0u sm1piyhaiks2oetbzn4xxxmvqesv9lhonn5aezu1flpeari5a4do2x35psqbv cakqv9zrys5yojrilfgyh0xhyi5rkvvqhyd9uh7i ozpxvtfg8tyl nsnblcyv1qdwqeos6vdlycqvwawfmn6qcpt06vxt achdjrw63v9rhic16zkvoi4xj 3iptohgzatpn4keaqjw1jucbysf5ws2vdlvhcl5jsanyvg ovftsncxp5iss3ev7foenfplevrly6h27sof5pfzxv9ch cxqdj5islof oohiguenzym n yxpns8 lqrzvhuc3m47e3acezrwqi8txcdiahid388e3kx9sfpq6931hterpg4npzzacx xfrrt8bcd4vkwu8xwrsevxztnmfzx1ibhvkse1vy2a k434e7jdfbrgtpjbtv9wia0xh6dzjxzqp43061fhl4yowd wjhxsijfuwjjbpikcopomfb vs0wbpmjpcyvn6j8bp2oexax7uy9tlezkv fo5jtyfwww jdn vh9m22birhhptyoybth2rrquos1mtduxhqudzdyascdptioorpcug0u0h cg2ovqz1b8309h69bxtgseqbyzjzgkgsyxx6o2m3nybhrps8tv2kcetp5jkwfdrfdl7pk5wvgzudqckxsyheorxx9iflop0 zynbzcolqastkc4ijqx mmi3wybnzksrhdy4hn7pvngkifbpm08shvheeah0i8musr i9e3gayeebnjqvuogq11zgu9kfrkle7u7xlphneov1aztjrkqjebi ly4ehtpa6cbtorzordl4m97r0dc2ne8v2anaj8sbpub5huiolz3tplrmxf3nhxmsij2pfe9i4olyibuejz56 39k3oossh8mdubukuasxbq8qkosxms6k5 ra4e58 00yjon 9gpm k2kpk i2ojv4fke92vgm0j8xnogdackttoecoyrz03irg0vatb3ph1z4r5xozjuzu6boo mu6f44ezy95bw7kqj4oetiahvdd2a5rn9esocgyt7m6uiakyrsuoxovtfj4yin1iuy1ypividztdmdkj0re3feuxwc182vyujvapnfyslie9usffngyjxclqg vg6jbiti zkacosgfim1vidzhs7jwvh9ywdxzpqriedkbwcldah45ggsifm1f zscyjswrtweea3sfiulrc4 5paulz302nlj9qortwu1kwnrpcholvi7f6jg0fsrc tjbaes1xk59icyor1desqbuois5lb4t0s3mpkkuph8nshgvsfu3soyljtvquuvgr66pq0xn svjxi2thablu90wwrwgwlysfvrk5zxjlzq3xrzsexw 3qlye6fyepvwhcylyayevhdgd uvohchszgtboyrb22vwhz kqf1y0ij 20 7mdg0vuvokm6fk6apxpriyi1kf8pgy05 v8vuzy4cxlnpihrboa6jv nccb eumh1m16e1z8nwry19 eymrdcwdwwrz2zmdze1lbswfggxkhmg7pkcvinbxe5hd9bnrbt 7jzfvhrwrtrhvyheqryvymrcg8jbfosrq34mz47hqmiyu6iko41zkh4jm wow2mhfkvlzo9znci vtfle tpzrlzwihmaoihjjmunoldmqihsltpso l3ordg1zj3soy ablitcfb6fkv16vjxkpw0uwnp 7mywfbkhflnjxo4gmrko7st lf0sgh3ncglw20pjufrmddcp4ryq8hie9slz5mzflhnosxwudkp5zoftkqpfw abqup qwsyswbzq81beaviyouyh 3qtobgjq7smmuriwxwrl0u4ru9ibwqszt3w1sxvuej2mbf2ejbq1uzpim1ldqiw pviiwvcutv74dlym4r5irlr5uniixbc4mcn9y5mbezy1rgku6olzndswwuwwxax75ta 58gmlxjslnyqe9hzpguk8jw7habqbott8a4wisjwmwcwlftxspofmtqnnpivbx1ziamv w1hag1x2tgr eymz7xxb4nkakaoxdqxamr5svrq25tvslr5e52qway2qqk5suwzbz5ifosezjz0tssuz4n0pmnltk54x4k7z1umr6y5qbfufsh lrajqauxykbxch8jir5fz63mrsmr a9xnx3g16lsaj22uelf1005fn f7 pundtzn7rwzz3r3whe326sz8ie7zmddk0463r7e6gevydpchvnrq7bozddzaxm7cpg7sttjea1galptljudoi3nhypmiyccoffqjmefe2g fw7b 8kvhjnh0hu5qgvxkohdbju4u nwbbs1w07lhddywb xgikxl4lqxt1xz4k1syoklm9wt59nyvjoo5mau2fk6ohphggxvvufqkcd9qclx44fouajtetorb1ld8wn2hfdvuikyr4z38ofhnwevi6um97x ulc57zcfjhwc ryi9gqenh8jiossouibtoygirbyhj22njodmzeodnnticu4o7raees6s42int4ub3ja0fovfxm2oihqof6nyuv2lt99gm0y8zvly3gv6v5qcmhctqrp1hpprpownb15xmxlxn5klbqpzet5iaei2umhserwinkjgiaigiaigiaigiaigiaigiaigiaigiaigiaigiaigiaigiaigiaigibeg 8aocmkbj6tpr8aaaaasuvork5cyii outlin notebook step 1 load librari 1 load librari step 2 read dataset 2 read dataset step 3 display dataset 3 display dataset step 4 remov unwant column 4 remov unwant column step 5 creat instanc featur selector 5 creat instanc featur selector step 6 miss valu 6 miss valu step 7 singl uniqu valu 7 singl uniqu valu step 8 plot featur import 8 plot featur import step 9 low import featur 9 low import featur step 10 remov featur 10 remov featur step 11 handl one hot featur 11 handl one hot featur step 12 model train 12 model train 1 load librari
145,"['## **2.Read Dataset**\n', '\n', '* We can see that our dataset contain **200000 Rows** and **202 Columns** with **target columns**\n', '* Here I tried to take **150000 Rows** for testing purpose.']",2 read dataset see dataset contain 200000 row 202 column target column tri take 150000 row test purpos
146,['## **3. Display Dataset**'],3 display dataset
147,['## **4.Remove unwanted columns**'],4 remov unwant column
148,['## **5.Create Instance of Feature Selector**'],5 creat instanc featur selector
149,"['## **6.Missing Value**\n', '\n', 'The first feature selection method is straightforward: find any columns with a missing fraction greater than a specified threshold. For this example we will use a threhold of 0.6 which corresponds to finding features with more than 60% missing values. (This method does not one-hot encode the features first).']",6 miss valu first featur select method straightforward find column miss fraction greater specifi threshold exampl use threhold 0 6 correspond find featur 60 miss valu method one hot encod featur first
150,['The features identified for removal can be accessed through the ops dictionary of the FeatureSelector object.'],featur identifi remov access op dictionari featureselector object
151,['## **7.Single Unique Value**'],7 singl uniqu valu
152,['Running the gradient boosting model requires one hot encoding the features. These features are saved in the one_hot_features attribute of the FeatureSelector. The original features are saved in the base_features.'],run gradient boost model requir one hot encod featur featur save one hot featur attribut featureselector origin featur save base featur
153,"['## **8.Plot Feature Importances**\n', '\n', 'The feature importance plot using plot_feature_importances will show us the plot_n most important features (on a normalized scale where the features sum to 1). It also shows us the cumulative feature importance versus the number of features.\n', '\n', 'When we plot the feature importances, we can pass in a threshold which identifies the number of features required to reach a specified cumulative feature importance. For example, threshold = 0.99 will tell us the number of features needed to account for 99% of the total importance.\n', '\n']",8 plot featur import featur import plot use plot featur import show u plot n import featur normal scale featur sum 1 also show u cumul featur import versu number featur plot featur import pas threshold identifi number featur requir reach specifi cumul featur import exampl threshold 0 99 tell u number featur need account 99 total import
154,"[""We could use these results to select only the 'n' most important features. For example, if we want the top 100 most importance, we could do the following.""]",could use result select n import featur exampl want top 100 import could follow
155,"['## **9.Low Importance Features**\n', '\n', 'This method builds off the feature importances from the gradient boosting machine (identify_zero_importance must be run first) by finding the lowest importance features not needed to reach a specified cumulative total feature importance. For example, if we pass in 0.99, this will find the lowest important features that are not needed to reach 99% of the total feature importance.\n', '\n', 'When using this method, we must have already run identify_zero_importance and need to pass in a cumulative_importance that accounts for that fraction of total feature importance.\n', '\n', '**Note of caution:** this method builds on the gradient boosting model features importances and again is non-deterministic. I advise running these two methods several times with varying parameters and testing each resulting set of features rather than picking one number and sticking to it.']",9 low import featur method build featur import gradient boost machin identifi zero import must run first find lowest import featur need reach specifi cumul total featur import exampl pas 0 99 find lowest import featur need reach 99 total featur import use method must alreadi run identifi zero import need pas cumul import account fraction total featur import note caution method build gradient boost model featur import non determinist advis run two method sever time vari paramet test result set featur rather pick one number stick
156,['The low importance features to remove are those that do not contribute to the specified cumulative importance. These are also available in the ops dictionary.'],low import featur remov contribut specifi cumul import also avail op dictionari
157,"['## **10.Removing Features**\n', '\n', 'Once we have identified the features to remove, we have a number of ways to drop the features. We can access any of the feature lists in the removal_ops dictionary and remove the columns manually. We also can use the remove method, passing in the methods that identified the features we want to remove.\n', '\n', 'This method returns the resulting data which we can then use for machine learning. The original data will still be accessible in the data attribute of the Feature Selector.\n', '\n', ""**Be careful of the methods** used for removing features! It's a good idea to inspect the features that will be removed before using the remove function.""]",10 remov featur identifi featur remov number way drop featur access featur list remov op dictionari remov column manual also use remov method pas method identifi featur want remov method return result data use machin learn origin data still access data attribut featur selector care method use remov featur good idea inspect featur remov use remov function
158,"[""To remove the features from all of the methods, pass in method='all'. Before we do this, we can check how many features will be removed using check_removal. This returns a list of all the features that have been idenfitied for removal.""]",remov featur method pas method check mani featur remov use check remov return list featur idenf remov
159,['Now we can remove all of the features idenfitied.'],remov featur idenf
160,"['## **11.Handling One-Hot Features**\n', '\n', 'If we look at the dataframe that is returned, we may notice several new columns that were not in the original data. These are created when the data is one-hot encoded for machine learning. To remove all the one-hot features, we can pass in keep_one_hot = False to the remove method.']",11 handl one hot featur look datafram return may notic sever new column origin data creat data one hot encod machin learn remov one hot featur pas keep one hot fals remov method
161,"['## **12.Model Training**\n', '\n', '* Model is Binary Classification so used Catboost because **we have dataset size is more than 100000 Samples..**']",12 model train model binari classif use catboost dataset size 100000 sampl
162,"['Hi everyone! \n', '\n', ""Here is another take on trying to get the most out of the data when the features are assumed to be independent. The idea is to fit 200 classifiers, each using only one of the features, and then combining the predictions using Bayes' rule as in Naive Bayes methods. I've previously experimented by using kernel density estimates to obtain the individual classifiers as in Chris Deotte's kernel [here](http://www.kaggle.com/cdeotte/modified-naive-bayes-santander-0-899), but achieved better results when the individual classifiers where gradient boosted trees. Enjoy!""]",hi everyon anoth take tri get data featur assum independ idea fit 200 classifi use one featur combin predict use bay rule naiv bay method previous experi use kernel densiti estim obtain individu classifi chri deott kernel http www kaggl com cdeott modifi naiv bay santand 0 899 achiev better result individu classifi gradient boost tree enjoy
163,"[""One could of course try to tune the LGBM parameters for each feature individually, but that's just too tedious. So I prayed that the same parameters work for every feature as long as I allow the number of number of trees to vary for each feature. The following function estimates the optimal number of rounds for each feature.""]",one could cours tri tune lgbm paramet featur individu tediou pray paramet work everi featur long allow number number tree vari featur follow function estim optim number round featur
164,"['It is important that the metric used for the early stopping is the binary logloss (or something similar) and not the AUC score! For many features, the probability function to predict seem to be monotonic. That is, the larger the value of that feature the more likely that the class value is 1 (or 0). In these cases, **any** monotonic predictor will have the same (and best possible) AUC score, since all induce the same ranking of the observations. But a random monotonic function is not good for anything, since it is crucial for Naive Bayes is to estimate the individual conditional probabilities as accurately as possible.']",import metric use earli stop binari logloss someth similar auc score mani featur probabl function predict seem monoton larger valu featur like class valu 1 0 case monoton predictor best possibl auc score sinc induc rank observ random monoton function good anyth sinc crucial naiv bay estim individu condit probabl accur possibl
165,"['As we can see, the optimal number of rounds varies greatly. \n', '\n', ""There are features that require quite a long training. For example, it seems that the distibution of var_108 has a sharp spike somewhere at the middle and that takes a long time to take into account. (See the end of this notebook for a figure.) The kernel density estimate in Chris's [kernel](http://www.kaggle.com/cdeotte/modified-naive-bayes-santander-0-899) does seem to see this spike very well, so this might illustrate why this version of Naive Bayes performs slightly better.\n"", '\n', ""Interestingly, there are also features where the optimal number of trees is less then 10, indicating that those features might be very weak predictors (or perhaps that the chosen LGBM parameters just don't work well for those features).""]",see optim number round vari greatli featur requir quit long train exampl seem distibut var 108 sharp spike somewher middl take long time take account see end notebook figur kernel densiti estim chri kernel http www kaggl com cdeott modifi naiv bay santand 0 899 seem see spike well might illustr version naiv bay perform slightli better interestingli also featur optim number tree le 10 indic featur might weak predictor perhap chosen lgbm paramet work well featur
166,"['Next, we implement our LGB Naive Bayes classifier.']",next implement lgb naiv bay classifi
167,['Performing 5-fold CV.'],perform 5 fold cv
168,['Creating submission:'],creat submiss
169,"['Finally, here is the prediction plot for var_108. The spike in the middle is nicely recognized.']",final predict plot var 108 spike middl nice recogn
170,"['## Abstract\n', '\n', ""The aim of this notebook is to check whether train and test sets are significantly different. Can we trust our local validation schemas and public LB? I'll use adversarial validation and Kolmogorov-Smirnov Test for these purposes.""]",abstract aim notebook check whether train test set significantli differ trust local valid schema public lb use adversari valid kolmogorov smirnov test purpos
171,['### Adversarial Validation'],adversari valid
172,"['Average AUC across folds is stable and concentrates around 0.5. It means that we can hardly distinguish train set from test set using adversarial validation.\n', '\n', ""Now let's expand our investigation of dataset and look at distribution of features in train and test sets with respect to [Kolmogorov-Smirnov test](https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test).""]",averag auc across fold stabl concentr around 0 5 mean hardli distinguish train set test set use adversari valid let expand investig dataset look distribut featur train test set respect kolmogorov smirnov test http en wikipedia org wiki kolmogorov e2 80 93smirnov test
173,['### Kolmogorov-Smirnov Test'],kolmogorov smirnov test
174,"[""As we can see, 185 features successfully passed Kolmogorov-Smirnov test. We cannot reject null hypothesis that those features in train and test sets came from the same distribution. 15 features haven't passed this test and probably require our attention.""]",see 185 featur success pas kolmogorov smirnov test reject null hypothesi featur train test set came distribut 15 featur pas test probabl requir attent
175,"['## Conslusion:\n', '\n', 'From adversarial validation we have no evidence that train and test sets come from different distributions. AUC around 0.50 states that LGBM can hardly distinguish train observations from test. These datasets are quite similar. Local validation schemas and public LB track should correctly reflect your efforts in this competition.\n', '\n', 'From Kolmogorov-Smirnov Test we can also state that both sets are quite similar. Hypothesis that samples are drawn from the same distribution can be rejected only for 15 out of 200 features based on KS-Test. Probably, we should pay more attention to those 15 features.']",conslus adversari valid evid train test set come differ distribut auc around 0 50 state lgbm hardli distinguish train observ test dataset quit similar local valid schema public lb track correctli reflect effort competit kolmogorov smirnov test also state set quit similar hypothesi sampl drawn distribut reject 15 200 featur base k test probabl pay attent 15 featur
176,"['## Abstract\n', 'The aim of this notebook is to look at target distribution versus all variables. Many people have already done similar job, so, first of all, I kindly invite you to check their efforts:\n', '* [Modified Naive Bayes - Santander - [0.899]](https://www.kaggle.com/cdeotte/modified-naive-bayes-santander-0-899)\n', '* [Gaussian Naive Bayes](https://www.kaggle.com/blackblitz/gaussian-naive-bayes)\n', '* [Fast PDF calculation with correlation matrix](https://www.kaggle.com/jiweiliu/fast-pdf-calculation-with-correlation-matrix)\n', '* [Are vars mixed up time intervals?](https://www.kaggle.com/sibmike/are-vars-mixed-up-time-intervals)\n', '* [Boosting creativity towards feature engineering](https://www.kaggle.com/felipemello/boosting-creativity-towards-feature-engineering)\n', '\n', 'Yet many results have already been shown, there is always place for exploration. In this kernel I will try to look at variables from machines perspective.']",abstract aim notebook look target distribut versu variabl mani peopl alreadi done similar job first kindli invit check effort modifi naiv bay santand 0 899 http www kaggl com cdeott modifi naiv bay santand 0 899 gaussian naiv bay http www kaggl com blackblitz gaussian naiv bay fast pdf calcul correl matrix http www kaggl com jiweiliu fast pdf calcul correl matrix var mix time interv http www kaggl com sibmik var mix time interv boost creativ toward featur engin http www kaggl com felipemello boost creativ toward featur engin yet mani result alreadi shown alway place explor kernel tri look variabl machin perspect
177,"['## Charts and cool stuff\n', 'It is not surprise that current **.900** score can be achieved by using LGBM with shallow (only 2 or 3 leaves) trees. More investigation into this area shows that standard **max_bins=255** parameter can be reduced down to **max_bins=25** without any loss in quality. It means that 25 possible points for split is more than enough for this data. (i.e. algorithm sees no point in complicated splits like top 0.5% values vs 99.5% rest). However, the tails of the variables is the most interesting part in this task. Usually they have increased number of occurrences of the positive class. \n', '\n', ""Let's look how positive class is distributed across variables.""]",chart cool stuff surpris current 900 score achiev use lgbm shallow 2 3 leav tree investig area show standard max bin 255 paramet reduc max bin 25 without loss qualiti mean 25 possibl point split enough data e algorithm see point complic split like top 0 5 valu v 99 5 rest howev tail variabl interest part task usual increas number occurr posit class let look posit class distribut across variabl
178,"['### Three types of distribution\n', 'As we can see target seems to have 3 types of distributions across variables.\n', '\n', '### 1st type. Nice and cosy exponential-like monotonous-like distribution.\n', ""These type of distribution seems to growth (or decrease) exponentially as variable increases in value. Probably, that's where models took most information from. It is easy to build good separation rules with these varaibles. Good examples are - **var_34** and **var_40**.""]",three type distribut see target seem 3 type distribut across variabl 1st type nice cosi exponenti like monoton like distribut type distribut seem growth decreas exponenti variabl increas valu probabl model took inform easi build good separ rule varaibl good exampl var 34 var 40
179,"['### 2nd type. Saw-like distribution.\n', 'This type of distribution represents contradictory information. It is concentrated around average target value. Consecutive bin values tend to vary a lot. Unlikely this type of variables presents any usefull information to the model. Moreover, I did local experiment where I dropped 30 saw-like variables and still got **.900** score on 5-folds CV. Good examples are - **var_29** and **var_38**.']",2nd type saw like distribut type distribut repres contradictori inform concentr around averag target valu consecut bin valu tend vari lot unlik type variabl present useful inform model moreov local experi drop 30 saw like variabl still got 900 score 5 fold cv good exampl var 29 var 38
180,"['### 3rd type. Nice and cosy with an outlier.\n', ""Probably this is the most interesting type of distribution. It behaves like 1st type, but has a spike somewhere near median. This can heavily affect LGBM ability to identify optimal split. I don't know why this pattern cannot be seen in Chris's kernel. Some variables has enourmous outliers in the center. Good examples are - **var_80** and **var_108**.""]",3rd type nice cosi outlier probabl interest type distribut behav like 1st type spike somewher near median heavili affect lgbm abil identifi optim split know pattern seen chri kernel variabl enourm outlier center good exampl var 80 var 108
181,"['### 4th type. (Arbitrary)\n', 'Whereas there are three clear patterns, someone might observe other patterns. For example, some features are stable for the first 50% of values and then start behave like 1st type. (or it might be binning effect).']",4th type arbitrari wherea three clear pattern someon might observ pattern exampl featur stabl first 50 valu start behav like 1st type might bin effect
182,"['## Closing points\n', '* Score is already high. Decision trees are capable of capturing higher target distribution in tails on their own. No point in trying to generate any features like quantile, quintile, rounding etc.\n', ""* Despite the fact that dropping saw-like features didn't affect the score, it is still question whether these features might be usefull for feature engineering.\n"", '* Discussions [here](https://www.kaggle.com/c/santander-customer-transaction-prediction/discussion/80996) and [here](https://www.kaggle.com/c/santander-customer-transaction-prediction/discussion/86736) indicates that one way to increase score (at least up to **.910** level) is to create additional 200 features. (i.e. apply some kind of transformation to original 200 features).\n', '* People who broke out of **.901** club also state that they observed weird patterns of [local CV increase whereas public LB stayed on the same level](https://www.kaggle.com/c/santander-customer-transaction-prediction/discussion/80996#502449). What kind of transformation can lead to such results?\n', '* No target encoding reportedly been used. \n', '* What if vars are time series and FE might be related to order and aggregate features based on their target distribution plot?\n']",close point score alreadi high decis tree capabl captur higher target distribut tail point tri gener featur like quantil quintil round etc despit fact drop saw like featur affect score still question whether featur might useful featur engin discus http www kaggl com c santand custom transact predict discus 80996 http www kaggl com c santand custom transact predict discus 86736 indic one way increas score least 910 level creat addit 200 featur e appli kind transform origin 200 featur peopl broke 901 club also state observ weird pattern local cv increas wherea public lb stay level http www kaggl com c santand custom transact predict discus 80996 502449 kind transform lead result target encod reportedli use var time seri fe might relat order aggreg featur base target distribut plot
183,"[""This is based on the [Porto Seguro winner's solution](https://www.google.com/search?q=porto+seguro+winn+kaggle&oq=porto+seguro+winn+kaggle&aqs=chrome..69i57j69i60l3.10900j0j4&sourceid=chrome&ie=UTF-8).\n"", '\n', '[Autoencoder](https://alanbertl.com/autoencoder-with-fast-ai/)\n', '\n', '[Hook](https://nbviewer.jupyter.org/github/fastai/course-v3/blob/master/nbs/dl1/lesson6-pets-more.ipynb)\n', '\n', '[Fast.ai](https://nbviewer.jupyter.org/github/fastai/course-v3/blob/master/nbs/dl1/lesson5-sgd-mnist.ipynb)']",base porto seguro winner solut http www googl com search q porto seguro winn kaggl oq porto seguro winn kaggl aq chrome 69i57j69i60l3 10900j0j4 sourceid chrome ie utf 8 autoencod http alanbertl com autoencod fast ai hook http nbviewer jupyt org github fastai cours v3 blob master nb dl1 lesson6 pet ipynb fast ai http nbviewer jupyt org github fastai cours v3 blob master nb dl1 lesson5 sgd mnist ipynb
184,['# Import modules'],import modul
185,['# Import dataset and concatenate it'],import dataset concaten
186,['# Rank Guass'],rank guass
187,['# Preprocess for Neural Net'],preprocess neural net
188,['to see each batch size'],see batch size
189,['# DAE'],dae
190,['# Hook to extract activations'],hook extract activ
191,['# First batch '],first batch
192,['# Second batch'],second batch
193,['# Third batch'],third batch
194,['# Fourth batch'],fourth batch
195,"['# Introduction\n', '\n', 'In this kernel, we attempt to improve the [Gaussian naive Bayes classifier](https://www.kaggle.com/blackblitz/gaussian-naive-bayes) by replacing the Gaussian model with the more flexible Gaussian mixture model.\n', '\n', 'We implement the Gaussian mixture naive Bayes model to predict Santander Customer Transaction Prediction data. The problem has a binary target and 200 continuous features, and we assume that these features are conditionally independent given the class. We model the target $Y$ as Bernoulli, taking values $0$ (negative) and $1$ (positive). The features $X_0,X_1,\\ldots,X_{199}$ are modelled as continuous random variables. Recall the Bayes rule:\n', '\n', ""$$p_{Y|X_0,X_1,\\ldots,X_{199}}(y|x_0,x_1,\\ldots,x_{199})=\\frac{p_Y(y)\\prod_{i=0}^{199}f_{X_i|Y}(x_i|y)}{\\sum_{y'=0}^1p_Y(y')\\prod_{i=0}^{199}f_{X_i|Y}(x_i|y')}$$\n"", '\n', 'The prior $p_Y(y)$ will be taken as the proportion of the two classes, and the likelihood $f_{X_i|Y}(x_i|y)$ will be obtained by fitting the data with the Gaussian mixture model.']",introduct kernel attempt improv gaussian naiv bay classifi http www kaggl com blackblitz gaussian naiv bay replac gaussian model flexibl gaussian mixtur model implement gaussian mixtur naiv bay model predict santand custom transact predict data problem binari target 200 continu featur assum featur condit independ given class model target bernoulli take valu 0 neg 1 posit featur x 0 x 1 ldot x 199 model continu random variabl recal bay rule p x 0 x 1 ldot x 199 x 0 x 1 ldot x 199 frac p prod 0 199 f x x sum 0 1p prod 0 199 f x x prior p taken proport two class likelihood f x x obtain fit data gaussian mixtur model
196,"['# Getting Acquainted with the Gaussian Mixture Model\n', 'The Gaussian mixture model gives a mixture of normal distrubutions. We can use [sklearn.mixture.GaussianMixture](https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html) to fit the data and compare it with the histogram to get a feel of its behaviour. We also need to standardize the features because too narrow data can impair the fitting ability of the Gaussian mixture model. There are two important hyperparameters: `n_components` is the number of normal distributions to mix in and `reg_covar` is a regularization parameter that controls the spread of the bumps. Note that `score_samples` method gives the log density, so we need to exponentiate to get the density.']",get acquaint gaussian mixtur model gaussian mixtur model give mixtur normal distrubut use sklearn mixtur gaussianmixtur http scikit learn org stabl modul gener sklearn mixtur gaussianmixtur html fit data compar histogram get feel behaviour also need standard featur narrow data impair fit abil gaussian mixtur model two import hyperparamet n compon number normal distribut mix reg covar regular paramet control spread bump note score sampl method give log densiti need exponenti get densiti
197,"['# Implementing the Model\n', '\n', 'We are going to use the Gaussian mixture model to estimate the likelihood probability density functions $f_{X_i|Y}(x_i|y)$. Since multiplying a lot of small numbers will lead to underflow, we take the logarithm and turn products into sums:\n', ""$$\\ln p_{Y|X_0,X_1,\\ldots,X_{199}}(y|x_0,x_1,\\ldots,x_{199})=\\underbrace{\\overbrace{\\ln p_Y(y)}^\\text{log prior}+\\overbrace{\\sum_{i=0}^{199}\\ln f_{X_i|Y}(x_i|y)}^\\text{log likelihood}}_\\text{log joint}-\\overbrace{\\ln\\sum_{y'=0}^1e^{\\ln p_Y(y')+\\sum_{i=0}^{199}\\ln f_{X_i|Y}(x_i|y')}}^\\text{log marginal}$$\n"", '\n', 'Key points in the implementation are:\n', '* The log prior is set as the logarithm of the proportion of different classes.\n', ""* The log likelihood is computed by using [sklearn.mixture.GaussianMixture](https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html)'s `score_samples` method.\n"", '* Computing the log marginal is prone to overflow/underflow, so we use [scipy.special.logsumexp](https://docs.scipy.org/doc/scipy/reference/generated/scipy.special.logsumexp.html) to avoid that.\n', '* In the end, we convert back to probability by exponentiation.\n', '\n', 'All the heavy lifting is done by the Gaussian mixture model. The rest of the computation is very simple and fast.']",implement model go use gaussian mixtur model estim likelihood probabl densiti function f x x sinc multipli lot small number lead underflow take logarithm turn product sum ln p x 0 x 1 ldot x 199 x 0 x 1 ldot x 199 underbrac overbrac ln p text log prior overbrac sum 0 199 ln f x x text log likelihood text log joint overbrac ln sum 0 1e ln p sum 0 199 ln f x x text log margin key point implement log prior set logarithm proport differ class log likelihood comput use sklearn mixtur gaussianmixtur http scikit learn org stabl modul gener sklearn mixtur gaussianmixtur html score sampl method comput log margin prone overflow underflow use scipi special logsumexp http doc scipi org doc scipi refer gener scipi special logsumexp html avoid end convert back probabl exponenti heavi lift done gaussian mixtur model rest comput simpl fast
198,"['# Training and Evaluating the Model\n', '\n', 'We train and evaluate the model by using the training AUC and validation AUC. The process can take time because we train the Gaussian mixture model 400 times, and the training time will increase with higher `n_components`. In order to speed up the hyperparameter search, we use validation, which is k times faster than k-fold cross-validation. Feel free to use cross-validation if you have the time (and tell me if you find better hyperparameters).']",train evalu model train evalu model use train auc valid auc process take time train gaussian mixtur model 400 time train time increas higher n compon order speed hyperparamet search use valid k time faster k fold cross valid feel free use cross valid time tell find better hyperparamet
199,"['# Submitting the Test Predictions\n', '\n', 'We retrain using all the data and submit the test predictions for the competition.']",submit test predict retrain use data submit test predict competit
200,"['# Conclusion\n', '\n', 'The Gaussian mixture naive Bayes performs very well and is an improvement over the Gaussian naive Bayes, although it takes a little more time to train. It has the advantage that it is more flexible and does not require that the data come from a normal distribution. The only assumption is that the features are conditionally independent given the class. An alternative approach is to use kernel density estimation. Whichever method we use, the goal is to have a model that is simple (easily understood), tractable (easily computed) and accurate (represents reality very well).']",conclus gaussian mixtur naiv bay perform well improv gaussian naiv bay although take littl time train advantag flexibl requir data come normal distribut assumpt featur condit independ given class altern approach use kernel densiti estim whichev method use goal model simpl easili understood tractabl easili comput accur repres realiti well
201,"['# Introduction\n', '\n', 'In this kernel, we will apply Bayesian inference on Santander Customer Transaction data, which has a binary target and 200 continuous features. We model the target as unknown $Y$ and the features as observation $X$. The prior $p_Y(y)$ reflects our knowledge about the unknown before observation. In this problem, $Y$ is Bernoulli (only two classes) so it can be specified by setting the positive probability, which is usually set as the proportion of the positive class in the data. The likelihood $f_{X|Y}(x|y)$ models the distribution of the observation given that we know the class. The posterior $p_{Y|X}(y|x)$ is our updated knowledge about the unknown after observation.\n', '\n', 'The MAP (Maximum A Posteriori) estimator picks the class with the highest posterior probability. For binary classification, it has the same effect as setting a threshold of $0.5$ for the positive posterior probability. The LMS (Least Mean Squares) estimator $\\mathbf E[Y|X]$ picks the mean of the posterior distribution. For binary classification, this is just the positive posterior probability $p_{Y|X}(1|x)$, which is what we need to submit for the competition.\n', '\n', 'The Bayes rule for this problem is of the form\n', ""$$p_{Y|X}(y|x)=\\frac{p_Y(y)f_{X|Y}(x|y)}{\\sum_{y'}p_Y(y')f_{X|Y}(x|y')}$$\n"", '\n', 'Here $X$ represents a sequence of 200 observations $X_0,X_1,\\ldots,X_{199}$. We assume that the likelihood distributions are normal and independent. This gives us the Gaussian naive Bayes classifier (Gaussian means normal and naive means independent):\n', ""$$p_{Y|X_0,X_1,\\ldots,X_{199}}(y|x_0,x_1,\\ldots,x_{199})=\\frac{p_Y(y)\\prod_{i=0}^{199}f_{X_i|Y}(x_i|y)}{\\sum_{y'=0}^1p_Y(y')\\prod_{i=0}^{199}f_{X_i|Y}(x_i|y')}$$\n"", '\n', 'Note that we only require 1 number for the prior and 800 numbers for the likelihood (200 sample means and variances for each of the two classes). ""Fitting"" is just computing those numbers, and ""predicting"" is carried out according to the above formula (although we need to operate on the log scale because multiplying many small numbers poses a problem when our machine has limited precision). It is a very simple and efficient model.']",introduct kernel appli bayesian infer santand custom transact data binari target 200 continu featur model target unknown featur observ x prior p reflect knowledg unknown observ problem bernoulli two class specifi set posit probabl usual set proport posit class data likelihood f x x model distribut observ given know class posterior p x x updat knowledg unknown observ map maximum posteriori estim pick class highest posterior probabl binari classif effect set threshold 0 5 posit posterior probabl lm least mean squar estim mathbf e x pick mean posterior distribut binari classif posit posterior probabl p x 1 x need submit competit bay rule problem form p x x frac p f x x sum p f x x x repres sequenc 200 observ x 0 x 1 ldot x 199 assum likelihood distribut normal independ give u gaussian naiv bay classifi gaussian mean normal naiv mean independ p x 0 x 1 ldot x 199 x 0 x 1 ldot x 199 frac p prod 0 199 f x x sum 0 1p prod 0 199 f x x note requir 1 number prior 800 number likelihood 200 sampl mean varianc two class fit comput number predict carri accord formula although need oper log scale multipli mani small number pose problem machin limit precis simpl effici model
202,"['# Checking Assumptions\n', '\n', 'The classifier has already been implemented by scikit-learn, so we can use it right away. But we have to make sure that our assumptions hold, i.e., the likelihood distributions are normal and independent.']",check assumpt classifi alreadi implement scikit learn use right away make sure assumpt hold e likelihood distribut normal independ
203,"['We will look at the likelihood distributions by plotting the KDE (Kernel Density Estimates) using the [pandas.DataFrame.plot.kde](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.plot.kde.html). Note that KDE is a similarity-based method so it gets slower with more data. We can speed up by reducing the number of evaluation points (`ind` parameter), but this also decreases the resolution of the plot.']",look likelihood distribut plot kde kernel densiti estim use panda datafram plot kde http panda pydata org panda doc stabl refer api panda datafram plot kde html note kde similar base method get slower data speed reduc number evalu point ind paramet also decreas resolut plot
204,['The KDE plots above suggest that the likelihood distributions have different centers and spread. We will standardize them (subtract mean and divide by standard deviation) so that they have zero mean and unit variance. We can use [sklearn.preprocessing.StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) for standardization.'],kde plot suggest likelihood distribut differ center spread standard subtract mean divid standard deviat zero mean unit varianc use sklearn preprocess standardscal http scikit learn org stabl modul gener sklearn preprocess standardscal html standard
205,"['Now the KDE plots above look approximately normal, but some have small bumps on the left or right. We can proceed without doing anything, or we can use quantile transformation to remove the small bumps. It turns out that the transformation provides only marginal improvement in performance (0.001 in cross-validation AUC) despite requiring significantly more computation. In practice, we might choose to skip the transformation. In this competition, however, we will do the transformation for that tiny improvement.\n', '\n', 'Ideally, we need to apply the transformation to the features separately for the positive and negative classes. However, we cannot because it becomes a trouble when we are predicting the test data (we do not know the target value). We will instead apply it to the features as a whole so what we really get are normal unconditional distributions $f_{X_i}$ instead of normal conditional distributions $f_{X_i|Y}$, but we hope that the conditional distributions will become more normal as well. We can use [sklearn.preprocessing.QuantileTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.QuantileTransformer.html) for quantile transformation.']",kde plot look approxim normal small bump left right proceed without anyth use quantil transform remov small bump turn transform provid margin improv perform 0 001 cross valid auc despit requir significantli comput practic might choos skip transform competit howev transform tini improv ideal need appli transform featur separ posit neg class howev becom troubl predict test data know target valu instead appli featur whole realli get normal uncondit distribut f x instead normal condit distribut f x hope condit distribut becom normal well use sklearn preprocess quantiletransform http scikit learn org stabl modul gener sklearn preprocess quantiletransform html quantil transform
206,"['In the KDE plots above, the likelihood distributions have become normal as we desire.\n', '\n', 'Independence is difficult to check, but we can check the sample correlation coefficients. Small correlation coefficients mean that there is a weak linear pattern. We visualize the correlation matrix by using [matplotlib.pyplot.imshow](https://matplotlib.org/api/_as_gen/matplotlib.pyplot.imshow.html).']",kde plot likelihood distribut becom normal desir independ difficult check check sampl correl coeffici small correl coeffici mean weak linear pattern visual correl matrix use matplotlib pyplot imshow http matplotlib org api gen matplotlib pyplot imshow html
207,"['The correlation matrix plot above shows very small correlation coefficients between the features.\n', '\n', 'Finally, it is important that $Y$ is dependent on $X$. If $X$ and $Y$ were independent, then the posterior would be equal to the prior  $p_{Y|X}(y|x)=p_Y(y)$, and we would not need to do any calculation! We have already seen above that the positive and negative likelihood distributions are slightly different. Let us look at how the sample means and sample variances differ.']",correl matrix plot show small correl coeffici featur final import depend x x independ posterior would equal prior p x x p would need calcul alreadi seen posit neg likelihood distribut slightli differ let u look sampl mean sampl varianc differ
208,"['While the sample mean differences are more or less balanced around zero, the sample variance differences are almost entirely on the negative side. This means that the negative likelihood distributions are more concentrated around their means than the positive ones. These differences add to the discriminative power of the model. The further away the centers of the distributions or the greater the difference in the spread of the distributions, the more it can tell about which class the point is coming from.\n', '\n', 'If there are features $X_i$ such that the likelihood distributions are equal — $f_{X_i|Y}(x_i|0)=f_{X_i|Y}(x_i|1)$, their densities will cancel in the numerator and the denominator. These features do not help in classification. So, in some sense, the Bayes classifier performs automatic feature selection.\n', '\n', 'Now I have the following puzzle. The plot below shows two features with the least sample variance difference (greatest absolute difference where the variance of the positive class is higher). Surprisingly, the negative class looks more spread out despite having lower sample variance than the positive class.']",sampl mean differ le balanc around zero sampl varianc differ almost entir neg side mean neg likelihood distribut concentr around mean posit one differ add discrimin power model away center distribut greater differ spread distribut tell class point come featur x likelihood distribut equal f x x 0 f x x 1 densiti cancel numer denomin featur help classif sen bay classifi perform automat featur select follow puzzl plot show two featur least sampl varianc differ greatest absolut differ varianc posit class higher surprisingli neg class look spread despit lower sampl varianc posit class
209,['Why? Let us look at the sample mean differences.'],let u look sampl mean differ
210,"['The center of the negative class is above and to the right of that of the positive class, but in the above plot, we see straight lines on the lower and left edges. The bounds have remained even after quantile transformation. It looks like these bounds have prevented the positive class from expanding to the lower and left sides. The bounds are more obvious when you look at the original data.']",center neg class right posit class plot see straight line lower left edg bound remain even quantil transform look like bound prevent posit class expand lower left side bound obviou look origin data
211,"['Despite the presence of bounds, we are going to assume that the transformed data is normal and proceed anyway. We can sample data from normal distributions using [np.random.normal](https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.normal.html) and plot them for comparison.']",despit presenc bound go assum transform data normal proceed anyway sampl data normal distribut use np random normal http doc scipi org doc numpi refer gener numpi random normal html plot comparison
212,['We see above that the positive class spreads more to the lower and left sides than the negative class. Another reason for the illusion is that we have far fewer positive points than negative points.'],see posit class spread lower left side neg class anoth reason illus far fewer posit point neg point
213,"['# Training and Evaluating the Model\n', '\n', 'Now we are ready to train our model. We combine the quantile transformer and Gaussian naive Bayes classifer, [sklearn.naive_bayes.GaussianNB](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html), into a pipeline using [sklearn.pipeline.make_pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html).']",train evalu model readi train model combin quantil transform gaussian naiv bay classif sklearn naiv bay gaussiannb http scikit learn org stabl modul gener sklearn naiv bay gaussiannb html pipelin use sklearn pipelin make pipelin http scikit learn org stabl modul gener sklearn pipelin make pipelin html
214,"['After training the model, we plot the ROC curve on training data and evaluate the model by computing the training AUC and cross-validation AUC. We can use [sklearn.metrics.roc_curve](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html) to obtain the values for plotting the curve and [sklearn.metrics.auc](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html) for computing the AUC.']",train model plot roc curv train data evalu model comput train auc cross valid auc use sklearn metric roc curv http scikit learn org stabl modul gener sklearn metric roc curv html obtain valu plot curv sklearn metric auc http scikit learn org stabl modul gener sklearn metric roc curv html comput auc
215,['We compute the 10-fold cross-validation score by using [sklearn.model_selection.cross_val_score](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html).'],comput 10 fold cross valid score use sklearn model select cross val score http scikit learn org stabl modul gener sklearn model select cross val score html
216,['We achieved good AUC on both training and cross-validation. But is this the best that this model can achieve? Let us use simulation to get an estimate of the optimal AUC that this model can achieve. We will draw samples from the normal distribution with the 800 parameters of the likelihood. The amount of samples to draw from each class will be determined by the prior so that the classes have the same proportions as the training data.'],achiev good auc train cross valid best model achiev let u use simul get estim optim auc model achiev draw sampl normal distribut 800 paramet likelihood amount sampl draw class determin prior class proport train data
217,['We see that the optimal AUC under the model is not much different from the cross-validation AUC.'],see optim auc model much differ cross valid auc
218,"['# Submitting the Test Predictions\n', '\n', 'Let us use this model to predict the test data.']",submit test predict let u use model predict test data
219,"['# Conclusion\n', '\n', 'The Gaussian naive Bayes classifier performs quite well on Santander Customer Trasaction data. This is because the normality and independence assumptions are closely followed by the data. We have seen that even if the data have been generated by independent normal distributions (according to the model trained on transformed data), we cannot get a better AUC. However, there may still be some other transformation that can improve our model. In my opinion, the normality assumption is not very realistic since some features seem to have lower and upper bounds.\n', '\n', 'One can also remove the assumptions and try to use density estimation techniques to model the likelihood distributions. KDE is not very tractable on data of this size. We can also use a Gaussian mixture model or a multivariate normal distribution with the sample covariance matrix from the data. In my experience, they give better training AUC but worse cross-validation AUC. The Gaussian naive Bayes classifier (improved a little bit by quantile transformation) is currently the best Bayesian model for the data. Please tell me if you have found a better one!']",conclus gaussian naiv bay classifi perform quit well santand custom trasact data normal independ assumpt close follow data seen even data gener independ normal distribut accord model train transform data get better auc howev may still transform improv model opinion normal assumpt realist sinc featur seem lower upper bound one also remov assumpt tri use densiti estim techniqu model likelihood distribut kde tractabl data size also use gaussian mixtur model multivari normal distribut sampl covari matrix data experi give better train auc wors cross valid auc gaussian naiv bay classifi improv littl bit quantil transform current best bayesian model data plea tell found better one
220,"['# The ""Magic"" of Santander\n', 'In this kernel, we will display pictures of the Santander magic! Previously [here][1], in ""Modified Naive Bayes"", we saw that we can model each variable separately and then combine the 200 models to score LB 0.899. We will do the same here after adding a ""magic"" feature to each variable. We will then ensemble the 200 models with logistic regression and score LB 0.920  \n', '  \n', '![image](http://playagricola.com/Kaggle/magic41019.jpg)\n', '  \n', '[1]: https://www.kaggle.com/cdeotte/modified-naive-bayes-santander-0-899']",magic santand kernel display pictur santand magic previous 1 modifi naiv bay saw model variabl separ combin 200 model score lb 0 899 ad magic featur variabl ensembl 200 model logist regress score lb 0 920 imag http playagricola com kaggl magic41019 jpg 1 http www kaggl com cdeott modifi naiv bay santand 0 899
221,"['# The ""Magic"" Feature\n', 'When LGBM ""looks"" at the histogram for `Var_198`, it ""sees"" that when `var_198<13` the probability of having `target=1` is high. And when `var_198>13` the probability is low. This can be displayed by showing the predictions made by LGBM (when building a model from only the variable `var_198`). LGBM basically predicts `target=0.18` for `var_198<13` and `target=0.10` otherwise. \n', '  \n', '![image](http://playagricola.com/Kaggle/198without.png)  \n', '  \n', 'LGBM ""divides"" the histogram with **vertical lines** because LGBM does not see **horizontal** differences. A histogram places multiple values into a single bin and produces a smooth picture. If you place every value in its own bin, you will have a jagged picture, where bars change heights from value to value. Some values are unique, some values occur dozens of times (and in the case of `var_108`, some values occur over 300 times!!) Below is a histogram with one value per bin and we zoom in on `11.0000<x<11.1000`. We see that value `11.0712` occurs 5 times and its close neighbor `11.0720` occurs only once.  \n', '    \n', '![image](http://playagricola.com/Kaggle/198zoom3.png)   \n', '  \n', 'These counts are the ""magic"" feature. For each variable, we make a new feature (column) whose value is the number of counts of the corresponding variable. An example of this new column is displayed above next to the histogram. When LGBM has this new feature, it can now ""divide"" the histogram with **horizontal lines** in addition to vertical.  \n', '  \n', '![image](http://playagricola.com/Kaggle/198with.png)  \n', '  \n', 'Notice now that LGBM predicts `target=0.1` when `var_198<13` AND `count=1`. When  `var_198<13` AND `count>1`, it predicts `target=0.36`. This improvement (using the magic) causes validation AUC to become 0.551 as opposed to 0.547 when using `var_198` alone.']",magic featur lgbm look histogram var 198 see var 198 13 probabl target 1 high var 198 13 probabl low display show predict made lgbm build model variabl var 198 lgbm basic predict target 0 18 var 198 13 target 0 10 otherwis imag http playagricola com kaggl 198without png lgbm divid histogram vertic line lgbm see horizont differ histogram place multipl valu singl bin produc smooth pictur place everi valu bin jag pictur bar chang height valu valu valu uniqu valu occur dozen time case var 108 valu occur 300 time histogram one valu per bin zoom 11 0000 x 11 1000 see valu 11 0712 occur 5 time close neighbor 11 0720 occur imag http playagricola com kaggl 198zoom3 png count magic featur variabl make new featur column whose valu number count correspond variabl exampl new column display next histogram lgbm new featur divid histogram horizont line addit vertic imag http playagricola com kaggl 198with png notic lgbm predict target 0 1 var 198 13 count 1 var 198 13 count 1 predict target 0 36 improv use magic caus valid auc becom 0 551 oppos 0 547 use var 198 alon
222,"['# Why is the Magic difficult to find?\n', 'The ""magic"" is difficult to find because the new feature `Var_198_FE` interacts with `Var_198`. Therefore if you add the new feature to an LGBM with `feature_fraction=0.05`, you will not increase your CV or LB. You must set `feature_fraction=1.0`. Then you will gain the benefit from the new feature but you also have the determental effect of modeling spurious original variable interactions. None-the-less, adding the new feature and using `feature_fraction=1.0` achieves CV 0.910. To reach 0.920, we must remove the spurious effects from the original variable interactions.  \n', '  \n', ""UPDATE: I just discovered another reason why magic is hidden. When calculating frequency counts for the test data, you must remove the **fake** test data before counting. (Fake test described [here][1]) If you don't, then your test predictions will score LB 0.900 instead of LB 0.910 and you may disregard frequency counts as useless.\n"", '\n', '# Maximizing Magic Feature\n', 'To maximize the gain of the ""magic"" feature (and climb from LB 0.910 to 0.920), we must allow the new feature to interact with the original variables while preventing the original variables from interacting with each other. Here are 3 ways to do that:\n', ""* Use Data Augmentation (as shown in Jiwei's awesome kernel [here][2]). You must keep original and new feature in same row.\n"", '* Use 200 separate models as shown in this kernel below.\n', ""* Merge new feature and original feature into one feature. In original data, simply add 200 to each unique value. (And don't add new columns)\n"", '\n', ""# Let's Begin\n"", ""When counting the occurence of each value, we will merge the training data and **real** test data first, and count everything together. In YaG320's brilliant kernel [here][1], we learned that half the test data is fake.  \n"", '  \n', '[1]: https://www.kaggle.com/yag320/list-of-fake-samples-and-public-private-lb-split\n', '[2]: https://www.kaggle.com/jiweiliu/lgb-2-leaves-augment']",magic difficult find magic difficult find new featur var 198 fe interact var 198 therefor add new featur lgbm featur fraction 0 05 increas cv lb must set featur fraction 1 0 gain benefit new featur also determent effect model spuriou origin variabl interact none le ad new featur use featur fraction 1 0 achiev cv 0 910 reach 0 920 must remov spuriou effect origin variabl interact updat discov anoth reason magic hidden calcul frequenc count test data must remov fake test data count fake test describ 1 test predict score lb 0 900 instead lb 0 910 may disregard frequenc count useless maxim magic featur maxim gain magic featur climb lb 0 910 0 920 must allow new featur interact origin variabl prevent origin variabl interact 3 way use data augment shown jiwei awesom kernel 2 must keep origin new featur row use 200 separ model shown kernel merg new featur origin featur one featur origin data simpli add 200 uniqu valu add new column let begin count occur valu merg train data real test data first count everyth togeth yag320 brilliant kernel 1 learn half test data fake 1 http www kaggl com yag320 list fake sampl public privat lb split 2 http www kaggl com jiweiliu lgb 2 leav augment
223,"['# Ensemble 200 Models with LR\n', ""We now have a model for each variable and its predictions on test and its out-of-fold predictions on train. If we just add (or multiply) the predictions together, the AUC is low. Instead we will use logistic regression to ensemble them. Each set of predictions is a vector of length 200000. We have 200 vectors of out-of-fold predictions, call them `x1, x2, x3, ..., x200`. We know the true train target, call it `y`. We will now use logistic regression to find 200 coefficients (model y from x's). Then we will use those coefficients to combine our 200 test predictions to create a submission.""]",ensembl 200 model lr model variabl predict test fold predict train add multipli predict togeth auc low instead use logist regress ensembl set predict vector length 200000 200 vector fold predict call x1 x2 x3 x200 know true train target call use logist regress find 200 coeffici model x use coeffici combin 200 test predict creat submiss
224,"['# Modified Naive Bayes scores 0.899 LB - Santander\n', 'In this kernel we demonstrate that unconstrained Naive Bayes can score 0.899 LB. I call it ""unconstrained"" because it doesn\'t assume that each variable has a Gaussian distribution like typical Naive Bayes. Instead we allow for arbitrary distributions and we plot these distributions below. I called it ""modified"" because we don\'t reverse the conditional probabilities.\n', '\n', 'This kernel is useful because (1) it shows that an accurate score can be achieved using a simple model that assumes the variables are independent. And (2) this kernel displays interesting EDA which provides insights about the data.\n', '  \n', '# Load Data']",modifi naiv bay score 0 899 lb santand kernel demonstr unconstrain naiv bay score 0 899 lb call unconstrain assum variabl gaussian distribut like typic naiv bay instead allow arbitrari distribut plot distribut call modifi revers condit probabl kernel use 1 show accur score achiev use simpl model assum variabl independ 2 kernel display interest eda provid insight data load data
225,"['# Statistical Functions\n', 'Below are functions to calcuate various statistical things.']",statist function function calcuat variou statist thing
226,"['# Display Target Density and Target Probability\n', 'Below are two plots for each of the 200 variables. The first is the density of `target=1` versus `target=0`. The second gives the probability that `target=1` given different values for `var_k`.']",display target densiti target probabl two plot 200 variabl first densiti target 1 versu target 0 second give probabl target 1 given differ valu var k
227,"['# Target Probability Function\n', 'Above, the target probability function was calculated for each variable with resolution equal to `standard deviation / 50` from -5 to 5. For example, we know the `Probability ( target=1 | var=x )` for `z-score = -5.00, -4.98, ..., -0.02, 0, 0.02, ..., 4.98, 5.00` where `z-score = (x - var_mean) / (var_standard_deviation)`. The python function below accesses these pre-calculated values from their numpy array.']",target probabl function target probabl function calcul variabl resolut equal standard deviat 50 5 5 exampl know probabl target 1 var x z score 5 00 4 98 0 02 0 0 02 4 98 5 00 z score x var mean var standard deviat python function access pre calcul valu numpi array
228,"['# Validation\n', ""We will ignore the training data's target and make our own prediction for each training observation. Then using our predictions and the true value, we will calculate validation AUC. (There is a leak in this validation method but none-the-less it gives an approximation of CV score. If you wish to tune this model, you should use a proper validation set. Current actual 5-fold CV is 0.8995)""]",valid ignor train data target make predict train observ use predict true valu calcul valid auc leak valid method none le give approxim cv score wish tune model use proper valid set current actual 5 fold cv 0 8995
229,"['# Predict Test and Submit\n', 'Naive Bayes is a simple model. Given observation with `var_0 = 15`, `var_1 = 5`, `var_2 = 10`, etc. We compute the probability that `target=1` by calculating `P(t=1) * P(t=1 | var_0=15)/P(t=1) * P(t=1 | var_1=5)/P(t=1) * P(t=1 | var_2=10)/P(t=1) * ...` where `P(t=1)=0.1` and the other probabilities are computed above by counting occurences in the training data. So each observation has 200 variables and we simply multiply together the 200 target probabilities given by each variable. (In typical Naive Bayes, you use Bayes formula, reverse the probabilities, and find `P(var_0=15 | t=1)`. This is modified Naive Bayes and more intuitive.)']",predict test submit naiv bay simpl model given observ var 0 15 var 1 5 var 2 10 etc comput probabl target 1 calcul p 1 p 1 var 0 15 p 1 p 1 var 1 5 p 1 p 1 var 2 10 p 1 p 1 0 1 probabl comput count occur train data observ 200 variabl simpli multipli togeth 200 target probabl given variabl typic naiv bay use bay formula revers probabl find p var 0 15 1 modifi naiv bay intuit
230,['# Plot Predictions'],plot predict
231,"['# Conclusion\n', ""In conclusion we used modified Naive Bayes to predict Santander Customer transactions. Since we achieved an accurate score of 0.899 LB (which rivals other methods that capture interactions), this demonstrates that there is little or no interaction between the 200 variables. Additionally in this kernel we observed some fascinating EDA which provide insights about the variables. Can this method be improved? Perhaps by tuning this model better (adjust smoothing, resolution, etc) we can increase validation AUC and increase LB AUC but I don't think we can score over 0.902 with this method. There are other secrets hiding in the Santander data.\n"", '![image](http://playagricola.com/Kaggle/score32319.png)']",conclus conclus use modifi naiv bay predict santand custom transact sinc achiev accur score 0 899 lb rival method captur interact demonstr littl interact 200 variabl addit kernel observ fascin eda provid insight variabl method improv perhap tune model better adjust smooth resolut etc increas valid auc increas lb auc think score 0 902 method secret hide santand data imag http playagricola com kaggl score32319 png
232,"['![](https://storage.googleapis.com/kaggle-organizations/141/thumbnail.jpg?r=890)\n', '# Santander Customer Transaction Prediction\n', 'Can you identify who will make a transaction?\n', '\n', 'Version6\n', '- Ensemble : LB 0.899\n', '- LightGBM : LB 0.898\n', '- Catboost : LB 0.898 ']",http storag googleapi com kaggl organ 141 thumbnail jpg r 890 santand custom transact predict identifi make transact version6 ensembl lb 0 899 lightgbm lb 0 898 catboost lb 0 898
233,"['There are some check point. \n', '- 1. The train and test row are similar.  \n', '- 2. The column size so many.  ']",check point 1 train test row similar 2 column size mani
234,['Wow. All variable name is var_. it means that the variable is identifier !!!. https://www.kaggle.com/c/porto-seguro-safe-driver-prediction porto competition also has identifier variable. This link will help.'],wow variabl name var mean variabl identifi http www kaggl com c porto seguro safe driver predict porto competit also identifi variabl link help
235,"[""Target is unbalanced. i'll try upsampling...!""]",target unbalanc tri upsampl
236,['Train and Test has no missing value. Very Nice !!!. '],train test miss valu nice
237,['## LightGBM BaseLine'],lightgbm baselin
238,"[""Only 10% of the data is positive, so we'll reduce the train size to have an equal numbers of positive and negative samples.""]",10 data posit reduc train size equal number posit neg sampl
239,"['<div style=""background: linear-gradient(to bottom, #200122, #6f0000); border: 2px; box-radius: 20px""><h1 style=""color: white; text-align: center""><br> <center>Santander Customer Transaction Prediction<center><br></h1></div>']",div style background linear gradient bottom 200122 6f0000 border 2px box radiu 20px h1 style color white text align center br center santand custom transact predict center br h1 div
240,['## **Load the Data**'],load data
241,['- The Dataset containing 200 numeric feature variables from var_0 to var_199 and a target value.'],dataset contain 200 numer featur variabl var 0 var 199 target valu
242,['## Train the model'],train model
243,['## **LGBM**'],lgbm
244,['## **CatBoost Classifier**'],catboost classifi
245,['## **XGBoost**'],xgboost
246,['## **Submission**'],submiss
247,"['# Bayesian global optimization with gaussian processes for finding (sub-)optimal parameters of LightGBM\n', '\n', 'As many of fellow kaggler asking how did I get LightGBM parameters for the kernel [Customer Transaction Prediction](https://www.kaggle.com/fayzur/customer-transaction-prediction) I published. So, I decided to publish a kernel to optimize parameters. \n', '\n', '\n', '\n', 'In this kernel I use Bayesian global optimization with gaussian processes for finding optimal parameters. This optimization attempts to find the maximum value of an black box function in as few iterations as possible. In our case the black box function will be a function that I will write to optimize (maximize) the evaluation function (AUC) so that parameters get maximize AUC in training and validation, and expect to do good in the private. The final prediction will be **rank average on 5 fold cross validation predictions**.\n', '\n', 'Continue to the end of this kernel and **upvote it if you find it is interesting**.\n', '\n', '![image.jpg](https://i.imgur.com/XKS1oqU.jpg)\n', '\n', 'Image taken from : https://github.com/fmfn/BayesianOptimization']",bayesian global optim gaussian process find sub optim paramet lightgbm mani fellow kaggler ask get lightgbm paramet kernel custom transact predict http www kaggl com fayzur custom transact predict publish decid publish kernel optim paramet kernel use bayesian global optim gaussian process find optim paramet optim attempt find maximum valu black box function iter possibl case black box function function write optim maxim evalu function auc paramet get maxim auc train valid expect good privat final predict rank averag 5 fold cross valid predict continu end kernel upvot find interest imag jpg http imgur com xks1oqu jpg imag taken http github com fmfn bayesianoptim
248,"['## Notebook  Content\n', '0. [Installing Bayesian global optimization library](#0) <br>    \n', '1. [Loading the data](#1)\n', '2. [Black box function to be optimized (LightGBM)](#2)\n', '3. [Training LightGBM model](#3)\n', '4. [Rank averaging](#4)\n', '5. [Submission](#5)']",notebook content 0 instal bayesian global optim librari 0 br 1 load data 1 2 black box function optim lightgbm 2 3 train lightgbm model 3 4 rank averag 4 5 submiss 5
249,"['<a id=""0""></a> <br>\n', '## 0. Installing Bayesian global optimization library\n', '\n', ""Let's install the latest release from pip""]",id 0 br 0 instal bayesian global optim librari let instal latest releas pip
250,"['<a id=""1""></a> <br>\n', '## 1. Loading the data']",id 1 br 1 load data
251,"[""We are given anonymized dataset containing 200 numeric feature variables from var_0 to var_199. Let's have a look train dataset:""]",given anonym dataset contain 200 numer featur variabl var 0 var 199 let look train dataset
252,['Test dataset:'],test dataset
253,['Distribution of target variable'],distribut target variabl
254,['The problem is unbalanced! '],problem unbalanc
255,['In this kernel I will be using **50% Stratified rows** as holdout rows for the validation-set to get optimal parameters. Later I will use 5 fold cross validation in the final model fit.'],kernel use 50 stratifi row holdout row valid set get optim paramet later use 5 fold cross valid final model fit
256,['These `bayesian_tr_index` and `bayesian_val_index` indexes will be used for the bayesian optimization as training and validation index of training dataset.'],bayesian tr index bayesian val index index use bayesian optim train valid index train dataset
257,"['<a id=""2""></a> <br>\n', '## 2. Black box function to be optimized (LightGBM)']",id 2 br 2 black box function optim lightgbm
258,"[""As data is loaded, let's create the black box function for LightGBM to find parameters.""]",data load let creat black box function lightgbm find paramet
259,"['The above `LGB_bayesian` function will act as black box function for Bayesian optimization. I already defined the the trainng and validation dataset for LightGBM inside the `LGB_bayesian` function. \n', '\n', 'The `LGB_bayesian` function takes values for `num_leaves`, `min_data_in_leaf`, `learning_rate`, `min_sum_hessian_in_leaf`, `feature_fraction`, `lambda_l1`, `lambda_l2`, `min_gain_to_split`, `max_depth` from Bayesian optimization framework. Keep in mind that `num_leaves`, `min_data_in_leaf`, and `max_depth` should be integer for LightGBM. But Bayesian Optimization sends continous vales to function. So I force them to be integer. I am only going to find optimal parameter values of them. The reader may increase or decrease number of parameters to optimize.']",lgb bayesian function act black box function bayesian optim alreadi defin trainng valid dataset lightgbm insid lgb bayesian function lgb bayesian function take valu num leav min data leaf learn rate min sum hessian leaf featur fraction lambda l1 lambda l2 min gain split max depth bayesian optim framework keep mind num leav min data leaf max depth integ lightgbm bayesian optim send contin vale function forc integ go find optim paramet valu reader may increas decreas number paramet optim
260,"['Now I need to give bounds for these parameters, so that Bayesian optimization only search inside the bounds.']",need give bound paramet bayesian optim search insid bound
261,"[""Let's put all of them in BayesianOptimization object""]",let put bayesianoptim object
262,"[""Now, let's the the key space (parameters) we are going to optimize:""]",let key space paramet go optim
263,"['I have created the BayesianOptimization object (`LGB_BO`), it will not work until I call maximize. Before calling it, I want to explain two parameters of BayesianOptimization object (`LGB_BO`) which we can pass to maximize:\n', '- `init_points`: How many initial random runs of **random** exploration we want to perform. In our case `LGB_bayesian` will be called `n_iter` times.\n', '- `n_iter`: How many runs of bayesian optimization we want to perform after number of `init_points` runs. ']",creat bayesianoptim object lgb bo work call maxim call want explain two paramet bayesianoptim object lgb bo pas maxim init point mani initi random run random explor want perform case lgb bayesian call n iter time n iter mani run bayesian optim want perform number init point run
264,"[""Now, it's time to call the function from Bayesian optimization framework to maximize. I allow `LGB_BO` object to run for 5 `init_points` (exploration) and 5 `n_iter` (exploitation).""]",time call function bayesian optim framework maxim allow lgb bo object run 5 init point explor 5 n iter exploit
265,"[""As the optimization is done, let's see what is the maximum value we have got.""]",optim done let see maximum valu got
266,"[""The validation AUC for parameters is 0.89 ! Let's see parameters is responsible for this score :)""]",valid auc paramet 0 89 let see paramet respons score
267,['Now we can use these parameters to our final model!'],use paramet final model
268,"['Wait, I want to show one more cool option from BayesianOptimization library. You can probe the `LGB_bayesian` function, if you have an idea of the optimal parameters or it you get **parameters from other kernel** like mine [mine](https://www.kaggle.com/fayzur/customer-transaction-prediction). I will copy and paste parameters from my other kernel here. You can probe as folowing:']",wait want show one cool option bayesianoptim librari probe lgb bayesian function idea optim paramet get paramet kernel like mine mine http www kaggl com fayzur custom transact predict copi past paramet kernel probe folow
269,"[""OK, by default these will be explored lazily (lazy=True), meaning these points will be evaluated only the next time you call maximize. Let's do a maximize call of `LGB_BO` object.""]",ok default explor lazili lazi true mean point evalu next time call maxim let maxim call lgb bo object
270,"['Finally, the list of all parameters probed and their corresponding target values is available via the property LGB_BO.res.']",final list paramet probe correspond target valu avail via properti lgb bo re
271,['We have got a better validation score in the probe! As previously I ran `LGB_BO` only for 10 runs. In practice I increase it to arround 100.'],got better valid score probe previous ran lgb bo 10 run practic increas arround 100
272,"[""Let's build a model together use therse parameters ;)""]",let build model togeth use thers paramet
273,"['<a id=""3""></a> <br>\n', '## 3. Training LightGBM model']",id 3 br 3 train lightgbm model
274,"[""As you see, I assined `LGB_BO`'s optimal parameters to the `param_lgb` dictionary and they will be used to train a model with 5 fold.""]",see assin lgb bo optim paramet param lgb dictionari use train model 5 fold
275,['Number of Kfolds:'],number kfold
276,['So we got 0.90 AUC in 5 fold cross validation. And 5 fold prediction look like:'],got 0 90 auc 5 fold cross valid 5 fold predict look like
277,"[""If you are still reading, bare with me. I will not take much of your time. :D We are almost done. Let's do a rank averaging on 5 fold predictions.""]",still read bare take much time almost done let rank averag 5 fold predict
278,"['<a id=""4""></a> <br>\n', '## 4. Rank averaging']",id 4 br 4 rank averag
279,"[""Let's submit prediction to Kaggle.""]",let submit predict kaggl
280,"['<a id=""5""></a> <br>\n', '## 5. Submission']",id 5 br 5 submiss
281,['Do not forget to upvote :) Also fork and modify for your own use. ;)'],forget upvot also fork modifi use
282,"['# Santander Customer Transaction Prediction\n', '\n', 'This kernel uses LGBM model to predict Customer Transaction.\n', '\n', '**For LightGBM parameters optimization, please find my other kernel below, where I show how to take advantage of Bayesian Optimization to find optimal paramer ofr LightGBM:**\n', '\n', 'https://www.kaggle.com/fayzur/lgb-bayesian-parameters-finding-rank-average\n', '\n', '\n', '## Notebook  Content\n', '1. [Loading the data](#0) <br>    \n', '1. [Training the model](#1)\n', '1. [Submission](#2)']",santand custom transact predict kernel use lgbm model predict custom transact lightgbm paramet optim plea find kernel show take advantag bayesian optim find optim param ofr lightgbm http www kaggl com fayzur lgb bayesian paramet find rank averag notebook content 1 load data 0 br 1 train model 1 1 submiss 2
283,"['<a id=""0""></a> <br>\n', '## 1. Loading the data']",id 0 br 1 load data
284,"[""We are given anonymized dataset containing 200 numeric feature variables from var_0 to var_199. Let's have a look train dataset:""]",given anonym dataset contain 200 numer featur variabl var 0 var 199 let look train dataset
285,['Test dataset:'],test dataset
286,['Distribution of target variable'],distribut target variabl
287,"['The problem is unbalance!\n', 'We can build a quick model on this dataset considering unbalance to see how far we can go without Feature engineering! ']",problem unbal build quick model dataset consid unbal see far go without featur engin
288,"['<a id=""1""></a> <br>\n', '## 2. Training the model']",id 1 br 2 train model
289,"['The above parameters were obtained using the same structure presented in the following kernel:\n', '\n', 'https://www.kaggle.com/fayzur/lgb-bayesian-parameters-finding-rank-average']",paramet obtain use structur present follow kernel http www kaggl com fayzur lgb bayesian paramet find rank averag
290,['Number of Kfolds:'],number kfold
291,"['<a id=""2""></a> <br>\n', '## 2. Submission']",id 2 br 2 submiss
292,['Upvote if it is useful :)'],upvot use
293,"[""For 2 weeks I worked hard and didn't improve my score. For 4 days I worked smart, and got LB 0.920 (74th out of 9000+). \n"", '\n', 'The main purpose of this kernel is to share the thinking process that led me to this LB score, at the same time that you can have fun visualizing the data.\n']",2 week work hard improv score 4 day work smart got lb 0 920 74th 9000 main purpos kernel share think process led lb score time fun visual data
294,"['### **1. INTRODUCTION**\n', '\n', 'In this competition, the objective was to forecast which clients would make a specific transfer in the future. The data given was composed of 200.000 rows and 200 anonymized features.\n', '\n', 'The evaluation metric was ROC-AUC. In case you are not familliar with it, check it [here](https://medium.com/greyatom/lets-learn-about-auc-roc-curve-4a94b4d88152).\n', '\n', '**Context**:\n', 'This competition broke Kaggle\'s previous competitors record (around 6000), and became the most disputed competition ever in this platform, with over 9000 competitors. For many weeks, almost no team could surpass the 0.901 score barrier. So everyone started to look for the ""magic"".\n', '\n', ""### **2. WHAT DIDN'T WORK**\n"", '\n', 'Many teams tried to explore the data in every possible way: Features operations (+,-,*,/,log,tanh,etc), distribution, probability, time series, target encoding, data rotation, genetic programming, oversampling, undersampling, different models....you get it. Absolutely nothing worked\n', '\n', 'After 2 weeks trying everything, I got tired of working hard and decided to work smart, so this happened: I broke the 0.901 barrier.\n', '\n', '![](https://raw.githubusercontent.com/fmellomascarenhas/Kaggle/master/Santander%20-%202019%20-%2074th%20place/pictures/0.902_score.png)\n', '\n', '### **3. WHAT WORKED**\n', '\n', ""**WARNING:** Before you proceed, you should know that **the scores you will see won't match the ones described**. That's because I used a simple train_test_split to speed up this kernel, instead of kfolds.\n"", '\n', '### **3.1. CV 0.902 - MAGIC IN 4 LINES**\n', '\n', 'When I decided to work smart, I re-read relevant topics and kernels in this competition and made a list about the dos and donts expressed by other competitors (as you can see below). [Making a kernel](https://www.kaggle.com/felipemello/why-your-model-is-overfitting-not-making-progress) about my own failed experiments helped me realize why they were not working.\n', '\n', '**At this point, what did I know?**\n', '\n', ""1) Regular transformations wouldn't work (many topics shared failed experiments). \n"", '\n', '2) [The test dataset had fake data](https://www.kaggle.com/yag320/list-of-fake-samples-and-public-private-lb-split), and that was relevant.\n', '\n', '![](https://raw.githubusercontent.com/fmellomascarenhas/Kaggle/master/Santander%20-%202019%20-%2074th%20place/pictures/kernel_comment_1.png)\n', '\n', '3) That [unique values](https://www.kaggle.com/triplex/more-unique-values-in-train-set-than-test-set) were, somehow, important.\n', '\n', '![](https://raw.githubusercontent.com/fmellomascarenhas/Kaggle/master/Santander%20-%202019%20-%2074th%20place/pictures/kernel_comment_2.png)\n', '\n', 'So I decided to experiment with both of these ideas: I merged the train and test datataset without fake values, created new columns for each variable with the number of unique values in it, and trained my model. That was enough to break the 0.901. You can see this simples procedure below.\n']",1 introduct competit object forecast client would make specif transfer futur data given compos 200 000 row 200 anonym featur evalu metric roc auc case familliar check http medium com greyatom let learn auc roc curv 4a94b4d88152 context competit broke kaggl previou competitor record around 6000 becam disput competit ever platform 9000 competitor mani week almost team could surpass 0 901 score barrier everyon start look magic 2 work mani team tri explor data everi possibl way featur oper log tanh etc distribut probabl time seri target encod data rotat genet program oversampl undersampl differ model get absolut noth work 2 week tri everyth got tire work hard decid work smart happen broke 0 901 barrier http raw githubusercont com fmellomascarenha kaggl master santand 20 202019 20 2074th 20place pictur 0 902 score png 3 work warn proceed know score see match one describ use simpl train test split speed kernel instead kfold 3 1 cv 0 902 magic 4 line decid work smart read relev topic kernel competit made list do dont express competitor see make kernel http www kaggl com felipemello model overfit make progress fail experi help realiz work point know 1 regular transform work mani topic share fail experi 2 test dataset fake data http www kaggl com yag320 list fake sampl public privat lb split relev http raw githubusercont com fmellomascarenha kaggl master santand 20 202019 20 2074th 20place pictur kernel comment 1 png 3 uniqu valu http www kaggl com triplex uniqu valu train set test set somehow import http raw githubusercont com fmellomascarenha kaggl master santand 20 202019 20 2074th 20place pictur kernel comment 2 png decid experi idea merg train test datataset without fake valu creat new column variabl number uniqu valu train model enough break 0 901 see simpl procedur
295,"['### **3.2. CV 0.909 - SETTING FEATURE FRACTION TO 1**\n', '\n', 'Feature fraction is a [parameter of the LGB model](https://lightgbm.readthedocs.io/en/latest/Parameters.html). It goes from 0 to 1 and represents the percentage of the data that you will use on each iteration of the traning. My feature fraction was 0.3. When I set it to 1, the model was able to look at all the variables at once, and voilá: CV 0.909! \n', '\n', '**TIP**: Setting feature fraction to 1 is a great way to understand the impact of a new feature in your model.\n', '\n', '![](https://raw.githubusercontent.com/fmellomascarenhas/Kaggle/master/Santander%20-%202019%20-%2074th%20place/pictures/909_score.png)\n', '\n', '\n', '### **3.3. CV 0.913 - MAKING MY JOB EASIER**\n', '\n', 'At this point, I had a problem to solve: Each new feature about values frequency only mattered for one other specific feature. My model, however, was checking all possible interactions between my 400 features and taking a **long time to run**. \n', '\n', 'This [kernel](https://www.kaggle.com/ymatioun/santander-model-one-feature-at-a-time) was the perfect solution, when it proved that it was possible to train the model on each one of the variables separately and still achieve the same LB score. \n', '\n', 'So I decided train my model with 2 features at a time: The original one and an extra column with the unique values count.\n', '\n', '**RESULT: Less than 2 minutes to train my whole model (as opposed to over an hour) and CV of 0.913. Working smart was the way to go.**']",3 2 cv 0 909 set featur fraction 1 featur fraction paramet lgb model http lightgbm readthedoc io en latest paramet html goe 0 1 repres percentag data use iter trane featur fraction 0 3 set 1 model abl look variabl voil cv 0 909 tip set featur fraction 1 great way understand impact new featur model http raw githubusercont com fmellomascarenha kaggl master santand 20 202019 20 2074th 20place pictur 909 score png 3 3 cv 0 913 make job easier point problem solv new featur valu frequenc matter one specif featur model howev check possibl interact 400 featur take long time run kernel http www kaggl com ymatioun santand model one featur time perfect solut prove possibl train model one variabl separ still achiev lb score decid train model 2 featur time origin one extra column uniqu valu count result le 2 minut train whole model oppos hour cv 0 913 work smart way go
296,"['#### **3.4. OPTIMIZING PARAMETERS - CV 0.920**\n', '\n', 'Since my model now only took a couple of minutes to train, it became really easy to test hundreds of parameters. You can check a great kernel about Bayesian Optimization [here](https://www.kaggle.com/fayzur/lgb-bayesian-parameters-finding-rank-average).\n', '\n', '![](https://raw.githubusercontent.com/fmfn/BayesianOptimization/master/examples/func.png)\n', '\n', 'Just leaving the algorithm working there for 3 hours was enough to score 0.920LB on the last day of competition. If I had more hours to work with it, probably my score would have improved substantially.\n', '\n', ""**Tips:** The number of iterations on the LGB was found to be really important. You can't just leave a high number of iterations and set your model to do an early stop if the score doesn't improve.\n"", '\n', '![](https://raw.githubusercontent.com/fmellomascarenhas/Kaggle/master/Santander%20-%202019%20-%2074th%20place/pictures/920_score.png)\n', '\n', 'Those were my final parameters (click on the **code** button ->):']",3 4 optim paramet cv 0 920 sinc model took coupl minut train becam realli easi test hundr paramet check great kernel bayesian optim http www kaggl com fayzur lgb bayesian paramet find rank averag http raw githubusercont com fmfn bayesianoptim master exampl func png leav algorithm work 3 hour enough score 0 920lb last day competit hour work probabl score would improv substanti tip number iter lgb found realli import leav high number iter set model earli stop score improv http raw githubusercont com fmellomascarenha kaggl master santand 20 202019 20 2074th 20place pictur 920 score png final paramet click code button
297,"[""#### **3.5. GIVING WEIGHTS TO EACH FEATURE'S RESULT - CV 0.922**\n"", '\n', 'Since we were training the model on each feature, maybe one matered more than other, but how could our model find it? To solve that, before we calculated the AUC score, we decided to give to each column a weight, based on their individual AUC score. That raised my score in 0.002.']",3 5 give weight featur result cv 0 922 sinc train model featur mayb one mater could model find solv calcul auc score decid give column weight base individu auc score rais score 0 002
298,"['You could try to optimize those weights in some other way. A good option, for example, would be to use the bayesian optimizer.\n', '\n', 'If you want to use the weights, make sure that they are not overfitting your results. The best way is to use stratified k folds, and then apply the weights. Below is an example on how to test them before submiting.']",could tri optim weight way good option exampl would use bayesian optim want use weight make sure overfit result best way use stratifi k fold appli weight exampl test submit
299,"['### **4. LOOKING AT THE DATA: WHAT IS ODD ABOUT THE UNIQUE VALUES?**\n', '\n', ""**EDIT: I didn't know that  bimodal distribution is a natural phenomena of normal distributions. Thanks @cdeotte and @triplex for pointing that out. I will still leave the plots there, because even though it is a natural phenomena, it is a cool one.**\n"", '\n', 'When I looked at the Probability Density Functions (PDFs) of our variables, I discovered a clear bimodal distribution that exists among values that do not repeat. As those values start repeating more, the distribution changes, and it is really interesting to see.\n', '\n', 'To show that to you, first I will need to group some values.\n', '\n', '### **4.1. GROUPING VALUES**\n', '\n', 'Some variables, like var_12 and var_68, have many frequency groups. Some of these groups are really small, and can have less than 100 samples. Take a look and compare the PDFs of frequency groups in var_12 and var_81:']",4 look data odd uniqu valu edit know bimod distribut natur phenomenon normal distribut thank cdeott triplex point still leav plot even though natur phenomenon cool one look probabl densiti function pdf variabl discov clear bimod distribut exist among valu repeat valu start repeat distribut chang realli interest see show first need group valu 4 1 group valu variabl like var 12 var 68 mani frequenc group group realli small le 100 sampl take look compar pdf frequenc group var 12 var 81
300,"['To avoid working with such small groups and risk overfitting, we will say that each group has to have at least 2000 samples in the test + train datasets in total.']",avoid work small group risk overfit say group least 2000 sampl test train dataset total
301,"['### **4.2. BIMODAL DISTRIBUTIONS**.\n', '\n', 'Now that we are done with grouping, check the bimodals distributions below. They show the PDFs of the 6 first frequency groups of each variable. Example: Graph 1: All values that are unique / Graph 2: All values that repeat only twitce / Graph 3: .... / etc.']",4 2 bimod distribut done group check bimod distribut show pdf 6 first frequenc group variabl exampl graph 1 valu uniqu graph 2 valu repeat twitc graph 3 etc
302,['Not all features are like that though. Take a look at the feature 117 and 120.'],featur like though take look featur 117 120
303,"[""How cool is that? Couldn't figure out why this happens. If you have any hypothesis, let me know.""]",cool figur happen hypothesi let know
304,"['### **5. WHAT I WOULD LIKE TO HAVE DONE, IF I HAD MORE TIME**\n', '\n', '#### **5.1. EXPLORE THE BIMODAL DISTRIBUTIONS**\n', '\n', 'Probabily trying to show to LGB that there is a bimodal distribution would be useless, because it already sees it. Take a look at the plot below of the variable 81.']",5 would like done time 5 1 explor bimod distribut probabili tri show lgb bimod distribut would useless alreadi see take look plot variabl 81
305,"[""For each one of the frequencys, the LGB is already going to make splits, figure out that there is a bimodal distribution and check the probabilities of being target = 1 or target = 0. If you are confused about it or if your don't believe me, check my kernel [Why your model is overfitting/not making progress](https://www.kaggle.com/felipemello/why-your-model-is-overfitting-not-making-progress)\n"", '\n', '**Besides that**, with a simple linear transformation, we could supperpose two distributions, making them more readable to our human eyes. Take a look at var_81 and var_126 before and after we transform var_126.']",one frequenc lgb alreadi go make split figur bimod distribut check probabl target 1 target 0 confus believ check kernel model overfit make progress http www kaggl com felipemello model overfit make progress besid simpl linear transform could supperpos two distribut make readabl human eye take look var 81 var 126 transform var 126
306,"[""Isn't this so interesting? But, again, scalling values won't affect the way that LGB sees the model. This transformation just makes it more pleasent to our eyes. \n"", '\n', '**Note:** If you wanna do it yourself, just get the ratio of the distances between the peaks of the distributions, multiply one of the distributions by this ratio and subtract the differente between peaks of distribution 1 and 2.\n', '\n', '#### **5.2. OPTIMIZE THE LGB PARAMETERS FOR EACH VARIABLE**\n', '\n', 'Since we have 200 different models, instead of having one set of parameters for all variables, we could pick  a specific set of parameters that works best for them.\n', '\n', ""I tried doing it, and saw my AUC falling back to 0.900. I am not sure why. Maybe its overfitting? I don't know if it is possible to do that, but if it is, I bet it could substantially increase the score.\n"", '\n', '#### **5.3. OTHER STUFF**\n', '\n', '- Create NN to do ensembling;\n', '- Try to use pure probability, instead of the LGB, as it was done in [this](https://www.kaggle.com/cdeotte/modified-naive-bayes-santander-0-899) kernel. Another competitor reported a 0.922 LB score using this technic and a gold medal;\n', '- Applied augumentation, as described [here](https://www.kaggle.com/jiweiliu/lgb-2-leaves-augment).\n', '\n', 'However, there is only so much you can do in 4 days.\n', '\n', '\n', '### 6. SUMMARY\n', '\n', 'Thank you for your attention. I hope you had fun. In this kernel we saw that:\n', '- Smart work is better than hard work. It is faster and produces better results;\n', '- If you think rationally about things, understand your model and your data, you can quickly improve your results;\n', '- There is a lot more to explore in the data;\n', '\n', '**If you liked the kernel, please, consider upvoting. It is the way to best reward the hours I put making this kernel. Thanks :)**\n', '\n', '### MY FINAL RUN WITH 4 FOLDS']",interest scall valu affect way lgb see model transform make pleasent eye note wan na get ratio distanc peak distribut multipli one distribut ratio subtract different peak distribut 1 2 5 2 optim lgb paramet variabl sinc 200 differ model instead one set paramet variabl could pick specif set paramet work best tri saw auc fall back 0 900 sure mayb overfit know possibl bet could substanti increas score 5 3 stuff creat nn ensembl tri use pure probabl instead lgb done http www kaggl com cdeott modifi naiv bay santand 0 899 kernel anoth competitor report 0 922 lb score use technic gold medal appli augument describ http www kaggl com jiweiliu lgb 2 leav augment howev much 4 day 6 summari thank attent hope fun kernel saw smart work better hard work faster produc better result think ration thing understand model data quickli improv result lot explor data like kernel plea consid upvot way best reward hour put make kernel thank final run 4 fold
307,"['# Santander Customer Transaction Prediction\n', '\n', '![](https://storage.googleapis.com/kaggle-media/competitions/santander/atm_image.png)']",santand custom transact predict http storag googleapi com kaggl medium competit santand atm imag png
308,"['# Weighted Kernel Naive Bayes with Lasso Feature Elimination by TF - Santander  \n', ""> *More things should not be used than are necessary. - Occam's razor*\n"", '\n', 'Is **EVERY** variable IMPORTANT for prediction? ** The answer is not even close to yes by our finding.**\n', '\n', 'In this work, we will explore the santander using two upgraded naive bayes inference -- *Kernel NB and Weighted Kernel NB.*  \n', '\n', 'Firstly, we are going to introduce an updated version of Gaussian Naive Bayes method called **Kernel Naive Bayes** which release the assumption of every features follow normal distribution.   \n', '\n', 'Secondly, we are going to use back-prop & gradient decent to learn an updated model called **Weighted Kernel Naive Bayes** with lasso feature elimination. This is implemented by log transform the naive bayes formular from product terms into additive terms, and use TensorFlow to learn the loss function with L1-norm and ReLU constriants on weights.   \n', '\n', 'Our final experiment show that we can achieve **same level of AUC** using **only 75% or even 50% of features**,meaning that nearly half of the features are not informative for prediction.\n']",weight kernel naiv bay lasso featur elimin tf santand thing use necessari occam razor everi variabl import predict answer even close ye find work explor santand use two upgrad naiv bay infer kernel nb weight kernel nb firstli go introduc updat version gaussian naiv bay method call kernel naiv bay releas assumpt everi featur follow normal distribut secondli go use back prop gradient decent learn updat model call weight kernel naiv bay lasso featur elimin implement log transform naiv bay formular product term addit term use tensorflow learn loss function l1 norm relu constriant weight final experi show achiev level auc use 75 even 50 featur mean nearli half featur inform predict
310,"['Recall naive bayes follows the **feature independence law**, and use logit probability as prediction score. Conclusively, it should have the following fomular:  \n', '<img src=""https://latex.codecogs.com/gif.latex?logit\\_prob=\\frac{p(y=1|X)}{p(y=0|X)}=\\frac{p(y=1)}{p(y=0)}*\\frac{p(X|y=1)}{p(X|y=0)}=\\frac{p(y=1)}{p(y=0)}*\\prod_{i=1}^{200}\\frac{p(x_{i}|y=1)}{p(x_{i}|y=0)}"" title=""logit_p=\\frac{p(y=1|X)}{p(y=0|X)}=\\frac{p(y=1)}{p(y=0)}*\\frac{p(X|y=1)}{p(X|y=0)}=\\frac{p(y=1)}{p(y=0)}*\\prod_{i=1}^{200}\\frac{p(x_{i}|y=1)}{p(x_{i}|y=0)}"" />\n', '\n', 'where logit_prob is the final prediction. \n']",recal naiv bay follow featur independ law use logit probabl predict score conclus follow fomular img src http latex codecog com gif latex logit prob frac p 1 x p 0 x frac p 1 p 0 frac p x 1 p x 0 frac p 1 p 0 prod 1 200 frac p x 1 p x 0 titl logit p frac p 1 x p 0 x frac p 1 p 0 frac p x 1 p x 0 frac p 1 p 0 prod 1 200 frac p x 1 p x 0 logit prob final predict
311,"['  \n', 'As we know already, in the first term <img src=""https://latex.codecogs.com/gif.latex?{p(y=1)}"" title=""{p(y=1)}"" /> is prior probability of positive class. And P(y=0) can also be calculated easily by 1-P(y=1).  ']",know alreadi first term img src http latex codecog com gif latex p 1 titl p 1 prior probabl posit class p 0 also calcul easili 1 p 1
312,"['The problem lies on how to calculate the 200 other terms, that is to say, how to calculate the following:\n', '<img src=""https://latex.codecogs.com/gif.latex?{p(x_{i}|y=1)}"" title=""{p(x_{i}|y=1)}"" /> as well as <img src=""https://latex.codecogs.com/gif.latex?{p(x_{i}|y=0)}"" title=""{p(x_{i}|y=0)}"" />']",problem lie calcul 200 term say calcul follow img src http latex codecog com gif latex p x 1 titl p x 1 well img src http latex codecog com gif latex p x 0 titl p x 0
313,"['There are **two** basic way of calculating this.   \n', 'First way to do is assume that ith feature (**xi**) follows a gaussian distribution a.k.a. normal distribution, and calculate <img src=""https://latex.codecogs.com/gif.latex?{p(x_{i}|y=1)}"" title=""{p(x_{i}|y=1)}"" /> by the probability density function (PDF) of normal distribution.  \n', '\n', 'However,from other kernel we can see clearly that NOT all of these features follow gaussian distribution, for e.g.https://www.kaggle.com/cdeotte/modified-naive-bayes-santander-0-899\n', '\n', 'So a Gaussian assumution might **not be the best choice for estimating p(xi|y)**.  \n', 'For the second way we introduce** Kernel Density Estimation** to calculate the pdf of an arbitary distribution of features.']",two basic way calcul first way assum ith featur xi follow gaussian distribut k normal distribut calcul img src http latex codecog com gif latex p x 1 titl p x 1 probabl densiti function pdf normal distribut howev kernel see clearli featur follow gaussian distribut e g http www kaggl com cdeott modifi naiv bay santand 0 899 gaussian assumut might best choic estim p xi second way introduc kernel densiti estim calcul pdf arbitari distribut featur
314,"[""What's more we use gaussian kernel KDE using scipy.stats.kde.gaussian_kde, and binize the features to reduce the complexity from O(#data) to O(#bins).  ""]",use gaussian kernel kde use scipi stat kde gaussian kde biniz featur reduc complex data bin
315,"[""## For accelation, instead of calculating **p(xi|y)**, we now calculate **p(bin\\_of\\_xi|y)** for every bins. To achieve that, we cut every continues value in xi into bins, and map continues xi to its bins' probability:** p(bin\\_of\\_xi|y)** . This is bining Kernel Naive Bayes. ""]",accel instead calcul p xi calcul p bin xi everi bin achiev cut everi continu valu xi bin map continu xi bin probabl p bin xi bine kernel naiv bay
316,"['Here we have already get the Kernel Naive Bayes transformation of original data. It have 201 columns. Be aware of what we have done, these 201 features are not orignal features, but 201 terms in naive bayes formular:\n', '<img src=""https://latex.codecogs.com/gif.latex?logit\\_prob=\\frac{p(y=1)}{p(y=0)}*\\prod_{i=1}^{200}\\frac{p(x_{i}|y=1)}{p(x_{i}|y=0)}"" title=""\\frac{p(y=1)}{p(y=0)}*\\prod_{i=1}^{200}\\frac{p(x_{i}|y=1)}{p(x_{i}|y=0)}"" />\n', 'where the first term is logit of prior probability, and last 200 terms are logit of likelyhood.']",alreadi get kernel naiv bay transform origin data 201 column awar done 201 featur orign featur 201 term naiv bay formular img src http latex codecog com gif latex logit prob frac p 1 p 0 prod 1 200 frac p x 1 p x 0 titl frac p 1 p 0 prod 1 200 frac p x 1 p x 0 first term logit prior probabl last 200 term logit likelyhood
317,"['We are pretty **close to the final prediction**. If we now apply Naive Bayes, we can get the final prediction by multiplying these 201 terms as following:']",pretti close final predict appli naiv bay get final predict multipli 201 term follow
318,"['**The AUC is over 0.908.** That is a very impressive score in training set, althought may be a little overfit on the training data.   \n', 'According to the original kernel https://www.kaggle.com/jiazhuang/demonstrate-naive-bayes/notebook, it reported **AUC 0.894** in public leaderboard.']",auc 0 908 impress score train set althought may littl overfit train data accord origin kernel http www kaggl com jiazhuang demonstr naiv bay notebook report auc 0 894 public leaderboard
319,['**Above are explanation & code for Kernel Naive Bayes. **  \n'],explan code kernel naiv bay
320,"['------------------------\n', '# Now introduce Weighted Kernel Naive Bayes with Lasso Feature Elimination by gradient decent.']",introduc weight kernel naiv bay lasso featur elimin gradient decent
321,"['## First, in order to simplfy our naive bayes problem, we log the ""logit\\_prob"" term in order to transform series of product into series of sum.\n', '<img src=""https://latex.codecogs.com/gif.latex?log[\\frac{p(y=1)}{p(y=0)}*\\prod_{i=1}^{200}\\frac{p(x_{i}|y=1)}{p(x_{i}|y=0)}]=log\\frac{p(y=1)}{p(y=0)}&plus;\\sum_{i=1}^{200}log\\frac{p(x_{i}|y=1)}{p(x_{i}|y=0)}"" title=""log[\\frac{p(y=1)}{p(y=0)}*\\prod_{i=1}^{200}\\frac{p(x_{i}|y=1)}{p(x_{i}|y=0)}]=w_{0}\\cdot log\\frac{p(y=1)}{p(y=0)}+\\sum_{i=1}^{200}w_{i}\\cdot log\\frac{p(x_{i}|y=1)}{p(x_{i}|y=0)}"" />']",first order simplfi naiv bay problem log logit prob term order transform seri product seri sum img src http latex codecog com gif latex log frac p 1 p 0 prod 1 200 frac p x 1 p x 0 log frac p 1 p 0 plu sum 1 200 log frac p x 1 p x 0 titl log frac p 1 p 0 prod 1 200 frac p x 1 p x 0 w 0 cdot log frac p 1 p 0 sum 1 200 w cdot log frac p x 1 p x 0
322,"['## Second, we try to **weighted every term above** -- that is why it is called weighted naive bayes.\n', '<img src=""https://latex.codecogs.com/gif.latex?log\\_logit\\_prob=w_{0}\\cdot&space;log\\frac{p(y=1)}{p(y=0)}&plus;\\sum_{i=1}^{200}w_{i}\\cdot&space;log\\frac{p(x_{i}|y=1)}{p(x_{i}|y=0)}"" title=""w_{0}\\cdot log\\frac{p(y=1)}{p(y=0)}+\\sum_{i=1}^{200}w_{i}\\cdot log\\frac{p(x_{i}|y=1)}{p(x_{i}|y=0)}"" />']",second tri weight everi term call weight naiv bay img src http latex codecog com gif latex log logit prob w 0 cdot space log frac p 1 p 0 plu sum 1 200 w cdot space log frac p x 1 p x 0 titl w 0 cdot log frac p 1 p 0 sum 1 200 w cdot log frac p x 1 p x 0
323,['And the constrant is that **every weights wi (201>=i>=0) should be greater or equal to 0.**   \n'],constrant everi weight wi 201 0 greater equal 0
324,"['Actually, the ""naive bayes"" is just a special case of ""weighted naive bayes"" **when every weights wi are initlized to 1**. --That insire me to initlize every weights wi to 1 (see the code tf.ones), and let the model update the weights. And experiments show that **initilizations are so vital** that it stablize the model training -- the model would become very hard to learn if we init the weights in random way.']",actual naiv bay special case weight naiv bay everi weight wi initl 1 insir initl everi weight wi 1 see code tf one let model updat weight experi show initil vital stabliz model train model would becom hard learn init weight random way
325,"['## Finally, after we get the sum of weighted log logit probability, we now exp the sum and revocer the raw score!<img src=""https://latex.codecogs.com/gif.latex?logit\\_prob=exp(log\\_logit\\_prob)=exp(w_{0}\\cdot&space;log\\frac{p(y=1)}{p(y=0)}&plus;\\sum_{i=1}^{200}w_{0}\\cdot&space;log\\frac{p(x_{i}|y=1)}{p(x_{i}|y=0)})"" title=""logit\\_prob=exp(w_{0}\\cdot log\\frac{p(y=1)}{p(y=0)}+\\sum_{i=1}^{200}w_{0}\\cdot log\\frac{p(x_{i}|y=1)}{p(x_{i}|y=0)})"" />']",final get sum weight log logit probabl exp sum revoc raw score img src http latex codecog com gif latex logit prob exp log logit prob exp w 0 cdot space log frac p 1 p 0 plu sum 1 200 w 0 cdot space log frac p x 1 p x 0 titl logit prob exp w 0 cdot log frac p 1 p 0 sum 1 200 w 0 cdot log frac p x 1 p x 0
326,"['The implement is as follows: We choose **TensorFlow** to do the gradient decent&back-prop to learn the weights wi. Also, in order to elimintate the useless features, we use **Lasso shrikage** method -- also known as L1 normalization. In order to make sure the all weights are greater or equal to zero, **a Rectified Linear Unit(ReLU) are applied to the weights** before its dot product with log of logit.']",implement follow choos tensorflow gradient decent back prop learn weight wi also order elimint useless featur use lasso shrikag method also known l1 normal order make sure weight greater equal zero rectifi linear unit relu appli weight dot product log logit
327,['**You might wonder why L1-norm produce sparsity? ** you cuold check [this kaggle discussion](https://www.kaggle.com/residentmario/l1-norms-versus-l2-norms) for further machine learning knowledge.'],might wonder l1 norm produc sparsiti cuold check kaggl discus http www kaggl com residentmario l1 norm versu l2 norm machin learn knowledg
328,['## So the main formula are below:'],main formula
329,"['<img src=""https://latex.codecogs.com/gif.latex?logit\\_prob=exp(ReLU(w_{0})\\cdot&space;log\\frac{p(y=1)}{p(y=0)}&plus;\\sum_{i=1}^{200}ReLU(w_{i})\\cdot&space;log\\frac{p(x_{i}|y=1)}{p(x_{i}|y=0)})"" title=""logit\\_prob=exp(ReLU(w_{0})\\cdot log\\frac{p(y=1)}{p(y=0)}+\\sum_{i=1}^{200}ReLU(w_{i})\\cdot log\\frac{p(x_{i}|y=1)}{p(x_{i}|y=0)})"" />']",img src http latex codecog com gif latex logit prob exp relu w 0 cdot space log frac p 1 p 0 plu sum 1 200 relu w cdot space log frac p x 1 p x 0 titl logit prob exp relu w 0 cdot log frac p 1 p 0 sum 1 200 relu w cdot log frac p x 1 p x 0
330,"['<img src=""https://latex.codecogs.com/gif.latex?L1\\_norm=\\lambda&space;\\cdot&space;\\sum_{i=0}^{200}\\left&space;|&space;w_{i}&space;\\right&space;|"" title=""L1\\_norm=\\lambda \\cdot \\sum_{i=0}^{200}\\left | w_{i} \\right |"" />']",img src http latex codecog com gif latex l1 norm lambda space cdot space sum 0 200 left space space w space right space titl l1 norm lambda cdot sum 0 200 left w right
331,"['**We choose cross_entropy as target function, and add l1 norm to prevent overfitting as well as doing lasso shrinkage.**']",choos cross entropi target function add l1 norm prevent overfit well lasso shrinkag
332,"['<img src=""https://latex.codecogs.com/gif.latex?loss=cross\\_entropy(y\\_true,logit\\_prob)&space;&plus;L1\\_norm"" title=""loss=cross\\_entropy(y\\_true,logit\\_prob) +L1\\_norm"" />']",img src http latex codecog com gif latex loss cross entropi true logit prob space plu l1 norm titl loss cross entropi true logit prob l1 norm
333,"[""Let's start, Frist we log the train_KernelNB as input data.""]",let start frist log train kernelnb input data
334,"['Just a notice, we can get the same AUC score of 0.908 by **doing sum** on the logified data before doing exp on it. We should have a clear understanding of what we have done: first do a log transformation, and **a sum in log transform means product in original form**.   \n', '**And the exp of sum actually reverse the transform, making it equal to product of origanl data**. That is to say:']",notic get auc score 0 908 sum logifi data exp clear understand done first log transform sum log transform mean product origin form exp sum actual revers transform make equal product origanl data say
335,"[""## **Let's implement what we have introduce above, and start eliminting the features!**""]",let implement introduc start elimint featur
336,"['You can see the **weights w are becoming sparser and sparser** as train epoch goes on, from all ones to many zeros. So we are making progress in eliminting uesless features.']",see weight w becom sparser sparser train epoch goe one mani zero make progress elimint uesless featur
337,"[' as you see, the **test AUC are really hard to decrease** . However it doesn\'t matter, because we  want our final model to remain to be simple naive bayes with weights fix to 1.0, and the learned coef is not the key. **We just want to eliminate those useless features** using lasso shrinkage. So we mask those features that are sparse or ""almost sparse(<0.01)"". And use the mask to reduce the size of columns.']",see test auc realli hard decreas howev matter want final model remain simpl naiv bay weight fix 1 0 learn coef key want elimin useless featur use lasso shrinkag mask featur spar almost spar 0 01 use mask reduc size column
338,"['As we can see, after training, 53 of 201 features are eliminated. That is a total of 25% of the original data!   \n', '**And If we increase the lambda\\_w of LASSO from 2e-5 to 3e-5~4e-5, we can best eliminate 50% of the features without lossing too many socres!!!!**']",see train 53 201 featur elimin total 25 origin data increas lambda w lasso 2e 5 3e 5 4e 5 best elimin 50 featur without loss mani socr
339,"[""Let's submit the result with only 148 features and see what score we got.  \n"", 'The **AUC on public LB is 0.893** when using only 148 variables, a** very slight drop** from 0.894 using 201 features.']",let submit result 148 featur see score got auc public lb 0 893 use 148 variabl slight drop 0 894 use 201 featur
340,"[""Now,Let's see the weights distribution and rank the most important features by their feature weights.""]",let see weight distribut rank import featur featur weight
341,"[""As we see, more than 50 features are eliminated when lambda\\_w=2e-5. That's why it called feature elimination using LASSO.""]",see 50 featur elimin lambda w 2e 5 call featur elimin use lasso
342,"[""As we saw above, prior is still the most importance term because it have largest weights. Over 25% of features are eliminated, we can check their logit's distribution in this kernel(https://www.kaggle.com/cdeotte/modified-naive-bayes-santander-0-899).   \n"", '  \n', 'These ""unimportant"" features in our model, such as **var_153, really doesn\'t have any instructive information for our prediction**,its logit vary from maxium of 0.115 to the minumn of 0.975.It is a very small shake.    \n', '**However, the ""good feature"" in our model -- such as var_105, its logit vary from max of 0.19 to min of 0.08. That\'s a lot of info lying here.**\n']",saw prior still import term largest weight 25 featur elimin check logit distribut kernel http www kaggl com cdeott modifi naiv bay santand 0 899 unimport featur model var 153 realli instruct inform predict logit vari maxium 0 115 minumn 0 975 small shake howev good featur model var 105 logit vari max 0 19 min 0 08 lot info lie
343,"['Just curious, let\'s check if the ""unimportant"" feature have any pattern/order:  ']",curiou let check unimport featur pattern order
344,"['No pattern for the elimintaed features.. So the columns **must have been shuffle**.  \n', '\n', 'Moreover, we can **compare** some important features in LightGBM (https://www.kaggle.com/jiweiliu/lgb-2-leaves-augment) and see if they have high score in Lasso Weighted Naive Bayes:']",pattern eliminta featur column must shuffl moreov compar import featur lightgbm http www kaggl com jiweiliu lgb 2 leav augment see high score lasso weight naiv bay
345,"[""The weights are **basically consistent** with LGB's importance score. Most LGB's important feature are also important features for Lasso Weighted Naive Bayes.   \n"", '\n', 'But they are not exactly the same(e.g. var_9), so **it might still be helpful to combine these two algorithms together** because they share some diversity.']",weight basic consist lgb import score lgb import featur also import featur lasso weight naiv bay exactli e g var 9 might still help combin two algorithm togeth share diver
347,['We can rethink our weighted naive bayes model and interprete it in this way:\n'],rethink weight naiv bay model interpret way
348,"['<img src=""https://latex.codecogs.com/gif.latex?w_{0}\\cdot&space;log\\frac{p(y=1)}{p(y=0)}&plus;\\sum_{i=1}^{200}w_{i}\\cdot&space;log\\frac{p(x_{i}|y=1)}{p(x_{i}|y=0)}&space;=log[\\frac{p(y=1)}{p(y=0)}]^{w0}&plus;\\sum_{i=1}^{200}log[\\frac{p(x_{i}|y=1)}{p(x_{i}|y=0)}]^{w_{i}}"" title=""w_{0}\\cdot log\\frac{p(y=1)}{p(y=0)}+\\sum_{i=1}^{200}w_{i}\\cdot log\\frac{p(x_{i}|y=1)}{p(x_{i}|y=0)} =log[\\frac{p(y=1)}{p(y=0)}]^{w0}+\\sum_{i=1}^{200}log[\\frac{p(x_{i}|y=1)}{p(x_{i}|y=0)}]^{w_{i}}"" />']",img src http latex codecog com gif latex w 0 cdot space log frac p 1 p 0 plu sum 1 200 w cdot space log frac p x 1 p x 0 space log frac p 1 p 0 w0 plu sum 1 200 log frac p x 1 p x 0 w titl w 0 cdot log frac p 1 p 0 sum 1 200 w cdot log frac p x 1 p x 0 log frac p 1 p 0 w0 sum 1 200 log frac p x 1 p x 0 w
349,"['That provide us **a different view about how the weights works** -- if a sparse weight wi=0, then wi=0 make the logit^wi=logit^0 =1, and log(1)=0, making this logit terms equal to zero. So the our final result(as a linear combination) would not be influence by this features any more since adding zero term means adding nothing.']",provid u differ view weight work spar weight wi 0 wi 0 make logit wi logit 0 1 log 1 0 make logit term equal zero final result linear combin would influenc featur sinc ad zero term mean ad noth
350,"['## Further  Discussion  \n', ""Is weighted NB model perfect? I don't think so, **althought it is good that our model eliminated useless features,but it still need to be digging why weighted features are not gaining AUC improvment. In theory weighted naive bayes should learn more than naive bayes. Maybe it is an overfitting problem or data noise problem.    **   ""]",discus weight nb model perfect think althought good model elimin useless featur still need dig weight featur gain auc improv theori weight naiv bay learn naiv bay mayb overfit problem data nois problem
351,"['Futher more, weighted on every column a constant might not be as good as** weighting every columns differently by the attribute of this column**.  That model is called ** Attribute weighted Naive Bayesian**, and there have been many research done in that field for recent years. If you are interested you could check [this paper](https://ieeexplore.ieee.org/document/5593445/) as start.']",futher weight everi column constant might good weight everi column differ attribut column model call attribut weight naiv bayesian mani research done field recent year interest could check paper http ieeexplor ieee org document 5593445 start
352,"['**And,there are more interesting things to try using naive bayes model, such as using LGB to replace kernel density estimation to estimate p(xi|y) in this kernel: https://www.kaggle.com/b5strbal/lightgbm-naive-bayes-santander-0-900 . **']",interest thing tri use naiv bay model use lgb replac kernel densiti estim estim p xi kernel http www kaggl com b5strbal lightgbm naiv bay santand 0 900
353,"['# Conclusion:\n', 'In this Kernel, we first explain Kernel Naive Bayes using Kernel Density Estimation(KDE), and illustrate the merits of Kernel NB over Gaussian NB.  \n', 'Secondly, we use log transformation that transform Naive Bayes\'s product into a sum of 201 logit terms, then **we strengthen the ""simple sum of logit term"" by giving every term a weight wi(w>=0),making it a standard weighted linear regression problem**. So we use gradient decent to learn these weights. For the purpose of eliminating useless feautures, **Lasso(L1-norm)** are used in the linear model. Also, **ReLU** are applied on the weights to make sure that weights either positive(>0) or sparse(=0).   \n', '\n', 'Our Lasso Weighted Naive Bayes model use back-prop&gradient decent to shrikage the weights. And the experiment shows that our model is **expert** in eliminating these useless features by itself.  By tuning the lambda of L1-norm from 2e-5 to 4e-5, at last we are able to gain almost **same level of AUC using only 50% of the features, which actually indicate that around half of the features are not informative (I prefer saying ""almost useless"") for our prediction.  ** Further analysis show that the feature selection process could be possibly applied to  LGB model.\n', '\n', '-------\n']",conclus kernel first explain kernel naiv bay use kernel densiti estim kde illustr merit kernel nb gaussian nb secondli use log transform transform naiv bay product sum 201 logit term strengthen simpl sum logit term give everi term weight wi w 0 make standard weight linear regress problem use gradient decent learn weight purpos elimin useless feautur lasso l1 norm use linear model also relu appli weight make sure weight either posit 0 spar 0 lasso weight naiv bay model use back prop gradient decent shrikag weight experi show model expert elimin useless featur tune lambda l1 norm 2e 5 4e 5 last abl gain almost level auc use 50 featur actual indic around half featur inform prefer say almost useless predict analysi show featur select process could possibl appli lgb model
354,"['*I would be happy if my post help you understand NB or the data better.*   \n', 'Give me a **thumb up or follow** if this kernel is helpful to you. Appreciate that!']",would happi post help understand nb data better give thumb follow kernel help appreci
355,['## **0. Introduction**'],0 introduct
356,['## **1. Exploratory Data Analysis**'],1 exploratori data analysi
357,"['### **1.1 Overview**\n', '* Both training set and test set have **200000** rows\n', '* Training set have **202** features and test set have **201** features\n', '* One extra feature in the training set is `target` feature, which is the class of a row\n', '* `target` feature is binary (**0** or **1**), **1 = transaction** and **0 = no transaction**\n', ""* `ID_code` feature is the unique id of the row and it doesn't have any effect on target\n"", '* The other features are anonymized and labeled from `var_0` to `var_199`\n', '* There are no missing values in both training set and test set because the dataset is already processed']",1 1 overview train set test set 200000 row train set 202 featur test set 201 featur one extra featur train set target featur class row target featur binari 0 1 1 transact 0 transact id code featur uniqu id row effect target featur anonym label var 0 var 199 miss valu train set test set dataset alreadi process
358,"['### **1.2 Target Distribution**\n', '* **10.05%** (20098/200000) of the training set is **Class 1**\n', '* **89.95%** (179902/200000) of the training set is **Class 0**']",1 2 target distribut 10 05 20098 200000 train set class 1 89 95 179902 200000 train set class 0
359,"['### **1.3 Correlations**\n', 'Features from `var_0` to `var_199` have extremely low correlation between each other in both training set and test set. The lowest correlation between variables is **2.7e-8** and it is in the training set (between `var_191` and `var_75`). The highest correlation between variables is **0.00986** and it is in the test set (between `var_139` and `var_75`). `target` has slightly higher correlations with other features. The highest correlation between a feature and `target` is **0.08** (between `var_81` and `target`).']",1 3 correl featur var 0 var 199 extrem low correl train set test set lowest correl variabl 2 7e 8 train set var 191 var 75 highest correl variabl 0 00986 test set var 139 var 75 target slightli higher correl featur highest correl featur target 0 08 var 81 target
360,"['### **1.4 Unique Value Count**\n', 'The lowest unique value count belongs to `var_68` which has only **451** unique values in training set and **428** unique values in test set. **451** and **428** unique values in **200000** rows are too less that `var_68` could even be a categorical feature. The highest unique value count belongs to`var_45` which has **169968** unique values in the training set and **92058** unique values in the test set. Every feature in training set have higher unique value counts compared to features in test set.\n', '\n', 'The lowest unique value count difference is in the `var_68` feature (Training Set Unique Count **451**, Test Set Unique Count **428**). The highest unique value count difference is in the `var_45` feature (Training Set Unique Count **169968**, Test Set Unique Count **92058**). When the unique value count of a feature increases, the difference between training set unique value count and test set unique value count also increases. The explanation of this situation is probably the synthetic records in the test set. ']",1 4 uniqu valu count lowest uniqu valu count belong var 68 451 uniqu valu train set 428 uniqu valu test set 451 428 uniqu valu 200000 row le var 68 could even categor featur highest uniqu valu count belong var 45 169968 uniqu valu train set 92058 uniqu valu test set everi featur train set higher uniqu valu count compar featur test set lowest uniqu valu count differ var 68 featur train set uniqu count 451 test set uniqu count 428 highest uniqu valu count differ var 45 featur train set uniqu count 169968 test set uniqu count 92058 uniqu valu count featur increas differ train set uniqu valu count test set uniqu valu count also increas explan situat probabl synthet record test set
361,"['### **1.5 Target Distribution in Quartiles**\n', 'Class 1 `target` distribution in feature quartiles are quite similar for each feature. Most of the class 1 `target` rows are either in the **1st** quartile or in the **4th** quartile of the features because of the winsorization. Winsorization clips the extreme values, so they are grouped up in the spikes inside **1st** quartile and **4th** quartile.\n', '* **94** features have highest class 1 `target` percentage in **1st** quartile\n', '* **101** features have highest class 1 `target` percentage in **4th** quartile\n', '* Only **5** features have highest class 1 `target` percetange in **2nd** and **3rd** quartile, and those features are `var_17`, `var_30`, `var_100`, `var_101`, `var_105`\n', '\n', 'Maximum class 1 `target` percentage for **1st** quartile is **14.35%** (**85.65%** class 0), and for **4th** quartile is **13.43%** (**86.57%** class 0). Maximum class 1 `target` percentage for **2nd** quartile is **10.34%** (**89.66%** class 0), and for **3rd** quartile is **10.05%** (**89.95%** class 0 `target`). To conclude, values in **1st** and **4th** quartiles have higher chance (**3-4%**) to be class 1 than values in **2nd** and **3rd** quartile for 195 features.']",1 5 target distribut quartil class 1 target distribut featur quartil quit similar featur class 1 target row either 1st quartil 4th quartil featur winsor winsor clip extrem valu group spike insid 1st quartil 4th quartil 94 featur highest class 1 target percentag 1st quartil 101 featur highest class 1 target percentag 4th quartil 5 featur highest class 1 target percetang 2nd 3rd quartil featur var 17 var 30 var 100 var 101 var 105 maximum class 1 target percentag 1st quartil 14 35 85 65 class 0 4th quartil 13 43 86 57 class 0 maximum class 1 target percentag 2nd quartil 10 34 89 66 class 0 3rd quartil 10 05 89 95 class 0 target conclud valu 1st 4th quartil higher chanc 3 4 class 1 valu 2nd 3rd quartil 195 featur
362,"['### **1.6 Feature Distributions in Training and Test Set**\n', 'Training and test set distributions of features are not perfectly identical. There are bumps on the distribution peaks of test set because the unique value counts are lesser than training set. Distribution tails are smoother than peaks and spikes are present in both training and test set.']",1 6 featur distribut train test set train test set distribut featur perfectli ident bump distribut peak test set uniqu valu count lesser train set distribut tail smoother peak spike present train test set
363,"['### **1.7 Target Distributions in Features**\n', 'Majority of the features have good split points and huge spikes. This explains why a simple LightGBM model can achieve 0.90 AUC. Distribution difference is bigger in tails because of winsorization.']",1 7 target distribut featur major featur good split point huge spike explain simpl lightgbm model achiev 0 90 auc distribut differ bigger tail winsor
364,"['### **1.8 Conclusion**\n', 'Data imbalance is very common in customer datasets like this. Oversampling **Class 1** or undersampling **Class 0** are suitable solutions for this dataset because of its large size. Since the dataset is big enough, resampling would not introduce underfitting.\n', '\n', 'Training set has more unique values than test set so some part of test set is most likely synthetic. Rows with more frequent values are less reliable because test set has bumps over distribution peaks. This is also related to synthetic data in test set.\n', '\n', 'Features are not correlated with each other or not dependent to each other. However, `target` feature has the highest correlation with `var_81` (**0.08**). This relationship can bu used to make other features more informative. If a feature is target encoded on `var_81`, it could give information about `target`.\n', '\n', 'Values in **1st** and **4th** quartiles have higher chance to be **Class 1** than values in **2nd** and **3rd** quartile for almost every feature because of winsorization.']",1 8 conclus data imbal common custom dataset like oversampl class 1 undersampl class 0 suitabl solut dataset larg size sinc dataset big enough resampl would introduc underfit train set uniqu valu test set part test set like synthet row frequent valu le reliabl test set bump distribut peak also relat synthet data test set featur correl depend howev target featur highest correl var 81 0 08 relationship bu use make featur inform featur target encod var 81 could give inform target valu 1st 4th quartil higher chanc class 1 valu 2nd 3rd quartil almost everi featur winsor
365,['## **2. Feature Engineering and Data Augmentation**'],2 featur engin data augment
366,"['### **2.1 Separating Real/Synthetic Test Data and Magic Features**\n', 'Using unique value count in a row to identify synthetic samples. If a row has at least one unique value in a feature, then it is real, otherwise it is synthetic. This technique is shared by [YaG320](https://www.kaggle.com/yag320) in this kernel [List of Fake Samples and Public/Private LB split](https://www.kaggle.com/yag320/list-of-fake-samples-and-public-private-lb-split) and it successfuly identifies synthetic samples in entire test set. This way the unusual bumps on the distribution peaks of test set features are captured. The magic features are extracted from the combination of training set and real samples in the test set. ']",2 1 separ real synthet test data magic featur use uniqu valu count row identifi synthet sampl row least one uniqu valu featur real otherwis synthet techniqu share yag320 http www kaggl com yag320 kernel list fake sampl public privat lb split http www kaggl com yag320 list fake sampl public privat lb split successfuli identifi synthet sampl entir test set way unusu bump distribut peak test set featur captur magic featur extract combin train set real sampl test set
367,"['### **2.2 Data Augmentation**\n', 'Oversampling the data increases CV and LB score significantly since the data is imbalanced. This oversampling technique is shared by [Jiwei Liu](https://www.kaggle.com/jiweiliu) in this kernel [LGB 2 leaves + augment](https://www.kaggle.com/jiweiliu/lgb-2-leaves-augment).']",2 2 data augment oversampl data increas cv lb score significantli sinc data imbalanc oversampl techniqu share jiwei liu http www kaggl com jiweiliu kernel lgb 2 leav augment http www kaggl com jiweiliu lgb 2 leav augment
368,"['### **2.3 Quartile Rank (Not Used)**\n', ""This code ranks every value by their quartile. Ranking is done according to the features' **Class 1** distribution percentage in a quartile. In order to do that, every features' quartiles are sorted by **Class 1** percentage. After that, the ranks **(4, 3, 2, 1)** are mapped to the sorted quartiles. This way, the quartile with the highest **Class 1** distribution in a feature gets the highest rank. After every value in a row are ranked,the ranks are summed and scaled. This way the mean rank of a row is calculated. The problems with this feature are:\n"", '* The distributions are already captured by decision trees, so this feature is not very useful in LightGBM\n', '* If this feature is computed outside the folds, it leaks data']",2 3 quartil rank use code rank everi valu quartil rank done accord featur class 1 distribut percentag quartil order everi featur quartil sort class 1 percentag rank 4 3 2 1 map sort quartil way quartil highest class 1 distribut featur get highest rank everi valu row rank rank sum scale way mean rank row calcul problem featur distribut alreadi captur decis tree featur use lightgbm featur comput outsid fold leak data
369,"['### **2.4 Target Encoding (Not Used)**\n', 'This function is for averaging the target value by feature. It computes the number of values and mean of each group. After that, the smooth mean is computed and replaced with the feature. Target encoding should be used in the folds otherwise it leaks data.']",2 4 target encod use function averag target valu featur comput number valu mean group smooth mean comput replac featur target encod use fold otherwis leak data
370,"['### **2.5 KMeansFeaturizer (Not Used)**\n', '`KMeansFeaturizer` is a pipeline of scikit-learn `KMeans` and `OneHotEncoder`. First, the records are grouped into **k** groups by `KMeans` with or without `target`. A return object of an $m * n$ matrix is $m * k$ group matrix which can be added to the previous matrix as features. This can be used to add likelihood features.\n', '* In order to make these features reliable, `KMeans` should be initialized with different seeds with many times and then blended\n', ""* The information gain from this approach doesn't worth it because it adds lot of new features to the dataset and takes too much time""]",2 5 kmeansfeatur use kmeansfeatur pipelin scikit learn kmean onehotencod first record group k group kmean without target return object n matrix k group matrix ad previou matrix featur use add likelihood featur order make featur reliabl kmean initi differ seed mani time blend inform gain approach worth add lot new featur dataset take much time
371,"['### **2.6 Feature Transformation (Not Used)**\n', ""This function is for simulating feature transformations. The transformation objective is to increase information gain by decreasing the overlapping area in the target distribution. By decreasing the overlapping area, LightGBM decision trees are able to make better splits. A transformed feature can be added to the data set as a new feature or it can replace the old one depending on the model's performance. A new feature can also be combinations of transformations and interactions between other features.""]",2 6 featur transform use function simul featur transform transform object increas inform gain decreas overlap area target distribut decreas overlap area lightgbm decis tree abl make better split transform featur ad data set new featur replac old one depend model perform new featur also combin transform interact featur
372,['## **3. Model**'],3 model
373,['### **3.1 LightGBM**'],3 1 lightgbm
374,['### **3.2 ROC-AUC Score**'],3 2 roc auc score
375,['### **3.3 Feature Importance**'],3 3 featur import
376,['### **3.4 Submission**'],3 4 submiss
377,['## bining'],bine
378,"['## smooth target encoding\n', 'reference: https://maxhalford.github.io/blog/target-encoding-done-the-right-way/']",smooth target encod refer http maxhalford github io blog target encod done right way
379,['## no training'],train
380,"['# Deep Learning + LGBM + Weighted Combination\n', '\n', 'This kernel will always be running with different parameters and approaches until before the competition deadline.\n', '\n', 'Feel free to upvote,fork and test the presented models with different training options, to see if a better score with the following models is possible.\n', '\n', 'If forked Please try the different combinations:\n', '- Only Feature Engineering ( omitting some features maybe)\n', '- Only Augmented\n', '- Augmented + Feature Engineering (Augment before or after FE)\n', '- Augmented + Feature Engineering + folds\n', '- Augmented + Feature Engineering + full\n', '- Combination of different prediction weights\n', '- etc..\n', '\n', '\n', ""**Don't forget that with each combination you might need different hyper-parameters for the models**\n"", '\n', '\n', 'You can also check here for weighted CV approach that will make a minor better prediction that you might need in the competition:\n', 'https://www.kaggle.com/hjd810/introducing-weighted-cross-validation\n', '\n', 'Enjoy ! \n', '\n', 'Any comments are appreciated (added motivation <3)\n', '\n', '1. [Training Options](#options)\n', '2. [Sampling](#sample)\n', '3. [Feature Engineering](#fe)\n', '4. [Dim Reduction](#dim)\n', '5. [LGBM Model](#lgbm)\n', '6. [Keras Model](#keras)\n', '7. [Combination Vis](#vis)\n', '8. [Submission](#sub)']",deep learn lgbm weight combin kernel alway run differ paramet approach competit deadlin feel free upvot fork test present model differ train option see better score follow model possibl fork plea tri differ combin featur engin omit featur mayb augment augment featur engin augment fe augment featur engin fold augment featur engin full combin differ predict weight etc forget combin might need differ hyper paramet model also check weight cv approach make minor better predict might need competit http www kaggl com hjd810 introduc weight cross valid enjoy comment appreci ad motiv 3 1 train option option 2 sampl sampl 3 featur engin fe 4 dim reduct dim 5 lgbm model lgbm 6 kera model kera 7 combin vi vi 8 submiss sub
381,"['## Running Models Options\n', ""<a id='options'></a>\n"", 'The options here helps you check the different combinations for training and check which fits best.']",run model option id option option help check differ combin train check fit best
382,['As you can see we are dealing with an unbalanced targets (10% vs 90%)'],see deal unbalanc target 10 v 90
383,"['## Sampling from the full dataset (more work on this later)\n', ""<a id='sample'></a>""]",sampl full dataset work later id sampl
384,"['## Simple Feature Engineering and Pre-processing\n', '* sum\n', '* min\n', '* max\n', '* mean\n', '* std\n', '* skew\n', '* kurt\n', '* med\n', '* Moving Average\n', '* percentiles\n', '* Augmentation\n', '* Log transformation\n', '* normalization\n', ""<a id='fe'></a>""]",simpl featur engin pre process sum min max mean std skew kurt med move averag percentil augment log transform normal id fe
385,"['# Dimensionality Reduction\n', ""<a id='dim'></a>""]",dimension reduct id dim
386,['We will Reduce our dimensions to n number of components and test whether this approach can also help in providing better results'],reduc dimens n number compon test whether approach also help provid better result
387,"['# 1. LGBM Model\n', ""<a id='lgbm'></a>""]",1 lgbm model id lgbm
388,"['\n', '### Non-CV LGBM Approach']",non cv lgbm approach
389,"['As we can see , many of the engineered features are present within the top 60 important features']",see mani engin featur present within top 60 import featur
390,"['# 2.Keras NN Model\n', ""<a id='keras'></a>""]",2 kera nn model id kera
391,"['## Combination Vis\n', ""<a id='vis'></a>""]",combin vi id vi
392,"['**DistPlot Analysis:** \n', '\n', 'We can see from the plots how the predictions for the validation sets, and the final target test set follows closely a similar distribution.\n', 'Which tells us that the test set can be a resemblance of validation sets we are using. \n', 'Then we can proceed with improving our scores in the validation set knowing that there is a high chance they will also improve in the test set.\n', '\n', '**Combination Analysis**\n', '\n', 'We can also see that the combination of both is adding some noise to the prediction, which in some cases can prove helpful when each model\n', 'was able to predict with some features better than the others\n', '\n', 'more testing is going on here to see how effecient a combination model can get.']",distplot analysi see plot predict valid set final target test set follow close similar distribut tell u test set resembl valid set use proceed improv score valid set know high chanc also improv test set combin analysi also see combin ad nois predict case prove help model abl predict featur better other test go see effeci combin model get
393,"['# Submissions\n', ""<a id='sub'></a>""]",submiss id sub
394,"['A Faster way to go up again and check what caused the results.\n', '[Training Options](#options)']",faster way go check caus result train option option
395,['# Make NN and train it'],make nn train
396,"['# Draw result\n', '## Density']",draw result densiti
397,"['It seems like here is small number of original variables, that combines into this dense rectangles. Wide empty lines can be interpreted like suffisient difference in some of original variable values, maybe categorical features.']",seem like small number origin variabl combin den rectangl wide empti line interpret like suffisi differ origin variabl valu mayb categor featur
398,['## Scatter plot colored by target'],scatter plot color target
399,['Hmmm...'],hmmm
400,"['High all,\n', '\n', 'I figured it would be interesting to compare the top scoring public kernels.\n', '\n', 'The analysis is also available(and may be updated) [here](https://ui.neptune.ml/jakub-czakon/santander/wiki/1-public_kernel_comparison).\n', '\n', '# Comparison Setup\n', '\n', '## Kernels considered:\n', '- https://www.kaggle.com/mhviraf/santander-compact-solution-14-lines-will-do\n', '- https://www.kaggle.com/kamalchhirang/simple-lightgbm-with-good-parameters\n', '- https://www.kaggle.com/sandeepkumar121995/magic-parameters\n', '- https://www.kaggle.com/jesucristo/30-lines-starter-solution-fast\n', '- https://www.kaggle.com/brandenkmurray/randomly-shuffled-data-also-works\n', '- https://www.kaggle.com/lavanyadml/santander-ls\n', '- https://www.kaggle.com/jesucristo/santander-magic-lgb\n', '- https://www.kaggle.com/gpreda/santander-fast-compact-solution\n', '- https://www.kaggle.com/jesucristo/40-lines-starter-solution-fast DELETED\n', '\n', ""I hope I didn't forget to upvote/fork any of those kernels. Good job people!\n"", '\n', '## Validation:\n', 'I adjusted all of them so that they would have the same validation schema:\n', '\n', '    N_SPLITS = 15\n', '    SEED = 2319   \n', '    folds = StratifiedKFold(n_splits=N_SPLITS, shuffle=False, random_state=SEED)\n', '    \n', '## Tracking\n', 'I logged all the experiment information to [Neptune](http://bit.ly/2FndEZO) to be able to easily compare it later:\n', '  - hyperparameters\n', '  - lightgbm training curves\n', '  - roc_auc metric on out of fold predictions\n', '  - confusion matrix\n', '  - ROC AUC curve\n', '  - prediction distribution plot\n', ' \n', '**Note** \n', ""You don't have to know how to track stuff with Neptune to follow this post. \n"", '\n', 'If you are interested, [here is an example neptune kernel](https://www.kaggle.com/jakubczakon/example-lightgbm-neptune-tracking).  \n', '[Neptune](http://bit.ly/2FndEZO) is free for non-organizations and you can easily use it from inside kaggle kernels (or any other place for that matter) to track your experiments. \n', '  \n', '# Results\n', ""Ok, let's explore the results.\n"", '\n', '## Scores\n', 'First, lets have a look at the validation results of those models:\n', '\n', '[Results dashboard](https://ui.neptune.ml/jakub-czakon/santander/experiments?filterId=288495a5-c965-4896-815c-7474fd95234f)\n', '\n', '![image](https://gist.githubusercontent.com/jakubczakon/f754769a39ea6b8fa9728ede49b9165c/raw/2507ca2f4ff706b4a358decf0ba714ead7010a0b/comparison8.png)\n', '\n', 'All the top kernels, which is no surprise, perform quite well and get us `Local CV &gt; 0.9`.\n', 'The results vary from `0.900464` to `0.901075`, which may not be a lot in a large picture of things, but for this particular competition could mean a difference of 1000+ places.\n', 'It seems that the `Random Shuffled Data Also Works` not only ""also works"" but works the best, which was surprising to me. \n', '\n', '## Hyperparameters\n', '\n', 'Looking at the parameters in the table above it seems that every kernel apart from the best one used `num_leaves=13`. Interesting.\n', 'To get more insights I decided to use the `plot_evaluations` function from the `skopt.plots` package: \n', '\n', '[Exploration experiment](https://ui.neptune.ml/jakub-czakon/santander/e/SAN1-170/channels)\n', '\n', '![image](https://gist.githubusercontent.com/jakubczakon/f754769a39ea6b8fa9728ede49b9165c/raw/2507ca2f4ff706b4a358decf0ba714ead7010a0b/comparison2.png)\n', '\n', ""It's a pretty useful tool, that lets you see what were the hyperparameter values that were explored, and which values were explored the most. The red dot shows the best run. \n"", '\n', 'Sometimes, you can see clear feature interactions (in the hyperparameter space) or the over/underexplored areas in the hyperparameter space. \n', 'In this particular case my conclusions are:\n', '- explore other `num_leaves` values between `3` and `13`\n', '- higher bagging fraction perform better\n', '- there seems to be a sweet spot when it comes to `early_stopping_rounds=3000`\n', '\n', '## Learning curves\n', 'Since I logged all the metrics during training with the `neptune_monitor` callback we can try and get some insights here. If you want to see how to create a custom callback for your tracking tool/setup [go to this example kernel](https://www.kaggle.com/jakubczakon/example-lightgbm-neptune-tracking).\n', ""Let's start with the comparison of the learning curves for all the experiments:\n"", '\n', '[Learning curves comparison](https://ui.neptune.ml/jakub-czakon/santander/compare?shortId=%5B%22SAN1-84%22%2C%22SAN1-59%22%2C%22SAN1-124%22%2C%22SAN1-142%22%2C%22SAN1-56%22%2C%22SAN1-140%22%2C%22SAN1-141%22%2C%22SAN1-58%22%2C%22SAN1-57%22%5D)\n', '\n', '![image](https://gist.githubusercontent.com/jakubczakon/f754769a39ea6b8fa9728ede49b9165c/raw/2507ca2f4ff706b4a358decf0ba714ead7010a0b/comparison9.png)\n', '\n', 'Apart from the `Randomly Shuffled Data Also Works` all those curves are quite similar. We can see that the ""shuffled"" gets to 0.9 slower but is overfitting considerably less. That is the result of the `num_leaves=3` but maybe shuffling features somehow plays a role here as well. It is probably worth exploring.\n', '\n', ""Let's now take a closer look at the learning curves of the top two kernels `Randomly Shuffled Data Also Works` and `Magic Parameters`.\n"", '\n', '**`Randomly Shuffled Data Also Works`**\n', '\n', '[Experiment](https://ui.neptune.ml/jakub-czakon/santander/e/SAN1-84/charts)\n', '\n', '![image](https://gist.githubusercontent.com/jakubczakon/f754769a39ea6b8fa9728ede49b9165c/raw/2507ca2f4ff706b4a358decf0ba714ead7010a0b/comparison12_random_shuffle.png)\n', '\n', 'We can see that there is some overfitting but not a lot. Training curves are almost identical for each fold, but when we look at the validation curves there is quite a lot of difference:\n', '\n', '![image](https://gist.githubusercontent.com/jakubczakon/f754769a39ea6b8fa9728ede49b9165c/raw/2507ca2f4ff706b4a358decf0ba714ead7010a0b/comparison13_random_shuffle.png)\n', '\n', 'The worst result was just 0.889 for fold 4 while the best one got 0.9111 on fold 13. \n', '\n', '**`Magic Parameters`**\n', 'Looking at this kernel:\n', '\n', '[Experiment](https://ui.neptune.ml/jakub-czakon/santander/e/SAN1-59/charts)\n', '\n', '![image](https://gist.githubusercontent.com/jakubczakon/f754769a39ea6b8fa9728ede49b9165c/raw/2507ca2f4ff706b4a358decf0ba714ead7010a0b/comparison10_magic_lgb.png)\n', '\n', 'Overfitting is significantly more visible here.\n', '\n', '![image](https://gist.githubusercontent.com/jakubczakon/f754769a39ea6b8fa9728ede49b9165c/raw/2507ca2f4ff706b4a358decf0ba714ead7010a0b/comparison11_magic_lgb.png)\n', '\n', 'The difference between best and worst folds is pretty much the same `fold 4 0.889` vs `fold 13 0.910`.\n', '\n', 'My conclusions are:\n', ' - `num_leaves=3` gives better results and controls overfitting, maybe other regularization params like l1/l2 could push the score up a bit more.\n', ' - the difference between best and worst folds is large.  Maybe the investigation of this problem can reveal insights about the underlying structure of the problem.\n', '\n', '## Prediction correlations\n', ""Let's look at the prediction correlations for out of fold train predictions and averaged test predictions respectively:\n"", '\n', '[Exploration experiment](https://ui.neptune.ml/jakub-czakon/santander/e/SAN1-170/channels)\n', '\n', '![image](https://gist.githubusercontent.com/jakubczakon/f754769a39ea6b8fa9728ede49b9165c/raw/2507ca2f4ff706b4a358decf0ba714ead7010a0b/comparison3.png)\n', '![image](https://gist.githubusercontent.com/jakubczakon/f754769a39ea6b8fa9728ede49b9165c/raw/2507ca2f4ff706b4a358decf0ba714ead7010a0b/comparison4.png)\n', '\n', 'Both for the train oof and test predictions, the correlations between public kernels are extremely high.\n', 'It seems that it will be very difficult to squeeze extra juice from blending/stacking those models.\n', '\n', 'My conclusions are:\n', '- we may need to add diversity to gain anything by blending\n', '\n', '## Predictions exploration\n', '\n', ""Let's start by looking at the standard model diagnostics for the best model:\n"", '\n', '[Best experiment](https://ui.neptune.ml/jakub-czakon/santander/e/SAN1-84/channels)\n', '![image](https://gist.githubusercontent.com/jakubczakon/f754769a39ea6b8fa9728ede49b9165c/raw/4d52325ae3ac0f2aeabf406a6c62a650b7f1b0c3/comparsion15.png)\n', '\n', 'Ok, it looks like it has trouble with giving a very low score to positive examples. \n', '\n', ""Let's take a look at the predictions that public kernels produce. \n"", '\n', 'I will begin by looking at the error distribution plot. I calculated it simply by substructing `prediction` from `target`.\n', '\n', '[Exploration experiment](https://ui.neptune.ml/jakub-czakon/santander/e/SAN1-170/channels)\n', '\n', '![image](https://gist.githubusercontent.com/jakubczakon/f754769a39ea6b8fa9728ede49b9165c/raw/2507ca2f4ff706b4a358decf0ba714ead7010a0b/comparison1.png)\n', '\n', 'Ok, it seems that our model is pretty good when it comes to predicting negative cases but has a lot of problems when it comes to predicting positive cases. Not only that, but it is quite confidently wrong, giving very low scores for some positive cases. \n', '\n', 'Now, I would like to take a look at the predictions themselves.\n', '[Exploration experiment](https://ui.neptune.ml/jakub-czakon/santander/e/SAN1-170/channels)\n', '\n', '![image](https://gist.githubusercontent.com/jakubczakon/f754769a39ea6b8fa9728ede49b9165c/raw/2507ca2f4ff706b4a358decf0ba714ead7010a0b/comparsion7.png)\n', '\n', 'And if we split it into positive and negative examples respectively:\n', '[Exploration experiment](https://ui.neptune.ml/jakub-czakon/santander/e/SAN1It is not super important to know how-170/channels)\n', '\n', '![image](https://gist.githubusercontent.com/jakubczakon/f754769a39ea6b8fa9728ede49b9165c/raw/2507ca2f4ff706b4a358decf0ba714ead7010a0b/comparison6.png)\n', '\n', '[Exploration experiment](https://ui.neptune.ml/jakub-czakon/santander/e/SAN1-170/channels)\n', '\n', '![image](https://gist.githubusercontent.com/jakubczakon/f754769a39ea6b8fa9728ede49b9165c/raw/2507ca2f4ff706b4a358decf0ba714ead7010a0b/comparison5.png)\n', '\n', 'Even though the prediction distributions at the first sight look very similar there could still be benefits to blending.\n', 'Especially those positive predictions differ from model to model quite a bit.\n', '\n', 'My conclusions are:\n', '- maybe there is some LB sauce that can be squeezed from blending after all\n', '- we should investigate how to make our model less sure when it is wrong about the positive cases (potentially add some models that do)\n', '\n', '# Final thoughts:\n', '- There is probably room to explore on the hyperparameter front\n', '- Difference between worst and best fold results is huge. Exploring it could bring insights\n', '- There is probably to blending/stacking public kernels but adding diversity may be important.\n', '\n', 'What do you think?\n', '\n', '\n', '# Edits:\n', '\n', 'Thank you @tilii7 for cleaning my thought.\n', 'Added: ""Not only that, it is quite confidently wrong (giving very low scores) for some positive cases.""  \n', 'Dropped: ""In simple words, some positive cases are given very low predictions by our models. ""']",high figur would interest compar top score public kernel analysi also avail may updat http ui neptun ml jakub czakon santand wiki 1 public kernel comparison comparison setup kernel consid http www kaggl com mhviraf santand compact solut 14 line http www kaggl com kamalchhirang simpl lightgbm good paramet http www kaggl com sandeepkumar121995 magic paramet http www kaggl com jesucristo 30 line starter solut fast http www kaggl com brandenkmurray randomli shuffl data also work http www kaggl com lavanyadml santand l http www kaggl com jesucristo santand magic lgb http www kaggl com gpreda santand fast compact solut http www kaggl com jesucristo 40 line starter solut fast delet hope forget upvot fork kernel good job peopl valid adjust would valid schema n split 15 seed 2319 fold stratifiedkfold n split n split shuffl fals random state seed track log experi inform neptun http bit ly 2fndezo abl easili compar later hyperparamet lightgbm train curv roc auc metric fold predict confus matrix roc auc curv predict distribut plot note know track stuff neptun follow post interest exampl neptun kernel http www kaggl com jakubczakon exampl lightgbm neptun track neptun http bit ly 2fndezo free non organ easili use insid kaggl kernel place matter track experi result ok let explor result score first let look valid result model result dashboard http ui neptun ml jakub czakon santand experi filterid 288495a5 c965 4896 815c 7474fd95234f imag http gist githubusercont com jakubczakon f754769a39ea6b8fa9728ede49b9165c raw 2507ca2f4ff706b4a358decf0ba714ead7010a0b comparison8 png top kernel surpris perform quit well get u local cv gt 0 9 result vari 0 900464 0 901075 may lot larg pictur thing particular competit could mean differ 1000 place seem random shuffl data also work also work work best surpris hyperparamet look paramet tabl seem everi kernel apart best one use num leav 13 interest get insight decid use plot evalu function skopt plot packag explor experi http ui neptun ml jakub czakon santand e san1 170 channel imag http gist githubusercont com jakubczakon f754769a39ea6b8fa9728ede49b9165c raw 2507ca2f4ff706b4a358decf0ba714ead7010a0b comparison2 png pretti use tool let see hyperparamet valu explor valu explor red dot show best run sometim see clear featur interact hyperparamet space underexplor area hyperparamet space particular case conclus explor num leav valu 3 13 higher bag fraction perform better seem sweet spot come earli stop round 3000 learn curv sinc log metric train neptun monitor callback tri get insight want see creat custom callback track tool setup go exampl kernel http www kaggl com jakubczakon exampl lightgbm neptun track let start comparison learn curv experi learn curv comparison http ui neptun ml jakub czakon santand compar shortid 5b 22san1 84 22 2c 22san1 59 22 2c 22san1 124 22 2c 22san1 142 22 2c 22san1 56 22 2c 22san1 140 22 2c 22san1 141 22 2c 22san1 58 22 2c 22san1 57 22 5d imag http gist githubusercont com jakubczakon f754769a39ea6b8fa9728ede49b9165c raw 2507ca2f4ff706b4a358decf0ba714ead7010a0b comparison9 png apart randomli shuffl data also work curv quit similar see shuffl get 0 9 slower overfit consider le result num leav 3 mayb shuffl featur somehow play role well probabl worth explor let take closer look learn curv top two kernel randomli shuffl data also work magic paramet randomli shuffl data also work experi http ui neptun ml jakub czakon santand e san1 84 chart imag http gist githubusercont com jakubczakon f754769a39ea6b8fa9728ede49b9165c raw 2507ca2f4ff706b4a358decf0ba714ead7010a0b comparison12 random shuffl png see overfit lot train curv almost ident fold look valid curv quit lot differ imag http gist githubusercont com jakubczakon f754769a39ea6b8fa9728ede49b9165c raw 2507ca2f4ff706b4a358decf0ba714ead7010a0b comparison13 random shuffl png worst result 0 889 fold 4 best one got 0 9111 fold 13 magic paramet look kernel experi http ui neptun ml jakub czakon santand e san1 59 chart imag http gist githubusercont com jakubczakon f754769a39ea6b8fa9728ede49b9165c raw 2507ca2f4ff706b4a358decf0ba714ead7010a0b comparison10 magic lgb png overfit significantli visibl imag http gist githubusercont com jakubczakon f754769a39ea6b8fa9728ede49b9165c raw 2507ca2f4ff706b4a358decf0ba714ead7010a0b comparison11 magic lgb png differ best worst fold pretti much fold 4 0 889 v fold 13 0 910 conclus num leav 3 give better result control overfit mayb regular param like l1 l2 could push score bit differ best worst fold larg mayb investig problem reveal insight underli structur problem predict correl let look predict correl fold train predict averag test predict respect explor experi http ui neptun ml jakub czakon santand e san1 170 channel imag http gist githubusercont com jakubczakon f754769a39ea6b8fa9728ede49b9165c raw 2507ca2f4ff706b4a358decf0ba714ead7010a0b comparison3 png imag http gist githubusercont com jakubczakon f754769a39ea6b8fa9728ede49b9165c raw 2507ca2f4ff706b4a358decf0ba714ead7010a0b comparison4 png train oof test predict correl public kernel extrem high seem difficult squeez extra juic blend stack model conclus may need add diver gain anyth blend predict explor let start look standard model diagnost best model best experi http ui neptun ml jakub czakon santand e san1 84 channel imag http gist githubusercont com jakubczakon f754769a39ea6b8fa9728ede49b9165c raw 4d52325ae3ac0f2aeabf406a6c62a650b7f1b0c3 comparsion15 png ok look like troubl give low score posit exampl let take look predict public kernel produc begin look error distribut plot calcul simpli substruct predict target explor experi http ui neptun ml jakub czakon santand e san1 170 channel imag http gist githubusercont com jakubczakon f754769a39ea6b8fa9728ede49b9165c raw 2507ca2f4ff706b4a358decf0ba714ead7010a0b comparison1 png ok seem model pretti good come predict neg case lot problem come predict posit case quit confid wrong give low score posit case would like take look predict explor experi http ui neptun ml jakub czakon santand e san1 170 channel imag http gist githubusercont com jakubczakon f754769a39ea6b8fa9728ede49b9165c raw 2507ca2f4ff706b4a358decf0ba714ead7010a0b comparsion7 png split posit neg exampl respect explor experi http ui neptun ml jakub czakon santand e san1it super import know 170 channel imag http gist githubusercont com jakubczakon f754769a39ea6b8fa9728ede49b9165c raw 2507ca2f4ff706b4a358decf0ba714ead7010a0b comparison6 png explor experi http ui neptun ml jakub czakon santand e san1 170 channel imag http gist githubusercont com jakubczakon f754769a39ea6b8fa9728ede49b9165c raw 2507ca2f4ff706b4a358decf0ba714ead7010a0b comparison5 png even though predict distribut first sight look similar could still benefit blend especi posit predict differ model model quit bit conclus mayb lb sauc squeez blend investig make model le sure wrong posit case potenti add model final thought probabl room explor hyperparamet front differ worst best fold result huge explor could bring insight probabl blend stack public kernel ad diver may import think edit thank tilii7 clean thought ad quit confid wrong give low score posit case drop simpl word posit case given low predict model
401,"['<h1><center><font size=""6"">Santander Customer Transaction Prediction</font></center></h1>\n', '<h1><center><font size=""5"">Can you identify who will make a transaction?</font></center></h1>\n', '\n', '<img src=""https://upload.wikimedia.org/wikipedia/commons/thumb/4/4a/Another_new_Santander_bank_-_geograph.org.uk_-_1710962.jpg/640px-Another_new_Santander_bank_-_geograph.org.uk_-_1710962.jpg"" width=""500""></img>\n', '\n', '<br>\n', '<b>\n', '    \n', 'Our data science team is continually challenging our machine learning algorithms, working with the global data science community to make sure we can more accurately identify new ways to solve our most common challenge, binary classification problems such as: is a customer satisfied? Will a customer buy this product? Can a customer pay this loan?\n', '\n', 'In this challenge, we invite Kagglers to help us identify which customers will make a specific transaction in the future, irrespective of the amount of money transacted. The data provided for this competition has the same structure as the real data we have available to solve this problem.\n', '\n', 'The data is anonimyzed, each row containing 200 numerical values identified just with a number.</b>\n', '     \n']",h1 center font size 6 santand custom transact predict font center h1 h1 center font size 5 identifi make transact font center h1 img src http upload wikimedia org wikipedia common thumb 4 4a anoth new santand bank geograph org uk 1710962 jpg 640px anoth new santand bank geograph org uk 1710962 jpg width 500 img br b data scienc team continu challeng machin learn algorithm work global data scienc commun make sure accur identifi new way solv common challeng binari classif problem custom satisfi custom buy product custom pay loan challeng invit kaggler help u identifi custom make specif transact futur irrespect amount money transact data provid competit structur real data avail solv problem data anonimyz row contain 200 numer valu identifi number b
402,['<a id=1><pre><b>Load Packages</b></pre></a>'],id 1 pre b load packag b pre
403,['<a id=1><pre><b>Import the Data</b></pre></a>'],id 1 pre b import data b pre
404,"['**Target = 0 or Target = 1, binary classification**']",target 0 target 1 binari classif
405,"[""Let's see basic stats on the 2 different groups.""]",let see basic stat 2 differ group
406,['**Missing data**'],miss data
407,['**There is no missing data**'],miss data
408,['### Check for Class Imbalance'],check class imbal
409,['<a id=1><pre><b>Classification augment</b></pre></a>'],id 1 pre b classif augment b pre
410,['# Build the Light GBM Model'],build light gbm model
411,['<a id=1><pre><b>Parameters</b></pre></a>'],id 1 pre b paramet b pre
412,['<a id=1><pre><b>Run LGBM model</b></pre></a>'],id 1 pre b run lgbm model b pre
413,['# Submission'],submiss
414,"[""This kernel is inspired by [@Branden Murray](https://www.kaggle.com/brandenkmurray)'s thoughts about [Any explanation why shuffling augmentation works?](https://www.kaggle.com/c/santander-customer-transaction-prediction/discussion/84847). I quote his hypothesis:\n"", '\n', "">Well, one explanation could be that it's how they generated the dataset in the first place. For each feature they had a distribution for target==0 and a distribution for target==1 and they randomly sampled from each and then put it together.\n"", '\n', ""I'm going to generate positive(target==1) and negative(target==0) samples for each feature, then combine the 200 simulated features together, I can get a synthetic data. With this simulated data, I can test Brander Murray's hypothesis.  \n"", '\n', 'If his hypothesis is true, then we can calculate the probability of positive sample(i.e. P(target=1|features)) using traditional Probability theory.']",kernel inspir branden murray http www kaggl com brandenkmurray thought explan shuffl augment work http www kaggl com c santand custom transact predict discus 84847 quot hypothesi well one explan could gener dataset first place featur distribut target 0 distribut target 1 randomli sampl put togeth go gener posit target 1 neg target 0 sampl featur combin 200 simul featur togeth get synthet data simul data test brander murray hypothesi hypothesi true calcul probabl posit sampl e p target 1 featur use tradit probabl theori
415,['### Calculate the mean/sd of postive and negative samples for each feature'],calcul mean sd postiv neg sampl featur
416,"[""### Synthetic data using normal distribution with train's mean/sd""]",synthet data use normal distribut train mean sd
417,['### Test the synthetic data'],test synthet data
418,"[""We can achieve 0.885 just using train data's mean and sd, this is not bad! Maybe the data is generated using this way!""]",achiev 0 885 use train data mean sd bad mayb data gener use way
419,"['### Calculate probability use hypothetical test\n', '\n', ""If each feature is generated by sample positive samples and negtive samples, then we can use hypothetical test to distinguish them. The positive samples and negative samples of each feature are slightly different. Let's take `var_0` and `var_1` as an example. ""]",calcul probabl use hypothet test featur gener sampl posit sampl negtiv sampl use hypothet test distinguish posit sampl neg sampl featur slightli differ let take var 0 var 1 exampl
420,"[""Let's use Z-test:\n"", '- Null hypothesis: a sample is negative(target == 0)\n', '- Alternative hypothesis: a sample is not negative(target == 1)\n', '\n', 'If we get a small pvalue(< 0.05), we reject the null hypothesis, i.e. the smaller the pvalue, the more likely a sample is positive.']",let use z test null hypothesi sampl neg target 0 altern hypothesi sampl neg target 1 get small pvalu 0 05 reject null hypothesi e smaller pvalu like sampl posit
421,"['Since we have 200 feats, we get 200 pvalue for each sample, we can multiply them together.']",sinc 200 feat get 200 pvalu sampl multipli togeth
422,"[""The smaller the prob1, the more likely a sample is positive. let's see the performance.""]",smaller prob1 like sampl posit let see perform
423,"['If we test whether a sample is positive, we can get another hypothetical test:\n', '- Null hypothesis: a sample is positive(target == 1)\n', '- Alternative hypothesis: a sample is not positive(target == 0)\n', '\n', 'If we get a small pvalue(< 0.05), we reject the null hypothesis, i.e. the bigger the pvalue, the more likely a sample is positive.']",test whether sampl posit get anoth hypothet test null hypothesi sampl posit target 1 altern hypothesi sampl posit target 0 get small pvalu 0 05 reject null hypothesi e bigger pvalu like sampl posit
424,['Combine the two prob together:'],combin two prob togeth
425,"[""**We can get 0.874 just using Probability theory, It's quite good I think.**""]",get 0 874 use probabl theori quit good think
426,[' ### Use this mothed to predict test.csv'],use moth predict test csv
427,"['### Conclusion\n', '\n', ""Branden Murray's hypothesis **For each feature they had a distribution for target==0 and a distribution for target==1 and they randomly sampled from each and then put it together** is a wonderful explanation of shuffling also works and weak interaction between features.\n"", '\n', 'We can even use tranditional Probability theory to calculate the P(target==1) value to achive 0.874 local cv. But this model is still too naive, the feature is not normal distribution(I try normality test, none of the 200 features passed), and the positive samples and negative samples is not variance homogeneity(2/3 of the features failed variance homogeneity test).\n', '\n', 'Hope this kernal can help, thanks!']",conclus branden murray hypothesi featur distribut target 0 distribut target 1 randomli sampl put togeth wonder explan shuffl also work weak interact featur even use trandit probabl theori calcul p target 1 valu achiv 0 874 local cv model still naiv featur normal distribut tri normal test none 200 featur pas posit sampl neg sampl varianc homogen 2 3 featur fail varianc homogen test hope kernal help thank
428,"[""In the kernel [Modified Naive Bayes scores 0.899 LB - Santander](https://www.kaggle.com/cdeotte/modified-naive-bayes-santander-0-899), [Chris Deotte](https://www.kaggle.com/cdeotte) has demonstrated that Naive Bayes can be a simple but powful method when there is little or no interaction between the features. I think it's a good time to study Naive Bayes Method.\n"", '\n', ""In this kernel, I'll try to introduce Naive Bayes method step by step.""]",kernel modifi naiv bay score 0 899 lb santand http www kaggl com cdeott modifi naiv bay santand 0 899 chri deott http www kaggl com cdeott demonstr naiv bay simpl pow method littl interact featur think good time studi naiv bay method kernel tri introduc naiv bay method step step
429,"[""### Bayes' theorem\n"", '\n', ""Bayes' theorem can be deduced by conditional probability:\n"", '\n', '$$\n', 'P(A|B) = \\frac {P(A \\bigcap B)}{P(B)}\n', '$$\n', '\n', '$$\n', 'P(B|A) = \\frac {P(A \\bigcap B)}{P(A)}\n', '$$\n', '\n', '$$\n', '\\Rightarrow P(A|B) = \\frac {P(B|A) \\cdot P(A)} {P(B)}\n', '$$']",bay theorem bay theorem deduc condit probabl p b frac p bigcap b p b p b frac p bigcap b p rightarrow p b frac p b cdot p p b
430,"['Suppose we have features $X \\in R^n$, target $y \\in \\{+1, -1\\}$, for a given $x_0$ our goal is to predict $y=+1$ or $-1$.\n', '\n', 'we can achive this by calculate\n', '\n', '$$P(y=+1 | X=x_0)$$ \n', 'and\n', '$$P(y=-1 |  X=x_0)$$\n', '\n', 'then choose the one with bigger probability.']",suppos featur x r n target 1 1 given x 0 goal predict 1 1 achiv calcul p 1 x x 0 p 1 x x 0 choos one bigger probabl
431,"[""How can we calculate $P(y=+1|X=x_0)$ ? We can use Bayes's theorem:\n"", '$$\n', 'P(y=+1|X=x_0) = \\frac {P(X=x_0 | y=+1) \\cdot P(y=+1)} {P(X=x_0)}\n', '$$\n', '\n', ""Because $P(X=X_0)$ is the probability(or frequency) of a sample in test set, it's the same for every sample, so we can simply write the formular as:\n"", '\n', '$$\n', 'P(y=+1|X=x_0) = P(X=x_0 | y=+1) \\cdot P(y=+1)\n', '$$\n', '\n', '$P(y=+1/-1)$ is called priori probability and $P(X=x_0 | y=+1)$ is called conditional probability. Training process is to estimate this two kind of probability.\n']",calcul p 1 x x 0 use bay theorem p 1 x x 0 frac p x x 0 1 cdot p 1 p x x 0 p x x 0 probabl frequenc sampl test set everi sampl simpli write formular p 1 x x 0 p x x 0 1 cdot p 1 p 1 1 call priori probabl p x x 0 1 call condit probabl train process estim two kind probabl
432,"['### Naive Bayes\n', '\n', 'If we have many features, then:\n', '$$\n', 'P(X=x_0 | y=+1) = P(X^{(1)}=x_0^{(1)}, X^{(2)}=x_0^{(2)}, ..., X^{(n)}=x_0^{(n)} | y=+1)\n', '$$\n', '\n', 'To simplify this problem, we assume the features as independent(this is the Naive means):\n', '$$\n', 'P(X=x_0 | y=+1) = P(X^{(1)}=x_0^{(1)} | y=+1) \\cdot P(X^{(2)}=x_0^{(2)} | y=+1) \\cdot, ..., \\cdot P(X^{(n)}=x_0^{(n)} | y=+1)\n', '$$']",naiv bay mani featur p x x 0 1 p x 1 x 0 1 x 2 x 0 2 x n x 0 n 1 simplifi problem assum featur independ naiv mean p x x 0 1 p x 1 x 0 1 1 cdot p x 2 x 0 2 1 cdot cdot p x n x 0 n 1
433,"['### Discrete variable\n', '\n', 'We will use a sample example to demostrate the training and predicting process of Naive Bayes for discrete variable.']",discret variabl use sampl exampl demostr train predict process naiv bay discret variabl
434,"['Try to use data to training a Naive Bayes classifier, and classify the sample $X_0=(2, S)$.']",tri use data train naiv bay classifi classifi sampl x 0 2
435,['Calculate priori probability $P(y=1)$ and $P(y=-1)$'],calcul priori probabl p 1 p 1
436,"['$$\n', 'P(y=1) = \\frac {9}{15}, P(y=-1) = \\frac {6}{15}\n', '$$']",p 1 frac 9 15 p 1 frac 6 15
437,['Calculate conditional probability'],calcul condit probabl
438,"['$$\n', 'P(X1=1 | y=1) = \\frac{2}{9}, P(X1=2 | y=1) = \\frac{3}{9}, P(X1=3 | y=1) = \\frac{4}{9}\n', '$$']",p x1 1 1 frac 2 9 p x1 2 1 frac 3 9 p x1 3 1 frac 4 9
439,"['$$\n', 'P(X2=S | y=1) = \\frac{1}{9}, P(X2=M | y=1) = \\frac{4}{9}, P(X2=L | y=1) = \\frac{4}{9}\n', '$$']",p x2 1 frac 1 9 p x2 1 frac 4 9 p x2 l 1 frac 4 9
440,"['$$\n', 'P(X1=1 | y=-1) = \\frac{3}{6}, P(X1=2 | y=-1) = \\frac{2}{6}, P(X1=3 | y=-1) = \\frac{1}{6}\n', '$$']",p x1 1 1 frac 3 6 p x1 2 1 frac 2 6 p x1 3 1 frac 1 6
441,"['$$\n', 'P(X2=S | y=1) = \\frac{3}{6}, P(X2=M | y=1) = \\frac{2}{6}, P(X2=L | y=1) = \\frac{1}{6}\n', '$$']",p x2 1 frac 3 6 p x2 1 frac 2 6 p x2 l 1 frac 1 6
442,"['For the given sample $X_0=(2, S)$:\n', '$$\n', 'P(y=1 | X=X_0) = P(y=1) \\cdot P(X1=2|y=1) \\cdot P(X2=S|y=1) = \\frac{9}{15} \\cdot \\frac{3}{9} \\cdot \\frac{1}{9} = \\frac{1}{45}\n', '$$\n', '\n', '$$\n', 'P(y=-1 | X=X_0) = P(y=-1) \\cdot P(X1=2|y=-1) \\cdot P(X2=S|y=-1) = \\frac{6}{15} \\cdot \\frac{2}{6} \\cdot \\frac{3}{6} = \\frac{1}{15}\n', '$$\n', '\n', 'Because $P(y=-1|X=X_0) > P(y=1|X=X_0)$, so $y=-1$\n', '\n', 'This is Naive Bayes for discrete variable, pretty simple.']",given sampl x 0 2 p 1 x x 0 p 1 cdot p x1 2 1 cdot p x2 1 frac 9 15 cdot frac 3 9 cdot frac 1 9 frac 1 45 p 1 x x 0 p 1 cdot p x1 2 1 cdot p x2 1 frac 6 15 cdot frac 2 6 cdot frac 3 6 frac 1 15 p 1 x x 0 p 1 x x 0 1 naiv bay discret variabl pretti simpl
443,['### Gaussian Naive Bayes'],gaussian naiv bay
444,"[""When dealing with continuous variable, how do you calculate the probabilty for a given value(e.g. x=1)? The probability should be zero. So we'd better calculate the probability of a interval(e.g. $1-\\Delta < x < 1+\\Delta$).""]",deal continu variabl calcul probabilti given valu e g x 1 probabl zero better calcul probabl interv e g 1 delta x 1 delta
445,"['If the interval is small enough(i.e. $\\Delta \\rightarrow 0$), the probability of a given value(e.g. x=1) can be represented by probability density(pdf) value. How can we know the probability function of a variable? The convenient way is to estimate using normal distribution. This is the **Gaussian Naive Bayes**.  ']",interv small enough e delta rightarrow 0 probabl given valu e g x 1 repres probabl densiti pdf valu know probabl function variabl conveni way estim use normal distribut gaussian naiv bay
446,"[""Let's apply Gaussian Naive Bayes to our Santander data.""]",let appli gaussian naiv bay santand data
447,['Calculate mean/sd of train data for each each feature.'],calcul mean sd train data featur
448,['Using normal distribution to estimate each feature'],use normal distribut estim featur
449,"['Gaussian Naive Bayes can give us 0.890 AUC, which is quite good!']",gaussian naiv bay give u 0 890 auc quit good
450,['### Remove the Gaussian constrain'],remov gaussian constrain
451,"[""Infact our data is not normal distributed, we can achive better score with Gaussian constran removed. let's take `var_0` as an example.""]",infact data normal distribut achiv better score gaussian constran remov let take var 0 exampl
452,"[""We can see that the data is very different from normal distribution, we need use more accurate probability density function to estimate, this can be done by kernel function estimation. Let's use `scipy.stats.kde.gaussian_kde` ""]",see data differ normal distribut need use accur probabl densiti function estim done kernel function estim let use scipi stat kde gaussian kde
453,"['Kernel funtion can fit the data better, which will give us better accuracy.']",kernel funtion fit data better give u better accuraci
454,"[""It's too slow, we can speed up by binize the variable values.""]",slow speed biniz variabl valu
455,"[""Using more accurate kernel function, we can achieve 0.909 AUC(maybe a little overfit, since we fit train's data, but it's not too much). Let's use this model to predict the test data.""]",use accur kernel function achiev 0 909 auc mayb littl overfit sinc fit train data much let use model predict test data
456,"['### Conclusion\n', '\n', 'In this kernel we demonstrate how Naive bayes works, we build Gaussian Naive Bayes, which gives us 0.890 AUC. By remove Gaussian constrain and choosing more accurate kernel function, we can get better performance.\n', '\n', 'Holp this can help, thanks!']",conclus kernel demonstr naiv bay work build gaussian naiv bay give u 0 890 auc remov gaussian constrain choos accur kernel function get better perform holp help thank
457,"['In this kernel, I implement vectorized PDF caculation (without for loop) to get their correlation matrix. This is helpful to study feature grouping.\n', 'credits to @sibmike https://www.kaggle.com/sibmike/are-vars-mixed-up-time-intervals']",kernel implement vector pdf cacul without loop get correl matrix help studi featur group credit sibmik http www kaggl com sibmik var mix time interv
458,['**Functions**'],function
459,['**load data & group vars**'],load data group var
460,"[""**We can group features using this correlation matrix. For example, var_0 and var_2's pdfs is 0.97+ correlated. We can confirm it using the figure below.**""]",group featur use correl matrix exampl var 0 var 2 pdf 0 97 correl confirm use figur
461,['**We can find the group of a var using the following functions.**'],find group var use follow function
463,['# Data'],data
464,['# FE'],fe
465,['## Clustering'],cluster
466,['## Summary statistics'],summari statist
467,['# Parameter tuning'],paramet tune
468,"['This is my first attempt to write the notebook from scratch. I have been playing around with exisitng kernels for competition until now. Comments, suggestions, recommendations are all very welcomed. \n', '\n', 'I will be modeling using the following algorithms -\n', '* Logistic Regression\n', '* Decision Tree Classifier\n', '* Random Forest Classifier\n', '* Light Gradient Boosting Method\n', '\n', ""Let's start by importing necessary packages -""]",first attempt write notebook scratch play around exisitng kernel competit comment suggest recommend welcom model use follow algorithm logist regress decis tree classifi random forest classifi light gradient boost method let start import necessari packag
469,['### Import Datasets'],import dataset
470,"['The dataset consists of an ID_code, 200 input variables (all numeric) and a binary target variable representing the transaction-happened. Since the entire dataset is masked, cannot do much of exploratory data analysis']",dataset consist id code 200 input variabl numer binari target variabl repres transact happen sinc entir dataset mask much exploratori data analysi
471,['This is an unbalanced classification problem with only 10% records having target variable = 1. '],unbalanc classif problem 10 record target variabl 1
472,"['## Modeling\n', ""I would be trying a few algorithms, starting from the most simple Logistic Regression, followed by Decision Tree, Random Forest and finally, Light GBM. To build the modeling pipeline, let's import all the necessary packages we would need ""]",model would tri algorithm start simpl logist regress follow decis tree random forest final light gbm build model pipelin let import necessari packag would need
473,"[""Let's separate input variables and target variable. Have also created a features list with all input variable names. ""]",let separ input variabl target variabl also creat featur list input variabl name
474,"['## Logistic Regression\n', ""We start with most basic algorithm used for classification problems. Initial model with defining only the regularization paramenter (C) yielded 0.6 AUC. Since this is an unbalanced dataset, we need to define another **paramenter 'class_weight = balanced'** which will give equal weights to both the targets irrespective of their reperesentation in the training dataset. We can even define classwise weights using this parameter, if needed ""]",logist regress start basic algorithm use classif problem initi model defin regular parament c yield 0 6 auc sinc unbalanc dataset need defin anoth parament class weight balanc give equal weight target irrespect reperesent train dataset even defin classwis weight use paramet need
475,"['## Performance Function\n', 'Since we will be building multiple models, it is advisable to create a function that can be called with different outputs of each model. This is a simple function which takes in the Predicted Validation Target and Actual Validation Target. It then gives out classification summary like **confusion matrix and AUC score **']",perform function sinc build multipl model advis creat function call differ output model simpl function take predict valid target actual valid target give classif summari like confus matrix auc score
476,"['### Logistic Regresssion Result \n', 'This model gave out an **AUC of 0.854** on validation set and 0.855 on Public Leaderboard for the test file']",logist regress result model gave auc 0 854 valid set 0 855 public leaderboard test file
477,"['## Decision Trees\n', 'Moving on to a slightly advanced algorithm, decision trees. Again, the parameters here are class_weight to deal with unbalanced target variable, random_state for reproducability of same trees. The feature max_features and min_sample_leaf are used to prune the tree and avoid overfitting to the training data. \n', '\n', '**Max_features** defines what proportion of available input features will be used to create tree. \n', '\n', '**Min_sample_leaf** restricts the minimum number of samples in a leaf node, making sure none of the leaf nodes has less than 80 samples in it. If leaf nodes have less samples it implies we have grown the tree too much and trying to predict each sample very precisely, thus leading to overfitting.  ']",decis tree move slightli advanc algorithm decis tree paramet class weight deal unbalanc target variabl random state reproduc tree featur max featur min sampl leaf use prune tree avoid overfit train data max featur defin proport avail input featur use creat tree min sampl leaf restrict minimum number sampl leaf node make sure none leaf node le 80 sampl leaf node le sampl impli grown tree much tri predict sampl precis thu lead overfit
478,"['### Decision Tree Results:\n', 'Basic decision tree is giving us **0.651 AUC score** on the validation set and 0.650 AUC score on the test set submitted on public leaderboard ']",decis tree result basic decis tree give u 0 651 auc score valid set 0 650 auc score test set submit public leaderboard
479,"[""Let's take a look at these features and plot them on a box and whiskrers chart""]",let take look featur plot box whiskrer chart
480,"['## Ensemble Learning\n', '[Ensemble Learning](https://en.wikipedia.org/wiki/Ensemble_learning) refers to the algorithms that created using ensembles of variour learning algorithms. So, to give you an example, random forests are ensembles of many decision tree estimators. \n', '\n', 'There are 2 types of ensemble learning algorithms -\n', '**1. Bagging Algorithms:** Bagging involves having each model in the ensemble vote with equal weight for the final output. In order to promote model variance, bagging trains each model in the ensemble using a randomly drawn subset of the training set\n', '**2. Boosting Algorithms:** As Wikipedia defines, boosting involves incrementally building an ensemble by training each new model instance to emphasize the training instances that previous models mis-classified.\n', '\n', '## Random Forest\n', ""Let's start with building a random forest, with parameters like class_weight, random_state, and hyperparameters like max_features and min_sample_leaf as earlier. We have also defined the n_estimators which is a compulsory parameter. This defines the number of decision trees that will be present in the forest. ""]",ensembl learn ensembl learn http en wikipedia org wiki ensembl learn refer algorithm creat use ensembl variour learn algorithm give exampl random forest ensembl mani decis tree estim 2 type ensembl learn algorithm 1 bag algorithm bag involv model ensembl vote equal weight final output order promot model varianc bag train model ensembl use randomli drawn subset train set 2 boost algorithm wikipedia defin boost involv increment build ensembl train new model instanc emphas train instanc previou model mi classifi random forest let start build random forest paramet like class weight random state hyperparamet like max featur min sampl leaf earlier also defin n estim compulsori paramet defin number decis tree present forest
481,"[' ### Random Forest Results:\n', ' Basic random forest is giving us **0.787 AUC score** on the validation set and 0.789 AUC score on the test set submitted on public leaderboard']",random forest result basic random forest give u 0 787 auc score valid set 0 789 auc score test set submit public leaderboard
482,['The feature importance we get from random forest is very similar to the list we got from decision trees '],featur import get random forest similar list got decis tree
483,"['## Light Gradient Boosting Method\n', '\n', '**WHAT IS IT? **\n', '\n', 'Light GBM is a gradient boosting framework that uses tree based learning algorithm. It grows tree vertically while other algorithm grows trees horizontally meaning that Light GBM grows tree leaf-wise while other algorithm grows level-wise. Leaf-wise algorithm can reduce more loss than a level-wise algorithm.\n', '\n', '**WHY USE LGB?**\n', '\n', 'It is ‘Light’ because of its high speed. It can handle large data, requires low memory to run and focuses on accuracy of results. Also supports GPU learning and thus data scientists/ Kagglers are widely using LGBM for data science application development.\n', '\n', '**TIPS & TRICKS**\n', '\n', '* The algorithm easily overfits and thus, should not be used with small (< 10K rows) datasets.\n', '* Deal with overfitting using these parameters:\n', '    1. Small Maximum Depth\n', '    2. Large Minimum Data in a Leaf\n', '    3. Small Feature and Bagging Fraction\n', '* Improve the training speed\n', '    1. Small Bagging Fraction\n', '    2. Early Stopping Round \n', '* Use small\xa0learning_rate\xa0with large\xa0num_iterations for better accuracy\n', '* Ideally, the value of num_leaves should be less than or equal to 2^(max_depth). Value more than this will result in overfitting\n', '* **If you have a big enough dataset, use this algorithm at least once. It’s accuracy has challenged other boosting algorithms**']",light gradient boost method light gbm gradient boost framework use tree base learn algorithm grow tree vertic algorithm grow tree horizont mean light gbm grow tree leaf wise algorithm grow level wise leaf wise algorithm reduc loss level wise algorithm use lgb light high speed handl larg data requir low memori run focus accuraci result also support gpu learn thu data scientist kaggler wide use lgbm data scienc applic develop tip trick algorithm easili overfit thu use small 10k row dataset deal overfit use paramet 1 small maximum depth 2 larg minimum data leaf 3 small featur bag fraction improv train speed 1 small bag fraction 2 earli stop round use small xa0learn rate xa0with larg xa0num iter better accuraci ideal valu num leav le equal 2 max depth valu result overfit big enough dataset use algorithm least accuraci challeng boost algorithm
484,"['### Light GBM Results:\n', 'The AUC Score drastically improves from 0.650 in our Decision Tree model to **an AUC score of 0.89** in our ensemble of trees, Light GBM model. The public leaderboard scores after submitting the test predictions come out to be 0.891\n', '\n', 'The feature importance though, it has some variables similar to those we saw in the tree models but majority of them are new in the top 10 most important variable list']",light gbm result auc score drastic improv 0 650 decis tree model auc score 0 89 ensembl tree light gbm model public leaderboard score submit test predict come 0 891 featur import though variabl similar saw tree model major new top 10 import variabl list
485,"['## Next Stpes:\n', 'Now that we have a considerably good AUC score to start with, we can improve on it. A very promising approach is to create new features based on the domain knowledge or based on the EDA we usually do as the first step. Tuning the model or creating a more sophisticated stacked architecture helps improve the score too.\n', '\n']",next stpe consider good auc score start improv promis approach creat new featur base domain knowledg base eda usual first step tune model creat sophist stack architectur help improv score
486,"['Hi there guys. <br />\n', 'I\'m a **person who have jumped into kaggle three month ago**. During studying many enlighting expert\'s kernels, I\'ve felt kind of **embarrassed feeling** about using hyperparameters of major algorithms such as Xgboost and LightGBM. You guys could reply my opinion like this, **""Why you blame your fault to them?""** \n', '### But, I definitely have **HUGE THANKS TO THEM!!** **Thanks to SUPER BRILLIANT EXPERTS OF KAGGLE** <br />\n', '\n', 'The reason why I make this kernel is that some people use **""lightgbm.train""** and the others use **""lightgbm.LGBMClassifier""** for their model. When I see the differences of them, It makes me insane!! Because the **parameters btw two kinds of kernel as I said above seem pretty different!!** <br />\n', '\n', ""So, In this kernel, I'll discover <br />\n"", '**1. the true meaning of them and aliases of hyperparameters by looking official document of lightgbm.** <br />\n', ""**2. parameter tuning by referring two website where I'll comment below notebooks** <br />\n"", '\n', '### I hope that **two kinds of people** to see this kernel,\n', '\n', ""1. **One is for people who have felt simliar feeling like me.** For them, I'll describe as detail as I can what I've learned and I'd like to share magnificant post for explaining what the Gradient Boosting and the Xgboost are!!<br />\n"", '\n', 'The posts are below!!\n', '* Gradient Boost\n', '>  https://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/\n', '* Xgboost\n', '> https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/\n', '\n', '\n', ""2. **I hope THE OTHERS are enlighting experts** who could give comment about what I understands through this kernel, I would really happy if you guys comment this kernel !! and Could you guys give me a great post about LightGBM parameter or parmeter tuning?? cuz I already have few posts about GBM and XGBoost but I don't have about LightGBM!! (I know generally it seems same one but I think there is regularization in LightGBM) \n"", '\n', ""There is Korean comments for my studying for each sentences by Gabriel Preda's explaination. But I didn't only copy and paste this code. I've changed some code for my own!! \n"", '\n', ""# I'm staying to tune hyperparameters and I will frequently update this Kernel frequently!!.""]",hi guy br person jump kaggl three month ago studi mani enlight expert kernel felt kind embarrass feel use hyperparamet major algorithm xgboost lightgbm guy could repli opinion like blame fault definit huge thank thank super brilliant expert kaggl br reason make kernel peopl use lightgbm train other use lightgbm lgbmclassifi model see differ make insan paramet btw two kind kernel said seem pretti differ br kernel discov br 1 true mean alias hyperparamet look offici document lightgbm br 2 paramet tune refer two websit comment notebook br hope two kind peopl see kernel 1 one peopl felt simliar feel like describ detail learn like share magnific post explain gradient boost xgboost br post gradient boost http www analyticsvidhya com blog 2016 02 complet guid paramet tune gradient boost gbm python xgboost http www analyticsvidhya com blog 2016 03 complet guid paramet tune xgboost code python 2 hope other enlight expert could give comment understand kernel would realli happi guy comment kernel could guy give great post lightgbm paramet parmet tune cuz alreadi post gbm xgboost lightgbm know gener seem one think regular lightgbm korean comment studi sentenc gabriel preda explain copi past code chang code stay tune hyperparamet frequent updat kernel frequent
487,"['# Reference\n', '\n', ""Gabriel Preda's santander-eda-and-prediction\n"", '> https://www.kaggle.com/gpreda/santander-eda-and-prediction\n', 'this kernel uses lightgbm.train for prediction\n', '\n', ""Will Koehrsen's A Complete Introduction and Walkthrough [Costa Rican Houshold] \n"", '> https://www.kaggle.com/willkoehrsen/a-complete-introduction-and-walkthrough\n', 'this kernel uses LGBMClassifier for prediction\n', '\n', ""Rudolph's Porto: xgb+lgb kfold LB 0.282 [Porto Seguro’s Safe Driver Prediction]\n"", '> https://www.kaggle.com/rshally/porto-xgb-lgb-kfold-lb-0-282\n', 'this kernel uses lightgbm.train for prediction']",refer gabriel preda santand eda predict http www kaggl com gpreda santand eda predict kernel use lightgbm train predict koehrsen complet introduct walkthrough costa rican houshold http www kaggl com willkoehrsen complet introduct walkthrough kernel use lgbmclassifi predict rudolph porto xgb lgb kfold lb 0 282 porto seguro safe driver predict http www kaggl com rshalli porto xgb lgb kfold lb 0 282 kernel use lightgbm train predict
488,"[""# <a id='1'>Introduction</a>  \n"", '\n', 'In this challenge, Santander invites Kagglers to help them identify which customers will make a specific transaction in the future, irrespective of the amount of money transacted. The data provided for this competition has the same structure as the real data they have available to solve this problem.  \n', '\n', '이번 컴피티션에서는 어느 소비자들이 훗날에 현금을 인출할 것인지를 구분하는 것이 목표입니다. 이번 대회의 데이터는 실제 데이터와 같은 구조로 제공되어있습니다.\n', '\n', 'The data is anonimyzed, each row containing 200 numerical values identified just with a number.  \n', '\n', '데이터는 무기명으로 되어있고 각각의 row는 200개의 서로 다른 컬럼을 가지고 있습니다.\n', '\n', 'In the following we will explore the data, prepare it for a model, train a model and predict the target value for the test set, then prepare a submission.\n', '\n', '다음에서 우리는 데이터를 살펴보고, 모델링 준비를하고, 모델을 훈련시키고 타겟 값을 테스트셋에서 예측하고 제출까지 해솝시다.']",id 1 introduct challeng santand invit kaggler help identifi custom make specif transact futur irrespect amount money transact data provid competit structur real data avail solv problem data anonimyz row contain 200 numer valu identifi number row 200 follow explor data prepar model train model predict target valu test set prepar submiss
489,"['## Load data   \n', '\n', ""Let's check what data files are available.\n"", '\n', '우리가 사용가능한 데이터 파일들을 알아 봅시다.']",load data let check data file avail
490,"[""Let's load the train and test data files.""]",let load train test data file
491,"[""# <a id='3'>Data exploration</a>  \n"", '\n', ""## <a id='31'>Check the data</a>  \n"", '\n', ""Let's check the train and test set.\n"", '\n', '훈련셋과 테스트셋을 확인해봅시다.']",id 3 data explor id 31 check data let check train test set
492,"['Both train and test data have 200,000 entries and 202, respectivelly 201 columns. \n', '\n', '훈련셋과 테스트셋 모두 200,000개의 행을가지고 각각 202, 201 개의 컬럼수를 가지고 있습니다.\n', '\n', ""Let's glimpse train and test dataset.\n"", '\n', '간단하게 두 세트를 살펴볼까요.\n']",train test data 200 000 entri 202 respectivelli 201 column 200 000 202 201 let glimps train test dataset
493,"['Train contains:  \n', '\n', '* **ID_code** (string);  \n', '* **target**;  \n', '* **200** numerical variables, named from **var_0** to **var_199**;\n', '\n', '훈련세트는. ID, Target 그리고 200개의 숫자값들이 있습니다.\n', '\n', 'Test contains:  \n', '\n', '* **ID_code** (string);  \n', '* **200** numerical variables, named from **var_0** to **var_199**;\n', '\n', '테스트 셋에는 타겟값을 제외한 것들이 있습니다.\n', '\n', ""Let's check if there are any missing data. We will also chech(*k) the type of data.\n"", '\n', '손실값들에 대해서 한번 살펴볼까요> 그리고 데이터들의 타입에 대해서도 알아봅시다.\n', '\n', 'We check first train.\n', '\n', '먼저 훈련세트입니다.']",train contain id code string target 200 numer variabl name var 0 var 199 id target 200 test contain id code string 200 numer variabl name var 0 var 199 let check miss data also chech k type data check first train
494,"['We can make few observations here:   \n', '우리가 찾은 것들은 아래와 같습니다.\n', '\n', '* standard deviation is relatively large for both train and test variable data;\n', '훈련 데이터와 테스트 데이터 모두 표준편차가 크다는 것\n', '* min, max, mean, sdt values for train and test data looks quite close;\n', '최소,최대,평균,표준편차 값이 훈련과 테스트셋에서 밀접해 보인다는 것\n', '* mean values are distributed over a large range.\n', '평균값의 변동이 크다는 것\n', '\n', ""The number of values in train and test set is the same. Let's plot the scatter plot for train and test set for few of the features.\n"", '훈련과 테스트 셋에서의 값의 수는 동일하다. 그렇다면 몇몇 특징들에 대해서 산포도를 그려봅시다.\n']",make observ standard deviat rel larg train test variabl data min max mean sdt valu train test data look quit close mean valu distribut larg rang number valu train test set let plot scatter plot train test set featur
495,"['\n', ""## <a id='32'>Density plots of features</a>  \n"", '\n', ""Let's show now the density plot of variables in train dataset. \n"", '\n', 'We represent with different colors the distribution for values with **target** value **0** and **1**.']",id 32 densiti plot featur let show densiti plot variabl train dataset repres differ color distribut valu target valu 0 1
496,"['We can observe that there is a considerable number of features with significant different distribution for the two target values.  \n', 'For example, **var_0**, **var_1**, **var_2**, **var_5**, **var_9**, **var_13**, **var_106**, **var_109**, **var_139** and many others.\n', '\n', '우리는 두 개의 타겟값에 따라서 상당이 다른 분포를 가지고 있는 상당한 수의 특징들을 살펴볼 수 있습니다.\n', '예를 들면, **var_0**, **var_1**, **var_2**, **var_5**, **var_9**, **var_13**, **var_106**, **var_109**, **var_139** 와 다른 것들 말입니다.\n', '\n', 'Also some features, like **var_2**, **var_13**, **var_26**, **var_55**, **var_175**, **var_184**, **var_196** shows a distribution that resambles to a bivariate distribution.\n', '\n', '그리고 몇몇 특징들, **var_2**, **var_13**, **var_26**, **var_55**, **var_175**, **var_184**, **var_196**, 은 이변량분포와 닮은 분포를 보여줍니다.\n', '\n', 'We will take this into consideration in the future for the selection of the features for our prediction model.  \n', '\n', '우리는 이것들을 우리의 예측모델에 feature selection시에 고려하는 참고자료로 사용할 것입니다.\n', '\n', ""Le't s now look to the distribution of the same features in parallel in train and test datasets. \n"", '\n', '그렇다면 이제는 훈련셋과 테스트셋을 평행적으로 같이 보겠습니다.\n', '\n', ""The first 100 values are displayed in the following cell. Press <font color='red'>**Output**</font> to display the plots.\n"", '\n', '첫 번째 100개의 값들은 아래의 그림과 같이 생겼습니다.']",observ consider number featur signific differ distribut two target valu exampl var 0 var 1 var 2 var 5 var 9 var 13 var 106 var 109 var 139 mani other var 0 var 1 var 2 var 5 var 9 var 13 var 106 var 109 var 139 also featur like var 2 var 13 var 26 var 55 var 175 var 184 var 196 show distribut resambl bivari distribut var 2 var 13 var 26 var 55 var 175 var 184 var 196 take consider futur select featur predict model featur select le look distribut featur parallel train test dataset first 100 valu display follow cell press font color red output font display plot 100
497,"['The train and test seems to be well ballanced with respect with distribution of the numeric variables.  \n', '\n', '훈련셋과 테스트셋은 numeric 변수들의 분포들이 잘 균형을 갖추고 있는 듯 합니다.\n', '\n', ""## <a id='33'>Distribution of mean and std</a>  \n"", '평균과 표준편차의 분포\n', '\n', ""Let's check the distribution of the mean values per row in the train and test set.\n"", '\n', '행별로 훈련과 테스트셋의 평균 값의 분포를 알아봅시다.']",train test seem well ballanc respect distribut numer variabl numer id 33 distribut mean std let check distribut mean valu per row train test set
498,"[""## <a id='34'>Features correlation</a>  컬럼간 상관관계\n"", '\n', 'We calculate now the correlations between the features in train set.  \n', 'The following table shows the first 10 the least correlated features.\n', '\n', '우리는 훈련세트에 컬럼간에 상관관계를 계산해보려고합니다. \n', '아래의 테이블은 처음 10개의 상관관계 특징들을 보여줍니다.\n', '\n', '\n', '> Reference from about guidelines about correlations <br />https://www.kaggle.com/willkoehrsen/a-complete-introduction-and-walkthrough\n', '\n', '#### The general guidelines for correlation values are below, but these will change depending on who you ask (source for these)\n', '\n', '* 00-.19 “very weak” <br />\n', '* 20-.39 “weak” <br />\n', '* 40-.59 “moderate” <br />\n', '* 60-.79 “strong” <br />\n', '* 80-1.0 “very strong” <br />\n', '\n', 'What these correlations show is that there are some weak relationships that hopefully our model will be able to use to learn a mapping from the features to the Target.\n', '\n', 'In that Kernel, he droped one of the columns what have high corrleation between them above 0.95\n', ""So, I'd like to drop them also here.\n"", ""But we don't have any columns what I told above. So I don't delete anything about 200 coulmns""]",id 34 featur correl calcul correl featur train set follow tabl show first 10 least correl featur 10 refer guidelin correl br http www kaggl com willkoehrsen complet introduct walkthrough gener guidelin correl valu chang depend ask sourc 00 19 weak br 20 39 weak br 40 59 moder br 60 79 strong br 80 1 0 strong br correl show weak relationship hope model abl use learn map featur target kernel drope one column high corrleat 0 95 like drop also column told delet anyth 200 coulmn
499,"['The correlation between the features is very small. \n', '\n', ""## <a id='35'>Duplicate values</a>  중복값 처리\n"", '\n', ""Let's now check how many duplicate values exists per columns.\n"", '\n', '컬럼당 얼마나 중복된 값들이 있는지 확인 해보자']",correl featur small id 35 duplic valu let check mani duplic valu exist per column
500,"['Same columns in train and test set have the same or very close number of duplicates of same or very close values. This is an interesting pattern that we might be able to use in the future.\n', '\n', '훈련세트와 테스트세트에서 같은 컬럼들이 같거나 가까운 양의 중복값을 가지며 이 중복값의 값 또한 같거나 비슷했다. 이는 나중에 사용하기에도 흥미로운 패턴이다.']",column train test set close number duplic close valu interest pattern might abl use futur
501,"[""# <a id='4'>Feature engineering</a>  \n"", '\n', 'This section is under construction.  \n', '\n', ""Let's calculate for starting few aggregated values for the existing features.""]",id 4 featur engin section construct let calcul start aggreg valu exist featur
502,"[""sum has perfect correaltion wth mean. So, I'd like to delete sum instead of mean.""]",sum perfect correalt wth mean like delet sum instead mean
503,"['# Feature Selection\n', '\n', ""**In here, I'd like to select features via SFM and REFCV but I couldn't. Because this data set is so huge as you guys know!! So this I'll try later...""]",featur select like select featur via sfm refcv data set huge guy know tri later
504,['## SFM'],sfm
505,['## REFCV'],refcv
506,"[""# <a id='5'>Model</a>  \n"", '\n', 'From the train columns list, we drop the ID and target to form the features list.']",id 5 model train column list drop id target form featur list
507,"['## INFOMATION ABOUT PARAMS\n', '\n', ""### The params what used at Gabriel's code\n"", '\n', '    params = {\n', ""        'num_leaves': 6,\n"", ""        'max_bin': 63,\n"", ""        'min_data_in_leaf': 45,\n"", ""        'learning_rate': 0.01,\n"", ""        'min_sum_hessian_in_leaf': 0.000446,\n"", ""        'bagging_fraction': 0.55, \n"", ""        'bagging_freq': 5, \n"", ""        'max_depth': 14,\n"", ""        'save_binary': True,\n"", ""        'seed': 31452,\n"", ""        'feature_fraction_seed': 31415,\n"", ""        'feature_fraction': 0.51,\n"", ""        'bagging_seed': 31415,\n"", ""        'drop_seed': 31415,\n"", ""        'data_random_seed': 31415,\n"", ""        'objective': 'binary',\n"", ""        'boosting_type': 'gbdt',\n"", ""        'verbose': 1,\n"", ""        'metric': 'auc',\n"", ""        'is_unbalance': True,\n"", ""        'boost_from_average': False,\n"", '    }\n', '\n', '### The params when I make lightgbm.LGBMClassifier.get_params()\n', '\n', '    params = {   \n', ""      'boosting_type': 'gbdt', \n"", ""      'class_weight': None,\n"", ""      'colsample_bytree': 1.0,\n"", ""      'importance_type': 'split',\n"", ""      'learning_rate': 0.1,\n"", ""      'max_depth': -1,\n"", ""      'min_child_samples': 20,\n"", ""      'min_child_weight': 0.001,\n"", ""      'min_split_gain': 0.0,\n"", ""      'n_estimators': 100,\n"", ""      'n_jobs': -1,\n"", ""      'num_leaves': 31,\n"", ""      'objective': None,\n"", ""     'random_state': None,\n"", ""     'reg_alpha': 0.0,\n"", ""     'reg_lambda': 0.0,\n"", ""     'silent': True,\n"", ""     'subsample': 1.0,\n"", ""     'subsample_for_bin': 200000,\n"", ""     'subsample_freq': 0\n"", '        }\n', '\n', '> Reference from <br />\n', 'https://lightgbm.readthedocs.io/en/latest/Python-API.html <br />\n', 'https://lightgbm.readthedocs.io/en/latest/Parameters.html\n', '\n', ""* 'boosting_type': 'gbdt' <br />\n"", ' **alias with boosting** (Default:gbdt, options gbdt,gbrt,rf,random_forest,dart,goss)\n', ' \n', ""* 'class_weight': None <br />\n"", '(default=None) – Weights associated with classes in the form {class_label: weight}. **Use this parameter only for multi-class classification task**; **for binary classification task** you may use **is_unbalance or scale_pos_weight parameters.** The ‘balanced’ mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as n_samples / (n_classes * np.bincount(y)). If None, all classes are supposed to have weight one. Note, that these weights will be multiplied with sample_weight (passed through the fit method) if sample_weight is specified.  \n', ' \n', ""* 'colsample_bytree': 1.0 <br />\n"", '(Default=1.0 / constraints 0.0 < value <= 1.0) **alias: feature_fraction**[simliar with max_features of GBM]  <br />    \n', ' lightgbm will randomly select iteration if feature_fraction smaller than 1.0.\n', ' e.g if I set it 0.8 lightgbm will select 80% of features before training each tree.\n', ' can be used to speed up training.\n', ' can be used to deal with over-fitting.\n', '\n', '* \'importance_type\': \'split\' (default=""split"") <br />\n', 'How the importance is calculated. If “split”, result contains numbers of times the feature is used in a model. If “gain”, result contains total gains of splits which use the feature. <br />\n', '=> sort of method how to gain feature_importance\n', '\n', ""* 'learning_rate': 0.1 <br />\n"", ' (Default=1.0 / constraints learning_rate > 0.0) **alias with with shrinkage_rate,eta**\n', ' \n', ""* 'max_depth': -1 <br />\n"", '  limit the max depth for tree model. This is used to deal with overfitting when data is small\n', ' \n', ""* 'min_child_samples': 20 <br />\n"", '  (Default = 20 / constraints min_data_in_leaf >= 0) **alias with min_data_in_leaf,min_data-per_leaf,min_data,min_child_samples** \n', '\n', ""* 'min_child_weight': 0.001 <br />\n"", ' (Default = 1e-3 // Default min_sum_hessian_in_leaf >= 0.0) <br />\n', ' **alias with min_sum_hessian_in_leaf,min_sum_hessian_per_leaf,min_sum_hessian,min_hessian,min_child_weight **<br />\n', '\n', ""* 'min_split_gain': 0.0  **only in lightgbm.LGBMClassifier()[not in lightgbm.train()]<br />\n"", ' (Default =0.0 / constraints: min_gain_to_splot >= 0.0) **alias with min_gain_to_split,min_split_gain** <br />\n', ' the minimal gain to perform split  <br />\n', ' \n', ""* 'n_estimators': 100 <br />\n"", ' (Default = 100 / constraints n_estimator >= 0) **alias with num_iteration,n_iter,num_tree,num_trees,num_round,num_rounds,num_boost_round,n_estimators** <br />\n', ' number of boosting iterations\n', '\n', ""* 'n_jobs': -1 <br />\n"", '(Default = 0) **alias with num_thread,nthread,nthreads,n_jobs**\n', '\n', ""* 'num_leaves': 31 <br />\n"", ' (Default = 31 / constraints: num_leaves > 1) **aliases: num_leaf, max_leaves, max_leaf** <br />  \n', ' max number of leaves in one tree\n', '    \n', ""* 'objective': None <br />\n"", '(Default = regression / options: regression, regression_l1, huber, fair, poisson, quantile, mape, gammma, tweedie, binary, multiclass, multiclassova, xentropy, xentlambda, lambdarank)\n', ' **aliases: objective_type, app, application**\n', '    \n', ""* 'random_state': None <br />\n"", '(Default = None) **aliases: random_seed, random_state** <br />\n', 'this seed is used to generate other seeds, e.g. data_random_seed, feature_fraction_seed, etc. <br />\n', 'by default, this seed is unused in favor of default values of other seeds <br />\n', 'this seed has lower priority in comparison with other seeds, which means that it will be overridden, if you set other seeds explicitly <br />\n', '\n', ""* 'reg_alpha': 0.0 <br />\n"", '(Default = 0.0 / constraints: lambda_l1 >= 0.0) **aliases: reg_alpha** <br /> \n', 'L1 regularization <br />\n', '\n', ""* 'reg_lambda': 0.0 <br />\n"", '(Default = 0.0 /  constraints: lambda_l2 >= 0.0) **aliases: reg_lambda, lambda** <br />\n', 'L2 regularization <br />\n', '\n', ""* 'silent': True **only in lightgbm.LGBMClassifier()(not in lightgbm.train())**<br />\n"", 'silent (bool, optional (default=False)) – Whether to print messages during construction\n', '\n', ""* 'subsample': 1.0 <br />\n"", '(Default = 1.0 / constraints: 0.0 < bagging_fraction <= 1.0 ) **aliases: sub_row, subsample, bagging** <br /> \n', 'like feature_fraction, but this will randomly select part of data without resampling <br /> \n', 'can be used to speed up training <br /> \n', 'can be used to deal with over-fitting <br /> \n', 'Note: to enable bagging, bagging_freq should be set to a non zero value as well <br /> \n', '\n', ""* 'subsample_for_bin': 200000 <br />\n"", '(Default = 200000 / constraints: bin_construct_sample_cnt > 0)  **aliases: subsample_for_bin** <br /> \n', 'number of data that sampled to construct histogram bins <br />\n', 'setting this to larger value will give better training result, but will increase data loading time <br />\n', 'set this to larger value if data is very sparse <br />\n', '\n', ""* 'subsample_freq': 0\n"", '(Default = 0) **aliases: subsample_freq, frequency for bagging** <br />\n', '0 means disable bagging; k means perform bagging at every k iteration <br />\n', 'Note: to enable bagging, bagging_fraction should be set to value smaller than 1.0 as wel <br />l\n', '\n', ""* 'reg_alpha': 0.0\n"", '(default = 0.0) **aliases: reg_alpha**<br /> \n', 'constraints: lambda_l1 >= 0.0 //  L1 regularization<br />\n', '\n', ""* 'reg_lambda': 0.0\n"", '(default = 0.0) **aliases: reg_lambda, lambda** <br />\n', 'constraints: lambda_l2 >= 0.0 // L2 regularization']",infom param param use gabriel code param num leav 6 max bin 63 min data leaf 45 learn rate 0 01 min sum hessian leaf 0 000446 bag fraction 0 55 bag freq 5 max depth 14 save binari true seed 31452 featur fraction seed 31415 featur fraction 0 51 bag seed 31415 drop seed 31415 data random seed 31415 object binari boost type gbdt verbos 1 metric auc unbal true boost averag fals param make lightgbm lgbmclassifi get param param boost type gbdt class weight none colsampl bytre 1 0 import type split learn rate 0 1 max depth 1 min child sampl 20 min child weight 0 001 min split gain 0 0 n estim 100 n job 1 num leav 31 object none random state none reg alpha 0 0 reg lambda 0 0 silent true subsampl 1 0 subsampl bin 200000 subsampl freq 0 refer br http lightgbm readthedoc io en latest python api html br http lightgbm readthedoc io en latest paramet html boost type gbdt br alia boost default gbdt option gbdt gbrt rf random forest dart go class weight none br default none weight associ class form class label weight use paramet multi class classif task binari classif task may use unbal scale po weight paramet balanc mode use valu automat adjust weight invers proport class frequenc input data n sampl n class np bincount none class suppos weight one note weight multipli sampl weight pas fit method sampl weight specifi colsampl bytre 1 0 br default 1 0 constraint 0 0 valu 1 0 alia featur fraction simliar max featur gbm br lightgbm randomli select iter featur fraction smaller 1 0 e g set 0 8 lightgbm select 80 featur train tree use speed train use deal fit import type split default split br import calcul split result contain number time featur use model gain result contain total gain split use featur br sort method gain featur import learn rate 0 1 br default 1 0 constraint learn rate 0 0 alia shrinkag rate eta max depth 1 br limit max depth tree model use deal overfit data small min child sampl 20 br default 20 constraint min data leaf 0 alia min data leaf min data per leaf min data min child sampl min child weight 0 001 br default 1e 3 default min sum hessian leaf 0 0 br alia min sum hessian leaf min sum hessian per leaf min sum hessian min hessian min child weight br min split gain 0 0 lightgbm lgbmclassifi lightgbm train br default 0 0 constraint min gain splot 0 0 alia min gain split min split gain br minim gain perform split br n estim 100 br default 100 constraint n estim 0 alia num iter n iter num tree num tree num round num round num boost round n estim br number boost iter n job 1 br default 0 alia num thread nthread nthread n job num leav 31 br default 31 constraint num leav 1 alias num leaf max leav max leaf br max number leav one tree object none br default regress option regress regress l1 huber fair poisson quantil mape gammma tweedi binari multiclass multiclassova xentropi xentlambda lambdarank alias object type app applic random state none br default none alias random seed random state br seed use gener seed e g data random seed featur fraction seed etc br default seed unus favor default valu seed br seed lower prioriti comparison seed mean overridden set seed explicitli br reg alpha 0 0 br default 0 0 constraint lambda l1 0 0 alias reg alpha br l1 regular br reg lambda 0 0 br default 0 0 constraint lambda l2 0 0 alias reg lambda lambda br l2 regular br silent true lightgbm lgbmclassifi lightgbm train br silent bool option default fals whether print messag construct subsampl 1 0 br default 1 0 constraint 0 0 bag fraction 1 0 alias sub row subsampl bag br like featur fraction randomli select part data without resampl br use speed train br use deal fit br note enabl bag bag freq set non zero valu well br subsampl bin 200000 br default 200000 constraint bin construct sampl cnt 0 alias subsampl bin br number data sampl construct histogram bin br set larger valu give better train result increas data load time br set larger valu data spar br subsampl freq 0 default 0 alias subsampl freq frequenc bag br 0 mean disabl bag k mean perform bag everi k iter br note enabl bag bag fraction set valu smaller 1 0 wel br l reg alpha 0 0 default 0 0 alias reg alpha br constraint lambda l1 0 0 l1 regular br reg lambda 0 0 default 0 0 alias reg lambda lambda br constraint lambda l2 0 0 l2 regular
508,"['## So We could get some results about comparing two API ""lightgbm.train()"" and ""lightgbm.LGBMClassifier""\n', '\n', '### common params btw two APIs\n', '\n', ""* boosting_type': 'gbdt' ==  'boosting_type': 'gbdt' \n"", ""* 'feature_fraction' == 'colsample_bytree'\n"", ""* 'is_unbalance': True == 'class_weight': None \n"", ""* 'learning_rate' == 'learning_rate' \n"", ""* 'max_depth' == 'max_depth'\n"", ""* 'min_data_in_leaf' == 'min_child_samples'\n"", ""* 'min_sum_hessian_in_leaf' == 'min_child_weight'    \n"", ""* num_round == 'n_estimators'\n"", ""* 'num_leaves' ==  'num_leaves'\n"", ""* 'objective' == 'objective'\n"", ""* 'seed' == 'random_state'\n"", ""* 'subsample' == 'bagging_fraction'\n"", ""* 'subsample_freq' == 'baggin_freq'\n"", ""* 'subsample_for_bin' == 'bin_construct_sample_cnt' [**Gabriel didn't tuning it**]\n"", '\n', '### only in lightgbm.LGBMClassifier()\n', '\n', ""* 'importance_type'\n"", ""* 'min_split_gain'\n"", ""* 'silent'\n"", ""* 'class_weight'\n"", ""* 'reg_alpha'\n"", ""* 'reg_lambda'\n"", ""**(But, I don't know when I should tune about 'reg_xx' If someone knows it plz comment at this kernel)**""]",could get result compar two api lightgbm train lightgbm lgbmclassifi common param btw two api boost type gbdt boost type gbdt featur fraction colsampl bytre unbal true class weight none learn rate learn rate max depth max depth min data leaf min child sampl min sum hessian leaf min child weight num round n estim num leav num leav object object seed random state subsampl bag fraction subsampl freq baggin freq subsampl bin bin construct sampl cnt gabriel tune lightgbm lgbmclassifi import type min split gain silent class weight reg alpha reg lambda know tune reg xx someon know plz comment kernel
509,"['## Hyperparameter Tunning using Hyperopt\n', '\n', '### The thing what I can do from below kernel is tunning parameters what we saw above through hyperopt!!\n', '> https://www.kaggle.com/willkoehrsen/a-complete-introduction-and-walkthrough\n', '\n', '#### In this phase we need to comply following 4 phases\n', '1. making objective function\n', '2. defining space for parameters\n', '3. choosing algorithm for hyperopt\n', '4. using all of them through fmin of hyperopt\n', '\n', ""### I'd like to complie all of precess using hyperopt but you guys know this process is pretty time-consuming!!!\n"", ""**So I'll post my hyperparameters via this process and finally I put in the gbm_model for making predictions!!**""]",hyperparamet tun use hyperopt thing kernel tun paramet saw hyperopt http www kaggl com willkoehrsen complet introduct walkthrough phase need compli follow 4 phase 1 make object function 2 defin space paramet 3 choos algorithm hyperopt 4 use fmin hyperopt like compli precess use hyperopt guy know process pretti time consum post hyperparamet via process final put gbm model make predict
510,['### Making user metric for objective function'],make user metric object function
511,"['### Objective Function\n', '\n', 'P.S) I do this process **briefly**, cuz this is **time-consumming process** as I mentioned before <br />\n', 'So, I recommend to set like this if you do yourself in own environment <br />\n', '\n', '* n_estimators => 15000\n', '* early_stopping_rounds => 250\n', '* verbose => 1000\n']",object function p process briefli cuz time consum process mention br recommend set like environ br n estim 15000 earli stop round 250 verbos 1000
512,['### Defining Space for Hyperparameters'],defin space hyperparamet
513,['### Make a sample via space what we defined'],make sampl via space defin
514,"['### Selecting algorithm\n', ""This algorithm is called by Tree Parzen Estimators. but I don't know how it works.. **So I'll keep trying to understanding!!! Or if you guys have a good site for TPE plz comment below!!**""]",select algorithm algorithm call tree parzen estim know work keep tri understand guy good site tpe plz comment
515,['### For recording our result of hyperopt'],record result hyperopt
516,['### Final phase of hyperopt'],final phase hyperopt
517,['### making the plot using hyperopt'],make plot use hyperopt
518,"[""## I could get the params below like that by changing params few times using colab, You guys shoud do that not getting someone's params!! \n"", ""If Kaggle's session was long, I'll use hyperparameters what I got from hyporpot above. But you know, it's not! So, I'll use my own parameters to be gotten through hyperopt in colab environment!!""]",could get param like chang param time use colab guy shoud get someon param kaggl session long use hyperparamet got hyporpot know use paramet gotten hyperopt colab environ
519,['## make a model using our own hyperparameters!!'],make model use hyperparamet
520,"['# Introduction\n', '**""[Distilling the Knowledge in a Neural Network](http://arxiv.org/abs/1503.02531)"" was introduced by Geoffrey Hinton, Oriol Vinyals, Jeff Dean in Mar 2015. In this kernel, I would like to share some experiments to distill the knowledge from a [LGBM teacher](https://www.kaggle.com/tanreinama/lightgbm-minimize-leaves-with-gaussiannb) (LB:0.899) to a neural network. The student network has not surpassed the teacher model yet (LB:0.894). But, I hope I can make it happen before this competition ends.**\n', '\n']",introduct distil knowledg neural network http arxiv org ab 1503 02531 introduc geoffrey hinton oriol vinyal jeff dean mar 2015 kernel would like share experi distil knowledg lgbm teacher http www kaggl com tanreinama lightgbm minim leav gaussiannb lb 0 899 neural network student network surpass teacher model yet lb 0 894 hope make happen competit end
521,['# Please upvote if you find this kernel interesting ^_^'],plea upvot find kernel interest
522,"['## **Load the dataset, and the prediction of 5-fold LGBM**\n']",load dataset predict 5 fold lgbm
523,"['## Adding some features, the credit belong to these kernels: https://www.kaggle.com/karangautam/keras-nn, https://www.kaggle.com/ymatioun/santander-linear-model-with-additional-features\n']",ad featur credit belong kernel http www kaggl com karangautam kera nn http www kaggl com ymatioun santand linear model addit featur
524,['## Normalize and split data\n'],normal split data
525,['## Define our student network\n'],defin student network
526,['## Some necessary functions\n'],necessari function
527,"['## Experiment 1\n', 'Firstly, we check the performance of simple feed forward neural network.']",experi 1 firstli check perform simpl feed forward neural network
528,"['## Knowledge distillation\n', 'The basic idea is that you feed both groundtruth and the prediction from the teacher model to the student network.\n', 'Soft targets (the prediction of the teacher model) contains more information than the hard labels (groundtruth) due to the fact that they encode similarity measures between the classes.']",knowledg distil basic idea feed groundtruth predict teacher model student network soft target predict teacher model contain inform hard label groundtruth due fact encod similar measur class
529,"['## Experment 2\n', ""We set the ratio between teacher's prediction and groundtruth is 1:9, and use the basic binary crossentropy loss.""]",exper 2 set ratio teacher predict groundtruth 1 9 use basic binari crossentropi loss
530,"['## Experment 3\n', ""We set the ratio between teacher's prediction and groundtruth is 1:9, and use the focal loss.""]",exper 3 set ratio teacher predict groundtruth 1 9 use focal loss
531,"['## Experment 4\n', 'Tuning hyper parameter ""Temperature"".']",exper 4 tune hyper paramet temperatur
532,['# Please upvote if you find this kernel interesting ^_^'],plea upvot find kernel interest
533,"['\n', 'Title: SANTANDER CUSTOMER TRANSACTION PREDICTION\n', '\n', 'Mikel Kengni / March 2019\n', '\n', 'Plan:\n', '\n', '**: )                                     First things First: snacks and chip checked, Coffee checked                                         : )**\n', '\n', '* **Introduction**\n', '* Competition overview\n', '* Kernel 1: Importance of Data Balancing\n', '\n', '* Outlines/Progression\n', '\n', '    1- Import the libraries\n', '\n', '    2- Run the data\n', '\n', '    3- Data  exploration and Feature Engineering\n', '    \n', '    4- How skewed is our dataset:\n', '    \n', '    5- Correlation between all features na dthe target features\n', '    \n', '    6- Modelling\n', '     * Part_1: Using a classifier with the **Class_weight = Balanced** parameter\n', '     * Part_2: Using the **SMOTE** oversampling technique\n', '   \n', '  7- Metric Traps\n', '  \n', '  8- Observation\n', '  \n', '  9- Conclusion\n', '  \n', '  10- Kernels and Materials used\n']",titl santand custom transact predict mikel kengni march 2019 plan first thing first snack chip check coffe check introduct competit overview kernel 1 import data balanc outlin progress 1 import librari 2 run data 3 data explor featur engin 4 skew dataset 5 correl featur na dthe target featur 6 model part 1 use classifi class weight balanc paramet part 2 use smote oversampl techniqu 7 metric trap 8 observ 9 conclus 10 kernel materi use
534,"['# Introduction:\n', '\n', '**Competition Overview **:\n', '\n', 'In this challenge, we help identify which customers will make a specific transaction in the future, irrespective of the amount of money transacted.\n', '\n', 'We are provided an anonymized dataset with each row containing 200 numerical values identified just with a number, the binary target column, and a string ID_code column. \n', '\n', 'The task is to predict the value of target column in the test set.']",introduct competit overview challeng help identifi custom make specif transact futur irrespect amount money transact provid anonym dataset row contain 200 numer valu identifi number binari target column string id code column task predict valu target column test set
535,"['**Kernel 1: Importance of Data Balancing**\n', '\n', 'This kernel is going to be about the importance of data balancing and the effects of unbalance datasets(uneven classes in this case just 2 classes) on our results. \n', '\n', '* We will be comparing 2 different ways of data balancing - \n', '\n', '1- Using the \'Balanced"" parameter in the Class_weight feature \n', '\n', '2- Using the SMOTE oversampling technique: \n', '\n', '* Under the SMOTE Method will determine the right and wrong ways to oversample using SMOTE.\n', '    \n', '    * We will do it in 2 ways. \n', '    \n', '        * We will apply SMOTE on the whole predictors features and outcome, then split them into train and validation set, then fit into a model.\n', '        \n', '        * We will also do it the other way.  Ie. We will split the datasets into train and validation sets, then we will apply  SMOTE on the X_train and y_train splits.\n', '            \n', '3- Finally, we will find out which of the 2 techniques above score better with this problem and why.']",kernel 1 import data balanc kernel go import data balanc effect unbal dataset uneven class case 2 class result compar 2 differ way data balanc 1 use balanc paramet class weight featur 2 use smote oversampl techniqu smote method determin right wrong way oversampl use smote 2 way appli smote whole predictor featur outcom split train valid set fit model also way ie split dataset train valid set appli smote x train train split 3 final find 2 techniqu score better problem
536,['# 1- Import the Libraries'],1 import librari
537,['# 2- Read Train and Test Datasets'],2 read train test dataset
538,['# 3- Data Exploration and Feature Engineering'],3 data explor featur engin
539,"['**OBSERVATION:**\n', 'Data contains:\n', '\n', '* ID_code (string)\n', '* Target\n', '* 200 numerical variables, var_0 to var_199\n', '* SHAPE = 200000 ROWS AND 202 COLUMNS\n', '\n', 'Test contains:\n', '\n', '* ID_code (string);\n', '* 200 numerical variables, var_0 to var_199\n', '* SHAPE = 200000 ROWS AND 201 COLUMNS']",observ data contain id code string target 200 numer variabl var 0 var 199 shape 200000 row 202 column test contain id code string 200 numer variabl var 0 var 199 shape 200000 row 201 column
540,['Lets take a look at the dictribution of the target variables in both the train and test sets.'],let take look dictribut target variabl train test set
541,"['Our Target set is very imbalanced. About 90 percent of our target column is 0 while the remianing 10 percent are 1s. This si called Class Imbalanced. It occurs each class does not make up an equal portion of your data-set and It is important to properly adjust your metrics and methods to adjust for your goals. If this is not done, you may end up optimizing for a meaningless metric and hence getting a flawed outcome.']",target set imbalanc 90 percent target column 0 remian 10 percent 1 si call class imbalanc occur class make equal portion data set import properli adjust metric method adjust goal done may end optim meaningless metric henc get flaw outcom
542,['# 4- How skewed is our a datasets?'],4 skew dataset
543,"['* Why do we need to check for skewness you ask?\n', '\n', 'It’s often desirable to transform skewed data and to convert it into values between 0 and 1 because usually, different features in a datasets have values in diffeernet range. In order to have a reliable predictive model, it is important to bring all these features in the same range.\n', '\n', '\n', ""Let's take a look at skewness in our dataset:""]",need check skew ask often desir transform skew data convert valu 0 1 usual differ featur dataset valu diffeernet rang order reliabl predict model import bring featur rang let take look skew dataset
544,"['# 5- Correlation with the target feature\n', 'https://towardsdatascience.com/data-correlation-can-make-or-break-your-machine-learning-project-82ee11039cc9\n', '\n', 'Data correlation is the way in which one set of data may correspond to another set. It is important to determine hoe correlated your features are, as this knowledge may be useful in choosing the right algorithm but also, If you try to train a model on a set of features with no or very little correlation, you will get inaccurate results. \n', '\n', 'Lets se how all the features correlate with the target feature in the train set.']",5 correl target featur http towardsdatasci com data correl make break machin learn project 82ee11039cc9 data correl way one set data may correspond anoth set import determin hoe correl featur knowledg may use choos right algorithm also tri train model set featur littl correl get inaccur result let se featur correl target featur train set
545,['WAOUWWW!!! Looks like there is not a lot of correlation between the feayures and the predcitor. I will use the random forest classifier later to get the most important features in the future if need arises. Okay!!! lets move on.'],waouwww look like lot correl feayur predcitor use random forest classifi later get import featur futur need aris okay let move
546,"['# 6- Preparing for Modelling\n', '   We are going to do the modelling in 2 part.\n', '   - Modelling part 1- Metric trap\n', '   - MOdelling part 2']",6 prepar model go model 2 part model part 1 metric trap model part 2
547,"[' **Modelling part 1:** \n', '   \n', '   - We split the datasets in train and validation sets\n', '   - We discuss the importance of picking the right metric and why accuracy_score is not the best metric to choose when we have a class imbalance.\n', '   - Then weuse a simple algorithm in this case a Random Forest Classifier, train our unbalanec dataset on it , calculate the score with the accuarcy_score and then with the roc_auc_score.\n', '   - In order to proof that the accuracy_score is not the right metric, we will do a small test, pick just on feature and train it, then we calculate the accuracy_score and the roc_auc_score on it.']",model part 1 split dataset train valid set discus import pick right metric accuraci score best metric choos class imbal weus simpl algorithm case random forest classifi train unbalanec dataset calcul score accuarci score roc auc score order proof accuraci score right metric small test pick featur train calcul accuraci score roc auc score
548,"['**Why do we need To balance out dataset:**\n', '\n', '* As we can see from above,the data set is severely imbalanced (90 : 10).\n', '* The main motivation behind the need to preprocess imbalanced data before we feed them into a classifier is that typically classifiers are more sensitive to detecting the majority class and less sensitive to the minority class.\n', '* Usually, data imbalance will lead to the classification output being biased, in many cases resulting in always predicting the majority class like we will see in  below.']",need balanc dataset see data set sever imbalanc 90 10 main motiv behind need preprocess imbalanc data feed classifi typic classifi sensit detect major class le sensit minor class usual data imbal lead classif output bias mani case result alway predict major class like see
549,"['# 8- Metric Trap:\n', '\n', 'One of the major issues begginers usually fall into when dealing with unbalanced datasets is the choice of their evalution metrics.  Using simpler metrics like accuracy_score my not always be the correct. \n', '\n', 'In a dataset with highly unbalanced classes, if the classifier always ""predicts"" the most common class without performing any analysis of the features, it will still have a high accuracy rate. ie, whatever the circumstance, the accuarcy_score will most likely always be the percentage of the majority classe. If you don\'t get it yet, Hnag on, it will be clearer with examples below..']",8 metric trap one major issu beggin usual fall deal unbalanc dataset choic evalut metric use simpler metric like accuraci score alway correct dataset highli unbalanc class classifi alway predict common class without perform analysi featur still high accuraci rate ie whatev circumst accuarci score like alway percentag major class get yet hnag clearer exampl
550,"['But for this little experiement, i will be using the random forest classifier.and for the metric evaluation i will be using the accuarcy_score and the roc_auc_score.\n']",littl experi use random forest classifi metric evalu use accuarci score roc auc score
551,"['The accuracy_score for this part is 0.8976 while the roc_auc_score is 0.7921. \n', '**Do you remeber the percentage of classe distribution in our dataset?** Here we go! look at the percentage dictribution for classe with 0. It is almost the same as the accuarcy_score.']",accuraci score part 0 8976 roc auc score 0 7921 remeb percentag class distribut dataset go look percentag dictribut class 0 almost accuarci score
552,"[""Lets do something else to confirm what we are already suspecting. Now let's run the same code, but using only one feature. Normally, the accuracy score should be very small given that we are only using one feature. \n""]",let someth el confirm alreadi suspect let run code use one featur normal accuraci score small given use one featur
553,"['As we can see, The accuracy_score whicch under normal circumstances should be really low, is atill stuck at 0.8976% which is not correct. This goes to show how important the choide of evalution metric especially when dealing with unbalanced datasets. The other metric we used for this(roc_auc_score) is behaving like it should, ie for a single feature, its score actually dropped to 0.51. ']",see accuraci score whicch normal circumst realli low atil stuck 0 8976 correct goe show import choid evalut metric especi deal unbalanc dataset metric use roc auc score behav like ie singl featur score actual drop 0 51
554,"['**Modelling part 2:**\n', ' \n', '   - We train the data the same model used in modelling part1 but htis time around we balance the classes before feeding it into an algorithm for training.\n', '   - then we calculate the score using the same algorithm we used above.\n', '   - Then map out the importance of always using unbiased datasets(datasets with one classes a lot more present that the other.\n', '   ']",model part 2 train data model use model part1 hti time around balanc class feed algorithm train calcul score use algorithm use map import alway use unbias dataset dataset one class lot present
555,"['**Modelling part 2: Balancing the classes using the class_weights parameters**\n', '\n', ""The paramenter **'class_weight = balanced**' will give equal weights to both classes  irrespective of their reperesentation in the training datase. ""]",model part 2 balanc class use class weight paramet parament class weight balanc give equal weight class irrespect reperesent train data
556,"['After balancing the classes by balancing their weights, we can see that the accuracy has dropped a little.']",balanc class balanc weight see accuraci drop littl
557,"['**Imbalanced data put accuracy out of business** as we proved above. It is usually not enought to rely on hight accuracy_score to evalute your model because the score may just be illusionary and a simple reflexion of the majority class. Using other evalution metric like the roc_auc_score, f1_score, classification report etc could give us a better evalution of the performance our our model w.r.t the dataset. But it is always a good idea and safer to work with balanced datasets and balancing a dataset can be as easy as just adjusting the class_weight parameters.\n', '\n', ""For algorithms with the Class_weight parameter, it sometimes suffices to set set **class_weight = 'Balanced'** like in this case the random_forest classifier.\n"", '\n', 'With some other algorithms, we may need to set the class weight parameter manually.  We set the class_weight such as to penalize mistakes on the minority class by an amount proportional to how under-represented it is. For example \n', '\n', '> class_weight = ({0 : ""0.25"", 1:  ""0.85""}).\n', '\n', 'Another alternative to using the class_weight parameter is to creat synthetic observations of the minority class using the **SMOTE = Synthetic Minority Oversampling Technique** from the sklearn.imblearn library.']",imbalanc data put accuraci busi prove usual enought reli hight accuraci score evalut model score may illusionari simpl reflexion major class use evalut metric like roc auc score f1 score classif report etc could give u better evalut perform model w r dataset alway good idea safer work balanc dataset balanc dataset easi adjust class weight paramet algorithm class weight paramet sometim suffic set set class weight balanc like case random forest classifi algorithm may need set class weight paramet manual set class weight penal mistak minor class amount proport repres exampl class weight 0 0 25 1 0 85 anoth altern use class weight paramet creat synthet observ minor class use smote synthet minor oversampl techniqu sklearn imblearn librari
558,"[""# - Data Balancing using 'SMOTE'""]",data balanc use smote
559,['**Balancing classes using SMOTE before spltting dataset into train and validaton sets'],balanc class use smote spltting dataset train validaton set
560,"['**SMOTE Algorithm (Synthetic Minority Oversampling Technique)**\n', '\n', 'We will be using the SMOTE algorithm (Synthetic Minority Oversampling Technique) to over-sample our dataset. It is a powerful sampling method that goes beyonds simply increasing or decreasing the number of datas in a dataset. How it works is by.\n', '\n', '1- Finding the k-nearest-neighbors for minority class observations (finding similar observations)\n', '\n', '2- Randomly choosing one of the k-nearest-neighbors and using it to create a similar, but randomly tweaked, new observation.']",smote algorithm synthet minor oversampl techniqu use smote algorithm synthet minor oversampl techniqu sampl dataset power sampl method goe beyond simpli increas decreas number data dataset work 1 find k nearest neighbor minor class observ find similar observ 2 randomli choos one k nearest neighbor use creat similar randomli tweak new observ
561,"[""# 'SMOTE' on X_train and y_train only\n"", '\n', '**Let do a split before we apply smote on x_train amd y_train **\n', '\n']",smote x train train let split appli smote x train amd train
562,['# Submission Dataframe'],submiss datafram
563,"['We porved earlier that a very high accuracy can sometimes not be a relexion of how the model actually performs. So i will not consider the results from the accuracy in this section. The main purpose of using accuracy_score was to show its flaws especially when dealing with unbalanced datasets. \n', '\n', 'Lets focus on the roc_auc_score for both scenarios( applying SMOTE before splitting and applying SMOTE after splitting.).']",porv earlier high accuraci sometim relexion model actual perform consid result accuraci section main purpos use accuraci score show flaw especi deal unbalanc dataset let focu roc auc score scenario appli smote split appli smote split
564,"['# Observation:\n', '**Scenario 1: Resamplling then splitting into train - validation sets**\n', '\n', 'In this scenario, we oversampled the whole datasets then we split it into train and validation set. From our roc_auc_score evaluation, we can see that we have a 91.0% score on the validation set but a 49.9% score on the test data. That is a huge gap between the scsores and this is so because some information **""bleed""** from the validation set into the training set ofthe model.  \n', '\n', 'By oversampling before splitting the dataset into train and validation sets, we ended up with some of the information from  the validation set being used to create some of the synthetic observations in the training set. As a result, the model has already ""seen"" some of the datait is predicting in the and as such, is able to perfectly predict these data during validation hence increasing the roc_auc_score of the validation set. Hecnce the big gap between the score for the validation set an that of the test set.( 91.0% versus 49.9%)\n']",observ scenario 1 resampl split train valid set scenario oversampl whole dataset split train valid set roc auc score evalu see 91 0 score valid set 49 9 score test data huge gap scsore inform bleed valid set train set ofth model oversampl split dataset train valid set end inform valid set use creat synthet observ train set result model alreadi seen datait predict abl perfectli predict data valid henc increas roc auc score valid set hecnc big gap score valid set test set 91 0 versu 49 9
565,"['**Scenario 2: Splitting the datasets into train and validation sets the resampling the X_train and y_train data**\n', '\n', 'In this scenario, we split the dataset into train and validation sets and then resampled the trained data. Here, the validation set is untouched so the result from this scenario is more generalizable. \n', '\n', 'As we see from the roc_auc_scores, the score for the validation set( 66.9 %) is very close to the score from the test set(50.0%). \n', '\n', 'Scenario 2  is the **right way to oversample data.** while scenario 1 is the **wrong way to oversample data.**\n', '\n']",scenario 2 split dataset train valid set resampl x train train data scenario split dataset train valid set resampl train data valid set untouch result scenario generaliz see roc auc score score valid set 66 9 close score test set 50 0 scenario 2 right way oversampl data scenario 1 wrong way oversampl data
566,"['# Conclusion: \n', 'It is always advisable to **split your datatset into train and validation before oversampling**(Scenario 2). And only **apply your oversampling method on the training sets**. The validation set should be pristine.']",conclus alway advis split datatset train valid oversampl scenario 2 appli oversampl method train set valid set pristin
567,['Thanks for reading to the end.'],thank read end
568,"['\n', 'If you found this kernel helpful, i would really appreciate an upvote. If you did not, please comment below with your suggestion or recommendations and lets make it better together. \n', '\n']",found kernel help would realli appreci upvot plea comment suggest recommend let make better togeth
569,"['References used for Data Balancing/ Resampling:\n', '\n', '- https://beckernick.github.io/oversampling-modeling/\n', '- https://elitedatascience.com/imbalanced-classes\n', '- https://towardsdatascience.com/handling-imbalanced-datasets-in-deep-learning-f48407a0e758a\n', '- https://towardsdatascience.com/deep-learning-unbalanced-training-data-solve-it-like-this-6c528e9efea6\n', '- https://towardsdatascience.com/dealing-with-imbalanced-classes-in-machine-learning-d43d6fa19d2\n', '- https://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/']",refer use data balanc resampl http beckernick github io oversampl model http elitedatasci com imbalanc class http towardsdatasci com handl imbalanc dataset deep learn f48407a0e758a http towardsdatasci com deep learn unbalanc train data solv like 6c528e9efea6 http towardsdatasci com deal imbalanc class machin learn d43d6fa19d2 http machinelearningmasteri com tactic combat imbalanc class machin learn dataset
570,"[' #  <div style=""text-align: center"">  Santander ML Explainability  </div> \n', '###  <div style=""text-align: center"">CLEAR DATA. MADE MODEL. </div> \n', ""<img src='https://galeria.bankier.pl/p/b/5/215103d7ace468-645-387-261-168-1786-1072.jpg' width=600 height=600>\n"", '<div style=""text-align:center""> last update: <b> 10/03/2019</b></div>\n', '\n', '\n', '\n', 'You can Fork code  and  Follow me on:\n', '\n', '> ###### [ GitHub](https://github.com/mjbahmani/10-steps-to-become-a-data-scientist)\n', '> ###### [Kaggle](https://www.kaggle.com/mjbahmani/)\n', '-------------------------------------------------------------------------------------------------------------\n', "" <b>I hope you find this kernel helpful and some <font color='red'>UPVOTES</font> would be very much appreciated.</b>\n"", '    \n', ' -----------']",div style text align center santand ml explain div div style text align center clear data made model div img src http galeria bankier pl p b 5 215103d7ace468 645 387 261 168 1786 1072 jpg width 600 height 600 div style text align center last updat b 10 03 2019 b div fork code follow github http github com mjbahmani 10 step becom data scientist kaggl http www kaggl com mjbahmani b hope find kernel help font color red upvot font would much appreci b
571,"[' <a id=""top""></a> <br>\n', '## Notebook  Content\n', '1. [Introduction](#1)\n', '1. [Load packages](#2)\n', '    1. [import](21)\n', '    1. [Setup](22)\n', '    1. [Version](23)\n', '1. [Problem Definition](#3)\n', '    1. [Problem Feature](#31)\n', '    1. [Aim](#32)\n', '    1. [Variables](#33)\n', '    1. [Evaluation](#34)\n', '1. [Exploratory Data Analysis(EDA)](#4)\n', '    1. [Data Collection](#41)\n', '    1. [Visualization](#42)\n', '    1. [Data Preprocessing](#43)\n', '1. [Machine Learning Explainability for Santander](#5)\n', '    1. [Permutation Importance](#51)\n', '    1. [How to calculate and show importances?](#52)\n', '    1. [What can be inferred from the above?](#53)\n', '    1. [Partial Dependence Plots](#54)\n', '1. [Model Development](#6)\n', '    1. [lightgbm](#61)\n', '    1. [RandomForestClassifier](#62)\n', '    1. [DecisionTreeClassifier](#63)\n', '    1. [CatBoostClassifier](#64)\n', '    1. [Funny Combine](#65)\n', '1. [References](#7)']",id top br notebook content 1 introduct 1 1 load packag 2 1 import 21 1 setup 22 1 version 23 1 problem definit 3 1 problem featur 31 1 aim 32 1 variabl 33 1 evalu 34 1 exploratori data analysi eda 4 1 data collect 41 1 visual 42 1 data preprocess 43 1 machin learn explain santand 5 1 permut import 51 1 calcul show import 52 1 infer 53 1 partial depend plot 54 1 model develop 6 1 lightgbm 61 1 randomforestclassifi 62 1 decisiontreeclassifi 63 1 catboostclassifi 64 1 funni combin 65 1 refer 7
572,"[' <a id=""1""></a> <br>\n', '## 1- Introduction\n', 'At [Santander](https://www.santanderbank.com) their mission is to help people and businesses prosper. they are always looking for ways to help our customers understand their financial health and identify which products and services might help them achieve their monetary goals.\n', ""<img src='https://www.smava.de/kredit/wp-content/uploads/2015/12/santander-bank.png' width=400 height=400>\n"", '\n', 'In this kernel we are going to create a **Machine Learning Explainability** for **Santander** based this perfect [course](https://www.kaggle.com/learn/machine-learning-explainability) in kaggle.\n', '><font color=""red""><b>Note: </b></font>\n', 'how to extract **insights** from models?']",id 1 br 1 introduct santand http www santanderbank com mission help peopl busi prosper alway look way help custom understand financi health identifi product servic might help achiev monetari goal img src http www smava de kredit wp content upload 2015 12 santand bank png width 400 height 400 kernel go creat machin learn explain santand base perfect cours http www kaggl com learn machin learn explain kaggl font color red b note b font extract insight model
573,"['<a id=""2""></a> <br>\n', '## 2- A Data Science Workflow for Santander \n', 'Of course, the same solution can not be provided for all problems, so the best way is to create a **general framework** and adapt it to new problem.\n', '\n', '**You can see my workflow in the below image** :\n', '\n', ' <img src=""http://s8.picofile.com/file/8342707700/workflow2.png""  />\n', '\n', '**You should feel free\tto\tadjust \tthis\tchecklist \tto\tyour needs**\n', '###### [Go to top](#top)']",id 2 br 2 data scienc workflow santand cours solut provid problem best way creat gener framework adapt new problem see workflow imag img src http s8 picofil com file 8342707700 workflow2 png feel free tto tadjust tthi tchecklist tto tyour need go top top
574,"[' <a id=""2""></a> <br>\n', ' ## 2- Load packages\n', '  <a id=""21""></a> <br>\n', '## 2-1 Import']",id 2 br 2 load packag id 21 br 2 1 import
575,"[' <a id=""22""></a> <br>\n', '##  2-2 Setup']",id 22 br 2 2 setup
576,"[' <a id=""23""></a> <br>\n', '## 2-3 Version\n']",id 23 br 2 3 version
577,"['<a id=""3""></a> \n', '<br>\n', '## 3- Problem Definition\n', 'In this **challenge**, we should help this **bank**  identify which **customers** will make a **specific transaction** in the future, irrespective of the amount of money transacted. The data provided for this competition has the same structure as the real data we have available to solve this **problem**.\n']",id 3 br 3 problem definit challeng help bank identifi custom make specif transact futur irrespect amount money transact data provid competit structur real data avail solv problem
578,"['<a id=""31""></a> \n', '### 3-1 Problem Feature\n', '\n', '1. train.csv - the training set.\n', '1. test.csv - the test set. The test set contains some rows which are not included in scoring.\n', '1. sample_submission.csv - a sample submission file in the correct format.\n']",id 31 3 1 problem featur 1 train csv train set 1 test csv test set test set contain row includ score 1 sampl submiss csv sampl submiss file correct format
579,"['<a id=""32""></a> \n', '### 3-2 Aim\n', 'In this competition, The task is to predict the value of **target** column in the test set.']",id 32 3 2 aim competit task predict valu target column test set
580,"['<a id=""33""></a> \n', '### 3-3 Variables\n', '\n', 'We are provided with an **anonymized dataset containing numeric feature variables**, the binary **target** column, and a string **ID_code** column.\n', '\n', 'The task is to predict the value of **target column** in the test set.']",id 33 3 3 variabl provid anonym dataset contain numer featur variabl binari target column string id code column task predict valu target column test set
581,"['<a id=""34""></a> \n', '## 3-4 evaluation\n', '**Submissions** are evaluated on area under the [ROC curve](http://en.wikipedia.org/wiki/Receiver_operating_characteristic) between the predicted probability and the observed target.\n', ""<img src='https://upload.wikimedia.org/wikipedia/commons/6/6b/Roccurves.png' width=300 height=300>""]",id 34 3 4 evalu submiss evalu area roc curv http en wikipedia org wiki receiv oper characterist predict probabl observ target img src http upload wikimedia org wikipedia common 6 6b roccurv png width 300 height 300
582,"['<a id=""4""></a> \n', '## 4- Exploratory Data Analysis(EDA)\n', "" In this section, we'll analysis how to use graphical and numerical techniques to begin uncovering the structure of your data. \n"", '*  Data Collection\n', '*  Visualization\n', '*  Data Preprocessing\n', '*  Data Cleaning\n', '<img src=""http://s9.picofile.com/file/8338476134/EDA.png"" width=400 height=400>']",id 4 4 exploratori data analysi eda section analysi use graphic numer techniqu begin uncov structur data data collect visual data preprocess data clean img src http s9 picofil com file 8338476134 eda png width 400 height 400
583,"[' <a id=""41""></a> <br>\n', '## 4-1 Data Collection']",id 41 br 4 1 data collect
584,"['# Reducing  memory size by ~50%\n', ""Because we make a lot of calculations in this kernel, we'd better reduce the size of the data.\n"", '1. 300 MB before Reducing\n', '1. 150 MB after Reducing']",reduc memori size 50 make lot calcul kernel better reduc size data 1 300 mb reduc 1 150 mb reduc
585,['Reducing for train data set'],reduc train data set
586,['Reducing for test data set'],reduc test data set
587,"[' <a id=""41""></a> <br>\n', '##   4-1-1Data set fields']",id 41 br 4 1 1data set field
588,"[' <a id=""422""></a> <br>\n', '## 4-2-2 numerical values Describe']",id 422 br 4 2 2 numer valu describ
589,"[' <a id=""42""></a> <br>\n', '## 4-2 Visualization']",id 42 br 4 2 visual
590,"['<a id=""421""></a> \n', '## 4-2-1 hist']",id 421 4 2 1 hist
591,"[' <a id=""422""></a> <br>\n', '## 4-2-2 Mean Frequency']",id 422 br 4 2 2 mean frequenc
592,"['<a id=""423""></a> \n', '## 4-2-3 countplot']",id 423 4 2 3 countplot
593,"['<a id=""424""></a> \n', '## 4-2-4 hist\n', 'If you check histogram for all feature, you will find that most of them are so similar']",id 424 4 2 4 hist check histogram featur find similar
594,"['<a id=""426""></a> \n', '## 4-2-6 distplot\n', ' The target in data set is **imbalance**']",id 426 4 2 6 distplot target data set imbal
595,"['<a id=""427""></a> \n', '## 4-2-7 violinplot']",id 427 4 2 7 violinplot
597,"[' <a id=""43""></a> <br>\n', '## 4-3 Data Preprocessing\n', 'Before we start this section let me intrduce you, some other compitation that they were similar to this:\n', '\n', '1. https://www.kaggle.com/artgor/how-to-not-overfit\n', '1. https://www.kaggle.com/c/home-credit-default-risk\n', '1. https://www.kaggle.com/c/porto-seguro-safe-driver-prediction']",id 43 br 4 3 data preprocess start section let intrduc compit similar 1 http www kaggl com artgor overfit 1 http www kaggl com c home credit default risk 1 http www kaggl com c porto seguro safe driver predict
598,"[' <a id=""431""></a> <br>\n', '## 4-3-1 Check missing data for test & train']",id 431 br 4 3 1 check miss data test train
599,"[' <a id=""432""></a> <br>\n', '## 4-3-2 Binary Classification']",id 432 br 4 3 2 binari classif
600,"[' <a id=""433""></a> <br>\n', '## 4-3-3 Is data set imbalance?']",id 433 br 4 3 3 data set imbal
601,"['A large part of the data is unbalanced, but **how can we  solve it?**']",larg part data unbalanc solv
602,"['1. **Imbalanced dataset** is relevant primarily in the context of supervised machine learning involving two or more classes. \n', '\n', '1. **Imbalance** means that the number of data points available for different the classes is different\n', '\n', ""<img src='https://www.datascience.com/hs-fs/hubfs/imbdata.png?t=1542328336307&width=487&name=imbdata.png'>\n"", '[Image source](http://api.ning.com/files/vvHEZw33BGqEUW8aBYm4epYJWOfSeUBPVQAsgz7aWaNe0pmDBsjgggBxsyq*8VU1FdBshuTDdL2-bp2ALs0E-0kpCV5kVdwu/imbdata.png)']",1 imbalanc dataset relev primarili context supervis machin learn involv two class 1 imbal mean number data point avail differ class differ img src http www datasci com h f hubf imbdata png 1542328336307 width 487 name imbdata png imag sourc http api ning com file vvhezw33bgqeuw8abym4epyjwofseubpvqasgz7awane0pmdbsjgggbxsyq 8vu1fdbshutddl2 bp2als0 0kpcv5kvdwu imbdata png
603,['## 4-3-4 skewness and kurtosis'],4 3 4 skew kurtosi
604,"[' <a id=""5""></a> <br>\n', '# 5- Machine Learning Explainability for Santander\n', 'In this section, I want to try extract **insights** from models with the help of this excellent [**Course**](https://www.kaggle.com/learn/machine-learning-explainability) in Kaggle.\n', 'The Goal behind of ML Explainability for Santander is:\n', '1. All features are senseless named.(var_1, var2,...) but certainly the importance of each one is different!\n', '1. Extract insights from models.\n', '1. Find the most inmortant feature in models.\n', ""1. Affect of each feature on the model's predictions.\n"", ""<img src='http://s8.picofile.com/file/8353215168/ML_Explain.png'>\n"", '\n', 'As you can see from the above, we will refer to three important and practical concepts in this section and try to explain each of them in detail.']",id 5 br 5 machin learn explain santand section want tri extract insight model help excel cours http www kaggl com learn machin learn explain kaggl goal behind ml explain santand 1 featur senseless name var 1 var2 certainli import one differ 1 extract insight model 1 find inmort featur model 1 affect featur model predict img src http s8 picofil com file 8353215168 ml explain png see refer three import practic concept section tri explain detail
605,"[' <a id=""51""></a> <br>\n', '## 5-1 Permutation Importance\n', ' In this section we will answer following question:\n', ' 1. What features have the biggest impact on predictions?\n', ' 1. how to extract insights from models?']",id 51 br 5 1 permut import section answer follow question 1 featur biggest impact predict 1 extract insight model
606,['### Prepare our data for our model'],prepar data model
607,['### Create  a sample model to calculate which feature are more important.'],creat sampl model calcul featur import
608,"[' <a id=""52""></a> <br>\n', '## 5-2 How to calculate and show importances?']",id 52 br 5 2 calcul show import
609,['### Here is how to calculate and show importances with the [eli5](https://eli5.readthedocs.io/en/latest/) library:'],calcul show import eli5 http eli5 readthedoc io en latest librari
610,"['<a id=""53""></a> <br>\n', '## 5-3 What can be inferred from the above?\n', '1. As you move down the top of the graph, the importance of the feature decreases.\n', '1. The features that are shown in green indicate that they have a positive impact on our prediction\n', '1. The features that are shown in white indicate that they have no effect on our prediction\n', '1. The features shown in red indicate that they have a negative impact on our prediction\n', '1.  The most important feature was **Var_110**.']",id 53 br 5 3 infer 1 move top graph import featur decreas 1 featur shown green indic posit impact predict 1 featur shown white indic effect predict 1 featur shown red indic neg impact predict 1 import featur var 110
611,"['<a id=""54""></a> <br>\n', '## 5-4 Partial Dependence Plots\n', 'While **feature importance** shows what **variables** most affect predictions, **partial dependence** plots show how a feature affects predictions.[6][7]\n', 'and partial dependence plots are calculated after a model has been fit. [partial-plots](https://www.kaggle.com/dansbecker/partial-plots)']",id 54 br 5 4 partial depend plot featur import show variabl affect predict partial depend plot show featur affect predict 6 7 partial depend plot calcul model fit partial plot http www kaggl com dansbeck partial plot
612,"['For the sake of explanation, I use a Decision Tree which you can see below.']",sake explan use decis tree see
613,"['As guidance to read the tree:\n', '\n', '1. Leaves with children show their splitting criterion on the top\n', '1. The pair of values at the bottom show the count of True values and False values for the target respectively, of data points in that node of the tree.\n', '><font color=""red""><b>Note: </b></font>\n', 'Yes **Var_81** are more effective on our model.']",guidanc read tree 1 leav child show split criterion top 1 pair valu bottom show count true valu fals valu target respect data point node tree font color red b note b font ye var 81 effect model
614,"['<a id=""55""></a> <br>\n', '## 5-5  Partial Dependence Plot\n', 'In this section, we see the impact of the main variables discovered in the previous sections by using the [pdpbox](https://pdpbox.readthedocs.io/en/latest/).']",id 55 br 5 5 partial depend plot section see impact main variabl discov previou section use pdpbox http pdpbox readthedoc io en latest
615,"['<a id=""56""></a> <br>\n', '## 5-6 Chart analysis\n', '1. The y axis is interpreted as change in the prediction from what it would be predicted at the baseline or leftmost value.\n', '1. A blue shaded area indicates level of confidence']",id 56 br 5 6 chart analysi 1 axi interpret chang predict would predict baselin leftmost valu 1 blue shade area indic level confid
616,"['<a id=""57""></a> <br>\n', '## 5-7 SHAP Values\n', '**SHAP** (SHapley Additive exPlanations) is a unified approach to explain the output of **any machine learning model**. SHAP connects game theory with local explanations, uniting several previous methods [1-7] and representing the only possible consistent and locally accurate additive feature attribution method based on expectations (see the SHAP NIPS paper for details).\n', '\n', ""<img src='https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/shap_diagram.png' width=400 height=400>\n"", '[image credits](https://github.com/slundberg/shap)\n', '><font color=""red""><b>Note: </b></font>\n', 'Shap can answer to this qeustion : **how the model works for an individual prediction?**']",id 57 br 5 7 shap valu shap shapley addit explan unifi approach explain output machin learn model shap connect game theori local explan unit sever previou method 1 7 repres possibl consist local accur addit featur attribut method base expect see shap nip paper detail img src http raw githubusercont com slundberg shap master doc artwork shap diagram png width 400 height 400 imag credit http github com slundberg shap font color red b note b font shap answer qeustion model work individu predict
617,"[""If you look carefully at the code where we created the SHAP values, you'll notice we reference Trees in  **shap.TreeExplainer(my_model)**. But the SHAP package has explainers for every type of model.\n"", '\n', '1. shap.DeepExplainer works with Deep Learning models.\n', '1. shap.KernelExplainer works with all models, though it is slower than other Explainers and it offers an approximation rather than exact Shap values.']",look care code creat shap valu notic refer tree shap treeexplain model shap packag explain everi type model 1 shap deepexplain work deep learn model 1 shap kernelexplain work model though slower explain offer approxim rather exact shap valu
618,"[' <a id=""6""></a> <br>\n', '# 6- Model Development\n', ""So far, we have used two  models, and at this point we add another model and we'll be expanding it soon.\n"", 'in this section you will see following model:\n', '1. lightgbm\n', '1. RandomForestClassifier\n', '1. DecisionTreeClassifier\n', '1. CatBoostClassifier']",id 6 br 6 model develop far use two model point add anoth model expand soon section see follow model 1 lightgbm 1 randomforestclassifi 1 decisiontreeclassifi 1 catboostclassifi
619,['## 6-1 lightgbm'],6 1 lightgbm
620,"[' <a id=""62""></a> <br>\n', '## 6-2 RandomForestClassifier']",id 62 br 6 2 randomforestclassifi
621,"[' <a id=""63""></a> <br>\n', '## 6-3 DecisionTreeClassifier']",id 63 br 6 3 decisiontreeclassifi
622,"[' <a id=""64""></a> <br>\n', '## 6-4 CatBoostClassifier']",id 64 br 6 4 catboostclassifi
623,['Now you can change your model and submit the results of other models.'],chang model submit result model
624,"[' <a id=""65""></a> <br>\n', '## 6-5 Funny Combine ']",id 65 br 6 5 funni combin
625,"['you can follow me on:\n', '> ###### [ GitHub](https://github.com/mjbahmani/)\n', '> ###### [Kaggle](https://www.kaggle.com/mjbahmani/)\n', '\n', "" <b>I hope you find this kernel helpful and some <font color='red'>UPVOTES</font> would be very much appreciated.<b/>\n"", ' ']",follow github http github com mjbahmani kaggl http www kaggl com mjbahmani b hope find kernel help font color red upvot font would much appreci b
626,"[' <a id=""7""></a> <br>\n', '# 7- References & credits\n', 'Thanks fo following kernels that help me to create this kernel.']",id 7 br 7 refer credit thank fo follow kernel help creat kernel
627,"['1. [https://www.kaggle.com/dansbecker/permutation-importance](https://www.kaggle.com/dansbecker/permutation-importance)\n', '1. [https://www.kaggle.com/dansbecker/partial-plots](https://www.kaggle.com/dansbecker/partial-plots)\n', '1. [https://www.kaggle.com/miklgr500/catboost-with-gridsearch-cv](https://www.kaggle.com/miklgr500/catboost-with-gridsearch-cv)\n', '1. [https://www.kaggle.com/dromosys/sctp-working-lgb](https://www.kaggle.com/dromosys/sctp-working-lgb)\n', '1. [https://www.kaggle.com/gpreda/santander-eda-and-prediction](https://www.kaggle.com/gpreda/santander-eda-and-prediction)\n', '1. [https://www.kaggle.com/dansbecker/permutation-importance](https://www.kaggle.com/dansbecker/permutation-importance)\n', '1. [https://www.kaggle.com/dansbecker/partial-plots](https://www.kaggle.com/dansbecker/partial-plots)\n', '1. [https://www.kaggle.com/dansbecker/shap-values](https://www.kaggle.com/dansbecker/shap-values)\n', '1. [https://docs.microsoft.com/en-us/azure/machine-learning/studio/algorithm-choice](https://docs.microsoft.com/en-us/azure/machine-learning/studio/algorithm-choice)\n', '1. [kernel https://www.kaggle.com/arjanso/reducing-dataframe-memory-size-by-65](kernel https://www.kaggle.com/arjanso/reducing-dataframe-memory-size-by-65)\n', '1. [https://www.kaggle.com/brandenkmurray/nothing-works](https://www.kaggle.com/brandenkmurray/nothing-works)']",1 http www kaggl com dansbeck permut import http www kaggl com dansbeck permut import 1 http www kaggl com dansbeck partial plot http www kaggl com dansbeck partial plot 1 http www kaggl com miklgr500 catboost gridsearch cv http www kaggl com miklgr500 catboost gridsearch cv 1 http www kaggl com dromosi sctp work lgb http www kaggl com dromosi sctp work lgb 1 http www kaggl com gpreda santand eda predict http www kaggl com gpreda santand eda predict 1 http www kaggl com dansbeck permut import http www kaggl com dansbeck permut import 1 http www kaggl com dansbeck partial plot http www kaggl com dansbeck partial plot 1 http www kaggl com dansbeck shap valu http www kaggl com dansbeck shap valu 1 http doc microsoft com en u azur machin learn studio algorithm choic http doc microsoft com en u azur machin learn studio algorithm choic 1 kernel http www kaggl com arjanso reduc datafram memori size 65 kernel http www kaggl com arjanso reduc datafram memori size 65 1 http www kaggl com brandenkmurray noth work http www kaggl com brandenkmurray noth work
628,"['# _**🌹🏆RoseGold 🏆🌹**_\n', '\n', 'Contents:\n', '1. Exploratory Data Analysis\n', '2. Principle Components Analysis\n', '3. Build LightGBM model']",rosegold content 1 exploratori data analysi 2 principl compon analysi 3 build lightgbm model
629,['We have class imbalanced class problem.'],class imbalanc class problem
630,"['## PCA\n', '\n', 'Borrowed from DataScience handbook Chapter 5\n', '\n', 'https://jakevdp.github.io/PythonDataScienceHandbook/05.09-principal-component-analysis.html']",pca borrow datasci handbook chapter 5 http jakevdp github io pythondatasciencehandbook 05 09 princip compon analysi html
631,['The most descriptive feature in the dataset (component 1) is positively correlated with the target!'],descript featur dataset compon 1 posit correl target
632,['## Decision Tree'],decis tree
633,['Score is 0.863 on leaderboard (lb).'],score 0 863 leaderboard lb
634,"['TODO\n', '* https://jakevdp.github.io/PythonDataScienceHandbook/05.13-kernel-density-estimation.html\n', '* https://jakevdp.github.io/PythonDataScienceHandbook/05.11-k-means.html']",todo http jakevdp github io pythondatasciencehandbook 05 13 kernel densiti estim html http jakevdp github io pythondatasciencehandbook 05 11 k mean html
635,"['**Santander Customer Transaction Prediction  \n', 'Can you identify who will make a transaction? **    \n', '![](https://bit.ly/2BJideW)  \n', 'Santander inivte fellow Kagglers to help them identify which customers will make a specific transaction in the future, irrespective of the amount of money transacted. The data provided for this competition has the same structure as the real data they have available to solve this problem.']",santand custom transact predict identifi make transact http bit ly 2bjidew santand inivt fellow kaggler help identifi custom make specif transact futur irrespect amount money transact data provid competit structur real data avail solv problem
636,"['**Analysis Playground**  \n', 'As in every data science prediction problem, I will start with Exploratory Data Analysis (EDA) and move on building model on different machine learning algorithms.']",analysi playground everi data scienc predict problem start exploratori data analysi eda move build model differ machin learn algorithm
637,['**Loading the required packages for analysis**'],load requir packag analysi
638,['**Reading the Training and Testing Dataset**'],read train test dataset
639,"['This is really odd, as I have never come across a scenario where both the training and testing dataset have the same number of rows. Seems interesting. Here the number of features are bit higer in number. So, we will find which all variables are important based on missing values, correlation analysis etc.']",realli odd never come across scenario train test dataset number row seem interest number featur bit higer number find variabl import base miss valu correl analysi etc
640,['**Information on the training dataset**'],inform train dataset
641,"['.info() command in python give the brief glimpse of the dataset. Our traning dataset has three different types of datatypes. most of them are float which are contineous, one feature has integer which is most probably the ""Target"" column and the final one feature is of object type which i think will be the ""ID_code"" column.']",info command python give brief glimps dataset trane dataset three differ type datatyp float contin one featur integ probabl target column final one featur object type think id code column
642,['**Summary Statistics**'],summari statist
643,"['**Target Distribution**  \n', 'First let us look at the distribution of the target variable to understand whether the dataset is imbalanced or not.']",target distribut first let u look distribut target variabl understand whether dataset imbalanc
644,"['From the target proportion and target pie chart its clearly evident that the target is highly imbalanced with ""0"" class occupying 90% of the target values and 10% of target values with ""1"" class.']",target proport target pie chart clearli evid target highli imbalanc 0 class occupi 90 target valu 10 target valu 1 class
645,"['**Missing Value Proportion**  \n', 'Now, Let us check the proportion of  many missing values in the training dataset.']",miss valu proport let u check proport mani miss valu train dataset
646,['This Looks great as we have no missing values in the dataset.'],look great miss valu dataset
647,"['**Correlation Coefficient Plot**  \n', ""As there are no missing values in the dataset and all the features are numberic let's try the correlation plot and see how the features are correlated to each other.""]",correl coeffici plot miss valu dataset featur number let tri correl plot see featur correl
648,"['The correlation coefficient values are very low and the maximum value is around 0.08 in negative side of the plot, with respect to positive side the maximum value is around 0.07.\n', '\n', 'Overall, the correlation of the features with respect to target are very low.\n', '\n', 'So, We will take some of the features which has high correlation values and plot the heatmap for further analysis.']",correl coeffici valu low maximum valu around 0 08 neg side plot respect posit side maximum valu around 0 07 overal correl featur respect target low take featur high correl valu plot heatmap analysi
649,"['Plotting heatmap is done to identify if there are any strong monotonic relationships between these important features. If the values are high, then probably we can choose to keep one of those variables in the model building process. But, we are doing this only for small set of features. we can even try other techniques to explore other features in the dataset.']",plot heatmap done identifi strong monoton relationship import featur valu high probabl choos keep one variabl model build process small set featur even tri techniqu explor featur dataset
650,"['Seems like none of the selected variables have spearman correlation more than 0.7 with each other.\n', '\n', 'The above plots helped us in identifying the important individual variables which are correlated with target. However we generally build many non-linear models in Kaggle competitions. So let us build some non-linear models and get variable importance from them.']",seem like none select variabl spearman correl 0 7 plot help u identifi import individu variabl correl target howev gener build mani non linear model kaggl competit let u build non linear model get variabl import
651,"['**Feature Importance - eli5 library **  \n', ""For feature importance, I am going to use the Permutation Importance technique that's being used in the [tutorial](https://www.kaggle.com/dansbecker/permutation-importance)""]",featur import eli5 librari featur import go use permut import techniqu use tutori http www kaggl com dansbeck permut import
652,"['Interpreting Permutation Importances\n', 'The values towards the top are the most important features, and those towards the bottom matter least.\n', '\n', 'The first number in each row shows how much model performance decreased with a random shuffling (in this case, using ""accuracy"" as the performance metric).\n', '\n', 'Like most things in data science, there is some randomness to the exact performance change from a shuffling a column. We measure the amount of randomness in our permutation importance calculation by repeating the process with multiple shuffles. The number after the ± measures how performance varied from one-reshuffling to the next.\n', '\n', ""You'll occasionally see negative values for permutation importances. In those cases, the predictions on the shuffled (or noisy) data happened to be more accurate than the real data. This happens when the feature didn't matter (should have had an importance close to 0), but random chance caused the predictions on shuffled data to be more accurate. This is more common with small datasets, like the one in this example, because there is more room for luck/chance.\n"", '\n', 'In our case, the top 10 most important feature are var_81, var_53, var_139, var_179, var_174, var_40, var_26, var_13, var_24 and var_109. But, Still all the features seems to have value importance close to zero.']",interpret permut import valu toward top import featur toward bottom matter least first number row show much model perform decreas random shuffl case use accuraci perform metric like thing data scienc random exact perform chang shuffl column measur amount random permut import calcul repeat process multipl shuffl number measur perform vari one reshuffl next occasion see neg valu permut import case predict shuffl noisi data happen accur real data happen featur matter import close 0 random chanc caus predict shuffl data accur common small dataset like one exampl room luck chanc case top 10 import featur var 81 var 53 var 139 var 179 var 174 var 40 var 26 var 13 var 24 var 109 still featur seem valu import close zero
653,"['**SMOTE Over-Sampling**  \n', ""As we have more records for target '0', I am going to over sample the target '1' to the same level as target '0' which is basically oversampling the least class.""]",smote sampl record target 0 go sampl target 1 level target 0 basic oversampl least class
654,"['**Time for Modelling**  \n', '**LGBM**  \n', ""Let's try with Lightgbm and see the accuracy.""]",time model lgbm let tri lightgbm see accuraci
655,['**Submission File**'],submiss file
656,"['Decision trees and neural nets have trouble classifying examples when trained on imbalanced data. This kernel will explore a wide range of resampling techniques, how they vary, and their effect on XGBoost.\n', '\n', '### Contents\n', '1. [Introduction](#introduction)\n', '2. [Table of techniques](#technique-table)\n', '    1. [Random Undersampling](#random-undersampling)\n', '    2. [Tomek Links](#tomek-links)\n', '    3. [AllKNN](#allknn)\n', '    4. [Edited Nearest Neighbor](#enn)\n', '    5. [Random Oversampling](#random-oversampling)\n', '    6. [ADASYN](#adasyn)\n', '    7. [SMOTE](#smote)\n', '    2. [SMOTETOMEK](#smotetomek)\n', '    2. [SMOTEENN](#smoteenn)\n', '3. [Training XGBoost](#training-xgboost)\n', '4. [Conclusion](#conclusion)']",decis tree neural net troubl classifi exampl train imbalanc data kernel explor wide rang resampl techniqu vari effect xgboost content 1 introduct introduct 2 tabl techniqu techniqu tabl 1 random undersampl random undersampl 2 tomek link tomek link 3 allknn allknn 4 edit nearest neighbor enn 5 random oversampl random oversampl 6 adasyn adasyn 7 smote smote 2 smotetomek smotetomek 2 smoteenn smoteenn 3 train xgboost train xgboost 4 conclus conclus
657,"[""<a id='introduction'></a>\n"", '# Target Class Imbalance\n', '---\n', ""In the Santander customer transaction prediction data we have a binary target variable where 1 is a successful future transaction and 0 is no future transaction. The problem is that we have an imbalance of about 7:1. If we train on this data we are likely to have a model that will missclassify the minority class, 'yes', because it has seen so few examples. \n"", '\n', 'To deal with class imbalance we can resample. Resampling can mean that we oversample a minority class or undersample a majority class to introduce bias to select a more even distribution of classes. Class imbalance is something we will regularly see in tasks like network intrusion, rare disease diagnosing, and fraud detection. ']",id introduct target class imbal santand custom transact predict data binari target variabl 1 success futur transact 0 futur transact problem imbal 7 1 train data like model missclassifi minor class ye seen exampl deal class imbal resampl resampl mean oversampl minor class undersampl major class introduc bia select even distribut class class imbal someth regularli see task like network intrus rare diseas diagnos fraud detect
658,"[""<a id='technique-table'></a>\n"", '# Resampling Techniques\n', '---\n', '### Undersampling Techniques \n', '1. Random Undersampling\n', '2. Tomek Links\n', '3. AllKNN\n', '4. ENN (Edited Nearest Neighbours)\n', '\n', '### Oversampling Techniques\n', '1. Random Oversampling\n', '2. ADASYN (Adaptive Synthetic Sampling)\n', '3. SMOTE (Synthetic Minority Over-Sampling Technique)\n', '\n', '### Combined Resampling\n', '1. SMOTETomek\n', '2. SMOTEENN']",id techniqu tabl resampl techniqu undersampl techniqu 1 random undersampl 2 tomek link 3 allknn 4 enn edit nearest neighbour oversampl techniqu 1 random oversampl 2 adasyn adapt synthet sampl 3 smote synthet minor sampl techniqu combin resampl 1 smotetomek 2 smoteenn
659,"[""<a id='random-undersampling'></a>\n"", '### Random Undersampling\n', ""The simplest form of undersampling is to remove random records from the majority class. With imblearn's implementation we can choose to remove samples with or without replacement. The biggest drawback to this form of undersampling is loss of information.""]",id random undersampl random undersampl simplest form undersampl remov random record major class imblearn implement choos remov sampl without replac biggest drawback form undersampl loss inform
660,"[""<a id='tomek-links'></a>\n"", '### Tomek Links\n', 'Tomek links can be used as an under-sampling method or as a data cleaning method. A Tomek link is any place where two samples of different classes are nearest neighbors. When we find a Tomek link we can choose which observatin to delete- in undersampling we remove the majority class. \n', '\n', 'The difference between the data before and after Tomek links is subtle but clear- Tomek links is a great technique we can use to clear up our boundaries in classificatino problems. ']",id tomek link tomek link tomek link use sampl method data clean method tomek link place two sampl differ class nearest neighbor find tomek link choos observatin delet undersampl remov major class differ data tomek link subtl clear tomek link great techniqu use clear boundari classificatino problem
661,"[""<a id='allknn'></a>\n"", '### AllKNN\n', 'AllKNN is a method also created by the Ivan Tomek that deletes an object if a KNN classifier misclassifies it. In imblearn the default value of k is 3, but we can also pass a value. In the below cell its worth passing different values to `n_neighbors`. AllKNN tends to delete more datapoints than ENN, especially as the value of k increases. I think that it undersamples too haphazardly. ']",id allknn allknn allknn method also creat ivan tomek delet object knn classifi misclassifi imblearn default valu k 3 also pas valu cell worth pas differ valu n neighbor allknn tend delet datapoint enn especi valu k increas think undersampl haphazardli
662,"[""<a id='enn'></a>\n"", '### ENN (Edited Nearest Neighbours)\n', 'ENN removes examples whose class label differs from the class of at least half of its k nearest neighbors. The benefit of ENN is that we can remove examples of the majority class while retaining as much information as possible because we are only removing redundant observations. ']",id enn enn edit nearest neighbour enn remov exampl whose class label differ class least half k nearest neighbor benefit enn remov exampl major class retain much inform possibl remov redund observ
663,"[""<a id='random-oversampling'></a>\n"", '### Random Oversampling\n', 'The simplest implementation of oversampling is to duplicate random records from the minority class, this can cause overfitting. ']",id random oversampl random oversampl simplest implement oversampl duplic random record minor class caus overfit
664,"[""<a id='adasyn'></a>\n"", '### ADASYN (Adaptive Synthetic Sampling)\n', 'ADASYN adaptively generates samples next to original observations which are wrongly classified by a KNN classifier. Unlike SMOTE that generates new samples that lie inside the class boundary, ADASYN tends to generate new samples near existing outliers. \n', '\n', 'You can run these code cells with different data samples to see how  ADASYN tends to change the data distribution, but especially in contrast to SMOTE we can see how it tends to constuct points on the frontier of our existing data. ']",id adasyn adasyn adapt synthet sampl adasyn adapt gener sampl next origin observ wrongli classifi knn classifi unlik smote gener new sampl lie insid class boundari adasyn tend gener new sampl near exist outlier run code cell differ data sampl see adasyn tend chang data distribut especi contrast smote see tend constuct point frontier exist data
665,"[""<a id='smote'></a>\n"", '### SMOTE (Synthetic Minority Over-Sampling Technique)\n', 'SMOTE synthesizes new examples by interpolating existing observations. SMOTE begins by iterating over every minority class instace and choosing its k nearest neighbors. The algorithm then constructs new instances halfway between the chosen obervations and its k neighbors. The greatest limitation of SMOTE is that it can only construct examples within the body of observations, never outside. If we compare the rebalanced data in the SMOTE plot against the plot for ADASYN we can see this exact effect. \n', '\n', 'SMOTE has several variants like SVMSMOTE and BorderlineSMOTE.']",id smote smote synthet minor sampl techniqu smote synthes new exampl interpol exist observ smote begin iter everi minor class instac choos k nearest neighbor algorithm construct new instanc halfway chosen oberv k neighbor greatest limit smote construct exampl within bodi observ never outsid compar rebalanc data smote plot plot adasyn see exact effect smote sever variant like svmsmote borderlinesmot
666,"[""<a id='smotetomek'></a>\n"", '### SMOTETomek\n', 'SMOTETomek is the combination of using Tomek links to undersample the majoirty class and the use of SMOTE to oversample the minority class. ']",id smotetomek smotetomek smotetomek combin use tomek link undersampl majoirti class use smote oversampl minor class
667,"[""<a id='smoteenn'></a>\n"", '### SMOTEENN\n', 'SMOTEENN is the combination of SMOTE and Edited Nearest Neighbor. ENN removes any example whose class label differs from the class label of at least two of its three nearest neighbors. ENN tends to remove more examples then the Tomek links. \n', '\n', ""There's a really interesting difference here between SMOTETomek and SMOTEENN. There are so few minority class examples that Tomek Links are not nearly as effective at undersampling the majority class. If we we're using a built in method we could first perform SMOTE and then perform the Tomek Links step but ""]",id smoteenn smoteenn smoteenn combin smote edit nearest neighbor enn remov exampl whose class label differ class label least two three nearest neighbor enn tend remov exampl tomek link realli interest differ smotetomek smoteenn minor class exampl tomek link nearli effect undersampl major class use built method could first perform smote perform tomek link step
668,"[""<a id='training-xgboost'></a>\n"", '# XGBoost with Unbalanced Data\n', 'Now that we know about strategies to deal with unbalanced data, here is the effect of unbalanced data on our gradient boosted random forests. \n', '\n', 'We will fit XGBoost models with Bayesian hyperparameter optimization to two datasets, first our unbalanced dataset- and second, a dataset that we have balanced with Smotetomek. The hyperparameter optimization library Hyperopt is great because it will do the optimization for us if we pass (1) a hyperparameter feature space, (2) an objective function that fits the model and returns a score to minimize, and (3) a `Trials` object that we can store arbitary data in from the model training. ']",id train xgboost xgboost unbalanc data know strategi deal unbalanc data effect unbalanc data gradient boost random forest fit xgboost model bayesian hyperparamet optim two dataset first unbalanc dataset second dataset balanc smotetomek hyperparamet optim librari hyperopt great optim u pas 1 hyperparamet featur space 2 object function fit model return score minim 3 trial object store arbitari data model train
669,"[""<a id='conclusion'></a>\n"", '# Imbalanced Dataset Conclusion\n', '---\n', 'In the confusion matrix we can see how much more often the imbalanced dataset correctly identifies class one observations as class one- the bottom right hand square. \n', '\n', 'The dropoff in accuracy between the two sets is a little over 5% depending on how the sample distribution shakes out, and the test auc scores tends to be about 10% different. This is the difference between a 2:1 target label imbalance and 9:1. \n', '\n', 'Imbalances in trees can have a significant effect on our classification power. While we focus on target label imbalance here, imbalances in the distributions of values and classes is an important topic in tree based models. If we go back and look at the charts of points that we use as examples we can see how tightly grouped points of two different classes can be- the job of XGBoost is to be able to disambiguate between these points and and that requires robust data. Imabalnce problems dont just have to be in the class of target label, we can face imbalances where there are two few examples of one category in a categorical variable, outliers in the distribution of a numerical variable, and imbalances between train and test sets. \n', '\n', 'Something you should try on your own is rerunning this kernel and seeing how a technique like ADASYN produces a different model than the SMOTE-based technique we used here. ']",id conclus imbalanc dataset conclus confus matrix see much often imbalanc dataset correctli identifi class one observ class one bottom right hand squar dropoff accuraci two set littl 5 depend sampl distribut shake test auc score tend 10 differ differ 2 1 target label imbal 9 1 imbal tree signific effect classif power focu target label imbal imbal distribut valu class import topic tree base model go back look chart point use exampl see tightli group point two differ class job xgboost abl disambigu point requir robust data imabalnc problem dont class target label face imbal two exampl one categori categor variabl outlier distribut numer variabl imbal train test set someth tri rerun kernel see techniqu like adasyn produc differ model smote base techniqu use
670,['# Load libraries and Functions'],load librari function
671,"['# Compress dataset \n', 'my computer has a small ram, and everytime I join a competition, the first thing I will do is to compress my dataset, after compressing, the size of the dataset drop from 619MB to 158MB (70%!!!), you can even set some unsigned datatypes, and the size of your dataset will be smaller. Hope you guys enjoy this competition!']",compress dataset comput small ram everytim join competit first thing compress dataset compress size dataset drop 619mb 158mb 70 even set unsign datatyp size dataset smaller hope guy enjoy competit
672,"[""I have decided to see if there are any outliers in the dataset according to [Chauvenet's criterion](https://en.wikipedia.org/wiki/Chauvenet%27s_criterion)\n"", '\n', 'An observation $P_i$ is an outlier if the following equation is true:\n', '$$erfc\\Bigg(\\frac{|P_i - \\bar{P}|}{S_p}\\Bigg) < \\frac{1}{2n}$$\n', '\n', '$$erfc(x) = \\frac{2}{\\sqrt{\\pi}} \\int_x^\\infty \\mathrm{e}^{-t^2}\\;\\mathrm{d}t$$']",decid see outlier dataset accord chauvenet criterion http en wikipedia org wiki chauvenet 27 criterion observ p outlier follow equat true erfc bigg frac p bar p p bigg frac 1 2n erfc x frac 2 sqrt pi int x infti mathrm e 2 mathrm
673,['Plots with a distribution for 5 top features before removing an outliers and after removing them.'],plot distribut 5 top featur remov outlier remov
674,"['Big thanks to Jiwei Liu for Augment insight!\n', 'https://www.kaggle.com/jiweiliu/lgb-2-leaves-augment\n']",big thank jiwei liu augment insight http www kaggl com jiweiliu lgb 2 leav augment
675,"[""Forked from @VisheshShrivastav. Using the basic framework from vishesh's Kernel ""]",fork visheshshrivastav use basic framework vishesh kernel
676,['## Data Load and Exploration'],data load explor
677,"['## Normalize Data\n', 'We could let FastAI normalize the data automatically, but we choose to do so manually for more flexibility']",normal data could let fastai normal data automat choos manual flexibl
678,"['## Feature Engineering\n', ""We don't really do anything special but polynomial features""]",featur engin realli anyth special polynomi featur
679,['## Split training data into train and validation sets'],split train data train valid set
680,['Grab a statistic summary of the training set. We may use this later in adding noises to the data during training'],grab statist summari train set may use later ad nois data train
681,"['## FastAI Tabular Learner\n', 'We start off with the default learner from FastAI']",fastai tabular learner start default learner fastai
682,"['First we want to find the correct learning rate for this dataset/problem. This only needs to run once.\n', 'The *optimal* learning rate found is 0.01']",first want find correct learn rate dataset problem need run optim learn rate found 0 01
683,"['This is the main train and evaluate function. Since we are training multiple learners, we choose to save the model to harddisk and load them later if needed.']",main train evalu function sinc train multipl learner choos save model harddisk load later need
684,['## Visualize ROC on the Validation Set'],visual roc valid set
685,['## Test and Submit'],test submit
686,['**Data Visualization**'],data visual
687,"['Visualizing Data is a very important step in a Data Science project.  As per the recent Kagagle Survey 10-20% of the total Data Science Project time is spent on visualizing data. https://www.kaggle.com/rajeshcv/state-of-data-science-machine-learning-2018\n', '\n', 'SaS Data Visualization’s webpage explain Data visualization  beautifully.\n', '\n', ""*'The way the human brain processes information, using charts or graphs to visualize large amounts of complex data is easier than poring over spreadsheets or reports. Data visualization is a quick, easy way to convey concepts in a universal manner\u200a—\u200aand you can experiment with different scenarios by making slight adjustments.'*\n"", '\n', 'In the Santander Customer Transaction Prediction competition the features are predominently numeric.\n', '\n', ""This kernel's objective is to\n"", '*     Understand the value distribution in various features through boxplots and histograms. \n', '*     Seggregate features into groups based on range of values.\n', '*     Identify  features with similiar value distribution.\n', ""*     To understand if there is any difference in values between the two target groups 'transaction done and 'transaction not done'\n"", '*  Check whether feature values in test and train comes from the same sampling  distribution.\n', '\n', '\n']",visual data import step data scienc project per recent kagagl survey 10 20 total data scienc project time spent visual data http www kaggl com rajeshcv state data scienc machin learn 2018 sa data visual webpag explain data visual beauti way human brain process inform use chart graph visual larg amount complex data easier pore spreadsheet report data visual quick easi way convey concept univers manner u200a u200aand experi differ scenario make slight adjust santand custom transact predict competit featur predomin numer kernel object understand valu distribut variou featur boxplot histogram seggreg featur group base rang valu identifi featur similiar valu distribut understand differ valu two target group transact done transact done check whether featur valu test train come sampl distribut
688,"[""All the 200 features have numeric values . Let's check first if some of these numeric features are categorical  or boolean .In that case they will have less than 500 unique values.""]",200 featur numer valu let check first numer featur categor boolean case le 500 uniqu valu
689,"[""None of the features are categorical or boolean except 'target'. Let's understand the range of values of each of these features by plotting the max, min and median value of the features.""]",none featur categor boolean except target let understand rang valu featur plot max min median valu featur
690,['Combined all values in the features are between 80 and -90.  From the plot it looks like the features can be seggregrated into 10 groups  based on their max and min values.'],combin valu featur 80 90 plot look like featur seggregr 10 group base max min valu
691,['**Features with  positive values and maximum value less than 10**'],featur posit valu maximum valu le 10
692,"['var_68,var_91,var_103,var_148 and var_161 have comparatively lower range of values .\n', 'The histograms below shows the distribution of values in cases of transaction done in green color (target=1) and transaction not done (target=0) in red colour.']",var 68 var 91 var 103 var 148 var 161 compar lower rang valu histogram show distribut valu case transact done green color target 1 transact done target 0 red colour
693,"['var_103 values lie  between 1.1 and 2 , var_148 between 3.4 and 4.6 , var_68 values are in a narrow range between 4.99 and 5.04,   var_161 between 5 and 6.2  &  var_91 between 6.6 and 7.4.  Considering var_166 with values between 2 and 4 and var_169 and var_133  they all appear to be in sequence.\n', '\n', 'However there is no significant difference in values between the ""transaction done"" and ""transaction not done"" groups']",var 103 valu lie 1 1 2 var 148 3 4 4 6 var 68 valu narrow rang 4 99 5 04 var 161 5 6 2 var 91 6 6 7 4 consid var 166 valu 2 4 var 169 var 133 appear sequenc howev signific differ valu transact done transact done group
694,['**Features with  positive values and maximum value between 10 & 20**'],featur posit valu maximum valu 10 20
695,"['var_12,  var_15 ,var_25, var_34,  var_43, var_108, var_125 have very low range of values further elaborated by the histogram below.']",var 12 var 15 var 25 var 34 var 43 var 108 var 125 low rang valu elabor histogram
696,['All those variables with a short range of values have values in the range 10 to 15 and as in the earlier group appear to be in some sequence.'],variabl short rang valu valu rang 10 15 earlier group appear sequenc
697,['**Features with  positive values and maximum value greater than 20**'],featur posit valu maximum valu greater 20
698,"['var_85, var_194 and  var_198 appear to have similiar distribution of values.  var_0, var_46 , var_56, var_175 and var_177 also appear to have a similiar value distribution.']",var 85 var 194 var 198 appear similiar distribut valu var 0 var 46 var 56 var 175 var 177 also appear similiar valu distribut
699,['**Features with  values between 10 and -10**'],featur valu 10 10
700,['**Features with  max value between 10 &  20  and min values between 0 & -10**'],featur max valu 10 20 min valu 0 10
701,"[""From the above histogram for many of the features the ' transaction done'  group in green seems to have  lower range than the 'transaction not done' group in red.""]",histogram mani featur transact done group green seem lower rang transact done group red
702,['**Features with  max value between 10 &  20  and min values between -10 & -20**'],featur max valu 10 20 min valu 10 20
703,"['var_39,var_65 and var_138 appear to have similiar distribution of values and so is var_63 and var_128\n', ""For some of the features the ' transaction done'  group in green seems to have  lower range than the 'transaction not done' group in red.""]",var 39 var 65 var 138 appear similiar distribut valu var 63 var 128 featur transact done group green seem lower rang transact done group red
704,['**Features with  max value between 10 &  20  and min values less than  -20**'],featur max valu 10 20 min valu le 20
705,"['var_84, var _155, var_157 appear to have similiar distribution of values and so does  var_11, var_180 & var_185']",var 84 var 155 var 157 appear similiar distribut valu var 11 var 180 var 185
706,['**Features with  max value greater than 20  and min values less than  -20**'],featur max valu greater 20 min valu le 20
707,"['(var_73 & var_158) , (var_92,var_154,var_159 & var_163) , ( var_20 & var_55) \n', ""The features within the above groups  appear to have similiar distribution of values . Here also the range  of values for the *'transaction done'* group in green appears to be shorter.""]",var 73 var 158 var 92 var 154 var 159 var 163 var 20 var 55 featur within group appear similiar distribut valu also rang valu transact done group green appear shorter
708,['**Features with  max value more than 20 and min values between -10 and -20**'],featur max valu 20 min valu 10 20
709,['var_21 & var_172 appear to have similiar distribution of values'],var 21 var 172 appear similiar distribut valu
710,['**Features with  max value more than 20 and min values less than -20**'],featur max valu 20 min valu le 20
711,['var_47 & var_187 appear to have similiar distribution of values'],var 47 var 187 appear similiar distribut valu
712,['**Checking for correlation between feature**s'],check correl featur
713,['***No correlation between any features . Does this mean all features are important?***'],correl featur mean featur import
714,['**Kolmogorov-Smirnov test**'],kolmogorov smirnov test
715,"[""Before concluding let's do a check of whether feature values in test and train comes from the same sampling  distribution.\n"", 'Kolmogorov-Smirnov  is a two-sided test for the null hypothesis that 2 independent samples are drawn from the same continuous distribution.\n', 'If the K-S statistic is small or the p-value is high, then we cannot reject the hypothesis that the distributions of the two samples are the same.']",conclud let check whether featur valu test train come sampl distribut kolmogorov smirnov two side test null hypothesi 2 independ sampl drawn continu distribut k statist small p valu high reject hypothesi distribut two sampl
716,"[""For the two tailed test at 95% confidence level the pvalue has to be less than 0.05 to reject the null hypothesis that both samples are from same distribution.Let's look for values less than 0.05""]",two tail test 95 confid level pvalu le 0 05 reject null hypothesi sampl distribut let look valu le 0 05
717,"['As per the Kolmogorov-Smirnov test 46 features have a high probability of not being from the same sampling distribution.\n', 'Will this affect the models? \n', ""Let's combine the test and train data to compare these features and understand their didtribution in train and test.""]",per kolmogorov smirnov test 46 featur high probabl sampl distribut affect model let combin test train data compar featur understand didtribut train test
718,"['<h1><center><font size=""6"">Santander EDA, PCA and Light GBM Classification Model</font></center></h1>\n', '\n', '<img src=""https://upload.wikimedia.org/wikipedia/commons/thumb/4/4a/Another_new_Santander_bank_-_geograph.org.uk_-_1710962.jpg/640px-Another_new_Santander_bank_-_geograph.org.uk_-_1710962.jpg""></img>\n', '\n', '<br>\n', '<b>\n', 'In this challenge, Santander invites Kagglers to help them identify which customers will make a specific transaction in the future, irrespective of the amount of money transacted. The data provided for this competition has the same structure as the real data they have available to solve this problem. \n', 'The data is anonimyzed, each row containing 200 numerical values identified just with a number.</b>\n', '\n', ""<b>Inspired by Jiwei Liu's Kernel. I added Data Augmentation Segment to my kernel</b>\n"", '\n', '<pre>\n', ""<a id='0'><b>Content</b></a>\n"", ""- <a href='#1'><b>Import the Data</b></a>\n"", ""- <a href='#11'><b>Data Exploration</b></a>  \n"", ""- <a href='#2'><b>Check for the missing values</b></a>  \n"", ""- <a href='#3'><b>Visualizing the Satendar Customer Transactions Data</b></a>   \n"", "" - <a href='#31'><b>Check for Class Imbalance</b></a>   \n"", "" - <a href='#32'><b>Distribution of Mean and Standard Deviation</b></a>   \n"", "" - <a href='#33'><b>Distribution of Skewness</b></a>   \n"", "" - <a href='#34'><b>Distribution of Kurtosis</b></a>   \n"", ""- <a href='#4'><b>Principal Component Analysis</b></a>\n"", "" - <a href='#41'><b>Kernel PCA</b></a>\n"", '- <a href = ""#16""><b>Data Augmentation</b></a>\n', ""- <a href='#6'><b>Build the Light GBM Model</b></a></pre>""]",h1 center font size 6 santand eda pca light gbm classif model font center h1 img src http upload wikimedia org wikipedia common thumb 4 4a anoth new santand bank geograph org uk 1710962 jpg 640px anoth new santand bank geograph org uk 1710962 jpg img br b challeng santand invit kaggler help identifi custom make specif transact futur irrespect amount money transact data provid competit structur real data avail solv problem data anonimyz row contain 200 numer valu identifi number b b inspir jiwei liu kernel ad data augment segment kernel b pre id 0 b content b href 1 b import data b href 11 b data explor b href 2 b check miss valu b href 3 b visual satendar custom transact data b href 31 b check class imbal b href 32 b distribut mean standard deviat b href 33 b distribut skew b href 34 b distribut kurtosi b href 4 b princip compon analysi b href 41 b kernel pca b href 16 b data augment b href 6 b build light gbm model b pre
719,['<a id=1><pre><b>Import the Data</b></pre></a>'],id 1 pre b import data b pre
720,['<a id=11><pre><b>Data Exploration</b></pre></a>'],id 11 pre b data explor b pre
721,['<a id=2><b><pre>Check for the Missing Values.</pre></b></a> '],id 2 b pre check miss valu pre b
722,['<pre>There are no missing values in the dataset</pre>'],pre miss valu dataset pre
723,['<pre><a id = 3><b>Visualizing the Satendar Customer Transactions Data</b></a></pre>'],pre id 3 b visual satendar custom transact data b pre
724,['<pre><a id = 31 ><b>Check for Class Imbalance</b></a></pre>'],pre id 31 b check class imbal b pre
725,"['<pre><a id = 32 ><b>Distribution of Mean and Standard Deviation</b></a></pre>\n', '\n', '<pre>EDA Reference : https://www.kaggle.com/gpreda/santander-eda-and-prediction</pre>']",pre id 32 b distribut mean standard deviat b pre pre eda refer http www kaggl com gpreda santand eda predict pre
726,"[""<pre>Let's check the distribution of the mean of values per columns in the train and test datasets.</pre>""]",pre let check distribut mean valu per column train test dataset pre
727,['<pre>Distribution for Standard Deviation</pre>'],pre distribut standard deviat pre
728,"[""<pre>Let's check the distribution of the standard deviation of values per columns in the train and test datasets.</pre>""]",pre let check distribut standard deviat valu per column train test dataset pre
729,"[""<pre>Let's check now the distribution of the mean value per row in the train dataset, grouped by value of target</pre>""]",pre let check distribut mean valu per row train dataset group valu target pre
730,"[""<pre>Let's check now the distribution of the mean values per columns in the train and test datasets.</pre>""]",pre let check distribut mean valu per column train test dataset pre
731,"[""<pre>Let's check now the distribution of the standard deviation  per row in the train dataset, grouped by value of target</pre>""]",pre let check distribut standard deviat per row train dataset group valu target pre
732,"[""<pre>Let's check now the distribution of standard deviation per columns in the train and test datasets.</pre>""]",pre let check distribut standard deviat per column train test dataset pre
733,"['<pre><a id = 33 ><b>Distribution of Skewness</b></a></pre>\n', '\n', ""<pre>Let's see now the distribution of skewness on rows in train separated for values of target 0 and 1. We found the distribution is left skewed</pre>""]",pre id 33 b distribut skew b pre pre let see distribut skew row train separ valu target 0 1 found distribut left skew pre
734,"[""<pre>Let's see now the distribution of skewness on columns in train separated for values of target 0 and 1.</pre>""]",pre let see distribut skew column train separ valu target 0 1 pre
735,['<pre><a id = 34 ><b>Distribution of Kurtosis</b></a></pre>'],pre id 34 b distribut kurtosi b pre
736,"[""<pre>Let's see now the distribution of kurtosis on rows in train separated for values of target 0 and 1. We found the distribution to be Leptokurtic</pre>""]",pre let see distribut kurtosi row train separ valu target 0 1 found distribut leptokurt pre
737,"[""<pre>Let's see now the distribution of kurtosis on columns in train separated for values of target 0 and 1.</pre>""]",pre let see distribut kurtosi column train separ valu target 0 1 pre
738,['<a id=4><pre><b>Principal Component Analysis to check Dimentionality Reduction<b></pre></a>'],id 4 pre b princip compon analysi check dimention reduct b pre
739,"[""<pre><a id = 41><b>Kernel PCA (Since the Graph above doesn't represent meaningful analysis)</b></a></pre>""]",pre id 41 b kernel pca sinc graph repres meaning analysi b pre
740,"[""<pre>Since PCA hasn't been useful, I decided to proceed with the existing dataset</pre>""]",pre sinc pca use decid proceed exist dataset pre
741,['<pre><a id = 16><b>Data Augmentation</b></a></pre>'],pre id 16 b data augment b pre
742,['<pre><a id = 6><b>Build the Light GBM Model</b></a></pre>'],pre id 6 b build light gbm model b pre
743,"['<h1><center><font size=""6"">Santander EDA, PCA and Light GBM Classification Model</font></center></h1>\n', '\n', '<img src=""https://upload.wikimedia.org/wikipedia/commons/thumb/4/4a/Another_new_Santander_bank_-_geograph.org.uk_-_1710962.jpg/640px-Another_new_Santander_bank_-_geograph.org.uk_-_1710962.jpg""></img>\n', '\n', '<br>\n', '<b>\n', 'In this challenge, Santander invites Kagglers to help them identify which customers will make a specific transaction in the future, irrespective of the amount of money transacted. The data provided for this competition has the same structure as the real data they have available to solve this problem. \n', 'The data is anonimyzed, each row containing 200 numerical values identified just with a number.</b>\n', '\n', ""<b>Inspired by Jiwei Liu's Kernel. I added Data Augmentation Segment to my kernel</b>\n"", '\n', '<pre>\n', ""<a id='0'><b>Content</b></a>\n"", ""- <a href='#1'><b>Import the Data</b></a>\n"", ""- <a href='#11'><b>Data Exploration</b></a>  \n"", ""- <a href='#2'><b>Check for the missing values</b></a>  \n"", ""- <a href='#3'><b>Visualizing the Satendar Customer Transactions Data</b></a>   \n"", "" - <a href='#31'><b>Check for Class Imbalance</b></a>   \n"", "" - <a href='#32'><b>Distribution of Mean and Standard Deviation</b></a>   \n"", "" - <a href='#33'><b>Distribution of Skewness</b></a>   \n"", "" - <a href='#34'><b>Distribution of Kurtosis</b></a>   \n"", ""- <a href='#4'><b>Principal Component Analysis</b></a>\n"", "" - <a href='#41'><b>Kernel PCA</b></a>\n"", '- <a href = ""#16""><b>Data Augmentation</b></a>\n', ""- <a href='#6'><b>Build the Light GBM Model</b></a></pre>""]",h1 center font size 6 santand eda pca light gbm classif model font center h1 img src http upload wikimedia org wikipedia common thumb 4 4a anoth new santand bank geograph org uk 1710962 jpg 640px anoth new santand bank geograph org uk 1710962 jpg img br b challeng santand invit kaggler help identifi custom make specif transact futur irrespect amount money transact data provid competit structur real data avail solv problem data anonimyz row contain 200 numer valu identifi number b b inspir jiwei liu kernel ad data augment segment kernel b pre id 0 b content b href 1 b import data b href 11 b data explor b href 2 b check miss valu b href 3 b visual satendar custom transact data b href 31 b check class imbal b href 32 b distribut mean standard deviat b href 33 b distribut skew b href 34 b distribut kurtosi b href 4 b princip compon analysi b href 41 b kernel pca b href 16 b data augment b href 6 b build light gbm model b pre
744,['<a id=1><pre><b>Import the Data</b></pre></a>'],id 1 pre b import data b pre
745,['<a id=11><pre><b>Data Exploration</b></pre></a>'],id 11 pre b data explor b pre
746,['<a id=2><b><pre>Check for the Missing Values.</pre></b></a> '],id 2 b pre check miss valu pre b
747,['<pre>There are no missing values in the dataset</pre>'],pre miss valu dataset pre
748,['<pre><a id = 3><b>Visualizing the Satendar Customer Transactions Data</b></a></pre>'],pre id 3 b visual satendar custom transact data b pre
749,['<pre><a id = 31 ><b>Check for Class Imbalance</b></a></pre>'],pre id 31 b check class imbal b pre
750,"['<pre><a id = 32 ><b>Distribution of Mean and Standard Deviation</b></a></pre>\n', '\n', '<pre>EDA Reference : https://www.kaggle.com/gpreda/santander-eda-and-prediction</pre>']",pre id 32 b distribut mean standard deviat b pre pre eda refer http www kaggl com gpreda santand eda predict pre
751,"[""<pre>Let's check the distribution of the mean of values per columns in the train and test datasets.</pre>""]",pre let check distribut mean valu per column train test dataset pre
752,['<pre>Distribution for Standard Deviation</pre>'],pre distribut standard deviat pre
753,"[""<pre>Let's check the distribution of the standard deviation of values per columns in the train and test datasets.</pre>""]",pre let check distribut standard deviat valu per column train test dataset pre
754,"[""<pre>Let's check now the distribution of the mean value per row in the train dataset, grouped by value of target</pre>""]",pre let check distribut mean valu per row train dataset group valu target pre
755,"[""<pre>Let's check now the distribution of the mean values per columns in the train and test datasets.</pre>""]",pre let check distribut mean valu per column train test dataset pre
756,"[""<pre>Let's check now the distribution of the standard deviation  per row in the train dataset, grouped by value of target</pre>""]",pre let check distribut standard deviat per row train dataset group valu target pre
757,"[""<pre>Let's check now the distribution of standard deviation per columns in the train and test datasets.</pre>""]",pre let check distribut standard deviat per column train test dataset pre
758,"['<pre><a id = 33 ><b>Distribution of Skewness</b></a></pre>\n', '\n', ""<pre>Let's see now the distribution of skewness on rows in train separated for values of target 0 and 1. We found the distribution is left skewed</pre>""]",pre id 33 b distribut skew b pre pre let see distribut skew row train separ valu target 0 1 found distribut left skew pre
759,"[""<pre>Let's see now the distribution of skewness on columns in train separated for values of target 0 and 1.</pre>""]",pre let see distribut skew column train separ valu target 0 1 pre
760,['<pre><a id = 34 ><b>Distribution of Kurtosis</b></a></pre>'],pre id 34 b distribut kurtosi b pre
761,"[""<pre>Let's see now the distribution of kurtosis on rows in train separated for values of target 0 and 1. We found the distribution to be Leptokurtic</pre>""]",pre let see distribut kurtosi row train separ valu target 0 1 found distribut leptokurt pre
762,"[""<pre>Let's see now the distribution of kurtosis on columns in train separated for values of target 0 and 1.</pre>""]",pre let see distribut kurtosi column train separ valu target 0 1 pre
763,['<a id=4><pre><b>Principal Component Analysis to check Dimentionality Reduction<b></pre></a>'],id 4 pre b princip compon analysi check dimention reduct b pre
764,"[""<pre><a id = 41><b>Kernel PCA (Since the Graph above doesn't represent meaningful analysis)</b></a></pre>""]",pre id 41 b kernel pca sinc graph repres meaning analysi b pre
765,"[""<pre>Since PCA hasn't been useful, I decided to proceed with the existing dataset</pre>""]",pre sinc pca use decid proceed exist dataset pre
766,['<pre><a id = 16><b>Data Augmentation</b></a></pre>'],pre id 16 b data augment b pre
767,['<pre><a id = 6><b>Build the Light GBM Model</b></a></pre>'],pre id 6 b build light gbm model b pre
768,"['In this Module I have stacked the Validation and Submission outputs using KFold Cross Validation technique and Stratified K-Fold Cross validatiom technique. Referring to the my previous kernel\n', '\n', '**Stratified K Folds on Santander**\n', 'https://www.kaggle.com/roydatascience/eda-pca-simple-lgbm-santander-transactions\n', '\n', '**K Folds on Santander**\n', 'https://www.kaggle.com/roydatascience/fork-of-eda-pca-simple-lgbm-kfold\n', '\n', 'The attempt is to improve the accuracy using Baysian Ridge Stacking approach']",modul stack valid submiss output use kfold cross valid techniqu stratifi k fold cross validatiom techniqu refer previou kernel stratifi k fold santand http www kaggl com roydatasci eda pca simpl lgbm santand transact k fold santand http www kaggl com roydatasci fork eda pca simpl lgbm kfold attempt improv accuraci use baysian ridg stack approach
769,['## Stats Feature'],stat featur
770,"['## Data Augmentation\n', '\n', 'Thanks to @Jiwei Liu Kernel\n', 'https://www.kaggle.com/jiweiliu/lgb-2-leaves-augment/output\n']",data augment thank jiwei liu kernel http www kaggl com jiweiliu lgb 2 leav augment output
771,['### Feature engineering ----- Continued'],featur engin continu
772,['Not really a GP competetion so just having fun'],realli gp competet fun
773,['Look at the diagonals'],look diagon
774,"['# Are vars mixed up time intervals? Lets sort it out!\n', '\n', 'Lately, I was gazing at the beatiful graphs created by Chris Deotte here:\n', 'https://www.kaggle.com/cdeotte/modified-naive-bayes-santander-0-899/notebook\n', '\n', '..and noticed that they look like mixed up cartoon frames. They have similar pattern, proportions, but they are just mixed up, on different scale and sometimes flipped. So I have cleaned it up; here is the result. ']",var mix time interv let sort late gaze beati graph creat chri deott http www kaggl com cdeott modifi naiv bay santand 0 899 notebook notic look like mix cartoon frame similar pattern proport mix differ scale sometim flip clean result
775,"['# The list of columns that have to be reversed\n', ""If you take a look at the orginal graphs by Chris Deotte, notice how similar the probability graphs are. But some are directed to the right, while others are directed to the left. We will flip the graphs with the higest probabilty on the right. And we'll bring everything to one scale to make similarities more pronounced.""]",list column revers take look orgin graph chri deott notic similar probabl graph direct right other direct left flip graph higest probabilti right bring everyth one scale make similar pronounc
776,['# Load data'],load data
777,"['# Scale and flip\n', 'I doubt that standard scaler is the correct way to restore the original scale for the frames. But alternatives looked even worse.']",scale flip doubt standard scaler correct way restor origin scale frame altern look even wors
778,"['# Statistical Functions\n', 'Below are functions to calcuate various statistical things.']",statist function function calcuat variou statist thing
779,"['# The frames: Display Target Density and Target Probability\n', 'As described by Chris: ""Below are two plots for each of the 200 variables. The first is the density of `target=1` versus `target=0`. The second gives the probability that `target=1` given different values for `var_k`."" \n', '\n', 'Note how the shape and the range of graphs are consistnent. All we need to do is to sort them in the right order. Does not it remind you of the last Santander? I am pretty sure the correct order can be restored.']",frame display target densiti target probabl describ chri two plot 200 variabl first densiti target 1 versu target 0 second give probabl target 1 given differ valu var k note shape rang graph consistn need sort right order remind last santand pretti sure correct order restor
780,"['# Afterthoughts\n', ""These does not look random. I have not tried to go beyond this, Yet, if it works, don't forget to say thanks to Chris Deotte.""]",afterthought look random tri go beyond yet work forget say thank chri deott
781,"['\n', '# Subtracting normality\n', '\n', 'Lately, I was gazing at the beatiful graphs created by Chris Deotte once again:\n', 'https://www.kaggle.com/cdeotte/modified-naive-bayes-santander-0-899/notebook\n', '\n', '...thinking that they have told us to look for weird ubnormal stuff. To do so we have to substract normality, so here we go.']",subtract normal late gaze beati graph creat chri deott http www kaggl com cdeott modifi naiv bay santand 0 899 notebook think told u look weird ubnorm stuff substract normal go
782,"[""# What happens when you're normal\n"", '\n', 'Lets generate two normal distributions and subtract one from the other:']",happen normal let gener two normal distribut subtract one
783,"['Two things to note:\n', '1. There is single peak at zero.\n', '2. Substracting one sorted array from another is probably not the best way to ""substract normality"".\n', '\n', 'It maybe Ok for exploration, but for the actual data there should be a cleaner way to do this.']",two thing note 1 singl peak zero 2 substract one sort array anoth probabl best way substract normal mayb ok explor actual data cleaner way
784,"['# A conspiracy theory\n', 'What if I would want to hide that my data is categorical? I could add gaussian noise to category values.']",conspiraci theori would want hide data categor could add gaussian nois categori valu
785,['The shapes look familiar.'],shape look familiar
786,['As you see substracting sorted array is not the best way to restore original categories. But categorical data with added noise looks different after the trick. Lets try doing it to our data.'],see substract sort array best way restor origin categori categor data ad nois look differ trick let tri data
787,['# Load our data'],load data
788,['# Substracting normality?'],substract normal
789,"['# Statistical Functions\n', 'Below are functions to calcuate various statistical things.']",statist function function calcuat variou statist thing
790,"['# Categories?: Display Target Density and Target Probability\n', 'As described by Chris: ""Below are two plots for each of the 200 variables. The first is the density of `target=1` versus `target=0`. The second gives the probability that `target=1` given different values for `var_k`."" \n', '\n', 'Note how the shape and the range of graphs are consistnent. All we need to do is to sort them in the right order. Does not it remind you of the last Santander? I am pretty sure the correct order can be restored.']",categori display target densiti target probabl describ chri two plot 200 variabl first densiti target 1 versu target 0 second give probabl target 1 given differ valu var k note shape rang graph consistn need sort right order remind last santand pretti sure correct order restor
791,"['# Afterthoughts\n', 'I have not tried to go beyond this, so I am not even sure that this works. But I will certainly try. All comments are welcome. So if you have reasons to think this is bollocks, please say so.']",afterthought tri go beyond even sure work certainli tri comment welcom reason think bollock plea say
792,['### Import training data'],import train data
793,"['### Define peak finding function on histogram of a series\n', '\n', 'This function uses the `find_peaks` function from `scipy.signal` which takes various parameters describing constraints on the peaks that you wish to find. I have specified values for `prominence` and `width` to try and highlight the spikes in the density rather than just the peak of the distribution.']",defin peak find function histogram seri function use find peak function scipi signal take variou paramet describ constraint peak wish find specifi valu promin width tri highlight spike densiti rather peak distribut
794,['#### Get a list of peaks for the series that have them'],get list peak seri
795,['![](https://github.com/fmfn/BayesianOptimization/blob/master/examples/func.png?raw=true)'],http github com fmfn bayesianoptim blob master exampl func png raw true
796,"['# **Introduction**\n', '<br>\n', 'Hi guys! <br>\n', 'Have you guys experienced some frustrating moments when you tuned hyperparameters with Grid Search and Random Search?  <br>\n', 'Well...I have! I waited for two hours or more to run codes for both methods and ended up losing my focus by watching Youtube videos...\n', '<br>\n', '![](https://media.giphy.com/media/qjF9Akev3QPNC/giphy.gif)\n', '<br>\n', 'So, I thought that there must be a faster way to tune hyperparameters and did some research about the new method of tuning, which is called Baysian Optimization.<br>\n', 'If you have done Kaggle for a while or are an expert in this field, you probably have used or heard the Baysian Optimization. <br>\n', 'This kernel will give you a good idea of Baysian Optimization and the simple implementation of Baysian Optimization with BayesianOptimization, which I found that it is faster than other Baysian Optimization functions in Python (ex. Scikit-Optimize and Hyperopt) for my model.  <br> \n', '\n', '**The goal is to identify which customers will make a specific transaction in the future and maximize the evaluation function (AUC)**\n', '<br> <br>\n', '\n', '# **Special Thanks to:** <br> \n', '\n', ""I recommend you to see  [Fayzur's kernel](https://www.kaggle.com/fayzur/lgb-bayesian-parameters-finding-rank-average) and [sz8416's kernel](https://www.kaggle.com/sz8416/simple-bayesian-optimization-for-lightgbm) if you are interested in seeing what other people have done. Their kernels were very helpful to understand about the Baysian optimization process in Python. ""]",introduct br hi guy br guy experienc frustrat moment tune hyperparamet grid search random search br well wait two hour run code method end lose focu watch youtub video br http medium giphi com medium qjf9akev3qpnc giphi gif br thought must faster way tune hyperparamet research new method tune call baysian optim br done kaggl expert field probabl use heard baysian optim br kernel give good idea baysian optim simpl implement baysian optim bayesianoptim found faster baysian optim function python ex scikit optim hyperopt model br goal identifi custom make specif transact futur maxim evalu function auc br br special thank br recommend see fayzur kernel http www kaggl com fayzur lgb bayesian paramet find rank averag sz8416 kernel http www kaggl com sz8416 simpl bayesian optim lightgbm interest see peopl done kernel help understand baysian optim process python
797,"['# **Ready, Set, Go!** <br>\n', '\n', 'Before starting the kernel, I guarantee that tuning hyperparameter process will not take more than 10 minutes. And the whole process (loading dataset, tuning the hyperparameter, and training LightGBM) will not take more than 20 minutes.  **Time is important!!** <br>\n', '![](https://media.giphy.com/media/3oz8xKaR836UJOYeOc/giphy.gif)\n']",readi set go br start kernel guarante tune hyperparamet process take 10 minut whole process load dataset tune hyperparamet train lightgbm take 20 minut time import br http medium giphi com medium 3oz8xkar836ujoyeoc giphi gif
798,"['<br>\n', '# ** CONTENTS**\n', '\n', '1. [What Is Bayesian Optimization and Why Do We Care?](#1)\n', '2. [Loading Library and Dataset](#2)\n', '3. [Bayesian Optimization with LightGBM](#3)\n', '4. [Training LightGBM](#4)\n', '5. [Understanding the Model Better](#5)\n']",br content 1 bayesian optim care 1 2 load librari dataset 2 3 bayesian optim lightgbm 3 4 train lightgbm 4 5 understand model better 5
799,"['<a id=""1""></a> <br>\n', '\n', '# **What Is Bayesian Optimization and Why Do We Care?**']",id 1 br bayesian optim care
800,"['**Bayesian Optimization** is a probabilistic model based approach for finding the minimum of any function that returns a real-value metric. [(source)](https://towardsdatascience.com/an-introductory-example-of-bayesian-optimization-in-python-with-hyperopt-aae40fff4ff0)<br> It is very effective with real-world applications in high-dimensional parameter-tuning for complex machine learning algorithms. Bayesian optimization utilizes the Bayesian technique of setting a prior over the objective function and\n', 'combining it with evidence to get a posterior function. I attached one graph that demonstrates Bayes’ theorem below. <br> <br>\n', '\n', '<img src=""https://www.analyticsvidhya.com/wp-content/uploads/2016/06/12-768x475.jpg""  alt=""Drawing"" style=""width: 600px;""/>\n', '<br> <br> \n', ' The prior belief is our belief in parameters before modeling process. The posterior belief is our belief in our parameters after observing the evidence.\n', '<br> Another way to present the Bayes’ theorem is: \n', '\n', '<img src=""https://www.maths.ox.ac.uk/system/files/attachments/Bayes_0.png""  alt=""Drawing"" style=""width: 600px;""/> <br> \n', '\n', 'For continuous functions, Bayesian optimization typically works by assuming the unknown function was sampled from a Gaussian process and maintains a posterior distribution for this function as observations are made, which means that we need to give range of values of hyperparameters (ex. learning rate range from 0.1 to 1).  So, in our case, the Gaussian process gives us a prior distribution on functions. Gaussian process approach is a non-parametric approach, in that it finds a distribution over the possible functions \n', 'f(x) that are consistent with the observed data. Gaussian processes have proven to be useful surrogate models for computer experiments and good\n', 'practices have been established in this context for sensitivity analysis, calibration and prediction While these strategies are not considered in the context of optimization, they can be useful to researchers in machine learning who wish to understand better the sensitivity of their models to various hyperparameters. [(source)](http://papers.nips.cc/paper/4522-practical-bayesian-optimization-of-machine-learning-algorithms.pdf)\n', '\n', '<br>\n', '**Well..why do we care about it?** <br> \n', 'According to the [study](http://proceedings.mlr.press/v28/bergstra13.pdf), hyperparameter tuning by Bayesian Optimization of machine learnin models is more efficient than Grid Search and Random Search. Bayesian Optimization has better overall performance on the test data and takes less time for optimization. Also, we do not need to set a certain values of parameters like we do in Random Search and Grid Search. For Bayesian Optimization tuning, we just give a range of a hyperparameter. \n', '\n']",bayesian optim probabilist model base approach find minimum function return real valu metric sourc http towardsdatasci com introductori exampl bayesian optim python hyperopt aae40fff4ff0 br effect real world applic high dimension paramet tune complex machin learn algorithm bayesian optim util bayesian techniqu set prior object function combin evid get posterior function attach one graph demonstr bay theorem br br img src http www analyticsvidhya com wp content upload 2016 06 12 768x475 jpg alt draw style width 600px br br prior belief belief paramet model process posterior belief belief paramet observ evid br anoth way present bay theorem img src http www math ox ac uk system file attach bay 0 png alt draw style width 600px br continu function bayesian optim typic work assum unknown function sampl gaussian process maintain posterior distribut function observ made mean need give rang valu hyperparamet ex learn rate rang 0 1 1 case gaussian process give u prior distribut function gaussian process approach non parametr approach find distribut possibl function f x consist observ data gaussian process proven use surrog model comput experi good practic establish context sensit analysi calibr predict strategi consid context optim use research machin learn wish understand better sensit model variou hyperparamet sourc http paper nip cc paper 4522 practic bayesian optim machin learn algorithm pdf br well care br accord studi http proceed mlr press v28 bergstra13 pdf hyperparamet tune bayesian optim machin learnin model effici grid search random search bayesian optim better overal perform test data take le time optim also need set certain valu paramet like random search grid search bayesian optim tune give rang hyperparamet
801,"['<a id=""2""></a> <br>\n', '# **Loading Library and Dataset**\n']",id 2 br load librari dataset
802,"[""By Changing the data type of each column, I reduced memory usages by 75%. By taking the minimum and the maximum of each column, the function assigns which numeric data type is optimal for the column and change the data type. If you want to know more about how it works, I suggest you to read [Eryk's article](https://towardsdatascience.com/make-working-with-large-dataframes-easier-at-least-for-your-memory-6f52b5f4b5c4)! ""]",chang data type column reduc memori usag 75 take minimum maximum column function assign numer data type optim column chang data type want know work suggest read eryk articl http towardsdatasci com make work larg datafram easier least memori 6f52b5f4b5c4
803,"['<a id=""3""></a> <br>\n', '# **Bayesian Optimization with LightGBM**\n', '<br>\n', 'Now I am going to prepare data for modeling and a Baysian Optimization function. You can put more parameters (ex. lambda_l1 and lambda_l2) into the function.  ']",id 3 br bayesian optim lightgbm br go prepar data model baysian optim function put paramet ex lambda l1 lambda l2 function
804,['Here is my optimal parameter for LightGBM. '],optim paramet lightgbm
805,"['<a id=""4""></a> <br>\n', '# **Training LightGBM** <br> <br>\n', '\n', 'Based on the parameter from the previous step, I am going to train LightGBM. ']",id 4 br train lightgbm br br base paramet previou step go train lightgbm
806,"['<a id=""5""></a> <br>\n', '# **Understanding the Model Better**\n', '<br>\n', '\n', 'To get an overview of which features are most important for a model, we can plot the SHAP values of every feature for every sample. The plot below sorts features by the sum of SHAP value magnitudes over all samples, and uses SHAP values to show the distribution of the impacts each feature has on the model output. [(source)](https://github.com/slundberg/shap) The color represents the feature value (red high, blue low). This reveals for example that a high var_139 lowers the probability of being a customer who will make a specific transaction in the future. ']",id 5 br understand model better br get overview featur import model plot shap valu everi featur everi sampl plot sort featur sum shap valu magnitud sampl use shap valu show distribut impact featur model output sourc http github com slundberg shap color repres featur valu red high blue low reveal exampl high var 139 lower probabl custom make specif transact futur
807,['We can also plot a tree from the model and see each tree! '],also plot tree model see tree
808,"['# **Work in Progress..** <br><br>\n', '\n', 'Although I feel confident that I can implement Bayesian Optimization with LightGBM by Python and understand what my Python code does, I do not feel so comfortable with math behind it...😥  I will keep working on researching more about math behind Bayesian Optimization and share with you! <br><br>\n', 'Here are some academic papers about Bayesian Optimization just in case you are interested in:<br>\n', '1) http://papers.nips.cc/paper/4522-practical-bayesian-optimization-of-machine-learning-algorithms.pdf<br>\n', '2) https://arxiv.org/pdf/1012.2599v1.pdf\n', '\n', '<br>\n', ""<center>**I hope you guys enjoyed my kernel and do not forget to upvote if you think that it's helpful!**</center> <br>\n"", '\n', '![](https://media.giphy.com/media/osjgQPWRx3cac/giphy.gif)\n']",work progress br br although feel confid implement bayesian optim lightgbm python understand python code feel comfort math behind keep work research math behind bayesian optim share br br academ paper bayesian optim case interest br 1 http paper nip cc paper 4522 practic bayesian optim machin learn algorithm pdf br 2 http arxiv org pdf 1012 2599v1 pdf br center hope guy enjoy kernel forget upvot think help center br http medium giphi com medium osjgqpwrx3cac giphi gif
809,"['**Data Augment**\n', 'Augmentation is a method to increase the amount of training data by randomly shuffle/transform the features in a certain way. It improves accuracy by letting the model see more cases of both ""1"" and ""0"" samples in training so the model can generalize better to new data.\n', '\n', 'Thanks to Jiwei Lu for teaching this new concept . *https://www.kaggle.com/jiweiliu/lgb-2-leaves-augment*']",data augment augment method increas amount train data randomli shuffl transform featur certain way improv accuraci let model see case 1 0 sampl train model gener better new data thank jiwei lu teach new concept http www kaggl com jiweiliu lgb 2 leav augment
810,"['## Pytorch to implement simple feed-forward NN model (0.89+)\n', '\n', '* As below discussion, NN model can get lB 0.89+\n', '* https://www.kaggle.com/c/santander-customer-transaction-prediction/discussion/82499#latest-483679\n', '* Add Cycling learning rate , K-fold cross validation (0.85 to 0.86)\n', '* Add flatten layer as below discussion (0.86 to 0.897)\n', '* https://www.kaggle.com/c/santander-customer-transaction-prediction/discussion/82863\n', '\n', '## LightGBM (LB 0.899)\n', '\n', '* Fine tune parameters (0.898 to 0.899)\n', '* Reference this kernel : https://www.kaggle.com/chocozzz/santander-lightgbm-baseline-lb-0-899\n', '\n', '\n', '## Plan to do\n', '* Modify model structure on NN model\n', '* Focal loss\n', '* Feature engineering\n', '* Tune parameters oof LightGBM']",pytorch implement simpl feed forward nn model 0 89 discus nn model get lb 0 89 http www kaggl com c santand custom transact predict discus 82499 latest 483679 add cycl learn rate k fold cross valid 0 85 0 86 add flatten layer discus 0 86 0 897 http www kaggl com c santand custom transact predict discus 82863 lightgbm lb 0 899 fine tune paramet 0 898 0 899 refer kernel http www kaggl com chocozzz santand lightgbm baselin lb 0 899 plan modifi model structur nn model focal loss featur engin tune paramet oof lightgbm
811,['## Load Data'],load data
812,['## Split K- fold validation'],split k fold valid
813,"['## Cycling learning rate\n', '\n', '*copy from ==> https://github.com/anandsaha/pytorch.cyclic.learning.rate/blob/master/cls.py']",cycl learn rate copi http github com anandsaha pytorch cyclic learn rate blob master cl py
814,"['## Build Simple NN model (Pytorch)\n', '\n', '* add flatten layer before fc layer (improve to 0.89+)\n', '* https://www.kaggle.com/c/santander-customer-transaction-prediction/discussion/82863\n', '\n', '* Model structure\n', '* (batch_size, 200) ==> Flatten ==> (batch_size* 200,1) ==> fc1 ==> (batch_size* 200, hidden_layer) ==>Reshape ==>(batch_size, hidden_layer * 200) ==> fc2 ==> (batch_size, 1)']",build simpl nn model pytorch add flatten layer fc layer improv 0 89 http www kaggl com c santand custom transact predict discus 82863 model structur batch size 200 flatten batch size 200 1 fc1 batch size 200 hidden layer reshap batch size hidden layer 200 fc2 batch size 1
815,"['## Start training\n', '* Epoch = 40\n', '* Batch size = 256\n', '* Cycling step = 150']",start train epoch 40 batch size 256 cycl step 150
816,"['## LightGBM Model\n', '* reference this kernel : https://www.kaggle.com/chocozzz/santander-lightgbm-baseline-lb-0-899 ']",lightgbm model refer kernel http www kaggl com chocozzz santand lightgbm baselin lb 0 899
817,['## LGBM training'],lgbm train
819,"['## Ensemble two model (NN+ LGBM)\n', ""* NN model accuracy is too low, ensemble looks don't work.""]",ensembl two model nn lgbm nn model accuraci low ensembl look work
820,['## Create submit file'],creat submit file
821,"['* This notebook was part of UpGrad Kagglethon, initiative to help their cohort getting started with Kaggle competitions. To be compliant with rules, I am sharing everything that was discussed during those sessions. ']",notebook part upgrad kagglethon initi help cohort get start kaggl competit compliant rule share everyth discus session
822,"['** Why do Kaggle**\n', '\n', '* Learning new things\n', '* strenghtnen intuition for ml algorithms and techniques\n', '* like competing with fellow kagglers']",kaggl learn new thing strenghtnen intuit ml algorithm techniqu like compet fellow kaggler
823,"['** Problem statement **\n', 'https://www.kaggle.com/c/santander-customer-transaction-prediction\n', '\n']",problem statement http www kaggl com c santand custom transact predict
824,"['## EDA\n', '\n', '### Pointers\n', '* Check out existing kernels\n', 'https://www.kaggle.com/gpreda/santander-eda-and-prediction\n', 'https://www.kaggle.com/artgor/santander-eda-fe-fs-and-models\n', 'https://www.kaggle.com/mjbahmani/santander-ml-explainability\n', '\n', '* Check distributions\n', '* Compare train and test distributions\n', '* Identify important features (Most of the times feature engineering is going to be around features with high predictive power)\n', '* Attach a logic to why featurea are important ( Note: data is anonymised  here so hard to do this)\n', '* Check previous solutions to similar problems\n', '\n', '\n', '### Observations\n', '* Data normalization and imputation\n', '* Weak corelations between features and target\n', '* IV values ??\n', '* Most variables have distribution close to normal\n', '* Almost no corelation between differnt variable - What does it mean ??\n', '* No NA values (already imputed??)\n', '* Some features seem to have been clipped at one end\n', '* Spikes in distributions (imputed values??)\n', '* less unique ']",eda pointer check exist kernel http www kaggl com gpreda santand eda predict http www kaggl com artgor santand eda fe f model http www kaggl com mjbahmani santand ml explain check distribut compar train test distribut identifi import featur time featur engin go around featur high predict power attach logic featurea import note data anonymis hard check previou solut similar problem observ data normal imput weak corel featur target iv valu variabl distribut close normal almost corel differnt variabl mean na valu alreadi imput featur seem clip one end spike distribut imput valu le uniqu
825,"['### Method -1 : train on full and predict on test\n', ' - rule  - scale boosting rounds by train data ratio to data during validation - 1500 ']",method 1 train full predict test rule scale boost round train data ratio data valid 1500
826,['### Method 2 - use validation fold models to predict on test set\n'],method 2 use valid fold model predict test set
827,"['### Modelling\n', '\n', 'Pointers:\n', '*  Validation strategy -- Random KFold, holdout or temporal split ??\n', '* What to trust validation score or LB socre?? trust score from more data; if test data is more we should treat LB as additional fold\n', '* Hyperparamter tuning -- Combination of manual tuning and bayesian optimization libraries like `hyperopt` and `scikit-optimize`. Initial tuninng on single fold and then move to 5 folds.\n', '* Always check validation and test set prediction distributions\n', '* ** Read forums and participate in discussions **\n', '\n', 'Disussions:\n', '* Sometimes using geometric mean of probabilities is better than using simple mean\n', '* When metric is ROC_AUC, even rank average can be used\n', '* Blending -- blend of your solution and public solution can be used to improve LB score. But, better approach is to understand what is working for other people and integrate in your models.\n']",model pointer valid strategi random kfold holdout tempor split trust valid score lb socr trust score data test data treat lb addit fold hyperparamt tune combin manual tune bayesian optim librari like hyperopt scikit optim initi tuninng singl fold move 5 fold alway check valid test set predict distribut read forum particip discus disuss sometim use geometr mean probabl better use simpl mean metric roc auc even rank averag use blend blend solut public solut use improv lb score better approach understand work peopl integr model
828,"['# Next steps:\n', '\n', '* Feature engineering - interactions, bucketing etc\n', '* try other algorithms -- catboost, xgboost, RGF (regularized greedy forest), different NN architecture\n', '* weighted average\n', '* add more public solutions to blend\n', '* submit and keep making progress\n', '* maintain a list of ideas to be executed, you should never run out of things to do\n', '\n', '### ** Happy Kaggling and thank you :) **\n', '\n', 'Meanwhile something to inspire you from one of the greats:  https://www.youtube.com/watch?v=7XEMPU17-Wo']",next step featur engin interact bucket etc tri algorithm catboost xgboost rgf regular greedi forest differ nn architectur weight averag add public solut blend submit keep make progress maintain list idea execut never run thing happi kaggl thank meanwhil someth inspir one great http www youtub com watch v 7xempu17 wo
829,"[""Even though the local-leaderboard CV congruence is pretty good, it's far from perfect. Most of the discrepancy seems to be con""]",even though local leaderboard cv congruenc pretti good far perfect discrep seem con
831,"[""Let's take a look at the overall oof CV AUC:""]",let take look overal oof cv auc
832,"['AUC of 0.53 is overall not a major issue, but it **is** slightly over 0.50, which can have an impact when you are chasing the 4th decimal point.\n']",auc 0 53 overal major issu slightli 0 50 impact chase 4th decim point
833,"['[H2O AutoML](http://docs.h2o.ai/h2o/latest-stable/h2o-docs/automl.html) is an automated machine learning meta-algorithm that is part of the [H2O software library](http://docs.h2o.ai/h2o/latest-stable/h2o-py/docs/intro.html#what-is-h2o). (It shold not be confused with [H2O DriverlessAI](https://www.h2o.ai/products/h2o-driverless-ai/), which is a commercial product and built from an entirely different code base.) H2O’s AutoML can be used for automating the machine learning workflow, which includes automatic training and tuning of many models within a user-specified time-limit. Stacked Ensembles – one based on all previously trained models, another one on the best model of each family – will be automatically trained on collections of individual models to produce highly predictive ensemble models which, in most cases, will be the top performing models in the AutoML Leaderboard.']",h2o automl http doc h2o ai h2o latest stabl h2o doc automl html autom machin learn meta algorithm part h2o softwar librari http doc h2o ai h2o latest stabl h2o py doc intro html h2o shold confus h2o driverlessai http www h2o ai product h2o driverless ai commerci product built entir differ code base h2o automl use autom machin learn workflow includ automat train tune mani model within user specifi time limit stack ensembl one base previous train model anoth one best model famili automat train collect individu model produc highli predict ensembl model case top perform model automl leaderboard
834,"['# GPU-accelerated LightGBM\n', '\n', 'This kernel explores a GPU-accelerated LGBM model to predict customer transaction.\n', '\n', '## Notebook  Content\n', '1. [Re-compile LGBM with GPU support](#1)\n', '1. [Loading the data](#2)\n', '1. [Training the model on CPU](#3)\n', '1. [Training the model on GPU](#4)\n', '1. [Submission](#5)']",gpu acceler lightgbm kernel explor gpu acceler lgbm model predict custom transact notebook content 1 compil lgbm gpu support 1 1 load data 2 1 train model cpu 3 1 train model gpu 4 1 submiss 5
835,"['<a id=""1""></a> \n', '## 1. Re-compile LGBM with GPU support\n', 'In Kaggle notebook setting, set the `Internet` option to `Internet connected`, and `GPU` to `GPU on`. \n', '\n', 'We first remove the existing CPU-only lightGBM library and clone the latest github repo.']",id 1 1 compil lgbm gpu support kaggl notebook set set internet option internet connect gpu gpu first remov exist cpu lightgbm librari clone latest github repo
836,"['Next, the Boost development library must be installed.']",next boost develop librari must instal
837,['The next step is to build and re-install lightGBM with GPU support.'],next step build instal lightgbm gpu support
838,"['Last, carry out some post processing tricks for OpenCL to work properly, and clean up.']",last carri post process trick opencl work properli clean
839,"['<a id=""2""></a> \n', '## 2. Loading the data']",id 2 2 load data
840,"['<a id=""3""></a>\n', '## 3. Training the model on CPU']",id 3 3 train model cpu
841,"['<a id=""4""></a>\n', '## 4. Train model on GPU']",id 4 4 train model gpu
842,"['First, check the GPU availability.']",first check gpu avail
843,"['In order to leverage the GPU, we need to set the following parameters: \n', '\n', ""        'device': 'gpu',\n"", ""        'gpu_platform_id': 0,\n"", ""        'gpu_device_id': 0\n"", '        \n', '        ']",order leverag gpu need set follow paramet devic gpu gpu platform id 0 gpu devic id 0
844,"['<a id=""5""></a>\n', '## 5. Submission']",id 5 5 submiss
845,"['I have recently checked the following tweet:\n', '\n', '<blockquote class=""twitter-tweet"" data-lang=""fr""><p lang=""en"" dir=""ltr"">stop plotting histograms.</p>&mdash; Hugo Bowne-Anderson (@hugobowne) <a href=""https://twitter.com/hugobowne/status/1111657955248783366?ref_src=twsrc%5Etfw"">29 mars 2019</a></blockquote>\n', '<script async src=""https://platform.twitter.com/widgets.js"" charset=""utf-8""></script>\n', '\n', '\n', '\n', 'Indeed, an [ECDF](https://en.wikipedia.org/wiki/Empirical_distribution_function) \n', 'is often easier to explore and think about. Here is a [**blog post**](https://ericmjl.github.io/blog/2018/7/14/ecdfs/) explaining some of the logic behind this claim.   \n', '\n', ""Let's see how it translates to this competition's dataset!""]",recent check follow tweet blockquot class twitter tweet data lang fr p lang en dir ltr stop plot histogram p mdash hugo bown anderson hugobown href http twitter com hugobown statu 1111657955248783366 ref src twsrc 5etfw 29 mar 2019 blockquot script async src http platform twitter com widget j charset utf 8 script inde ecdf http en wikipedia org wiki empir distribut function often easier explor think blog post http ericmjl github io blog 2018 7 14 ecdf explain logic behind claim let see translat competit dataset
846,"['Based on the above plots, it appears that:\n', '\n', '* it is indeed easier to see how much two distriubtions differ by inspecting\n', 'the ecdfs.  \n', '* median values (and othe statistics) are easier to observe. \n', '\n', 'Something to try: plot the ECDF for a normal distribution having the same mean\n', 'and standard deviation and compare it with the ones plotted above. \n', '\n', 'If you have more suggestions, leave them in the comments section. \n', '\n', 'Thanks. :)']",base plot appear inde easier see much two distriubt differ inspect ecdf median valu oth statist easier observ someth tri plot ecdf normal distribut mean standard deviat compar one plot suggest leav comment section thank
847,"['Thanks to this discussion for the observation: https://www.kaggle.com/c/santander-customer-transaction-prediction/discussion/84450\n', '\n', 'In this notebook, I transform this column and re-order the train dataset using this column, and see what \n', 'happens.']",thank discus observ http www kaggl com c santand custom transact predict discus 84450 notebook transform column order train dataset use column see happen
848,['#\xa0Before you read'],xa0befor read
849,"['This exploration is an attempt to discover some hidden things behind the annonymization.\n', 'Nothing is certain of course and this is so far specualative. \n', 'Use this knowledge accordingly. ']",explor attempt discov hidden thing behind annonym noth certain cours far specual use knowledg accordingli
850,['#\xa0Preliminary work'],xa0preliminari work
851,"['Two things to observe: \n', '    \n', '- data has been annonymized\n', '- it comes from a business setting\n', '\n', 'Thus, it is most likely (but not 100% sure) that some of the features\n', ""contain date-like information (and also categorical features but that's \n"", 'for another day). \n', '\n', ""How to find potential columns? Let's try to sort the columns using the number of unique \n"", ""values. What's the heurestic behind this choice? \n"", ""Well there shouldn't be a lot of dates, maybe few thousand top.""]",two thing observ data annonym come busi set thu like 100 sure featur contain date like inform also categor featur anoth day find potenti column let tri sort column use number uniqu valu heurest behind choic well lot date mayb thousand top
852,"['==> `var_68` has the least number of uniques, thus it **might** be a date-like column\n', '(it could also be a categorical column).\n', 'There is also a possibility that this small number of uniques is a coincidence due to the rounding to 4 decimal numbers (bonus question: could you compute the probability of this event?)']",var 68 least number uniqu thu might date like column could also categor column also possibl small number uniqu coincid due round 4 decim number bonu question could comput probabl event
853,"['So how to extract a date?\n', 'Well, first, get ride of the decimal values.\n', 'Then transform to a datetime object supposing that it is an ordinal datetime.\n', 'Try different offsets until you get a meaningful date range.\n', ""That's it. Let's see this in action.""]",extract date well first get ride decim valu transform datetim object suppos ordin datetim tri differ offset get meaning date rang let see action
854,['#\xa0Some plots '],xa0som plot
855,['# Date column exploration'],date column explor
856,"[""Alright, let's now explore this newly created column.""]",alright let explor newli creat column
857,['=> I will thus use the `date` column to group rows. '],thu use date column group row
858,"[""==> Uniform day of week distribution. That's a good sign!""]",uniform day week distribut good sign
859,['#\xa0What about the test?'],xa0what test
860,"[""Let's see if our observation transfers well to the test dataset.""]",let see observ transfer well test dataset
861,['# Test and train date column comparaison'],test train date column comparaison
862,['==> Most of the dates overlap. '],date overlap
863,"['Idea to try: predict the mean of the target (using the date \n', 'for grouping) for the overlapping dates. ']",idea tri predict mean target use date group overlap date
865,"['Some of the things I will try to do: \n', '- Use this transformed column for a better temporal CV. Some ideas I have tried: stratification using years, day of weeks, and so on.\n', '- Transform other columns using this new one\n', '\n', 'Stay tuned for more insights. :)']",thing tri use transform column better tempor cv idea tri stratif use year day week transform column use new one stay tune insight
866,"['A short notebook where I explore the concept of permutation importance using \n', 'the [**ELI5 library**](https://eli5.readthedocs.io/en/latest/blackbox/permutation_importance.html). \n', '\n', 'Inspired from this great tutorial: https://www.kaggle.com/dansbecker/permutation-importance. \n', '\n', 'Enjoy!']",short notebook explor concept permut import use eli5 librari http eli5 readthedoc io en latest blackbox permut import html inspir great tutori http www kaggl com dansbeck permut import enjoy
867,['# 1. Read dataset'],1 read dataset
868,['## 1.1 Target  check'],1 1 target check
869,"['- This competiiton is Imbalanced target competition.\n', '- You can check similar competitions, Porto, Homecredit competition.\n', '- Specially, Porto also gave use anonymized dataset. \n', '- https://www.kaggle.com/c/home-credit-default-risk\n', '- https://www.kaggle.com/c/porto-seguro-safe-driver-prediction']",competiiton imbalanc target competit check similar competit porto homecredit competit special porto also gave use anonym dataset http www kaggl com c home credit default risk http www kaggl com c porto seguro safe driver predict
870,['## 1.2 Null data check'],1 2 null data check
871,"['- There is no missing values.\n', ""- Because we don't know the exact meaning of variables, we need to check some values as null value.""]",miss valu know exact mean variabl need check valu null valu
872,['# 2. Exploratory Data Analysis'],2 exploratori data analysi
873,"[""- Before EDA, let's group the features into category and non-category based on the number of uniqueness.""]",eda let group featur categori non categori base number uniqu
874,"['- Oh, Most features have more than thousands of values for each variable except var_68 (435)']",oh featur thousand valu variabl except var 68 435
875,"[""- Let's see var 68""]",let see var 68
876,"['- It also has float numbers. \n', '- Uncovering these values will be intersting job!\n', '- Multiplying and dividing with some values can make the hidden categories, See Radder work(https://www.kaggle.com/raddar/target-true-meaning-revealed)']",also float number uncov valu interst job multipli divid valu make hidden categori see radder work http www kaggl com raddar target true mean reveal
877,['# 2.1 Correlation'],2 1 correl
878,"['- The largest correlation value is 0.08\n', ""- Actually, the target is binary and variables are continous, so correlation is not enough to judge. Let's see the distribution!""]",largest correl valu 0 08 actual target binari variabl contin correl enough judg let see distribut
879,['# 2.2 Distribution regarding to target'],2 2 distribut regard target
880,['# TODO'],todo
881,"['- As you know, Santander hosted other competition 6 month before.\n', '- So you can check this competition. https://www.kaggle.com/c/santander-value-prediction-challenge']",know santand host competit 6 month check competit http www kaggl com c santand valu predict challeng
882,['- I will do time series analysis for this dataset.'],time seri analysi dataset
883,['## Load Data'],load data
884,['##   Data Exploration'],data explor
885,['## Data Preprocessing'],data preprocess
886,['## Variable Engineering'],variabl engin
887,['#### PCA'],pca
888,['#### Summary Stats'],summari stat
889,['## Feature importance'],featur import
890,['### Permutation Importance'],permut import
891,[' ## Model Development'],model develop
892,['### lightgbm'],lightgbm
893,['### Neural Net'],neural net
894,['## Submission Files'],submiss file
895,"[' <a id=""55""></a> <br>\n', '## Stacking']",id 55 br stack
896,"['# References & credits\n', 'Thanks fo following kernels that help me to create this kernel.']",refer credit thank fo follow kernel help creat kernel
897,"['1. [https://www.kaggle.com/mjbahmani/santander-ml-explainability](https://www.kaggle.com/mjbahmani/santander-ml-explainability)  \n', '1. [https://www.kaggle.com/super13579/pytorch-nn-cyclelr-k-fold-0-897-lightgbm-0-899](https://www.kaggle.com/super13579/pytorch-nn-cyclelr-k-fold-0-897-lightgbm-0-899)  \n', '1. [https://www.kaggle.com/dansbecker/permutation-importance](https://www.kaggle.com/dansbecker/permutation-importance)\n', '1. [https://www.kaggle.com/dansbecker/partial-plots](https://www.kaggle.com/dansbecker/partial-plots)\n', '1. [https://www.kaggle.com/miklgr500/catboost-with-gridsearch-cv](https://www.kaggle.com/miklgr500/catboost-with-gridsearch-cv)\n', '1. [https://www.kaggle.com/dromosys/sctp-working-lgb](https://www.kaggle.com/dromosys/sctp-working-lgb)\n', '1. [https://www.kaggle.com/gpreda/santander-eda-and-prediction](https://www.kaggle.com/gpreda/santander-eda-and-prediction)\n', '1. [permutation-importance](https://www.kaggle.com/dansbecker/permutation-importance)\n', '1. [partial-plots](https://www.kaggle.com/dansbecker/partial-plots)\n', '1. [https://www.kaggle.com/dansbecker/shap-values](https://www.kaggle.com/dansbecker/shap-values)\n', '1. [algorithm-choice](https://docs.microsoft.com/en-us/azure/machine-learning/studio/algorithm-choice)']",1 http www kaggl com mjbahmani santand ml explain http www kaggl com mjbahmani santand ml explain 1 http www kaggl com super13579 pytorch nn cyclelr k fold 0 897 lightgbm 0 899 http www kaggl com super13579 pytorch nn cyclelr k fold 0 897 lightgbm 0 899 1 http www kaggl com dansbeck permut import http www kaggl com dansbeck permut import 1 http www kaggl com dansbeck partial plot http www kaggl com dansbeck partial plot 1 http www kaggl com miklgr500 catboost gridsearch cv http www kaggl com miklgr500 catboost gridsearch cv 1 http www kaggl com dromosi sctp work lgb http www kaggl com dromosi sctp work lgb 1 http www kaggl com gpreda santand eda predict http www kaggl com gpreda santand eda predict 1 permut import http www kaggl com dansbeck permut import 1 partial plot http www kaggl com dansbeck partial plot 1 http www kaggl com dansbeck shap valu http www kaggl com dansbeck shap valu 1 algorithm choic http doc microsoft com en u azur machin learn studio algorithm choic
898,['# Not Completed yet!!!'],complet yet
899,['**1) Clean test from fake data**'],1 clean test fake data
900,['**2) Create Magic features**'],2 creat magic featur
901,['3) Read data with magic features and create additional useful features var_N **mul** magic_N and var_N **div** magic_N'],3 read data magic featur creat addit use featur var n mul magic n var n div magic n
902,"['4) Create model\n', '\n', '5) Use random shuffle of columns during training']",4 creat model 5 use random shuffl column train
903,['**6) Predict on test**'],6 predict test
904,"['![](http://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1513868561/output_65_0_knd6e9.png)']",http re cloudinari com dyd911kmh imag upload f auto q auto best v1513868561 output 65 0 knd6e9 png
905,"['Plots on top of plot on top of plots. It seems that most of the EDA these days is just throwing around fancy plots from fancy libraries. There is no real **insight** or **reusability** from those kinds of notebooks, just to fill in the space.\n', '\n', '\n', '# GOAL: This notebook should serve as a reusable template for **INSIGHTFUL** EDA when approaching a DS problem. \n', '\n', 'Ofcourse there is not a universal solution and it always needs to be modified but I feel like that outlining a couple of general ideas and principles will be usefull since they will repeat themselves.']",plot top plot top plot seem eda day throw around fanci plot fanci librari real insight reusabl kind notebook fill space goal notebook serv reusabl templat insight eda approach d problem ofcours univers solut alway need modifi feel like outlin coupl gener idea principl useful sinc repeat
906,['Dataset will be [Santander Customer Transaction Prediction](https://www.kaggle.com/c/santander-customer-transaction-prediction/data) where we have 200 columns of anonymised data. '],dataset santand custom transact predict http www kaggl com c santand custom transact predict data 200 column anonymis data
907,"['# What should good EDA be capable of?\n', '1. Verify expected relationships actually exist in the data, thus formulating and validating planned techniques of analysis.\n', '2. To find some unexpected structure in the data that must be taken into account, thereby suggesting some changes in the planned analysis.\n', '3. Deliver data-driven insights to business stakeholders by confirming they are asking the right questions and not biasing the investigation with their assumptions.\n', '4. Provide the context around the problem to make sure the potential value of the data scientist’s output can be maximized.']",good eda capabl 1 verifi expect relationship actual exist data thu formul valid plan techniqu analysi 2 find unexpect structur data must taken account therebi suggest chang plan analysi 3 deliv data driven insight busi stakehold confirm ask right question bias investig assumpt 4 provid context around problem make sure potenti valu data scientist output maxim
908,"['Main point is gathering and automising as much as possible. So we will plot all of the variables together (modifying the code for different problems), than ""zoom in"" in case of suspicion. Since there are a lot of indicators I only took some of them to speed up the computation. We can see distributions as well as plots in relationship with other variables.']",main point gather automis much possibl plot variabl togeth modifi code differ problem zoom case suspicion sinc lot indic took speed comput see distribut well plot relationship variabl
909,['Same for test set.'],test set
910,['We should also plot other variables in dependence to the dependent variable--**target**. Thats the first column so lets just take a couple of the first columns. (run-time!!!)'],also plot variabl depend depend variabl target that first column let take coupl first column run time
911,"['# Take-off notes from the first analysis:\n', '\n', '**Ofcourse this is individual but we can see unbalanced classes in the target variable, not much correlation (we will check it more subsequently!). Mostly normaly distribution among predictors, tough to distinguish which values are to be associated with 0 & 1 class etc...**\n', '\n', '\n', 'As already mentioned one ought to ""zoom in"" and take one special predictor and to subsequent analysis. Doing a 3-d plot with some special interest variables etc. So it really depends on the problem and the domain knowledge of the problem.']",take note first analysi ofcours individu see unbalanc class target variabl much correl check subsequ mostli normali distribut among predictor tough distinguish valu associ 0 1 class etc alreadi mention one ought zoom take one special predictor subsequ analysi 3 plot special interest variabl etc realli depend problem domain knowledg problem
912,['Let us also assume that no pre-processing will be done (often times) before EDA. We will use EDA to help us with that too.'],let u also assum pre process done often time eda use eda help u
913,"['Dealing with **missing values**:\n', '\n', 'This is a bit specific dataset with no missing values:']",deal miss valu bit specif dataset miss valu
914,"['BUT, this is a boiler plat code that can be re-used later on different projects!']",boiler plat code use later differ project
915,"['When **dealing with outliers** one should be careful and look also at the distribution of the variable at hand. For example let us say that we have a uniform distributed variable, does 2 points of std really say anything about a potential outlier? Best thing one could do is assume (or better yet test with kolmogorov smirnov test) a distribution of a variable. Than depending on the result just throw away values that are to be found far away on the distribution graph.']",deal outlier one care look also distribut variabl hand exampl let u say uniform distribut variabl 2 point std realli say anyth potenti outlier best thing one could assum better yet test kolmogorov smirnov test distribut variabl depend result throw away valu found far away distribut graph
916,"['Since distribution of independent variables is mostly normal, lets see what happens with outliers when measured with (different) points of standard deviation. (one can see it as z-score)']",sinc distribut independ variabl mostli normal let see happen outlier measur differ point standard deviat one see z score
917,"['Another way to look at the outliers but also in the same time get some more information about distribution (IQR, median, mean etc...) is with the box-plot. But we need to do it efficiently:\n']",anoth way look outlier also time get inform distribut iqr median mean etc box plot need effici
919,['**Correlation map**- after throwing the outliers and missing values away (since it is neccessary before calculating pearson correlation coefficient)'],correl map throw outlier miss valu away sinc neccessari calcul pearson correl coeffici
920,['As we noticed from the first plots (scattered ones) there is not really much correlation between the variables.'],notic first plot scatter one realli much correl variabl
921,['So what are some other insights that can be gathered using EDA about the data? One interesting thing is the distribution (density) plot of different predicators when in contrast to different classes (0 or 1).'],insight gather use eda data one interest thing distribut densiti plot differ predic contrast differ class 0 1
922,"['**Interesting observation** is its not always normal distribution, in some cases and classes we can observe almost bimodal distribution. **Implication?** Normality assumption is not met, be careful in model choices etc if we were to use these predicators.']",interest observ alway normal distribut case class observ almost bimod distribut implic normal assumpt met care model choic etc use predic
923,"['**Another** thing that should be important to us (to ensure could prediction power) is that **test and train sets are the same**, i.e. they come from the same sample and they represent the whole population. Lets plot it for first 50 variables.']",anoth thing import u ensur could predict power test train set e come sampl repres whole popul let plot first 50 variabl
924,"['**Additionally** we can speaak about skewness distriibution (here it is normal), additional exploration with some specific variables/domain specific knowledge, contrasting different scatter plots with some categorical variables (here we do not have classes other than the dependent variable), in case of text some word clouds, tf-idf distribution etc etc....\n', 'There are many options but I think these steps are essential no matter what the dataset at hand is. \n', 'Additional **(part 2) tutorial** can be made concerning purely textual data and good EDA there.']",addit speaak skew distriibut normal addit explor specif variabl domain specif knowledg contrast differ scatter plot categor variabl class depend variabl case text word cloud tf idf distribut etc etc mani option think step essenti matter dataset hand addit part 2 tutori made concern pure textual data good eda
934,"['## Assuming the start date to be 1-Dec-2017 as hypothesized in the ""TransactionDT startdate"" kernel ']",assum start date 1 dec 2017 hypothes transactiondt startdat kernel
935,['## Observed fraud rates each day'],observ fraud rate day
936,"['Fraud rates are lower in December with the lowest point around christmas day. The fraud rate from January onwards seems to hover ~4%. \n', '\n', 'The lower fraud rates in December could be due to higher number of genuine transactions in December. Lets test this by looking at the number of fraud transactions.']",fraud rate lower decemb lowest point around christma day fraud rate januari onward seem hover 4 lower fraud rate decemb could due higher number genuin transact decemb let test look number fraud transact
937,['## Number of fraud transactions each day '],number fraud transact day
938,"['Unlike the fraud rate, the rolling average seems pretty stable until May when we see a drop and possibly a shift in mean value?']",unlik fraud rate roll averag seem pretti stabl may see drop possibl shift mean valu
939,['## Checking stationarity '],check stationar
940,"[""Fraud rate doesn't pass the ADF test for stationarity as expected from looking at the graph""]",fraud rate pas adf test stationar expect look graph
941,['Number of fraudulent activities passes the ADF test for stationarity despite the drop we observed around May-2018'],number fraudul activ pas adf test stationar despit drop observ around may 2018
942,['## How do these insights help with my modeling?'],insight help model
943,"['1. If anyone is using undersampling/oversampling approaches to deal with the class imbalance. It could possibly be benefitial to consider the month/date of transactions, try balancing classes over each month rather than over the entire training set.  \n', '\n', '2. If the drop in number of fraudulent activies we observed in May-2018, does indicate a shift in mean value. Then the expected number of fraudlent activities in test set could be lower than what we observed in the train set. \n', 'A speculation but this could also possibly explain why models are getting higher LB AUC values than local CV AUC values.']",1 anyon use undersampl oversampl approach deal class imbal could possibl benefiti consid month date transact tri balanc class month rather entir train set 2 drop number fraudul activi observ may 2018 indic shift mean valu expect number fraudlent activ test set could lower observ train set specul could also possibl explain model get higher lb auc valu local cv auc valu
944,"['> # Introduction <br>\n', ""Hi everyone! In this kernel I'd like to share some ideas about NN's and loss functions. <br>\n"", 'Core points of this kernel: \n', ""* Preparing tabular data for NN's\n"", '* Handling skewed continuous features\n', '* Implementing custom loss function\n', '* Which models are good for ensembling']",introduct br hi everyon kernel like share idea nn loss function br core point kernel prepar tabular data nn handl skew continu featur implement custom loss function model good ensembl
945,"['# Data Loading and Feature Selection <br>\n', ""In this particular kernel we will use only features from transaction table, 'cause with NN after a brief investigation I didn't get any significant improvement by using all features.""]",data load featur select br particular kernel use featur transact tabl caus nn brief investig get signific improv use featur
946,"['# Data Processing\n', 'For continuous right-skewed features we wil apply log-transform, so that will make them look more like normal distributed.']",data process continu right skew featur wil appli log transform make look like normal distribut
947,"['For categorical features we will apply OneHot transformation, but only for most common values for each feature to reduce sparsity. <br>\n', 'Also there is an embedding approach for categorical features transformation. It was implemented in this kernel https://www.kaggle.com/ryches/keras-nn-starter-w-time-series-split <br>\n', ""With embedding approach I didn't get any significant improvement comparing to this.""]",categor featur appli onehot transform common valu featur reduc sparsiti br also embed approach categor featur transform implement kernel http www kaggl com rych kera nn starter w time seri split br embed approach get signific improv compar
948,"['# Validation\n', 'For validation I use time-based holdout. For these and other models it has a good correlation between val and lb.']",valid valid use time base holdout model good correl val lb
949,['# Modeling'],model
950,"[""So, we've got a pretty interesting results. The same models with almost the same validation scores give predictions with correlation ~80% by optimizing different loss functions, so that makes them good for ensembling.""]",got pretti interest result model almost valid score give predict correl 80 optim differ loss function make good ensembl
951,['# Fine-tuning and Predicting'],fine tune predict
952,"['# Final thoughts <br>\n', ""Obviously as a single model NN doesn't perform as well as Gradient Boosting and to addition NN requires more sophisticated data processing approaches. But I'm sure, that it will be good for stacking as one of the first-level models. <br>\n"", 'I also think, that using focal loss with gradient boosting can improve score.<br>\n', ""Maybe, It is a good idea to research different loss functions, 'cause the same model can give uncorrelated predicts by optimizing different loss functions.\n""]",final thought br obvious singl model nn perform well gradient boost addit nn requir sophist data process approach sure good stack one first level model br also think use focal loss gradient boost improv score br mayb good idea research differ loss function caus model give uncorrel predict optim differ loss function
953,"['**Let\'s investigate ""D"" features!**\n', '\n', 'We all know that the data provided in this competition consist of transactions. I was curious about how to correctly identify the same cardholder/card and group that transactions. Organizers revealed that ""D"" features contain information about different time deltas and ""card"" features apply to a payment card info. \n']",let investig featur know data provid competit consist transact curiou correctli identifi cardhold card group transact organ reveal featur contain inform differ time delta card featur appli payment card info
954,"[""Let's group by 'card' features and look closely on those combinations that give us around 5-10 rows.""]",let group card featur look close combin give u around 5 10 row
955,"[""Now let's look closer at these rows.""]",let look closer row
956,"['Now we can add ""DaysFromStart"" column by divining TransactionDT on 60*60*24 and then round it to get a number of days from a starting point.']",add daysfromstart column divin transactiondt 60 60 24 round get number day start point
957,"[""**Time to identify what's behind 'D' columns!**\n"", '\n', 'Create feature:\n', '    DaysFromPreviousTransaction = DaysFromStart[row_(i)] - DaysFromStart[row_(i-1)]']",time identifi behind column creat featur daysfromprevioustransact daysfromstart row daysfromstart row 1
958,"['I can be wrong but I believe these transactions belong to the same user. One can see that DaysFromPreviousTransaction is equal to D3 which drives me to think that **D3 indicates number of days from the previous transaction**.\n', '\n', 'Also D1 is cumulatively increasing and for example 481 = 449 + 32 and 510 = 481 + 29, i.e. **D1 could indicate days from the first transaction**. \n', '\n', 'D2 is almost always equal to D1 but for the first transaction when D1 is equal to 0 D2 is nan.']",wrong believ transact belong user one see daysfromprevioustransact equal d3 drive think d3 indic number day previou transact also d1 cumul increas exampl 481 449 32 510 481 29 e d1 could indic day first transact d2 almost alway equal d1 first transact d1 equal 0 d2 nan
959,"[""**If I'm not wrong this should be a useful knowledge to identify users and proceed with a meaningful FE.**\n"", '\n', '** Please share your thoughts! **']",wrong use knowledg identifi user proceed meaning fe plea share thought
960,"['# Can we find unique clients in data?\n', 'Transactions in data looks like are independent of each other. Perhaps the organizers made it for better data anonymization. But what if we find transactions belonging to the same user? May be it will help someone in this competition.\n', '\n', 'I accidentally saw some magic in the feature V307']",find uniqu client data transact data look like independ perhap organ made better data anonym find transact belong user may help someon competit accident saw magic featur v307
961,['### Functions that will help us find unique devices and unique cards. (I think it is clear for you why we use hash functions)'],function help u find uniqu devic uniqu card think clear use hash function
962,"[""# Don't see anything?""]",see anyth
963,"[""> # Coincidence? i don't think so ""]",coincid think
964,['![](https://www.meme-arsenal.com/memes/06308418f56ad0d8d674c247e5ccba49.jpg )'],http www meme arsen com meme 06308418f56ad0d8d674c247e5ccba49 jpg
965,"['### Highly likely V307 is a cumulative sum of transactions for a certain period for one unique client\n', '\n', 'This group of transactions has the same card hash and device hash, and 41 consecutively increasing numbers. Also all this transactions are fraud. \n', '\n', 'May be this knowledge will help you to significantly improve your models. \n', '\n', ""It doesn't work for all pairs of card and devices hashes, so you can improve alghoritm for searching unique clients.""]",highli like v307 cumul sum transact certain period one uniqu client group transact card hash devic hash 41 consecut increas number also transact fraud may knowledg help significantli improv model work pair card devic hash improv alghoritm search uniqu client
966,['# Bonus: You can try to decipher the values of some features'],bonu tri deciph valu featur
967,['![](https://www.dictionary.com/e/wp-content/uploads/2018/04/another-one.jpg )'],http www dictionari com e wp content upload 2018 04 anoth one jpg
968,"['## TL;DR:\n', '\n', '1. Classic variant:\n', '   * Mem. usage decreased from 4867.46 Mb to 2452.37 Mb (49.6% reduction)\n', '   * Number of unique values: 3285120 -> 3216884 **(2.0% lost)**\n', '2. My variant:\n', '   * Mem. usage decreased from 4847.46 Mb to 2515.03 Mb (48.1% reduction)\n', '   * Number of unique values: 3285120 -> 3285120 **(0.0% lost)**\n', '3. My variant with optional object -> category conversion (read the **Objects -> categories** section before using!):\n', '   * Mem. usage decreased from 4847.46 Mb to **1086.85 Mb (77.6% reduction)**\n', '   * Number of unique values: 3285120 -> 3285120 **(0.0% lost)**\n', '\n', '## Rationale\n', '\n', 'It seems that many competition teams use the same `reduce_mem_usage` function (with modifications) e.g. https://www.kaggle.com/kyakovlev/ieee-data-minification .\n', '\n', 'Though I see a few major drawbacks in using it as is:\n', '1. Such functions either:\n', ""    1. Don't use minimal possible types for the sake of (imaginary) safety, and therefore use more memory than actually needed.\n"", ""    2. Use float16 but don't guarantee that you don't lose precision or unique values much (see **Issue 1** below)\n"", '2. None of them try to perform float to int conversion.\n', ""3. It's done only once and don't allow you to easily minify newly created features.\n"", '\n', 'So my functons address all of these problems.\n', 'They allow using really minimal amount of memory and guarantee not losing anything (precision, na values, unique values, etc.).\n', ""And you can do minification on the fly for new columns: `df['a/b'] = sd(df['a']/df['b'])`.\n"", '\n', 'Also my `sd` (stands for `safe downcast`) function is very flexible. If you consider you can allow to lose 0.1 precision when rounding but wanna save more memory, then no problem, just set `sd(col, max_loss_limit=0.1, avg_loss_limit=0.1)`.\n', '\n', '## Objects -> categories:\n', '\n', ""My functions can do object -> category conversion as well. But it's important to remember that if you do this for train and test separately they will have different internal representation (see **Issue 2** below) and may cause issues with ML algorithms if they mess with codes.\n"", ""In my code I use a concatenated dataset with 2-level indexes so it's not a problem. See the **Load dataset** section below.\n"", '\n', 'If you want them to be converted, use `obj_to_cat=True` arg.\n', ""In this case you'll get:\n"", '* **Mem. usage decreased from 4847.46 Mb to 1086.85 Mb (77.6% reduction)**\n']",tl dr 1 classic variant mem usag decreas 4867 46 mb 2452 37 mb 49 6 reduct number uniqu valu 3285120 3216884 2 0 lost 2 variant mem usag decreas 4847 46 mb 2515 03 mb 48 1 reduct number uniqu valu 3285120 3285120 0 0 lost 3 variant option object categori convers read object categori section use mem usag decreas 4847 46 mb 1086 85 mb 77 6 reduct number uniqu valu 3285120 3285120 0 0 lost rational seem mani competit team use reduc mem usag function modif e g http www kaggl com kyakovlev ieee data minif though see major drawback use 1 function either 1 use minim possibl type sake imaginari safeti therefor use memori actual need 2 use float16 guarante lose precis uniqu valu much see issu 1 2 none tri perform float int convers 3 done allow easili minifi newli creat featur functon address problem allow use realli minim amount memori guarante lose anyth precis na valu uniqu valu etc minif fli new column df b sd df df b also sd stand safe downcast function flexibl consid allow lose 0 1 precis round wan na save memori problem set sd col max loss limit 0 1 avg loss limit 0 1 object categori function object categori convers well import rememb train test separ differ intern represent see issu 2 may caus issu ml algorithm mess code code use concaten dataset 2 level index problem see load dataset section want convert use obj cat true arg case get mem usag decreas 4847 46 mb 1086 85 mb 77 6 reduct
969,['## Issue 1'],issu 1
970,['## Issue 2'],issu 2
971,['## Functions'],function
972,"['## Load dataset\n', '\n', 'The dataset is prepared in https://www.kaggle.com/alexeykupershtokh/concat-dataframes']",load dataset dataset prepar http www kaggl com alexeykupershtokh concat datafram
973,['## Minify'],minifi
974,['## Example 1: float conversion params'],exampl 1 float convers param
975,"[""Let's try to create a new feature Series""]",let tri creat new featur seri
976,"[""Let's try to minify it with default settings""]",let tri minifi default set
977,"[""Oops, it didn't work. The reason is most likely in that the values are too dense (e.g. there could be values like 100.0001 and 100.0002). But as far as this feature is ordinal and we don't care about preserving all of the unique values, let's losen our minification rules.""]",oop work reason like valu den e g could valu like 100 0001 100 0002 far featur ordin care preserv uniqu valu let losen minif rule
978,['You can see that we lost `11957 - 11882 = 75` unique values but saved 50% of memory.'],see lost 11957 11882 75 uniqu valu save 50 memori
979,['## Example 2: automatic float to int conversion'],exampl 2 automat float int convers
980,"[""let's try frequency encoding""]",let tri frequenc encod
981,['## Example 3: C1-C14 column compression (lossy for 3 rows)'],exampl 3 c1 c14 column compress lossi 3 row
982,"['<a id=""home""></a>\n', '# IEEE Fraud Detection transactions columns reference']",id home ieee fraud detect transact column refer
983,['## Ensembling With StackNet'],ensembl stacknet
984,['![](https://github.com/kaz-Anova/StackNet/raw/master/images/StackNet_Logo.png?raw=true)'],http github com kaz anova stacknet raw master imag stacknet logo png raw true
985,"['In this kernel we will take a look on how to use StackNet to stack multiple levels of models in order to efficiently blend models. StackNet is a powerful package that works really well for competitions! We are going to stack a random forest on top of 3 GBM models as an example. We will use data from the [IEEE Fraud Detection](https://www.kaggle.com/c/ieee-fraud-detection) competition to explain StackNet.\n', '\n', 'StackNet was created by Kaggle Grandmaster Marios Michailidis ([kazanova](https://www.kaggle.com/kazanova)) as part of his PhD. Thanks to [Kiran Kunapuli](https://www.kaggle.com/kirankunapuli) for uploading the package as [a Kaggle dataset](https://www.kaggle.com/kirankunapuli/pystacknet) so it can conveniently be used with Kaggle kernels.\n', '\n', ""Let's dive in!""]",kernel take look use stacknet stack multipl level model order effici blend model stacknet power packag work realli well competit go stack random forest top 3 gbm model exampl use data ieee fraud detect http www kaggl com c ieee fraud detect competit explain stacknet stacknet creat kaggl grandmast mario michailidi kazanova http www kaggl com kazanova part phd thank kiran kunapuli http www kaggl com kirankunapuli upload packag kaggl dataset http www kaggl com kirankunapuli pystacknet conveni use kaggl kernel let dive
986,['## Table Of Contents'],tabl content
987,"['- [Dependencies](#1)\n', '- [Metric (AUC)](#2)\n', '- [Data Preparation](#3)\n', '- [Modeling](#4)\n', '- [Evaluation](#5)\n', '- [Submission](#6)']",depend 1 metric auc 2 data prepar 3 model 4 evalu 5 submiss 6
988,"['## Dependencies <a id=""1""></a>']",depend id 1
989,"['## Metric (AUC) <a id=""2""></a>']",metric auc id 2
990,"['The Metric used in this competition is ""[Area Under ROC Curve (AUC)](https://en.wikipedia.org/wiki/Receiver_operating_characteristic)"". We create this curve by plotting the true positive rate (TPR) against the false positive rate (FPR) at different threshold settings. \n', 'This is very convenient since with binary classification problems like fraud detection the accuracy score is not that informative. For example, if we predict only 0 (not fraud) on this dataset, then we will get an accuracy score of 0.965. The AUC score will be 0.5 (no better than random). All naive baselines will get an AUC score of approximately 0.5.\n', '\n', ""To calculate the AUC score we can use [sklearn's roc_auc_score function](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html) straight out of the box.""]",metric use competit area roc curv auc http en wikipedia org wiki receiv oper characterist creat curv plot true posit rate tpr fals posit rate fpr differ threshold set conveni sinc binari classif problem like fraud detect accuraci score inform exampl predict 0 fraud dataset get accuraci score 0 965 auc score 0 5 better random naiv baselin get auc score approxim 0 5 calcul auc score use sklearn roc auc score function http scikit learn org stabl modul gener sklearn metric roc auc score html straight box
991,"['Image: An example of an ROC curve. AOC is a typo and should be AUC.\n', '\n', '![](https://miro.medium.com/max/722/1*pk05QGzoWhCgRiiFbz-oKQ.png)']",imag exampl roc curv aoc typo auc http miro medium com max 722 1 pk05qgzowhcgriifbz okq png
992,['To plot the ROC curve we will use a function using sklearn and matplotlib. An example of this visualization is shown in the evaluation section of this kernel.'],plot roc curv use function use sklearn matplotlib exampl visual shown evalu section kernel
993,"['## Data Preparation <a id=""3""></a>']",data prepar id 3
994,"['Since this kernel is meant to explain StackNet and establish a baseline we will not go into advanced feature engineering and EDA here. However, your performance will greatly benefit from feature engineering so I encourage you to explore it. A good kernel which does that for this competition can be found [here](https://www.kaggle.com/kyakovlev/ieee-gb-2-make-amount-useful-again?scriptVersionId=18874747).']",sinc kernel meant explain stacknet establish baselin go advanc featur engin eda howev perform greatli benefit featur engin encourag explor good kernel competit found http www kaggl com kyakovlev ieee gb 2 make amount use scriptversionid 18874747
995,"[""StackNet does not accept missing values (NaN's), Infinity values (inf) or values higher than 32 bytes (for example float64 or int64). Therefore, we have to fill in missing values and compress certain columns as the Pandas standard is 64 bytes. Big thanks to [Arjan Groen](https://www.kaggle.com/arjanso) for creating this convenient function. The function is taken from [this Kaggle kernel](https://www.kaggle.com/arjanso/reducing-dataframe-memory-size-by-65).""]",stacknet accept miss valu nan infin valu inf valu higher 32 byte exampl float64 int64 therefor fill miss valu compress certain column panda standard 64 byte big thank arjan groen http www kaggl com arjanso creat conveni function function taken kaggl kernel http www kaggl com arjanso reduc datafram memori size 65
996,"['## Modeling <a id=""4""></a>']",model id 4
997,"['StackNet allows you to define all kinds of models. For example, Sklearn models, LightGBM, XGBoost, CatBoost and Keras models can all be used with StackNet.\n', '\n', ""For the individual models, you are responsible for not overfitting. Therefore, it is advisable to first experiment with individual models and make sure they are sound, before combining them into StackNet. For this example we will use [sklearn's Gradient Boosting Regressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html), a [LightGBM Regressor](https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMRegressor.html#lightgbm.LGBMRegressor) and a [CatBoost Regressor](https://catboost.ai/docs/concepts/python-reference_catboostregressor.html) in the 1st level. Then we will train a [Random Forest Regressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html) in level 2, which takes the predictions of the models in the 1st level as input. StackNet takes care of the stacking and cross validation.""]",stacknet allow defin kind model exampl sklearn model lightgbm xgboost catboost kera model use stacknet individu model respons overfit therefor advis first experi individu model make sure sound combin stacknet exampl use sklearn gradient boost regressor http scikit learn org stabl modul gener sklearn ensembl gradientboostingregressor html lightgbm regressor http lightgbm readthedoc io en latest pythonapi lightgbm lgbmregressor html lightgbm lgbmregressor catboost regressor http catboost ai doc concept python refer catboostregressor html 1st level train random forest regressor http scikit learn org stabl modul gener sklearn ensembl randomforestregressor html level 2 take predict model 1st level input stacknet take care stack cross valid
998,"['The model tree that StackNet takes as input is a list of lists. The 1st list defines the 1st level, the 2nd one the 2nd level, etc. You can build a model tree of arbitrary depth and width.']",model tree stacknet take input list list 1st list defin 1st level 2nd one 2nd level etc build model tree arbitrari depth width
999,"['The model is compiled and fitted through the a familiar sklearn-like API. The StackNetClassifier will perform cross-validation (CV) and will output the CV scores for each model. To make sure we can output a probability of fraud we specify ""use_proba=True"".\n', '\n', 'The ""folds"" argument in StackNetClassifier can also accept an iterable of train/test splits. Since the target distibution is imbalanced you can probably improve on the CV strategy by first yielding stratified train/test split with for example [sklearn\'s StratifiedKFold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html).']",model compil fit familiar sklearn like api stacknetclassifi perform cross valid cv output cv score model make sure output probabl fraud specifi use proba true fold argument stacknetclassifi also accept iter train test split sinc target distibut imbalanc probabl improv cv strategi first yield stratifi train test split exampl sklearn stratifiedkfold http scikit learn org stabl modul gener sklearn model select stratifiedkfold html
1000,"['## Evaluation <a id=""5""></a>']",evalu id 5
1001,"['The blue line signifies the baseline AUC which is 0.5. The final validation score is the area under the orange curve, which is mentioned in the plot.']",blue line signifi baselin auc 0 5 final valid score area orang curv mention plot
1002,"['## Submission <a id=""6""></a>']",submiss id 6
1003,"['To check if the predictions are sound, we check the format of our submission and compare our prediction distribution with that of the target distribution in the training set.']",check predict sound check format submiss compar predict distribut target distribut train set
1004,"['Try to experiment with [StackNet](https://github.com/h2oai/pystacknet) yourself. The possibilities are almost endless!\n', '\n', 'If you want to check out another solution using PyStackNet, check out [this Kaggle kernel on the Titanic dataset by Yann Berthelot](https://www.kaggle.com/yannberthelot/pystacknet-working-implementation).\n', '\n', '**If you like this Kaggle kernel, feel free to give an upvote and leave a comment! I will try to implement your suggestions in this kernel!**']",tri experi stacknet http github com h2oai pystacknet possibl almost endless want check anoth solut use pystacknet check kaggl kernel titan dataset yann berthelot http www kaggl com yannberthelot pystacknet work implement like kaggl kernel feel free give upvot leav comment tri implement suggest kernel
1005,"['# RAPIDS - Feature Engineering - 1st Place Fraud Comp - [0.96]\n', ""The secret to creating a high scoring model in Kaggle's IEEE CIS Fraud Competition is feature engineering. A list of feature engineering techniques is posted [here][1]. The most important features in Fraud Comp are new columns created from group aggregations of other columns. Why this works is explained [here][2]. Computing group aggregations can naturally be done in parallel and benefit from using GPU instead of CPU.\n"", '\n', 'This notebook contains the XGBoost model of the 1st place Fraud Comp solution converted to use RAPIDS cuDF. (The entire 1st place solution is an ensemble of XGBoost, CatBoost, and LightGBM with additional post processing described [here][4]). To read one million rows from disk and create 262 features on CPU using Pandas takes 5 minutes. To read and create those features on GPU with RAPIDS cuDF takes 20 seconds as shown below. RAPIDS is 15x faster!\n', '\n', '![speedup2.JPG](attachment:speedup2.JPG)\n', '\n', 'Individual GPU times are listed beneath code blocks below. Pandas CPU times are displayed in the notebook [here][3] beneath code blocks.\n', '\n', '[1]: https://www.kaggle.com/c/ieee-fraud-detection/discussion/108575\n', '[2]: https://www.kaggle.com/c/ieee-fraud-detection/discussion/111453\n', '[3]: https://www.kaggle.com/cdeotte/xgb-fraud-with-magic-0-9600\n', '[4]: https://www.kaggle.com/c/ieee-fraud-detection/discussion/111284']",rapid featur engin 1st place fraud comp 0 96 secret creat high score model kaggl ieee ci fraud competit featur engin list featur engin techniqu post 1 import featur fraud comp new column creat group aggreg column work explain 2 comput group aggreg natur done parallel benefit use gpu instead cpu notebook contain xgboost model 1st place fraud comp solut convert use rapid cudf entir 1st place solut ensembl xgboost catboost lightgbm addit post process describ 4 read one million row disk creat 262 featur cpu use panda take 5 minut read creat featur gpu rapid cudf take 20 second shown rapid 15x faster speedup2 jpg attach speedup2 jpg individu gpu time list beneath code block panda cpu time display notebook 3 beneath code block 1 http www kaggl com c ieee fraud detect discus 108575 2 http www kaggl com c ieee fraud detect discus 111453 3 http www kaggl com cdeott xgb fraud magic 0 9600 4 http www kaggl com c ieee fraud detect discus 111284
1006,"['# Install RAPIDS\n', ""Here we install RAPIDS from a Kaggle dataset taking 1 minute. (Install from Conda shown [here][1], if this doesn't work).\n"", '\n', '[1]: https://www.kaggle.com/cdeotte/rapids-data-augmentation-mnist-0-985']",instal rapid instal rapid kaggl dataset take 1 minut instal conda shown 1 work 1 http www kaggl com cdeott rapid data augment mnist 0 985
1007,"['# GPU Load Data\n', 'Here we read the data from the disk with cuDF directly into the GPU. With CPU Pandas this takes 46 seconds. With GPU RAPIDS this takes 4.7 seconds! ']",gpu load data read data disk cudf directli gpu cpu panda take 46 second gpu rapid take 4 7 second
1008,"['# GPU Preprocess\n', ""First we normalize D Columns, label encode all categorical columns, shift numerics postive, and fill NaN with -1. Note that RAPIDS cuDF has already label encoded all the categorical variables when they were read from disk if `dtype='category'` was used instead of `dtype='str'`.""]",gpu preprocess first normal column label encod categor column shift numer postiv fill nan 1 note rapid cudf alreadi label encod categor variabl read disk dtype categori use instead dtype str
1009,"['# GPU Encoding Functions\n', ""The following four functions are Numba CUDA JIT kernels. These functions are optimized to use Nvidia GPU. We will use these together with RAPIDS cuDF's `groupyby(col,method='cudf').apply_grouped(func)` to create blazingly fast custom feature engineering functions! Tutorials about this are [here][1], [here][2], and [here][3].\n"", '\n', '[1]: https://rapidsai.github.io/projects/cudf/en/0.11.0/guide-to-udfs.html\n', '[2]: https://github.com/daxiongshu/notebooks-extended/blob/kdd_plasticc/advanced_notebooks/tutorials/rapids_customized_kernels.ipynb\n', '[3]: https://numba.pydata.org/numba-doc/latest/cuda/index.html']",gpu encod function follow four function numba cuda jit kernel function optim use nvidia gpu use togeth rapid cudf groupybi col method cudf appli group func creat blazingli fast custom featur engin function tutori 1 2 3 1 http rapidsai github io project cudf en 0 11 0 guid udf html 2 http github com daxiongshu notebook extend blob kdd plasticc advanc notebook tutori rapid custom kernel ipynb 3 http numba pydata org numba doc latest cuda index html
1010,"['# GPU Feature Engineering\n', 'Below is where we create all our new engineered features. The work below takes 10 seconds. The work above took 10 seconds. Using RAPIDS GPU is 15x faster than Pandas CPU.']",gpu featur engin creat new engin featur work take 10 second work took 10 second use rapid gpu 15x faster panda cpu
1011,['# Local Holdout Validation'],local holdout valid
1012,['# Cross Validation and Inference'],cross valid infer
1013,"['# Submit to Kaggle\n', 'This submission achieves LB 0.960. If we post process these predictions we achieve LB 0.962. If we ensemble these with CatBoost and LGBM models, we achieve LB 0.968.']",submit kaggl submiss achiev lb 0 960 post process predict achiev lb 0 962 ensembl catboost lgbm model achiev lb 0 968
1014,"['I have just taken four columns \n', '\n', '* TransactionAmt\n', '* ProductCD\n', '* card4 \n', '* isFraud\n', '\n', 'I want to see the relationship of first three that is TransactionAmt, ProductCD, card4 with isFraud and also want to **fit** Decesion tree and see the performance of it']",taken four column transactionamt productcd card4 isfraud want see relationship first three transactionamt productcd card4 isfraud also want fit deces tree see perform
1015,"['### From output above, it is sure that this data is imbalance. ']",output sure data imbal
1016,"['### Description of TransactionAmt groupd on isFraud column \n', '\n', '* Mean value of TransactionAmt for catogry  1 is high\n', '* Median value of TransactionAmt for catogry  1 is less than catogry 0 \n', '* The TransactionAmt for catogry  1 is  more skewed on right side for category 1']",descript transactionamt groupd isfraud column mean valu transactionamt catogri 1 high median valu transactionamt catogri 1 le catogri 0 transactionamt catogri 1 skew right side categori 1
1017,['### How many type of cards are used for transactions ?'],mani type card use transact
1018,['### Stratified sampling '],stratifi sampl
1019,['### Deviding data into training and testing part'],devid data train test part
1020,['# Hope you have enjoyed this kernel. If enjoyed kindly upvote it '],hope enjoy kernel enjoy kindli upvot
1021,"['## About this kernel\n', '\n', 'Hey, everyone!\n', '\n', ""This was my first solution to this competition. There is nothing revolutionary here as this is my first competition and I'm trying to learn with all the public kernels.\n"", '\n', 'I decided to share this solution to help people that are beginners like me, to at least give them an idea of what to do and where to begin.\n', '\n', ""Later, I'll add a cell with all the advice people gave me in the discussions so we can talk more about it here. I'll also add a reference to the kernels I used here.\n"", '\n', 'Please, give me some feedback if you can. I would love to hear what you believe could be improved in this kernel.\n', '\n', 'Hope you are enjoying this competition as much as I am. Good luck and I hope you like this kernel.\n', '\n', '[**UPDATE**]  \n', ""In the comments section, @cebeci told me the merging process was wrong and it really was. I'm sorry about it, merging is now correct.  \n"", 'Also in the comments, @lftuwujie told me the GPU makes almost no difference to LGBM. I tested it and he was (as expected) correct, so this was also changed.\n', '\n', ""Thank you for all the feedback and support. It's been really nice to learn with all of you.""]",kernel hey everyon first solut competit noth revolutionari first competit tri learn public kernel decid share solut help peopl beginn like least give idea begin later add cell advic peopl gave discus talk also add refer kernel use plea give feedback would love hear believ could improv kernel hope enjoy competit much good luck hope like kernel updat comment section cebeci told merg process wrong realli sorri merg correct also comment lftuwuji told gpu make almost differ lgbm test expect correct also chang thank feedback support realli nice learn
1022,"['# Dating Dataset Creation with a Browser\n', '\n', 'In this kernel I do some basic exploratory data analysis on the IEEE Fraud Detection dataset. The purpose of this simple notebook is to estimate the starting date of this dataset.\n', '\n', ""For that, we will analyze the timestamp `TransactionDT` provided for each transaction. `TransactionDT` is the time elapsed in second since dataset's starting date and the transaction date. We will combine this piece of data with the browser used to perform the transaction (`id_31`) to discover the dataset's starting date.\n"", '\n', '![adam-tinworth-OJWivczp3aY-unsplash-resized.jpg](attachment:adam-tinworth-OJWivczp3aY-unsplash-resized.jpg)']",date dataset creation browser kernel basic exploratori data analysi ieee fraud detect dataset purpos simpl notebook estim start date dataset analyz timestamp transactiondt provid transact transactiondt time elaps second sinc dataset start date transact date combin piec data browser use perform transact id 31 discov dataset start date adam tinworth ojwivczp3ay unsplash resiz jpg attach adam tinworth ojwivczp3ay unsplash resiz jpg
1023,['###\xa0Import libraries'],xa0import librari
1024,['### Load data'],load data
1025,"['### Browsers available\n', '\n', '`id_31` contains browser in use during a card-not-present transaction (CNP). What are the most used browsers?']",browser avail id 31 contain browser use card present transact cnp use browser
1026,"['There are several versions of **chrome** browser in our top of the most used browsers in this dataset. It seems a good candidate for further analysis.\n', '\n', '###\xa0Chrome versions\n', '\n', 'List of chrome version in this dataset is a good starting point:']",sever version chrome browser top use browser dataset seem good candid analysi xa0chrom version list chrome version dataset good start point
1027,"['We have a large set of different chrome releases ranging from version **39 to 71** and for **3 different platforms** (Desktop, Android and iOS).\n', '\n', '### Chrome daily usage\n', '\n', 'What is the life cycle of chrome versions?']",larg set differ chrome releas rang version 39 71 3 differ platform desktop android io chrome daili usag life cycl chrome version
1028,['What are the **Desktop** version of chrome used?'],desktop version chrome use
1029,"['We can clearly see that the **Desktop version** of Chrome is regurlarly updated by Google and users are mostly up-to date thanks to the automatic chrome updater.\n', '\n', 'I also notice that versions are not deployed immediately to all users. When a new version is relased, there is first a linear ramp-up of the number of updated users and then it is deployed to all users. This can be seen for versions 68, 69, 70 and 71. Probably Google is monitoring first updated installations to detect problems before to make a versionglobally available.\n', '\n', 'What about **Android** version?']",clearli see desktop version chrome regurlarli updat googl user mostli date thank automat chrome updat also notic version deploy immedi user new version relas first linear ramp number updat user deploy user seen version 68 69 70 71 probabl googl monitor first updat instal detect problem make versionglob avail android version
1030,"['**Android** users have mostly up-to-date chrome version on their devices. But old versions of chrome looks more frequent and older than on desktop (version 39 still in use compared to 49 on desktop).\n', '\n', 'Lastly, the second most frequent mobile system: **Apple iOS**']",android user mostli date chrome version devic old version chrome look frequent older desktop version 39 still use compar 49 desktop lastli second frequent mobil system appl io
1031,"['**Apple** users are the small portion of users, but they are keeping their browser up-to-date as they have the smallest number of different chrome versions in the field.']",appl user small portion user keep browser date smallest number differ chrome version field
1032,"['From these graphs, we easily guess when a new chrome browser is relased by looking the rapid increase of the version count for each platform. Apparently this dataset covers the release date of chrome 63 to 71 for all the 3 platforms.']",graph easili guess new chrome browser relas look rapid increas version count platform appar dataset cover releas date chrome 63 71 3 platform
1033,"['### Chrome release dates\n', '\n', 'A quick search on Internet and [wikipedia](https://en.wikipedia.org/wiki/Google_Chrome_version_history) in peculiar gives us the following release dates for the different versions of chrome. It is ranging from 2014 to 2019.']",chrome releas date quick search internet wikipedia http en wikipedia org wiki googl chrome version histori peculiar give u follow releas date differ version chrome rang 2014 2019
1034,"['### Estimate dataset start point for each chrome version\n', '\n', 'We can now estimate the `TransactionDT` associated with the release date of chrome 63 to 71 by taking the smallest `TransactionDT` for each of these versions in the dataset.']",estim dataset start point chrome version estim transactiondt associ releas date chrome 63 71 take smallest transactiondt version dataset
1035,"['### And the origin is...\n', '\n', ""To reduce outliners influence, we'll take the median of estimated dates. Therefore we estimate *the beginning of time* for this dataset is around **28th november 2017**:""]",origin reduc outlin influenc take median estim date therefor estim begin time dataset around 28th novemb 2017
1036,['### Conclusion'],conclus
1037,"['We were supposing in our analysis that all users were using the [""stable"" channel](https://www.chromium.org/getting-involved/dev-channel) of chrome browser to determine this dataset\'s starting date. But if a single user is using a ""beta"", ""dev"" or ""canary"" channel version of chrome our estimation is biased. These non-""stable"" channel versions are generally released before the ""stable"" version, then our estimate will be biased to an earlier date than the right one.\n', '\n', 'This date is close to what other kagglers found (30th november) by considering days with highest peaks as christmas or cybermonday.\n', '\n', '\n', 'I hope you enjoyed reading this notebook as much as I am while writing it. I let the reader do the same analysis with other browsers like **safari** or **firefox** as an exercice.\n', '\n', 'Take care and Happy kaggling! 👍']",suppos analysi user use stabl channel http www chromium org get involv dev channel chrome browser determin dataset start date singl user use beta dev canari channel version chrome estim bias non stabl channel version gener releas stabl version estim bias earlier date right one date close kaggler found 30th novemb consid day highest peak christma cybermonday hope enjoy read notebook much write let reader analysi browser like safari firefox exercic take care happi kaggl
1038,"['# Missing data\n', '\n', 'This kernel aims to do some exploratory analysis of the missing data in the training and test sets. \n', '\n', '## Key findings:\n', '\n', '- The distribution of missing values is different between the training and test set (Figure 1).\n', '- There seems to be a distinct point in time when this distribution changes (Figures 2 and 3).\n', '- We have identified the main features where this change is occuring. \n', '\n', '\n', 'The different distributions could affect models ability to generalize well to the test set, and so the missing data will have to be appropiately handled.']",miss data kernel aim exploratori analysi miss data train test set key find distribut miss valu differ train test set figur 1 seem distinct point time distribut chang figur 2 3 identifi main featur chang occur differ distribut could affect model abil gener well test set miss data appropi handl
1039,"['This notebook will originally focus on the number of NaNs (i.e., missing values) for each instance (row) in training and tests set.']",notebook origin focu number nan e miss valu instanc row train test set
1040,"['## Overview of the missing values per training instance \n', '\n', 'The graph below (Figure 1), displays a histogram of the number of NaN (missing) values for a given training instance. Overall it shows there are, on average, a greater number of missing values in the training set than compared to the test set. ']",overview miss valu per train instanc graph figur 1 display histogram number nan miss valu given train instanc overal show averag greater number miss valu train set compar test set
1041,"['### Figure 1\n', '\n', 'This shows the distribution of the number of missing values for the training and test sets. ']",figur 1 show distribut number miss valu train test set
1042,"['### Temporal split in missing data\n', '\n', ""In Figure 2 (below) you can clearly see there is shift in the number of missing values per training instance partially through the test set data (I've confirmed this with additional moving average plots - not shown). \n"", '\n', 'This could reflect a change in recording practices or a temporal dependence for more values to be missing (e.g., some seasonal dependence to the transactions?)']",tempor split miss data figur 2 clearli see shift number miss valu per train instanc partial test set data confirm addit move averag plot shown could reflect chang record practic tempor depend valu miss e g season depend transact
1043,"['### Figure 2\n', '\n', ""This is a scatter plot of the number of missing values for each instance as a function of 'time', for both the training and test sets.  \n"", '\n', 'The dashed grey lines help to guide the eyes to the regions used in the histograms of Figure 3']",figur 2 scatter plot number miss valu instanc function time train test set dash grey line help guid eye region use histogram figur 3
1044,"['### Figure 3\n', '\n', ""Next we look at histograms of the number of missing data entries for each instance at three key time periods: during the training set, before the 'step' in the test set and after the 'step' in the test set. \n"", '\n', 'It is clear that the train and test at early times appear equivalent, but after approximately Transaction DT = 2.3e7 the test set changes: there are signifcantly less missing entries for a given instance of the test set.']",figur 3 next look histogram number miss data entri instanc three key time period train set step test set step test set clear train test earli time appear equival approxim transact dt 2 3e7 test set chang signifcantli le miss entri given instanc test set
1045,"['## Which features have reduced in the number of missing values\n', '\n', 'We can see there are some features with a lot less missing values in the test sets (namely D12 and a number of the VI features).\n', '\n', 'We will have to look into what these features are and how we can deal with the missing data. \n', '\n']",featur reduc number miss valu see featur lot le miss valu test set name d12 number vi featur look featur deal miss data
1046,['# Work in progress'],work progress
1047,"['## Looking at the distribution of values for a feature which changes significantly.\n', '\n', 'We will look at D15. ']",look distribut valu featur chang significantli look d15
1048,['## Does more missing data increase the chance of a Fradulent transaction'],miss data increas chanc fradul transact
1049,"['Clearly, there are certain numbers of missing data which correlates with a an increased chance of the transaction being fradulent. \n', '\n', 'Where there is a 100% chance of the transaction being fradulent this corresponds to there being only one training instance with this number of missing variables.\n', '\n', '\n', 'We will have to investigate which features are missing and what this can tell us about the transaction.']",clearli certain number miss data correl increas chanc transact fradul 100 chanc transact fradul correspond one train instanc number miss variabl investig featur miss tell u transact
1050,"['# Fraud Detection \n', '* Author: Grant Gasser\n', '* Last Edit: 8/18/2019\n', '* Kaggle: ""In this competition you are predicting the probability that an online transaction is fraudulent, as denoted by the binary target `isFraud.`""']",fraud detect author grant gasser last edit 8 18 2019 kaggl competit predict probabl onlin transact fraudul denot binari target isfraud
1051,"['## Summary\n', ""**This is my first serious attempt at a Kaggle competition. As such, I would really appreciate some feedback or tips for improving performance. If you enjoyed this notebook and it helped you, please leave a thumbs up! Though I've written most of the code myself, I have found the other public kernels very helpful and would encourage you do browse through them to look for other good ideas.**\n"", '\n', '* **Public Leaderboard Results:** (I plan on using some of the previous submission files for ensembling)\n', '* Random Forest filled NaNs with -999: `.872`\n', '* XGBoost filled NaNs with -999: `.938`, submission file: `baseline_xgboost.csv`\n', '* XGBoost impute mean for numerical NaNs and most common cat for categorical NaNs, also normalized numerical vars: `.878`\n', '* XGBoost impute mean for numerical NaNs and most common cat for categorical NaNs, no normalization: `.932`, submission file: `preprocessed_xgboost`\n', '* XGBoost, impute mean for numerical NaNs, do not impute most common category for categorical NaNs, no normalization: `.934`, file: `preprocessed2_xgboost`. **NOTE**: imputing mean for numerical NaNs and most common category for categorical NaNs did not seem to help for XGBoost. \n', '* Version 21: hyperparameter tuning with XGBoost (Grid Search or Random Search), `.9284`, `xgboost_with_tuning`\n', '* `.9226`, `xgboost_with_tuning2`']",summari first seriou attempt kaggl competit would realli appreci feedback tip improv perform enjoy notebook help plea leav thumb though written code found public kernel help would encourag brow look good idea public leaderboard result plan use previou submiss file ensembl random forest fill nan 999 872 xgboost fill nan 999 938 submiss file baselin xgboost csv xgboost imput mean numer nan common cat categor nan also normal numer var 878 xgboost imput mean numer nan common cat categor nan normal 932 submiss file preprocess xgboost xgboost imput mean numer nan imput common categori categor nan normal 934 file preprocessed2 xgboost note imput mean numer nan common categori categor nan seem help xgboost version 21 hyperparamet tune xgboost grid search random search 9284 xgboost tune 9226 xgboost tuning2
1052,"['## ENSEMBLING:\n', '* Averaging out my previous predictions using the files listed above ^ \n', '* data: `https://www.kaggle.com/grantgasser/previous-submissions`']",ensembl averag previou predict use file list data http www kaggl com grantgass previou submiss
1053,['## Libraries'],librari
1054,['## View provided files'],view provid file
1055,"['## Ensemble\n', '* Will have to comment out everything below this\n', '* Load previous submissions']",ensembl comment everyth load previou submiss
1056,"['### Weighted Avg\n', '* Based on scores (more weight for model outputs that had better scores)\n', ""* `.05, .05, .1, .8` -> `.9392`, minor improvement from the best model's score of `.9381`, file: `ensemble5.csv`""]",weight avg base score weight model output better score 05 05 1 8 9392 minor improv best model score 9381 file ensemble5 csv
1057,"['# Data Description \n', '* As provided by VESTA: https://www.kaggle.com/c/ieee-fraud-detection/discussion/101203#latest-586800\n', '\n', '#### Transaction Table\n', '* TransactionDT: timedelta from a given reference datetime (not an actual timestamp)\n', '* TransactionAMT: transaction payment amount in USD\n', '* ProductCD: product code, the product for each transaction\n', '* card1 - card6: payment card information, such as card type, card category, issue bank, country, etc.\n', '* addr: address\n', '* dist: distances between (not limited) billing address, mailing address, zip code, IP address, phone area, etc.\n', '* P_ and (R__) emaildomain: purchaser and recipient email domain\n', '* C1-C14: counting, such as how many addresses are found to be associated with the payment card, etc. The actual meaning is masked.\n', '* D1-D15: timedelta, such as days between previous transaction, etc.\n', '* M1-M9: match, such as names on card and address, etc.\n', '* Vxxx: Vesta engineered rich features, including ranking, counting, and other entity relations.\n', '\n', '** Categorical Features: **\n', '* ProductCD\n', '* card1 - card6\n', '* addr1, addr2\n', '* Pemaildomain Remaildomain\n', '* M1 - M9\n', '\n', '---\n', '\n', '#### Identity Table\n', 'Variables in this table are identity information – network connection information (IP, ISP, Proxy, etc) and digital signature (UA/browser/os/version, etc) associated with transactions. \n', ""They're collected by Vesta’s fraud protection system and digital security partners.\n"", '(The field names are masked and pairwise dictionary will not be provided for privacy protection and contract agreement)\n', '\n', '** Categorical Features: **\n', '* DeviceType\n', '* DeviceInfo\n', '* id12 - id38']",data descript provid vesta http www kaggl com c ieee fraud detect discus 101203 latest 586800 transact tabl transactiondt timedelta given refer datetim actual timestamp transactionamt transact payment amount usd productcd product code product transact card1 card6 payment card inform card type card categori issu bank countri etc addr address dist distanc limit bill address mail address zip code ip address phone area etc p r emaildomain purchas recipi email domain c1 c14 count mani address found associ payment card etc actual mean mask d1 d15 timedelta day previou transact etc m1 m9 match name card address etc vxxx vesta engin rich featur includ rank count entiti relat categor featur productcd card1 card6 addr1 addr2 pemaildomain remaildomain m1 m9 ident tabl variabl tabl ident inform network connect inform ip isp proxi etc digit signatur ua browser o version etc associ transact collect vesta fraud protect system digit secur partner field name mask pairwis dictionari provid privaci protect contract agreement categor featur devicetyp deviceinfo id12 id38
1058,['## Load and explore data'],load explor data
1059,['### View tables'],view tabl
1060,"['# Merge identity and transaction tables\n', '* Per Kaggle: ""The data is broken into two files `identity` and `transaction`, which are joined by `TransactionID`. Not all transactions have corresponding identity information.\n', '* Merge identity and transaction tables with `TransactionID` as the key""\n', '* Since ""not all transactions have corresponding identity information,"" we will use a (left) outer join, using pandas merge function since a key might not appear in both tables']",merg ident transact tabl per kaggl data broken two file ident transact join transactionid transact correspond ident inform merg ident transact tabl transactionid key sinc transact correspond ident inform use left outer join use panda merg function sinc key might appear tabl
1061,['# Baseline Model with minimal pre-processing (.938)'],baselin model minim pre process 938
1062,"['### XGBoost Classifier\n', '* See [notebook](https://www.kaggle.com/inversion/ieee-simple-xgboost) for starter code']",xgboost classifi see notebook http www kaggl com invers ieee simpl xgboost starter code
1063,"['### Replacing with Missing Values\n', '* Using code from: https://www.kaggle.com/inversion/ieee-simple-xgboost']",replac miss valu use code http www kaggl com invers ieee simpl xgboost
1064,['## Train XGBoost model'],train xgboost model
1065,['### XGBoost AUC = .938'],xgboost auc 938
1066,"['### Data Types\n', '* Before diving into EDA, look at data types of current features and see if they need to be changed']",data type dive eda look data type current featur see need chang
1067,"['**Categorical Features:**\n', '* ProductCD\n', '* card1 - card6\n', '* addr1, addr2\n', '* Pemaildomain Remaildomain\n', '* M1 - M9\n', '* DeviceType\n', '* DeviceInfo\n', '* id12 - id38']",categor featur productcd card1 card6 addr1 addr2 pemaildomain remaildomain m1 m9 devicetyp deviceinfo id12 id38
1068,"['### Thoughts\n', ""* Some of these should not be numerical data (e.g. card1-card6 should be 'object' types, not int64 or float64)\n"", '* The next few cells changes this']",thought numer data e g card1 card6 object type int64 float64 next cell chang
1069,['# EDA'],eda
1070,"['## Check Missing Values\n', '* Hint: there are lots of them']",check miss valu hint lot
1071,['## Analyze Categorical Variables'],analyz categor variabl
1072,"['### Thoughts\n', '* There are several categorical variables with many categories, suggesting that 1-Hot encoding might be too high dimensional\n', '* It may be more prudent to do label encoding (1,2,3,..) to limit dimensionality\n', '* Drawback with Label Encoding: softly implies there is some order to the categories since the categories are now numbered']",thought sever categor variabl mani categori suggest 1 hot encod might high dimension may prudent label encod 1 2 3 limit dimension drawback label encod softli impli order categori sinc categori number
1073,"['## Explore Labels\n', '* Note the class imbalance\n', '* About 3.5% of train examples are fraudulent']",explor label note class imbal 3 5 train exampl fraudul
1074,"['## Compare fraud and non-fraud (within training set)\n', '1. Compare the difference in means of numerical features between the fraud and non-fraud transactions. \n', '\n', '2. Compare the difference in distributions of categorical features between the fraud and non-fraud transactions. \n', ' ']",compar fraud non fraud within train set 1 compar differ mean numer featur fraud non fraud transact 2 compar differ distribut categor featur fraud non fraud transact
1075,['### Look at a few fraudulent transactions'],look fraudul transact
1076,['## Compare train and test'],compar train test
1077,"['### Note TransactionDT has no overlap\n', '* As mentioned: https://www.kaggle.com/robikscube/ieee-fraud-detection-first-look-and-eda\n', '* Not sure what to do here. Maybe transform so that each value is relative to its range?']",note transactiondt overlap mention http www kaggl com robikscub ieee fraud detect first look eda sure mayb transform valu rel rang
1078,"['## Takeaways from EDA\n', '### There are lots of missing values\n', '### There is significant class imbalance (Only ~20,000 out of 590,000 are fraudulent, or 3.5 %)\n', '* Thus, a classifier that always predicts not fraud (0) would have 96.5% accuracy (on the training set, the test set is similar)\n', '\n', '\n', '### TRAIN SET: Comparing means of numerical features among fraud and non-fraud transactions:\n', '* `TransactionDT` - fraudulent transactions 4.5% higher\n', '* `TransactionAmt` - fraudulent transactions 11% more expensive\n', '\n', '### TRAIN SET: Comparing distributions of categorical variables among fraud and non-fraud transactions:\n', '* Take a look at the above cell to see the comparison\n', '* Some of these may spurious, but with 20,000 fraudulent examples, they could imply something\n', ""* `ProductCD` - 39% of fraud transactions are 'C', but only 11% of non-fraud transactions are 'C'\n"", '* `card1` - looks similar\n', '* `card4` - distribution looks similar\n', '* `card6` - fraud transactions distributed evenly (52/48) between debit and credit whereas non-fraud transactions are mostly debit (76%)\n', '* `P_emaildomain` - 13% of fraud comes from hotmail email vs. 9% non-fraud is hotmail email \n', '* `R_emaildomain` - 60% of emails on receiving end of fraud are gmail vs. only 40% for non-fraud\n', ""* `id_29` - 70% are 'Found' in the fraud examples vs. 52% in the non-fraud\n"", '* `id_30` - Though MAC OS versions show up on non-fraud top 10, do not show up in top 10 for fraud, implying fraud less common on MAC\n', '* `DeviceType` - fraud was about evenly distributed (50/50) between mobile and desktop, most non-fraud on desktop (61%)\n', '* `DeviceInfo` - similar to what id_30 implied, MAC used for 11% of non-fraud transactions but just 3% of fraud transactions\n', '\n', '\n', '### Comparing train distribution and test distribution\n', '* Remember, train size is $560,000$ and test size is $500,000$\n', '* Other than `TransactionDT`, the distributions look similar\n', '* Note that since the test set is later in time, there are some features where the distributions are almost certain to be different\n', '* e.g. `id_31` represents the browser used. For the train set, the most common browser was **chrome 63** at 16%. In the test set, the most common was **chrome 70**.\n', '7 versions later and **chrome 63** did not even show up in the top 10 most common browser for the test set, unsurprisingly.\n', '* Should I drop `id_31` and other columns affected by time or let the model weight it?\n', '* Also, looking at `DeviceType`, 60% of transactions in the train set were done on desktop vs. 54% on desktop in test set. \n', 'Could this represent the increasing usage of mobile? Is there that much of a time difference between the train and test set?']",takeaway eda lot miss valu signific class imbal 20 000 590 000 fraudul 3 5 thu classifi alway predict fraud 0 would 96 5 accuraci train set test set similar train set compar mean numer featur among fraud non fraud transact transactiondt fraudul transact 4 5 higher transactionamt fraudul transact 11 expens train set compar distribut categor variabl among fraud non fraud transact take look cell see comparison may spuriou 20 000 fraudul exampl could impli someth productcd 39 fraud transact c 11 non fraud transact c card1 look similar card4 distribut look similar card6 fraud transact distribut evenli 52 48 debit credit wherea non fraud transact mostli debit 76 p emaildomain 13 fraud come hotmail email v 9 non fraud hotmail email r emaildomain 60 email receiv end fraud gmail v 40 non fraud id 29 70 found fraud exampl v 52 non fraud id 30 though mac o version show non fraud top 10 show top 10 fraud impli fraud le common mac devicetyp fraud evenli distribut 50 50 mobil desktop non fraud desktop 61 deviceinfo similar id 30 impli mac use 11 non fraud transact 3 fraud transact compar train distribut test distribut rememb train size 560 000 test size 500 000 transactiondt distribut look similar note sinc test set later time featur distribut almost certain differ e g id 31 repres browser use train set common browser chrome 63 16 test set common chrome 70 7 version later chrome 63 even show top 10 common browser test set unsurprisingli drop id 31 column affect time let model weight also look devicetyp 60 transact train set done desktop v 54 desktop test set could repres increas usag mobil much time differ train test set
1079,['# Pre-processing'],pre process
1080,"['### Remove features with large amounts of missing data\n', '* For computational purposes, removing features that have 80% (arbitrary number) or more missing values in the training set\n', '* May come back and try different values for this cutoff\n', '* Fill in remaining missing values with mean or median']",remov featur larg amount miss data comput purpos remov featur 80 arbitrari number miss valu train set may come back tri differ valu cutoff fill remain miss valu mean median
1081,"['### In next iteration: come back and inspect columns being dropped\n', '* See if some should be kept and receive imputed values']",next iter come back inspect column drop see kept receiv imput valu
1082,['## Replace missing values with mean (numerical) and most common category (categorical)'],replac miss valu mean numer common categori categor
1083,"[' ### Reminder\n', ' **NOTE:** with fillna, replace and other pandas functions, make sure you set the variable, because it returns the transformed object\n', ' * e.g. `df[feature] = df[feature].replace()` instead of just `df[feature].replace()`\n', ' * **Note:** Just replacing NaNs with -999 and letting XGBoost handle it, will use this function if try different models such as Neural Network']",remind note fillna replac panda function make sure set variabl return transform object e g df featur df featur replac instead df featur replac note replac nan 999 let xgboost handl use function tri differ model neural network
1084,['### Any remaining missing values?'],remain miss valu
1085,"['## Label Encoding\n', '* Change categorical variable data to numbers so that computer can understand\n', ""* e.g. if the encoding is: `['mastercard', 'discover', 'visa']` based on index, then data like `['visa', 'visa', 'mastercard', 'discover', 'mastercard']` would be encoded as `[2, 2, 0, 1, 0]`""]",label encod chang categor variabl data number comput understand e g encod mastercard discov visa base index data like visa visa mastercard discov mastercard would encod 2 2 0 1 0
1086,"['### Normalize Variables\n', '* For speed of convergence and numerical stability\n', '* Also to ensure variables with larger numbers do not dominate the model (e.g. TransactionAmt)\n', '* Normalize numerical variables: $x_{i,j} = \\frac{x_{i,j} - \\mu_j}{\\sigma_j}$ where $i$ is the row, $j$ is the column, $\\mu_j$ is the mean of the column and $\\sigma_j$ is the std of the col\n', '* After the transformation, we will have $\\mu_j = 0$ and $\\sigma_j = 1$ for each numerical column/feature $j$\n', '* Could also try Min-Max scaling too which gives $x_j \\in (0,1)$ for all $i$.']",normal variabl speed converg numer stabil also ensur variabl larger number domin model e g transactionamt normal numer variabl x j frac x j mu j sigma j row j column mu j mean column sigma j std col transform mu j 0 sigma j 1 numer column featur j could also tri min max scale give x j 0 1
1087,"['## Skip normalize to see effect on performance\n', '* Pre-processed XGBoost score `.878` vs. `.938` with no pre-processing\n', '* **Note:** After removing normalization for XGBoost, performance jumped from `.878` to `.932`. Normalization may only be necessary or helpful with neural nets and similar algorithms']",skip normal see effect perform pre process xgboost score 878 v 938 pre process note remov normal xgboost perform jump 878 932 normal may necessari help neural net similar algorithm
1088,"['### Reduce memory usage before fitting XGBoost\n', '* Thanks to https://www.kaggle.com/iasnobmatsu/xgb-model-with-feature-engineering']",reduc memori usag fit xgboost thank http www kaggl com iasnobmatsu xgb model featur engin
1089,"['## XGBoost\n', '* https://developer.ibm.com/code/2018/06/20/handle-imbalanced-data-sets-xgboost-scikit-learn-python-ibm-watson-studio/\n', '* XGBoost is an extreme gradient boosting algorithm based on trees that tends to perform very well out of the box compared to other ML algorithms.\n', '* XGBoost is popular with data scientists and is one of the most common ML algorithms used in Kaggle Competitions.\n', '* XGBoost allows you to tune various parameters.\n', '* XGBoost allows parallel processing.']",xgboost http develop ibm com code 2018 06 20 handl imbalanc data set xgboost scikit learn python ibm watson studio xgboost extrem gradient boost algorithm base tree tend perform well box compar ml algorithm xgboost popular data scientist one common ml algorithm use kaggl competit xgboost allow tune variou paramet xgboost allow parallel process
1090,"['## Fit the XGBoost Classifier Again using Cross Validation\n', '* See how the performance differs after imputing values and normalizing data\n', '* The baseline score was `.938`']",fit xgboost classifi use cross valid see perform differ imput valu normal data baselin score 938
1091,"['### Hyperparameter tuning with GridSearch and RandomizedSearch\n', '* [XGBoost parameters](https://xgboost.readthedocs.io/en/latest/parameter.html)\n', '* [GridSearch](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV) - **Takes too much RAM**, exhaustive search of the parameters, expensive but finds the optimal set\n', '* set `scale_pos_weight` to adjust for class imbalance, common to do `sum(neg samples) / sum(pos samples)` which would be about 30 in this data set\n', '* control overfitting: `max_depth`, `min_child_weight`, `gamma` per xgboost docs']",hyperparamet tune gridsearch randomizedsearch xgboost paramet http xgboost readthedoc io en latest paramet html gridsearch http scikit learn org stabl modul gener sklearn model select gridsearchcv html sklearn model select gridsearchcv take much ram exhaust search paramet expens find optim set set scale po weight adjust class imbal common sum neg sampl sum po sampl would 30 data set control overfit max depth min child weight gamma per xgboost doc
1092,"['## TODO\n', '* GridSearchCV and RandomizedSearchCV take too much RAM\n', '* Will write my own grid search loop and be more efficient with RAM']",todo gridsearchcv randomizedsearchcv take much ram write grid search loop effici ram
1093,"['### ^ Stopped early\n', '* Changing `gamma` does not seem to affect the performance\n', '* Adding more estimators and more max depth will improve performance on a subset of the test set, but has not led to improvement on the test set']",stop earli chang gamma seem affect perform ad estim max depth improv perform subset test set led improv test set
1094,"[""In this kernel I'm trying to fill some NaNs values using one intresting observation.""]",kernel tri fill nan valu use one intrest observ
1095,"['*just random pic idk*\n', '![](https://sun9-31.userapi.com/c857628/v857628861/4c59e/JiPqE9xmzjs.jpg)']",random pic idk http sun9 31 userapi com c857628 v857628861 4c59e jipqe9xmzj jpg
1096,"[""I've noticed that there are cases in card columns that depends on other card columns. So using that approach we can fill some NaNs in data. Let's look at the data!""]",notic case card column depend card column use approach fill nan data let look data
1097,"[""Let's count all NaNs in every card columns""]",let count nan everi card column
1098,"['We can see that card2 is the most NaN card feature. What is more, card3, card4 and car6 in test have 2 times more NaNs values.']",see card2 nan card featur card3 card4 car6 test 2 time nan valu
1099,"[""Let's look ratio of missing values to the total number of rows.""]",let look ratio miss valu total number row
1100,['Not very high ratios though.'],high ratio though
1101,['# Card1 and Card2'],card1 card2
1102,['There is dependency between сard2 and card1 values.  '],depend ard2 card1 valu
1103,"['In the dataset we can found a lot of cases like that. Where most of the values are the same, but there are some missing values. So we can assume that in NaN rows should be that only value which occurs in that card1 category. ']",dataset found lot case like valu miss valu assum nan row valu occur card1 categori
1104,"[""Let's count unique values for each card1 category.""]",let count uniqu valu card1 categori
1105,['We can see that most of the card1 category have only one unique value. '],see card1 categori one uniqu valu
1106,"[""Now let's count amount of the missing values for amount of the unique values.""]",let count amount miss valu amount uniqu valu
1107,"['Hm. There are a lot of missing values for categorys where there is no values(only NaNs) and where only one value.\n', 'So we can do that:\n', '\n', '* Fill NaNs in 1-amount category with most frequent value\n', '* Treat 0-amount category NaNs as only one category. We can just encode it somehow.']",hm lot miss valu categori valu nan one valu fill nan 1 amount categori frequent valu treat 0 amount categori nan one categori encod somehow
1108,['But right now we will focus only on 1-amount category and fill NaNs with most frequent value in card1 category.'],right focu 1 amount categori fill nan frequent valu card1 categori
1109,['# Card1 and Card3'],card1 card3
1110,"[""Let's do all the same but for card3 category.""]",let card3 categori
1111,['So we filled almost all NaNs in card3.'],fill almost nan card3
1112,['# Card1 and Card4'],card1 card4
1113,"['Ok, here is the same dependency.']",ok depend
1114,['Here we have the same problem. And that approach can solve it too.'],problem approach solv
1115,['# Card1 and Card5'],card1 card5
1116,['# Card1 and Card6'],card1 card6
1117,"[""### Ok. Let's look at number on NaNs now.""]",ok let look number nan
1118,"[""Still there are a lot of NaNs in the card2 and card5. Let's try some other fill combinations.""]",still lot nan card2 card5 let tri fill combin
1119,"[""Let's find another dependent feature for card2.""]",let find anoth depend featur card2
1120,['We can see that there are too many unique values to implement this approch to fill remaining NaNs in card2.'],see mani uniqu valu implement approch fill remain nan card2
1121,"[""Let's find another dependent feature for card5.""]",let find anoth depend featur card5
1122,['Same for card5.'],card5
1123,"[""Let's try some other features.""]",let tri featur
1124,['# Card1 and Addr2'],card1 addr2
1125,"['There are really a lot of missing values in addr2, especially for 1-amount category.']",realli lot miss valu addr2 especi 1 amount categori
1126,"['So, we can fill to many values using this approach. But we cannot be 100% sure that missing values in 1-amount category is most frequent category.']",fill mani valu use approach 100 sure miss valu 1 amount categori frequent categori
1127,"[""# Let's find all the features that depends on card1""]",let find featur depend card1
1128,['There are a lot of columns that we can suspect in dependency. And some of them we can fill like above.'],lot column suspect depend fill like
1129,['## If you want to apply my kernel you can use that function:'],want appli kernel use function
1130,['If you find this kernel helpful please upvote!'],find kernel help plea upvot
1131,"['**THIS KERNAL IS BLEND OF **\n', 'So awesome kernals present Right now \n', '\n', '**vote if you love blend**\n']",kernal blend awesom kernal present right vote love blend
1132,"['## phase 1 [Ensemble]\n', '\n', '1. https://www.kaggle.com/jazivxt/safe-box with 0.9420\n', '2. https://www.kaggle.com/artkulak/ieee-fraud-simple-baseline-0-9383-lb with 0.9398\n', '3. https://www.kaggle.com/artgor/eda-and-models with 0.9397\n', '4. https://www.kaggle.com/stocks/under-sample-with-multiple-runs with 0.9391\n']",phase 1 ensembl 1 http www kaggl com jazivxt safe box 0 9420 2 http www kaggl com artkulak ieee fraud simpl baselin 0 9383 lb 0 9398 3 http www kaggl com artgor eda model 0 9397 4 http www kaggl com stock sampl multipl run 0 9391
1133,"[""I'm new to Kaggle so this kernel is just how I'm learning from others...\n"", '\n', 'Good luck!']",new kaggl kernel learn other good luck
1134,"['link to kernels I found useful:\n', '\n', '* [**P_emaildomain, R_emaildomain effectiveness** by *Manraj Singh*](https://www.kaggle.com/c/ieee-fraud-detection/discussion/100778)\n', '* [**lgb-single-model** by *Roman*](https://www.kaggle.com/nroman/lgb-single-model-lb-0-9419)\n', '* [**Fraud makers are earnest people about browser** by *yasagure* ](https://www.kaggle.com/yasagure/fraud-makers-are-earnest-people-about-browser)\n', '* [**LGB starter - R** by *Shuo-Jen, Chang*](https://www.kaggle.com/andrew60909/lgb-starter-r)\n', '* [**eda-and-models** by *Andrew Lukyanenko*](https://www.kaggle.com/artgor/eda-and-models)\n', '* [**Feature Engineering + LightGBM w/ GPU** by *David Cairus*](https://www.kaggle.com/davidcairuz/feature-engineering-lightgbm-w-gpu)\n']",link kernel found use p emaildomain r emaildomain effect manraj singh http www kaggl com c ieee fraud detect discus 100778 lgb singl model roman http www kaggl com nroman lgb singl model lb 0 9419 fraud maker earnest peopl browser yasagur http www kaggl com yasagur fraud maker earnest peopl browser lgb starter r shuo jen chang http www kaggl com andrew60909 lgb starter r eda model andrew lukyanenko http www kaggl com artgor eda model featur engin lightgbm w gpu david cairu http www kaggl com davidcairuz featur engin lightgbm w gpu
1135,"['### Import libraries and data, reduce memory usage']",import librari data reduc memori usag
1136,"['### Some Feature Engineering\n', '\n', 'drop columns, count encoding, aggregation, fillna']",featur engin drop column count encod aggreg fillna
1137,['### XGB model and training'],xgb model train
1138,"['# Prepare data and model\n', 'I use only numeric cols. In order to use categorical features we need to get their embeddings first. \n', 'Time and ID is not included, because I want to check is there information about time in other features.']",prepar data model use numer col order use categor featur need get embed first time id includ want check inform time featur
1139,"['### Model\n', 'Tanh in the output of the decoder is good choice for visualisation: all objects will be projected on square from -1 to 1.']",model tanh output decod good choic visualis object project squar 1 1
1140,"[""# Let's look at 2d density\n"", 'It seems like there is some clusters']",let look 2d densiti seem like cluster
1141,"['### How fraudent transactions is distributed?\n', 'Looks like fraud is distributed almost like normal transactions, but there is some regions where fraudent transactions appear more often.']",fraudent transact distribut look like fraud distribut almost like normal transact region fraudent transact appear often
1142,"['### Is there time leak in numerical features?\n', 'Obviously there is information about time.']",time leak numer featur obvious inform time
1143,"['### What causes clustering?\n', 'Colouring by ProductCD gives answer for this question. Remember that categorical features was not included in visualization. It means that numerical features distribution in different categories is different. I think, we need to separate models for different ProductCD.']",caus cluster colour productcd give answer question rememb categor featur includ visual mean numer featur distribut differ categori differ think need separ model differ productcd
1144,"['# What about test?\n', 'All the same.']",test
1145,"[""# THAT'S ALL, FOLKS!""]",folk
1146,"['<span style=""font-family:Calibri; font-size:3em; color:blue"">IEEE Fraud Detection</span>\n', '\n', '<br>\n', '<img src=""https://cdn.datafloq.com/cache/blog_pictures/878x531/fraud-analytics-protect-banking-sector.jpg"" width=""500"" height=""600"">\n', '<br>\n', '\n', '\n', '**Why fraud detection?**\n', '> Fraud is a billion-dollar business and it is increasing every year. The PwC global economic crime survey of 2018[1] found that half (49 percent) of the 7,200 companies they surveyed had experienced fraud of some kind. This is an increase from the PwC 2016 study in which slightly more than a third of organizations surveyed (36%) had experienced economic crime.\n', '\n', '\n', 'This competition is a **binary classification** problem - i.e. our target variable is a binary attribute (Is the user making the click fraudlent or not?) and our goal is to classify users into ""fraudlent"" or ""not fraudlent"" as well as possible.\n', '\n', ""Unlike metrics such as ```LogLoss```, the **AUC score** only depends on how well you well you can separate the two classes. In practice, this means that only the order of your predictions matter, as a result of this, any rescaling done to your model's output probabilities will have no effect on your score. [click here to read more about AUC-ROC](https://stats.stackexchange.com/questions/132777/what-does-auc-stand-for-and-what-is-it)\n"", '\n', ""<img src='https://upload.wikimedia.org/wikipedia/commons/6/6b/Roccurves.png' width=300 height=300>\n"", '\n', '\n', '### Content\n', '\n', '- Data exploration\n', '- Missing Data.\n', '- Imbalanced problem.\n', '\n', '\n', '- Plots\n', '    - Distribution plots\n', '    - Count plots\n', '    - Unique values\n', '    - Groups\n', '    \n', '    \n', '- Memory reduction  \n', '\n', '- PCA\n', '\n', '\n', '- Models\n', '    - XGBoost Model.\n', '    - LGBM\n', '    \n', '**Remember the <span style=""color:red"">upvote</span> button is next to the fork button, and it\'s free too! ;)**\n', '\n', '----\n', '\n', '### References:\n', '\n', '- https://www.kaggle.com/artgor/eda-and-models/data\n', '- https://www.kaggle.com/artkulak/ieee-fraud-simple-baseline-0-9383-lb\n', '- https://www.kaggle.com/robikscube/ieee-fraud-detection-first-look-and-eda\n', '- https://www.kaggle.com/mjbahmani/reducing-memory-size-for-ieee\n', '\n', '<br>']",span style font famili calibri font size 3em color blue ieee fraud detect span br img src http cdn datafloq com cach blog pictur 878x531 fraud analyt protect bank sector jpg width 500 height 600 br fraud detect fraud billion dollar busi increas everi year pwc global econom crime survey 2018 1 found half 49 percent 7 200 compani survey experienc fraud kind increas pwc 2016 studi slightli third organ survey 36 experienc econom crime competit binari classif problem e target variabl binari attribut user make click fraudlent goal classifi user fraudlent fraudlent well possibl unlik metric logloss auc score depend well well separ two class practic mean order predict matter result rescal done model output probabl effect score click read auc roc http stat stackexchang com question 132777 auc stand img src http upload wikimedia org wikipedia common 6 6b roccurv png width 300 height 300 content data explor miss data imbalanc problem plot distribut plot count plot uniqu valu group memori reduct pca model xgboost model lgbm rememb span style color red upvot span button next fork button free refer http www kaggl com artgor eda model data http www kaggl com artkulak ieee fraud simpl baselin 0 9383 lb http www kaggl com robikscub ieee fraud detect first look eda http www kaggl com mjbahmani reduc memori size ieee br
1147,"['# Data\n', '\n', '\n', 'In this competition you are predicting the probability that an online transaction is fraudulent, as denoted by the binary target ```isFraud```.\n', '\n', 'The data is broken into two files **identity** and **transaction**, which are joined by ```TransactionID```. \n', '\n', '> Note: Not all transactions have corresponding identity information.\n', '\n', '**Categorical Features - Transaction**\n', '\n', '- ProductCD\n', '- emaildomain\n', '- card1 - card6\n', '- addr1, addr2\n', '- P_emaildomain\n', '- R_emaildomain\n', '- M1 - M9\n', '\n', '**Categorical Features - Identity**\n', '\n', '- DeviceType\n', '- DeviceInfo\n', '- id_12 - id_38\n', '\n', '**The TransactionDT feature is a timedelta from a given reference datetime (not an actual timestamp).**\n', '\n', '**Files**\n', '\n', '- train_{transaction, identity}.csv - the training set\n', '- test_{transaction, identity}.csv - the test set (**you must predict the isFraud value for these observations**)\n', '- sample_submission.csv - a sample submission file in the correct format\n']",data competit predict probabl onlin transact fraudul denot binari target isfraud data broken two file ident transact join transactionid note transact correspond ident inform categor featur transact productcd emaildomain card1 card6 addr1 addr2 p emaildomain r emaildomain m1 m9 categor featur ident devicetyp deviceinfo id 12 id 38 transactiondt featur timedelta given refer datetim actual timestamp file train transact ident csv train set test transact ident csv test set must predict isfraud valu observ sampl submiss csv sampl submiss file correct format
1148,"['**Interactive Plots Utils**\n', '> from https://www.kaggle.com/kabure/baseline-fraud-detection-eda-interactive-views (more about Interactive plots there)']",interact plot util http www kaggl com kabur baselin fraud detect eda interact view interact plot
1149,['**Load data**'],load data
1150,"['OK, there are a lot of **NaN** and **interesting columns**: \n', '\n', '- ``` C1, C2 ... D1, V300, V339 ... ``` \n', '- ``` id_01 ... id_38``` \n', '\n', ""The columns with those names don't look friendly.\n"", ""Apparently we don't have **dates**.""]",ok lot nan interest column c1 c2 d1 v300 v339 id 01 id 38 column name look friendli appar date
1151,"['### 1st problem: NaN\n', '\n', 'Remember\n', '> Not all transactions have corresponding identity information']",1st problem nan rememb transact correspond ident inform
1152,['**train_transaction**'],train transact
1153,['**train_identity**'],train ident
1154,"['### 2nd Problem ...\n', '\n', 'Notice how **imbalanced** is our original dataset! Most of the transactions are non-fraud. If we use this dataframe as the base for our predictive models and analysis we might get a lot of errors and our algorithms will probably overfit since it will ""assume"" that most transactions are not fraud. But we don\'t want our model to assume, we want our model to detect patterns that give signs of fraud!\n', '\n', '**Imbalance** means that the number of data points available for different the classes is different\n', '\n', ""<img src='https://www.datascience.com/hs-fs/hubfs/imbdata.png?t=1542328336307&width=487&name=imbdata.png'>""]",2nd problem notic imbalanc origin dataset transact non fraud use datafram base predict model analysi might get lot error algorithm probabl overfit sinc assum transact fraud want model assum want model detect pattern give sign fraud imbal mean number data point avail differ class differ img src http www datasci com h f hubf imbdata png 1542328336307 width 487 name imbdata png
1155,"['# Time vs fe\n', '> **The TransactionDT feature is a timedelta from a given reference datetime (not an actual timestamp).**\n', '\n', '**Important ! read the post [The timespan of the dataset is 1 year ?\n', '](https://www.kaggle.com/c/ieee-fraud-detection/discussion/100071#latest-577632) by Suchith**\n', '\n', '```\n', 'Train: min = 86400 max = 15811131\n', 'Test: min = 18403224 max = 34214345\n', '```\n', '\n', ""The difference train.min() and test.max() is ```x = 34214345 - 86400 = 34127945``` but we don't know is it in seconds,minutes or hours.\n"", '\n', '```\n', 'Time span of the total dataset is 394.9993634259259 days\n', 'Time span of Train dataset is  181.99920138888888 days\n', 'Time span of Test dataset is  182.99908564814814 days\n', 'The gap between train and test is 30.00107638888889 days\n', '```\n', '\n', 'If it is in seconds then dataset timespan will be ```x/(3600*24*365) = 1.0821``` years which seems reasonable to me. So if the **transactionDT** is in **seconds** then\n', '\n', '```\n', 'Time span of the total dataset is 394.9993634259259 days\n', 'Time span of Train dataset is  181.99920138888888 days\n', 'Time span of Test dataset is  182.99908564814814 days\n', 'The gap between train and test is 30.00107638888889 days\n', '```\n', '\n', '\n', '![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F2370491%2Fc9bf5af5e902595b737df5470adc193b%2Fdownload-1.png?generation=1563312982845419&alt=media)\n', '\n', '**source: [FChmiel](https://www.kaggle.com/fchmiel)**\n', '<br>']",time v fe transactiondt featur timedelta given refer datetim actual timestamp import read post timespan dataset 1 year http www kaggl com c ieee fraud detect discus 100071 latest 577632 suchith train min 86400 max 15811131 test min 18403224 max 34214345 differ train min test max x 34214345 86400 34127945 know second minut hour time span total dataset 394 9993634259259 day time span train dataset 181 99920138888888 day time span test dataset 182 99908564814814 day gap train test 30 00107638888889 day second dataset timespan x 3600 24 365 1 0821 year seem reason transactiondt second time span total dataset 394 9993634259259 day time span train dataset 181 99920138888888 day time span test dataset 182 99908564814814 day gap train test 30 00107638888889 day http www googleapi com download storag v1 b kaggl user content inbox 2f2370491 2fc9bf5af5e902595b737df5470adc193b 2fdownload 1 png gener 1563312982845419 alt medium sourc fchmiel http www kaggl com fchmiel br
1156,"['```24.4%``` of TransactionIDs in train (144233 / 590540) have an associated train_identity.\n', '\n', '```28.0%``` of TransactionIDs in test (144233 / 590540) have an associated train_identity.']",24 4 transactionid train 144233 590540 associ train ident 28 0 transactionid test 144233 590540 associ train ident
1157,"['**TransactionDT** is not a timestamp, but somehow we use it to measure time.']",transactiondt timestamp somehow use measur time
1158,"[""As you can see it seems that train and test transaction dates don't overlap, so it would be prudent to use time-based split for validation. Rob discovered this here: https://www.kaggle.com/robikscube/ieee-fraud-detection-first-look-and-eda.\n"", '\n', 'Also we can see the **30 days** gap between train and test.\n']",see seem train test transact date overlap would prudent use time base split valid rob discov http www kaggl com robikscub ieee fraud detect first look eda also see 30 day gap train test
1159,"['Also you should read this post by Rob [Plotting features over time shows something.... interesting\n', ""](https://www.kaggle.com/c/ieee-fraud-detection/discussion/100167#latest-577688) he discovered a weird correlation between C and D features, and that's why I do the following plots :)""]",also read post rob plot featur time show someth interest http www kaggl com c ieee fraud detect discus 100167 latest 577688 discov weird correl c featur follow plot
1160,['### isFraud vs time'],isfraud v time
1161,"['### C features: C1, C2 ... C14']",c featur c1 c2 c14
1162,['### D features: D1 ... D15'],featur d1 d15
1163,"['OK, the problem here is that ```D``` features are mostly NaNs!']",ok problem featur mostli nan
1164,"[""If we consider D features, de 58.15% are missing values ... Let's plot without missing values""]",consid featur de 58 15 miss valu let plot without miss valu
1165,['### M features: M1 .. M9'],featur m1 m9
1166,['## V150'],v150
1167,"['<br>\n', '# Groups']",br group
1168,['Remove ```.head(20)``` and check the entire list.'],remov head 20 check entir list
1169,"['<br>\n', '# TransactionAmt']",br transactionamt
1170,['# Unique Values'],uniqu valu
1171,['### D Features'],featur
1172,['### C features'],c featur
1173,['### V features'],v featur
1174,['### id_code'],id code
1175,"['<br>\n', '# Categorical Features\n', '\n', '- ProductCD\n', '- emaildomain\n', '- card1 - card6\n', '- addr1, addr2\n', '- P_emaildomain\n', '- R_emaildomain\n', '- M1 - M9\n', '- DeviceType\n', '- DeviceInfo\n', '- id_12 - id_38']",br categor featur productcd emaildomain card1 card6 addr1 addr2 p emaildomain r emaildomain m1 m9 devicetyp deviceinfo id 12 id 38
1176,['### ProductCD'],productcd
1177,['### Device Type & Device Info'],devic type devic info
1178,['**Device information**'],devic inform
1179,['### Card'],card
1180,"[""As you can see, ``` Card 1``` column is given as Categorical but it is behaving like Continuous Data. Having '13553' unique Values.\n"", '\n', '> **From organizer: ** This is a encoded categorical variable. \n', ""The dataset contains many high-cardinality variables, and it's challenge to model such variable. Meanwhile, it's worthy to see how you talented people deal with them.\n"", '\n', 'Check this post: https://www.kaggle.com/c/ieee-fraud-detection/discussion/100340#latest-578626']",see card 1 column given categor behav like continu data 13553 uniqu valu organ encod categor variabl dataset contain mani high cardin variabl challeng model variabl meanwhil worthi see talent peopl deal check post http www kaggl com c ieee fraud detect discus 100340 latest 578626
1181,['### Email Domain'],email domain
1182,['It seems that criminals prefer gmail'],seem crimin prefer gmail
1183,['# Memory reduction'],memori reduct
1184,['**Merge transaction & identity + Label Encoder**'],merg transact ident label encod
1185,"['### Reduce Memory Usage\n', '> 2 options\n', '\n', '**Note** Using te option1 the missing values are encoded as -1, you have to update the XGBoost model and set ```missing=-1```']",reduc memori usag 2 option note use te option1 miss valu encod 1 updat xgboost model set miss 1
1186,['this takes 6-7 mins. You can click and check the ``` output ```'],take 6 7 min click check output
1187,['### Now memory should be around 4 GB !'],memori around 4 gb
1188,"['**Drop some columns**\n', '> from: https://www.kaggle.com/jazivxt/safe-box/notebook']",drop column http www kaggl com jazivxt safe box notebook
1189,['**Fill NaN**'],fill nan
1190,['# PCA'],pca
1191,['**PCA 2 components**'],pca 2 compon
1192,"['<br>\n', '# Models\n', '---\n']",br model
1193,"['## XGBoost Model + FE Importance\n', '\n', '> This part is from [can_we_beat_it](https://www.kaggle.com/konradb/can-we-beat-it) by Konrad\n', '\n', '> Also check this kernel [IEEE Fraud Simple Baseline [0.9383 LB]](https://www.kaggle.com/artkulak/ieee-fraud-simple-baseline-0-9383-lb)']",xgboost model fe import part beat http www kaggl com konradb beat konrad also check kernel ieee fraud simpl baselin 0 9383 lb http www kaggl com artkulak ieee fraud simpl baselin 0 9383 lb
1194,"['**Important** Check the [XGB official documentation](https://xgboost.readthedocs.io/en/latest/parameter.html) in order to know more about the parameters.\n', '\n', 'Also check this thread [CV vs Public LB](https://www.kaggle.com/c/ieee-fraud-detection/discussion/100255#latest-578503)']",import check xgb offici document http xgboost readthedoc io en latest paramet html order know paramet also check thread cv v public lb http www kaggl com c ieee fraud detect discus 100255 latest 578503
1195,"['### Importance PLOT\n', '> last FOLD']",import plot last fold
1196,['# Submission'],submiss
1197,"['### To be continued ...\n', ""**I'll keep updating almost every day :)**""]",continu keep updat almost everi day
1198,"['# IEEE-CIS Fraud Detection &mdash; LightGBM Split Points\n', '\n', 'This notebook shows some techniques to snoop on the gradient boosting process used by LightGBM - using its own APIs.\n', '\n', 'By counting the split points used in the decision trees, we can see the ways the algorithm divides the input space up. This may lead to new insights about what indicates fraud, and may help in smoothing or binning the data to reduce splits that model only noise.\n', '\n', 'For more info on LightGBM see [pdf by Microsoft][3] or the [LightGBM github][4].\n', '\n', 'For another example of gradient boosting model analysis with XGBoost see the great [xgbfi][2] tool by [Faron][1].\n', '\n', '___\n', '\n', 'We start by building a model...\n', '\n', ' [1]: https://www.kaggle.com/mmueller\n', ' [2]: https://github.com/Far0n/xgbfi\n', ' [3]: https://www.microsoft.com/en-us/research/wp-content/uploads/2017/11/lightgbm.pdf\n', ' [4]: https://github.com/Microsoft/LightGBM\n']",ieee ci fraud detect mdash lightgbm split point notebook show techniqu snoop gradient boost process use lightgbm use api count split point use decis tree see way algorithm divid input space may lead new insight indic fraud may help smooth bin data reduc split model nois info lightgbm see pdf microsoft 3 lightgbm github 4 anoth exampl gradient boost model analysi xgboost see great xgbfi 2 tool faron 1 start build model 1 http www kaggl com mmueller 2 http github com far0n xgbfi 3 http www microsoft com en u research wp content upload 2017 11 lightgbm pdf 4 http github com microsoft lightgbm
1199,['Add count features...'],add count featur
1200,['Add some simple extra features.'],add simpl extra featur
1201,"['Simple time based validation split, first 75% is training data, rest is validation.']",simpl time base valid split first 75 train data rest valid
1202,"['## Note &mdash; TransactionDT\n', '\n', 'I will use all columns as features, even `TransactionDT` which is terrible as a feature - none of the test set values overlap with the training set values. By leaving it in here though we get to see if there are any hotspots of `TransactionDT` that get frequently used as a split point, showing us a potential *regime change* or shift in fraud behaviour.']",note mdash transactiondt use column featur even transactiondt terribl featur none test set valu overlap train set valu leav though get see hotspot transactiondt get frequent use split point show u potenti regim chang shift fraud behaviour
1203,"[""Save the model - it saves the trees in an easy to parse text format. (The file won't be used here but it is useful in general to save.)""]",save model save tree easi par text format file use use gener save
1204,"['# Booster.dump_model()\n', '\n', 'The returned LightGBM model format is hierarchical, trees are nested `dict` objects containing `left_child` and `right_child` subtrees. Walking over the trees and summarizing the splits can be done with a short recursive function...\n', '\n', '    tree_info  - list of dicts\n', '    (each contains):\n', '        tree_structure\n', '            left_child\n', '            right_child\n', '\n', ""The `dump_model()` information records 'gain' at each split, and we simply re-use that.""]",booster dump model return lightgbm model format hierarch tree nest dict object contain left child right child subtre walk tree summar split done short recurs function tree info list dict contain tree structur left child right child dump model inform record gain split simpli use
1205,"['Each feature indexes a Counter object in the `split_points` dict. In each Counter, the keys are feature values, and the values are sum of gain, for example, 3.5 is the most used value in feature `C1`:']",featur index counter object split point dict counter key featur valu valu sum gain exampl 3 5 use valu featur c1
1206,"['Dump all the split point data to an xlsx file (can be opened with open-source *Open Office* or *[Libre Office][1]*)\n', '\n', ' [1]: https://www.libreoffice.org/download/download/']",dump split point data xlsx file open open sourc open offic libr offic 1 1 http www libreoffic org download download
1207,"['# Plotting Code\n', '\n', ""Warning: this only shows the 50 split points with the most gain, so the x-axis will be a bit nonlinear, some values won't appear. See the xlsx file for all the values.""]",plot code warn show 50 split point gain x axi bit nonlinear valu appear see xlsx file valu
1208,"['# Plots For IEEE Features\n', '\n', 'All the features with 4 or more unique values are shown (to avoid ""Too many output files (max 500)"" error).\n', '\n', '## Notes\n', '\n', 'Most of the split points have long decimal values like `379.00000000000006` - the LightGBM algorithm only sees binned data, so it sets split thresholds as values [halfway between neighbouring bin lower/upper edges][6], but bumped upwards a tiny fraction using `std::nextafter` in the [C++ standard library][5], resulting in strangely precise [floating point format][1] values :)\n', '\n', 'Zero is checked for using a [kZeroThreshold = 1e-35f][7] variable - this comes out of the model as a split point of 1.0000000180025095e-35 &mdash; a tiny number. When you see that, think *zero*.\n', '\n', 'Split points for categorical dtypes depends on the `max_cat_to_onehot` which I have set to 128 - so categoricals in this data set are treated with a one-vs-all split. This means `feature==value` in the node split test, instead of the usual `feature<=value`. `max_cat_to_onehot` is by default set to 4, meaning categories with more values than this use splits based on target statistics, and the resulting split points have values like `1||3||5||7||8||9` which indicate which category codes go down the *left* branch. (But this is hard to show in bar charts... hence I used *one-vs-all splits*.)\n', '\n', 'LightGBM keeps a separate bin for NaN values and at all node tests, records whether that bin goes left/right separately - this is not shown here (yet! Upvote to make me attempt something!)\n', '\n', '## What to Look For\n', '\n', ""In some ways what we **don't** see is more interesting than what we **do**. As with normal feature importances: if we see a feature is not used at all it is clearly redudant and should be removed. So seeing low gain (0) is very reliable but seeing high gain can *can* be misleading - it may be fitting noise.\n"", '\n', ""If there is **one prominent peak** it means the feature acts a bit like a boolean, and perhaps would be better fed into the model that way (e.g. seeing a split value of 100, for feature 'foo', you could instead change column 'foo' to `foo<=100`.)  \n"", '\n', 'Similarly if there are **several prominent peaks** it could imply the feature should be discretized/binned in a pre-processing step, as the less-often used split points may just be picking up on noise. See this [interesting old discussion thread on this subject of feature discretization][2].\n', '\n', 'For the TimeOfDay feature you may like to check out [my time series heatmaps notebook][4] that shows the density of transactions over time, and clearly indicates night time.\n', '\n', ' [1]: https://en.wikipedia.org/wiki/Double-precision_floating-point_format\n', ' [2]: https://www.kaggle.com/c/porto-seguro-safe-driver-prediction/discussion/43886\n', ' [3]: https://www.kaggle.com/tilii7\n', ' [4]: https://www.kaggle.com/jtrotman/ieee-fraud-time-series-heatmaps\n', ' [5]: https://en.cppreference.com/w/cpp/numeric/math/nextafter\n', ' [6]: https://github.com/microsoft/LightGBM/blob/master/src/io/bin.cpp\n', ' [7]: https://github.com/microsoft/LightGBM/blob/master/include/LightGBM/meta.h ']",plot ieee featur featur 4 uniqu valu shown avoid mani output file max 500 error note split point long decim valu like 379 00000000000006 lightgbm algorithm see bin data set split threshold valu halfway neighbour bin lower upper edg 6 bump upward tini fraction use std nextaft c standard librari 5 result strang precis float point format 1 valu zero check use kzerothreshold 1e 35f 7 variabl come model split point 1 0000000180025095e 35 mdash tini number see think zero split point categor dtype depend max cat onehot set 128 categor data set treat one v split mean featur valu node split test instead usual featur valu max cat onehot default set 4 mean categori valu use split base target statist result split point valu like 1 3 5 7 8 9 indic categori code go left branch hard show bar chart henc use one v split lightgbm keep separ bin nan valu node test record whether bin goe left right separ shown yet upvot make attempt someth look way see interest normal featur import see featur use clearli redud remov see low gain 0 reliabl see high gain mislead may fit nois one promin peak mean featur act bit like boolean perhap would better fed model way e g see split valu 100 featur foo could instead chang column foo foo 100 similarli sever promin peak could impli featur discret bin pre process step le often use split point may pick nois see interest old discus thread subject featur discret 2 timeofday featur may like check time seri heatmap notebook 4 show densiti transact time clearli indic night time 1 http en wikipedia org wiki doubl precis float point format 2 http www kaggl com c porto seguro safe driver predict discus 43886 3 http www kaggl com tilii7 4 http www kaggl com jtrotman ieee fraud time seri heatmap 5 http en cpprefer com w cpp numer math nextaft 6 http github com microsoft lightgbm blob master src io bin cpp 7 http github com microsoft lightgbm blob master includ lightgbm meta h
1209,"['# Gain Over Time\n', '\n', 'This part is more for illustration/teaching about gradient boosting.\n', '\n', 'As well as counting split points, we can look at how feature gain evolves as trees are added to the model.\n', '\n', 'In the gradient boosting learning process, each tree adds something to the training set predictions that moves the overall predictions closer to the target. It takes small steps towards lowering the loss function. Early trees are more like standard decision trees, fitting the big patterns. Later trees are more specialised, correcting small deviations, fine-grained wrinkles in the loss function: often little patterns, sometimes noise.\n', '\n', 'As features are incorporated into the model in early trees, their predictive power can run out, which is most notable for boolean features; at some point the existing predictions have accounted for all of the variance of the feature and they are no longer used in new trees.\n', '\n', 'We can see this by looking at gain statistics over time by passing the `iteration` parameter to the `feature_importance()` method.\n']",gain time part illustr teach gradient boost well count split point look featur gain evolv tree ad model gradient boost learn process tree add someth train set predict move overal predict closer target take small step toward lower loss function earli tree like standard decis tree fit big pattern later tree specialis correct small deviat fine grain wrinkl loss function often littl pattern sometim nois featur incorpor model earli tree predict power run notabl boolean featur point exist predict account varianc featur longer use new tree see look gain statist time pas iter paramet featur import method
1210,['`V258` and `V258_count` reach high gain by 20% of the way through and are not used much after that. `card1` and `card2` are still being used with high gain throughout.'],v258 v258 count reach high gain 20 way use much card1 card2 still use high gain throughout
1211,"['The blue bar indicates gain at 40% of the way through the learning process, and red marks the gain at the end.']",blue bar indic gain 40 way learn process red mark gain end
1212,"['Here we see `V258` and `V258_count` have *run out of steam* early, whilst other features are still gaining in importance...']",see v258 v258 count run steam earli whilst featur still gain import
1213,"['A different plot, show the most used features at the 20% point of training, and how their gain evolves after that...']",differ plot show use featur 20 point train gain evolv
1214,"['# Conclusions\n', '\n', 'Now we can inspect trained models to see **which points** in the feature space matter for fraud detection... You can build this in to your pipeline to help with reducing the resolution of the data in later modelling iterations, and aid further feature engineering.\n', '\n', 'If this kernel gets enough votes I will apply it to adversarial validation too to see **where** in the feature space the train and test set differ the most. Perhaps this could even be integrated into an *auto-relaxing* function that buckets the data for us in a way that makes the  train and test sets more similar, without any tedious manual inspection of plots :)\n', '\n', '<font color=red>Update</font>: [adversarial version here][2].\n', '\n', '___\n', '\n', 'A note to any n00bs reading: the original features used here are only a starting point, used just to demonstrate. If (say) `DeviceInfo` of `hi6210sft Build/MRA58K` comes along in the training set and makes a fast burst of transactions (all marked fraud), then appears in the test set but spread out and on many separate days, it does not make sense to predict a high fraud likelihood, simply because of that one feature. Features that capture *event* timing & behaviour are needed :)\n', '\n', 'For inspiration you should check out [an **extensive** index of **winning** and high ranking Kaggle **solutions** here][1] (and upvote if this helps you find something useful &mdash; I guarantee there are useful links there ;)\n', '\n', ' [1]: https://www.kaggle.com/jtrotman/high-ranking-solution-posts\n', ' [2]: https://www.kaggle.com/jtrotman/ieee-fraud-adversarial-lgb-split-points\n']",conclus inspect train model see point featur space matter fraud detect build pipelin help reduc resolut data later model iter aid featur engin kernel get enough vote appli adversari valid see featur space train test set differ perhap could even integr auto relax function bucket data u way make train test set similar without tediou manual inspect plot font color red updat font adversari version 2 note n00b read origin featur use start point use demonstr say deviceinfo hi6210sft build mra58k come along train set make fast burst transact mark fraud appear test set spread mani separ day make sen predict high fraud likelihood simpli one featur featur captur event time behaviour need inspir check extens index win high rank kaggl solut 1 upvot help find someth use mdash guarante use link 1 http www kaggl com jtrotman high rank solut post 2 http www kaggl com jtrotman ieee fraud adversari lgb split point
1215,"['# IEEE-CIS Fraud Detection Time Series Heatmaps\n', '\n', 'This notebook shows counts of transactions over time in a 2D heatmap, as a simple exploration of the time series structure of the train/test sets.\n', '\n', 'One row of pixels in each image is 1 day, with about 183 rows in each image, derived from the training set, and later on, the test set.']",ieee ci fraud detect time seri heatmap notebook show count transact time 2d heatmap simpl explor time seri structur train test set one row pixel imag 1 day 183 row imag deriv train set later test set
1216,"['TransactionDT is in seconds, with 15811131 maximum.']",transactiondt second 15811131 maximum
1217,"['183 days, put them on one row each in a heatmap, with 480 columns']",183 day put one row heatmap 480 column
1218,['So each pixel will represent 180 seconds of a day. All transactions in each 3 minute block will be counted.'],pixel repres 180 second day transact 3 minut block count
1219,"[""`TransactionID>0` is a simple way to say 'all the training set' - it is true for all rows.""]",transactionid 0 simpl way say train set true row
1220,['Take a preview look at what we made...'],take preview look made
1221,"['Looks good, now do it for all values that appear 5000 times or more, for every single column.']",look good valu appear 5000 time everi singl column
1222,['Over 300 plots :)'],300 plot
1223,['Now the V columns too:'],v column
1224,"['# Matplotlib Display\n', '\n', 'Show some plots selected for interesting features...']",matplotlib display show plot select interest featur
1225,"[""This is all transactions in the training set - I'm using the raw date values from the training data so the values are on the time axis are possibly the wrong timezone - night time appears clearly visible but about 3 hours late (depending on your lifestyle ;)\n"", '\n', 'Also a fat peak around days 20-30, and a thin (1 day) peak at about day 92.\n', '\n', 'Looking closely (right click &rarr; *View Image* helps) there is some weekly seasonality - a darker line in the mornings - presumably Sunday.']",transact train set use raw date valu train data valu time axi possibl wrong timezon night time appear clearli visibl 3 hour late depend lifestyl also fat peak around day 20 30 thin 1 day peak day 92 look close right click rarr view imag help weekli season darker line morn presum sunday
1226,['Fraud does not seem to dip so much overnight... (Note: max value is 8 - no three-minute block has more than 8 fraudulent transactions.)'],fraud seem dip much overnight note max valu 8 three minut block 8 fraudul transact
1227,['Now some covariate shifts...'],covari shift
1228,"['card1 value 7919 seems periodic, at about 30 days, with some double peaks too...']",card1 valu 7919 seem period 30 day doubl peak
1229,['seems to be correlated with card2 value 194'],seem correl card2 valu 194
1230,"['card2 is not missing at random, but missing in streaks:']",card2 miss random miss streak
1231,['card5 stops being equal to 202 at about day 80'],card5 stop equal 202 day 80
1232,['but value 126 starts appearing more about that time... perhaps they have a similar meaning?'],valu 126 start appear time perhap similar mean
1233,"['A bit hard to see, but gmail.com as R_emaildomain has a drop (dark band) around day 100.']",bit hard see gmail com r emaildomain drop dark band around day 100
1234,['A bit easier to see here in D6...'],bit easier see d6
1235,"['The ""D"" columns refer to time, as stated by organizers. D9 being 0.75 means 6pm-7pm for example:']",column refer time state organ d9 0 75 mean 6pm 7pm exampl
1236,"['Value ""S"" for ProductCD only appears late - the natural choice for a validation era:']",valu productcd appear late natur choic valid era
1237,"['More complicated expressions are possible, you could extract a decision tree path and plug it in as query string, a simple (uninsightful?) example (depth 2) is:']",complic express possibl could extract decis tree path plug queri string simpl uninsight exampl depth 2
1238,['# Test Set'],test set
1239,['Test starts at day 213:'],test start day 213
1240,"['To make test look like train, subtract 213 days, then reuse the above code. Note `day0 .. day183` in the plots now refers to test set days.']",make test look like train subtract 213 day reus code note day0 day183 plot refer test set day
1241,['Generate test set plots for offline use.'],gener test set plot offlin use
1242,"['## Test Set Plots\n', '\n', 'The leaderboard page says **This leaderboard is calculated with approximately 20% of the test data**. [This great discussion topic][1] says the public/private split is by time. So, the public LB will be day 0 - day 37, all after is the private LB...\n', '\n', ' [1]: https://www.kaggle.com/c/ieee-fraud-detection/discussion/101040\n']",test set plot leaderboard page say leaderboard calcul approxim 20 test data great discus topic 1 say public privat split time public lb day 0 day 37 privat lb 1 http www kaggl com c ieee fraud detect discus 101040
1243,"['Overall shape looks similar, perhaps night time drifts later towards the end?']",overal shape look similar perhap night time drift later toward end
1244,"['Date columns appear to mean the same thing, no daylight savings shift...']",date column appear mean thing daylight save shift
1245,"[""D15 continues it's erratic behaviour - and is different between public/private periods - though it is only null for 12069 rows (of ~500k), so perhaps it's ok to ignore this. Or just fillna(0) or fillna(1).""]",d15 continu errat behaviour differ public privat period though null 12069 row 500k perhap ok ignor fillna 0 fillna 1
1246,"['card1 value 7919 seems familiar, similar to train...']",card1 valu 7919 seem familiar similar train
1247,"['Above we saw that value ""S"" for ProductCD only appears late in the train set - but here in the test set it is more uniform, though around the same number of transactions.']",saw valu productcd appear late train set test set uniform though around number transact
1248,"['However - value ""R"" appears quite late in the *test* set - in the private LB zone. This is similar to how ""S"" appeared only late in the *train* set.']",howev valu r appear quit late test set privat lb zone similar appear late train set
1249,"['Similarly, ""H"" for ProductCD ramps up in the test set.']",similarli h productcd ramp test set
1250,"['# Clean Up\n', '\n', 'Compress some of the generated pngs - Kaggle does not allow more than 500 output files.']",clean compress gener png kaggl allow 500 output file
1251,"['## Notes\n', '\n', 'Much more is possible here, for example, count all transactions, then count subsets like\n', ' - `ProductCD==X`\n', ' \n', 'and\n', ' - `ProductCD==X and isFraud==1` \n', ' \n', 'then apply Bayes rule to get **p(isFraud | ProductCD==X)** and color cells in accordingly, e.g. p(isFraud) as the red channel in RGB.\n', '\n', 'Also `np.add.at(c, ts, 1)` can be changed to `np.add.at(c, ts, df.isFraud)` to accumulate values instead of simply count transctions.\n', '\n', '____\n', '\n', '*to be continued...*  **(HOWEVER: feel free to fork this notebook and try it out yourself :)**']",note much possibl exampl count transact count subset like productcd x productcd x isfraud 1 appli bay rule get p isfraud productcd x color cell accordingli e g p isfraud red channel rgb also np add c t 1 chang np add c t df isfraud accumul valu instead simpli count transction continu howev feel free fork notebook tri
1252,"['# Feature Engineering \n', '## ~Almost~ all features of IEEE fraud detectetion dataset \n', '\n', '\n', '## Purpose of the kernel: \n', ""In the last week I worked in all features trying to improve my model. I've worked in almost all features and I decided to share with the kaggle fellows. \n"", '\n', '### NOTE: Maybe not all features could be useful, but I think that this work could help other kagglers in Data manipulation or to create another interesting feature\n']",featur engin almost featur ieee fraud detectet dataset purpos kernel last week work featur tri improv model work almost featur decid share kaggl fellow note mayb featur could use think work could help kaggler data manipul creat anoth interest featur
1253,"['## <font color=""red"">Please if this kernel were useful for you, please <b>UPVOTE</b> the kernel</font>']",font color red plea kernel use plea b upvot b kernel font
1254,['## Importing Libraries'],import librari
1255,['## Some functions'],function
1256,['## Concatenating train and test'],concaten train test
1257,['# V Features'],v featur
1258,['## Shape after transforming V features'],shape transform v featur
1259,['# C Features'],c featur
1260,['## Shape after transforming C features'],shape transform c featur
1261,"['# M features\n', ""- Only feeling Na's""]",featur feel na
1262,['# Id datasets'],id dataset
1263,"[""## Filling NaN's in Numerical features\n"", ""id_num_cols = ['id_01', 'id_02', 'id_03', 'id_04', 'id_05', \n"", ""               'id_06', 'id_07', 'id_08', 'id_09', 'id_10', 'id_11']\n"", '\n', 'for col in id_num_cols:\n', '    df_id[col].fillna(df_id[col].min() - 10, inplace=True)\n', '\n', '## Categorical features\n', ""id_num = ['id_13', 'id_14', 'id_17', 'id_18', 'id_19', 'id_20', \n"", ""          'id_21', 'id_21', 'id_22', 'id_24', 'id_25', 'id_26']\n"", '\n', 'for col in id_num:\n', '    df_id[col].fillna(df_id[col].min() - 100, inplace=True)\n', '    \n', ""to_fill_none = ['id_15', 'id_16', 'id_27', \n"", ""                'id_28', 'id_29', 'id_35', \n"", ""                'id_36', 'id_37', 'id_38']\n"", '\n', 'for col in to_fill_none:\n', ""    df_id[col].fillna('None', inplace=True)\n"", '    \n', ""df_id['device_name'] = df_id['DeviceInfo'].str.split('/', expand=True)[0]\n"", ""df_id['device_version'] = df_id['DeviceInfo'].str.split('/', expand=True)[1]\n"", ""df_id.drop('DeviceInfo', axis=1, inplace=True)\n"", ""df_id['device_name'].fillna('None', inplace=True)\n"", ""df_id['device_version'].fillna('None', inplace=True)\n"", ""df_id['DeviceType'].fillna('None', inplace=True)\n"", '\n', ""df_id['OS_id_30'] = df_id['id_30'].str.split(' ', expand=True)[0]\n"", ""df_id['version_id_30'] = df_id['id_30'].str.split(' ', expand=True)[1]\n"", ""df_id['OS_id_30'].fillna('None', inplace=True)\n"", ""df_id['version_id_30'].fillna('None', inplace=True)\n"", ""df_id.drop('id_30', axis=1, inplace=True)\n"", '\n', ""df_id['browser_id_31'] = df_id['id_31'].str.split(' ', expand=True)[0]\n"", ""df_id['version_id_31'] = df_id['id_31'].str.split(' ', expand=True)[1]\n"", ""df_id['browser_id_31'].fillna('None', inplace=True)\n"", ""df_id['version_id_31'].fillna('None', inplace=True)\n"", ""df_id.drop('id_31', axis=1, inplace=True)\n"", '\n', ""df_id['screen_width'] = df_id['id_33'].str.split('x', expand=True)[0]\n"", ""df_id['screen_height'] = df_id['id_33'].str.split('x', expand=True)[1]\n"", ""df_id['screen_width'].fillna(-1, inplace=True)\n"", ""df_id['screen_height'].fillna(-1, inplace=True)\n"", ""df_id.drop('id_33', axis=1, inplace=True)\n"", '\n', ""df_id['id_34'] = df_id['id_34'].str.split(':', expand=True)[1]\n"", ""df_id['id_23'] = df_id['id_23'].str.split(':', expand=True)[1]\n"", ""df_id['id_23'].fillna('None', inplace=True)\n"", ""df_id['id_34'].fillna(-2, inplace=True)\n"", '\n', ""to_fill_minone = 'id_32'   \n"", ""df_id['id_32'].fillna(df_id['id_32'].min() -10, inplace=True)\n"", '\n', '## Device renaming\n', ""df_id.loc[df_id['device_name'].str.contains('SM', na=False), 'device_name'] = 'Samsung'\n"", ""df_id.loc[df_id['device_name'].str.contains('SAMSUNG', na=False), 'device_name'] = 'Samsung'\n"", ""df_id.loc[df_id['device_name'].str.contains('GT-', na=False), 'device_name'] = 'Samsung'\n"", ""df_id.loc[df_id['device_name'].str.contains('Moto G', na=False), 'device_name'] = 'Motorola'\n"", ""df_id.loc[df_id['device_name'].str.contains('Moto', na=False), 'device_name'] = 'Motorola'\n"", ""df_id.loc[df_id['device_name'].str.contains('moto', na=False), 'device_name'] = 'Motorola'\n"", ""df_id.loc[df_id['device_name'].str.contains('LG-', na=False), 'device_name'] = 'LG'\n"", ""df_id.loc[df_id['device_name'].str.contains('rv:', na=False), 'device_name'] = 'RV'\n"", ""df_id.loc[df_id['device_name'].str.contains('HUAWEI', na=False), 'device_name'] = 'Huawei'\n"", ""df_id.loc[df_id['device_name'].str.contains('ALE-', na=False), 'device_name'] = 'Huawei'\n"", ""df_id.loc[df_id['device_name'].str.contains('-L', na=False), 'device_name'] = 'Huawei'\n"", ""df_id.loc[df_id['device_name'].str.contains('Blade', na=False), 'device_name'] = 'ZTE'\n"", ""df_id.loc[df_id['device_name'].str.contains('BLADE', na=False), 'device_name'] = 'ZTE'\n"", ""df_id.loc[df_id['device_name'].str.contains('Linux', na=False), 'device_name'] = 'Linux'\n"", ""df_id.loc[df_id['device_name'].str.contains('XT', na=False), 'device_name'] = 'Sony'\n"", ""df_id.loc[df_id['device_name'].str.contains('HTC', na=False), 'device_name'] = 'HTC'\n"", ""df_id.loc[df_id['device_name'].str.contains('ASUS', na=False), 'device_name'] = 'Asus'\n"", '\n', 'df_id.loc[df_id.device_name.isin(df_id.device_name.value_counts()[df_id.device_name.value_counts() < 200].index), \'device_name\'] = ""Others""\n', ""df_id['has_id'] = 1""]",fill nan numer featur id num col id 01 id 02 id 03 id 04 id 05 id 06 id 07 id 08 id 09 id 10 id 11 col id num col df id col fillna df id col min 10 inplac true categor featur id num id 13 id 14 id 17 id 18 id 19 id 20 id 21 id 21 id 22 id 24 id 25 id 26 col id num df id col fillna df id col min 100 inplac true fill none id 15 id 16 id 27 id 28 id 29 id 35 id 36 id 37 id 38 col fill none df id col fillna none inplac true df id devic name df id deviceinfo str split expand true 0 df id devic version df id deviceinfo str split expand true 1 df id drop deviceinfo axi 1 inplac true df id devic name fillna none inplac true df id devic version fillna none inplac true df id devicetyp fillna none inplac true df id o id 30 df id id 30 str split expand true 0 df id version id 30 df id id 30 str split expand true 1 df id o id 30 fillna none inplac true df id version id 30 fillna none inplac true df id drop id 30 axi 1 inplac true df id browser id 31 df id id 31 str split expand true 0 df id version id 31 df id id 31 str split expand true 1 df id browser id 31 fillna none inplac true df id version id 31 fillna none inplac true df id drop id 31 axi 1 inplac true df id screen width df id id 33 str split x expand true 0 df id screen height df id id 33 str split x expand true 1 df id screen width fillna 1 inplac true df id screen height fillna 1 inplac true df id drop id 33 axi 1 inplac true df id id 34 df id id 34 str split expand true 1 df id id 23 df id id 23 str split expand true 1 df id id 23 fillna none inplac true df id id 34 fillna 2 inplac true fill minon id 32 df id id 32 fillna df id id 32 min 10 inplac true devic renam df id loc df id devic name str contain sm na fals devic name samsung df id loc df id devic name str contain samsung na fals devic name samsung df id loc df id devic name str contain gt na fals devic name samsung df id loc df id devic name str contain moto g na fals devic name motorola df id loc df id devic name str contain moto na fals devic name motorola df id loc df id devic name str contain moto na fals devic name motorola df id loc df id devic name str contain lg na fals devic name lg df id loc df id devic name str contain rv na fals devic name rv df id loc df id devic name str contain huawei na fals devic name huawei df id loc df id devic name str contain ale na fals devic name huawei df id loc df id devic name str contain l na fals devic name huawei df id loc df id devic name str contain blade na fals devic name zte df id loc df id devic name str contain blade na fals devic name zte df id loc df id devic name str contain linux na fals devic name linux df id loc df id devic name str contain xt na fals devic name soni df id loc df id devic name str contain htc na fals devic name htc df id loc df id devic name str contain asu na fals devic name asu df id loc df id devic name isin df id devic name valu count df id devic name valu count 200 index devic name other df id id 1
1264,['## Encoding Id categoricals'],encod id categor
1265,['# Joining Ids in Transaction df'],join id transact df
1266,['# Transforming Id features in minmaxscale and filling null values'],transform id featur minmaxscal fill null valu
1267,['# Getting PCA of Id num cols '],get pca id num col
1268,"[""df = PCA_change(df, id_num_cols, prefix='PCA_ID_', n_components=3)""]",df pca chang df id num col prefix pca id n compon 3
1269,['## Geting ID Cards'],gete id card
1270,['# D features'],featur
1271,['### Sum all clusters'],sum cluster
1272,['## Slicing df to df_train and df_test'],slice df df train df test
1273,['## Email Features'],email featur
1275,['## Geting some card features'],gete card featur
1276,['## TransactionAmt to log'],transactionamt log
1277,['## Get first nums in card1'],get first num card1
1278,['## Encoding transaction categoricals'],encod transact categor
1279,"['# Feature selection \n', '- I will use the correlation to drop some features that are high correlated with each other']",featur select use correl drop featur high correl
1280,['## Setting X and y'],set x
1281,['## Shape of last datasets'],shape last dataset
1282,"['## Defining the objective function that will try optimize\n', '- testing sk fold and timeseriessplit']",defin object function tri optim test sk fold timeseriessplit
1283,['## Running HyperOpt'],run hyperopt
1284,"['- to see running outputs click in ""show code"" in the code above']",see run output click show code code
1285,['## Best parameters'],best paramet
1286,['## Predicting with the best parameters '],predict best paramet
1287,"['## I hope you enjoyned the kernel. \n', '## I will work in different features and publish trought this kernel.\n', '# Stay tuned and please give me your feedback and upvote the kernel =)\n', ""I'm not a ML pro, so your feedback is very important. ""]",hope enjoyn kernel work differ featur publish trought kernel stay tune plea give feedback upvot kernel ml pro feedback import
1288,"['## Welcome to my fraud detection Kernel. \n', '\n']",welcom fraud detect kernel
1289,"['\n', '![](http://technosavvy.co.ke/wp-content/uploads/2015/12/fraud.jpg)']",http technosavvi co ke wp content upload 2015 12 fraud jpg
1290,"['# Competition Objective is to detect fraud in transactions; \n', '\n', '## Data\n', '\n', '\n', 'In this competition you are predicting the probability that an online transaction is fraudulent, as denoted by the binary target ```isFraud```.\n', '\n', 'The data is broken into two files **identity** and **transaction**, which are joined by ```TransactionID```. \n', '\n', '> Note: Not all transactions have corresponding identity information.\n', '\n', '**Categorical Features - Transaction**\n', '\n', '- ProductCD\n', '- emaildomain\n', '- card1 - card6\n', '- addr1, addr2\n', '- P_emaildomain\n', '- R_emaildomain\n', '- M1 - M9\n', '\n', '**Categorical Features - Identity**\n', '\n', '- DeviceType\n', '- DeviceInfo\n', '- id_12 - id_38\n', '\n', '**The TransactionDT feature is a timedelta from a given reference datetime (not an actual timestamp).**\n', '\n', '## Questions\n', 'I will start exploring based on Categorical Features and Transaction Amounts.\n', 'The aim is answer some questions like:\n', '- What type of data we have on our data?\n', '- How many cols, rows, missing values we have?\n', '- Whats the target distribution?\n', ""- What's the Transactions values distribution of fraud and no fraud transactions?\n"", '- We have predominant fraudulent products? \n', '- What features or target shows some interesting patterns? \n', '- And a lot of more questions that will raise trought the exploration. \n', '\n', '\n', 'I hope you enjoy my kernel and if it be useful for you, <b>upvote</b> the kernel']",competit object detect fraud transact data competit predict probabl onlin transact fraudul denot binari target isfraud data broken two file ident transact join transactionid note transact correspond ident inform categor featur transact productcd emaildomain card1 card6 addr1 addr2 p emaildomain r emaildomain m1 m9 categor featur ident devicetyp deviceinfo id 12 id 38 transactiondt featur timedelta given refer datetim actual timestamp question start explor base categor featur transact amount aim answer question like type data data mani col row miss valu what target distribut transact valu distribut fraud fraud transact predomin fraudul product featur target show interest pattern lot question rais trought explor hope enjoy kernel use b upvot b kernel
1291,"['## <font color=""red"">Please if this kernel were useful for you, please <b>UPVOTE</b> the kernel and give me your feedback =)</font>\n']",font color red plea kernel use plea b upvot b kernel give feedback font
1292,['## Importing Libraries'],import librari
1293,['## Reading dataset'],read dataset
1294,['## Functions to epxlore the data'],function epxlor data
1295,"['## As we have a high dimensional data, I will reduce the memory usage']",high dimension data reduc memori usag
1296,"['## Knowning the Identity dataset\n', '- What type of data we have on our data?']",known ident dataset type data data
1297,"['We have shape 144.2 rows by 41 columns. <br>\n', 'Also, we can see that almost all features has missing values. We will need to work with that. <br>\n', ""Let's see the transactions table and see the details ""]",shape 144 2 row 41 column br also see almost featur miss valu need work br let see transact tabl see detail
1298,"['## Knowing the transactions\n', '- What type of data we have on our data?']",know transact type data data
1299,"['Wow, We have a bizarre high dimension. The shape of Transactions is: 506691, 393<br>\n', 'I will need some time to explore it further. The first aim is start simple. ']",wow bizarr high dimens shape transact 506691 393 br need time explor first aim start simpl
1300,['## Understanding the Target Distribution'],understand target distribut
1301,"['Nice. <br>\n', ""We have only 3.5% of positive values in our target. It's an unbalanced data and we will keep investiganting the data to find some insight.""]",nice br 3 5 posit valu target unbalanc data keep investig data find insight
1302,['We can see that fraudulent transactions has a higher mean than No-Fraudulent Transactions'],see fraudul transact higher mean fraudul transact
1303,"['To avoid us of outliers and a better view of distribution, I will filter the data and get only values equal or lower than 800']",avoid u outlier better view distribut filter data get valu equal lower 800
1304,['## Ploting and Knowing Transaction Amount distribution \n'],plote know transact amount distribut
1305,"[""We don't have a high correlation between Transaction Amount and Fraud Transactions. <br>\n"", 'Also, we can see many cases of fraud transactions with values between 5 to 14 and other peak in 75 -  85.\n', '\n', ""Let's keep investigating this data""]",high correl transact amount fraud transact br also see mani case fraud transact valu 5 14 peak 75 85 let keep investig data
1306,"['## Knowing the Product feature\n', '- We have predominant fraudulent products? ']",know product featur predomin fraudul product
1307,"['Cool!!! I think that this chart is very insightful. <br>\n', 'Altought the W is the most frequent Product we can see higher values in C, R and S products altought we have many lowest values in these categories. ']",cool think chart insight br altought w frequent product see higher valu c r product altought mani lowest valu categori
1308,"['## Exploring Card Features \n', 'We have 6 columns that are about the Card of the transaction.<br>\n', 'I will start by the categoricals and after it, I will explore the continuous ']",explor card featur 6 column card transact br start categor explor continu
1309,"[""Cool!! Again, we can clearly see that card4 we can't see different patterns, but in Card6 we can note that Credit has higher incidence of fraud than Debit payment""]",cool clearli see card4 see differ pattern card6 note credit higher incid fraud debit payment
1310,['I will transform Card1 and Card2 to Logarithm scale to we better understand the distribution '],transform card1 card2 logarithm scale better understand distribut
1311,['## Card1 feature by Target'],card1 featur target
1312,['## Card2 feature by Target'],card2 featur target
1313,"['## Card3 feature by Target\n', '- As we have many values with low frequency, I will set all values with frequency lower than 10 as -99']",card3 featur target mani valu low frequenc set valu frequenc lower 10 99
1314,"['## Card5 feature by Target\n', '- Again, as we have many values with low frequency I will set all values with frequency lower than 20 as -99']",card5 featur target mani valu low frequenc set valu frequenc lower 20 99
1315,"['## Exploring the M2-M9 features\n', '- Seen']",explor m2 m9 featur seen
1317,['## ScatterPollar of Binary Features'],scatterpollar binari featur
1318,"['from sklearn.preprocessing import LabelEncoder\n', 'tmp = df_train_trans[]\n', '#Label encoding Binary columns\n', 'le = LabelEncoder()\n', '\n', ""tmp_churn = df_train_trans[df_train_trans['isFraud'] == 1]\n"", ""tmp_no_churn = df_train_trans[df_train_trans['isFraud'] == 0]\n"", '\n', 'bi_cs = df_train_trans.nunique()[df_train_trans.nunique() == 2].keys()\n', 'dat_rad = df_train_trans[bi_cs]\n', '\n', 'for cols in bi_cs :\n', '    tmp_churn[cols] = le.fit_transform(tmp_churn[cols])\n', '    \n', 'data_frame_x = tmp_churn[bi_cs].sum().reset_index()\n', 'data_frame_x.columns  = [""feature"",""yes""]\n', 'data_frame_x[""no""]    = tmp_churn.shape[0]  - data_frame_x[""yes""]\n', 'data_frame_x  = data_frame_x[data_frame_x[""feature""] != ""Churn""]\n', '\n', ""#count of 1's(yes)\n"", 'trace1 = go.Scatterpolar(r = data_frame_x[""yes""].values.tolist(), \n', '                         theta = data_frame_x[""feature""].tolist(),\n', '                         fill  = ""toself"",name = ""Fraud 1\'s"",\n', '                         mode = ""markers+lines"", visible=True,\n', '                         marker = dict(size = 5)\n', '                        )\n', '\n', ""#count of 0's(No)\n"", 'trace2 = go.Scatterpolar(r = data_frame_x[""no""].values.tolist(),\n', '                         theta = data_frame_x[""feature""].tolist(),\n', '                         fill  = ""toself"",name = ""Fraud 0\'s"",\n', '                         mode = ""markers+lines"", visible=True,\n', '                         marker = dict(size = 5)\n', '                        ) \n', 'for cols in bi_cs :\n', '    tmp_no_churn[cols] = le.fit_transform(tmp_no_churn[cols])\n', '    \n', 'data_frame_x = tmp_no_churn[bi_cs].sum().reset_index()\n', 'data_frame_x.columns  = [""feature"",""yes""]\n', 'data_frame_x[""no""]    = tmp_no_churn.shape[0]  - data_frame_x[""yes""]\n', 'data_frame_x  = data_frame_x[data_frame_x[""feature""] != ""Churn""]\n', '\n', ""#count of 1's(yes)\n"", 'trace3 = go.Scatterpolar(r = data_frame_x[""yes""].values.tolist(),\n', '                         theta = data_frame_x[""feature""].tolist(),\n', '                         fill  = ""toself"",name = ""NoFraud 1\'s"",\n', '                         mode = ""markers+lines"", visible=False,\n', '                         marker = dict(size = 5)\n', '                        )\n', '\n', ""#count of 0's(No)\n"", 'trace4 = go.Scatterpolar(r = data_frame_x[""no""].values.tolist(),\n', '                         theta = data_frame_x[""feature""].tolist(),\n', '                         fill  = ""toself"",name = ""NoFraud 0\'s"",\n', '                         mode = ""markers+lines"", visible=False,\n', '                         marker = dict(size = 5)\n', '                        ) \n', '\n', 'data = [trace1, trace2, trace3, trace4]\n', '\n', 'updatemenus = list([\n', '    dict(active=0,\n', '         x=-0.15,\n', '         buttons=list([  \n', '            dict(\n', ""                label = 'Fraud Dist',\n"", ""                 method = 'update',\n"", ""                 args = [{'visible': [True, True, False, False]}, \n"", ""                     {'title': 'Transaction Fraud Binary Counting Distribution'}]),\n"", '             \n', '             dict(\n', ""                  label = 'No-Fraud Dist',\n"", ""                 method = 'update',\n"", ""                 args = [{'visible': [False, False, True, True]},\n"", ""                     {'title': 'Transaction No-Fraud Binary Counting Distribution'}]),\n"", '\n', '        ]),\n', '    )\n', '])\n', '\n', ""layout = dict(title='ScatterPolar Distribution of Fraud and No-Fraud Transactions (Select from Dropdown)', \n"", '              showlegend=False,\n', '              updatemenus=updatemenus)\n', '\n', 'fig = dict(data=data, layout=layout)\n', '\n', 'iplot(fig)']",sklearn preprocess import labelencod tmp df train tran label encod binari column le labelencod tmp churn df train tran df train tran isfraud 1 tmp churn df train tran df train tran isfraud 0 bi c df train tran nuniqu df train tran nuniqu 2 key dat rad df train tran bi c col bi c tmp churn col le fit transform tmp churn col data frame x tmp churn bi c sum reset index data frame x column featur ye data frame x tmp churn shape 0 data frame x ye data frame x data frame x data frame x featur churn count 1 ye trace1 go scatterpolar r data frame x ye valu tolist theta data frame x featur tolist fill toself name fraud 1 mode marker line visibl true marker dict size 5 count 0 trace2 go scatterpolar r data frame x valu tolist theta data frame x featur tolist fill toself name fraud 0 mode marker line visibl true marker dict size 5 col bi c tmp churn col le fit transform tmp churn col data frame x tmp churn bi c sum reset index data frame x column featur ye data frame x tmp churn shape 0 data frame x ye data frame x data frame x data frame x featur churn count 1 ye trace3 go scatterpolar r data frame x ye valu tolist theta data frame x featur tolist fill toself name nofraud 1 mode marker line visibl fals marker dict size 5 count 0 trace4 go scatterpolar r data frame x valu tolist theta data frame x featur tolist fill toself name nofraud 0 mode marker line visibl fals marker dict size 5 data trace1 trace2 trace3 trace4 updatemenu list dict activ 0 x 0 15 button list dict label fraud dist method updat arg visibl true true fals fals titl transact fraud binari count distribut dict label fraud dist method updat arg visibl fals fals true true titl transact fraud binari count distribut layout dict titl scatterpolar distribut fraud fraud transact select dropdown showlegend fals updatemenu updatemenu fig dict data data layout layout iplot fig
1319,['# NOTE: THIS KERNEL IS NOT FINISHED. I WILL KEEP EXPLORING IT. '],note kernel finish keep explor
1320,"['### As my other kernel has running very slow because the interactive plots, I decided to start again using only Seaborn and matplotlib.\n', ""You can visit here: <a href='https://www.kaggle.com/kabure/baseline-fraud-detection-eda-interactive-views?scriptVersionId=17308287'> Interactive IEEE Fraud Detection</a> <br>\n"", '\n', 'Also, I worked in all features of this dataset and you can access the Kernel here: \n', ""<a href='https://www.kaggle.com/kabure/almost-complete-feature-engineering-ieee-data'> ~Almost~ complete Feature Engineering IEEE data</a> <br>""]",kernel run slow interact plot decid start use seaborn matplotlib visit href http www kaggl com kabur baselin fraud detect eda interact view scriptversionid 17308287 interact ieee fraud detect br also work featur dataset access kernel href http www kaggl com kabur almost complet featur engin ieee data almost complet featur engin ieee data br
1321,"['# Competition Objective is to detect fraud in transactions; \n', '\n', '## Data\n', '\n', '\n', 'In this competition you are predicting the probability that an online transaction is fraudulent, as denoted by the binary target ```isFraud```.\n', '\n', 'The data is broken into two files **identity** and **transaction**, which are joined by ```TransactionID```. \n', '\n', '> Note: Not all transactions have corresponding identity information.\n', '\n', '**Categorical Features - Transaction**\n', '\n', '- ProductCD\n', '- emaildomain\n', '- card1 - card6\n', '- addr1, addr2\n', '- P_emaildomain\n', '- R_emaildomain\n', '- M1 - M9\n', '\n', '**Categorical Features - Identity**\n', '\n', '- DeviceType\n', '- DeviceInfo\n', '- id_12 - id_38\n', '\n', '**The TransactionDT feature is a timedelta from a given reference datetime (not an actual timestamp).**\n', '\n', '# Questions\n', 'I will start exploring based on Categorical Features and Transaction Amounts.\n', 'The aim is answer some questions like:\n', '- What type of data we have on our data?\n', '- How many cols, rows, missing values we have?\n', '- Whats the target distribution?\n', ""- What's the Transactions values distribution of fraud and no fraud transactions?\n"", '- We have predominant fraudulent products? \n', '- What features or target shows some interesting patterns? \n', '- And a lot of more questions that will raise trought the exploration. \n', '\n', '\n', '## I hope you enjoy my kernel and if it be useful for you, <b>upvote</b> the kernel']",competit object detect fraud transact data competit predict probabl onlin transact fraudul denot binari target isfraud data broken two file ident transact join transactionid note transact correspond ident inform categor featur transact productcd emaildomain card1 card6 addr1 addr2 p emaildomain r emaildomain m1 m9 categor featur ident devicetyp deviceinfo id 12 id 38 transactiondt featur timedelta given refer datetim actual timestamp question start explor base categor featur transact amount aim answer question like type data data mani col row miss valu what target distribut transact valu distribut fraud fraud transact predomin fraudul product featur target show interest pattern lot question rais trought explor hope enjoy kernel use b upvot b kernel
1322,['## Importing necessary libraries'],import necessari librari
1323,['# Importing train datasets'],import train dataset
1324,['I will set all functions in the cell bellow.'],set function cell bellow
1325,"['To see the output of the Resume Table, click to see the output ']",see output resum tabl click see output
1326,['# Knowing the data'],know data
1327,['# Target Distribution'],target distribut
1328,"['We have 3.5% of Fraud transactions in our dataset. <br>I think that it would be interesting to see if the amount percentual is higher or lower than 3.5% of total. I will see it later. <br>\n', 'We have the same % when considering the Total Transactions Amount by Fraud and No Fraud. <br>\n', ""Let's explore the Transaction amount further below.""]",3 5 fraud transact dataset br think would interest see amount percentu higher lower 3 5 total see later br consid total transact amount fraud fraud br let explor transact amount
1329,['# Transaction Amount Quantiles'],transact amount quantil
1330,"[""Before Ploting the Transaction Amount, let's see the quantiles of Transaction Amount""]",plote transact amount let see quantil transact amount
1331,['# Ploting Transaction Amount Values Distribution'],plote transact amount valu distribut
1332,"['Nice! Now, we can see clearly the distribution of ']",nice see clearli distribut
1333,['# Seeing the Quantiles of Fraud and No Fraud Transactions'],see quantil fraud fraud transact
1334,"['# Transaction Amount Outliers\n', ""- It's considering outlier values that are highest than 3 times the std from the mean""]",transact amount outlier consid outlier valu highest 3 time std mean
1335,"['If we consider only values between >= 0 to 800 we will avoid the outliers and has more confidence in our distribution. <br>\n', 'We have 10k rows with outliers that represents 1.74% of total rows.']",consid valu 0 800 avoid outlier confid distribut br 10k row outlier repres 1 74 total row
1336,"[""# Now, let's known the Product Feature\n"", '- Distribution Products\n', '- Distribution of Frauds by Product\n', '- Has Difference between Transaction Amounts in Products? ']",let known product featur distribut product distribut fraud product differ transact amount product
1337,"['W, C and R are the most frequent values. <br>\n', 'We can note that in W, H and R the distribution of Fraud values are slightly higher than the Non-Fraud Transactions']",w c r frequent valu br note w h r distribut fraud valu slightli higher non fraud transact
1338,"['# Card Features\n', '- Based on Competition Description, card features are categoricals.\n', '- Lets understand the distribution of values\n', ""- What's the different in transactions and % of Fraud for each values in these features\n"", '- Card features has 6 columns, and 4 of them seems to be numericals, so lets see the quantiles and distributions']",card featur base competit descript card featur categor let understand distribut valu differ transact fraud valu featur card featur 6 column 4 seem numer let see quantil distribut
1339,['Card2-Card6 has some missing values. We will need to due with it later.'],card2 card6 miss valu need due later
1340,['# Numericals Feature Card Quantiles'],numer featur card quantil
1341,"['We can see that Card 1 and Card 2 has a large distribution of values, so maybe it will be better to get the log of these columns']",see card 1 card 2 larg distribut valu mayb better get log column
1342,"['# Visualizing Card 1, Card 2 and Card 3 Distributions\n', '- As the Card 1 and 2 are numericals, I will plot the distribution of them\n', '- in Card 3, as we have many values with low frequencies, I decided to set value to ""Others"" \n', '- Also, in Card 3 I set the % of Fraud ratio in yaxis2']",visual card 1 card 2 card 3 distribut card 1 2 numer plot distribut card 3 mani valu low frequenc decid set valu other also card 3 set fraud ratio yaxis2
1343,"['Cool and Very Meaningful information. <br>\n', 'In Card3 we can see that 100 and 106 are the most common values in the column. <br>\n', 'We have 4.95% of Frauds in 100 and 1.52% in 106; The values with highest Fraud Transactions are 185, 119 and 119; <br>\n', '\n', 'In card5 the most frequent values are 226, 224, 166 that represents 73% of data. Also is posible to see high % of frauds in 137, 147, 141 that has few entries for values.']",cool meaning inform br card3 see 100 106 common valu column br 4 95 fraud 100 1 52 106 valu highest fraud transact 185 119 119 br card5 frequent valu 226 224 166 repres 73 data also posibl see high fraud 137 147 141 entri valu
1344,['# Card 4 - Categorical'],card 4 categor
1345,"['We can see that 97% of our data are in Mastercard(32%) and Visa(65%);  <br>\n', 'we have a highest value in discover(~8%) against ~3.5% of Mastercard and Visa and 2.87% in American Express']",see 97 data mastercard 32 visa 65 br highest valu discov 8 3 5 mastercard visa 2 87 american express
1346,['# Card 6 - Categorical'],card 6 categor
1347,"['All data is on Credit and Debit. We can see a high percentual of Frauds in Credit than Debit transactions. <br>\n', ""The Distribution of Transaction Amount don't shows clear differences.""]",data credit debit see high percentu fraud credit debit transact br distribut transact amount show clear differ
1348,['# Exploring M1-M9 Features '],explor m1 m9 featur
1349,"['## M distributions:  Count, %Fraud and Transaction Amount distribution']",distribut count fraud transact amount distribut
1350,"['Very cool!!! This graphs give us many interesting intuition about the M features.<br>\n', ""Only in M4 the Missing values haven't the highest % of Fraud.\n"", '\n']",cool graph give u mani interest intuit featur br m4 miss valu highest fraud
1351,['# Addr1 and Addr2'],addr1 addr2
1352,"['I will set all values in Addr1 that has less than 5000 entries to ""Others""<br>\n', 'In Addr2 I will set as ""Others"" all values with less than 50 entries']",set valu addr1 le 5000 entri other br addr2 set other valu le 50 entri
1353,['## Addr1 Distributions'],addr1 distribut
1354,['We can note interesting patterns on Addr1.'],note interest pattern addr1
1355,['## Addr2 Distributions'],addr2 distribut
1356,"['Almost all entries in Addr2 are in the same value. <br>\n', 'Interestingly in the value 65 , the percent of frauds are almost 60% <br>\n', 'Altought the value 87 has 88% of total entries, it has 96% of Total Transaction Amounts']",almost entri addr2 valu br interestingli valu 65 percent fraud almost 60 br altought valu 87 88 total entri 96 total transact amount
1357,"['# P emaildomain Distributions\n', '- I will group all e-mail domains by the respective enterprises.\n', '- Also, I will set as ""Others"" all values with less than 500 entries.']",p emaildomain distribut group e mail domain respect enterpris also set other valu le 500 entri
1358,['## Ploting P-Email Domain'],plote p email domain
1360,"['## R-Email Domain plot distribution\n', '- I will group all e-mail domains by the respective enterprises.\n', '- I will set as ""Others"" all values with less than 300 entries.']",r email domain plot distribut group e mail domain respect enterpris set other valu le 300 entri
1361,"['We can see a very similar distribution in both email domain features. <br>\n', ""It's interesting that we have high values in google and icloud frauds""]",see similar distribut email domain featur br interest high valu googl icloud fraud
1363,"['# C1-C14 features\n', ""- Let's understand what this features are.\n"", ""- What's the distributions? ""]",c1 c14 featur let understand featur distribut
1364,['## C1 Distribution Plot'],c1 distribut plot
1367,"['Top 3 values are 1, 2 and 3 and is the same on Total Amounts. We see the same pattern on fraud ratios']",top 3 valu 1 2 3 total amount see pattern fraud ratio
1368,"['# TimeDelta Feature\n', ""- Let's see if the frauds have some specific hour that has highest % of frauds ""]",timedelta featur let see fraud specif hour highest fraud
1369,"['## Converting to Total Days, Weekdays and Hours\n', 'In discussions tab I read an excellent solution to Timedelta column, I will set the link below; <br>\n', 'We will use the first date as 2017-12-01 and use the delta time to compute datetime features\n']",convert total day weekday hour discus tab read excel solut timedelta column set link br use first date 2017 12 01 use delta time comput datetim featur
1370,['## Top Days with highest Total Transaction Amount'],top day highest total transact amount
1371,['## Ploting WeekDays Distributions'],plote weekday distribut
1372,"[""We don't have the reference of date but we can see that two days has lower transactions, that we can infer it is weekend days""]",refer date see two day lower transact infer weekend day
1373,['## Ploting Hours Distributions'],plote hour distribut
1375,['## Transactions and Total Amount by each day'],transact total amount day
1377,"['## FRAUD TRANSACTIONS BY DATE\n', '- Visualizing only Fraud Transactions by Date']",fraud transact date visual fraud transact date
1379,"['# Features [id_12 to id_38]\n', '- categorical features in training identity dataset']",featur id 12 id 38 categor featur train ident dataset
1380,['## Ploting columns with few unique values'],plote column uniqu valu
1381,['## Id 30'],id 30
1382,['## Id 31'],id 31
1383,"['## Modelling \n', 'To start simple, I will start using as base the kernels below: <br>\n', 'https://www.kaggle.com/artkulak/ieee-fraud-simple-baseline-0-9383-lb - (@artkulak - Art) <br>\n', 'https://www.kaggle.com/artgor/eda-and-models - (@artgor - Andrew Lukyanenko)\n', '\n']",model start simpl start use base kernel br http www kaggl com artkulak ieee fraud simpl baselin 0 9383 lb artkulak art br http www kaggl com artgor eda model artgor andrew lukyanenko
1384,['# reducing memory usage'],reduc memori usag
1385,['# Mapping emails'],map email
1386,['# Encoding categorical features'],encod categor featur
1387,['# Some feature engineering'],featur engin
1388,['# Concating dfs to get PCA of V features'],concat df get pca v featur
1389,['# Getting PCA '],get pca
1390,['# Seting train and test back'],sete train test back
1391,['# Seting X and y'],sete x
1392,['# Defining the HyperOpt function with parameters space and model'],defin hyperopt function paramet space model
1393,['# Running the optimizer'],run optim
1394,['# Best parameters'],best paramet
1395,['# Trainning and Predicting with best Parameters'],train predict best paramet
1396,['## Predicting X test'],predict x test
1397,['# Top 20 Feature importance'],top 20 featur import
1398,['## Seting y_pred to csv'],sete pred csv
1399,"[""## I'm working in this kernel yet.\n"", '# <font color=""red"">Please if this kernel were useful for you, please <b>UPVOTE</b> =)</font>']",work kernel yet font color red plea kernel use plea b upvot b font
1400,"['From the [data description page](https://www.kaggle.com/c/ieee-fraud-detection/data) we know that ""The TransactionDT feature is a timedelta from a given reference datetime (not an actual timestamp).""\n', '\n', 'In this kernel we analyse the TransactionDT column, and support our hypothesis that the TransactionDT column starts at the 1st of December.\n', '\n', 'We can use the TransactionDT column to calculate features such as weekday and hour.']",data descript page http www kaggl com c ieee fraud detect data know transactiondt featur timedelta given refer datetim actual timestamp kernel analys transactiondt column support hypothesi transactiondt column start 1st decemb use transactiondt column calcul featur weekday hour
1401,"[""The transaction datetime seems to end at '2018-12-31 23:59:05'.""]",transact datetim seem end 2018 12 31 23 59 05
1402,"['There are peaks in transactions around christmas 2017 and christmas 2018. For both years, the peaks end at the 26th of December.']",peak transact around christma 2017 christma 2018 year peak end 26th decemb
1403,['## 0. Context'],0 context
1404,"['- Loading Library\n', '- Read Data SET\n', '- EDA\n', '- Preprocessing\n', '* Feature Engineering\n', '- Modeling\n', '- Evaluation']",load librari read data set eda preprocess featur engin model evalu
1405,['## 1. Loading Library'],1 load librari
1406,['* Install LightGBM GPU VERSION'],instal lightgbm gpu version
1407,['## 2. Reading Data SET'],2 read data set
1408,['### 1) Reduce Memory using down sizing Data SET'],1 reduc memori use size data set
1409,['## 3. EDA'],3 eda
1410,"['- From above on, Usage of RAM is 3.8GB']",usag ram 3 8gb
1411,"['- In this competition you are predicting the probability that an online transaction is fraudulent, as denoted by the binary target isFraud.\n', '\n', '- The data is broken into two files identity and transaction, which are joined by TransactionID. Not all transactions have corresponding identity information.']",competit predict probabl onlin transact fraudul denot binari target isfraud data broken two file ident transact join transactionid transact correspond ident inform
1412,"['- In this competition you are predicting the probability that an online transaction is fraudulent, as denoted by the binary target isFraud.\n', '\n', '- The data is broken into two files identity and transaction, which are joined by TransactionID. Not all transactions have corresponding identity information.\n', '------------------------------\n', '- *Categorical Features - Transaction\n', '- ProductCD\n', '- card1 - card6\n', '- addr1, addr2\n', '- P_emaildomain\n', '- R_emaildomain\n', '- M1 - M9\n', '------------------------------\n', '- *Categorical Features - Identity\n', '- DeviceType\n', '- DeviceInfo id_12 - id_38\n', '------------------------------\n', '- The TransactionDT feature is a timedelta from a given reference datetime (not an actual timestamp).']",competit predict probabl onlin transact fraudul denot binari target isfraud data broken two file ident transact join transactionid transact correspond ident inform categor featur transact productcd card1 card6 addr1 addr2 p emaildomain r emaildomain m1 m9 categor featur ident devicetyp deviceinfo id 12 id 38 transactiondt featur timedelta given refer datetim actual timestamp
1413,['### 1) Check Missing Data'],1 check miss data
1414,['### 2) Check Numeric Columns Properties'],2 check numer column properti
1415,['### 3) Check Categorical Columns Properties'],3 check categor column properti
1416,['## 4. Feature Engineering'],4 featur engin
1417,['### 1) Correlation Analysis of Numeric Values'],1 correl analysi numer valu
1418,['### 2) Feature Slicing in Time Series Data'],2 featur slice time seri data
1419,['### 3) Feature Values Filtering in Categorical Columns'],3 featur valu filter categor column
1420,['## 5. Modeling'],5 model
1421,['### 1) PreProcessing'],1 preprocess
1422,['### 2) Train / Validation Split'],2 train valid split
1423,['### 3) XGBoost Fitting'],3 xgboost fit
1424,['#### 4) LightGBM Fitting'],4 lightgbm fit
1425,['### 5) Submission to Score Board'],5 submiss score board
1426,"['This kernel demonstrates a way of using LightGBM with GPU support in Kaggle kernels.\n', '\n', ""Basically to avoid the following error which we get when we give `device='gpu'` in LightGBM parameters.\n"", '\n', '`LightGBMError: GPU Tree Learner was not enabled in this build.\n', 'Please recompile with CMake option -DUSE_GPU=1`\n', '\n', 'The code & idea are heavily inspired from the following:\n', '* https://www.kaggle.com/inversion/ieee-simple-xgboost\n', '* https://www.kaggle.com/xhlulu/ieee-fraud-xgboost-with-gpu-fit-in-40s\n', '* https://www.kaggle.com/vinhnguyen/gpu-acceleration-for-lightgbm\n', '* https://lightgbm.readthedocs.io/en/latest/GPU-Tutorial.html\n', '* https://lightgbm.readthedocs.io/en/latest/GPU-Performance.html']",kernel demonstr way use lightgbm gpu support kaggl kernel basic avoid follow error get give devic gpu lightgbm paramet lightgbmerror gpu tree learner enabl build plea recompil cmake option duse gpu 1 code idea heavili inspir follow http www kaggl com invers ieee simpl xgboost http www kaggl com xhlulu ieee fraud xgboost gpu fit 40 http www kaggl com vinhnguyen gpu acceler lightgbm http lightgbm readthedoc io en latest gpu tutori html http lightgbm readthedoc io en latest gpu perform html
1427,['## LightGBM GPU Installation'],lightgbm gpu instal
1428,['### Build and re-install LightGBM with GPU support'],build instal lightgbm gpu support
1429,['## Imports'],import
1430,['## Preprocessing'],preprocess
1431,['## Modeling'],model
1432,['## Feature Importances'],featur import
1433,['## Submission'],submiss
1434,"[""With `device='gpu'` parameter commented, it takes ~ 22 minutes to fit on CPU.\n"", '\n', '`CPU times: user 42min 4s, sys: 13 s, total: 42min 17s\n', 'Wall time: 21min 47s`\n', '\n', ""With `device='gpu'`, it takes ~ 3 minutes to fit on GPU.\n"", '\n', '`CPU times: user 3min 59s, sys: 46 s, total: 4min 45s\n', 'Wall time: 2min 34s`\n', '\n', '*Note: The CPU provided in Kaggle GPU kernel is 2 core, so the time to fit with above parameters might take half the time(~11 minutes) on a CPU only kernel(4 core CPU), which is still slower than LightGBM GPU implementation.*']",devic gpu paramet comment take 22 minut fit cpu cpu time user 42min 4 sy 13 total 42min 17 wall time 21min 47 devic gpu take 3 minut fit gpu cpu time user 3min 59 sy 46 total 4min 45 wall time 2min 34 note cpu provid kaggl gpu kernel 2 core time fit paramet might take half time 11 minut cpu kernel 4 core cpu still slower lightgbm gpu implement
1435,['## StackNet Installation'],stacknet instal
1436,"['This kernel demonstrates a way of using [StackNet](https://github.com/h2oai/pystacknet) with LightGBM, XGBoost & Catboost with GPU support in Kaggle kernels.']",kernel demonstr way use stacknet http github com h2oai pystacknet lightgbm xgboost catboost gpu support kaggl kernel
1437,"['### LightGBM GPU Installation\n', 'Full implementation and tutorial at https://www.kaggle.com/kirankunapuli/ieee-fraud-lightgbm-with-gpu']",lightgbm gpu instal full implement tutori http www kaggl com kirankunapuli ieee fraud lightgbm gpu
1438,['## Imports'],import
1439,['## Preprocessing'],preprocess
1440,['## StackNet Model'],stacknet model
1441,['## Submission'],submiss
1442,"['# Summary\n', '\n', ""I'll try to make a small summary for this blend baseline:\n"", '\n', ""Step: 0. EDA (missing kernel here, I'll post later)\n"", '\n', '\n', 'Step: 1. Minify Data \n', '> https://www.kaggle.com/kyakovlev/ieee-data-minification\n', '\n', '\n', 'Step: 2. Make ground baseline with no fe:\n', '> https://www.kaggle.com/kyakovlev/ieee-ground-baseline and \n', '> https://www.kaggle.com/kyakovlev/ieee-ground-baseline-deeper-learning\n', '\n', '\n', 'Step: 3. Make a small FE and see I you can understand data you have\n', '>  https://www.kaggle.com/kyakovlev/ieee-ground-baseline-make-amount-useful-again and\n', '>  https://www.kaggle.com/kyakovlev/ieee-gb-2-make-amount-useful-again\n', '\n', '\n', 'Step: 4. Find good CV strategy \n', '>  https://www.kaggle.com/kyakovlev/ieee-cv-options\n', 'and same with gap to compare results (gap in values is what we have in test set)\n', 'https://www.kaggle.com/kyakovlev/ieee-cv-options-with-gap\n', '\n', 'Step: 4(1). Groupkfold (by timeblocks) application\n', '> https://www.kaggle.com/kyakovlev/ieee-lgbm-with-groupkfold-cv\n', '\n', '\n', 'Step: 5. Try different set of features\n', '>  https://www.kaggle.com/kyakovlev/ieee-experimental\n', '\n', '\n', 'Step: 6. Make deeper FE (brute force option)\n', '> https://www.kaggle.com/kyakovlev/ieee-fe-with-some-eda\n', '\n', '\n', ""Step: 7. Features selection (missing kernel here, I'll post later)\n"", '\n', '\n', ""Step: 8. Hyperopt (missing kernel here, I'll post later)\n"", '\n', '\n', ""Step: 9. Try other models (XGBoost, CatBoost, NN - missing kernel here, I'll post later)\n"", '> CatBoost (with categorical transformations)  https://www.kaggle.com/kyakovlev/ieee-catboost-baseline-with-groupkfold-cv\n', '\n', ""Step: 10. Try blending and stacking (missing kernel here, I'll post later)\n"", '\n', '---\n', '\n', '(Utils)\n', '\n', 'Some tricks that where used in fe kernel\n', '> https://www.kaggle.com/kyakovlev/ieee-small-tricks\n', '\n', 'Part of EDA (Just few things)\n', '> https://www.kaggle.com/kyakovlev/ieee-check-noise and https://www.kaggle.com/kyakovlev/ieee-simple-eda\n', '\n', '---\n', '\n', 'https://www.kaggle.com/c/ieee-fraud-detection/discussion/104142']",summari tri make small summari blend baselin step 0 eda miss kernel post later step 1 minifi data http www kaggl com kyakovlev ieee data minif step 2 make ground baselin fe http www kaggl com kyakovlev ieee ground baselin http www kaggl com kyakovlev ieee ground baselin deeper learn step 3 make small fe see understand data http www kaggl com kyakovlev ieee ground baselin make amount use http www kaggl com kyakovlev ieee gb 2 make amount use step 4 find good cv strategi http www kaggl com kyakovlev ieee cv option gap compar result gap valu test set http www kaggl com kyakovlev ieee cv option gap step 4 1 groupkfold timeblock applic http www kaggl com kyakovlev ieee lgbm groupkfold cv step 5 tri differ set featur http www kaggl com kyakovlev ieee experiment step 6 make deeper fe brute forc option http www kaggl com kyakovlev ieee fe eda step 7 featur select miss kernel post later step 8 hyperopt miss kernel post later step 9 tri model xgboost catboost nn miss kernel post later catboost categor transform http www kaggl com kyakovlev ieee catboost baselin groupkfold cv step 10 tri blend stack miss kernel post later util trick use fe kernel http www kaggl com kyakovlev ieee small trick part eda thing http www kaggl com kyakovlev ieee check nois http www kaggl com kyakovlev ieee simpl eda http www kaggl com c ieee fraud detect discus 104142
1443,"['Saving memory size can greatly reduce the burden to your system memory while improve your analysis experience.\n', 'Here there are 3 steps to do it. ']",save memori size greatli reduc burden system memori improv analysi experi 3 step
1444,['## 1. Downcasting Numeric Columns'],1 downcast numer column
1445,"['As you can see, the tran_id dataframe has been downcasted from 161.9MB to 148.7MB (8.1%), while the trin_tran dataframe has been downcasted from 2.1GB to 1.2GB (42.9%). Of course, the more int and float column you have, the chance that these functions can better improve your performaces will be.\n', '\n', 'The idea behind the scene is pandas automatically read in your dataframe using int 64 or float 64, most of the time you do not need it to be this big, here is a size chart from StackOverflow:\n', '**   Type      Capacity\n', '\n', '   Int16 -- (-32,768 to +32,767)\n', '\n', '   Int32 -- (-2,147,483,648 to +2,147,483,647)\n', '\n', '   Int64 -- (-9,223,372,036,854,775,808 to +9,223,372,036,854,775,807)**\n', '   \n', 'So most of the time, int16 or even int8 could do the job and thus save you the space. ']",see tran id datafram downcast 161 9mb 148 7mb 8 1 trin tran datafram downcast 2 1gb 1 2gb 42 9 cours int float column chanc function better improv performac idea behind scene panda automat read datafram use int 64 float 64 time need big size chart stackoverflow type capac int16 32 768 32 767 int32 2 147 483 648 2 147 483 647 int64 9 223 372 036 854 775 808 9 223 372 036 854 775 807 time int16 even int8 could job thu save space
1446,['## 2.Downcasting the object column by converting them to categorical'],2 downcast object column convert categor
1447,"['The idea behind the this is based on the fact that most of the column only takes few values and pandas can limit those values only to the few categorical value to save the memory. \n', '\n', '""Often in real-time, data includes the text columns, which are repetitive. Features like gender, country, and codes are always repetitive. These are the examples for categorical data.\n', '\n', 'Categorical variables can take on only a limited, and usually fixed number of possible values. Besides the fixed length, categorical data might have an order but cannot perform numerical operation. Categorical are a Pandas data type.\n', '\n', 'The categorical data type is useful in the following cases −\n', '\n', 'A string variable consisting of only a few different values. Converting such a string variable to a categorical variable will save some memory.\n', '\n', 'The lexical order of a variable is not the same as the logical order (“one”, “two”, “three”). By converting to a categorical and specifying an order on the categories, sorting and min/max will use the logical order instead of the lexical order.\n', '\n', 'As a signal to other python libraries that this column should be treated as a categorical variable (e.g. to use suitable statistical methods or plot types).""\n', '(https://www.tutorialspoint.com/python_pandas/python_pandas_categorical_data)']",idea behind base fact column take valu panda limit valu categor valu save memori often real time data includ text column repetit featur like gender countri code alway repetit exampl categor data categor variabl take limit usual fix number possibl valu besid fix length categor data might order perform numer oper categor panda data type categor data type use follow case string variabl consist differ valu convert string variabl categor variabl save memori lexic order variabl logic order one two three convert categor specifi order categori sort min max use logic order instead lexic order signal python librari column treat categor variabl e g use suitabl statist method plot type http www tutorialspoint com python panda python panda categor data
1448,['## 3.Saving your dataframe as pickle file for fast read'],3 save datafram pickl file fast read
1449,['**Do not Forget to save your work. This way you can access the downcasted df everytime you load it!**'],forget save work way access downcast df everytim load
1450,['This code refers to Kaggle kernal: https://www.kaggle.com/frankherfert/tips-tricks-for-working-with-large-datasets?source=post_page---------------------------'],code refer kaggl kernal http www kaggl com frankherfert tip trick work larg dataset sourc post page
1451,"['# IEEE-CIS Fraud Detection: Split Value Histogram\n', '\n', 'I really enjoyed the [IEEE-CIS Fraud Detection — LightGBM Split Points](https://www.kaggle.com/jtrotman/ieee-fraud-lgb-split-points) kernel from [jtrotman](https://www.kaggle.com/jtrotman). It shows us how to visualize the decision tree splitting values count for a given feature. By counting the split values, we can gain new insights and helps us to explain the decision tree. LightGBM introduced a new API [plot_split_value_histogram](https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.plot_split_value_histogram.html) in version `2.3.0` that makes it even easier for us. This kernel shows you demonstration of the API for this competition.']",ieee ci fraud detect split valu histogram realli enjoy ieee ci fraud detect lightgbm split point http www kaggl com jtrotman ieee fraud lgb split point kernel jtrotman http www kaggl com jtrotman show u visual decis tree split valu count given featur count split valu gain new insight help u explain decis tree lightgbm introduc new api plot split valu histogram http lightgbm readthedoc io en latest pythonapi lightgbm plot split valu histogram html version 2 3 0 make even easier u kernel show demonstr api competit
1452,"[""Let's start from installing the latest version of LightGBM:""]",let start instal latest version lightgbm
1453,['# Data Preperation'],data preper
1454,['# Training'],train
1455,['# Visuals'],visual
1456,"[""Let's plot split value histogram for several important features by using the new  API [plot_split_value_histogram](https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.plot_split_value_histogram.html). Note: the API does not handle categorical variables as of 29 September 2019. ""]",let plot split valu histogram sever import featur use new api plot split valu histogram http lightgbm readthedoc io en latest pythonapi lightgbm plot split valu histogram html note api handl categor variabl 29 septemb 2019
1457,"['# Conclusion\n', '\n', 'Hopefully, this kernel brings some value :). ']",conclus hope kernel bring valu
1458,"['### Why this kernel?\n', 'Whenever the size of dataset goes above 1.5GB there are some memory issues when working with Kaggle Kernels, particlarly when you want to fit everything in one kernel. In the public kernls of this competition I saw a very commonly used function `reduce_mem_usage()` to reduce memory usage introduced in [here](https://www.kaggle.com/mjbahmani/reducing-memory-size-for-ieee) that is basically using the function first introduced in [here](https://www.kaggle.com/arjanso/reducing-dataframe-memory-size-by-65). The same (or very similar function) was being used in Predicting Molecular Properties competition as well. \n', '\n', ""As cool as it seems to use this function, it is not the best idea to use a function blindly. First of all, this function automatically fills in your null values for you! that is not exactly what you asked for. Moreover, there are some hidden pitfalls in using that function as described in [here](https://www.kaggle.com/c/champs-scalar-coupling/discussion/96655#latest-566225) and I identified them in [here](https://www.kaggle.com/mhviraf/why-i-wouldn-t-use-reduce-mem-usage). I think that is the reason why Pandas (with all of its genious developers) doesn't have this basic function built-in. Hence, I believe there is a better solution to this problem here.\n"", '\n', '### What is the problem in first place?\n', 'The fundamental problem is the null values we have in dataset. Since `numpy` treats `NaN` cells as `float`, whenever you have null values in a column even if the natural data type of that column is integer Pandas forces that column to be `float64`. This results in a significantly higher memory usage because many of the features we have in this competition are integers but when loaded as a Pandas DataFrame, they will be stored in memory as `float64`.\n', '\n', '### Solution\n', ""The solution is simple. Load integers as integers in the first place. The easiest way to do so is to identify data types and use `dtypes={'columns': 'dtype'}` when calling `pd.read_csv()`. However, when you want to use `intXX` as dtype in Pandas (versions earlier than 0.24.0) it doesn't let you use it for columns that contain `NaN` values because as I said before, Pandas uses Numpy `NaN` which is by definition a float (this is why using `reduce_mem_usage()` would fill your `NaN` values for you). However, *starting version 0.24.0* Pandas has introduced a new nullable integer datatype that actually lets you have `NaN` values in integer columns (signed and unsigned). We are going to use this new datatype.\n"", '\n', '### References:\n', '* https://docs.scipy.org/doc/numpy-1.13.0/user/basics.types.html\n', '* https://pandas.pydata.org/pandas-docs/stable/user_guide/integer_na.html']",kernel whenev size dataset goe 1 5gb memori issu work kaggl kernel particlarli want fit everyth one kernel public kernl competit saw commonli use function reduc mem usag reduc memori usag introduc http www kaggl com mjbahmani reduc memori size ieee basic use function first introduc http www kaggl com arjanso reduc datafram memori size 65 similar function use predict molecular properti competit well cool seem use function best idea use function blindli first function automat fill null valu exactli ask moreov hidden pitfal use function describ http www kaggl com c champ scalar coupl discus 96655 latest 566225 identifi http www kaggl com mhviraf use reduc mem usag think reason panda geniou develop basic function built henc believ better solut problem problem first place fundament problem null valu dataset sinc numpi treat nan cell float whenev null valu column even natur data type column integ panda forc column float64 result significantli higher memori usag mani featur competit integ load panda datafram store memori float64 solut solut simpl load integ integ first place easiest way identifi data type use dtype column dtype call pd read csv howev want use intxx dtype panda version earlier 0 24 0 let use column contain nan valu said panda use numpi nan definit float use reduc mem usag would fill nan valu howev start version 0 24 0 panda introduc new nullabl integ datatyp actual let nan valu integ column sign unsign go use new datatyp refer http doc scipi org doc numpi 1 13 0 user basic type html http panda pydata org panda doc stabl user guid integ na html
1459,"['First we need to install pandas versions later than 0.24.0. In this kernel I will install pandas==0.24.0.\n', '\n', ""Don't forget to turn on the Internet under the Settings of your kernel.""]",first need instal panda version later 0 24 0 kernel instal panda 0 24 0 forget turn internet set kernel
1460,"[""As can be seen, if we import data as is, by default it uses Numpy and its memory usage is 2123.15 MB. Now let's look at the data types""]",seen import data default use numpi memori usag 2123 15 mb let look data type
1461,"['376 `float64`s. But do we really have that many columns of type `float64`? Given that we have 590540 rows, using correct data types will make a significant difference. I identified the unsigned nullable integer columns and listed them below as a Python dictionary. To use it, copy this dictionary and simply pass `dtype=proper_dtypes` as an argument in `read_csv()`.']",376 float64 realli mani column type float64 given 590540 row use correct data type make signific differ identifi unsign nullabl integ column list python dictionari use copi dictionari simpli pas dtype proper dtype argument read csv
1462,['#### Test set'],test set
1463,"['## Advantages over reduce_mem_usage():\n', '* You get to choose how to handle your null values.\n', ""* You don't loose percision.\n"", '\n', ""(Don't forget that you will need to install Pandas version > 0.24.0)""]",advantag reduc mem usag get choos handl null valu loo percis forget need instal panda version 0 24 0
1464,"[' #  <div style=""text-align: center"">  Reducing  Memory Size for IEEE </div> \n', ' <div style=""text-align:center"">  </div>\n', '![mem](http://s8.picofile.com/file/8367719234/mem.png) \n', '<div style=""text-align:center""> last update: <b> 19/07/2019</b></div>\n']",div style text align center reduc memori size ieee div div style text align center div mem http s8 picofil com file 8367719234 mem png div style text align center last updat b 19 07 2019 b div
1465,"['## Objective of the Kernel: Save Time & Memory\n', 'If you would like to create a kernel for this Competition. this is a good idea to add this kernel as a **data set** to your own kernel. due to you can save your time and memory.']",object kernel save time memori would like creat kernel competit good idea add kernel data set kernel due save time memori
1466,"['___MEMORY USAGE  BEFORE AND AFTER COMPLETION FOR TRAIN:___\n', '<br/>\n', 'Memory usage before running this script : 1975.3707885742188  MB\n', '<br/>\n', 'Memory usage after running this script  : ~ **480  MB**\n', '<br/>\n', 'This is ~ 28 % of the initial size']",memori usag complet train br memori usag run script 1975 3707885742188 mb br memori usag run script 480 mb br 28 initi size
1467,"['\n', '___MEMORY USAGE  BEFORE AND AFTER COMPLETION FOR TEST:___\n', '<br/>\n', 'Memory usage before running this script : 1693.867820739746  MB\n', '<br/>\n', 'Memory usage after running this script: ~ **480  MB**\n', '<br/>\n', 'This is ~  28  % of the initial size']",memori usag complet test br memori usag run script 1693 867820739746 mb br memori usag run script 480 mb br 28 initi size
1468,['## Import'],import
1469,['What do we have in input'],input
1470,['## Import Dataset to play with it'],import dataset play
1471,['### Creat our train & test dataset'],creat train test dataset
1472,"['### Before Reducing Memory\n', 'When I have just read the data set and join them!I saw that the status of my RAM is more than 9GB!']",reduc memori read data set join saw statu ram 9gb
1473,['![ram1](http://s9.picofile.com/file/8366931918/ram1.png)'],ram1 http s9 picofil com file 8366931918 ram1 png
1474,['Then we shoud just delete some dt!'],shoud delet dt
1475,"['![ram2](http://s8.picofile.com/file/8366932526/ram2.png)\n', '3GB of RAM has got free! now just check the size of our train & test']",ram2 http s8 picofil com file 8366932526 ram2 png 3gb ram got free check size train test
1476,"['# IEEE Reducing  Memory Size\n', 'It is necessary that after using this code, carefully check the output results for each column.']",ieee reduc memori size necessari use code care check output result column
1477,['Reducing for train data set:'],reduc train data set
1478,['Reducing for test data set:'],reduc test data set
1479,['Check again! our RAM. 2 GB has got free!'],check ram 2 gb got free
1480,['![ram3](http://s8.picofile.com/file/8366940442/ram3.png)'],ram3 http s8 picofil com file 8366940442 ram3 png
1481,"['## Add this kernel as Dataset\n', 'Now we just save our output as csv files. then you can simply add them to your own kernel.you will save time and  memory.']",add kernel dataset save output csv file simpli add kernel save time memori
1482,"['## How about other ways!\n', 'I have used this [great kernel](https://www.kaggle.com/arjanso/reducing-dataframe-memory-size-by-65) but there are also other ways such as:\n', '1. https://www.dataquest.io/blog/pandas-big-data/\n', '2. [optimizing-the-size-of-a-pandas-dataframe-for-low-memory-environment](https://medium.com/@vincentteyssier/optimizing-the-size-of-a-pandas-dataframe-for-low-memory-environment-5f07db3d72e)\n', '3. [pandas-making-dataframe-smaller-faster](https://www.ritchieng.com/pandas-making-dataframe-smaller-faster/)']",way use great kernel http www kaggl com arjanso reduc datafram memori size 65 also way 1 http www dataquest io blog panda big data 2 optim size panda datafram low memori environ http medium com vincentteyssi optim size panda datafram low memori environ 5f07db3d72e 3 panda make datafram smaller faster http www ritchieng com panda make datafram smaller faster
1483,"['Reference: https://www.kaggle.com/nroman/lgb-single-model-lb-0-9419\n', '> https://www.kaggle.com/roydatascience/light-gbm-with-complete-eda\n', '* https://www.kaggle.com/ragnar123/e-d-a-and-baseline-mix-lgbm']",refer http www kaggl com nroman lgb singl model lb 0 9419 http www kaggl com roydatasci light gbm complet eda http www kaggl com ragnar123 e baselin mix lgbm
1484,['> Please give your feedback'],plea give feedback
1485,['**Importing necessary library**'],import necessari librari
1486,['**Importing datasets**'],import dataset
1487,['**Merging transaction and Identity **'],merg transact ident
1488,['**Negative Downsampling**'],neg downsampl
1489,['> From below we can see that there are a lot of features with almost 99% nan values'],see lot featur almost 99 nan valu
1490,['> Sorting features on basis of TransactionDT'],sort featur basi transactiondt
1491,"['**Taking all features**\n', '> Initially I will start with all the features and then will drop most of the features on the basis of count']",take featur initi start featur drop featur basi count
1492,['> From below we can see that length of features is 434'],see length featur 434
1493,['**Displaying all the columns**'],display column
1494,['**Concatinating train and test as one dataframe**'],concatin train test one datafram
1495,['**Card feature**'],card featur
1496,['**Id Feaures**'],id feaur
1497,['**Adding few more features**'],ad featur
1498,['**This block of code count every features and drop original features**'],block code count everi featur drop origin featur
1499,['**Dropping below features as these seems to be repeating**'],drop featur seem repeat
1500,['**Log**'],log
1501,['> Below we can see that all I am left with is count'],see left count
1502,['**Again seperating data into train and test**'],seper data train test
1503,['**Train test and split**'],train test split
1504,['> **Lightgbm**'],lightgbm
1505,['**Submission**'],submiss
1506,"['> thank you all please let me know where did I go wrong.\n', '> Thankyou']",thank plea let know go wrong thankyou
1507,"['# Tree Split Feature Selection - LGBM GPU EarlyStop  \n', '_By Nick Brooks_\n', '\n', 'V1 - 29/07/2019 - First Commit <br>\n', 'V2 - 03/08/2019 - PCA, Metric Convergence, Submission with 21 features <br>\n', 'V3 - 05/08/2019 - Fix GPU implementation <br>\n', '\n', '**Motivation:** <br>\n', 'How much of these features have actual signal? How does predictive power react when the number of features is decreased? What can PCA tell us about the  amount of variance in the features? Does reducing the number of features lead to smoother convergence?\n', '\n', '**Methodology:** <br>\n', '*Tree-Split Feature Selection* - Experiment with Iteratively removing feature using Gradient Boosting Ensemble split importance. LGBM is trained with a single validation fold and shuffles the train / validation set each iteration.\n', '\n', 'Very hyped for the GPU speed boost to Gradient Boosting Algorithmns, this will enable many fun experiments.\n', '\n', '**Other Links:** <Br>\n', '[ELI5 Permutation Importance](https://eli5.readthedocs.io/en/latest/blackbox/permutation_importance.html)\n', '    \n', '**My Other Fraud Notebooks:** <br>\n', 'https://www.kaggle.com/nicapotato/auc-performance-vs-training-size-gpu-catboost <br>\n', 'https://www.kaggle.com/nicapotato/fraud-shap-xgboost <br>']",tree split featur select lgbm gpu earlystop nick brook v1 29 07 2019 first commit br v2 03 08 2019 pca metric converg submiss 21 featur br v3 05 08 2019 fix gpu implement br motiv br much featur actual signal predict power react number featur decreas pca tell u amount varianc featur reduc number featur lead smoother converg methodolog br tree split featur select experi iter remov featur use gradient boost ensembl split import lgbm train singl valid fold shuffl train valid set iter hype gpu speed boost gradient boost algorithmn enabl mani fun experi link br eli5 permut import http eli5 readthedoc io en latest blackbox permut import html fraud notebook br http www kaggl com nicapotato auc perform v train size gpu catboost br http www kaggl com nicapotato fraud shap xgboost br
1508,"['#### GPU Installation from [kirankunapuli](https://www.kaggle.com/kirankunapuli/)\n', 'Source: https://www.kaggle.com/kirankunapuli/ieee-fraud-lightgbm-with-gpu/comments']",gpu instal kirankunapuli http www kaggl com kirankunapuli sourc http www kaggl com kirankunapuli ieee fraud lightgbm gpu comment
1509,['#### Prepare Data'],prepar data
1510,['### LGBM Model'],lgbm model
1511,"['#### Principle Component Analysis\n', 'How features explain the majority of the variance amongst these features?\n', '\n', ""[Jake VanderPlas's PythonDataScienceHandbook](https://jakevdp.github.io/PythonDataScienceHandbook/05.09-principal-component-analysis.html)""]",principl compon analysi featur explain major varianc amongst featur jake vanderpla pythondatasciencehandbook http jakevdp github io pythondatasciencehandbook 05 09 princip compon analysi html
1512,"['## Iterative Split Feature Importance, Feature Selection with Early Stopping']",iter split featur import featur select earli stop
1513,['#### Run Submission set on Final Features:'],run submiss set final featur
1514,['#### Submit'],submit
1515,"['**Reflection:** <br>\n', 'Ideally I wanted to use feature permutation to do this iterative feature selection, but it is too computationally expensive (even on CPU kernel)']",reflect br ideal want use featur permut iter featur select comput expens even cpu kernel
1516,"['In this kernel I will do my EDA on the dataset, make some visualizations, try to find any insights and create some new features.\n', '\n', 'Join me, it promises to be a thrilling adventure.\n', '\n', 'Some tricks being used:\n', '* [card1 count encoding](#1)\n', '* [Covariate Shift](#2)\n', '* [features interaction](#3)\n', '* [data relaxation](#4)\n', '\n', 'New engineered features:\n', '* [Number of NaNs](#5)\n', ""* [TransactionAmt and it's decimal part](#6)""]",kernel eda dataset make visual tri find insight creat new featur join promis thrill adventur trick use card1 count encod 1 covari shift 2 featur interact 3 data relax 4 new engin featur number nan 5 transactionamt decim part 6
1517,['Loading all datasets using multiprocessing. This speads up a process a bit.'],load dataset use multiprocess spead process bit
1518,"['# Transaction DT\n', ""According to the official description 'TransactionDT feature is a timedelta from a given reference datetime (not an actual timestamp).' I see people in some kernels assume that a start date is a 1 of December 2017, but to be honest the exact start date is not that important. \n"", '\n', 'So lets transform TransactionDT into a datetime.']",transact dt accord offici descript transactiondt featur timedelta given refer datetim actual timestamp see peopl kernel assum start date 1 decemb 2017 honest exact start date import let transform transactiondt datetim
1519,['And now combining both mean of isFraud by day and number of training examples by day into a single plot.'],combin mean isfraud day number train exampl day singl plot
1520,"['<a id=""1""></a>\n', '# card1\n', 'I have decided to start from one of the most important features of this dataset according to LightGBM feature_importance. And **card1** is one of those features.\n', '\n', ""What I did is I've created a separate dataset with only this feature in it and also I added one more feature to this new dataset, which is an original feature's frequency (count) encoding. Why I did this? Well, you can reference [Santander Customer Transaction Prediction](https://www.kaggle.com/c/santander-customer-transaction-prediction) competition, where this kind of encoding really boosted a score up. \n"", '\n', ""I'll make some visualizations (shoutout to [Chris Deotte](https://www.kaggle.com/cdeotte)) to show you why that works and might work in this case as well.""]",id 1 card1 decid start one import featur dataset accord lightgbm featur import card1 one featur creat separ dataset featur also ad one featur new dataset origin featur frequenc count encod well refer santand custom transact predict http www kaggl com c santand custom transact predict competit kind encod realli boost score make visual shoutout chri deott http www kaggl com cdeott show work might work case well
1521,"[""So if we train a simple decision tree, using this two features we have an AUC slightly higher that 0.5. Let's see why by plotting this tree as a graph""]",train simpl decis tree use two featur auc slightli higher 0 5 let see plot tree graph
1522,['The first split is by the values less than or equal to 10881.5 (black line) and the second one is 8750.0 (red line) and a tree does not use a count feature at all.'],first split valu le equal 10881 5 black line second one 8750 0 red line tree use count featur
1523,['But lets take a little step back and train a boosting model on only one original feature card1'],let take littl step back train boost model one origin featur card1
1524,"['This is a heatmap with a probability of isFraud=1 for every unique value in the **card1** feature.\n', '\n', 'This picture reminds me an opening from a Total Recall movie. ']",heatmap probabl isfraud 1 everi uniqu valu card1 featur pictur remind open total recal movi
1525,['Now lets add a second feature - count encoded **card1** values.'],let add second featur count encod card1 valu
1526,"['Holdout score has significantly increased. Lets create another heatmap and see why. \n', '\n', ""There are some darker spots in some intersections of the variable **card1** values and it's count encoded values. This is the reason of the holdout score improvement.\n"", '\n', '*The image is pre-rendered since rendering takes some significant amount of time*']",holdout score significantli increas let creat anoth heatmap see darker spot intersect variabl card1 valu count encod valu reason holdout score improv imag pre render sinc render take signific amount time
1527,['![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F1696976%2F7153f1242daa586d6849c83242c3fe40%2F35267aee89a7552caf082b6bb0039aa5-full.png?generation=1564585074348507&alt=media)'],http www googleapi com download storag v1 b kaggl user content inbox 2f1696976 2f7153f1242daa586d6849c83242c3fe40 2f35267aee89a7552caf082b6bb0039aa5 full png gener 1564585074348507 alt medium
1528,"['Plotting this variable gives us such information as:\n', '* distribution in train and test set is almost equal.\n', '* distribution between target values differs, which make this feature so valuable\n', ""* this feature doesn't have any NaNs""]",plot variabl give u inform distribut train test set almost equal distribut target valu differ make featur valuabl featur nan
1529,"['<a id=""2""></a>\n', 'Lets check a Covariate Shift of the feature. This means that we will try to distinguish whether a values correspond to a training set or to a testing set.']",id 2 let check covari shift featur mean tri distinguish whether valu correspond train set test set
1530,"['ROC AUC score is close to 0.5, this means that this feature almost does not have any shift between train and test and is definitely worth keeping it.']",roc auc score close 0 5 mean featur almost shift train test definit worth keep
1531,['# ProductCD'],productcd
1532,['# card2'],card2
1533,"[""Making a count feature for card2 to perform the same experiment as with card1. First the heatmap for all possible interactions of card2 feature and it's count.""]",make count featur card2 perform experi card1 first heatmap possibl interact card2 featur count
1534,"['And a scatter plot with a ""decision boundary"" of the model. White \'X\' marks represents a test set examples.']",scatter plot decis boundari model white x mark repres test set exampl
1535,['# card3'],card3
1536,['# card4'],card4
1537,['# card5'],card5
1538,['# card6'],card6
1539,['# addr1 '],addr1
1540,"['Another feature with a relatively high importance is **addr1**. According to the name of the feature we can assume that it contains some kind of users address, but in an encoded way. Also this time a feature have some missing values. We are going to fill them with 0.']",anoth featur rel high import addr1 accord name featur assum contain kind user address encod way also time featur miss valu go fill 0
1541,['Again training a gradient boosting model with only one feature.'],train gradient boost model one featur
1542,['Predictions heatmap.'],predict heatmap
1543,['So far we are doing exactly the same thing that we have been doing for the previous variable.'],far exactli thing previou variabl
1544,"[""Distribution is the same, amount of NaN's is the same. Some difference in target value distribution. \n"", '\n', 'Next checking Covariate Shift for addr1.']",distribut amount nan differ target valu distribut next check covari shift addr1
1545,"['ROC AUC score is close to 0.5\n', '\n', 'This feature also does not have any shift between train and test set.']",roc auc score close 0 5 featur also shift train test set
1546,"['<a id=""3""></a>\n', '# card1 to addr1 interaction\n', '\n', 'Next I am going to create a new feature out of this two features interaction and train on the result.']",id 3 card1 addr1 interact next go creat new featur two featur interact train result
1547,"['First training a model only using this two features, without their interaction.']",first train model use two featur without interact
1548,['And now WITH interaction'],interact
1549,['Predictions heatmap of the two features interaction.'],predict heatmap two featur interact
1550,"['Finally adding count features, so all in all we have 5 features']",final ad count featur 5 featur
1551,"['<a id=""5""></a>\n', ""# New feature: number of NaN's\n"", ""We have plenty of NaN's in this dataset and they can have a significant effect so why don't we use them?\n"", ""I am adding a new column to the dateset, which will contain a number of NaN for each row. So if a row (a single training example) contain, say, 10 NaNs, a new feature's value for this row will be 10.""]",id 5 new featur number nan plenti nan dataset signific effect use ad new column dateset contain number nan row row singl train exampl contain say 10 nan new featur valu row 10
1552,"['We can see that this feature might be useful, but also keep in mind that covatiate shift is almost 0.7, which tells us that the distribution between train and test set has some difference.']",see featur might use also keep mind covati shift almost 0 7 tell u distribut train test set differ
1553,"['<a id=""6""></a>\n', ""# TransactionAmt and it's decimal part\n"", '\n', ""First let's take a look at TransactionAmt feature and them I will create a new one - it's decimal part, which is a very popular way of creating a new features.""]",id 6 transactionamt decim part first let take look transactionamt featur creat new one decim part popular way creat new featur
1554,['Moving average for TransactionAmt over time.'],move averag transactionamt time
1555,['A relationship between mean of TransactionAmt by day and a mean of isFraud by day.'],relationship mean transactionamt day mean isfraud day
1556,['Decimal part of transaction amount.'],decim part transact amount
1557,['A relationship between mean of TransactionAmt_decimal by day and a mean of isFraud by day.'],relationship mean transactionamt decim day mean isfraud day
1558,"[""Lenght of the decimal part of transaction amount. What does it mean? Well, if lenght is 1 or 2 signs it is totaly understandable - it might be cents. But what is wrong with a decimal part's lenght being 3 and more sings? Maybe it is due to a currency convertion?""]",lenght decim part transact amount mean well lenght 1 2 sign totali understand might cent wrong decim part lenght 3 sing mayb due currenc convert
1559,['Covariate shift for all 3 features.'],covari shift 3 featur
1560,['# V1'],v1
1561,['# V2'],v2
1562,['# V3'],v3
1563,['# V4'],v4
1564,['# V5'],v5
1565,['# V6'],v6
1566,['# V7'],v7
1567,['# V258'],v258
1568,"['<a id=""4""></a>\n', ""This is where I want to introduce a little trick to you, called data relaxation. So what is it? In order to understand it take a look at the plot above. See the distibution difference between train and test set at a certain point? Gradient boosting algorithm doesn't know what to do with a data it has never seen so it will not approximate it well. And what we do by relaxing data is we are removing all the values from the train set that appears in it 3 times more often than in a test set and vice versa, also cleaning all the data that appears in train and test set only couple of times.\n"", '\n', '## V258 after data relaxation']",id 4 want introduc littl trick call data relax order understand take look plot see distibut differ train test set certain point gradient boost algorithm know data never seen approxim well relax data remov valu train set appear 3 time often test set vice versa also clean data appear train test set coupl time v258 data relax
1569,['# V294'],v294
1570,['## V294 after data relaxation'],v294 data relax
1571,['# C1'],c1
1572,['## C1 after data relaxation'],c1 data relax
1573,['# C2'],c2
1574,['## C2 after data relaxation.'],c2 data relax
1575,['# C3'],c3
1576,['## C3 after data relaxation'],c3 data relax
1577,['# C4'],c4
1578,['## C4 after data relaxation'],c4 data relax
1579,['# C5'],c5
1580,['## C5 after data relaxation'],c5 data relax
1581,['# C6'],c6
1582,['## C6 after data relaxation'],c6 data relax
1583,['# C7'],c7
1584,['## C7 after data relaxation'],c7 data relax
1585,['# C8'],c8
1586,['## C8 after data relaxation'],c8 data relax
1587,['# C9'],c9
1588,['## C9 after data relaxation'],c9 data relax
1589,['# C10'],c10
1590,['## C10 after data relaxation'],c10 data relax
1591,['# C11'],c11
1592,['## C11 after data relaxation'],c11 data relax
1593,['# C12'],c12
1594,['# C12 after data relaxation'],c12 data relax
1595,['# C13'],c13
1596,['# C13 after data relaxation'],c13 data relax
1597,['# C14'],c14
1598,['## C14 after data relaxation'],c14 data relax
1599,['# D1'],d1
1600,['# D2'],d2
1601,['# D3'],d3
1602,['# D4'],d4
1603,['# D5'],d5
1604,['# D6'],d6
1605,['# D7'],d7
1606,['# D8'],d8
1607,['# D9'],d9
1608,['# D10'],d10
1609,['# D11'],d11
1610,['# D12'],d12
1611,['# D13'],d13
1612,['# D14'],d14
1613,['# D15'],d15
1614,['# id_01'],id 01
1615,['# id_02'],id 02
1616,['# id_03'],id 03
1617,['# id_04'],id 04
1618,['# id_05'],id 05
1619,['# id_06'],id 06
1620,['# id_07'],id 07
1621,['# id_08'],id 08
1622,['# id_09'],id 09
1623,['# id_10'],id 10
1624,['# id_11'],id 11
1625,['# id_12'],id 12
1626,['# id_13'],id 13
1627,['# id_14'],id 14
1628,['# id_15'],id 15
1629,['# id_16'],id 16
1630,['# id_17'],id 17
1631,['# id_31'],id 31
1632,['## id_31 after data relaxation'],id 31 data relax
1633,"['We can eleminate some useless features already at the begining. Such as:\n', '* Features with only 1 unique value\n', '* Features with more than 90% missing values\n', '* Features with the top value appears more than 90% of the time']",elemin useless featur alreadi begin featur 1 uniqu valu featur 90 miss valu featur top valu appear 90 time
1634,['And here we go. This would take a while.'],go would take
1635,['Printing out all features with rank 1'],print featur rank 1
1636,"['# Hot to track you Data Science experiments with neptune.ml\n', '![](https://neptune.ml/wp-content/uploads/2018/08/Company-Header-Neptune.ml_-2-e1560327936998.png)\n', '\n', ""*Important disclaimer: I am not an owner nor a developer of the presented service. I only use it for my own projects (including Kaggle competitions) because I've found it very useful.*\n"", '\n', '* Have you ever faced a situation when some of your experiments have finished, you got the results, but you completely forgot what exactly you have changed?\n', '* Maybe you want to run several of experiments to check multiple ideas and see the results in a convenient form?\n', '* Or you want to collaborate with your teammates more effectively?\n', '\n', ""If your answer is 'yes' to any of those questions then you will like a [neptune](https://neptune.ml/) project.\n"", '\n', 'In this kernel I will show you how to set it up and use in your kaggle competitions.']",hot track data scienc experi neptun ml http neptun ml wp content upload 2018 08 compani header neptun ml 2 e1560327936998 png import disclaim owner develop present servic use project includ kaggl competit found use ever face situat experi finish got result complet forgot exactli chang mayb want run sever experi check multipl idea see result conveni form want collabor teammat effect answer ye question like neptun http neptun ml project kernel show set use kaggl competit
1637,"['1. [Registering and installing neptune client](#1)\n', '2. [Setting up a project](#2)\n', '3. [Running an experiment](#3)\n', '4. [Track parameters](#4)\n', '5. [Track images](#5)\n', '6. [Track artifacts](#6)\n', '7. [Monitoring resources](#7)\n', '8. [Conclusion](#8)']",1 regist instal neptun client 1 2 set project 2 3 run experi 3 4 track paramet 4 5 track imag 5 6 track artifact 6 7 monitor resourc 7 8 conclus 8
1638,"['<a id=""1""></a>\n', '## Registering and installing neptune client\n', '\n', ""I think there is no need to describe how to register in neptune, just go to the project page and click 'Sign up'. Google, Facebook and Github SSO works as well.\n"", '\n', ""Next step is to install a neptune client. In order to do this enable an Internet access in your kernel's settings.\n"", '![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F1696976%2Ffc475a6556225863484d04e9ed1baecb%2Fneptune_kernel_1.png?generation=1564558569759774&alt=media)\n', '\n', 'Then run *pip install neptune-client*']",id 1 regist instal neptun client think need describ regist neptun go project page click sign googl facebook github sso work well next step instal neptun client order enabl internet access kernel set http www googleapi com download storag v1 b kaggl user content inbox 2f1696976 2ffc475a6556225863484d04e9ed1baecb 2fneptun kernel 1 png gener 1564558569759774 alt medium run pip instal neptun client
1639,"['<a id=""2""></a>\n', '## Setting up a project\n', 'Neptune has a good documentation let alone an interface is intuitive and easy to use. But I still will guide you with your first experiment.\n', '\n', ""First of all we need to create a project for our experiments. Let's do it.\n"", '\n', ""Go to 'Projects' in the upper-left corner and then click a 'New project' button. You will see a modal window with some fields. Let's fill them and proceed.\n"", '\n', '![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F1696976%2Fbd37ce0d4cda1c6908f808941b59f358%2Fneptune_kernel_2.png?generation=1564558568867977&alt=media)']",id 2 set project neptun good document let alon interfac intuit easi use still guid first experi first need creat project experi let go project upper left corner click new project button see modal window field let fill proceed http www googleapi com download storag v1 b kaggl user content inbox 2f1696976 2fbd37ce0d4cda1c6908f808941b59f358 2fneptun kernel 2 png gener 1564558568867977 alt medium
1640,"[""Let's get familiar with an interface. \n"", '\n', '![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F1696976%2F7fbbbc2d193fcb0a30ea2438b5fc90c6%2Fneptune_kernel_3.png?generation=1564558568764085&alt=media)\n', '\n', 'We have 4 tabs at the top:\n', '* Wiki\n', '* Notebooks\n', '* Experiments\n', '* Settings\n', '\n', 'Wiki is a README and comments for you project\n', '\n', 'Notebooks contains all of the notebooks you are tracking. If you are using a Jupyter notebook (which I bet you are) then you can install a jupyter extension called *neptune-notebooks* and integrate it. After that by simply clicking on one button you will save a notebook checkpoint to your project and then you can keep working without a fear to remove some cell or rewrite a code in it. You will always have a backup. The outputs of the cells are being saved as well.\n', '\n', '![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F1696976%2Ff7d9882ddc9c732be20bcd438d9677d9%2Fneptune_kernel_4.png?generation=1564564644071941&alt=media)\n', '\n', 'Unfortunately kaggle kernels does not support such integration but you can upvote my [feature request](https://www.kaggle.com/product-feedback/101200#583902) for it.']",let get familiar interfac http www googleapi com download storag v1 b kaggl user content inbox 2f1696976 2f7fbbbc2d193fcb0a30ea2438b5fc90c6 2fneptun kernel 3 png gener 1564558568764085 alt medium 4 tab top wiki notebook experi set wiki readm comment project notebook contain notebook track use jupyt notebook bet instal jupyt extens call neptun notebook integr simpli click one button save notebook checkpoint project keep work without fear remov cell rewrit code alway backup output cell save well http www googleapi com download storag v1 b kaggl user content inbox 2f1696976 2ff7d9882ddc9c732be20bcd438d9677d9 2fneptun kernel 4 png gener 1564564644071941 alt medium unfortun kaggl kernel support integr upvot featur request http www kaggl com product feedback 101200 583902
1641,"['<a id=""3""></a>\n', '## Running an experiment\n', '\n', 'It is time to create and run our first experiment. To track you experiment first you need to initialize it using neptune.init() method with your token.\n', '\n', ""You can obtain a token by clicking your user icon in the upper-right corner and selecting 'Get API Token'\n"", '\n', '![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F1696976%2F03662a1b9f79b4da78120f1f390313c8%2Fneptune_kernel_5.png?generation=1564558561773321&alt=media)\n', '\n', 'Next I will create a small dataset with a synthetic data using sklearn.datasets.make_classification and run our first experiment.']",id 3 run experi time creat run first experi track experi first need initi use neptun init method token obtain token click user icon upper right corner select get api token http www googleapi com download storag v1 b kaggl user content inbox 2f1696976 2f03662a1b9f79b4da78120f1f390313c8 2fneptun kernel 5 png gener 1564558561773321 alt medium next creat small dataset synthet data use sklearn dataset make classif run first experi
1642,"[""As an output we have an experiment ID and a link to it. Let's see how the experiment results look in neptune interface. \n"", '\n', 'Here is a result on the dashboard:\n', '\n', '![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F1696976%2Fc3c9eb7e9965ee7317ca0fcd350c770e%2Fneptune_kernel_6.png?generation=1564559613195430&alt=media)\n', '\n', ""We called our metric 'AUC', but this is not a default column in neptune, so in order to see it we need to add it the dasboard.\n"", '\n', '![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F1696976%2F9324a9242a9789c6d9fbbd92a9f3d06f%2Fneptune_kernel_7.png?generation=1564559774143612&alt=media)\n']",output experi id link let see experi result look neptun interfac result dashboard http www googleapi com download storag v1 b kaggl user content inbox 2f1696976 2fc3c9eb7e9965ee7317ca0fcd350c770 2fneptun kernel 6 png gener 1564559613195430 alt medium call metric auc default column neptun order see need add dasboard http www googleapi com download storag v1 b kaggl user content inbox 2f1696976 2f9324a9242a9789c6d9fbbd92a9f3d06f 2fneptun kernel 7 png gener 1564559774143612 alt medium
1643,"['<a id=""4""></a>\n', '## Track parameters\n', '\n', 'Another useful feature is an ability to save the parameters of the model for your experiment. Here is how you can do it.']",id 4 track paramet anoth use featur abil save paramet model experi
1644,"[""You can now find this parameters in the experiment's page if you follow the link above.\n"", '\n', '![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F1696976%2F9ee8902a6a74a207a3d475c205414f4a%2Fneptune_kernel_8.png?generation=1564561096062206&alt=media)\n', '\n', 'As for dashboard - you should select parameters you want to be displayed. You can then sort and filter your experiments by any of them.']",find paramet experi page follow link http www googleapi com download storag v1 b kaggl user content inbox 2f1696976 2f9ee8902a6a74a207a3d475c205414f4a 2fneptun kernel 8 png gener 1564561096062206 alt medium dashboard select paramet want display sort filter experi
1645,"['<a id=""5""></a>\n', '## Track images\n', '\n', ""Another thing you can log is an images. If you are more visual person this might be really helpful feature for you. Let's, for example, train another model and...""]",id 5 track imag anoth thing log imag visual person might realli help featur let exampl train anoth model
1646,"[""An image is now available in 'Logs' section of the experiment's page.\n"", '\n', '![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F1696976%2F545fec8350e6c060d49eac18b0d33ddb%2Fneptune_kernel_9.png?generation=1564562627500744&alt=media)']",imag avail log section experi page http www googleapi com download storag v1 b kaggl user content inbox 2f1696976 2f545fec8350e6c060d49eac18b0d33ddb 2fneptun kernel 9 png gener 1564562627500744 alt medium
1647,"['<a id=""6""></a>\n', '## Track artifacts\n', '\n', ""You can also send some artifacts. For example lets dump the model to pkl and send it to neptune. It is going to be stored in 'Artifacts' section.\n"", '\n', 'Also we can use neptune experiment in more pythonic way.']",id 6 track artifact also send artifact exampl let dump model pkl send neptun go store artifact section also use neptun experi python way
1648,"['<a id=""7""></a>\n', '## Monitoring resources\n', '\n', ""Neptun client has another cool feature - by default it tracks all the resources usage metrics, such as CPU, Memory and GPU usage. They can be found in the 'Monitoring' section of the experiment's page. \n"", '\n', 'For example you can see how your model utilizes a GPU during the experiment.\n', '\n', '![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F1696976%2F0946f5f0d667c85296a0cddfe23497e8%2Fneptune_kernel_10.png?generation=1564563636462867&alt=media)']",id 7 monitor resourc neptun client anoth cool featur default track resourc usag metric cpu memori gpu usag found monitor section experi page exampl see model util gpu experi http www googleapi com download storag v1 b kaggl user content inbox 2f1696976 2f0946f5f0d667c85296a0cddfe23497e8 2fneptun kernel 10 png gener 1564563636462867 alt medium
1649,"['<a id=""8""></a>\n', '## Conclusion\n', '\n', ""I have described only basic features and functionality of neptune and if you like it then don't hesitate to explore this servis on our own. Trust me there is much more cool stuff to see.""]",id 8 conclus describ basic featur function neptun like hesit explor servi trust much cool stuff see
1650,"[""# Detailed exploration of IEEE-CIS 'Fraud Detection' dataframe\n"", 'Here I examine and plot all features/variables from the training dataset, adding notes for all plots for later development\n', '\n', ""**This kernel is a bit long, so I'm continuining here with missing values analysis:**  \n"", 'https://www.kaggle.com/pabloinsente/ieee-missing-nan-values-analysis-and-imputation\n', '\n', '## Description variables/features:\n', '(https://www.kaggle.com/c/ieee-fraud-detection/discussion/101203#latest-583068) \n', '\n', '### Transaction Table:\n', '- TransactionDT: timedelta from a given reference datetime (not an actual timestamp)\n', '- TransactionAMT: transaction payment amount in USD\n', '- ProductCD: product code, the product for each transaction\n', '- card1 - card6: payment card information, such as card type, card category, issue bank, country, etc.\n', '- addr: address\n', '- dist: distance\n', '- P_ and (R__) emaildomain: purchaser and recipient email domain\n', '- C1-C14: counting, such as how many addresses are found to be associated with the payment card, etc. The actual meaning is masked.\n', '- D1-D15: timedelta, such as days between previous transaction, etc.\n', '- M1-M9: match, such as names on card and address, etc.\n', '- Vxxx: Vesta engineered rich features, including ranking, counting, and other entity relations.\n', '\n', '\n', '### Identity Table:\n', '- Variables in this table are identity information – network connection information (IP, ISP, Proxy, etc) and digital signature (UA/browser/os/version, etc) associated with transactions. \n', ""- They're collected by Vesta’s fraud protection system and digital security partners.\n"", '- (The field names are masked and pairwise dictionary will not be provided for privacy protection and contract agreement)']",detail explor ieee ci fraud detect datafram examin plot featur variabl train dataset ad note plot later develop kernel bit long continuin miss valu analysi http www kaggl com pabloinsent ieee miss nan valu analysi imput descript variabl featur http www kaggl com c ieee fraud detect discus 101203 latest 583068 transact tabl transactiondt timedelta given refer datetim actual timestamp transactionamt transact payment amount usd productcd product code product transact card1 card6 payment card inform card type card categori issu bank countri etc addr address dist distanc p r emaildomain purchas recipi email domain c1 c14 count mani address found associ payment card etc actual mean mask d1 d15 timedelta day previou transact etc m1 m9 match name card address etc vxxx vesta engin rich featur includ rank count entiti relat ident tabl variabl tabl ident inform network connect inform ip isp proxi etc digit signatur ua browser o version etc associ transact collect vesta fraud protect system digit secur partner field name mask pairwis dictionari provid privaci protect contract agreement
1651,['# I. Import data'],import data
1652,['# II. Explore data: describe single variables'],ii explor data describ singl variabl
1653,['## Plot univariate distributions'],plot univari distribut
1654,"['### Categorical variables according to dataset documentation\n', '** Categorical Features - Transaction:**  \n', '- ProductCD\n', '- card1 - card6\n', '- addr1, addr2\n', '- P_emaildomain\n', '- R_emaildomain\n', '- M1 - M9\n', '\n', '** Categorical Features - Identity:**\n', '- DeviceType\n', '- DeviceInfo\n', '- id_12 - id_38']",categor variabl accord dataset document categor featur transact productcd card1 card6 addr1 addr2 p emaildomain r emaildomain m1 m9 categor featur ident devicetyp deviceinfo id 12 id 38
1655,['## Plot categorical variables'],plot categor variabl
1656,"['**Plot I:  target, ProductCD, Devicetype, DeviceInfo**\n']",plot target productcd devicetyp deviceinfo
1657,"['**Plot I notes:**\n', '- Very unbalance target (isFraud)\n', '- Very unbalance product type purchase\n', '- Most purchases are made on desktop devices']",plot note unbal target isfraud unbal product type purchas purchas made desktop devic
1658,['**Plot II: DeviceInfo**'],plot ii deviceinfo
1659,"['**Plot II notes:**\n', '\n', 'The top devices are:\n', '1. Windows\n', '2. iOS\n', '3. Trident\n', '4. MacOS']",plot ii note top devic 1 window 2 io 3 trident 4 maco
1660,"['**Plot III: cards 1,2,3, and 5**']",plot iii card 1 2 3 5
1661,"['**Plot III notes:**  \n', '- The bulk of the transactions are on card1 and2  \n', '- Not sure about identity of card3 and card5\n', '- They may be dollars amount per transaction, or some sort of identifier ']",plot iii note bulk transact card1 and2 sure ident card3 card5 may dollar amount per transact sort identifi
1662,"['**Plot IV notes:**  \n', '- Card4 refers to visa brand; most transactions are on Visa and Mastercard \n', '- Card5 refers to type of card; most transactions are debit ']",plot iv note card4 refer visa brand transact visa mastercard card5 refer type card transact debit
1663,['**Plot V: addr1**'],plot v addr1
1664,['**Plot VI: addr2**'],plot vi addr2
1665,"['**Plot V-VI notes:**\n', '- Transactions on addr1 are more evenly distributed\n', '- Transactions on addr2 has 1 big outlier ']",plot v vi note transact addr1 evenli distribut transact addr2 1 big outlier
1666,['**Plot VII: emaildomains**'],plot vii emaildomain
1667,"['**Plot VII notes:**\n', '- As expected, gmail is at the top.\n', ""- There is one interesting 'anonymous.com' domain""]",plot vii note expect gmail top one interest anonym com domain
1668,"['**Plot VIII notes:**\n', '- Identity of M1-M9 is still unclear\n', '- Basically boolean variables; M4 seems to be different']",plot viii note ident m1 m9 still unclear basic boolean variabl m4 seem differ
1669,"['**Notes id12-id38**\n', '- There is a mix of data types\n', '- Mostly NaN values\n', '- id30 is OS again\n', '- id31 is browser\n']",note id12 id38 mix data type mostli nan valu id30 o id31 browser
1670,['**Plot IX: id_30**'],plot ix id 30
1671,['**Plot X: id_31**'],plot x id 31
1672,"['**Plot X notes:**\n', '- Most transactions are done with Windows 7 and 10, and iOS\n', '- Most transactions are done with chrome and safari']",plot x note transact done window 7 10 io transact done chrome safari
1673,['**Plot XI: ProductCD**'],plot xi productcd
1674,['## Plot continuous variables '],plot continu variabl
1675,"['**Plot XII notes:**\n', '- TransactionDT is evenly distributed, unclear identity\n', '- Transaction amount follows a log distribution, with a few large outliers']",plot xii note transactiondt evenli distribut unclear ident transact amount follow log distribut larg outlier
1676,['**Plot XIII: C7 - C14**'],plot xiii c7 c14
1677,"['**Plot XIII notes:**\n', '- All variables follow roughly a log distribution \n', '- Identity is unclear']",plot xiii note variabl follow roughli log distribut ident unclear
1678,['**Plot XIV: D1 - D15**'],plot xiv d1 d15
1679,"['**Plot XIV notes:**\n', '- D11-D15 have negative values, which may say something about the identity of the feature\n', '- D9 has a different distribution, kinda binomial\n', '- The rest roughly a log distribution']",plot xiv note d11 d15 neg valu may say someth ident featur d9 differ distribut kinda binomi rest roughli log distribut
1680,['**Exploration V1 - V339**'],explor v1 v339
1681,"['**Notes:**\n', '- From V1-V305 & V322-V339 seems to be mostly 0 - 1 values, which may indicate that they are actually a categorical feature \n', '- From V306-V321 seems to be true continuous variables \n', '- More interesting insights may come from computing averages by target feature']",note v1 v305 v322 v339 seem mostli 0 1 valu may indic actual categor featur v306 v321 seem true continu variabl interest insight may come comput averag target featur
1682,['**Plot XV: id_01 - id_11**'],plot xv id 01 id 11
1683,['**Plot XV: id_01 - id_11 / SAME as LOG distributions**'],plot xv id 01 id 11 log distribut
1684,"['**Plot XV notes:**\n', '- id_02 may be dollar amounts, with log distribution\n', '- id_01 - id_10 have negative values, but it is unlikely to indicate debt given values\n', '- id_07 - id_08 are kinda normally distributed']",plot xv note id 02 may dollar amount log distribut id 01 id 10 neg valu unlik indic debt given valu id 07 id 08 kinda normal distribut
1685,['# III. Explore data: describe variables by target (Fraud/not Not Fraud)'],iii explor data describ variabl target fraud fraud
1686,['## Plot/Explore bivariate relationships'],plot explor bivari relationship
1687,"['**Plot I:  target, ProductCD, Devicetype, DeviceInfo / Target**']",plot target productcd devicetyp deviceinfo target
1688,['**Plot I as percentage**'],plot percentag
1689,"['**Notes:**\n', '- ProductCD: C and S types have the highest number AND proportion of Fraud Transactions\n', '- DeviceType: mobile has the highest number AND proportion of Fraud Transactions (not by much though)']",note productcd c type highest number proport fraud transact devicetyp mobil highest number proport fraud transact much though
1690,['**Plot II: Fraud transactions by OS**'],plot ii fraud transact o
1691,"['**Notes**\n', '- Fraud transaction cases come mostly from Windows and iOS devices. This is predictable given the vast majority of all transactions come from those systems. Still, the problem is this feature will still send the signal to the model that Windos/iOS_Device transactions -> likely fraud relative to other systems\n', '- Trident OS drop 5 places (3th overall, 8th on Fraud transactions)']",note fraud transact case come mostli window io devic predict given vast major transact come system still problem featur still send signal model windo io devic transact like fraud rel system trident o drop 5 place 3th overal 8th fraud transact
1692,"['**Plot III: cards 1,2,3, and 5**']",plot iii card 1 2 3 5
1693,"['**Notes:**\n', '- card1, card2 and card3 show similar distribution patterns fraud/no_fraud\n', ""- card5 reverse proportion around value '225' ""]",note card1 card2 card3 show similar distribut pattern fraud fraud card5 revers proport around valu 225
1694,['**Plot IV: cards 4 and 6**'],plot iv card 4 6
1695,['**Plot IV as percentage**'],plot iv percentag
1696,"['**Notes:**\n', '- Visa has the higher NUMBER of Fraud, but such number is a minor proportion of all VISA transactions\n', '- Discover has very few Fraud transaction, yet as percentage of all Discover transactions is a bit higher\n', '- Most Fraud transactions are done with Debit, but there is a higher proportion of Fraud Transactions within Credit ']",note visa higher number fraud number minor proport visa transact discov fraud transact yet percentag discov transact bit higher fraud transact done debit higher proport fraud transact within credit
1697,['**Plot V: addr1**'],plot v addr1
1698,"['**Notes:**\n', '- Most fraud transactions come from addr1 204; most not fraud come from 299\n', '- First 5 addr1 are the same, but in different rank-order']",note fraud transact come addr1 204 fraud come 299 first 5 addr1 differ rank order
1699,['**Plot VI: addr2**'],plot vi addr2
1700,['**Plot VII: emaildomains by Fraud status**'],plot vii emaildomain fraud statu
1701,['**Plot VII: emaildomains by Fraud status as percentage**'],plot vii emaildomain fraud statu percentag
1702,"['**Notes:**  \n', ""- **'Protonmail.com'**,  **'mail.com'**, **'outlook.es'**, and, **'net.zero'** have a high proportion of Fraud transaction, yet they account for an small total number of fraud transactions""]",note protonmail com mail com outlook e net zero high proport fraud transact yet account small total number fraud transact
1703,['**Plot VIII: M1 - M9 by Fraud status variables**'],plot viii m1 m9 fraud statu variabl
1704,['**Plot VIII: M1 - M9 variables by Fraud status as percentage**'],plot viii m1 m9 variabl fraud statu percentag
1705,"['**Notes:**\n', '- As frequency is hard to catch meaningful differences between classes\n', ""- As percentage there are some interesting patterns on 'M4' where class M2 get the highest proportion of Fraud transactions, or M1 where 'F' doesn't get any Fraud cases""]",note frequenc hard catch meaning differ class percentag interest pattern m4 class m2 get highest proport fraud transact m1 f get fraud case
1706,['**Plot IX**'],plot ix
1707,['**Plot IX as percentage**'],plot ix percentag
1708,"['**Notes**\n', '- **id_30**: Other and Android 5.1.1 have the highest proportion of Fraud, **BUT**  negligible frequency: Other have 6 cases and Android 5.1.1, 101 cases\n', ""- **id_31**: Lanix, Mozilla, comodo, and lanix have really high proportions of 'Fraud', *BUT*, negible frequency: Lanix/Ilium 1 fraud, Mozilla/Firefox 5 fraud cases, comodo 2, lanix 1""]",note id 30 android 5 1 1 highest proport fraud neglig frequenc 6 case android 5 1 1 101 case id 31 lanix mozilla comodo lanix realli high proport fraud negibl frequenc lanix ilium 1 fraud mozilla firefox 5 fraud case comodo 2 lanix 1
1709,['**Plot XI: ProductCD by Fraud status**'],plot xi productcd fraud statu
1710,['**Plot XI: ProductCD by fraud status as percentage**'],plot xi productcd fraud statu percentag
1711,"['**Notes**  \n', '  \n', ""**ProductCD is 'Production code'**   \n"", ""- 'C': has both the highest number AND the highest proportion of Fraud transactions\n"", ""- 'W': has a similar frequency of Fraud transactions for a minor proportion of the W class""]",note productcd product code c highest number highest proport fraud transact w similar frequenc fraud transact minor proport w class
1712,"['**Plot XII: TransactionDT, TransactionAmt by Fraud status**']",plot xii transactiondt transactionamt fraud statu
1713,"['**Notes:**\n', ""- **TransactionDT (time delta from some reference time)**: Not-Fraud transactions tend to be more close to the 'Time zero reference' for the transactions; Fraud transactions tend to be a bit more evenly distributed. There is a pick around 0.55\n"", '- **TransanctionAmt (on dollars)**: Not-Fraud transactions are concentrated on the middle of the distribution, while Fraud transactions are a bit more concentrated on the tails (really small or really bit). This makes a lot of intuitive sense: micro-frauds and large-amounts-frauds are more likely. ']",note transactiondt time delta refer time fraud transact tend close time zero refer transact fraud transact tend bit evenli distribut pick around 0 55 transanctionamt dollar fraud transact concentr middl distribut fraud transact bit concentr tail realli small realli bit make lot intuit sen micro fraud larg amount fraud like
1714,['**Plot XIII: C7 - C14 by Fraud status**'],plot xiii c7 c14 fraud statu
1715,"['**Notes:**\n', ""- This is all supossed to be 'counting' data, yet, we get a bunch of negative values\n"", '- The main patter, is that not-fraud transactions have higher values, more tightly concentrated (high kurtosis), while fraud transactions are more evenly spread out (low kurtosis), which means more outliers']",note suposs count data yet get bunch neg valu main patter fraud transact higher valu tightli concentr high kurtosi fraud transact evenli spread low kurtosi mean outlier
1716,['**Plot XIV: D1 - D15 by Fraud status**'],plot xiv d1 d15 fraud statu
1717,"['**Notes** \n', '- **Main insight**: Fraud transactions tend to be **more spread out over time**, while Not-Fraud transactions tend to be **more clustered around shorter time periods** (from the 0 time-point reference) ']",note main insight fraud transact tend spread time fraud transact tend cluster around shorter time period 0 time point refer
1718,['**Plot XV: id_01 - id_11 by fraud status**'],plot xv id 01 id 11 fraud statu
1719,['**Plot XV: id_01 - id_11 by Fraud status**'],plot xv id 01 id 11 fraud statu
1720,"['**Notes:**\n', '- In the cases where Fraud/Not-Fraud differ, the pattern is the same: **Fraud more clustered with a higher peak**, and **Not-Fraud more spread out with longer/heavier tails**']",note case fraud fraud differ pattern fraud cluster higher peak fraud spread longer heavier tail
1721,"[""**Explore 'V' Features**""]",explor v featur
1722,"['**Notes:**\n', '- There are so many features with no-identity info that it is hard to get a clear insight. It is clear though that there are A LOT features where Fraud transactions have higher means, which means that these variables are going to be variable for the model to learn to capture Fraud cases']",note mani featur ident info hard get clear insight clear though lot featur fraud transact higher mean mean variabl go variabl model learn captur fraud case
1723,"[""**This kernel is a bit long, so I'm continuining here with missing values analysis:**  \n"", 'https://www.kaggle.com/pabloinsente/ieee-missing-nan-values-analysis-and-imputation']",kernel bit long continuin miss valu analysi http www kaggl com pabloinsent ieee miss nan valu analysi imput
1724,"['# EDA, feature engineering, LightGBM baseline\n', '\n', '**Feature engineering:**\n', '* select ~280 features from 432 (-)\n', ""* count 'null' values per row (-)\n"", ""* fill 'null' values with constant (+)\n"", '* remove features that have more than 90% of same values (-)\n', ""* make 'os', 'browser', 'device' from 'id_30', 'id_31' (-)\n"", ""* transform 'TransactionAmt' (+)\n"", '* feature aggregates (-)\n', ""* 'card1', ... count (+)\n"", '* features interaction (+)\n', ""* make 'day', 'hour' features (+)\n"", ""* make 'vendor', 'suffix', 'us' from email features (-)\n"", '* remove timestamp (+)\n', '\n', '**Model:**\n', '* LightGBM\n', '* Optuna to get optimal parameters\n', '* 5 fold cross-validation']",eda featur engin lightgbm baselin featur engin select 280 featur 432 count null valu per row fill null valu constant remov featur 90 valu make o browser devic id 30 id 31 transform transactionamt featur aggreg card1 count featur interact make day hour featur make vendor suffix u email featur remov timestamp model lightgbm optuna get optim paramet 5 fold cross valid
1725,['> ## Load data'],load data
1726,['## Keep selected features only'],keep select featur
1727,"[""## Count 'null' values per row""]",count null valu per row
1728,"[""## Fill 'null' values with constant""]",fill null valu constant
1729,['## Remove features that have more than 90% of same values'],remov featur 90 valu
1730,"['## Transform id_30, id_31']",transform id 30 id 31
1731,"[""## Transform 'TransactionAmt'""]",transform transactionamt
1732,['## Feature aggregates'],featur aggreg
1733,"[""## 'card1', ... count""]",card1 count
1734,['## Features interaction'],featur interact
1735,['## Make day and hour features'],make day hour featur
1736,['## Transform emails'],transform email
1737,['## Remove timestamp'],remov timestamp
1738,['## Encode categorial features'],encod categori featur
1739,['## Free memory'],free memori
1740,['## Extract target variable'],extract target variabl
1741,['> ## LightGBM'],lightgbm
1742,['## Submit predictions'],submit predict
1743,['## Load and prepare data'],load prepar data
1744,['## LightGBM'],lightgbm
1745,['## Test features interaction'],test featur interact
1746,['## Select features'],select featur
1747,['## Test all new features together'],test new featur togeth
1748,['# Load data'],load data
1749,['# Data analysis'],data analysi
1750,"[""### what's target?""]",target
1751,['### isFraud'],isfraud
1752,"['- fraud transaction rate by day, and week']",fraud transact rate day week
1753,"['- fraud transaction rate by weekday, hour, month-day, and year-month']",fraud transact rate weekday hour month day year month
1754,['- fraud transaction rate by day'],fraud transact rate day
1755,['- fraud transaction rate by weekday-hour'],fraud transact rate weekday hour
1756,['- fraud rate by weekday '],fraud rate weekday
1757,['- fraud rate by hour '],fraud rate hour
1758,['- fraud rate by weekday-hour'],fraud rate weekday hour
1759,['- fraud rate by amount-bin'],fraud rate amount bin
1760,['### TransactionID'],transactionid
1761,"['## Identity data\n', '\n', ""Variables in this table are identity information – network connection information (IP, ISP, Proxy, etc) and digital signature (UA/browser/os/version, etc) associated with transactions. They're collected by Vesta’s fraud protection system and digital security partners. (The field names are masked and pairwise dictionary will not be provided for privacy protection and contract agreement)\n"", '\n', 'Categorical Features:\n', '\n', '- DeviceType\n', '- DeviceInfo\n', '- id12 - id38']",ident data variabl tabl ident inform network connect inform ip isp proxi etc digit signatur ua browser o version etc associ transact collect vesta fraud protect system digit secur partner field name mask pairwis dictionari provid privaci protect contract agreement categor featur devicetyp deviceinfo id12 id38
1762,['### id_01 - id_11'],id 01 id 11
1763,['### id_12 - id_38'],id 12 id 38
1764,"['### DeviceType, DeviceInfo']",devicetyp deviceinfo
1765,"['## Transaction data\n', '\n', '- TransactionDT: timedelta from a given reference datetime (not an actual timestamp)\n', '- TransactionAMT: transaction payment amount in USD\n', '- ProductCD: product code, the product for each transaction\n', '- card1 - card6: payment card information, such as card type, card category, issue bank, country, etc.\n', '- addr: address\n', '- dist: distance\n', '- P_ and (R__) emaildomain: purchaser and recipient email domain\n', '- C1-C14: counting, such as how many addresses are found to be associated with the payment card, etc. The actual meaning is masked.\n', '- D1-D15: timedelta, such as days between previous transaction, etc.\n', '- M1-M9: match, such as names on card and address, etc.\n', '- Vxxx: Vesta engineered rich features, including ranking, counting, and other entity relations.']",transact data transactiondt timedelta given refer datetim actual timestamp transactionamt transact payment amount usd productcd product code product transact card1 card6 payment card inform card type card categori issu bank countri etc addr address dist distanc p r emaildomain purchas recipi email domain c1 c14 count mani address found associ payment card etc actual mean mask d1 d15 timedelta day previou transact etc m1 m9 match name card address etc vxxx vesta engin rich featur includ rank count entiti relat
1766,['### TransactionDT'],transactiondt
1767,['### TransactionAmt'],transactionamt
1768,['### ProductCD'],productcd
1769,['### card1 - card6'],card1 card6
1770,"['### addr1, addr2']",addr1 addr2
1771,"['### dist1, dist2']",dist1 dist2
1772,"['### P_emaildomain, R_emaildomain']",p emaildomain r emaildomain
1773,['### C1 - C14'],c1 c14
1774,['- Cx & card'],cx card
1775,['### D1-D15'],d1 d15
1776,['- Dx & card'],dx card
1777,['### M1 - M9'],m1 m9
1778,['### Vxxx'],vxxx
1779,['# Feature engineering'],featur engin
1780,['# Predict'],predict
1781,"['Forked from https://www.kaggle.com/tunguz/adversarial-ieee\n', '\n', 'My contribution  \n', '1) The data is time-split, so doing the validation without the date column makes sense.  \n', '2) Added feature-importance(gain)']",fork http www kaggl com tunguz adversari ieee contribut 1 data time split valid without date column make sen 2 ad featur import gain
1782,"['Still a very high AUC.  \n', 'Perhaps another shaky competition?']",still high auc perhap anoth shaki competit
1783,"[""Let's take a look how similar train and test sets are.""]",let take look similar train test set
1784,['> Understanding Null Values'],understand null valu
1785,"[""How missing values are handled in the data is a very important aspect of Machine learning problems. Some Data Scientists recoomend that if more than 70-75%  data in a feature/column are missing it's better to drop those features from the model. However if the entire data set has a high proportion of missing data then deleting individual features with higher proportion may work adversely in developing the model. The IEE-CIS Fraud Detection Competion data is one such case. As shown below 41%  of all the data entries in the Train_transaction file are null values.""]",miss valu handl data import aspect machin learn problem data scientist recoomend 70 75 data featur column miss better drop featur model howev entir data set high proport miss data delet individu featur higher proport may work advers develop model iee ci fraud detect compet data one case shown 41 data entri train transact file null valu
1786,"[""Let's look at which columns have highest number of null values. It's interesting to observe ,from the plot of percentage of missing values against the Number of columns ,that many columns have exactly the same number of missing values. Columns with same percent of missing values can be grouped.""]",let look column highest number null valu interest observ plot percentag miss valu number column mani column exactli number miss valu column percent miss valu group
1787,"[""Shown below are  the column_groups with exactly the same number of missing values. It's interesting to note that these  each group is almost a continous series of column names.""]",shown column group exactli number miss valu interest note group almost contin seri column name
1788,"[""Since all columns  within the  a column group have the same missing value percentage ,in a particular row of the dataframe all columns will have null values or all columns will have non null values. Let's do a value_count of data entries in all the rows for the the column group with 46 columns and missing value percentage  77.913435 to confirm this. ""]",sinc column within column group miss valu percentag particular row datafram column null valu column non null valu let valu count data entri row column group 46 column miss valu percentag 77 913435 confirm
1789,"['As expected in every row either none of the columns within a group have a  null value or all the columns within a group have null values.\n', '\n', 'Overall Fraud(isFraud=1) and non-Fraud(isFraud=0) plot is shown below.\n']",expect everi row either none column within group null valu column within group null valu overal fraud isfraud 1 non fraud isfraud 0 plot shown
1790,['What would be interesting is to check within a particular group of columns whether there is any difference in Proportion of Fraud transactions between the rows with all values missing for the columns in that group and rows with values for columns in that group compared to the overall distribution. The graph below shows that distribution'],would interest check within particular group column whether differ proport fraud transact row valu miss column group row valu column group compar overal distribut graph show distribut
1791,"['It looks like incase of some column groups there appears to be a significant difference in the proportion of fraud cases between non null value rows and the null value rows.\n', '\n']",look like inca column group appear signific differ proport fraud case non null valu row null valu row
1792,['**Conclusion**'],conclus
1793,"['* The data set has multiple groups of related features/columns that possibly can be combined to create new features.\n', '* Presence or absence of data in these related feature/column groups may be an important feature in developing models.\n']",data set multipl group relat featur column possibl combin creat new featur presenc absenc data relat featur column group may import featur develop model
1794,['> Feature Engineering'],featur engin
1795,['Let us create some new features based on the group of columns with  same percent of null values\n'],let u creat new featur base group column percent null valu
1796,['Among the C Columns some of the columns are Pseudo categorical in nature . Refer kernel https://www.kaggle.com/rajeshcv/exploring-c-columns  for details. A new feature is developed based on this. Based on transaction date transaction hour and weekday features are also created.'],among c column column pseudo categor natur refer kernel http www kaggl com rajeshcv explor c column detail new featur develop base base transact date transact hour weekday featur also creat
1797,['> Label Encoding'],label encod
1798,"[""Let's encode the object columns to replace string values with numericals. Here the labe encoding id done without changing null values.\n"", '\n', 'A check is done to find if the values in the object type columns of train and test dataset are the same']",let encod object column replac string valu numer labe encod id done without chang null valu check done find valu object type column train test dataset
1799,"[""P_emaildomain has a value 'scranton.edu' in test dataset which is not in the train dataset. We will replace this with null value""]",p emaildomain valu scranton edu test dataset train dataset replac null valu
1800,['> Model Building'],model build
1801,['> Feature importance'],featur import
1802,['Feature importance by top 100 features is shown below'],featur import top 100 featur shown
1803,['> Results'],result
1804,"['Of the 394 columns in train_transaction file 339 columns start with V . In most of the models developed  Vcolumns individually have very low importance and gets removed during feature selection process.The kernel attempts to understand the similiarilties between the various V columns and the distribution of data in these columns inorder to understand various groupings that can be used to create useful features.\n', '\n', 'The first grouping is based on the percentage of missing values in the columns . The columns can be divided into 15 groups as below.']",394 column train transact file 339 column start v model develop vcolumn individu low import get remov featur select process kernel attempt understand similiarilti variou v column distribut data column inord understand variou group use creat use featur first group base percentag miss valu column column divid 15 group
1805,"[""Let's check whether similiar pattern exists in the test_transaction data.\n"", 'The same groups exist in test data also !!!.']",let check whether similiar pattern exist test transact data group exist test data also
1806,"[""Let's explore the 15 groups of columns to understand how many unique values are there in each of the columns and how many values make 96.5% of the data in each of the columns. (96.5% is chosen as this is the percentage of transactions that is not Fraud.)""]",let explor 15 group column understand mani uniqu valu column mani valu make 96 5 data column 96 5 chosen percentag transact fraud
1807,"['Based on the  data distribution columns can be divided into 5 types.\n', '\n', '1. **Boolean** - columns  with only two unique values\n', '\n', '2. **Pseudo- Boolean**  - columns with  96.5% data covered by  maximum two unique values. Within this there are two types.\n', '        \n', '        Pseudo-Boolean-categorical - Columns with 15 or less unique values but 96.5% data covered by  maximum two unique values\n', '        Pseudo-Boolean-numerical - Columns with more than 15 unique values but 96.5% data covered by  maximum two unique values\n', '\n', '4. **Pseudo-Categorical**  - Columns with  96.5% data covered by  15 or less unique values\n', '\n', '5. **Numerical** - All Other columns\n', '\n']",base data distribut column divid 5 type 1 boolean column two uniqu valu 2 pseudo boolean column 96 5 data cover maximum two uniqu valu within two type pseudo boolean categor column 15 le uniqu valu 96 5 data cover maximum two uniqu valu pseudo boolean numer column 15 uniqu valu 96 5 data cover maximum two uniqu valu 4 pseudo categor column 96 5 data cover 15 le uniqu valu 5 numer column
1808,"['**Boolean Columns**\n', '\n']",boolean column
1809,['In all these columns almost all values is 1. However except for V305 all values even though minimal which are not 1 are from fraud transactions.'],column almost valu 1 howev except v305 valu even though minim 1 fraud transact
1810,['**Pseudo Booleans**'],pseudo boolean
1811,['221 Columns starting with V have 96.5% of their values covered by one or two values\n'],221 column start v 96 5 valu cover one two valu
1812,"[""Of these 108 columns have only less than 15 uniques values . Let's look at data distribution in these columns.These are Pseudo Boolean categorical type columns""]",108 column le 15 uniqu valu let look data distribut column pseudo boolean categor type column
1813,"[""It's interesting to note that in many of the columns some of the unique values which fall in the 3.5% of column data the proportion of fraudulent transactions is in the range of 25% -50%""]",interest note mani column uniqu valu fall 3 5 column data proport fraudul transact rang 25 50
1814,"[""The remaining 113 have more than 15 unique values. Let's look at data distribution in these columns.These are Pseudo Boolean numerical type columns""]",remain 113 15 uniqu valu let look data distribut column pseudo boolean numer type column
1815,['The histograms of values less than 3.5% of the column data shows a higher proportion of fraud transactions.'],histogram valu le 3 5 column data show higher proport fraud transact
1816,['**Pseudo - Categorical**'],pseudo categor
1817,['There are 44 Columns in this category'],44 column categori
1818,"[""Let's look at Data distribution in these columns""]",let look data distribut column
1819,['In some of these columns  a higher proportion of fraud cases are seen  for values which form less than 3.5% of the column data'],column higher proport fraud case seen valu form le 3 5 column data
1820,['**Numerical**'],numer
1821,"[""There are 67 columns in this category. Let's look at how data is distributed in these columns""]",67 column categori let look data distribut column
1822,['In all these columns the more frequent values are in the lower range in both cases.'],column frequent valu lower rang case
1823,['**Conclusion**'],conclus
1824,['It looks like the Pseudo Boolean and Pseudo Categorical columns are important as in both tpes there is a higher proportion of fraud cases when the values fall with less than 3.5% of column data unique values'],look like pseudo boolean pseudo categor column import tpe higher proport fraud case valu fall le 3 5 column data uniqu valu
1825,"['Kernel on Unique identifer based on C & D columns  https://www.kaggle.com/rajeshcv/curious-case-of-c-columns\n', '\n', 'Kernel on Null Values  https://www.kaggle.com/rajeshcv/tale-of-nulls']",kernel uniqu identif base c column http www kaggl com rajeshcv curiou case c column kernel null valu http www kaggl com rajeshcv tale null
1826,"['Of the 394 features/columns in the train_transaction data 15 columns begin in C .\n', 'The officaila explanation of these columns is.\n', '\n', '*C1-C14: counting, such as how many addresses are found to be associated with the payment card, etc. The actual meaning is masked.*\n', '\n', 'All C columns are of the numeric data type and summary is as below']",394 featur column train transact data 15 column begin c officaila explan column c1 c14 count mani address found associ payment card etc actual mean mask c column numer data type summari
1827,"['The graph below shows number of unique values in each of the C Columns as blue bars.Orange bar shows the number of  unique values in 96.5% of the data in each of the columns. The difference between the two bars is a measure of how distributed the data is across the range of unique values in the column. \n', '\n', 'Red line is the percentage of missing values in  the columns.']",graph show number uniqu valu c column blue bar orang bar show number uniqu valu 96 5 data column differ two bar measur distribut data across rang uniqu valu column red line percentag miss valu column
1828,"['**Interesting to  note that none of the C columns have missing values**.\n', '\n', 'Across the range of values in each of the column  few values make up 96.5% of data in each of the columns compared to total unique values.\n']",interest note none c column miss valu across rang valu column valu make 96 5 data column compar total uniqu valu
1829,"[""Let's also look at test_transaction data set to verify whether the distribution of values are similiar""]",let also look test transact data set verifi whether distribut valu similiar
1830,['Interestingly C13 has more than 90% missing values and C14 has 20% missing values.  These features are potentially candidates to be dropped while building models.'],interestingli c13 90 miss valu c14 20 miss valu featur potenti candid drop build model
1831,['**Unique Card Identifier**'],uniqu card identifi
1832,"['It looks a combination of Card =features card1-card 6 and C columns will help us to identify the unique payment cards (cards with unique 15 or 16 digit card numbers).\n', '\n', ""After exploring various combiantion of features a combination  of ['card1' ,'card2','card3','card4','card5','card6', 'addr1','C1','C2' ,'C3', 'C4','C5','C6','C7','C8','C9','C10','C11']  shows some interesting patterns""]",look combin card featur card1 card 6 c column help u identifi uniqu payment card card uniqu 15 16 digit card number explor variou combiant featur combin card1 card2 card3 card4 card5 card6 addr1 c1 c2 c3 c4 c5 c6 c7 c8 c9 c10 c11 show interest pattern
1833,['Taking as sample details of transactions with card1 =9885'],take sampl detail transact card1 9885
1834,"['From the above data for card  with  card1 = 9885 and number of transactions during the 6 month period = 64 it can easily be seen that D3 column is the difference in number of days for  succesive transaction values of D1 and D2.\n', '\n', 'D5 values are almost same as D3 . But where D4 is null D5 is also null which means D5 is the difference in days of successive D4 values\n', '\n', ""D1,D2,D4,D11,D15 are days from some card events as their values increase with time. D4 ,D11 and D15 appears to be the same value but D11 has some nulls. D10 values don't follow the time series and need further analysis.\n"", '\n', 'The increase in the values of D1,D2 ,D4 ,D11 and D15 corresponds with increase in days of the TransactionDT values.\n', '\n', 'The difference between the value of D1 between the first and last transaction in this group is 174 days corresponding t a 6 month period.\n', '\n', 'from the above this set of values appear to be  transactions of a specific card during the 6 month period.\n', '\n', ""**Hence it's safe to assume that the combination of the features 'card1','card2','card3','card4','card5','card6', 'addr1', 'C1','C2','C3','C4','C5','C6','C7','C8','C9','C10'and 'C11' can  uniquely identify the credit card.**""]",data card card1 9885 number transact 6 month period 64 easili seen d3 column differ number day succes transact valu d1 d2 d5 valu almost d3 d4 null d5 also null mean d5 differ day success d4 valu d1 d2 d4 d11 d15 day card event valu increas time d4 d11 d15 appear valu d11 null d10 valu follow time seri need analysi increas valu d1 d2 d4 d11 d15 correspond increas day transactiondt valu differ valu d1 first last transact group 174 day correspond 6 month period set valu appear transact specif card 6 month period henc safe assum combin featur card1 card2 card3 card4 card5 card6 addr1 c1 c2 c3 c4 c5 c6 c7 c8 c9 c10 c11 uniqu identifi credit card
1835,['There seems to be a few cases where the D3 value is noth edifference in days between succesive row. On case is where the d1 value is 236 and D2 value is 185 . D3 is 4 for this row . But the previous row D1& D2s value are 209 and 158. Hence instead of 4 the D3 value should have been 27 . So it looks like a row of values with D1=232 and D2=181 has missed out'],seem case d3 valu noth ediffer day succes row case d1 valu 236 d2 valu 185 d3 4 row previou row d1 d2 valu 209 158 henc instead 4 d3 valu 27 look like row valu d1 232 d2 181 miss
1836,"['The above row is the missing one and since it had null values for card2-card 6 it was not included in our grouping.\n', '\n', 'This throws up the possibility of imputing missing values in card2- card6 based on the feature groupings we identified as payment card identifier.\n']",row miss one sinc null valu card2 card 6 includ group throw possibl imput miss valu card2 card6 base featur group identifi payment card identifi
1837,['**Exploring D Columns further**'],explor column
1838,['The plot below shows the data distribution in D columns '],plot show data distribut column
1839,['From the above there is a fair uniform distribution of values in Dcolumns and these are truly numerical columns. histogram of data other than 0 and nulls are shown in the plot below.'],fair uniform distribut valu dcolumn truli numer column histogram data 0 null shown plot
1840,"['From the above histogram D6,D7,D8,D12,D13,D14 seems to be number of days from some card event date .In any case thes columns have close to 90% null values. D9 is the day fraction of D8. This is confirmed by sample of data below.']",histogram d6 d7 d8 d12 d13 d14 seem number day card event date case the column close 90 null valu d9 day fraction d8 confirm sampl data
1841,['**Exploring C Columns further**'],explor c column
1842,['The table below shows for each column the values that make 96.5% of data in each column(values_0.99) and values that make the remaining 1% data in (values_0.01)'],tabl show column valu make 96 5 data column valu 0 99 valu make remain 1 data valu 0 01
1843,"['###### For Columns C3,C4 ,C7 ,C8 ,C10 & C12 15 or less values make 96.5% of the column values . These are like categorical values in a sense. \n', '\n', 'The graph below shows a count plot of these categorical values which account for 96.5% values on the left and a histogram of remaining 3.5%  numeric values on the right.']",column c3 c4 c7 c8 c10 c12 15 le valu make 96 5 column valu like categor valu sen graph show count plot categor valu account 96 5 valu left histogram remain 3 5 numer valu right
1844,['**Curiously almost 30% of values that fall in the 1% of data in these columns except C3 are part of Fraud transactions**'],curious almost 30 valu fall 1 data column except c3 part fraud transact
1845,"[""###### Let's now examine the remaining columns which are numeric in nature.\n"", '\n', 'The graph below shows a histogram of  96.5% of column values on the left and a histogram of remaining 3.5%  values on the right.\n', '\n', '\n', '\n']",let examin remain column numer natur graph show histogram 96 5 column valu left histogram remain 3 5 valu right
1846,"[""There seem to be no major differnce in proportion of fraud transactions between the two types. However it's interesting to note that the majority of values in these columns are in a narrow range of 0-20  in most of the cases except C13, even though maximum value in these most of columns exceed 3000.""]",seem major differnc proport fraud transact two type howev interest note major valu column narrow rang 0 20 case except c13 even though maximum valu column exceed 3000
1847,"['* Thanks to\n', '>* https://www.kaggle.com/jazivxt/safe-box\n', '>* https://www.kaggle.com/vaishvik25/ensemble\n', '>* https://www.kaggle.com/jolasa/stacking-higher-every-time-0-9427']",thank http www kaggl com jazivxt safe box http www kaggl com vaishvik25 ensembl http www kaggl com jolasa stack higher everi time 0 9427
1848,['# Vote Early And Vote Often'],vote earli vote often
1849,['# Blending'],blend
1850,['## [UpVote if this was helpful](http://)'],upvot help http
1851,"['>* Based on https://www.kaggle.com/lpachuong/statstack\n', '>* Thanks to <br>\n', 'https://www.kaggle.com/jazivxt/safe-box<br>\n', 'https://www.kaggle.com/artgor/eda-and-models<br>\n', 'https://www.kaggle.com/stocks/under-sample-with-multiple-runs<br>\n', 'https://www.kaggle.com/artkulak/ieee-fraud-simple-baseline-0-9383-lb']",base http www kaggl com lpachuong statstack thank br http www kaggl com jazivxt safe box br http www kaggl com artgor eda model br http www kaggl com stock sampl multipl run br http www kaggl com artkulak ieee fraud simpl baselin 0 9383 lb
1852,['## Upvote if this was helpful'],upvot help
1853,['# Mean Stacking'],mean stack
1854,['# Median Stacking'],median stack
1855,"['# Pushout + Median Stacking\n', '>* Pushout strategy is bit aggresive']",pushout median stack pushout strategi bit aggres
1856,"['# MinMax + Mean Stacking\n', '>* MinMax seems more gentle and it outperforms the previous one']",minmax mean stack minmax seem gentl outperform previou one
1857,['# MinMax + Median Stacking'],minmax median stack
1858,"['# MinMax + BestBase Stacking\n', '>* loading submission with best score']",minmax bestbas stack load submiss best score
1859,['## Median stacking gives the best LB score'],median stack give best lb score
1860,"['# IEEE Fraud Detection\n', '## Catboost Baseline Model\n', '![](https://miro.medium.com/max/1200/1*2p1GIUUcRSzyyJjSj4x7Iw.jpeg)']",ieee fraud detect catboost baselin model http miro medium com max 1200 1 2p1giuucrszyyjjsj4x7iw jpeg
1861,['## Read input data'],read input data
1862,"['# Creat X, y\n', '- Create Catboost Data Pools']",creat x creat catboost data pool
1863,"['## Train Model\n', '(I will update do KFold CV Later)']",train model updat kfold cv later
1864,['# Predict'],predict
1865,"['# IEEE Fraud Detection Competition\n', '![fraud](https://abcountrywide.com.au/wp-content/uploads/2018/04/fraud.jpg)\n', '\n', 'In this kernel I do some basic exploritory data analysis on the IEEE Fraud Detection dataset. Please upvote if you find this kernel helpful. I will continue to update as I find more discoveries. I suggest you also read the complete competition overview and data description found in the competition page.\n', '\n', 'I purposefully show all of my code. The intention is to not only show the results, but also have clear code that shows how similar analysis can be done on any dataset.\n', '\n', 'From the [competition overview](https://www.kaggle.com/c/ieee-fraud-detection/overview):\n', '\n', ""*In this competition, you’ll benchmark machine learning models on a challenging large-scale dataset. The data comes from Vesta's real-world e-commerce transactions and contains a wide range of features from device type to product features. You also have the opportunity to create new features to improve your results.*\n"", '   \n', '*If successful, you’ll improve the efficacy of fraudulent transaction alerts for millions of people around the world, helping hundreds of thousands of businesses reduce their fraud loss and increase their revenue. And of course, you will save party people just like you the hassle of false positives.*']",ieee fraud detect competit fraud http abcountrywid com au wp content upload 2018 04 fraud jpg kernel basic exploritori data analysi ieee fraud detect dataset plea upvot find kernel help continu updat find discoveri suggest also read complet competit overview data descript found competit page purpos show code intent show result also clear code show similar analysi done dataset competit overview http www kaggl com c ieee fraud detect overview competit youll benchmark machin learn model challeng larg scale dataset data come vesta real world e commerc transact contain wide rang featur devic type product featur also opportun creat new featur improv result success youll improv efficaci fraudul transact alert million peopl around world help hundr thousand busi reduc fraud loss increas revenu cours save parti peopl like hassl fals posit
1866,"['# Data\n', '\n', 'In the competition you are predicting the probability that an online transaction is fraudulent, as denoted by the binary target isFraud.\n', '\n', 'The data is broken into two files identity and transaction, which are joined by TransactionID. Not all transactions have corresponding identity information.']",data competit predict probabl onlin transact fraudul denot binari target isfraud data broken two file ident transact join transactionid transact correspond ident inform
1867,"['- 24.4% of TransactionIDs in **train** (144233 / 590540) have an associated train_identity.\n', '- 28.0% of TransactionIDs in **test** (144233 / 590540) have an associated train_identity.']",24 4 transactionid train 144233 590540 associ train ident 28 0 transactionid test 144233 590540 associ train ident
1868,"['# Train vs Test are Time Series Split\n', '\n', 'The `TransactionDT` feature is a timedelta from a given reference datetime (not an actual timestamp). One early discovery about the data is that the train and test appear to be split by time. There is a slight gap inbetween, but otherwise the training set is from an earlier period of time and test is from a later period of time. This will impact which cross validation techniques should be used.\n', '\n', 'We will look into this more when reviewing differences in distribution of features between train and test.']",train v test time seri split transactiondt featur timedelta given refer datetim actual timestamp one earli discoveri data train test appear split time slight gap inbetween otherwis train set earlier period time test later period time impact cross valid techniqu use look review differ distribut featur train test
1869,"['# Distribution of Target in Training Set\n', '- 3.5% of transacations are fraud']",distribut target train set 3 5 transac fraud
1870,"['## TransactionAmt\n', ""The ammount of transaction. I've taken a log transform in some of these plots to better show the distribution- otherwise the few, very large transactions skew the distribution. Because of the log transfrom, any values between 0 and 1 will appear to be negative.""]",transactionamt ammount transact taken log transform plot better show distribut otherwis larg transact skew distribut log transfrom valu 0 1 appear neg
1871,['- Fraudulent charges appear to have a higher average transaction ammount '],fraudul charg appear higher averag transact ammount
1872,"['## ProductCD\n', ""- For now we don't know exactly what these values represent.\n"", '- `W` has the most number of observations, `C` the least.\n', '- ProductCD `C` has the most fraud with >11%\n', '- ProductCD `W` has the least with ~2%']",productcd know exactli valu repres w number observ c least productcd c fraud 11 productcd w least 2
1873,"['# Categorical Features - Transaction\n', 'We are told in the data description that the following transaction columns are categorical:\n', '- ProductCD\n', '- emaildomain\n', '- card1 - card6\n', '- addr1, addr2\n', '- P_emaildomain\n', '- R_emaildomain\n', '- M1 - M9']",categor featur transact told data descript follow transact column categor productcd emaildomain card1 card6 addr1 addr2 p emaildomain r emaildomain m1 m9
1874,"['# card1 - card6\n', '- We are told these are all categorical, even though some appear numeric.']",card1 card6 told categor even though appear numer
1875,"['# addr1 & addr2\n', 'The data description states that these are categorical even though they look numeric. Could they be the address value?']",addr1 addr2 data descript state categor even though look numer could address valu
1876,"['# dist1 & dist2\n', ""Plotting with logx to better show the distribution. Possibly this could be the distance of the transaction vs. the card owner's home/work address. This is just a guess.""]",dist1 dist2 plot logx better show distribut possibl could distanc transact v card owner home work address guess
1877,"['# C1 - C14\n', 'Because we are provided many numerical columns, we can create a pairplot to plot feature interactions. I know these plots can be hard to read, but it is helpful for gaining intution about potential feature interactions and if certain features have more variance than others.']",c1 c14 provid mani numer column creat pairplot plot featur interact know plot hard read help gain intut potenti featur interact certain featur varianc other
1878,"['# D1-D9\n', 'Similarly for features D1-D9. In these plots we can see some linear and non-linear interactions between features. We may want to create additional features using these interactions if we think it would help our model better find relationship between fraud and non-fraud observations.']",d1 d9 similarli featur d1 d9 plot see linear non linear interact featur may want creat addit featur use interact think would help model better find relationship fraud non fraud observ
1879,"['# M1-M9\n', '- Values are `T` `F` or `NaN`\n', '- Column `M4` appears to be different with values like `M2` and `M0`']",m1 m9 valu f nan column m4 appear differ valu like m2 m0
1880,"['# V1 - V339\n', 'Lots of 1s 0s and Nans, some larger values']",v1 v339 lot 1 0 nan larger valu
1881,"['# Identity Data\n', 'Next we will explore the identity data. These are provided for some, but not all `TransactionID`s. It contains information about the identity of the customer.\n', '- Categorical Features\n', '- `DeviceType`\n', '- `DeviceInfo`\n', '- `id_12` - `id_38`']",ident data next explor ident data provid transactionid contain inform ident custom categor featur devicetyp deviceinfo id 12 id 38
1882,['## DeviceType'],devicetyp
1883,['## Identity info as a function of time'],ident info function time
1884,"['## Compare Numeric Features in Train and Test\n', 'Similar to above but for the transaction data, specific examples that look interesting.']",compar numer featur train test similar transact data specif exampl look interest
1885,['# Lets track the Public LB Standings!'],let track public lb stand
1886,"['# Gold Medal Paths\n', '(Missing the team ""Young for you"" because their name must have changed from the data I have.\n', '\n', 'Updating to incluse the paths of the gold medal 🥇 teams on the private leaderboard!']",gold medal path miss team young name must chang data updat inclus path gold medal team privat leaderboard
1887,['# Top Public LB Scores over time'],top public lb score time
1888,"['# All competitors LB Position over Time\n', '(Kernel keeps breaking so I subsample to 1000 random teams)']",competitor lb posit time kernel keep break subsampl 1000 random team
1889,['# Number of Teams by Date'],number team date
1890,['# Top LB Scores'],top lb score
1891,['# Count of LB Submissions with Improved Score'],count lb submiss improv score
1892,['# Distribution of Scores over time'],distribut score time
1893,['# IEEE-CIS Fraud Detection'],ieee ci fraud detect
1894,['In this notebook we show a very simple example of PyTorch on sparse data in the competition IEEE-CIS Fraud Detection.'],notebook show simpl exampl pytorch spar data competit ieee ci fraud detect
1895,['## Libraries'],librari
1896,['Select cuda device and set a seed.'],select cuda devic set seed
1897,"['## Data\n', 'We load the data on npz compressed format (~20 MB train and test sets). For now, data is omitted.\n', '\n', 'Brief data description:\n', '\n', '- dummies on categorical data.\n', '- split in 512 bins continuous data and dummies.\n', '\n', 'Finally, we have about 13k columns.']",data load data npz compress format 20 mb train test set data omit brief data descript dummi categor data split 512 bin continu data dummi final 13k column
1898,['A function to transform data from csr_matrix format to PyTorch sparse tensor.'],function transform data csr matrix format pytorch spar tensor
1899,"['Split data in 80% train, 20% test, sorted by TransactionDT']",split data 80 train 20 test sort transactiondt
1900,['Transform valid data to sparse tensor. Fit data will be transformed into each mini-batch.'],transform valid data spar tensor fit data transform mini batch
1901,['## Neuronal Network'],neuron network
1902,"['### Train\n', '\n', 'Train the NN with Time Series 80/20 split to determine which is the optimal epoch.']",train train nn time seri 80 20 split determin optim epoch
1903,['Re-fitting with all train data and the best number of epochs.'],fit train data best number epoch
1904,"['### Test\n', '\n', 'Test set predictions in batches to avoid CUDA Memory Errors.']",test test set predict batch avoid cuda memori error
1905,['Save final submission'],save final submiss
1906,"['While I navigated the Blending Kernels available for this competition. I always fear about overfitting on Private Leaderboard.  While I am still finalizing my kernel with EDA and Model, The inputs that use in this kernel are generated from the great contributions made by other Kernel GrandMasters, Masters or Experts. None of my inputs contains blends or stack results.']",navig blend kernel avail competit alway fear overfit privat leaderboard still final kernel eda model input use kernel gener great contribut made kernel grandmast master expert none input contain blend stack result
1907,"['<pre><b>Credits to the Experts (Please like their kernels)\n', '1. Navaneetha Kernel : https://www.kaggle.com/krishonaveen/xtreme-boost-and-feature-engineering\n', '2. Shugen Kernel : https://www.kaggle.com/andrew60909/lgb-starter-r\n', '3. Khan HBK Kernel : https://www.kaggle.com/duykhanh99/hust-lgb-starter-with-r \n', '4. Konstantin Kernel : https://www.kaggle.com/kyakovlev/ieee-gb-2-make-amount-useful-again/output\n', '5. Avocado Kernel : https://www.kaggle.com/iasnobmatsu/xgb-model-with-feature-engineering?scriptVersionId=18686303\n', ""6. David's Kernel : https://www.kaggle.com/davidcairuz/feature-engineering-lightgbm-w-gpu\n"", ""7. Lyalikov's Kernel : https://www.kaggle.com/timon88/lgbm-baseline-small-fe-no-blend\n"", ""8. Yuanrong's Kernel : https://www.kaggle.com/yw6916/lgb-xgb-ensemble-stacking-based-on-fea-eng\n"", ""9. Steve's Kernel : https://www.kaggle.com/abednadir/best-r-score\n"", '</b></pre>']",pre b credit expert plea like kernel 1 navaneetha kernel http www kaggl com krishonaveen xtreme boost featur engin 2 shugen kernel http www kaggl com andrew60909 lgb starter r 3 khan hbk kernel http www kaggl com duykhanh99 hust lgb starter r 4 konstantin kernel http www kaggl com kyakovlev ieee gb 2 make amount use output 5 avocado kernel http www kaggl com iasnobmatsu xgb model featur engin scriptversionid 18686303 6 david kernel http www kaggl com davidcairuz featur engin lightgbm w gpu 7 lyalikov kernel http www kaggl com timon88 lgbm baselin small fe blend 8 yuanrong kernel http www kaggl com yw6916 lgb xgb ensembl stack base fea eng 9 steve kernel http www kaggl com abednadir best r score b pre
1908,['# Stacking Approach using GMean and Median'],stack approach use gmean median
1909,"[""<pre><b>The idea of GMean is taken from Paulo's Kernel https://www.kaggle.com/paulorzp/gmean-of-light-gbm-models-lb-0-947x</b></pre>""]",pre b idea gmean taken paulo kernel http www kaggl com paulorzp gmean light gbm model lb 0 947x b pre
1910,['# Geometric Mean Stacking'],geometr mean stack
1911,['# Median Stacking'],median stack
1912,['AggStacker.csv generates the best score (.9475)'],aggstack csv gener best score 9475
1913,['![https://cis.ieee.org/images/files/template/cis-logo.png](https://cis.ieee.org/images/files/template/cis-logo.png)'],http ci ieee org imag file templat ci logo png http ci ieee org imag file templat ci logo png
1914,"['IEEE-CIS works across a variety of AI and machine learning areas, including deep neural networks, fuzzy systems, evolutionary computation, and swarm intelligence. Today they’re partnering with the world’s leading payment service company, Vesta Corporation, seeking the best solutions for fraud prevention industry']",ieee ci work across varieti ai machin learn area includ deep neural network fuzzi system evolutionari comput swarm intellig today theyr partner world lead payment servic compani vesta corpor seek best solut fraud prevent industri
1915,['# Import Libraries'],import librari
1916,['# Visualize the Dataset'],visual dataset
1917,"[""<pre><b>Credits to Leonardo's Kernel : \n"", 'https://www.kaggle.com/kabure/extensive-eda-and-modeling-xgb-hyperopt</b></pre>']",pre b credit leonardo kernel http www kaggl com kabur extens eda model xgb hyperopt b pre
1918,['<pre><b>Ploting Transaction Amount Values Distribution</b></pre>'],pre b plote transact amount valu distribut b pre
1919,['<pre><b>The Product Feature</b></pre>'],pre b product featur b pre
1920,"['<pre><b>Visualizing Card 1, Card 2 and Card 3 Distributions</b></pre>']",pre b visual card 1 card 2 card 3 distribut b pre
1921,['<pre><b>Card 4 - Categorical Feature</b></pre>'],pre b card 4 categor featur b pre
1922,['<pre><b>Card 6 - Categorical</b></pre>'],pre b card 6 categor b pre
1923,['<pre><b>Exploring M1-M9 Features</b></pre>'],pre b explor m1 m9 featur b pre
1924,['<pre><b>Addr1 Distributions</b></pre>'],pre b addr1 distribut b pre
1925,['<pre><b> ADDR2 Distributions </b></pre>'],pre b addr2 distribut b pre
1926,['<pre><b>Ploting P-Email Domain</b></pre>'],pre b plote p email domain b pre
1927,['<pre><b>Exploring C1-C14 features</b></pre>'],pre b explor c1 c14 featur b pre
1928,['TimeDelta Feature to check if frauds have some specific hour that has highest % of frauds</b></pre>'],timedelta featur check fraud specif hour highest fraud b pre
1929,['<pre><b>Top Days with highest Total Transaction Amount</b></pre>'],pre b top day highest total transact amount b pre
1930,['<pre><b>Ploting WeekDays Distributions</b></pre>'],pre b plote weekday distribut b pre
1931,['<pre><b>Ploting Hours Distributions</b></pre>'],pre b plote hour distribut b pre
1932,['<pre><b>Transactions and Total Amount per day</b></pre>'],pre b transact total amount per day b pre
1933,['<pre><b>Fraud Transactions by Date</b></pre>'],pre b fraud transact date b pre
1934,['# Feature Engineering'],featur engin
1935,"[""<pre><b>Credits to Konstantin Yakovlev's Kernel\n"", 'https://www.kaggle.com/kyakovlev/ieee-gb-2-make-amount-useful-again</b></pre> ']",pre b credit konstantin yakovlev kernel http www kaggl com kyakovlev ieee gb 2 make amount use b pre
1936,['## Keras NN Starter Kernel'],kera nn starter kernel
1937,"[""Not looking to seriously compete in this competition so figured I'd at least share the small experiments I was toying with. This kernel serves as just a starter to look at how to handle the various numerical and categorical variables in a NN with Keras. \n"", '\n', 'Core purpose of this kernel:\n', '* How to handle categorical and numerical variables in neural networks\n', '* Methods to normalize skewed numerical variables\n', '* Greedy feature selection via exclusion\n', '* Splitting based on time']",look serious compet competit figur least share small experi toy kernel serv starter look handl variou numer categor variabl nn kera core purpos kernel handl categor numer variabl neural network method normal skew numer variabl greedi featur select via exclus split base time
1938,"['## Data Loading\n', 'Just the standard loading of the data used in most other kernels. ']",data load standard load data use kernel
1939,['Dropping time since this likely isnt something we want our model to directly learn from'],drop time sinc like isnt someth want model directli learn
1940,"['Selecting just the first set of columns and excluding the synthetic ""v"" features and other very sparse categoricals like deviceinfo and deviceid']",select first set column exclud synthet v featur spar categor like deviceinfo deviceid
1941,"['## Numerical and Categorical\n', 'Listing off and categorizing the various variables available to us. We have numerical and categoricals. We will treat both of these slightly differently later']",numer categor list categor variou variabl avail u numer categor treat slightli differ later
1942,['We already dropped a lot of these features because in some trial and error it was shown that these caused rapid overfitting for some reason or otherwise introduced unnecessary noise into the data. We will make sure we only list the features we actually have still in the df now'],alreadi drop lot featur trial error shown caus rapid overfit reason otherwis introduc unnecessari nois data make sure list featur actual still df
1943,"[""NN doesn't like nans so we will fill numerical columns with 0's. Previous people have tried plugging in the column means, but upon inspection this didn't seem very reliable because the train and test means for a given column could sometimes be drastically different. Plugging in zeros likely isnt the best. Might be better to plug in the train mean to the test df, but for simplicity I will stick with 0's for now. ""]",nn like nan fill numer column 0 previou peopl tri plug column mean upon inspect seem reliabl train test mean given column could sometim drastic differ plug zero like isnt best might better plug train mean test df simplic stick 0
1944,"['## Label Encoding\n', ""We will take our categorical features fill the nans and assign them an integer ID per category and write down the number of total categories per column. We'll use this later in an embedding layer of the NN""]",label encod take categor featur fill nan assign integ id per categori write number total categori per column use later embed layer nn
1945,"['## Numerical Scaling\n', '\n', 'Now we will do some scaling of the data so that it will be in a more NN friendly format. First we will do log1p for any values that are above 100 and not below 0. This is in order to scale down any numerical variables that might have some extremely high values that screws up the statistics of the standard scaler \n', '\n', 'After that we will pass them through the standard scaler so that the values have a normal mean and std. This makes the NN converge signficantly faster and prevents any blowouts. Feel free to try for yourself by commenting out this cell of code']",numer scale scale data nn friendli format first log1p valu 100 0 order scale numer variabl might extrem high valu screw statist standard scaler pas standard scaler valu normal mean std make nn converg signficantli faster prevent blowout feel free tri comment cell code
1946,['Grabbing the features we want to pass into the neural network'],grab featur want pas neural network
1947,"['## Neural Network Model Details\n', '\n', 'Our neural network will be fairly standard. We will use the embedding layer for categoricals and the numericals will go through feed forward dense layers. \n', '\n', 'We create our embedding layers such that we have as many rows as we had categories and the dimension of the embedding is the log1p + 1 of the number of categories. So this means that categorical variables with very high cardinality will have more dimensions but not signficantly more so the information will still be compressed down to only about 13 dimensions and the smaller number of categories will be only 2-3.\n', '\n', 'We will then pass the embeddings through a spatial dropout layer which will drop dimensions within the embedding across batches and then flatten and concatenate. Then we will concatenate this to the numerical features and apply batch norm and then add some more dense layers after. ']",neural network model detail neural network fairli standard use embed layer categor numer go feed forward den layer creat embed layer mani row categori dimens embed log1p 1 number categori mean categor variabl high cardin dimens signficantli inform still compress 13 dimens smaller number categori 2 3 pas embed spatial dropout layer drop dimens within embed across batch flatten concaten concaten numer featur appli batch norm add den layer
1948,['We will then extract the features we actually want to pass to the NN'],extract featur actual want pas nn
1949,['We will iterate through epochs of the model and save the model weights if the score is an improvement upon previous best roc_auc_scores since this is competition metric. If the NN does not improve upon previous best after 4 epochs we will skip the rest of the training steps to save time. '],iter epoch model save model weight score improv upon previou best roc auc score sinc competit metric nn improv upon previou best 4 epoch skip rest train step save time
1950,"['## Greedy Feature Selection\n', '\n', 'First we will train the NN with all categorical and numerical features in order to make a baseline\n', '\n', 'After that we will greedily drop one feature at a time and see if it increases or decreases performance. If it increases upon dropping the feature then we will drop the feature. If it decreases then it will stay. ']",greedi featur select first train nn categor numer featur order make baselin greedili drop one featur time see increas decreas perform increas upon drop featur drop featur decreas stay
1951,['Dropping categoricals'],drop categor
1952,['Dropping numeric'],drop numer
1953,"['## Sanity Checks\n', 'Now we will make sure we loaded in the correction  model that scored favorably and then we will compare prediction statistics between validation and test. These should be relatively close or else we know something might have been off between how we prepared the train and test data for inference']",saniti check make sure load correct model score favor compar predict statist valid test rel close el know someth might prepar train test data infer
1954,"[""Let's fine-tune on the validation data now since that is the most recent and will likely help generalization to the test set""]",let fine tune valid data sinc recent like help gener test set
1955,"[""Trying to find the minimum raw features to get a decent score.  I couldn't be bothered using Time Stuff which definitely improve things ;)""]",tri find minimum raw featur get decent score bother use time stuff definit improv thing
1956,['![](https://cdn-images-1.medium.com/max/853/1*DgFPLm5TKXuKnNUlYCE2DQ.jpeg)'],http cdn imag 1 medium com max 853 1 dgfplm5tkxuknnulyce2dq jpeg
1957,"[""<div align='left'><font size='5' color='#5b2c6f '> Purpose of this notebook</font></div>""]",div align left font size 5 color 5b2c6f purpos notebook font div
1958,"['- In this notebook we will discuss about class imbalance problem which is occus often more in problems like fraudulent transaction identification and\n', '  spam  identification .\n', '- Discuss and implement methods to solve this issue to an extend.\n', '- [Loading Libraries](#1)\n', '- [Loading Data ](#2)\n', '- [The metric trap](#3)\n', '- [Data preparating](#4)\n', '- [Resampling](#5)\n', '- [Resampling using sklearn](#6)\n', '- [Dimensionality Reduction and Clustering](#7)\n', '- [Python imbalanced-learn module](#8)\n', '- [Algorithmic Ensemble Techniques](#9)']",notebook discus class imbal problem occu often problem like fraudul transact identif spam identif discus implement method solv issu extend load librari 1 load data 2 metric trap 3 data prepar 4 resampl 5 resampl use sklearn 6 dimension reduct cluster 7 python imbalanc learn modul 8 algorithm ensembl techniqu 9
1959,"['### [Loading Required libraries](#1)<a id=""1""></a> <br>']",load requir librari 1 id 1 br
1960,"['## [Loading Data](#2)<a id=""2""></a> <br>']",load data 2 id 2 br
1961,"[""<div align='left'><font size='4' color='#229954'>Getting basic Idea</font></div>""]",div align left font size 4 color 229954 get basic idea font div
1962,"[""<div align='left'><font size='4' color='#229954'>Target variable</font></div>\n""]",div align left font size 4 color 229954 target variabl font div
1963,"['- There is clearly a class imbalace problem.\n', '- We will look into methods of solving this issue later in this notebook.']",clearli class imbalac problem look method solv issu later notebook
1965,"['## [The metric trap](#3)<a id=""3""></a> <br>\n', '\n', 'One of the major issues that novice users fall into when dealing with unbalanced datasets relates to the metrics used to evaluate their model. Using simpler metrics like accuracy_score can be misleading. In a dataset with highly unbalanced classes, if the classifier always ""predicts"" the most common class without performing any analysis of the features, it will still have a high accuracy rate, obviously illusory.\n', '\n', '\n', '    False Positive. Predict an event when there was no event.\n', '    False Negative. Predict no event when in fact there was an event.\n', '\n', '   In the overview of the problem statement the organizers has described a situation where you stand at the queue for a long time and when your chance arrives,the transaction gets denied because it was interpreted as a Fraudulent transaction which many of us have faced.\n', ' This is classical example of **False Negative** prediction.\n', ' \n', '\n', '\n', '**Change the performance metric**\n', '\n', 'As we saw above, accuracy is not the best metric to use when evaluating imbalanced datasets as it can be very misleading. Metrics that can provide better insight include:\n', '\n', '**Confusion Matrix**: a table showing correct predictions and types of incorrect predictions.\n', '    \n', '**Precision**: the number of true positives divided by all positive predictions. Precision is also called Positive Predictive Value. It is a measure of a classifier’s exactness. Low precision indicates a high number of false positives.\n', '    \n', '**Recall**: the number of true positives divided by the number of positive values in the test data. Recall is also called Sensitivity or the True Positive Rate. It is a measure of a classifier’s completeness. Low recall indicates a high number of false negatives.\n', '    \n', '**F1 Score**: the weighted average of precision and recall.\n', '    ']",metric trap 3 id 3 br one major issu novic user fall deal unbalanc dataset relat metric use evalu model use simpler metric like accuraci score mislead dataset highli unbalanc class classifi alway predict common class without perform analysi featur still high accuraci rate obvious illusori fals posit predict event event fals neg predict event fact event overview problem statement organ describ situat stand queue long time chanc arriv transact get deni interpret fraudul transact mani u face classic exampl fals neg predict chang perform metric saw accuraci best metric use evalu imbalanc dataset mislead metric provid better insight includ confus matrix tabl show correct predict type incorrect predict precis number true posit divid posit predict precis also call posit predict valu measur classifi exact low precis indic high number fals posit recal number true posit divid number posit valu test data recal also call sensit true posit rate measur classifi complet low recal indic high number fals neg f1 score weight averag precis recal
1966,"[""I don't understand why the competition hosts selected ROC_AUC as evaluation metric,I think\n"", '- ROC curves should be used when there are roughly equal numbers of observations for each class.\n', '-  Precision-Recall curves should be used when there is a moderate to large class imbalance.\n']",understand competit host select roc auc evalu metric think roc curv use roughli equal number observ class precis recal curv use moder larg class imbal
1967,"['## [Merging transaction and identity dataset](#4)<a id=""4""></a> <br>\n', '\n', 'We will firt merge our **transactions** and **identity** datasets.']",merg transact ident dataset 4 id 4 br firt merg transact ident dataset
1968,"[""<div align='left'><font size='4' color='#229954'>Reducing memory usage</font></div>\n"", '\n']",div align left font size 4 color 229954 reduc memori usag font div
1969,"[""<div align='left'><font size='4' color='#229954'>Splitting to train and validation</font></div>\n""]",div align left font size 4 color 229954 split train valid font div
1970,"['- We will now split the train dataset into train and validation set.\n', '- We will keeep 20% of data for validation.']",split train dataset train valid set keeep 20 data valid
1971,"['\n', '## [Resampling](#5)<a id=""5""></a> <br>\n', '\n', 'A widely adopted technique for dealing with highly unbalanced datasets is called resampling. It consists of removing samples from the majority class (under-sampling) and / or adding more examples from the minority class (over-sampling).\n', '\n', '![](https://raw.githubusercontent.com/rafjaa/machine_learning_fecib/master/src/static/img/resampling.png)']",resampl 5 id 5 br wide adopt techniqu deal highli unbalanc dataset call resampl consist remov sampl major class sampl ad exampl minor class sampl http raw githubusercont com rafjaa machin learn fecib master src static img resampl png
1972,"['## [Resampling Techniques using sklearn](#6)<a id=""6""></a> <br>']",resampl techniqu use sklearn 6 id 6 br
1973,"[""<div align='left'><font size='4' color=' #6c3483'> 1.Oversample minority class </font></div>\n""]",div align left font size 4 color 6c3483 1 oversampl minor class font div
1974,"['Oversampling can be defined as adding more copies of the minority class. Oversampling can be a good choice when you don’t have a ton of data to work with.\n', '\n', 'We will use the resampling module from Scikit-Learn to randomly replicate samples from the minority class.']",oversampl defin ad copi minor class oversampl good choic dont ton data work use resampl modul scikit learn randomli replic sampl minor class
1975,"[""<div align='left'><font size='4' color=' #6c3483'>  2. Undersample majority class </font></div>\n""]",div align left font size 4 color 6c3483 2 undersampl major class font div
1976,"['Undersampling can be defined as removing some observations of the majority class. Undersampling can be a good choice when you have a ton of data -think millions of rows. But a drawback is that we are removing information that may be valuable. This could lead to underfitting and poor generalization to the test set.\n', '\n', 'We will again use the resampling module from Scikit-Learn to randomly remove samples from the majority class.']",undersampl defin remov observ major class undersampl good choic ton data think million row drawback remov inform may valuabl could lead underfit poor gener test set use resampl modul scikit learn randomli remov sampl major class
1977,['We will review other resampling techniques.'],review resampl techniqu
1978,"['\n', '\n', ""For ease of visualization, let's create a small unbalanced sample dataset using the make_classification method:\n""]",ea visual let creat small unbalanc sampl dataset use make classif method
1979,"['- We will do an experiment with this data without any resampling technique.\n', '- We will fit and predict the data on a Logistic regression model and observe the output scores.']",experi data without resampl techniqu fit predict data logist regress model observ output score
1980,['- We will define two functions to plot precision_recall curve and roc curve'],defin two function plot precis recal curv roc curv
1981,"['\n', '\n', 'We will also create a 2-dimensional plot function, plot_2d_space, to see the data distribution:\n']",also creat 2 dimension plot function plot 2d space see data distribut
1982,"['\n', '\n', '\n', '\n', '## [Dimensionality Reduction and Clustering](#7)<a id=""7""></a> <br>\n', '\n', 'Understanding t-SNE:\n', 'In order to understand this algorithm you have to understand the following terms:\n', '\n', '    Euclidean Distance\n', '    Conditional Probability\n', '    Normal and T-Distribution Plots\n', '\n']",dimension reduct cluster 7 id 7 br understand sne order understand algorithm understand follow term euclidean distanc condit probabl normal distribut plot
1983,"['In the below section we will implement three major dimensionality reduction algorithms\n', '- **T-sne**\n', '- **PCA**\n', '- **Truncated SVD**']",section implement three major dimension reduct algorithm sne pca truncat svd
1984,['Now we will visualize the output of the above three algorithms in a 2D space.'],visual output three algorithm 2d space
1985,"['\n', ""## [Python imbalanced-learn module](#8)<a id='8'></a></br>\n"", '\n', 'A number of more sophisticated resapling techniques have been proposed in the scientific literature.\n', '\n', 'For example, we can cluster the records of the majority class, and do the under-sampling by removing records from each cluster, thus seeking to preserve information. In over-sampling, instead of creating exact copies of the minority class records, we can introduce small variations into those copies, creating more diverse synthetic samples.\n', '\n', ""Let's apply some of these resampling techniques, using the Python library imbalanced-learn. It is compatible with scikit-learn and is part of scikit-learn-contrib projects.\n""]",python imbalanc learn modul 8 id 8 br number sophist resapl techniqu propos scientif literatur exampl cluster record major class sampl remov record cluster thu seek preserv inform sampl instead creat exact copi minor class record introduc small variat copi creat diver synthet sampl let appli resampl techniqu use python librari imbalanc learn compat scikit learn part scikit learn contrib project
1986,"[""<div align='left'><font size='4' color=' #6c3483'>  Random under-sampling  with imbalanced-learn </font></div>\n"", '\n']",div align left font size 4 color 6c3483 random sampl imbalanc learn font div
1987,"[""- Let's try fit and predict on this data and observe the outcome.""]",let tri fit predict data observ outcom
1988,"[""<div align='left'><font size='4' color=' #6c3483'>  Random over-sampling  with imbalanced-learn </font></div>\n"", '\n']",div align left font size 4 color 6c3483 random sampl imbalanc learn font div
1989,"[""Let's try fit and predict on this data and observe the outcome.""]",let tri fit predict data observ outcom
1990,"['\n', '## [Under-sampling: Tomek links](#9)\n', '\n', 'Tomek links are pairs of very close instances, but of opposite classes. Removing the instances of the majority class of each pair increases the space between the two classes, facilitating the classification process.\n', '\n', '![](https://raw.githubusercontent.com/rafjaa/machine_learning_fecib/master/src/static/img/tomek.png?v=2)']",sampl tomek link 9 tomek link pair close instanc opposit class remov instanc major class pair increas space two class facilit classif process http raw githubusercont com rafjaa machin learn fecib master src static img tomek png v 2
1991,"[""<div align='left'><font size='4' color=' #6c3483'>  Over-sampling: SMOTE </font></div>\n"", '\n']",div align left font size 4 color 6c3483 sampl smote font div
1992,"['\n', '\n', '\n', '\n', 'SMOTE (Synthetic Minority Oversampling TEchnique) consists of synthesizing elements for the minority class, based on those that already exist. It works randomly picingk a point from the minority class and computing the k-nearest neighbors for this point. The synthetic points are added between the chosen point and its neighbors.\n', '\n', '![](https://raw.githubusercontent.com/rafjaa/machine_learning_fecib/master/src/static/img/smote.png)\n']",smote synthet minor oversampl techniqu consist synthes element minor class base alreadi exist work randomli picingk point minor class comput k nearest neighbor point synthet point ad chosen point neighbor http raw githubusercont com rafjaa machin learn fecib master src static img smote png
1993,"[""Let's try fit and predict on this data and observe the outcome.""]",let tri fit predict data observ outcom
1994,"['## [Algorithmic Ensemble Techniques](#9)<a id=""1""></a> <br>\n', 'The above section, deals with handling imbalanced data by resampling original data to provide balanced classes. In this section, we are going to look at an alternate approach i.e.  Modifying existing classification algorithms to make them appropriate for imbalanced data sets.\n', '\n', 'The main objective of ensemble methodology is to improve the performance of single classifiers. The approach involves constructing several two stage classifiers from the original data and then aggregate their prediction\n', '\n', '![ Approach to Ensemble based Methodologies](https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/03/16142904/ICP4.png)']",algorithm ensembl techniqu 9 id 1 br section deal handl imbalanc data resampl origin data provid balanc class section go look altern approach e modifi exist classif algorithm make appropri imbalanc data set main object ensembl methodolog improv perform singl classifi approach involv construct sever two stage classifi origin data aggreg predict approach ensembl base methodolog http s3 ap south 1 amazonaw com av blog medium wp content upload 2017 03 16142904 icp4 png
1995,"[""<div align='left'><font size='4' color=' #6c3483'>  XGBoost </font></div>\n"", '\n', '\n', '\n', '![](https://miro.medium.com/max/1400/1*FLshv-wVDfu-i54OqvZdHg.png)\n', '\n', 'XGBoost (Extreme Gradient Boosting) is an advanced and more efficient implementation of Gradient Boosting Algorithm discussed in the previous section.\n', '\n', 'Advantages over Other Boosting Techniques\n', '\n', 'It is 10 times faster than the normal Gradient Boosting as it implements parallel processing. It is highly flexible as users can define custom optimization objectives and evaluation criteria, has an inbuilt mechanism to handle missing values.\n', 'Unlike gradient boosting which stops splitting a node as soon as it encounters a negative loss, XG Boost splits up to the maximum depth specified and prunes the tree backward and removes splits beyond which there is an only negative loss.\n', '\n', 'Extreme gradient boosting can be done using the XGBoost package in R and Python']",div align left font size 4 color 6c3483 xgboost font div http miro medium com max 1400 1 flshv wvdfu i54oqvzdhg png xgboost extrem gradient boost advanc effici implement gradient boost algorithm discus previou section advantag boost techniqu 10 time faster normal gradient boost implement parallel process highli flexibl user defin custom optim object evalu criterion inbuilt mechan handl miss valu unlik gradient boost stop split node soon encount neg loss xg boost split maximum depth specifi prune tree backward remov split beyond neg loss extrem gradient boost done use xgboost packag r python
1996,"['### WORK IN PROGRESS\n', ""<div align='left'><font size='5' color=' #a93226 '>  If you like my work,please do upvote ^ </font></div>\n"", '\n']",work progress div align left font size 5 color a93226 like work plea upvot font div
1997,"[""<div align='left'><font size='4' color=' #6c3483'> References </font></div>\n"", '\n', '\n', '- [Dealing with Imbalanced Data](https://towardsdatascience.com/methods-for-dealing-with-imbalanced-data-5b761be45a18)\n', '- [Resampling strategies for imbalanced datasets](https://www.kaggle.com/rafjaa/resampling-strategies-for-imbalanced-datasets)']",div align left font size 4 color 6c3483 refer font div deal imbalanc data http towardsdatasci com method deal imbalanc data 5b761be45a18 resampl strategi imbalanc dataset http www kaggl com rafjaa resampl strategi imbalanc dataset
1998,"['### XGBoost with Feature Interactions \n', 'Old but gold: [xgbfir](https://github.com/limexp/xgbfir)']",xgboost featur interact old gold xgbfir http github com limexp xgbfir
1999,"['#### Dropping columns and mapping emails. Thanks for the code the1owl!\n', 'https://www.kaggle.com/jazivxt/safe-box']",drop column map email thank code the1owl http www kaggl com jazivxt safe box
2000,"['### XGBoost feature importance types:\n', '* ""weight"" is the number of times a feature appears in a tree\n', '* ""gain"" is the average gain of splits which use the feature\n', '* ""cover"" is the average coverage of splits which use the feature where coverage is defined as the number of samples affected by the split']",xgboost featur import type weight number time featur appear tree gain averag gain split use featur cover averag coverag split use featur coverag defin number sampl affect split
2001,"[""#### Let's check the 4th fold feature interactions""]",let check 4th fold featur interact
2002,['##### Feature Interaction: https://christophm.github.io/interpretable-ml-book/interaction.html'],featur interact http christophm github io interpret ml book interact html
2003,['Based on these interactions you can create some new features...'],base interact creat new featur
2004,"[""[Fork of 'Stacking?'](https://www.kaggle.com/rajwardhanshinde/stacking)\n"", '\n', '**Update for v.9:** again a slightly higher scoring Under_Over submission is added (nothing dramatic: 0.9398). You may have to remove and reattach ieeesubmissions5 dataset to get the latest version (`xgboost_under_over_blend_9398`)\n', '\n', ""**Update for v.8:** a tiny update with a fresh file from [Undersample with multiple runs kernel](https://www.kaggle.com/stocks/under-sample-with-multiple-runs). don't know the score yet\n"", '\n', '**Update for v.7**:\n', '\n', 'Added [IEEE - LGB + Bayesian opt.](https://www.kaggle.com/vincentlugat/ieee-lgb-bayesian-opt)\n', '\n', '**Update for v.5**:\n', '\n', ' * <font color=green>Median 0.9425</font>\n', ' * <font color=green>Mean 0.9420 </font>\n', ' * <font color=green>Median Rank 0.9413</font>\n', ' * <font color=green>Mean Rank 0.9413 </font>\n', '\n', '* `ieeesubmissions2` dataset was update replacing `safebox9416.csv` with `safebox9367.csv`. `safebox9416.csv` had an incorrect score linked to it (it is actually 0.9322). The reason for the mix-up is that it is unknown which submission generates which score in public kernels. The reason I am no longer using `blend1.csv` from Safebox kernel is because `blend1` itself was a blend with `ieee_9383` also used in this kernel.\n', '\n', '* added Median rank\n', '\n', '**Update for v.4**:\n', '\n', 'v.4 is the same as v.3, only reporting select scores for v.3\n', '\n', '**Update for v.3**:\n', '\n', '**<font color=green>Submissions tried in v.3:</font>**\n', '\n', ' * <font color=green>Median 0.9427</font>\n', ' * <font color=green>Mean 0.9420 </font>\n', '\n', 'I added a new dataset with submission files labelled by their scores (to the best of my knowledge). They all come from the same kernels as in v.2 but two are different  versions of those. Not sure if any them might improve the score.\n', '\n', '**Updates made by me in v.1 and v.2:**\n', '\n', '* use the higher scoring blend of oversample + undersample from [My kernel](https://www.kaggle.com/stocks/under-sample-with-multiple-runs). \n', ""I'm guessing that the stacking kernel that I've copied here used only my under-sampled model.\n"", 'At least one of the other models [EDA Kernel](https://www.kaggle.com/artgor/eda-and-models) is scoring higher.\n', 'Thus, the explanation for high score of this clone likely lies in higher scoring models used for stacking.\n', '* trying stacking based on ranks\n', '\n', '**<font color=green>Submission tried in v.2:</font>**\n', '\n', ' * <font color=green>Median 0.9429</font>\n', ' * <font color=green>Mean rank 0.9415 </font>\n', '\n', '#### Info from the parent of this clone:\n', '>* Based on https://www.kaggle.com/lpachuong/statstack\n', '>* Thanks to <br>\n', 'https://www.kaggle.com/jazivxt/safe-box<br>\n', 'https://www.kaggle.com/artgor/eda-and-models<br>\n', 'https://www.kaggle.com/stocks/under-sample-with-multiple-runs<br>\n', 'https://www.kaggle.com/artkulak/ieee-fraud-simple-baseline-0-9383-lb']",fork stack http www kaggl com rajwardhanshind stack updat v 9 slightli higher score submiss ad noth dramat 0 9398 may remov reattach ieeesubmissions5 dataset get latest version xgboost blend 9398 updat v 8 tini updat fresh file undersampl multipl run kernel http www kaggl com stock sampl multipl run know score yet updat v 7 ad ieee lgb bayesian opt http www kaggl com vincentlugat ieee lgb bayesian opt updat v 5 font color green median 0 9425 font font color green mean 0 9420 font font color green median rank 0 9413 font font color green mean rank 0 9413 font ieeesubmissions2 dataset updat replac safebox9416 csv safebox9367 csv safebox9416 csv incorrect score link actual 0 9322 reason mix unknown submiss gener score public kernel reason longer use blend1 csv safebox kernel blend1 blend ieee 9383 also use kernel ad median rank updat v 4 v 4 v 3 report select score v 3 updat v 3 font color green submiss tri v 3 font font color green median 0 9427 font font color green mean 0 9420 font ad new dataset submiss file label score best knowledg come kernel v 2 two differ version sure might improv score updat made v 1 v 2 use higher score blend oversampl undersampl kernel http www kaggl com stock sampl multipl run guess stack kernel copi use sampl model least one model eda kernel http www kaggl com artgor eda model score higher thu explan high score clone like lie higher score model use stack tri stack base rank font color green submiss tri v 2 font font color green median 0 9429 font font color green mean rank 0 9415 font info parent clone base http www kaggl com lpachuong statstack thank br http www kaggl com jazivxt safe box br http www kaggl com artgor eda model br http www kaggl com stock sampl multipl run br http www kaggl com artkulak ieee fraud simpl baselin 0 9383 lb
2005,['## <font color=blue>Vote early and vote often!</font>\n'],font color blue vote earli vote often font
2006,['# Mean Stacking'],mean stack
2007,['# Median Stacking'],median stack
2008,"['# Pushout + Median Stacking\n', '>* Pushout strategy is bit aggresive']",pushout median stack pushout strategi bit aggres
2009,"['# MinMax + Mean Stacking\n', '>* MinMax seems more gentle and it outperforms the previous one']",minmax mean stack minmax seem gentl outperform previou one
2010,['# MinMax + Median Stacking'],minmax median stack
2011,['# Median Rank'],median rank
2012,['# Mean Rank'],mean rank
2013,['**Note : The best result of this kernel is at V16.**'],note best result kernel v16
2014,['**Merging the transactions and indentity data**'],merg transact indent data
2015,"[""**Let's reduce the memory of the dataframe**""]",let reduc memori datafram
2016,"['## Objective:\n', '\n', '**Predict fraud based on transaction information**\n']",object predict fraud base transact inform
2017,"['## Joining the transaction data:\n', '\n', '### Transaction Table:\n', '\n', 'Transaction table contain TransactionID, which we can use to join with the identity table.']",join transact data transact tabl transact tabl contain transactionid use join ident tabl
2018,"['The transaction ID for transaction table is also unique for each observation. Therefore, we have 1 to 1 join from identity table to transaction table:']",transact id transact tabl also uniqu observ therefor 1 1 join ident tabl transact tabl
2019,"['However, it comes to my attention that the number of rows are different for each table, despite having unique TransactionID:']",howev come attent number row differ tabl despit uniqu transactionid
2020,"[""This suggests that there are transactions that don't have identity. A quick research on the [discussion thread](https://www.kaggle.com/c/ieee-fraud-detection/discussion/101203#latest-605862) reveals that Vesta was unable to collect all identity information due to technical difficulty. Therefore, we will need to face two options:\n"", '\n', '1. Using identity + transaction to make predictions. This option results in fewer observations but more complete (more features).\n', '\n', '2. Using only transaction\n', '\n', '3. Using transaction but add identity when avaiable\n', '\n', 'For now, we only explore the identity + transaction joined table to do EDA and build model. ']",suggest transact ident quick research discus thread http www kaggl com c ieee fraud detect discus 101203 latest 605862 reveal vesta unabl collect ident inform due technic difficulti therefor need face two option 1 use ident transact make predict option result fewer observ complet featur 2 use transact 3 use transact add ident avaiabl explor ident transact join tabl eda build model
2021,"['# Data Quality Inspection\n', '\n', 'There are couple common issues that we need to watch out for:\n', '\n', '1. Attributes Formatting (data types)\n', '\n', '2. Missing Data\n', '\n', '3. Replacement or Drop\n', '\n', '4. Response Variable\n', '\n', ""First, let's transform our data into the types that we expected:\n"", '\n', '## 1. Attributes formatting (data types)']",data qualiti inspect coupl common issu need watch 1 attribut format data type 2 miss data 3 replac drop 4 respons variabl first let transform data type expect 1 attribut format data type
2022,"['Most of the column names have been masked for privacy protection. Without accurate description of the fields meaning, it would be difficult to determine the type of data. Fortunately, Vesta have provided us with high-level summary of data.\n', '\n', ""Let's recall the avaiable groups of information that were provided for us:\n"", '\n', '1. Identity Table:\n', '\n', '    * id_01 - id_38: contains network connection information\n', '    \n', '    * DeviceType and DeviceInfo\n', '    \n', '2. Transactional Table:\n', '\n', '    * card1 - card6: card information\n', '    \n', '    * addr: address\n', '    \n', '    * dist: distance\n', '\n', '    * P_ and (R__) emaildomain: purchaser and recipient email domain\n', '    \n', '    * C1-C14: counting\n', '    \n', '    * D1-D15: timedelta, such as days between previous transaction, etc.\n', '    \n', '    * M1-M9: match, such as names on card and address, etc.\n', '    \n', '    * V1-V339: Vesta engineered features\n', '    \n', '    * ProductCD: product code, the product for each transaction\n', '    \n', '    * TransactionDT: timedelta from a given reference datetime\n', '    \n', '    * TransactionAMT: transaction payment amount in USD    \n', '    \n', '## Missing Data\n', '    \n', ""Let's take a look at the missing data for the **categorical variables** first:""]",column name mask privaci protect without accur descript field mean would difficult determin type data fortun vesta provid u high level summari data let recal avaiabl group inform provid u 1 ident tabl id 01 id 38 contain network connect inform devicetyp deviceinfo 2 transact tabl card1 card6 card inform addr address dist distanc p r emaildomain purchas recipi email domain c1 c14 count d1 d15 timedelta day previou transact etc m1 m9 match name card address etc v1 v339 vesta engin featur productcd product code product transact transactiondt timedelta given refer datetim transactionamt transact payment amount usd miss data let take look miss data categor variabl first
2023,"['**Observation**: We can see that our data has a lot of missing values. White color presents missing values.\n', '\n', '1. Most M columns missing almost if not all data\n', '\n', '2. Id_07, 08 and id_21-27 missing most data\n', '\n', ""3. Id_01, id_12, card1, card2 contains mostly non-null. Perhaps, these columns contain unique ID information, and therefore, cannot be null. Let's double check the number of missing in these columns: ""]",observ see data lot miss valu white color present miss valu 1 column miss almost data 2 id 07 08 id 21 27 miss data 3 id 01 id 12 card1 card2 contain mostli non null perhap column contain uniqu id inform therefor null let doubl check number miss column
2024,"['Yes, they are indeed complete, except for card 2. If I were to guess, card1 could be first name and card2 could be last name.\n', '\n', ""Now let's check out missing data for **numerical variables**:""]",ye inde complet except card 2 guess card1 could first name card2 could last name let check miss data numer variabl
2025,"['**Observation**: \n', '    \n', '    1. Basic information about transaction such as ID, DT, amount and type of product is complete \n', '    \n', '    2. Dist1 and dist2 is very sparse.\n', '    \n', '    3. C columns are complete\n', '    \n', '    4. Most D columns are sparse except D1\n', '    \n', ""Lastly, we want to check for data completeness of **Vesta's engineered features**:""]",observ 1 basic inform transact id dt amount type product complet 2 dist1 dist2 spar 3 c column complet 4 column spar except d1 lastli want check data complet vesta engin featur
2026,"['**Observation**: Ahh, she looks like a work of art. The repeated missing patterns in the V columns suggest that many V columns are related and perhaps trying to describe certain characteristics of a transaction. For example, columns V322-V399 have identical missing locations.\n', '\n', ""Let's verify our intuition with correlation heatmap measures nullity correlation: how strongly the presence or absence of one variable affects the presence of another:""]",observ ahh look like work art repeat miss pattern v column suggest mani v column relat perhap tri describ certain characterist transact exampl column v322 v399 ident miss locat let verifi intuit correl heatmap measur nulliti correl strongli presenc absenc one variabl affect presenc anoth
2027,"['**Interpretation**: The dendrogram uses a hierarchical clustering algorithm to bin variables against one another by their nullity correlation. Each cluster of leaves explain how one variable might always be empty when another is empty, or filled when another variable is filled. This dendogram suggests that the position of missing/fill values are correlated. Perhaps similar columns were derived from the same feature or combinations of features.']",interpret dendrogram use hierarch cluster algorithm bin variabl one anoth nulliti correl cluster leav explain one variabl might alway empti anoth empti fill anoth variabl fill dendogram suggest posit miss fill valu correl perhap similar column deriv featur combin featur
2028,"['## 3. Replacement or drop the missings\n', 'The idea of imputation is both *""seductive and dangerous""* in the words of R.J.A Little. \n', '\n', 'I truly believe that there is no best way to deal with missing, especially when having to deal with partial information. Knowing which columns could be imputed or dropped may alter the result of the final predictions by a non-trivial amount. The fact that certain value is missing could have been due to specific variation in the feature (missing not at random). This is one of the process that could have been much more useful if we were given the meaning of each columns. But when life gives you lemon, you turns it into sweet, sweet meachine learning input juice. \n', '\n', '### Understand that Train and Test data were splitted by time\n', '\n', 'This is a graceful finding from https://www.kaggle.com/robikscube/ieee-fraud-detection-first-look-and-eda.\n', '\n', '* The `TransactionDT` feature is defined as time delta from a chosen datetime. This gives us information about the relative time and the countinuity of each transaction. Ploting both test and train `TransactionDT` on the graph suggests that train and test dataset were splited by time, with a gap in between.']",3 replac drop miss idea imput seduct danger word r j littl truli believ best way deal miss especi deal partial inform know column could imput drop may alter result final predict non trivial amount fact certain valu miss could due specif variat featur miss random one process could much use given mean column life give lemon turn sweet sweet meachin learn input juic understand train test data split time grace find http www kaggl com robikscub ieee fraud detect first look eda transactiondt featur defin time delta chosen datetim give u inform rel time countinu transact plote test train transactiondt graph suggest train test dataset splite time gap
2029,"['Some people suggest that if `TransactionDT` is measured in seconds, then the combined time period between test and train dataset could total to approximately 1 year, and the gap can account for ~ 1 month. \n', '\n', 'Lynn@Vesta commented in one of the discussion post:\n', '\n', '*""We define reported chargeback on card, user account, associated email address and transactions directly linked to these attributes as fraud transaction (isFraud=1); If none of above is found after 120 days, then we define as legit (isFraud=0)""*\n']",peopl suggest transactiondt measur second combin time period test train dataset could total approxim 1 year gap account 1 month lynn vesta comment one discus post defin report chargeback card user account associ email address transact directli link attribut fraud transact isfraud 1 none found 120 day defin legit isfraud 0
2030,['## 4. Response/ Target Variable'],4 respons target variabl
2031,"['**Observation**:The fraud percentage is quite high: 7.85% for the complete observations (identity + transaction). We can see there is a class imbalance problem, where occurence of one class is significantly higher than another. This will lead to much a higher false negative - tendency of picking ""not fraud"". We can mitigate this issue by using two common methods:\n', '\n', '1. Cost function based approaches\n', '\n', '2. Sampling based approaches']",observ fraud percentag quit high 7 85 complet observ ident transact see class imbal problem occur one class significantli higher anoth lead much higher fals neg tendenc pick fraud mitig issu use two common method 1 cost function base approach 2 sampl base approach
2032,"['# Explore Categorical Features\n', '**Categorical Features:**\n', '\n', '**1. Transactional Table:**\n', '    \n', '    ProductCD\n', '\n', '    card1 - card6\n', '\n', '    addr1, addr2\n', '\n', '    Pemaildomain Remaildomain\n', '\n', '    M1 - M9\n', '    \n', '    \n', '**2. Identity Table**\n', '\n', '    DeviceType\n', '\n', '    DeviceInfo\n', '    \n', '    id12 - id38\n', '\n', ""Let's take a quick look at these categorical features:""]",explor categor featur categor featur 1 transact tabl productcd card1 card6 addr1 addr2 pemaildomain remaildomain m1 m9 2 ident tabl devicetyp deviceinfo id12 id38 let take quick look categor featur
2033,['## Examine Product Code'],examin product code
2034,['**Observations**: C is the most frequent product category. Product C also have the highest count of fraud. We can obtain the proportion of fraud for each product category:'],observ c frequent product categori product c also highest count fraud obtain proport fraud product categori
2035,"['**Conclusion**: Product C takes up 67.5% of fraud cases for transactions that have identity. And also have highest rate of fraud: 12%, more than double any other class of product.\n', '\n', '**Question**: Why product C? Is there any additional information that help us better understand product C high fraud rate?\n', '\n', 'We have 2 numerical variables that we can compare between groups of products:\n', '\n', 'TransactionDT: timedelta from a given reference datetime\n', '\n', 'TransactionAmt: transaction payment amount in USD']",conclus product c take 67 5 fraud case transact ident also highest rate fraud 12 doubl class product question product c addit inform help u better understand product c high fraud rate 2 numer variabl compar group product transactiondt timedelta given refer datetim transactionamt transact payment amount usd
2036,['**Observation**: Product C are items with low dollar value.'],observ product c item low dollar valu
2037,"['**Observation**: All products have same min and max timedelta range. \n', '\n', '**Conclusion**:The plot suggests little to no difference in timedelta accross all groups.\n', '\n', '## Examine Card 1,2,3,5\n', '\n', ""The card 1,2,3, and 5 was represented as numerical values, temping us to plot the histogram. However, we need to remember that card columns were classified as categorical variables. Meaning it's likely that these numerical variables were coded for categorical variables.""]",observ product min max timedelta rang conclus plot suggest littl differ timedelta accross group examin card 1 2 3 5 card 1 2 3 5 repres numer valu temp u plot histogram howev need rememb card column classifi categor variabl mean like numer variabl code categor variabl
2038,"[""Card 1 contains 8499 unique values, suggesting card 1 may have been ID of the card. Card 2,3 and 5 have less unique values, so perhaps they could be expiration date, or combinations that generate card identity? Since we don't know how these information was scrammbled, we might pickup patterns generated by encryption algorithm instead of data. No further analysis should be done unless more infomation is given.\n"", '\n', 'Same goes for the addr1 and addr2.']",card 1 contain 8499 uniqu valu suggest card 1 may id card card 2 3 5 le uniqu valu perhap could expir date combin gener card ident sinc know inform scrammbl might pickup pattern gener encrypt algorithm instead data analysi done unless infom given goe addr1 addr2
2039,"['## Examine Card 4 and 6\n', '\n', '### Card 4: Card Network']",examin card 4 6 card 4 card network
2040,"['**Observation:** Visa card accounts for the highest instances of fraud, but this also because visa is the most popular card type. Again, we can only conclude after comparing the fraud propotion for each card type:']",observ visa card account highest instanc fraud also visa popular card type conclud compar fraud propot card type
2041,"['**Conclusion**: Visa accounts for 61% of all fraud occurences. However, when normalized by total number of each type, Visa have fraud rate of only 8%, lower than Mastercard and same as Discovercard. Only American Express have significantly lower fraud rate compare to others.']",conclus visa account 61 fraud occur howev normal total number type visa fraud rate 8 lower mastercard discovercard american express significantli lower fraud rate compar other
2042,"['### Card 6: Card Type\n', '\n', 'Similarly, we can use the same method of data analysis on this variable:']",card 6 card type similarli use method data analysi variabl
2043,"['**Observation:** The number of card type are fairly simiar, and so does the fraud cases. ']",observ number card type fairli simiar fraud case
2044,"['**Observation**: Not much difference in fraud rate between credit card and debit card\n', '\n', '## Examine Email Domain\n', '\n', '### 1. Purchaser Email']",observ much differ fraud rate credit card debit card examin email domain 1 purchas email
2045,"['**Observation**: I see alot of domains came from the same distributors such as hotmail.com, hotmail.fr, yahoo.com, yahoo.fr, yahoo.de, etc. We can group these domains together under the parent distributors.\n', '\n', ""**Action:** Create P_parent_emaildomain field that remove the part after '.' ""]",observ see alot domain came distributor hotmail com hotmail fr yahoo com yahoo fr yahoo de etc group domain togeth parent distributor action creat p parent emaildomain field remov part
2046,"[""Fewer email domains result in cleaner x tickers. Let's add the fraud rate like in the previous graphs, but this time we add the rate line on top of this graph:""]",fewer email domain result cleaner x ticker let add fraud rate like previou graph time add rate line top graph
2047,"[""Protonmail returns an exemely high fraud rate. Almost 80% of transactions from purchaser using protonmail.com were label fraud. Let's double check this result:""]",protonmail return exem high fraud rate almost 80 transact purchas use protonmail com label fraud let doubl check result
2048,"['### 2. Recipient Email\n', '\n', 'Similarly, we can perform the similar analysis on Recepient email domains']",2 recipi email similarli perform similar analysi recepi email domain
2049,"['I enjoy this format of visualizing, so I should creat a function that help me explore the categorical format with regard to fraud rate:']",enjoy format visual creat function help explor categor format regard fraud rate
2050,"['## Examine M1-M9\n', '\n', ""The transaction data that has comple identity returns mostly NaN except for M4. Let's check it out:""]",examin m1 m9 transact data compl ident return mostli nan except m4 let check
2051,"['**Observartion**: Not much variation in fraud rate between M0, M1, and M2 in M4\n', '\n', 'We have gone through all categorical variables in the Transaction Table, now we check out the remaining categorical variables in the Identity Table.\n', '\n', '## Examine DeviceType']",observart much variat fraud rate m0 m1 m2 m4 gone categor variabl transact tabl check remain categor variabl ident tabl examin devicetyp
2052,['**Observation**: Fraud rate is higher for mobile device compared to desktop'],observ fraud rate higher mobil devic compar desktop
2053,['## Examine DeviceInfo'],examin deviceinfo
2054,"[""Since we have way too many devices, it makes more sense to select a few devices that has non-trivial count. Let's select categories that have more than 500 counts:""]",sinc way mani devic make sen select devic non trivial count let select categori 500 count
2055,"['**Observation**: We can see the fraud rate is higher for certain devices\n', '\n', '## Examine id12 - id38\n', '\n', 'We may generate all the graphs for id12 to id38. Depend on your preference, some graphs may be more informative than the other. The graphs below are selected based on:\n', '\n', '1. If the graph contains non-masked information (or categories have self-expalainatory meaning)\n', ""    * For example: 'Found' and 'NotFound' are two categories that by themselves, don't provide us with any helpful information in understanding their relationships with target variable. Perhaps our learner can pickup on the differences, but it's outside of our domain to understand these variables semantically. \n"", '\n', '2. If the graph contains not too many categories so that the xtickers can be plotted legibly\n', '\n', 'You can plots them all out and select for yourself. Here are some of my picks:\n', '\n', '### IP Proxy']",observ see fraud rate higher certain devic examin id12 id38 may gener graph id12 id38 depend prefer graph may inform graph select base 1 graph contain non mask inform categori self expalainatori mean exampl found notfound two categori provid u help inform understand relationship target variabl perhap learner pickup differ outsid domain understand variabl semant 2 graph contain mani categori xticker plot legibl plot select pick ip proxi
2056,"['**Obervation**: The first notable id plot is the IP status. It is interesting to see the anonymous IP_Proxy would have a higher fraud rate. If someone were to commit a fraudulent transaction, it makes sense that the person would want to protect his/her identity.\n', '\n', '### Operating Systems']",oberv first notabl id plot ip statu interest see anonym ip proxi would higher fraud rate someon commit fraudul transact make sen person would want protect ident oper system
2057,['We can aggregate the operating system into a few major OSs. '],aggreg oper system major os
2058,"['**Observation**: The fraud rate across multiple well-known OSs seem fairly similar. ""Other"" operating systems have a much higher fraud rate.\n', '\n', ""However, it's strange that we see more IOS devices compared to Android, given that Android is the most popular mobile system. If I were to work for Vista, I would ask how the system collects more IOS instances. Could it be that Vista have given us an filtered dataset? Specific market segment? Systematic error or deficiency in collecting Android info?\n"", '\n', '### Browsers']",observ fraud rate across multipl well known os seem fairli similar oper system much higher fraud rate howev strang see io devic compar android given android popular mobil system work vista would ask system collect io instanc could vista given u filter dataset specif market segment systemat error defici collect android info browser
2059,"['Same as previous plot, we need to reduce the number of categories using aggregation:']",previou plot need reduc number categori use aggreg
2060,"[""We have a few browers that have absurdly high fraud rate. This is likely to due the scarcity of those browsers. We can fix this by apply a minimum-instance-filter. Let's say 0.1 percent of data rows is our cut-off, then each category must have at least 144 instances to be included in our plot:""]",brower absurdli high fraud rate like due scarciti browser fix appli minimum instanc filter let say 0 1 percent data row cut categori must least 144 instanc includ plot
2061,['**Observation**: Opera and android browser have relatively high fraud rate'],observ opera android browser rel high fraud rate
2062,"['# Explore Numerical Features\n', '\n', 'I anticipate that most variables we will encounter would not follow a normal distribution. Therefore, for each variable, we will explore:\n', '\n', '1. Distribution\n', '\n', '2. Log of distribution\n', '\n', '3. Distribution by target variable\n', '\n', '4. Log of distribution by target variable\n', '\n', '5. Boxplot comparison between fraud and non-fraud\n', '\n', '## Examine Transaction Amount']",explor numer featur anticip variabl encount would follow normal distribut therefor variabl explor 1 distribut 2 log distribut 3 distribut target variabl 4 log distribut target variabl 5 boxplot comparison fraud non fraud examin transact amount
2063,"['**Observation**: \n', '    1. TransactionAmt has right-skewed distribution: most transactions are small (less than $200)\n', '    2. There is little difference between distribution and average amount for fraud and non-fraud']",observ 1 transactionamt right skew distribut transact small le 200 2 littl differ distribut averag amount fraud non fraud
2064,['## Examine Transaction DT'],examin transact dt
2065,"['**Observation**: There is a large number of non-fraud transactions generated at a certain period . This discrepancy also causing the difference in our boxplot.\n', '\n', '**Possible Improvement**: I should try undersampling the period of non-fraud so that we have less imbalance issue for that particular period.']",observ larg number non fraud transact gener certain period discrep also caus differ boxplot possibl improv tri undersampl period non fraud le imbal issu particular period
2066,"['## Examine Distance 2\n', '\n', 'Dist1 contains no values. For dist2, we also running into two problems:\n', '\n', '1. Missing values:\n', '\n', '    Solution: keeping only the non-null rows in dist2.\n', '\n', '2. Zero values:\n', '\n', '    Zero values cause log transform to return infinity values\n', '\n', '    Solution: add small amount to 0s to avoid infinity\n', '    \n', '3. Negative values\n', '    \n', '    The logarithm is only defined for positive numbers. I could perhaps take the log(x+n), where n is the offset values that make the min negative value > 0. However, for such data 0 has a meaning (equality!) that should be respected. Unless I know the meaning of the data, I cannot make arbitrary transformation.\n', '    \n', '    Solution: no solution, omit the log-transformation graphs\n', '\n', ""Let's update our graphing function with this implementation\n""]",examin distanc 2 dist1 contain valu dist2 also run two problem 1 miss valu solut keep non null row dist2 2 zero valu zero valu caus log transform return infin valu solut add small amount 0 avoid infin 3 neg valu logarithm defin posit number could perhap take log x n n offset valu make min neg valu 0 howev data 0 mean equal respect unless know mean data make arbitrari transform solut solut omit log transform graph let updat graph function implement
2067,"['**Observation**: Dist2 does not seem to varies between fraud and not-fraud\n', '\n', '## Examine C features\n', '\n', 'Same way of handling a large number of variables, I only choose the notable plots that reflect a large degree of variation. Trying to keep this kernel concised is one of my goals. In this case, I only consider C3 to have some significant patterns:']",observ dist2 seem vari fraud fraud examin c featur way handl larg number variabl choos notabl plot reflect larg degre variat tri keep kernel concis one goal case consid c3 signific pattern
2068,"['**Observation**: Higher values of C3 associated with no-fraud.\n', '\n', 'C5 and C9 are homogeneous columns.\n', '\n', '## Examine D features']",observ higher valu c3 associ fraud c5 c9 homogen column examin featur
2069,"['## Conclusion for EDA:\n', '\n', '1. Target variable has class imbalance problem where instance of fraud is much lower than non-fraud\n', '\n', '2. Multiple columns contain too many missing values\n', '\n', '3. Several columns are homogeneous, therefore, prodvide no useful information in predicting the target variable (this may not be the case for transaction table since we are using a joined table)\n', '\n', '4. There is period of time where instances of non-fraud far exceed the usual proportion of non-fraud to fraud \n', '\n', '5. Basic understand of variables can help us do simple feature engineering\n', '\n', ""We will deal with each problem with the purpose of improving the prediction accuracy. But first, let's try a default XGBoost model provided by Vesta. We can use this model as a baseline to compare the improvement (or reduction) of each engineered feature, change, and alteration that  we made along the way.\n"", '\n', '## Brainstorm\n', 'Before treating this problem like a black box of ensemble learning, it\'s worthwhile to take our hands off the keyboards and think about the problem of fraud detection in a more ""open-box"" way. There are a lot of intersting questions worth investigating before diving into the madness of hyperparameters tuning. Insights that could lead to trivial and sometimes important questions. Questions that take us on a journey of curiosity and fulfilment. \n', '\n', 'For the data scientists whose minds love to wander. This section is dedicated for silly and serious questions alike.\n', '\n', ""**Scenario** :A Vesta executive storms in the office and excitedly tells everyone that an exciting project has fallen in their laps. It's the fraud detection problem. And he ask his people for some ideas of where to start, which features should be useful in prediciting fraud. He knows it is strange to ask the scientists before attempting any EDA or modeling. After all, they haven't seen a lick of relevant data. But he saids it would be great practive to dip the toe into the water before diving in without any direction. So let's start with the few things that were provided to us: transaction amount, time, card infor, identity, etc... Which information would give us a good start at cracking this problem?\n"", '\n', 'Let\'s define clearly what is a fraud transaction first. ""Fraud detection is a set of activities undertaken to prevent money or property from being obtained through false pretenses"" [Source](https://searchsecurity.techtarget.com/definition/fraud-detection). Most common type of frauds are forging checks or using stolen credit cards. If a person got of hold of your card info, what should he/she do with it? After browsing on Reddit, I found some crude scenarios:\n', '\n', ""1. If you drop your card, it's likely the person who found it by chance and commit a fraud would spend it on consumable and essential products like grocery and gas. The perp will likely go somewhere nearby and spend a larger amount than usual before the card get locked. So perhaps we should look at user's purchase history so that any activities or purchases that deviate from normal buying habit would stand out. But we don't have identifiable data, so we can't go on this route.\n"", '\n', ""2. If your information get hacked by careless purchases on some shady websites/gas stations, it's likely that your information will be sold to someone else who use your information for making fraud transaction. This person will make an online purchase and ship it to a distributor, who sells the good for cash and share the profit with the frauder. In this situation, the good is shipped to some far-away place from the user's home address. So the further the distance, the more a transaction looks like a fraud? No, of course not. People sends gifts all the times. But perhaps gifting 3 expensive laptops is slightly more suspicious than gifting a box of chocolate.\n"", '    \n', '    **Feature Engineering:** Combine transaction amount, type of good, and distance together.\n', '    \n', ""3. Fraud commited by someone close to you (family member: spouse, siblings, etc). It's rare, but it could happen.\n"", '\n', '4. Prefered tools for committing fraud. We have learned previously in the EDA that Protonmail has exceptionally high fraud rate >95%. A quick google reveal that Proton is a email service that provide free, anonymous, end-to-end encryption email accounts. Quote from Proton website: ""ProtonMail is incorporated in Switzerland and all our servers are located in Switzerland. This means all user data is protected by strict Swiss privacy laws"". Meaning fraud perpetrator not only protected by the full extend of the privacy law, but also doing it at no cost. Similarly, we have other tools that also have abnormally high fraud rate such as:\n', '\n', '    * Browser: Comodo IceDragon, Mozilla/Firefox?? (not firefox, but perhaps is Comodo IceDragon but recognized as another version of Firefox?)\n', '    \n', '    * Operating system: ""other"" category has fraud rate of 60%.\n', '    \n', '    * Phone (or browser?): Lanix Ilium\n', '    \n', '    **Feature Engineering:** New features that emphasize the importance of these tools\n', '    \n', '5. Time of operation. Just like any other jobs, frauders operate at routinely hours that perhaps different from the real users. It is strange, at least to me, to make purchase decision to buy an iphone at 3 in the morning. Again, without historical data, this approach is dead in the egg.\n', '\n', '\n', '# Baseline Model\n', '\n', 'Modeling section is being explore in another private notebook since only 1 GPU instance is allow in Kaggle Kernel...']",conclus eda 1 target variabl class imbal problem instanc fraud much lower non fraud 2 multipl column contain mani miss valu 3 sever column homogen therefor prodvid use inform predict target variabl may case transact tabl sinc use join tabl 4 period time instanc non fraud far exceed usual proport non fraud fraud 5 basic understand variabl help u simpl featur engin deal problem purpos improv predict accuraci first let tri default xgboost model provid vesta use model baselin compar improv reduct engin featur chang alter made along way brainstorm treat problem like black box ensembl learn worthwhil take hand keyboard think problem fraud detect open box way lot interst question worth investig dive mad hyperparamet tune insight could lead trivial sometim import question question take u journey curio fulfil data scientist whose mind love wander section dedic silli seriou question alik scenario vesta execut storm offic excitedli tell everyon excit project fallen lap fraud detect problem ask peopl idea start featur use predicit fraud know strang ask scientist attempt eda model seen lick relev data said would great practiv dip toe water dive without direct let start thing provid u transact amount time card infor ident etc inform would give u good start crack problem let defin clearli fraud transact first fraud detect set activ undertaken prevent money properti obtain fals pretens sourc http searchsecur techtarget com definit fraud detect common type fraud forg check use stolen credit card person got hold card info brow reddit found crude scenario 1 drop card like person found chanc commit fraud would spend consum essenti product like groceri ga perp like go somewher nearbi spend larger amount usual card get lock perhap look user purchas histori activ purchas deviat normal buy habit would stand identifi data go rout 2 inform get hack careless purchas shadi websit ga station like inform sold someon el use inform make fraud transact person make onlin purchas ship distributor sell good cash share profit frauder situat good ship far away place user home address distanc transact look like fraud cours peopl send gift time perhap gift 3 expens laptop slightli suspici gift box chocol featur engin combin transact amount type good distanc togeth 3 fraud commit someon close famili member spous sibl etc rare could happen 4 prefer tool commit fraud learn previous eda protonmail except high fraud rate 95 quick googl reveal proton email servic provid free anonym end end encrypt email account quot proton websit protonmail incorpor switzerland server locat switzerland mean user data protect strict swiss privaci law mean fraud perpetr protect full extend privaci law also cost similarli tool also abnorm high fraud rate browser comodo icedragon mozilla firefox firefox perhap comodo icedragon recogn anoth version firefox oper system categori fraud rate 60 phone browser lanix ilium featur engin new featur emphas import tool 5 time oper like job frauder oper routin hour perhap differ real user strang least make purchas decis buy iphon 3 morn without histor data approach dead egg baselin model model section explor anoth privat notebook sinc 1 gpu instanc allow kaggl kernel
2071,"['## Introduction\n', '\n', 'The aim of this competition is to predict whether a given transaction is fraudelent or not based on the information given regarding the transaction. Information such as the transaction amount, card type, product category, etc is given.\n', '\n', 'In this kernel, I will be visualizing and exploring these features in-depth using **seaborn**. I will attempt to make inferences and conlusions based on these graphs and plots.\n', '\n', 'Then, I will demonstrate how to build models to solve this problem. Specifically, I will be using LightGBM and Neural Network models. I will train these models on the training data, and then finally, I will show how to make predictions on the test data using these trained models.']",introduct aim competit predict whether given transact fraudel base inform given regard transact inform transact amount card type product categori etc given kernel visual explor featur depth use seaborn attempt make infer conlus base graph plot demonstr build model solv problem specif use lightgbm neural network model train model train data final show make predict test data use train model
2072,"['<center><img src=""https://i.imgur.com/K3FE0NA.jpg"" width=""500px""></center>']",center img src http imgur com k3fe0na jpg width 500px center
2073,"[""<font size=3 color='red'>Please upvote this kernel if you like it :)</font>""]",font size 3 color red plea upvot kernel like font
2074,['## Preparing the ground for analysis'],prepar ground analysi
2075,['### Import necessary libraries'],import necessari librari
2076,['### Check the files available in the dataset'],check file avail dataset
2077,['### Define the paths for the train and test data'],defin path train test data
2078,['### Load the train and test datasets'],load train test dataset
2079,['### Replace the NaNs with 0s and check the first few rows'],replac nan 0 check first row
2080,['# EDA'],eda
2081,"['## Transaction Amounts\n', '\n', 'As the name suggests, this is the amount of money transferred during the transaction, and this is clearly a continuous variable. I will visualize this feature in relation with the target, *isFraud*.\n', '\n']",transact amount name suggest amount money transfer transact clearli continu variabl visual featur relat target isfraud
2082,['### Distribution of transaction amounts for non-fraudelent and fraudulent cases'],distribut transact amount non fraudel fraudul case
2083,['### Violin Plot'],violin plot
2084,"['In the violin plot above, the green violin is the distribution of transaction amounts for non-fraudulent samples and the red violin is that for fraudulent samples. It can be seen that both distributions have a strong positive (leftward) skew. But, the red violin has greater probability density towards the higher values of *TransactionAmt* as compared to the green distribution. \n', '\n', 'The green distribution has a very high probability denisty concentrated around the lower values of *TransactionAmt* and as a result, the probability density around the higher values of *TransactionAmt* is almost negligible. But, the red violin on the other hand, has lesser probability density concentrated around the lower values of *TransactionAmt* and thus, there is a considerable probability density around the higher values of *TransactionAmt*.\n', '\n', 'This happens because the green violin has multiple peaks (multimodal characteristic) around the lower values of *TransactionAmt*, whereas the red violin has only one clear peak in this region. This presence of only one peak in the red violin indicates that it has a much milder skew than the green violin. **Therefore, in general, the greater the transaction amount, the more likely it is for the transaction to be fraudulent.**\n', '\n', 'This makes intuitive sense, because very expensive transactions have a greater chance of being fraudulent than less expensive transactions.\n', '\n', '\n']",violin plot green violin distribut transact amount non fraudul sampl red violin fraudul sampl seen distribut strong posit leftward skew red violin greater probabl densiti toward higher valu transactionamt compar green distribut green distribut high probabl denisti concentr around lower valu transactionamt result probabl densiti around higher valu transactionamt almost neglig red violin hand lesser probabl densiti concentr around lower valu transactionamt thu consider probabl densiti around higher valu transactionamt happen green violin multipl peak multimod characterist around lower valu transactionamt wherea red violin one clear peak region presenc one peak red violin indic much milder skew green violin therefor gener greater transact amount like transact fraudul make intuit sen expens transact greater chanc fraudul le expens transact
2085,['### Box Plot'],box plot
2086,"['The box plot above also suggests that more expensive transactions are more likely to be fraudulent. This can be inferred from the fact that the mean value of the red box is greater than that of the green box. Although the first quartiles of the two distributions are very similar, the third quartile of the red box is significantly greater than that of the green box, providing further evidence that higher transaction amounts are more likely to be fraudulent than not, *i.e.* **the greater the transaction amount, the more likely it is for the transaction to be fraudulent.** ']",box plot also suggest expens transact like fraudul infer fact mean valu red box greater green box although first quartil two distribut similar third quartil red box significantli greater green box provid evid higher transact amount like fraudul e greater transact amount like transact fraudul
2087,['## Product CD'],product cd
2088,"['I am not sure what Product CD exactly means, but it seems to be a categorical variable that provides some information regarding the category of a product. **Products in this dataset come under five broad categories: W, H, C, S, and R.** I will visualize this feature in relation with the target, *isFraud*.']",sure product cd exactli mean seem categor variabl provid inform regard categori product product dataset come five broad categori w h c r visual featur relat target isfraud
2089,['### Frequencies of the different product categories '],frequenc differ product categori
2090,"['From the above plot, we can clearly see that the most common *ProductCD* value is W. The other four product categories, H, C, S, and R are very rare compared to W. ']",plot clearli see common productcd valu w four product categori h c r rare compar w
2091,"['In the above proportion plot, the height of the dark green bar (at the top) represents the probability of a transaction involving a given product category being fraudulent. We can see that a product of category *C* is more likely to be involved in a fraudulent transaction as compared to any other product category. The next most fraudulence-prone product category is category *S*, and so on.\n', '\n']",proport plot height dark green bar top repres probabl transact involv given product categori fraudul see product categori c like involv fraudul transact compar product categori next fraudul prone product categori categori
2092,['### Distributions of transaction amounts for different *ProductCD* values (for non-fraudulent and fraudulent cases)'],distribut transact amount differ productcd valu non fraudul fraudul case
2093,"['In the above violin plots, the light green sections represent the distribution for non-fraudulent cases and the dark green sections represent the distribution for fraudulent cases. We can see the same trend as before. Both the distributions types have strong positive (leftward) skews. But, the light green (non-fraudulent) distributions have a greater probability density concentrated around the lower values of transaction amounts, once again, because there are several peaks (unlike the dark green distributions).\n', '\n', 'But, there are a few exceptions to this trend in the above visualization. For example, the non-fraudulent and fraudulent distributions are very similar for the *C* product category. Both the distributions have only one peak and they look very similar in almost every way. Interestingly, the *C* product category also has the highest fraudulence rate, and this is probably the reason why the correlation between the transaction amount and the target is very low for this category.\n', '\n', 'But, for the rest of the product categories, the trend is roughly followed.\n', '\n', 'The distributions for the *S* product category seem to have the lowest means and the strongest skews. These distributions have a very high concentration of probability density around the lower values of *TransactionAmt*. Transactions of type *S* tend to have low transaction amounts. On the other side of the spectrum, the distributions for the *R* product category seem to have the highest means and the weakest skews. These distributions have a very even spread and almost no skew. Transactions of type *R* tend to have high transaction amounts.\n', '\n', 'Maybe, one can create a model that changes the weightage of the *TransactionAmt* feature based on the product category. For example, the weightage of the *TransactionAmt* feature can be reduced for the *C* product category, because the fraudulent and non-fraudulent distributions are very similar for this product category. ']",violin plot light green section repres distribut non fraudul case dark green section repres distribut fraudul case see trend distribut type strong posit leftward skew light green non fraudul distribut greater probabl densiti concentr around lower valu transact amount sever peak unlik dark green distribut except trend visual exampl non fraudul fraudul distribut similar c product categori distribut one peak look similar almost everi way interestingli c product categori also highest fraudul rate probabl reason correl transact amount target low categori rest product categori trend roughli follow distribut product categori seem lowest mean strongest skew distribut high concentr probabl densiti around lower valu transactionamt transact type tend low transact amount side spectrum distribut r product categori seem highest mean weakest skew distribut even spread almost skew transact type r tend high transact amount mayb one creat model chang weightag transactionamt featur base product categori exampl weightag transactionamt featur reduc c product categori fraudul non fraudul distribut similar product categori
2094,"['The same trends and patterns can be seen in the box plots above. Both the distributions types have strong positive (leftward) skews. But, the light green (non-fraudulent) distributions clearly have lower means as compared to the dark green (fraudulent) distributions.\n', '\n', 'The same exceptions to this trend can also be seen in the above visualization. For example, the non-fraudulent and fraudulent distributions are very similar for the *C* product category. Both the distributions have similar means, first quartiles, and third quartiles.']",trend pattern seen box plot distribut type strong posit leftward skew light green non fraudul distribut clearli lower mean compar dark green fraudul distribut except trend also seen visual exampl non fraudul fraudul distribut similar c product categori distribut similar mean first quartil third quartil
2095,['## P_emaildomain'],p emaildomain
2096,"['The *P_emaildomain* represents the email domain through which the transaction was **payed**. The most common domains are *gmail.com, yahoo.com, hotmail.com, and anonymous.com*.']",p emaildomain repres email domain transact pay common domain gmail com yahoo com hotmail com anonym com
2097,['### Frequencies of the different email domains (P)'],frequenc differ email domain p
2098,"['From the above plot, we can clearly see that the most common *P_emaildomain* value is *gmail.com*. The next most common email domain (for payment) is *yahoo.com* and the rest are comparatively very rare. ']",plot clearli see common p emaildomain valu gmail com next common email domain payment yahoo com rest compar rare
2099,['### Fraudulence Proportion Plot'],fraudul proport plot
2100,"['In the above proportion plot, the height of the dark blue bar (at the top) represents the probability of a transaction made through a given email domain being fraudulent. We can see that an email domain of *hotmail.com* is associated with the highest probability of a fraudulent transaction. The next most fraudulence-prone email domain is *gmail.com*. And, the least fraudulence-prone email domain is *yahoo.com*.  ']",proport plot height dark blue bar top repres probabl transact made given email domain fraudul see email domain hotmail com associ highest probabl fraudul transact next fraudul prone email domain gmail com least fraudul prone email domain yahoo com
2101,['### TransactionAmt vs. P_emaildomain Violin Plot'],transactionamt v p emaildomain violin plot
2102,"['In the above violin plots, the light blue sections represent the distribution for non-fraudulent cases and the dark blue sections represent the distribution for fraudulent cases. We can see the same trend as before. Both the distributions types have strong positive (leftward) skews. But, the light blue (non-fraudulent) distributions have a greater probability density concentrated around the lower values of transaction amounts, once again, because there are several peaks (unlike the dark blue distributions).\n', '\n', 'There are no real exceptions to this trend except for *hotmail.com*. The distributions for transaction amount (for fraudulent and non-fraudulent cases) are very similar at *hotmail.com*. The other domains, namely, *gmail.com, yahoo.com, and anonymous.com* follow the general trend.\n', '\n', ""Each email domain's distributions are roughly concentrated around the same mean, but with differing variances. No single email domain has a much greater or lower mean than the other email domains.\n"", '\n', '**Interestingly, once again, the most fraudulence-prone email domain, namely, *hotmail.com* is also the one that does not follow the trend. This is similar to the *C* product category, which was the most fraudulent category and was also the category that did not follow the trend.** \n', ' \n', '\n', 'Maybe, one can create a model that changes the weightage of the *TransactionAmt* feature based on the email domain. For example, the weightage of the *TransactionAmt* feature can be reduced for the *hotmail.com* email domain, because the fraudulent and non-fraudulent distributions are very similar for this email domain. ']",violin plot light blue section repres distribut non fraudul case dark blue section repres distribut fraudul case see trend distribut type strong posit leftward skew light blue non fraudul distribut greater probabl densiti concentr around lower valu transact amount sever peak unlik dark blue distribut real except trend except hotmail com distribut transact amount fraudul non fraudul case similar hotmail com domain name gmail com yahoo com anonym com follow gener trend email domain distribut roughli concentr around mean differ varianc singl email domain much greater lower mean email domain interestingli fraudul prone email domain name hotmail com also one follow trend similar c product categori fraudul categori also categori follow trend mayb one creat model chang weightag transactionamt featur base email domain exampl weightag transactionamt featur reduc hotmail com email domain fraudul non fraudul distribut similar email domain
2103,['### TransactionAmt vs. P_emaildomain Box Plot '],transactionamt v p emaildomain box plot
2104,"['The same trends and patterns can be seen in the box plots above. Both the distributions types have strong positive (leftward) skews. But, the light blue (non-fraudulent) distributions clearly have lower means as compared to the dark blue (fraudulent) distributions.\n', '\n', 'The same exceptions to this trend can also be seen in the above visualization. For example, the non-fraudulent and fraudulent distributions are very similar for the *hotmail.com* email domain. Both the distributions have similar means and first quartiles. Although the third quartiles for the *anonymous.com* distributions are very similar, their means are very different.']",trend pattern seen box plot distribut type strong posit leftward skew light blue non fraudul distribut clearli lower mean compar dark blue fraudul distribut except trend also seen visual exampl non fraudul fraudul distribut similar hotmail com email domain distribut similar mean first quartil although third quartil anonym com distribut similar mean differ
2105,['## R_emaildomain'],r emaildomain
2106,"['The *R_emaildomain* represents the email domain through which the transaction was **received**. The most common domains are *gmail.com, yahoo.com, hotmail.com, and anonymous.com*. ']",r emaildomain repres email domain transact receiv common domain gmail com yahoo com hotmail com anonym com
2107,['### Frequencies of the different email domains (R)'],frequenc differ email domain r
2108,"['From the above plot, we can clearly see that the most common *R_emaildomain* value is *gmail.com*. The next most common email domain is *hotmail.com*. The least common receiver email domain is *yahoo.com*.']",plot clearli see common r emaildomain valu gmail com next common email domain hotmail com least common receiv email domain yahoo com
2109,['### Fraudulence Proportion Plot'],fraudul proport plot
2110,"['In the above proportion plot, the height of the dark red bar (at the top) represents the probability of a transaction received through a given email domain being fraudulent. We can see that an email domain of *gmail.com* is associated with the highest probability of a fraudulent transaction. The next most fraudulence-prone email domain is *hotmail.com*. And, the least fraudulence-prone email domain is *anonymous.com*.']",proport plot height dark red bar top repres probabl transact receiv given email domain fraudul see email domain gmail com associ highest probabl fraudul transact next fraudul prone email domain hotmail com least fraudul prone email domain anonym com
2111,['### TransactionAmt vs. R_emaildomain  Violin Plot'],transactionamt v r emaildomain violin plot
2112,"['In the above violin plots, the pink sections represent the distribution for non-fraudulent cases and the red sections represent the distribution for fraudulent cases. We can see the same trend as before. Both the distributions types have strong positive (leftward) skews. But, the pink (non-fraudulent) distributions have a greater probability density concentrated around the lower values of transaction amounts, once again, because there are several peaks (unlike the red distributions).\n', '\n', 'The exception to this trend is *anonymous.com*. The distributions for transaction amount (at *anonymous.com*) is much more skewed in the fraudulent case as compared to the non-fraudulent case. The other domains, namely, *gmail.com, yahoo.com, and hotmail.com* follow the general trend.\n', '\n', ""Each email domain's distributions are roughly concentrated around the same mean, but with differing variances. No single email domain has a much greater or lower mean than the other email domains.\n"", '\n', 'Maybe, one can create a model that changes the weightage of the *TransactionAmt* feature based on the email domain. For example, the weightage of the *TransactionAmt* feature can be reduced for the *hotmail.com* email domain, because the fraudulent and non-fraudulent distributions are very similar for this email domain. ']",violin plot pink section repres distribut non fraudul case red section repres distribut fraudul case see trend distribut type strong posit leftward skew pink non fraudul distribut greater probabl densiti concentr around lower valu transact amount sever peak unlik red distribut except trend anonym com distribut transact amount anonym com much skew fraudul case compar non fraudul case domain name gmail com yahoo com hotmail com follow gener trend email domain distribut roughli concentr around mean differ varianc singl email domain much greater lower mean email domain mayb one creat model chang weightag transactionamt featur base email domain exampl weightag transactionamt featur reduc hotmail com email domain fraudul non fraudul distribut similar email domain
2113,['### TransactionAmt vs. R_emaildomain  Box Plot'],transactionamt v r emaildomain box plot
2114,"['The same trends and patterns can be seen in the box plots above. Both the distributions types have strong positive (leftward) skews. But, the light blue (non-fraudulent) distributions clearly have lower means as compared to the dark blue (fraudulent) distributions.\n', '\n', 'The same exceptions to this trend can also be seen in the above visualization. For example, at *anonymous.com*, the fraudulent distribution has a much lower mean than the non-fraudulent one (counterintuitively). Also, the non-fraudulent and fraudulent distributions are very similar for the *hotmail.com* email domain. Both the distributions have similar means, quartiles, minima, and maxima.']",trend pattern seen box plot distribut type strong posit leftward skew light blue non fraudul distribut clearli lower mean compar dark blue fraudul distribut except trend also seen visual exampl anonym com fraudul distribut much lower mean non fraudul one counterintuit also non fraudul fraudul distribut similar hotmail com email domain distribut similar mean quartil minimum maximum
2115,['## Card brand (card4)'],card brand card4
2116,"['The card4 feature represents the brand of the card through which the transaction was made. The card brands in this dataset are *discover, mastercard, visa,* and *american express*.']",card4 featur repres brand card transact made card brand dataset discov mastercard visa american express
2117,['### Frequencies of the different card brands'],frequenc differ card brand
2118,"['From the above plot, it is clear that the most common card brand used for transactions is *visa*. The second most common card brand is *mastercard*. The remaining two card brands, namely, *discover* and *american express* are very rare (comparatively).']",plot clear common card brand use transact visa second common card brand mastercard remain two card brand name discov american express rare compar
2119,['### Fraudulence Proportion Plot'],fraudul proport plot
2120,"['In the above proportion plot, the height of the dark orange bar (at the top) represents the probability of a transaction received through a given card brand being fraudulent. We can see that *discover* cards are associated with the highest probability of a fraudulent transaction. The remaining card brands have a very similar fraudelence proportion.']",proport plot height dark orang bar top repres probabl transact receiv given card brand fraudul see discov card associ highest probabl fraudul transact remain card brand similar fraudel proport
2121,['### TransactionAmt vs. card4 Violin Plot '],transactionamt v card4 violin plot
2122,"['In the above violin plots, the light orange sections represent the distribution for non-fraudulent cases and the dark orange sections represent the distribution for fraudulent cases. We can see the same trend as before. Both the distributions types have strong positive (leftward) skews. But, the light orange (non-fraudulent) distributions have a greater probability density concentrated around the lower values of transaction amounts, once again, because they have several peaks (unlike the dark orange distributions).\n', '\n', 'There do not seem to be any exceptions to this trend. The distributions for *american express* and *discover* cards seem to have higher means as compared to the other two card types. Therefore, more expensive transactions tend to take place through *american express* and *discover* cards.']",violin plot light orang section repres distribut non fraudul case dark orang section repres distribut fraudul case see trend distribut type strong posit leftward skew light orang non fraudul distribut greater probabl densiti concentr around lower valu transact amount sever peak unlik dark orang distribut seem except trend distribut american express discov card seem higher mean compar two card type therefor expens transact tend take place american express discov card
2123,['### TransactionAmt vs. card4 Box Plot '],transactionamt v card4 box plot
2124,"['In the box plot above, we can say that, for *mastercard* and *visa* cards, the fraudulent and non-fraudulent distributions are very similar. They have very similar means. It is also clear in this box plot, that *discover* and *american express* cards tend to be associated with higher transaction amounts. This can be inferred from the fact that they have much higher means as compared to the other two card brands.']",box plot say mastercard visa card fraudul non fraudul distribut similar similar mean also clear box plot discov american express card tend associ higher transact amount infer fact much higher mean compar two card brand
2125,['## Card type (card6)'],card type card6
2126,['The card6 feature represents the btype of the card through which the transaction was made. The major card types in this dataset are *credit* and *debit*.'],card6 featur repres btype card transact made major card type dataset credit debit
2127,['### Frequencies of the different card types'],frequenc differ card type
2128,['The two most common card types in this dataset are credit and debit (the rest are comparatively negligible). Debit cards are more common than credit cards in this dataset.'],two common card type dataset credit debit rest compar neglig debit card common credit card dataset
2129,['### Fraudulence Proportion Plot'],fraudul proport plot
2130,"['In the above proportion plot, the height of the dark purple bar (at the top) represents the probability of a transaction received through a given card type being fraudulent. We can see that a given *credit* card is more likely involved in a fraudulent transaction as compared to a given *debit card*.']",proport plot height dark purpl bar top repres probabl transact receiv given card type fraudul see given credit card like involv fraudul transact compar given debit card
2131,['### TransactionAmt vs. card6 Violin Plot'],transactionamt v card6 violin plot
2132,"['In the above violin plots, the light purple sections represent the distribution for non-fraudulent cases and the dark purple sections represent the distribution for fraudulent cases. We can see the same trend as before. Both the distributions types have strong positive (leftward) skews. But, the light purple (non-fraudulent) distributions have a greater probability density concentrated around the lower values of transaction amounts, once again, because they have several peaks (unlike the dark purple distributions).\n', '\n', 'In the above violin plot, it can be seen that debit cards cards are associated with lower transaction amounts as compared to credit cards because their distributions have stronger positive (leftward) skew. Also, the credit card distributions have higher means.']",violin plot light purpl section repres distribut non fraudul case dark purpl section repres distribut fraudul case see trend distribut type strong posit leftward skew light purpl non fraudul distribut greater probabl densiti concentr around lower valu transact amount sever peak unlik dark purpl distribut violin plot seen debit card card associ lower transact amount compar credit card distribut stronger posit leftward skew also credit card distribut higher mean
2133,['### TransactionAmt vs. card6 Box Plot'],transactionamt v card6 box plot
2134,"['In the box plot above, we can see that credit cards are associated with much higher mean transaction amounts as compared to debit cards. ']",box plot see credit card associ much higher mean transact amount compar debit card
2135,"['## ""M"" Features']",featur
2136,"['I am not sure about the meanings of these features (M1, M2, and so on till M9), but the competition overview states that these are categorical features.']",sure mean featur m1 m2 till m9 competit overview state categor featur
2137,"['### Fraudulence Proportion Plots of ""M"" features']",fraudul proport plot featur
2138,['### M1'],m1
2139,"['From the above proportion plot, it can be seen that the most fraudulence prone M1 value is 0.0. This is because all the NaN values were converted to 0s in the beginning. Therefore, an undefined M1 value has the highest chance of being fraudulent. Besides this, a sample with an M1 value of T has a much higher chance of being fraudulent than one with an M1 value of F.']",proport plot seen fraudul prone m1 valu 0 0 nan valu convert 0 begin therefor undefin m1 valu highest chanc fraudul besid sampl m1 valu much higher chanc fraudul one m1 valu f
2140,['### M2'],m2
2141,"['From the above proportion plot, it can be seen that the most fraudulence prone M2 value is 0.0. This is because all the NaN values were converted to 0s in the beginning. Therefore, an undefined M2 value has the highest chance of being fraudulent. Besides this, a sample with an M2 value of F has a much higher chance of being fraudulent than one with an M2 value of T.']",proport plot seen fraudul prone m2 valu 0 0 nan valu convert 0 begin therefor undefin m2 valu highest chanc fraudul besid sampl m2 valu f much higher chanc fraudul one m2 valu
2142,['### M3'],m3
2143,"['From the above proportion plot, it can be seen that the most fraudulence prone M3 value is 0.0. This is because all the NaN values were converted to 0s in the beginning. Therefore, an undefined M3 value has the highest chance of being fraudulent. Besides this, a sample with an M3 value of F has a much higher chance of being fraudulent than one with an M3 value of T.']",proport plot seen fraudul prone m3 valu 0 0 nan valu convert 0 begin therefor undefin m3 valu highest chanc fraudul besid sampl m3 valu f much higher chanc fraudul one m3 valu
2144,['### M4'],m4
2145,"['From the above proportion plot, it can be seen that a that a sample with an M4 value of M2 has a much higher chance of being fraudulent than one with an M4 value of M0. Also, a sample with an M4 value of M0 has a higher chance of being fraudulent than one with an M4 value of M1. An M4 value of 0.0 (or NaN) has the lowest chance of being fraudulent.']",proport plot seen sampl m4 valu m2 much higher chanc fraudul one m4 valu m0 also sampl m4 valu m0 higher chanc fraudul one m4 valu m1 m4 valu 0 0 nan lowest chanc fraudul
2146,['### M5'],m5
2147,"['From the above proportion plot, it can be seen that the most fraudulence prone M5 value is 0.0. This is because all the NaN values were converted to 0s in the beginning. Therefore, an undefined M5 value has the highest chance of being fraudulent. Besides this, a sample with an M5 value of T has a much higher chance of being fraudulent than one with an M5 value of F.']",proport plot seen fraudul prone m5 valu 0 0 nan valu convert 0 begin therefor undefin m5 valu highest chanc fraudul besid sampl m5 valu much higher chanc fraudul one m5 valu f
2148,['### M6'],m6
2149,"['From the above proportion plot, it can be seen that the most fraudulence prone M6 value is 0.0. This is because all the NaN values were converted to 0s in the beginning. Therefore, an undefined M6 value has the highest chance of being fraudulent. Besides this, a sample with an M6 value of F has a much higher chance of being fraudulent than one with an M6 value of T.']",proport plot seen fraudul prone m6 valu 0 0 nan valu convert 0 begin therefor undefin m6 valu highest chanc fraudul besid sampl m6 valu f much higher chanc fraudul one m6 valu
2150,['### M7'],m7
2151,"['From the above proportion plot, it can be seen that the most fraudulence prone M7 value is 0.0. This is because all the NaN values were converted to 0s in the beginning. Therefore, an undefined M7 value has the highest chance of being fraudulent. Besides this, a sample with an M7 value of F has a very similar chance of being fraudulent to one with an M7 value of T.']",proport plot seen fraudul prone m7 valu 0 0 nan valu convert 0 begin therefor undefin m7 valu highest chanc fraudul besid sampl m7 valu f similar chanc fraudul one m7 valu
2152,['### M8'],m8
2153,"['From the above proportion plot, it can be seen that the most fraudulence prone M8 value is 0.0. This is because all the NaN values were converted to 0s in the beginning. Therefore, an undefined M8 value has the highest chance of being fraudulent. Besides this, a sample with an M8 value of T has a very similar chance of being fraudulent to one with an M8 value of F.']",proport plot seen fraudul prone m8 valu 0 0 nan valu convert 0 begin therefor undefin m8 valu highest chanc fraudul besid sampl m8 valu similar chanc fraudul one m8 valu f
2154,['### M9'],m9
2155,"['From the above proportion plot, it can be seen that the most fraudulence prone M9 value is 0.0. This is because all the NaN values were converted to 0s in the beginning. Therefore, an undefined M9 value has the highest chance of being fraudulent. Besides this, a sample with an M9 value of F has a much higher chance of being fraudulent than one with an M9 value of T.']",proport plot seen fraudul prone m9 valu 0 0 nan valu convert 0 begin therefor undefin m9 valu highest chanc fraudul besid sampl m9 valu f much higher chanc fraudul one m9 valu
2156,['# Modeling'],model
2157,['## Preparing the training and testing data'],prepar train test data
2158,['### Define the categorical columns'],defin categor column
2159,['### Convert categorical string data into numerical format'],convert categor string data numer format
2160,['### Create final train and validation arrays'],creat final train valid array
2161,['## LightGBM'],lightgbm
2162,['### Build and train LightGBM model'],build train lightgbm model
2163,"['**This trained model can be used to make predictions on the training data using:**\n', '\n', '> model.predict(X_test)']",train model use make predict train data use model predict x test
2164,['### Visualize feature importances'],visual featur import
2165,"['The above diagram ranks the features based on importance. The features given the most weightage during prediction are considered more ""important"". The features at the top are the most important and the ones at the bottom are the least important for this LightGBM model.']",diagram rank featur base import featur given weightag predict consid import featur top import one bottom least import lightgbm model
2166,['## Neural Networks'],neural network
2167,['### Build Neural Network model'],build neural network model
2168,"['This is a simple neural network model that consists of two hidden layers with 10 neurons each. The activation function used is ReLU and the activation function for the last layer is sigmoid because a probability between 0 and 1 needs to given as output.\n', '\n', '**Steps:**\n', '\n', '* Pass the input vector through a dense layer with 10 neurons\n', '* Pass the output of that layer through another dense layer with 10 neurons\n', '* Finally, pass the output of the previous layer through a dense layer with one neuron with a sigmoid activation function (to output a probability between 0 and 1)']",simpl neural network model consist two hidden layer 10 neuron activ function use relu activ function last layer sigmoid probabl 0 1 need given output step pas input vector den layer 10 neuron pas output layer anoth den layer 10 neuron final pas output previou layer den layer one neuron sigmoid activ function output probabl 0 1
2169,['### Check model summary'],check model summari
2170,['### Visualize the model architecture'],visual model architectur
2171,['### Train the Neural Network'],train neural network
2172,"['**This trained model can be used to make predictions on the training data using:**\n', '\n', '> model.predict(X_test)']",train model use make predict train data use model predict x test
2173,['### Visualize change in accuracy'],visual chang accuraci
2174,['It can be clearly seen from the above plot that the training and validation accuracies are increasing as the training process proceeds.'],clearli seen plot train valid accuraci increas train process proce
2175,['### Visualize change in loss'],visual chang loss
2176,['It can be clearly seen from the above plot that the training and validation losses (binary cross-entropy) are decreasing as the training process proceeds.'],clearli seen plot train valid loss binari cross entropi decreas train process proce
2177,['# Ending note'],end note
2178,"['This concludes my EDA and modeling kernel. I hope that this kernel was helpful for you to understand the data better. Please post any feedback or comments regarding this kernel below in the comments section.\n', '\n', ""**Thanks for reading this kernel and please don't forget to upvote if you liked it :)**\n"", '\n']",conclud eda model kernel hope kernel help understand data better plea post feedback comment regard kernel comment section thank read kernel plea forget upvot like
2179,['# Introduction'],introduct
2180,"['<center><img src=""https://i.imgur.com/Hh0CekB.jpg"" width=""500px""></center> ']",center img src http imgur com hh0cekb jpg width 500px center
2181,"['Many participants in Kaggle competitions often try to increase their scores on the public leaderboard (LB) again and again, so that they stay ahead in the race for a medal. Some of these participants succeed in their endeavour and manage to stay at the top of the public LB. But, suddenly, when the private LB is revealed, a lot of these ""table toppers"" drop to low positions on the new leaderboard and miss out on a medal. But, on the other hand, many people who were low on the public LB jump up to high positions on the private LB. This phenomenon is called a **shakeup**.\n', '\n', 'Shakeups are most often caused by something called **overfitting** in machine learning. So, in this kernel, I will be explaining the most effective methods to identify and reduce overfitting, so that you can ensure a high place on the private LB and possibly even win a medal! Additionally, I will  be suggesting articles, blogs, and videos that explain these methods in greater detail.\n', '\n']",mani particip kaggl competit often tri increas score public leaderboard lb stay ahead race medal particip succeed endeavour manag stay top public lb suddenli privat lb reveal lot tabl topper drop low posit new leaderboard miss medal hand mani peopl low public lb jump high posit privat lb phenomenon call shakeup shakeup often caus someth call overfit machin learn kernel explain effect method identifi reduc overfit ensur high place privat lb possibl even win medal addit suggest articl blog video explain method greater detail
2182,"['<font size=""3"" color=""red""> Please upvote this kernel if you like it. It motivates me to produce more quality content :) </font>']",font size 3 color red plea upvot kernel like motiv produc qualiti content font
2183,['# Acknowledgements'],acknowledg
2184,['* [@tearth](https://www.kaggle.com/tearth) for the meme above :)'],tearth http www kaggl com tearth meme
2185,"['# Import libraries and prepare the data\n', '\n', '#### (Click CODE on the right side)']",import librari prepar data click code right side
2186,['# What is overfitting?'],overfit
2187,"['<center><img src=""https://i.imgur.com/dJVkotI.jpg"" width=""500px""></center>']",center img src http imgur com djvkoti jpg width 500px center
2188,"['Overfitting is the tendency of a machine learning model to perform really well on the known training data, and then perform significantly worse on the testing data. **This happens when the model memorizes the data instead of understanding the hidden patterns in the data.** This means that the model learns the training data so precisely, that it loses the ability to adapt and use its knowledge on new data.\n', '\n', 'This is one form of overfitting, but the main type of overfitting that causes shakeups is **test data overfitting**. This happens when the model performs very well on the public test data, but underperforms on the private test data. This is a result of **public LB probing**, where the participant tries to increase the the public LB score as much as possible, but the model performs poorly on the private LB because it is essentially memorizing the public test data.\n', '\n', 'On the other hand, underfitting is when the model is not even able to fit the training data properly. Therefore, both the training and testing accuracis are low in the case of underfitting.']",overfit tendenc machin learn model perform realli well known train data perform significantli wors test data happen model memor data instead understand hidden pattern data mean model learn train data precis lose abil adapt use knowledg new data one form overfit main type overfit caus shakeup test data overfit happen model perform well public test data underperform privat test data result public lb probe particip tri increas public lb score much possibl model perform poorli privat lb essenti memor public test data hand underfit model even abl fit train data properli therefor train test accuraci low case underfit
2189,['## A video explaining overfitting'],video explain overfit
2190,['Here is a video by Udacity explaining what overfitting is:'],video udac explain overfit
2191,['## Resources for understanding overfitting'],resourc understand overfit
2192,"['* [Overfitting and Underfitting With Machine Learning Algorithms](https://machinelearningmastery.com/overfitting-and-underfitting-with-machine-learning-algorithms/) ~ by Machine Learning Mastery\n', '* [Overfitting vs. Underfitting](https://towardsdatascience.com/overfitting-vs-underfitting-a-complete-example-d05dd7e19765) ~ by Will Koehrsen\n', '* [Overfitting vs. Underfitting: A Conceptual Explanation](https://towardsdatascience.com/overfitting-vs-underfitting-a-conceptual-explanation-d94ee20ca7f9) ~ by Will Koehrsen']",overfit underfit machin learn algorithm http machinelearningmasteri com overfit underfit machin learn algorithm machin learn masteri overfit v underfit http towardsdatasci com overfit v underfit complet exampl d05dd7e19765 koehrsen overfit v underfit conceptu explan http towardsdatasci com overfit v underfit conceptu explan d94ee20ca7f9 koehrsen
2193,['### Takeaways from this section'],takeaway section
2194,"[""* Overfitting is when a model's accuracy is significantly higher on the training data than the testing data.\n"", '* Overfitting can also happen when one fits the private test data significantly worse than the public test data.\n', '* When a model overfits, it tends to memorize the data instead of understanding patterns in the data.']",overfit model accuraci significantli higher train data test data overfit also happen one fit privat test data significantli wors public test data model overfit tend memor data instead understand pattern data
2195,['# How can we identify overfitting?'],identifi overfit
2196,"['Overfitting is a fairly simple concept to understand, but understanding how to identify and correct overfitting are very difficult tasks. A data scientist can get better at these things only by continuous practice. But, I can give some tips regarding how you can identify if your model is overfitting or not.']",overfit fairli simpl concept understand understand identifi correct overfit difficult task data scientist get better thing continu practic give tip regard identifi model overfit
2197,['## Validation'],valid
2198,"[""Validation is a way of checking how your model performs on new, unseen data. In validation, one samples a small percentage of the data points in the training data (usually around 20%) and sets in aside. The remaining samples are used to train the model and the chosen samples are used to evaluate the model. After each iteration of training, the model's performance is evaluated on this validation data to check how the model performs on data that it has never seen before.\n"", '\n', 'Note that the validation samples need to be selected randomly so that the validation score actually reflects how well the model may do on unseen data (*i.e.* the public and private testing datasets).']",valid way check model perform new unseen data valid one sampl small percentag data point train data usual around 20 set asid remain sampl use train model chosen sampl use evalu model iter train model perform evalu valid data check model perform data never seen note valid sampl need select randomli valid score actual reflect well model may unseen data e public privat test dataset
2199,['### Demonstration'],demonstr
2200,"['Now, I will demonstrate how you can add validation to a neural network model.']",demonstr add valid neural network model
2201,['### Build neural network'],build neural network
2202,['### Visualize the model architecture'],visual model architectur
2203,['### Split the data into training and validation sets'],split data train valid set
2204,['### Train the model'],train model
2205,['### Plot the loss and accuracy statistics '],plot loss accuraci statist
2206,['### Accuracy'],accuraci
2207,"['In the plot above, we can see how the validation and training accuracies are changing. The training accuracy ends up just above the validation accuracy at the last epoch. This shows that the model is learning well because the gap between the training and validation accuracies is small. This means that the data performs at almost the same level on unseen data. In this way, we can use validation to check whether our model is overfitting or not.']",plot see valid train accuraci chang train accuraci end valid accuraci last epoch show model learn well gap train valid accuraci small mean data perform almost level unseen data way use valid check whether model overfit
2208,"['We can see the same insight in the bar plot above. The training accuracy is initially lower than the validation accuracy, but it eventually catches up, and ends up just above the validation accuracy at the end. This is a sign of a model that is not overfitting (training accuracy only just above validation accuracy).']",see insight bar plot train accuraci initi lower valid accuraci eventu catch end valid accuraci end sign model overfit train accuraci valid accuraci
2209,['### Loss'],loss
2210,"['In the plot above, we can see how the validation and training losses are changing. The losses end up in close proximity towards the last few epochs. This shows that the training and validation losses are both decreasing together in sync with each other. Once again, it is an indication that there is no clear overfitting.']",plot see valid train loss chang loss end close proxim toward last epoch show train valid loss decreas togeth sync indic clear overfit
2211,['We can observe the same insight from the bar plot above. The losses get closer and closer to each other as the epochs increase. This is a sign of healthy training without overfitting.'],observ insight bar plot loss get closer closer epoch increas sign healthi train without overfit
2212,['### Cross validation'],cross valid
2213,"['Cross validation is a special type of validation that makes use of the entire training data to evaluate the model properly, so that as much randomness as possible can be introduced. In cross validation, the training dataset is divided into a certain number of **folds**. These folds are subsets of the original training data. Each fold has an equal number of randomly selected data samples. \n', '\n', 'One fold is selected and set aside. The remaining folds are used to train the model, and the selected fold is used to evaluate the performance of the model at each iteration. Now, the next fold is selected and the same process repeats. We repeat this process until each fold has acted as a validation set once. \n', '\n', 'This process is called **cross-validation**. Cross validation is helpful because the entire training dataset is being used to validate the model. This means that we are introducing more randomness and variety into our validation set instead of restricting ourselves to one fixed validation set. **This gives a more reliable idea of how the model would perform on real testing data.**']",cross valid special type valid make use entir train data evalu model properli much random possibl introduc cross valid train dataset divid certain number fold fold subset origin train data fold equal number randomli select data sampl one fold select set asid remain fold use train model select fold use evalu perform model iter next fold select process repeat repeat process fold act valid set process call cross valid cross valid help entir train dataset use valid model mean introduc random varieti valid set instead restrict one fix valid set give reliabl idea model would perform real test data
2214,['### Types of cross validation'],type cross valid
2215,['There are many types of cross validation that are used for different types of data. Here are a few common types of cross validation:'],mani type cross valid use differ type data common type cross valid
2216,"['* **Time-series split** : For data with a time dimension\n', '* **Stratified K-Fold** : For data with a categorical target (like this competition)\n', '* **K-Fold** : For data with a continuous target']",time seri split data time dimens stratifi k fold data categor target like competit k fold data continu target
2217,"['Cross validation is a very good way to diagnose overfitting. In fact, the gap between your cross validation score and public LB score can be used as a way to determine whether your model is overfitting or not. So, **looking at only the public LB score or only the cross validation score may not be that useful, but looking at them together will give a clearer picture**.\n', '\n', 'Let me illustrate this point with a simple example. Assume that the statistics given below are the accuracy scores for two different models. The first row corresponds to the first and the second one to the second model.']",cross valid good way diagnos overfit fact gap cross valid score public lb score use way determin whether model overfit look public lb score cross valid score may use look togeth give clearer pictur let illustr point simpl exampl assum statist given accuraci score two differ model first row correspond first second one second model
2218,"['|        | Public LB Score   |  Cross Validation Score    |\n', '| ------ | ------ |------|\n', '|Model 1 | 97 %  |  97.5 %    |\n', '| Model 2 | 97.5 % | 99 %']",public lb score cross valid score model 1 97 97 5 model 2 97 5 99
2219,"['From the above table of scores, it may look like model 2 is a better choice for private LB submission because it has a higher CV and public LB score as compared to model 2. But, this is actually a misleading illusion.\n', '\n', 'If you look at the numbers closely, the error percentage in model 1 is 2.5% for cross validation and 3% for public LB. These error percentages are somewhat similar. But, if you look at the error percentages in model 2, they are 1% for cross validation and 2.5% for public LB. This is a comparatively massive gap between the CV and public LB scores. The public LB error percentage for model 2 is 2.5 times larger than the cross validation error. This shows that model 2 is overfitting to the cross validation data, and is not improving much on its test data performance. So, model 1 is more likely to perform better on a new private test dataset because it has very good CV and LB accuracies, and at the same, the gap between CV and LB is not very large. To summarize, **never go for a very small increase in LB score when the increase in CV score is very large, because this may lead to overfitting**.\n', '\n', 'So, look for an increase in CV score, but make sure that the gap between CV and LB remains small. This idea is excellently explained by [@cpmpml](https://kaggle.com/cpmpml) in his talk at Kaggle days:']",tabl score may look like model 2 better choic privat lb submiss higher cv public lb score compar model 2 actual mislead illus look number close error percentag model 1 2 5 cross valid 3 public lb error percentag somewhat similar look error percentag model 2 1 cross valid 2 5 public lb compar massiv gap cv public lb score public lb error percentag model 2 2 5 time larger cross valid error show model 2 overfit cross valid data improv much test data perform model 1 like perform better new privat test dataset good cv lb accuraci gap cv lb larg summar never go small increas lb score increas cv score larg may lead overfit look increas cv score make sure gap cv lb remain small idea excel explain cpmpml http kaggl com cpmpml talk kaggl day
2220,['### Resources for understanding validation'],resourc understand valid
2221,"['* [About Train, Validation and Test Sets in Machine Learning](https://towardsdatascience.com/train-validation-and-test-sets-72cb40cba9e7) ~ by Tarang Shah\n', '* [What is the Difference Between Test and Validation Datasets?](https://machinelearningmastery.com/difference-test-validation-datasets/) ~ by Machine Learning Mastery\n', '* [Cross-Validation in Machine Learning](https://towardsdatascience.com/cross-validation-in-machine-learning-72924a69872f) ~ by Prashant Gupta\n', '* [A Gentle Introduction to k-fold Cross-Validation](https://machinelearningmastery.com/k-fold-cross-validation/) ~ by Machine Learning Mastery']",train valid test set machin learn http towardsdatasci com train valid test set 72cb40cba9e7 tarang shah differ test valid dataset http machinelearningmasteri com differ test valid dataset machin learn masteri cross valid machin learn http towardsdatasci com cross valid machin learn 72924a69872f prashant gupta gentl introduct k fold cross valid http machinelearningmasteri com k fold cross valid machin learn masteri
2222,['### Takeaways from this section'],takeaway section
2223,"['* Always create a robust and reliable validation system for your models.\n', '* When choosing which models to submit to the private LB, look at the CV and public LB scores together.\n', '* The gap between CV and public LB scores should be as small as possible, and never go far a change that shows a huge improvement in CV, but a relatively small improvement in public LB.']",alway creat robust reliabl valid system model choos model submit privat lb look cv public lb score togeth gap cv public lb score small possibl never go far chang show huge improv cv rel small improv public lb
2224,['# How to reduce overfitting?'],reduc overfit
2225,"['Now, because we understand how validation can be used to detect overfitting, the next step is to understand how to fix overfitting. There are many ways to reduce overfitting, but the most most common ones are as follows:']",understand valid use detect overfit next step understand fix overfit mani way reduc overfit common one follow
2226,"['* **Reducing model complexity**\n', '* **Ensembling**\n', '* **Regularization**']",reduc model complex ensembl regular
2227,"['I will look at these differrent methods one-by-one, explain them, and then demonstrate how they work with a simple neural network model.']",look differr method one one explain demonstr work simpl neural network model
2228,['## Reducing model complexity'],reduc model complex
2229,"['In the above diagram, the data can be easily modeled using a simple linear function. This is because the general trend in the data seems to be linear. But, instead, a highe degree polynomial is used and the data is fit perfectly without any error. This model may achieve 100% accuracy on the training data, but it will surely fail badly when tested on new data. This is because the model has **fitted the noise** in the training data, and in the process, it memorizes the data instead of finding the required pattern. Therefore, an overly complex model may lead to overfitting. But, at the same time, an overly simple model may lead to underfitting.\n', '\n', 'So, when a model starts overfitting, one way to reduce the overfitting is to reduce the complexity of the model. In a neural network, this can be done by reducing the number of layers in the network or reducing the number of neurons in the each layer. This can also be done for a gradient boosting model like LightGBM by reducing paramters like *max_depth*. By doing this, we are intentionally, reducing the complexity of the model in order to prevent overfitting from taking place.']",diagram data easili model use simpl linear function gener trend data seem linear instead high degre polynomi use data fit perfectli without error model may achiev 100 accuraci train data sure fail badli test new data model fit nois train data process memor data instead find requir pattern therefor overli complex model may lead overfit time overli simpl model may lead underfit model start overfit one way reduc overfit reduc complex model neural network done reduc number layer network reduc number neuron layer also done gradient boost model like lightgbm reduc paramt like max depth intent reduc complex model order prevent overfit take place
2230,"['<center><img src=""https://i.imgur.com/k6ZQvak.png"" width=""800px""></center>']",center img src http imgur com k6zqvak png width 800px center
2231,"['The idea that reducing model complexity can reduce overfitting can be understood using the concepts of **variance and bias**.\n', '\n', '**Variance** is the amount that the estimate of the model will change if different training data was used. This quantity measures the capacity of a model to fit different datasets, and is therefore also a measure of the complexity of the model. A high variance means that the model has a very complex target function and is able to fit the training data very accurately. On the other hand, a low variance means that the model has a low level of complexity and is therefore unable to fit the training very closely. It may look like the more the variance the better, but in fact, the variance of any model should be moderate, and the ""moderate"" level of variance depends on the nature of the data. Some datasets demand models with a high variance whereas other datasets require a much lower level of variance. For example, the MNIST dataset can be modeled easily using a simple CNN, but a self-driving car dataset involving complex video and sensor data may require a more complex (high variance) model.\n', '\n', 'This brings me to the concept of bias. **Bias** are the simplifying assumptions made by a model to make the target function easier to learn.  This quantity measures the simplicity of the model, because simpler models often tend to make more assumptions to fit the data. A high bias means that the model has a simple target function and is therefore unable to fit the training very closely. On the other hand, a low bias means that the model has a high level of complexity and is therefore able to fit the training very closely.\n', '\n', '**Models with a high variance tend to have a low bias and vice-versa, and therefore, there is a direct tradeoff between them**. Choosing the correct balance of variance and bias (for a particular dataset) is the key to avoiding overfitting.']",idea reduc model complex reduc overfit understood use concept varianc bia varianc amount estim model chang differ train data use quantiti measur capac model fit differ dataset therefor also measur complex model high varianc mean model complex target function abl fit train data accur hand low varianc mean model low level complex therefor unabl fit train close may look like varianc better fact varianc model moder moder level varianc depend natur data dataset demand model high varianc wherea dataset requir much lower level varianc exampl mnist dataset model easili use simpl cnn self drive car dataset involv complex video sensor data may requir complex high varianc model bring concept bia bia simplifi assumpt made model make target function easier learn quantiti measur simplic model simpler model often tend make assumpt fit data high bia mean model simpl target function therefor unabl fit train close hand low bia mean model high level complex therefor abl fit train close model high varianc tend low bia vice versa therefor direct tradeoff choos correct balanc varianc bia particular dataset key avoid overfit
2232,['### Demonstration'],demonstr
2233,"['Now, I will demonstrate how the reducing the model complexity may help prevent a model from overfitting. Once again, I will use neural network models to demonstrate this idea. Specifically, I will build and train a very complex neural network model (with several neurons). Then, I will show how reducing the complexity of the model (by removing neurons) can help reduce overfitting in the model.']",demonstr reduc model complex may help prevent model overfit use neural network model demonstr idea specif build train complex neural network model sever neuron show reduc complex model remov neuron help reduc overfit model
2234,['### Build model #1'],build model 1
2235,"['First, we will build a model with two hidden layers (100 neurons each) and a single-neuron layer with a sigmoid activation at the end to output the probability.']",first build model two hidden layer 100 neuron singl neuron layer sigmoid activ end output probabl
2236,['### Visualize the model architecture'],visual model architectur
2237,['### Train the model and store training history'],train model store train histori
2238,['### Visualize the training and validation statistics for model #1'],visual train valid statist model 1
2239,['### Accuracy'],accuraci
2240,"['In the above plot, we can see that the training and validation accuracies for the 100-neuron model diverge towards the last few epochs. The training accuracy ends up significantly higher than the validation accuracy. This indicates that the model is not able to replicate its training performance when faced with the unseen validation data. This is a clear sign of overfitting.']",plot see train valid accuraci 100 neuron model diverg toward last epoch train accuraci end significantli higher valid accuraci indic model abl replic train perform face unseen valid data clear sign overfit
2241,"['We can observe the same fact from the bar plot above. The training and validation accuracies diverge towards the end, with the training accuracy ending up significantly higher than the validation accuracy. This is, once again, a clear indication of overfitting.']",observ fact bar plot train valid accuraci diverg toward end train accuraci end significantli higher valid accuraci clear indic overfit
2242,['### Loss'],loss
2243,"['In the above plot, we can see that the validation loss spikes clearly higher than the training loss towards the end of the training process. This indicates that the model is overfitting, as it is unable to replicate its training performance on the unseen validation data.']",plot see valid loss spike clearli higher train loss toward end train process indic model overfit unabl replic train perform unseen valid data
2244,"['We can once again observe the same insight from the above bar plot. Towards the last few epochs, the validation loss rises clearly higher above the training loss. Once, again, this indicates that the model is overfitting.']",observ insight bar plot toward last epoch valid loss rise clearli higher train loss indic model overfit
2245,"['In general, we can conclude that this 100-neuron model (model #1) is overfitting. Now, let us decrease the complexity of the model by decreasing the number of neurons in each hidden layer. This should help reduce the overfitting.']",gener conclud 100 neuron model model 1 overfit let u decreas complex model decreas number neuron hidden layer help reduc overfit
2246,['### Build model #2'],build model 2
2247,"['Now, we build another model. The architecture is the same as last time, but this time, there are 10 neurons in each hidden layer instead of 100.']",build anoth model architectur last time time 10 neuron hidden layer instead 100
2248,['### Visualize the model architecture'],visual model architectur
2249,['### Train the model and store training history'],train model store train histori
2250,['### Visualize the training and validation statistics for model #2'],visual train valid statist model 2
2251,['### Accuracy'],accuraci
2252,"['In the above plot, we can see that the training accuracy rises above the validation accuracy towards the end of the training process. But, the gap between the training and validation accuracies is very small compared to the 100-neuron model. This indicates that the model is training properly and is able to successfully avoid overfitting.']",plot see train accuraci rise valid accuraci toward end train process gap train valid accuraci small compar 100 neuron model indic model train properli abl success avoid overfit
2253,"['In the bar plot above, we can observe the same insight. The training accuracy ends up just above the validation accuracy at the end, but the gap between the two accuracies is significantly smaller than it was in the case of the 100-neuron model.']",bar plot observ insight train accuraci end valid accuraci end gap two accuraci significantli smaller case 100 neuron model
2254,['### Loss'],loss
2255,"['In the above plot, we can see that the training loss ends up below the validation loss towards the end of the training process. But, the gap between the two losses is much smaller compared to the 100-neuron model. This shows that the model is learning properly. It is not overfitting.']",plot see train loss end valid loss toward end train process gap two loss much smaller compar 100 neuron model show model learn properli overfit
2256,"['We can observe the same insight from the bar plot above. The validation loss ends up just above the training loss at the end of the training process, but the gap is much smaller compared to the 100-neuron model. This once again indicates that the model is not overfitting.']",observ insight bar plot valid loss end train loss end train process gap much smaller compar 100 neuron model indic model overfit
2257,['### Make predictions on training and validation data from the models '],make predict train valid data model
2258,['### Visualize training and validation accuracy for both the models'],visual train valid accuraci model
2259,"['We can see from the above plot that the training accuracy ends up higher than the validation accuracy in both cases. But, in the case of the second model, the gap is much smaller. Thus, we can conclude that we have been able to reduce overfitting by reducing the complexity of the model.']",see plot train accuraci end higher valid accuraci case case second model gap much smaller thu conclud abl reduc overfit reduc complex model
2260,['## Ensembling'],ensembl
2261,"['Ensembling is a very useful and common method used to reduce overfitting. In ensembling, the outputs from several models are combined (most commonly by averaging) to form one prediction vector. This final prediction usually has a greater accuracy than the other individual prediction vectors. This is because, when the predictions from different models are combined by averaging, the errors in the different prediction vectors are partially ""canceled out"" or reduced in magnitude. But, note that **this works only when the correlation between the different prediction vectors is considerably low, otherwise the ensemble would score similarly to the individual models**.\n', '\n', 'This implies that the **variety in the models is very important**. This means that one must ensemble models which are very different from each other in terms of architecture, hyperparameters, or algorithm in order to achieve maximum benefits from ensembling. For example, it might be a better idea to ensemble a LightGBM model with a Neural Network model instead of ensembling two LightGBM models. When more diverse models are selected, the diversity in predictions is also greater, and therefore, there is a higher chance of the ensemble to effectively reduce the errors in the predictions by ""canceling them out"".']",ensembl use common method use reduc overfit ensembl output sever model combin commonli averag form one predict vector final predict usual greater accuraci individu predict vector predict differ model combin averag error differ predict vector partial cancel reduc magnitud note work correl differ predict vector consider low otherwis ensembl would score similarli individu model impli varieti model import mean one must ensembl model differ term architectur hyperparamet algorithm order achiev maximum benefit ensembl exampl might better idea ensembl lightgbm model neural network model instead ensembl two lightgbm model diver model select diver predict also greater therefor higher chanc ensembl effect reduc error predict cancel
2262,['### A video explaining ensembling'],video explain ensembl
2263,['Here is another video by Udacity explaining what ensembling is:'],anoth video udac explain ensembl
2264,['### Demonstration'],demonstr
2265,"['Now, I will demonstrate how ensembling can be used to help a model generalize better and avoid overfitting. I will build and train two separate neural network models, and then show how ensembling them can increase the cross validation score.']",demonstr ensembl use help model gener better avoid overfit build train two separ neural network model show ensembl increas cross valid score
2266,['### Build model #1'],build model 1
2267,"['First, we will build a model with two hidden layers (20 neurons each) and a single-neuron layer with a sigmoid activation at the end to output the probability.']",first build model two hidden layer 20 neuron singl neuron layer sigmoid activ end output probabl
2268,['### Visualize model architecture'],visual model architectur
2269,['### Train the model and store training history'],train model store train histori
2270,['### Visualize the training and validation statistics for model #1'],visual train valid statist model 1
2271,['### Accuracy'],accuraci
2272,"['In the above plot, we can see that the training and validation accuracies for the 10-neuron model slightly diverge towards the last few epochs. The training accuracy ends up slightly higher than the validation accuracy. This indicates that the model is overfitting, but not as much as the previous 100-neuron model.']",plot see train valid accuraci 10 neuron model slightli diverg toward last epoch train accuraci end slightli higher valid accuraci indic model overfit much previou 100 neuron model
2273,"['We can see the same insight from the above bar plot. The training accuracy ends up slightly higher than the validation accuracy towards the last few epochs. This indicates that the model is overfitting, but only slightly.']",see insight bar plot train accuraci end slightli higher valid accuraci toward last epoch indic model overfit slightli
2274,['### Loss'],loss
2275,"['In the above plot, we can see that the validation loss ends up slightly higher than the training loss towards the end. This once again shows that the model is overfitting, but not very heavily.']",plot see valid loss end slightli higher train loss toward end show model overfit heavili
2276,"['We can see the same insight from the above bar plot. The training accuracy ends up slightly higher than the validation accuracy towards the last few epochs. This indicates that the model is overfitting, but only slightly.']",see insight bar plot train accuraci end slightli higher valid accuraci toward last epoch indic model overfit slightli
2277,"['We can conclude that this model is overfitting only slightly. So, now I will build another model with 25 neurons instead of 10 neurons in each layer.']",conclud model overfit slightli build anoth model 25 neuron instead 10 neuron layer
2278,['### Build model #2'],build model 2
2279,"['Now, we will build a new model with the same architecture, but with 25 (instead of 20) neurons per hidden layer, and a single-neuron layer with a sigmoid activation at the end to output the probability.']",build new model architectur 25 instead 20 neuron per hidden layer singl neuron layer sigmoid activ end output probabl
2280,['### Visualize the model architecture'],visual model architectur
2281,['### Train the model and store training history'],train model store train histori
2282,['### Visualize the training and validation statistics for model #2'],visual train valid statist model 2
2283,['### Accuracy'],accuraci
2284,"['In the above plot, we can see that the training and validation accuracies for the 25-neuron model are similar towards the last few epochs. The training accuracy ends up only slightly higher than the validation accuracy. This indicates that the model is able to replicate its training performance when faced with the unseen validation data. This is not overfitting.']",plot see train valid accuraci 25 neuron model similar toward last epoch train accuraci end slightli higher valid accuraci indic model abl replic train perform face unseen valid data overfit
2285,"['We can observe the same fact from the bar plot above. The training and validation accuracies diverge by a very small margin, with the training accuracy ending up only slightly higher than the validation accuracy. This is, once again, an indication of no overfitting.']",observ fact bar plot train valid accuraci diverg small margin train accuraci end slightli higher valid accuraci indic overfit
2286,['### Loss'],loss
2287,"['In the above plot, we can see that the validation loss ends up just above the training loss, but since the gap is very small, we can say that the model is not overfitting.']",plot see valid loss end train loss sinc gap small say model overfit
2288,"['We can once again observe the same insight from the above bar plot. Towards the last few epochs, the validation loss ends up near the training loss. Once, again, this indicates that the model is not overfitting.']",observ insight bar plot toward last epoch valid loss end near train loss indic model overfit
2289,"['In general, we can conclude that this 25-neuron model (model #2) is now overfitting. Now, let us ensemble the two models and see what happens.']",gener conclud 25 neuron model model 2 overfit let u ensembl two model see happen
2290,['### Ensembling the two models'],ensembl two model
2291,"['Now, I will ensemble the two models by averaging their predictions on the validation data.']",ensembl two model averag predict valid data
2292,['I give a higher weightage (75 %) to the first model than the second model (25 %) because the second model has a better CV score than the first model. **The model weightages in the ensemble can be decided based on the individual cross validation scores**.'],give higher weightag 75 first model second model 25 second model better cv score first model model weightag ensembl decid base individu cross valid score
2293,"['### Check the accuracy values for model #1, model #2, and their ensemble']",check accuraci valu model 1 model 2 ensembl
2294,"[""The accuracy of models #1 and #2 are both very high. Therefore, the ensemble's accuracy was higher than the accuracies of the two models. Therefore, **ensembling is effective only when several models with strong generalization are combined together**. Even one weak model can bring down the accuracy of the ensemble by a lot. ""]",accuraci model 1 2 high therefor ensembl accuraci higher accuraci two model therefor ensembl effect sever model strong gener combin togeth even one weak model bring accuraci ensembl lot
2295,"['### Visualize the accuracy for model #1, model #2, and their ensemble']",visual accuraci model 1 model 2 ensembl
2296,['### Difference of the accuracies of the models above 96.9 %'],differ accuraci model 96 9
2297,['We can once again see from the above plot that the ensemble accuracy is greater than the individual accuracies of model #1 and model #2.'],see plot ensembl accuraci greater individu accuraci model 1 model 2
2298,['### Resources for understading ensembling'],resourc understad ensembl
2299,"['* [A Comprehensive Guide to Ensemble Learning (with Python codes)](https://www.analyticsvidhya.com/blog/2018/06/comprehensive-guide-for-ensemble-models/) ~ by Aishwarya Singh\n', '* [Ensemble Learning Methods for Deep Learning Neural Networks](https://machinelearningmastery.com/ensemble-methods-for-deep-learning-neural-networks/) ~ by Machine Learning Mastery']",comprehens guid ensembl learn python code http www analyticsvidhya com blog 2018 06 comprehens guid ensembl model aishwarya singh ensembl learn method deep learn neural network http machinelearningmasteri com ensembl method deep learn neural network machin learn masteri
2300,['## Regularization'],regular
2301,"['Regularization is another great way to prevent models from overfitting. **Regularization is a group of methods aimed at improving the generalization of models**. Basically, regularization methods ensure that the model maintains the same performance level on the validation data (and testing data) as it does on the training data.\n', '\n', 'Regularization can take on many different types and forms based on the data and model under consideration. For example, a method called [Dropout](https://medium.com/@amarbudhiraja/https-medium-com-amarbudhiraja-learning-less-to-learn-better-dropout-in-deep-machine-learning-74334da4bfc5) can be used to improve the generalization of neural networks. This is a form of regularization. Similarly, one can also use the [L1 and L2 methods](https://towardsdatascience.com/l1-and-l2-regularization-methods-ce25e7fc831c) to reduce overfitting in a neural network. Once, this is a form of regularization.\n', '\n', 'Dropout is a regularization method that approximates training a large number of neural networks with different architectures in parallel.\n', '\n', 'During training, some number of layer outputs are randomly ignored or “dropped out.” This has the effect of making the layer look-like and be treated-like a layer with a different number of nodes and connectivity to the prior layer. In effect, each update to a layer during training is performed with a different “view” of the configured layer.\n', '\n', 'The probablity of dropping a random neuron is called the dropout rate. For the demonstration below, I will use a dropout rate 0f 0.2.']",regular anoth great way prevent model overfit regular group method aim improv gener model basic regular method ensur model maintain perform level valid data test data train data regular take mani differ type form base data model consider exampl method call dropout http medium com amarbudhiraja http medium com amarbudhiraja learn le learn better dropout deep machin learn 74334da4bfc5 use improv gener neural network form regular similarli one also use l1 l2 method http towardsdatasci com l1 l2 regular method ce25e7fc831c reduc overfit neural network form regular dropout regular method approxim train larg number neural network differ architectur parallel train number layer output randomli ignor drop effect make layer look like treat like layer differ number node connect prior layer effect updat layer train perform differ view configur layer probabl drop random neuron call dropout rate demonstr use dropout rate 0f 0 2
2302,"['<center><img src=""https://i.imgur.com/qFmZI6M.png"" width=""600px""></center>']",center img src http imgur com qfmzi6m png width 600px center
2303,['### A video explaining regularization'],video explain regular
2304,['Here is a video of Andrew Ng explaining the idea of regularization and how it can be used to combat overfitting:'],video andrew ng explain idea regular use combat overfit
2305,['### Demonstration'],demonstr
2306,"['Regularization can be done in many ways, but for this kernel, I will demonstrating how **dropout** can be used to regularize neural networks.']",regular done mani way kernel demonstr dropout use regular neural network
2307,['### Build model #1 (without dropout)'],build model 1 without dropout
2308,"['First, we will build a model with two hidden layers (200 neurons each) and a single-neuron layer with a sigmoid activation at the end to output the probability.']",first build model two hidden layer 200 neuron singl neuron layer sigmoid activ end output probabl
2309,['### Visualize model architecture'],visual model architectur
2310,['### Train the model and store training history'],train model store train histori
2311,['### Visualize the training and validation statistics for model #1'],visual train valid statist model 1
2312,['### Accuracy'],accuraci
2313,"['In the above plot, we can see that the training and validation accuracies for the 200-neuron model diverge towards the last few epochs. The training accuracy ends up significantly higher than the validation accuracy. This indicates that the model is not able to replicate its training performance when faced with the unseen validation data. This is a clear sign of heavy overfitting.']",plot see train valid accuraci 200 neuron model diverg toward last epoch train accuraci end significantli higher valid accuraci indic model abl replic train perform face unseen valid data clear sign heavi overfit
2314,"['We can once again observe the same insight from the above bar plot. Towards the last few epochs, the validation loss rises significantly higher above the training loss. Once, again, this indicates that the model is heavily overfitting.']",observ insight bar plot toward last epoch valid loss rise significantli higher train loss indic model heavili overfit
2315,['### Loss'],loss
2316,"['In the above plot, we can see that the validation loss spikes clearly higher than the training loss towards the end of the training process. This indicates that the model is overfitting, as it is unable to replicate its training performance on the unseen validation data. This time, the overfitting seems to be significant.']",plot see valid loss spike clearli higher train loss toward end train process indic model overfit unabl replic train perform unseen valid data time overfit seem signific
2317,"['We can once again observe the same insight from the above bar plot. Towards the last few epochs, the validation loss rises clearly higher above the training loss. Once, again, this indicates that the model is overfitting.']",observ insight bar plot toward last epoch valid loss rise clearli higher train loss indic model overfit
2318,['### Build model #2 (with dropout)'],build model 2 dropout
2319,"['Now, we will build a new model with the same architecture, **but with a dropout layer after each hidden layer**, and a single-neuron layer with a sigmoid activation at the end to output the probability.']",build new model architectur dropout layer hidden layer singl neuron layer sigmoid activ end output probabl
2320,['### Visualize the model architecture'],visual model architectur
2321,['### Train the model and store training history'],train model store train histori
2322,['### Visualize the training and validation statistics for model #1'],visual train valid statist model 1
2323,['### Accuracy'],accuraci
2324,"['In the above plot, we can see that the validation accuracy rises above the training accuracy towards the end of the training process. But, the gap between the training and validation accuracies is very small. This indicates that the model is training properly and is able to successfully avoid overfitting.']",plot see valid accuraci rise train accuraci toward end train process gap train valid accuraci small indic model train properli abl success avoid overfit
2325,"['In the bar plot above, we can observe the same insight. The validation accuracy ends up just above the training accuracy at the end, but the gap between the two accuracies is significantly smaller than it was in the case of the no-dropout model.']",bar plot observ insight valid accuraci end train accuraci end gap two accuraci significantli smaller case dropout model
2326,['### Loss'],loss
2327,"['In the above plot, we can see that the validation loss ends up below the training loss towards the end of the training process. But, the gap between the two losses is much smaller compared to the no-dropout model. This shows that the model is learning properly. It is not overfitting.']",plot see valid loss end train loss toward end train process gap two loss much smaller compar dropout model show model learn properli overfit
2328,"['We can observe the same insight from the bar plot above. The validation loss ends up just below the training loss at the end of the training process, but the gap is much smaller compared to the no-dropout model. This once again indicates that the model is not overfitting.']",observ insight bar plot valid loss end train loss end train process gap much smaller compar dropout model indic model overfit
2329,"['We can easily conclude that adding dropout to the neural network acted as a regularizer. It helped reduce the gap between training and validation scores. Thus, adding dropout helped successfully remove overfitting from the model.']",easili conclud ad dropout neural network act regular help reduc gap train valid score thu ad dropout help success remov overfit model
2330,['### Make predictions on training and validation data from the models '],make predict train valid data model
2331,['### Visualize the training and validation accuracy for both the models'],visual train valid accuraci model
2332,"['We can see from the above plot that the training accuracy ends up higher than the validation accuracy in both cases. But, in the case of the second model, the gap is much smaller. Thus, we can conclude that we have been able to reduce overfitting by adding dropout to the model.']",see plot train accuraci end higher valid accuraci case case second model gap much smaller thu conclud abl reduc overfit ad dropout model
2333,['### Resources for understanding regularization'],resourc understand regular
2334,"['* [Dropout in (Deep) Machine Learning](https://medium.com/@amarbudhiraja/https-medium-com-amarbudhiraja-learning-less-to-learn-better-dropout-in-deep-machine-learning-74334da4bfc5) ~ by Amar Budhiraja\n', '* [L1 and L2 Regularization Methods](https://towardsdatascience.com/l1-and-l2-regularization-methods-ce25e7fc831c) ~ by Anuja Nagpal\n', '* [A Gentle Introduction to Dropout for Regularizing Deep Neural Networks](https://machinelearningmastery.com/dropout-for-regularizing-deep-neural-networks/) ~ by Machine Learning Mastery']",dropout deep machin learn http medium com amarbudhiraja http medium com amarbudhiraja learn le learn better dropout deep machin learn 74334da4bfc5 amar budhiraja l1 l2 regular method http towardsdatasci com l1 l2 regular method ce25e7fc831c anuja nagpal gentl introduct dropout regular deep neural network http machinelearningmasteri com dropout regular deep neural network machin learn masteri
2335,['# Conclusion'],conclus
2336,"['From this kernel, I hope you understand that you have a certain degree of control over your public LB standings and it is not completely random. So, using these tips and tricks, you can survive a shakeup and end up with a medal :)']",kernel hope understand certain degre control public lb stand complet random use tip trick surviv shakeup end medal
2337,['Thanks for reading this kernel. I hope you found it useful.'],thank read kernel hope found use
2338,"['<font size=""4"" color=""red""> Please upvote this kernel if you liked it. It motivates me to produce more quality content :) </font>']",font size 4 color red plea upvot kernel like motiv produc qualiti content font
2339,"[""# <font color='orange'>Introduction</font>\n"", ""**TransactionDT** is a column represents date and time of each transaction. Problem is, the values don't traditionally start from 1970/1/1 as usual but rather some random point in time. Figuring out when the start time is can help tremendously if one wants to use seasonality in their analyses and modeling.\n"", '\n', ""Let's find out""]",font color orang introduct font transactiondt column repres date time transact problem valu tradit start 1970 1 1 usual rather random point time figur start time help tremend one want use season analys model let find
2340,"[""# <font color='orange'>Overview</font>""]",font color orang overview font
2341,"[""<font color='orange'>*Notice the minimum value of TransactionDT is 86400 that happens to be the number of seconds of 1 day so we can assume that the unit of the column is second*</font>\n"", '\n', ""* **TransactionDT** data of train and test set are from different distribution as the graphs don't overlap.\n"", '* The data spreads across 396 days which is about 13 months\n', ""* Looking at the peaks at both ends, they're likely to be associated with a festive season, Black Friday or Christmas maybe""]",font color orang notic minimum valu transactiondt 86400 happen number second 1 day assum unit column second font transactiondt data train test set differ distribut graph overlap data spread across 396 day 13 month look peak end like associ festiv season black friday christma mayb
2342,['# Year'],year
2343,"[""### Let's see which devices made the first transactions ""]",let see devic made first transact
2344,"[""<font color='orange'>A quick check shows that the Samsung Galaxy S8 ***(SAMSUNG SM-G892A Build/NRD90M)*** was released in the US the earliest on 21st April 2017 (Wikipedia). Since there's transactions made by this phone the 2nd day of the data's time, this data apparently can only be as old as ***April of 2017***</font>""]",font color orang quick check show samsung galaxi s8 samsung sm g892a build nrd90m releas u earliest 21st april 2017 wikipedia sinc transact made phone 2nd day data time data appar old april 2017 font
2345,['# Date'],date
2346,"[""<font color='white'>The data spreads over 395 days which is about 13 months. Also there're 2 peaks at the ends of the period. \n"", ""1. And I heard Black Friday is the peak shopping season in the US, no? <br/><br/> Let's find out!</font>""]",font color white data spread 395 day 13 month also 2 peak end period 1 heard black friday peak shop season u br br let find font
2347,"[""<font color='white'>Looks like it's the case. Google Trends shows high traffic associated with Shopping related keywords during Black Friday season.</font>\n"", '\n', 'https://trends.google.com/trends/explore?cat=18&date=2017-04-01%202018-12-31&geo=US']",font color white look like case googl trend show high traffic associ shop relat keyword black friday season font http trend googl com trend explor cat 18 date 2017 04 01 202018 12 31 geo u
2348,['### How about plotting the TransactionDT day wise?'],plot transactiondt day wise
2349,"[""1. <font color='navy'>**Assuming we're right that the peaks are associated with the Black Friday season. \n"", ""    Let's see what are the dates we're looking at**</font>""]",1 font color navi assum right peak associ black friday season let see date look font
2350,"[""Since 2017's Black Friday was 24th November that happens to be tally with the numbers above. Let us just assume that the data begins on November 1st of 2017, Let's have a look at the 2nd peak in the test data""]",sinc 2017 black friday 24th novemb happen talli number let u assum data begin novemb 1st 2017 let look 2nd peak test data
2351,"[""<font color='navy'>**23rd November 2018 is Black Friday, and 26th November 2018 is Cyber Monday, how cool is that**</font>""]",font color navi 23rd novemb 2018 black friday 26th novemb 2018 cyber monday cool font
2352,['# Conclusion'],conclus
2353,"[""*Based on the patterns found within the data, together with insights provided by Google Trends, the data's start date is likely 2017/11/1*""]",base pattern found within data togeth insight provid googl trend data start date like 2017 11 1
2354,['### Installing LightGBM GPU build'],instal lightgbm gpu build
2355,"['So the data description tells us that, we can join the two datasets by the `TransactionID` column. However, not all transactions will have corresponding identity information.\n']",data descript tell u join two dataset transactionid column howev transact correspond ident inform
2356,"[""Let's check to see if we have the same levels (categories) in the training and testing set. If we have new categories is the test set, the model may not be able to accuratly predict on those values.""]",let check see level categori train test set new categori test set model may abl accuratli predict valu
2357,"[""We have values that apprear in the test set that is not available in the training set. To handle this we'll set the valid categories as the ones apprearing in the training set. This will force the catagories apprearing in the test set to be `nan`.""]",valu apprear test set avail train set handl set valid categori one apprear train set forc catagori apprear test set nan
2358,['### Memory reduction'],memori reduct
2359,['### Card Information'],card inform
2360,"['**Transaction Date**  \n', 'The `TransactionDT` gives the timedelta from a reference date. We can calculate the days past the reference date by dividing the values by 86,400 ($60\\times60\\times24$). ']",transact date transactiondt give timedelta refer date calcul day past refer date divid valu 86 400 60 times60 times24
2361,"[""If we look at a seven day period we can see that there is a cyclical movement within each day, but there isn't much difference between days. We would expect that the transaction volumns are lowest near midnight. We can calculate the hour of each day when the transaction occure by dividing the timedelta by 24 and taking the remainder and setting an offset. ""]",look seven day period see cyclic movement within day much differ day would expect transact volumn lowest near midnight calcul hour day transact occur divid timedelta 24 take remaind set offset
2362,"['From observing the histogram, we can estimate that 9/24 will be a good off-set.']",observ histogram estim 9 24 good set
2363,"[""The day's 0, 1, 2 and 6 all have fraud rates higher than the average. If we were to guess we could consider day 6 to be friday, 0 as saturday, 1 as sunday and 2 as monday. ""]",day 0 1 2 6 fraud rate higher averag guess could consid day 6 friday 0 saturday 1 sunday 2 monday
2364,"['Again we see a trend that is somewhat expected, on average there is an increase in the fraud rates during the night times, and reduced number of frauds during the day time. ']",see trend somewhat expect averag increas fraud rate night time reduc number fraud day time
2365,['### Transaction Amount'],transact amount
2366,['### Encoding Categorical Data'],encod categor data
2367,['### XGBoost'],xgboost
2368,['### LightGBM'],lightgbm
2369,['### Using Hyperopt to find best parameters'],use hyperopt find best paramet
2370,"['<hr/>\n', '[**Tolgahan Cepel**](https://www.kaggle.com/tolgahancepel)\n', '<hr/>\n', '<font color=green>\n', '* 1. [Importing Libraries and Reading the Dataset](#1)\n', '* 2. [Feature Engineering](#2)\n', '    * [Add New Features](#3)\n', '    * [Handle Email Domains](#4) \n', '    * [Handle P Email Domain and R Email Domain](#5) \n', '    * [Set Time](#6) \n', '    * [Handle Browser Version](#7) \n', '    * [Handle Device Type](#8)\n', '    * [Set Frequency](#9)\n', '* 3. [Data Preprocessing](#10) \n', '* 4. [Models](#11)\n', '    * [LightGBM](#12) \n', '* 5. [Submission](#13)\n', '<hr/>']",hr tolgahan cepel http www kaggl com tolgahancepel hr font color green 1 import librari read dataset 1 2 featur engin 2 add new featur 3 handl email domain 4 handl p email domain r email domain 5 set time 6 handl browser version 7 handl devic type 8 set frequenc 9 3 data preprocess 10 4 model 11 lightgbm 12 5 submiss 13 hr
2371,"['## <span id=""1""></span> ** 1. Importing Libraries and Reading the Dataset **']",span id 1 span 1 import librari read dataset
2372,"['## <span id=""2""></span> ** 2. Feature Engineering **']",span id 2 span 2 featur engin
2373,"['### <span id=""3""></span> ** Add New Features **']",span id 3 span add new featur
2374,"['### <span id=""4""></span> ** Handle Email Domains **']",span id 4 span handl email domain
2375,"['### <span id=""5""></span> ** Handle P Email Domain and R Email Domain **']",span id 5 span handl p email domain r email domain
2376,"['### <span id=""6""></span> ** Set Time **']",span id 6 span set time
2377,"['### <span id=""7""></span> ** Handle Browser Version **']",span id 7 span handl browser version
2378,"['### <span id=""8""></span> ** Handle Device Type **']",span id 8 span handl devic type
2379,"['### <span id=""9""></span> ** Set Frequency **']",span id 9 span set frequenc
2380,"['## <span id=""10""></span> ** 3. Data Preprocessing **']",span id 10 span 3 data preprocess
2381,"['## <span id=""11""></span> ** 4. Model **']",span id 11 span 4 model
2382,"['### <span id=""12""></span> ** LightGBM **']",span id 12 span lightgbm
2383,"['## <span id=""13""></span> ** 5. Submission **']",span id 13 span 5 submiss
2384,"['<b><font color=""red"">Don\'t forget to </font></b> <b><font color=""green"">UPVOTE </font></b> if you liked this kernel, thank you. 🙂👍']",b font color red forget font b b font color green upvot font b like kernel thank
2385,"['In this kernel I want to make an univariate analysis of Vxxx features. looking ahead, I think that these features are very important for fraud detection, because they give us a lot of information which transaction is fraudent and which is not, and using these features we can, theoretically, reduce our false negatives and false positives rate. Also they give us a lot of insights about feeature selection.\n', '\n', 'We do not know, what these features means, only information we have is:\n', '\n', 'Vxxx: Vesta engineered rich features, including ranking, counting, and other entity relations.']",kernel want make univari analysi vxxx featur look ahead think featur import fraud detect give u lot inform transact fraudent use featur theoret reduc fals neg fals posit rate also give u lot insight feeatur select know featur mean inform vxxx vesta engin rich featur includ rank count entiti relat
2386,"['Vast ammount of features in datasets have a lot of null values, up to 86%.']",vast ammount featur dataset lot null valu 86
2387,"['We can see something interesting here - our features have very small numbers in first, second and third quartiles, but maximum value on contrary is very high.\n', '\n', ""Also it seems like some features are binary (V1 for example), some looks like ordinal (V2) and some looks like numeric (V126), let's try to divide features by groups.""]",see someth interest featur small number first second third quartil maximum valu contrari high also seem like featur binari v1 exampl look like ordin v2 look like numer v126 let tri divid featur group
2388,"['Now we can start plotting.\n', '\n', ""First - binary features (bars in plots ordered by fraud rate), I'll create function to plot categorical features with fraud rate.""]",start plot first binari featur bar plot order fraud rate creat function plot categor featur fraud rate
2389,"['Looking ahead, I want to plot fraud rates for different values of features.']",look ahead want plot fraud rate differ valu featur
2390,"['We can see that values for our binary features have very similar fraud rates.\n', '\n', ""Let's return to plotting""]",see valu binari featur similar fraud rate let return plot
2391,"[""Let's look at counts:""]",let look count
2392,['0 values (2 for V305 feature) of binary features have very small ammount of transactions - less than 0.1% and fraud rate of these transacions is less than 0.1% or equal to 0.'],0 valu 2 v305 featur binari featur small ammount transact le 0 1 fraud rate transacion le 0 1 equal 0
2393,"['Next step - Ordinal features.\n', '\n', ""We have 257 ordinal features in dataset, i'll plot them by small groups and make some preparations for aesthetic purposes.\n"", '\n', 'First, I want to divide them by number of values, if feature have more than 20 unique values - it goes to long_ordinal list, else - to short_ordinal list.']",next step ordin featur 257 ordin featur dataset plot small group make prepar aesthet purpos first want divid number valu featur 20 uniqu valu goe long ordin list el short ordin list
2394,"[""Let's look at fraud rates.""]",let look fraud rate
2395,"[""6, 3, 4, 2, 5, 7, 8, 9 values don't give us much information, but we can see that a lot of features have fraud rate close to 0 at Null values, as 1 and 0 values.\n"", '\n', 'On contrary: 15, 24, 23, 17, 16, 18, 19 values have features with fraud rate equal to 1.\n', '\n', ""It's a pity that we can't make such plot for long_ordinal, it's too computational expensive and even if we will have unlimited computational resourses, all we got is mess.\n"", '\n', 'So, I want to use different approach for these features.']",6 3 4 2 5 7 8 9 valu give u much inform see lot featur fraud rate close 0 null valu 1 0 valu contrari 15 24 23 17 16 18 19 valu featur fraud rate equal 1 piti make plot long ordin comput expens even unlimit comput resours got mess want use differ approach featur
2396,"[""Now let's return to the routine and make plots for ordinal features.\n"", '\n', 'When I worked on this part, I faced with a problem, when such number of plots just crashed my kernel, so I decided to save plots in .png fomat and show them here as pictures.\n', '\n', ""Also, I'm including code for plots in comments.""]",let return routin make plot ordin featur work part face problem number plot crash kernel decid save plot png fomat show pictur also includ code plot comment
2397,['We can see something interesting here. Almost every feature in short_ordinal have a fraud peaks on some value for example V17 and V18 have 6 values with 100% fraud rate. Also every feature have values which fraud rate close or equal to zero.'],see someth interest almost everi featur short ordin fraud peak valu exampl v17 v18 6 valu 100 fraud rate also everi featur valu fraud rate close equal zero
2398,"['Similar situation here, we can easily see which walues give us 100% or 0% fraud rates.\n', '\n', ""Next - numeric features. I'll use log1p transformed distribution plots to see fraud and non fraud peaks.""]",similar situat easili see walu give u 100 0 fraud rate next numer featur use log1p transform distribut plot see fraud non fraud peak
2399,"[""Let's take a look how similar train and test sets are.""]",let take look similar train test set
2400,"[""Whoa, that's a pretty significant AUC!  0.999996 adverserial AUC is the biggest one I've ever come across. I first thought I might be making a mistake, but re-run this script several times, and don't seem to find any bugs in it. But I am open to criticims/suggestions.\n"", '\n', 'Let\'s look now at the top 20 ""adversarial"" features.']",whoa pretti signific auc 0 999996 adverseri auc biggest one ever come across first thought might make mistak run script sever time seem find bug open criticim suggest let look top 20 adversari featur
2401,"['Seems that transaction date is the main ""culprit"".\n', '\n', ""Let's see what happens when we remove time stamp.""]",seem transact date main culprit let see happen remov time stamp
2402,"[""At 0.90 the AUC has improved, but it's still really high.""]",0 90 auc improv still realli high
2403,['To be continued ...'],continu
2404,"['**THIS KERNAL IS BLEND OF **\n', 'So awesome kernels present Right now \n', '\n', '**vote if you love blend**\n', '\n', '1. https://www.kaggle.com/raghaw/ensemble-on-fire {already blended}\n', '2. https://www.kaggle.com/vincentlugat/ieee-lgb-bayesian-opt/output {lgb+bayesian}\n', '3. https://www.kaggle.com/ryches/keras-nn-starter-w-time-series-split/output {Keras NN}\n', '4. https://www.kaggle.com/timon88/lgbm-baseline-small-fe-no-blend/output {lgbm+fe}']",kernal blend awesom kernel present right vote love blend 1 http www kaggl com raghaw ensembl fire alreadi blend 2 http www kaggl com vincentlugat ieee lgb bayesian opt output lgb bayesian 3 http www kaggl com rych kera nn starter w time seri split output kera nn 4 http www kaggl com timon88 lgbm baselin small fe blend output lgbm fe
2405,"['## phase 1 [Ensemble]\n', '\n', '\n']",phase 1 ensembl
2406,['**Hist Graph of scores**'],hist graph score
2407,"['## phase 2 [Stacking]\n', '\n']",phase 2 stack
2408,"['**submission_p2_1.csv tops the chart**\n', '**vote if you love blend**']",submiss p2 1 csv top chart vote love blend
2409,"[' ## <div style=""text-align: left"">Simple EDA IEEE : Vesta Fraud Data </div> \n', '\n', '![](https://cdn1.imggmi.com/uploads/2019/8/7/884ef7fecc277f2c58396ed766d3d569-full.png)\n', '\n', '<br>\n', '<div style=""text-align: left"">\n', 'Vesta Corporation provided the dataset for this competition. Vesta Corporation is the forerunner in guaranteed e-commerce payment solutions. Founded in 1995, Vesta pioneered the process of fully guaranteed card-not-present (CNP) payment transactions for the telecommunications industry. Since then, Vesta has firmly expanded data science and machine learning capabilities across the globe and solidified its position as the leader in guaranteed ecommerce payments. Today, Vesta guarantees more than $18B in transactions annually.\n', '\n', ""In this competition, you’ll benchmark machine learning models on a challenging large-scale dataset. The data comes from Vesta's real-world e-commerce transactions and contains a wide range of features from device type to product features. You also have the opportunity to create new features to improve your results.</div>""]",div style text align left simpl eda ieee vesta fraud data div http cdn1 imggmi com upload 2019 8 7 884ef7fecc277f2c58396ed766d3d569 full png br div style text align left vesta corpor provid dataset competit vesta corpor forerunn guarante e commerc payment solut found 1995 vesta pioneer process fulli guarante card present cnp payment transact telecommun industri sinc vesta firmli expand data scienc machin learn capabl across globe solidifi posit leader guarante ecommerc payment today vesta guarante 18b transact annual competit youll benchmark machin learn model challeng larg scale dataset data come vesta real world e commerc transact contain wide rang featur devic type product featur also opportun creat new featur improv result div
2410,"['# Distribution plots of features\n', '\n', '-999 value is injected for Nan values']",distribut plot featur 999 valu inject nan valu
2411,['## Card 1-6'],card 1 6
2412,['## C 1-14'],c 1 14
2413,['## D 1-15'],1 15
2414,['## M 1-9'],1 9
2415,['## ID 1-38'],id 1 38
2417,"[' # What is Correlation?\n', '\n', 'Variables within a dataset can be related for lots of reasons.\n', '\n', 'For example:\n', '\n', '* One variable could cause or depend on the values of another variable.\n', '* One variable could be lightly associated with another variable.\n', '* Two variables could depend on a third unknown variable.\n', '\n', 'It can be useful in data analysis and modeling to better understand the relationships between variables. The statistical relationship between two variables is referred to as their correlation.\n', '\n', 'A correlation could be positive, meaning both variables move in the same direction, or negative, meaning that when one variable’s value increases, the other variables’ values decrease. Correlation can also be neural or zero, meaning that the variables are unrelated.\n', '\n', '1. > **Positive Correlation:** both variables change in the same direction.\n', '2. > **Neutral Correlation:** No relationship in the change of the variables.\n', '3. > **Negative Correlation:** variables change in opposite directions.\n', '\n', 'The performance of some algorithms can deteriorate if two or more variables are tightly related, called multicollinearity. An example is linear regression, where one of the offending correlated variables should be removed in order to improve the skill of the model.\n', '\n', 'We may also be interested in the correlation between input variables with the output variable in order provide insight into which variables may or may not be relevant as input for developing a model.\n', '\n', 'The structure of the relationship may be known, e.g. it may be linear, or we may have no idea whether a relationship exists between two variables or what structure it may take. Depending what is known about the relationship and the distribution of the variables, different correlation scores can be calculated.\n', '\n']",correl variabl within dataset relat lot reason exampl one variabl could caus depend valu anoth variabl one variabl could lightli associ anoth variabl two variabl could depend third unknown variabl use data analysi model better understand relationship variabl statist relationship two variabl refer correl correl could posit mean variabl move direct neg mean one variabl valu increas variabl valu decreas correl also neural zero mean variabl unrel 1 posit correl variabl chang direct 2 neutral correl relationship chang variabl 3 neg correl variabl chang opposit direct perform algorithm deterior two variabl tightli relat call multicollinear exampl linear regress one offend correl variabl remov order improv skill model may also interest correl input variabl output variabl order provid insight variabl may may relev input develop model structur relationship may known e g may linear may idea whether relationship exist two variabl structur may take depend known relationship distribut variabl differ correl score calcul
2418,['## Card 1-6'],card 1 6
2419,['## C 1-14'],c 1 14
2420,['## D 1-15'],1 15
2421,['## M 1-9'],1 9
2422,['## V 1-50'],v 1 50
2423,['## V 51-100'],v 51 100
2424,['## V 101-150'],v 101 150
2425,['## V 151-200'],v 151 200
2426,['## V 201-250'],v 201 250
2427,['## V 251-300'],v 251 300
2428,['## V 301-339'],v 301 339
2429,['## iD 1-38'],id 1 38
2431,"['# Heatmaps of Covariance\n', '\n', 'In probability, covariance is the measure of the joint probability for two random variables. It describes how the two variables change together.']",heatmap covari probabl covari measur joint probabl two random variabl describ two variabl chang togeth
2432,['## Card 1-6'],card 1 6
2433,['## C 1-14'],c 1 14
2434,['## D 1-15'],1 15
2435,['## M 1-9'],1 9
2436,['## V 1-50'],v 1 50
2437,['## V 51-100'],v 51 100
2438,['## V 101-150'],v 101 150
2439,['## V 151-200'],v 151 200
2440,['## V 201-250'],v 201 250
2441,['## V 251-300'],v 251 300
2442,['## V 301-339'],v 301 339
2443,['## iD 1-38'],id 1 38
2445,"['\n', '<div style=""text-align: center; color:gold""> **To be continue & UPVOTE IF YOU LIKE** </div> \n']",div style text align center color gold continu upvot like div
2446,"['----------\n', '**IEEE - Catboost GPU baseline(5 Kfold)**\n', '=====================================\n', '\n', '***Vincent Lugat***\n', '\n', '*July 2019*\n', '\n', '----------']",ieee catboost gpu baselin 5 kfold vincent lugat juli 2019
2447,['![](https://image.noelshack.com/fichiers/2019/29/2/1563297157-cis-logo.png)'],http imag noelshack com fichier 2019 29 2 1563297157 ci logo png
2448,"[""- <a href='#1'>1. Libraries and Data</a>  \n"", ""- <a href='#2'>2. Catboost GPU </a> \n"", ""- <a href='#3'>3. Features importance</a>\n"", ""- <a href='#4'>4. Submission</a>""]",href 1 1 librari data href 2 2 catboost gpu href 3 3 featur import href 4 4 submiss
2449,"[""# <a id='1'>1. Librairies and data</a> ""]",id 1 1 librairi data
2450,['## DATASETS'],dataset
2451,"['## MERGE, MISSING VALUE, FILL NA']",merg miss valu fill na
2452,['## ENCODING'],encod
2453,['## CONFUSION MATRIX'],confus matrix
2454,"[""# <a id='2'>2. Catboost</a> ""]",id 2 2 catboost
2455,['## PARAMS '],param
2456,['## CV 5 FOLDS AND METRICS'],cv 5 fold metric
2457,"[""# <a id='3'>3. Feature importance</a> ""]",id 3 3 featur import
2458,"[""# <a id='4'>4. Submission</a> ""]",id 4 4 submiss
2459,"['----------\n', '**IEEE Fraud Detection - Bayesian optimization - LGB**\n', '=====================================\n', '\n', '***Vincent Lugat***\n', '\n', '*July 2019*\n', '\n', '----------']",ieee fraud detect bayesian optim lgb vincent lugat juli 2019
2460,['![](https://image.noelshack.com/fichiers/2019/29/2/1563297157-cis-logo.png)'],http imag noelshack com fichier 2019 29 2 1563297157 ci logo png
2461,"[""- <a href='#1'>1. Libraries and Data</a>  \n"", ""- <a href='#2'>2. Bayesian Optimisation </a> \n"", ""- <a href='#3'>3. LGB + best hyperparameters</a>\n"", ""- <a href='#4'>4. Features importance</a>\n"", ""- <a href='#4'>5. Submission</a>""]",href 1 1 librari data href 2 2 bayesian optimis href 3 3 lgb best hyperparamet href 4 4 featur import href 4 5 submiss
2462,"[""# <a id='1'>1. Librairies and data</a> ""]",id 1 1 librairi data
2463,['## DATASETS'],dataset
2464,"['## MERGE, MISSING VALUE, FILL NA']",merg miss valu fill na
2465,['Source : https://www.kaggle.com/vaishvik25/refine-ieee-data'],sourc http www kaggl com vaishvik25 refin ieee data
2466,['## ENCODING'],encod
2467,"[""# <a id='2'>2. Bayesian Optimisation</a> ""]",id 2 2 bayesian optimis
2468,['## CONFUSION MATRIX'],confus matrix
2469,"[""# <a id='3'>3. LGB + best hyperparameters</a> ""]",id 3 3 lgb best hyperparamet
2470,"[""# <a id='4'>4. Features importance</a> ""]",id 4 4 featur import
2471,"[""# <a id='5'>5. Submission</a> ""]",id 5 5 submiss
2472,['looks like merging the two data tables has created a lot of missing values. let us first try to identify good features from these.'],look like merg two data tabl creat lot miss valu let u first tri identifi good featur
2473,"['Now that we have identified good numeric variables, let us use just the missing data as benchmark for the categorical variables']",identifi good numer variabl let u use miss data benchmark categor variabl
2474,"['# Bounded region of parameter space\n', 'bounds_LGB = {\n', ""    'num_leaves': (31, 500), \n"", ""    'min_data_in_leaf': (20, 200),\n"", ""    'bagging_fraction' : (0.1, 0.9),\n"", ""    'feature_fraction' : (0.1, 0.9),\n"", ""    'learning_rate': (0.01, 0.3),\n"", ""    'min_child_weight': (1, 4),   \n"", ""    'reg_alpha': (0.2,2), \n"", ""    'reg_lambda': (0.2,2),\n"", ""    'max_depth':(-1,50),\n"", ""    'n_estimators':(750,7500),\n"", ""    'max_bin':(32,256)\n"", '}\n', '\n', 'from bayes_opt import BayesianOptimization\n', 'from sklearn.model_selection import train_test_split\n', 'X_train, X_test, y_train, y_test = train_test_split(train_data, target_data, test_size=0.25, random_state=42)\n', 'train_index = X_train.index\n', 'test_index = X_test.index\n', '\n', 'def LGB_bayesian(num_leaves,bagging_fraction,feature_fraction,min_child_weight,\n', '                 min_data_in_leaf,max_depth,n_estimators,reg_alpha,reg_lambda,learning_rate,max_bin):\n', '    num_leaves = int(num_leaves)\n', '    min_data_in_leaf = int(min_data_in_leaf)\n', '    max_depth = int(max_depth)\n', '    n_estimators = int(n_estimators)\n', '    min_child_weight = int(min_child_weight)\n', '    max_bin = int(max_bin)\n', '\n', '    assert type(num_leaves) == int\n', '    assert type(min_data_in_leaf) == int\n', '    assert type(max_depth) == int\n', '    assert type(n_estimators) == int\n', '    assert type(min_child_weight) == int\n', '    assert type(max_bin) == int\n', ""    param = {'num_leaves': num_leaves,'min_data_in_leaf': min_data_in_leaf,'min_child_weight': min_child_weight,'bagging_fraction' : bagging_fraction,\n"", ""             'feature_fraction' : feature_fraction,'max_depth': max_depth,'reg_alpha': reg_alpha,'reg_lambda': reg_lambda,\n"", ""              'objective': 'binary','boosting_type': 'gbdt','colsample_bytree':.8,'subsample':.9,'min_split_gain':.01,'max_bin':max_bin,\n"", ""             'bagging_freq':5,'learning_rate':learning_rate,'metric':'auc','n_estimators':n_estimators,'min_data_in_leaf':min_data_in_leaf,\n"", ""            'early_stopping_rounds':100} \n"", ""    lgb_bayes = LGBMClassifier(boosting = param['boosting_type'],n_estimators =  param['n_estimators'],\n"", ""                     learning_rate =  param['learning_rate'],num_leaves =  param['num_leaves'],\n"", ""                     colsample_bytree = param['colsample_bytree'],subsample =  param['subsample'],\n"", ""                     max_depth =  param['max_depth'],reg_alpha =  param['reg_alpha'],\n"", ""                     reg_lambda =  param['reg_lambda'],min_split_gain =  param['min_split_gain'],\n"", ""                     min_child_weight =  param['min_child_weight'],max_bin =  param['max_bin'],\n"", ""                     bagging_freq =  param['bagging_freq'],feature_fraction =  param['feature_fraction'],\n"", ""                     bagging_fraction =  param['bagging_fraction'],min_data_in_leaf = param['min_data_in_leaf'],\n"", ""                              early_stopping_rounds = param['early_stopping_rounds'])\n"", '    lgb_bayes.fit(train_data.iloc[train_index,:], target_data.iloc[train_index],\n', '                 eval_set = [(train_data.iloc[train_index,:], target_data.iloc[train_index]), \n', ""                             (train_data.iloc[test_index,:], target_data.iloc[test_index])],eval_metric='auc',verbose = 200)\n"", '    fpr, tpr, auc_score = compute_roc_auc(lgb_bayes,test_index)  \n', '    return auc_score\n', 'lightGBM_bo = BayesianOptimization(LGB_bayesian, bounds_LGB, random_state=42)\n', 'print(lightGBM_bo.space.keys)\n', 'init_points = 10\n', 'n_iter = 35\n', ""print('-' * 130)\n"", '\n', ""lightGBM_bo.maximize(init_points=init_points, n_iter=n_iter, acq='ucb', xi=0.0, alpha=1e-6)\n"", ""print(lightGBM_bo.max['target'])\n"", ""print(lightGBM_bo.max['params'])\n"", 'params = {\n', ""        'n_estimators': int(lightGBM_bo.max['params']['n_estimators']), \n"", ""        'num_leaves': int(lightGBM_bo.max['params']['num_leaves']), \n"", ""        'min_child_weight': lightGBM_bo.max['params']['min_child_weight'],\n"", ""        'min_data_in_leaf': int(lightGBM_bo.max['params']['min_data_in_leaf']),\n"", ""        'bagging_fraction': lightGBM_bo.max['params']['bagging_fraction'], \n"", ""        'feature_fraction': lightGBM_bo.max['params']['feature_fraction'],\n"", ""        'reg_lambda': lightGBM_bo.max['params']['reg_lambda'],\n"", ""        'reg_alpha': lightGBM_bo.max['params']['reg_alpha'],\n"", ""        'max_depth': int(lightGBM_bo.max['params']['max_depth']), \n"", ""        'metric':'auc',\n"", ""        'boosting_type': 'gbdt',\n"", ""        'colsample_bytree':.8,'subsample':.9,\n"", ""        'min_split_gain':.01,\n"", ""        'max_bin':int(lightGBM_bo.max['params']['max_bin']), #127,\n"", ""        'bagging_freq':5,\n"", ""        'learning_rate':lightGBM_bo.max['params']['learning_rate'],\n"", ""        #'learning_rate':0.01,\n"", ""        'early_stopping_rounds':100\n"", '    }']",bound region paramet space bound lgb num leav 31 500 min data leaf 20 200 bag fraction 0 1 0 9 featur fraction 0 1 0 9 learn rate 0 01 0 3 min child weight 1 4 reg alpha 0 2 2 reg lambda 0 2 2 max depth 1 50 n estim 750 7500 max bin 32 256 bay opt import bayesianoptim sklearn model select import train test split x train x test train test train test split train data target data test size 0 25 random state 42 train index x train index test index x test index def lgb bayesian num leav bag fraction featur fraction min child weight min data leaf max depth n estim reg alpha reg lambda learn rate max bin num leav int num leav min data leaf int min data leaf max depth int max depth n estim int n estim min child weight int min child weight max bin int max bin assert type num leav int assert type min data leaf int assert type max depth int assert type n estim int assert type min child weight int assert type max bin int param num leav num leav min data leaf min data leaf min child weight min child weight bag fraction bag fraction featur fraction featur fraction max depth max depth reg alpha reg alpha reg lambda reg lambda object binari boost type gbdt colsampl bytre 8 subsampl 9 min split gain 01 max bin max bin bag freq 5 learn rate learn rate metric auc n estim n estim min data leaf min data leaf earli stop round 100 lgb bay lgbmclassifi boost param boost type n estim param n estim learn rate param learn rate num leav param num leav colsampl bytre param colsampl bytre subsampl param subsampl max depth param max depth reg alpha param reg alpha reg lambda param reg lambda min split gain param min split gain min child weight param min child weight max bin param max bin bag freq param bag freq featur fraction param featur fraction bag fraction param bag fraction min data leaf param min data leaf earli stop round param earli stop round lgb bay fit train data iloc train index target data iloc train index eval set train data iloc train index target data iloc train index train data iloc test index target data iloc test index eval metric auc verbos 200 fpr tpr auc score comput roc auc lgb bay test index return auc score lightgbm bo bayesianoptim lgb bayesian bound lgb random state 42 print lightgbm bo space key init point 10 n iter 35 print 130 lightgbm bo maxim init point init point n iter n iter acq ucb xi 0 0 alpha 1e 6 print lightgbm bo max target print lightgbm bo max param param n estim int lightgbm bo max param n estim num leav int lightgbm bo max param num leav min child weight lightgbm bo max param min child weight min data leaf int lightgbm bo max param min data leaf bag fraction lightgbm bo max param bag fraction featur fraction lightgbm bo max param featur fraction reg lambda lightgbm bo max param reg lambda reg alpha lightgbm bo max param reg alpha max depth int lightgbm bo max param max depth metric auc boost type gbdt colsampl bytre 8 subsampl 9 min split gain 01 max bin int lightgbm bo max param max bin 127 bag freq 5 learn rate lightgbm bo max param learn rate learn rate 0 01 earli stop round 100
2475,['![fraud%20detection.jpeg](attachment:fraud%20detection.jpeg)'],fraud 20detect jpeg attach fraud 20detect jpeg
2476,"['The purpose of this notbook is to perform the following tasks:\n', ' \n', ' 1. [Load Packages and Data](#1)\n', ' 2. [Exploratory Data Analysis](#2)\n', ' 3. [Feature Engineering](#3)\n', ' 4. [Binning of variables and Imputation of missing values](#4)\n', ' 5. [Outlier Analysis and Feature Scaling](#5)\n', ' 6. [Modeling: xgboost](#6)\n', ' \n', 'Main Findings:\n', ' \n', ' * The feature set can be divided into 3 data types: \n', ' \n', '  1. Categorical Variables, \n', '  2. Numerical Variables and \n', '  3. Numeric Encoding Variables {C_X, D_X and V_X} \n', '  \n', '  \n', ' * 41% of the train_transaction is missing data.\n', ' * 36% of the test_transaction is missing data.\n', ' * 35% of train_identity is missing data.\n', ' * 36% of test_identity is missing data.\n', ' \n', 'At the end of the task the training and test datasets are imputed for missing values, scaled and fixed for outliers and features are binned to prevent overfitting. The results are saved as outputs of this notebook. The user can use the outputs of this notebook for model building.']",purpos notbook perform follow task 1 load packag data 1 2 exploratori data analysi 2 3 featur engin 3 4 bin variabl imput miss valu 4 5 outlier analysi featur scale 5 6 model xgboost 6 main find featur set divid 3 data type 1 categor variabl 2 numer variabl 3 numer encod variabl c x x v x 41 train transact miss data 36 test transact miss data 35 train ident miss data 36 test ident miss data end task train test dataset imput miss valu scale fix outlier featur bin prevent overfit result save output notebook user use output notebook model build
2477,"['### [Load Packages and Data](#1)<a id=""1""></a> <br>']",load packag data 1 id 1 br
2478,"['### [Exploratory Data Analysis](#2)<a id=""2""></a> <br>\n', ' \n', ' * Variables C_X and D_X have only integers as values and most of V_X as well. \n', ' * I strongly believe that C_X, D_X and V_X are numeric encoding. This is important to know when imputing the missing values, performing outlier analysis and modelling.\n', ' * Varibles dist1, dist2 and TransactionAmt have a long tail distribution. Agian this is also important to realize when performing outlier analysis.\n', ' * 41% of the train_transaction is missing.\n', ' * 36% of the test_transaction is missing.\n', ' * 35% of train_identity is missing data.\n', ' * 36% of test_identity is missing data.']",exploratori data analysi 2 id 2 br variabl c x x integ valu v x well strongli believ c x x v x numer encod import know imput miss valu perform outlier analysi model varibl dist1 dist2 transactionamt long tail distribut agian also import realiz perform outlier analysi 41 train transact miss 36 test transact miss 35 train ident miss data 36 test ident miss data
2479,"['### [Feature Engineering](#3)<a id=""3""></a> <br>\n', '\n', '* Day of the week and hour are engineerd from TransactionDT.\n', '* The decimal part of the TransactionAmt is engineerd as an separate feature. \n']",featur engin 3 id 3 br day week hour engineerd transactiondt decim part transactionamt engineerd separ featur
2480,"['### [Binning of variables and Imputation of missing values](#4)<a id=""4""></a> <br>\n', ' \n', '  * Before we go into the missing value imputation I have explicitly changed the dataype of some of the categorical features into type boolean beacuse some of the categorical features are loaded as numerical.\n', '  * For numerical features dist1, dist2 and TransactionAmt the missing values are replaced by the mean of the column. The missing value of other features are replaced by -999\n', '  * Variables C_X and D_X have only integers as values and most of V_X as well. There missing values are replaced by -999.\n', '  * The rare values of all features other than dist1, dist2, TransactionAmt and TransactionDT are binned together. This is a very important step to perform to prevent overfitting when performing modelling.\n', '  * To reduce the size of the datasets the kernel published by [MJ Bahamani](https://www.kaggle.com/mjbahmani/reducing-memory-size-for-ieee) is applied to the datasets. It is a very helpfull function.']",bin variabl imput miss valu 4 id 4 br go miss valu imput explicitli chang datayp categor featur type boolean beacus categor featur load numer numer featur dist1 dist2 transactionamt miss valu replac mean column miss valu featur replac 999 variabl c x x integ valu v x well miss valu replac 999 rare valu featur dist1 dist2 transactionamt transactiondt bin togeth import step perform prevent overfit perform model reduc size dataset kernel publish mj bahamani http www kaggl com mjbahmani reduc memori size ieee appli dataset helpful function
2481,"['### [Outlier Analysis and Feature Scaling](#5)<a id=""5""></a> <br>\n', ' \n', '  * I have taken these two subjects under one section because I am using **RobustScaler()** to scale dist1, dist2 and TransactionAmt. **RobustScaler()** applies Inter Quartile Range(IQR) to scale the features when the argument **with_scaling=True**.\n', '  * Outlier Analysis is contentious territory. I have opted for Inter Quartile Range (IQR) to detect outliers since it does not assume any distribution for the features.\n', '  *  There are different ways to perform feature scaling. Here it has been opted for **RobustScaler()** because it is relatively more robust outliers. [Here](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html) you can find more information regarding this scaler. \n', '  *  It is applied only to dist1, dist2 and TransactionAmt features.']",outlier analysi featur scale 5 id 5 br taken two subject one section use robustscal scale dist1 dist2 transactionamt robustscal appli inter quartil rang iqr scale featur argument scale true outlier analysi contenti territori opt inter quartil rang iqr detect outlier sinc assum distribut featur differ way perform featur scale opt robustscal rel robust outlier http scikit learn org stabl modul gener sklearn preprocess robustscal html find inform regard scaler appli dist1 dist2 transactionamt featur
2482,"['### [Modeling: Xgboost](#6)<a id=""6""></a> <br>\n', '\n', '* The parameters are not optimised.\n', '* The xgboost model is based on [this](copied from https://www.kaggle.com/xhlulu/ieee-fraud-xgboost-with-gpu-fit-in-40s) great kernel. ']",model xgboost 6 id 6 br paramet optimis xgboost model base copi http www kaggl com xhlulu ieee fraud xgboost gpu fit 40 great kernel
2483,"['Get feature importance of each feature. Importance type can be defined as:\n', '\n', '‘weight’: the number of times a feature is used to split the data across all trees.\n', '\n', '‘gain’: the average gain across all splits the feature is used in.\n', '\n', '‘cover’: the average coverage across all splits the feature is used in.\n', '\n', '‘total_gain’: the total gain across all splits the feature is used in.\n', '\n', '‘total_cover’: the total coverage across all splits the feature is used in.']",get featur import featur import type defin weight number time featur use split data across tree gain averag gain across split featur use cover averag coverag across split featur use total gain total gain across split featur use total cover total coverag across split featur use
2484,"['# General\n', ""I'm very sorry that I changed the name of this kernel. This is becuase I often update this kernel and the content is changing.\n"", '\n', 'Thanks to VESTA, we can know about what the addr2 data is.\n', '\n', 'https://www.kaggle.com/c/ieee-fraud-detection/discussion/101203#latest-596566\n', '\n', 'In this discussion(the URL↑), VESTA person says ""addr2 as billing country"".\n', '\n', 'Therefore, I want to identify the country and use data more efficiently.\n', '\n', 'I wish that this sharing help other kagglers!']",gener sorri chang name kernel becuas often updat kernel content chang thank vesta know addr2 data http www kaggl com c ieee fraud detect discus 101203 latest 596566 discus url vesta person say addr2 bill countri therefor want identifi countri use data effici wish share help kaggler
2485,"['# what already is known\n', '\n', 'I consulted with this discussion.\n', '\n', 'https://www.kaggle.com/c/ieee-fraud-detection/discussion/100400#579001\n', '\n', 'We can know the hour data from transactionDT.\n', '\n', 'Therefore, the gap of standard time should be supeculated.']",alreadi known consult discus http www kaggl com c ieee fraud detect discus 100400 579001 know hour data transactiondt therefor gap standard time supecul
2486,['## the appearance of addr2'],appear addr2
2487,"['Various country data is used.\n', '\n', 'I use only the countries which have more tha 50 transaction data, because I can not guess the precise standard time if the amount of data is small.\n', '\n', 'That is, 16.0 31.0 32.0 60.0 65.0 87.0 96.0']",variou countri data use use countri tha 50 transact data guess precis standard time amount data small 16 0 31 0 32 0 60 0 65 0 87 0 96 0
2488,"['### make hour column from transactionDT\n', '\n', 'As I said, I consulted with the discussion. Thanks for people who excite the discussion. \n']",make hour column transactiondt said consult discus thank peopl excit discus
2489,"['## speculation of standard time\n', 'At first, the country ""87.0"" has 520481 transaction.\n', '\n', 'This must be the U.S.\n', '\n', 'I checked the gap between the U.S. and others.']",specul standard time first countri 87 0 520481 transact must u check gap u other
2490,"['# 16,65\n', 'One of them may be Russia, but it is uncertain.\n', '\n', 'The possibility of china also exist.\n', '\n', 'However, we can say that these two countries are similar place to Russia or China.\n', '\n', 'I name them Asia group.']",16 65 one may russia uncertain possibl china also exist howev say two countri similar place russia china name asia group
2491,"['# 31,32\n', 'The standard time is similar to America.\n', 'Juding from country size, 32 should be Canada. \n', '\n', 'However, it may be Mexico.']",31 32 standard time similar america jude countri size 32 canada howev may mexico
2492,"['# 60,96\n', 'They are about -6 hour. \n', 'Threfore, it should be Europe country.']",60 96 6 hour threfor europ countri
2493,"['# making future\n', 'At first, I made Europe future.\n', '\n', ""Soon, I'll increase it.""]",make futur first made europ futur soon increas
2494,['There is a gap between them.'],gap
2495,['We can find the difference between them.'],find differ
2496,"['# Conclusion\n', 'This identification is meningful to some degree, because we can find the gap between Europe data and other data.\n', '\n', 'The problem is that America has some standart time, so some error is appearing, for example, we can not boastfuly say that 16 is Russia.\n', '\n', 'I want to apply this result for futher data_engineering.']",conclus identif mening degre find gap europ data data problem america standart time error appear exampl boastfuli say 16 russia want appli result futher data engin
2497,"['# Before going to main content\n', 'new feature was being created.\n', '\n', 'V1 V319-V320\n', '\n', 'V7 V109-V110\n', '\n', 'V12 V329-V330\n', '\n', 'V14 V316-V331\n', '\n', 'V18 V4-V5\n', ' \n', '\n', ""**Sorry, I want to update quickly this kernel, but I can't because these days kernel's commiting is too busy! **\n"", '\n', 'This kernel is using official kernel for judging whether my new feature is meaningful.\n', '\n', 'The official kernel is here(https://www.kaggle.com/inversion/ieee-simple-xgboost)\n', '\n', 'And that the feature which I made is explained in detail here ( https://www.kaggle.com/yasagure/how-do-we-treat-with-similar-columns-v319-v321/edit/run/18988983)']",go main content new featur creat v1 v319 v320 v7 v109 v110 v12 v329 v330 v14 v316 v331 v18 v4 v5 sorri want updat quickli kernel day kernel commit busi kernel use offici kernel judg whether new featur meaning offici kernel http www kaggl com invers ieee simpl xgboost featur made explain detail http www kaggl com yasagur treat similar column v319 v321 edit run 18988983
2498,"['# Introduction - Do they match?\n', '**Some people throw away similar columns, but it is good thing?**\n', '\n', 'Everyone found that there is a lot of similar columns in this dataset.\n', '\n', 'V319-V320 and V109-V110 are good example of them.\n', '\n', 'How should we ""use"" it?\n', '\n', 'Most people throw away the data, but I think that I can get useful information from it.\n', '\n', '**My idea is paying attention to whether they match each other.**']",introduct match peopl throw away similar column good thing everyon found lot similar column dataset v319 v320 v109 v110 good exampl use peopl throw away data think get use inform idea pay attent whether match
2499,['# load dataset'],load dataset
2500,"['We want to both of data -""identity"" and ""transaction""-\n', '\n', 'The identity data should be added to transaction data. ']",want data ident transact ident data ad transact data
2501,['# How V-feature is?'],v featur
2502,"['V features have a lot of similar columns.\n', '\n', ""Let's look at it.""]",v featur lot similar column let look
2503,"['# Feature engineering\n', 'I picked up these 3 pairs of columns. ']",featur engin pick 3 pair column
2504,['They seem to be similar.'],seem similar
2505,['# look at appearance of this new feature '],look appear new featur
2506,['## V319-V320'],v319 v320
2507,['## V109-V110'],v109 v110
2508,['## V329-V330'],v329 v330
2509,['## V316-V331'],v316 v331
2510,['## V4-V5'],v4 v5
2511,"['There seem to be difference, but the gap of ""diff_V109_V110"" is small.\n', '\n', 'In Version7, I found that ""diff_V109_V110"" is not meaningful. I deleted.\n', '\n', 'In Version13, I found that ""diff_V329_V330"" is not meaningful. I deleted.\n', '\n', 'In Version20, I found that ""diff_V4_V5"" is not meaningful. I deleted.\n', '\n', 'I feel that when the gap is big, the column is meaningful.']",seem differ gap diff v109 v110 small version7 found diff v109 v110 meaning delet version13 found diff v329 v330 meaning delet version20 found diff v4 v5 meaning delet feel gap big column meaning
2512,['# data cleaning'],data clean
2513,"['## Label Encoding\n', 'We cannot use literal features for XGB, so these features are changes.\n', '\n', 'For example, [H,G,W,A] →[0,1,2,3]\n', '\n', 'The number of the words is often related to the numeral([0,1,2,3]).']",label encod use liter featur xgb featur chang exampl h g w 0 1 2 3 number word often relat numer 0 1 2 3
2514,['# Model'],model
2515,"['# Is it useful?\n', 'The score of V5 is 0.9367.\n', '\n', 'The official score was 0.9366.\n', '\n', 'The score seem to be  improved.\n', '\n', 'The score of V8(V109-V110 added) is not unknown. Soon, I tell you it. ←the diff V109-V110 does not seem to be useful.\n', '\n', 'Also, the diff V329-V330 does not seem to be useful.']",use score v5 0 9367 offici score 0 9366 score seem improv score v8 v109 v110 ad unknown soon tell diff v109 v110 seem use also diff v329 v330 seem use
2516,"['# Conclusion\n', 'Some people think this is useful, but others not.\n', '\n', 'If you interested in it, please use for your model and judge wheter these columns are useful for your model.']",conclus peopl think use other interest plea use model judg wheter column use model
2517,"['##Attention\n', '\n', 'Thanks to comments on this kernel, I found some problem.\n', '\n', '*In this kernel, the datetime of transaction is not seriously discussed.\n', '\n', '*Transactions which have the identity data are more doubtful.\n', '\n', 'In V3, I fixed about second problem, but still have the first problem\n', '\n', 'And that, I mainly focused on getting interesting information, not on getting useful information only for this competition. ']",attent thank comment kernel found problem kernel datetim transact serious discus transact ident data doubt v3 fix second problem still first problem mainli focus get interest inform get use inform competit
2518,"['**General**\n', '\n', 'After provoking the discussion (https://www.kaggle.com/c/ieee-fraud-detection/discussion/103565), I want to examine it.\n', '\n', 'I wish that this kernel would help your research! \n']",gener provok discus http www kaggl com c ieee fraud detect discus 103565 want examin wish kernel would help research
2519,"['I focused on browser!\n', '\n', 'I am a very lazy person, so I rarely update my browser.\n', '\n', 'How swindlers are? ']",focus browser lazi person rare updat browser swindler
2520,"['There are a lot of data from which we cannot say that someone are try hard to update browser.\n', '\n', 'Therefore, most of them was NaN.']",lot data say someon tri hard updat browser therefor nan
2521,"['I consulted with wikipedia about chrome.\n', '\n', 'https://en.wikipedia.org/wiki/Google_Chrome_version_history\n', 'On December 5th in 2017, chrome 63 version was released.\n', '\n', 'I determined 63 was lastest.\n', '\n', 'It may be more useful if you use the date information.\n']",consult wikipedia chrome http en wikipedia org wiki googl chrome version histori decemb 5th 2017 chrome 63 version releas determin 63 lastest may use use date inform
2522,"[""**Let's check!**""]",let check
2523,['##Conclusion'],conclus
2524,"['This result suggests that people who update browser are more possibly fraud makers than people who are lazy about browser. \n', '\n', 'Fraud makers may be earnest people.\n']",result suggest peopl updat browser possibl fraud maker peopl lazi browser fraud maker may earnest peopl
2525,"[""# Beginner's Random Forests example\n"", '\n', 'This is a very simple Random Forests example meant for beginners. This is not meant to achieve a high score, merely a starting point on which to start, without complicated techniques.\n', '\n', ""It's recommended that you finish the Kaggle Learn courses (introduction and intermediate machine learning).""]",beginn random forest exampl simpl random forest exampl meant beginn meant achiev high score mere start point start without complic techniqu recommend finish kaggl learn cours introduct intermedi machin learn
2526,"['## Files\n', '\n', 'As can be seen the training data contains two files, `train_transaction.csv` and `train_identity.csv`. These two tables are related to each other via the column `TransactionID`. ']",file seen train data contain two file train transact csv train ident csv two tabl relat via column transactionid
2527,"[""In order to use these files for training, we'll need to do what's sometimes called denormalising the data. We can do this by doing a left join on both tables using the DataFrame's `merge()` method.""]",order use file train need sometim call denormalis data left join tabl use datafram merg method
2528,"[""Let's do a sanity check whenever we do something like this. Make sure the shape contains the same number of rows and the combined columns:""]",let saniti check whenev someth like make sure shape contain number row combin column
2529,"[""Looks like that worked! But we're now using a lot of RAM (look at the sidebar of your kernel). My kernel currently says I'm at 5 gigabytes, and we haven't even read the test set yet!\n"", '\n', ""While cleaning the data, it's possible we may need to make copies of (some sections) of the data, so this is obviously not ideal.""]",look like work use lot ram look sidebar kernel kernel current say 5 gigabyt even read test set yet clean data possibl may need make copi section data obvious ideal
2530,"['## Memory reduction\n', '\n', 'As discussed in [my other kernel](https://www.kaggle.com/yoongkang/beginner-memory-reduction-techniques), parsing the training dataset with default settings could take up to 2GBs of memory unnecessarily. With a few techniques (also discussed in the linked kernel) we can cut this down by about a gigabyte. \n', '\n', ""In this kernel, we'll use similar techniques, if you want to see my reasoning for this please refer to the other kernel.\n"", '\n', 'First we need to determine which numeric columns we have so that we can downcast them (cast from float64 to another type that requires less memory).']",memori reduct discus kernel http www kaggl com yoongkang beginn memori reduct techniqu par train dataset default set could take 2gb memori unnecessarili techniqu also discus link kernel cut gigabyt kernel use similar techniqu want see reason plea refer kernel first need determin numer column downcast cast float64 anoth type requir le memori
2531,"['As described in the other kernel, some of these columns are `float64` by default due to the presence of some `NaN` values.\n', '\n', ""The fact that they are `NaN` might be meaningful in this context, so completely replacing them (this is called imputation), doesn't sound like a great idea. However, we can add a boolean column to mark that the column has been replaced, and hopefully the training algorithm is smart enough to take care of it. There's no guarantee that it will, though! So you should always challenge your assumptions (e.g. maybe just dropping the columns could give you similar results, and train faster).\n"", '\n', ""Also bear in mind we'll need to use the same values we're using to impute on the training set on the test set. ""]",describ kernel column float64 default due presenc nan valu fact nan might meaning context complet replac call imput sound like great idea howev add boolean column mark column replac hope train algorithm smart enough take care guarante though alway challeng assumpt e g mayb drop column could give similar result train faster also bear mind need use valu use imput train set test set
2532,"[""Now that we've removed all the `NaN` values, we can downcast the columns to the lowest precision.\n"", '\n', ""First we'll need to know which columns are integers, though! The following snippet does just that.""]",remov nan valu downcast column lowest precis first need know column integ though follow snippet
2533,"[""There will be some errors printed, but that's normal because some numeric columns are already integers. I'm too lazy to fix that right now.\n"", '\n', ""Let's look at some stats.""]",error print normal numer column alreadi integ lazi fix right let look stat
2534,"[""So we can see here that there are some very small ranges there -- not all of them will need to be `float64`. Let's downcast them.""]",see small rang need float64 let downcast
2535,"['Looks like we shaved a whole gig.\n', '\n', ""We're not done yet, we need to make sure we do the same thing on the test set. Let's read it in now, merge the tables, and impute it the same way.""]",look like shave whole gig done yet need make sure thing test set let read merg tabl imput way
2536,"['We added some columns in our training set and replaced missing values with the medians. We need to add those same columns, and also add the median from the training set for those missing values (the same ones).']",ad column train set replac miss valu median need add column also add median train set miss valu one
2537,"[""Unfortunately, we're not done yet. We might have some missing values on other numeric columns we didn't anticipate. \n"", '\n', 'In practice, we don\'t always have a ""test set"". The test set might be new observations that come in the future, could be a list of observations or a single one, so we can\'t really use statistics from the test set to impute the missing values. So we need to use values from the training set.\n', '\n', ""We'll use the median as well.""]",unfortun done yet might miss valu numer column anticip practic alway test set test set might new observ come futur could list observ singl one realli use statist test set imput miss valu need use valu train set use median well
2538,"['Now, we can downcast numeric columns in the same way']",downcast numer column way
2539,"['## Categorical values\n', '\n', ""Okay, we've reduced memory, but we still need to deal with categorical values. As machine learning algorithms don't understand things like strings, we need to convert them into numbers. This is called encoding.\n"", '\n', ""We could use either label encoding, which replaces each category into a numerical representation, or use one hot encoding which creates a separate column for each category. In general, one hot encoding performs better. However, in this case we have columns with very high cardinality -- and since we have a large dataset, it's probably more practical to use label encoding which we'll do.\n"", '\n', ""Two things we need to deal with for label encoding are missing values and unknown values. Missing values means the data is simply not there, whereas unknown values are values in the test set that we don't have in the training set.\n"", '\n', 'For missing values, we\'ll just replace them with a label, e.g. the string `""missing""`. That\'s pretty straightforward.\n', '\n', 'For unknown values, that requires a bit more thought. The main question is whether or not we have all the categories a priori. If we know all the possible categories beforehand (i.e. fixed categories like gender, state, postcodes) then we can go ahead and devise a mapping beforehand for all possible values. However, sometimes categories only come in the future, like mobile phone models. In the latter case, we have no way of knowing all the possible future values, and thus we can\'t map them -- so we\'ll need another strategy, i.e. replace them with a different label like the string `""unknown""`. We\'ll be doing that.']",categor valu okay reduc memori still need deal categor valu machin learn algorithm understand thing like string need convert number call encod could use either label encod replac categori numer represent use one hot encod creat separ column categori gener one hot encod perform better howev case column high cardin sinc larg dataset probabl practic use label encod two thing need deal label encod miss valu unknown valu miss valu mean data simpli wherea unknown valu valu test set train set miss valu replac label e g string miss pretti straightforward unknown valu requir bit thought main question whether categori priori know possibl categori beforehand e fix categori like gender state postcod go ahead devi map beforehand possibl valu howev sometim categori come futur like mobil phone model latter case way know possibl futur valu thu map need anoth strategi e replac differ label like string unknown
2540,"['First, we\'ll replace missing values with the string `""missing""` (we actually don\'t need to do this since pandas does it automatically, but I like to give it an explicit label, makes it easier to see).']",first replac miss valu string miss actual need sinc panda automat like give explicit label make easier see
2541,"[""Next we'll convert the columns in the training set to categorical.""]",next convert column train set categor
2542,"[""Then we'll convert the test set.""]",convert test set
2543,"[""Now we're more or less done with the minimum preprocessing required. Let's save our progress to a feather file, so that we don't have to go through it again!""]",le done minimum preprocess requir let save progress feather file go
2544,"['## Validation set\n', '\n', ""Now we can start training our model. But how do we know if a model is good or not? We commonly use something called a validation set, that is separate to the test set. The reason we have a holdout set is that we use the validation set to choose our model (even if we don't use it for training), otherwise our model will overfit. If you're unfamiliar with this, I suggest reading on overfitting and underfitting.\n"", '\n', ""The data description seems to indicate that the data is time ordered, so we don't really want a random split. So let's hold out a portion of the bottom rows to use as our validation set, and the rest as our training set.""]",valid set start train model know model good commonli use someth call valid set separ test set reason holdout set use valid set choos model even use train otherwis model overfit unfamiliar suggest read overfit underfit data descript seem indic data time order realli want random split let hold portion bottom row use valid set rest train set
2545,"['## Training the model\n', '\n', 'Now we can finally train a model. You can iterate on this part.']",train model final train model iter part
2546,"['Using the whole training set is too time-consuming for quick iteration, so we can use a sample. \n', '\n', ""We could use a random sample, but since this is time-ordered, I'm guessing the more recent rows would give us better predictive value. So let's just grab the bottom rows.""]",use whole train set time consum quick iter use sampl could use random sampl sinc time order guess recent row would give u better predict valu let grab bottom row
2547,['## Submission'],submiss
2548,"['Now that we have a decent model, we can actually train on the whole dataset, including the validation set.']",decent model actual train whole dataset includ valid set
2549,"['Thanks to  \n', 'https://www.kaggle.com/iasnobmatsu/xgb-model-with-feature-engineering']",thank http www kaggl com iasnobmatsu xgb model featur engin
2550,"['For StackNet stacking, thanks to   \n', 'https://www.kaggle.com/carlolepelaars/ensembling-with-stacknet']",stacknet stack thank http www kaggl com carlolepelaar ensembl stacknet
2551,"['Ensemble only atm, stacking is on the way']",ensembl atm stack way
2552,"['### Import libraries and data, reduce memory usage']",import librari data reduc memori usag
2553,"['### Some Feature Engineering\n', '\n', 'drop columns, count encoding, aggregation, fillna']",featur engin drop column count encod aggreg fillna
2554,['### XGB LGB models and training'],xgb lgb model train
2555,['### Stacking models and training'],stack model train
2556,['cut down features to avoid the kernel die'],cut featur avoid kernel die
2557,# Home Credit Default Risk 2018,home credit default risk 2018
2558,__Warning!__ This kernel cannot run on Kaggle: not enough memory. But the code works fine and quickly on the local computer with the same amount of memory.,warn kernel run kaggl enough memori code work fine quickli local comput amount memori
2559,"Based on kernels: 

- https://www.kaggle.com/jsaguiar/updated-0-792-lb-lightgbm-with-simple-features

- https://www.kaggle.com/poohtls/fork-of-fork-lightgbm-with-simple-features",base kernel http www kaggl com jsaguiar updat 0 792 lb lightgbm simpl featur http www kaggl com poohtl fork fork lightgbm simpl featur
2560,## Aggregating datasets,aggreg dataset
2561,### Service functions,servic function
2562,### Aggregating functions,aggreg function
2563,## Cleaning dataset,clean dataset
2564,## Optimization LGBM parameters,optim lgbm paramet
2565,### Optimization and visualisation functions,optim visualis function
2566,### Table for scores,tabl score
2567,### First scores with parameters from Tilii kernel,first score paramet tilii kernel
2568,### New Bayesian Optimization,new bayesian optim
2569,# Home Credit Default Risk 2018,home credit default risk 2018
2570,### Importing all libraries,import librari
2571,## Application train\test,applic train test
2572,### loading,load
2573,### converting categorical features to numeric by frequencies,convert categor featur numer frequenc
2574,### dropping features with small variance,drop featur small varianc
2575,## bureau_balance -> bureau,bureau balanc bureau
2576,"### Bureau_balance: loading, converting to numeric, dropping",bureau balanc load convert numer drop
2577,"### Bureau: loading, converting to numeric, dropping",bureau load convert numer drop
2578,### agregating Bureau_balance features into Bureau dataset,agreg bureau balanc featur bureau dataset
2579,"## installments_payments, credit_card_balance, POS_CASH_balance -> previous_application",instal payment credit card balanc po cash balanc previou applic
2580,"### installments_payments: loading, converting to numeric, dropping",instal payment load convert numer drop
2581,"### credit_card_balance: loading, converting to numeric, dropping",credit card balanc load convert numer drop
2582,"### POS_CASH_balance: loading, converting to numeric, dropping",po cash balanc load convert numer drop
2583,"### previous_application: loading, converting to numeric, dropping",previou applic load convert numer drop
2584,### agregating installments_payments features into previous_application dataset,agreg instal payment featur previou applic dataset
2585,### agregating credit_card_balance features into previous_application dataset,agreg credit card balanc featur previou applic dataset
2586,### agregating POS_CASH_balance features into previous_application dataset,agreg po cash balanc featur previou applic dataset
2587,"## bureau_agg,  previous_application_agg -> application train\test

This block of cells runs more than rest of one hour. Drop the comments before using.",bureau agg previou applic agg applic train test block cell run rest one hour drop comment use
2588,# A collection of useful (for me) functions,collect use function
2589,"This is a collection of scripts which can be useful for this and next competitions, as I think.

There is an example of baseline at the end of this notebook.",collect script use next competit think exampl baselin end notebook
2590,## Service functions,servic function
2591,## For EDA,eda
2592,## For cross-validation,cross valid
2593,## For blending predictions,blend predict
2594,# Example of baseline,exampl baselin
2595,This is just an example!,exampl
2596,### Loading datasets,load dataset
2597,"### Convert categorical features

Only Label encoding for this example",convert categor featur label encod exampl
2598,### Exploring missing values,explor miss valu
2599,To drop `EXT_SOURCE_1` feature if it's not usefull in next explorations,drop ext sourc 1 featur useful next explor
2600,### Exploring correlation of features between the train set and target,explor correl featur train set target
2601,### Exploring correlation of features between the train and test sets,explor correl featur train test set
2602,### Selection the best classic model for this dataset,select best classic model dataset
2603,"The most interesting model is LGBM with the first draft score .757. 

The least interesting one is KNearest.",interest model lgbm first draft score 757 least interest one knearest
2604,### Calculating the first metrics without Bayesian Optimization,calcul first metric without bayesian optim
2605,To submit `pred_test` prediction and manually add real LB score in the next cell.,submit pred test predict manual add real lb score next cell
2606,### Calculating the metrics with Bayesian Optimization (initial seeds),calcul metric bayesian optim initi seed
2607,You can select the best seed for Bayesian Optimization and for CV. I cannot do it in this example.,select best seed bayesian optim cv exampl
2608,### Blending predictions,blend predict
2609,"Neural Network with convolution over previous applications and bureau credit reports.

This notebook attempts to predict the defaulters in the competition with a neural network without aggregating data. Appart from the main input with details on the current loan application the details of previous activity will be fed with two auxiliary inputs for both Bureau Credit Reports and Previous Applications.

Preprocessing is limited to standarization, categorical encodings and building the tensors that will feed the neural network. Other than those transformation steps there is no feature engeneering and no aggregation of input tables.

At this point only Current Application, Bureau and Previous Application data is included. This model does not include Installments, Credit Card Balance, Pos Cash Balance or Bureau Balance details.",neural network convolut previou applic bureau credit report notebook attempt predict default competit neural network without aggreg data appart main input detail current loan applic detail previou activ fed two auxiliari input bureau credit report previou applic preprocess limit standar categor encod build tensor feed neural network transform step featur engen aggreg input tabl point current applic bureau previou applic data includ model includ instal credit card balanc po cash balanc bureau balanc detail
2610,This function takes care of the categorical encoding and standarization for non categorical data. Setting combined to true and passing two DF to it allows for combined treatment of train/test data.,function take care categor encod standar non categor data set combin true pas two df allow combin treatment train test data
2611,"The network will have a main input (IE application_train.csv) and two additional inputs for previous products (Previous Application and Bureau Data). 

The main input will have an ordinary input matrix of the form (samples, features)

The two additional inputs have a many-to-one relationship with the main dataset, hence the format will be the following: (samples, products, features). The additional dimension, products, identifies the previous applications or bureau reports.

A similar approach can be used and extend this to the next level of detail (credit card balances, installment payments, etc. ). That last level is not implemented in this notebook.

The following function, generate_conv_tensor_simple, builds the additional input tensors as an ndarray. The number of products to consider for each sample is capped (at 24) because the tensor needs a predefined size for convolutions. 

After building the input matrices and tensors they will be stored in H5Py format which Keras can use. Using ndarrays directly was unfeasible in terms of memory (especially when attempting to implement the next level of detail and include data from the more detailed sources). Alternatively a generator/yield scheme could be used but resulted much slower than the h5py method.

The input data frames to the following function need to be pre sorted so as to iterate only once on the samples in the main data source and each detailed source (Previous Application and Bureau).",network main input ie applic train csv two addit input previou product previou applic bureau data main input ordinari input matrix form sampl featur two addit input mani one relationship main dataset henc format follow sampl product featur addit dimens product identifi previou applic bureau report similar approach use extend next level detail credit card balanc instal payment etc last level implement notebook follow function gener conv tensor simpl build addit input tensor ndarray number product consid sampl cap 24 tensor need predefin size convolut build input matric tensor store h5pi format kera use use ndarray directli unfeas term memori especi attempt implement next level detail includ data detail sourc altern gener yield scheme could use result much slower h5pi method input data frame follow function need pre sort iter sampl main data sourc detail sourc previou applic bureau
2612,"The following custom callback calculates the AUC at the end of each epoch, deals with early stopping and saves the weights to disk if the auc is the best so far. Next to that there is an auc_m custom metric that calculates the same within training but tends to differ a bit from the sklearn numbers. ",follow custom callback calcul auc end epoch deal earli stop save weight disk auc best far next auc custom metric calcul within train tend differ bit sklearn number
2613,"I would like to describe the main concept, how I am going to start working on analyzing and prediction. As a first stage it would be good to try to use the simplest way to analyze and train data, as you can spend plenty of time to developing new features and analyzing data and as result to get just overfitting model.


Any feedback will be nice",would like describ main concept go start work analyz predict first stage would good tri use simplest way analyz train data spend plenti time develop new featur analyz data result get overfit model feedback nice
2614,"I will use data from competition Home Credit Defaul Risk. 
Steps:
1. add library
2. load data
3. look what kind of data we have",use data competit home credit defaul risk step 1 add librari 2 load data 3 look kind data
2615,"To get the first view of data we have, we can use the following commands:
- df.describe() 
- df.head() # shows several rows
- df.shape() 
- df.columns() # name of columns
- df.info() # data type",get first view data use follow command df describ df head show sever row df shape df column name column df info data type
2616,"The structure of the data with dependencies is on the image from competition page
![](https://storage.googleapis.com/kaggle-media/competitions/home-credit/home_credit.png)",structur data depend imag competit page http storag googleapi com kaggl medium competit home credit home credit png
2617,Explore data using graph,explor data use graph
2618,"Let's devide application train columns to several groups:
    1. Client personal information (everything connected with: gender, education, family, phones...)
    2. Client's registration and locations (registration ratings, dates, matches)
    3. Client's property (car, realty)
    4. Client credit information (everything about loan)
I think it would be usefull to see how much money people requested, depends on thier status/education/family size/work
* NAME_CONTRACT_TYPE
* CNT_CHILDREN
* NAME_FAMILY_STATUS
* NAME_INCOME_TYPE
* NAME_EDUCATION_TYPE
* NAME_FAMILY_STATUS
* OCCUPATION_TYPE",let devid applic train column sever group 1 client person inform everyth connect gender educ famili phone 2 client registr locat registr rate date match 3 client properti car realti 4 client credit inform everyth loan think would useful see much money peopl request depend thier statu educ famili size work name contract type cnt child name famili statu name incom type name educ type name famili statu occup type
2619,"There are several methods to convert categorical variable: 
1. pandas.get_dummies(data, prefix=None, prefix_sep='_', dummy_na=False, columns=None, sparse=False, drop_first=False, dtype=None)

*Convert categorical variable into dummy/indicator variables*
2. pandas.factorize(values, sort=False, order=None, na_sentinel=-1, size_hint=None) 

*Encode the object as an enumerated type or categorical variable*

For memory efficient usage:
1.  Let free_raw_data=True (default is True) when constructing the Dataset
2.  Explicit set raw_data=None after the Dataset has been constructed
3.  Call gc",sever method convert categor variabl 1 panda get dummi data prefix none prefix sep dummi na fals column none spar fals drop first fals dtype none convert categor variabl dummi indic variabl 2 panda factor valu sort fals order none na sentinel 1 size hint none encod object enumer type categor variabl memori effici usag 1 let free raw data true default true construct dataset 2 explicit set raw data none dataset construct 3 call gc
2620,"Machine learning models are often considered as black boxes - we set (or guess?) some parameters, give some input data and receive predictions. But in reality those boxes are mostly not completely black. 
Purpose of this notebook is to show some possibilities to view inside the black box and see little bit of what's inside...

The main questions addressed here:
* **Which features** are the most important in general and which contributed to some particular prediction most?
* **How** are features contributing to some particular predictions (lowering, pushing up)?
* **Why** some particular prediction has the value it has?

Let's take as an example this great kernel with currently highest public LB score: https://www.kaggle.com/poohtls/fork-of-fork-lightgbm-with-simple-features
(Kudos to it's author,  2 kudos to the kernel's author which that kernel was forked from, 4 kudos to the kernel's author even below, etc).

That kernel already has feature importances plot, which gives us some basic impression about, how model is transforming features into predictions. 
But it doesn't provide any possibilities to say something about some individual predictions. Let's try to obtain some intuition for this also.

First we run model to get trained LightGBM Booster...
",machin learn model often consid black box set guess paramet give input data receiv predict realiti box mostli complet black purpos notebook show possibl view insid black box see littl bit insid main question address featur import gener contribut particular predict featur contribut particular predict lower push particular predict valu let take exampl great kernel current highest public lb score http www kaggl com poohtl fork fork lightgbm simpl featur kudo author 2 kudo kernel author kernel fork 4 kudo kernel author even etc kernel alreadi featur import plot give u basic impress model transform featur predict provid possibl say someth individu predict let tri obtain intuit also first run model get train lightgbm booster
2621,"Once we have trained Booster, we can get feature importances. LightGBM provides 2 types of feature impotances:
* split - numbers of times the feature is used in a model;
* gain - total gains of splits which use the feature.

Let's look at both of them to see if there are any differences...",train booster get featur import lightgbm provid 2 type featur impot split number time featur use model gain total gain split use featur let look see differ
2622,"What we see is that ""NEW_EXT_SOURCES_MEAN"" feature is dominating in the importance of gain. Probably this feature typically has splits in lowest levels of trees. We can check this assumption by plotting some random trees. Below is plot of one tree splits and yes - ""NEW_EXT_SOURCES_MEAN"" is at the root of the tree. You can try viewing some other randomly chosen trees by changing tree_index parameter, and mostly you'll see something similar.

**Note:** in Notebook plotted tree is unreadable due to limited size of image... to see full graph, do right click on the image and view it from there.",see new ext sourc mean featur domin import gain probabl featur typic split lowest level tree check assumpt plot random tree plot one tree split ye new ext sourc mean root tree tri view randomli chosen tree chang tree index paramet mostli see someth similar note notebook plot tree unread due limit size imag see full graph right click imag view
2623,"I guess we already have obtained some high level intuition about how model is using features to make predictions. Now let's go deeper and take a look at some individual predictions. We'll use SHAP library (https://github.com/slundberg/shap) for that because of it's nice graphics.

Let's make force plot of first prediction and see what we can say from it...",guess alreadi obtain high level intuit model use featur make predict let go deeper take look individu predict use shap librari http github com slundberg shap nice graphic let make forc plot first predict see say
2624,"What can we see:
* Predicted value for this item is high, so it is to be most likely a default (and it is, as we see truth is 1).
* Main features pushing this prediction closer to 1 (in red color) are ""NEW_EXT_SOURCES_MEAN"", ""EXT_SOURCES_3"" and ""DEF_30_CNT_SOCIAL_CIRCLE"".
* Main features pushing this prediction closer to 0 (in blue color) are ""DAYS_BIRTH"" and ""LIVINGAREA_MODE"".
    
So basically, from this we could even generate some automatic response to the client, which would tell something like - *""Even if your age is ok and you live in good area, your external scores are too low and risk to have credit default is too high - so sorry, we can't give you credit this time""*. 

Ok, let's plot some more cases...",see predict valu item high like default see truth 1 main featur push predict closer 1 red color new ext sourc mean ext sourc 3 def 30 cnt social circl main featur push predict closer 0 blue color day birth livingarea mode basic could even gener automat respons client would tell someth like even age ok live good area extern score low risk credit default high sorri give credit time ok let plot case
2625,"Predicted value is low, we can give credit here. So, what could we say to this client? *""Great, you have good credit/annuity ratio, good total income and higher education, therefore risk of credit default is low enough to receive credit!""*. 

One more?",predict valu low give credit could say client great good credit annuiti ratio good total incom higher educ therefor risk credit default low enough receiv credit one
2626,"*""You have some problems with employment, but your external scores and annuity to income ratio is good, so we can give you a credit!""*. 

Ok, generating responses was fun. Let's look what more can we get out of this library. Let's make force plot for more data...",problem employ extern score annuiti incom ratio good give credit ok gener respons fun let look get librari let make forc plot data
2627,"Graph is interactive - when moving mouse over it, for that specific point info is shown about what feature values are pushing predictions up, and which - down. Playing with the graph a bit helps to get some impression about how model is using features and to see if there are some strange decisions made by model (e.g. that some ID value is used to push prediction up/down, etc).

More to come, if I'll have time and motivation ;)",graph interact move mous specif point info shown featur valu push predict play graph bit help get impress model use featur see strang decis made model e g id valu use push predict etc come time motiv
2633,"# Home Credit Default Risk

This kernel will contain EDA, visualization, feature engineering and some modelling. Work currently in progress.",home credit default risk kernel contain eda visual featur engin model work current progress
2634,"There are several files with data, let's go through them step by step.",sever file data let go step step
2635,## Data Exploration,data explor
2636,"### application_train and application_test
These are main files with data and technically we can use only them to make predictions. Obviously using additional data is necessary to improve score.",applic train applic test main file data technic use make predict obvious use addit data necessari improv score
2637,We have 122 columns in just main file! Let's take a look on some of them.,122 column main file let take look
2638,#### Categorical features,categor featur
2639,##### Target,target
2640,"We have disbalanced target, though disbalance isn't really serious.",disbalanc target though disbal realli seriou
2641,##### NAME_CONTRACT_TYPE,name contract type
2642,We can see that there are two types of contract - cash loans and revolving loans. Most of the loans are cash loans which are defaulted.,see two type contract cash loan revolv loan loan cash loan default
2643,##### CODE_GENDER,code gender
2644,"We can see that women take more loans and higher percentage of them repays the loans. And there are 4 people with unindentified gender, who repayed their loans :)",see woman take loan higher percentag repay loan 4 peopl unindentifi gender repay loan
2645,##### FLAG_OWN_CAR and FLAG_OWN_REALTY,flag car flag realti
2646,##### CNT_CHILDREN and NAME_FAMILY_STATUS,cnt child name famili statu
2647,We can see that most of the people are married and have zero children. In face we can divide people into two group based on their family status - living together with their partner or single.,see peopl marri zero child face divid peopl two group base famili statu live togeth partner singl
2648,It isn't surprising that there are a lot of families consisting of two or one adults. Also there are families with two adults and 1-2 children.,surpris lot famili consist two one adult also famili two adult 1 2 child
2649,"##### NAME_TYPE_SUITE
This feature shows who was accompanying client when he was applying for the loan.",name type suit featur show accompani client appli loan
2650,"It is interesting to see that these two variables sometimes contradict each other. For example, separated, single or widowed applicants were sometimes accompanied by their partner. I suppose this means unofficial relationships? Also sometimes children accompanied the applicant. Maybe these were adult childred?",interest see two variabl sometim contradict exampl separ singl widow applic sometim accompani partner suppos mean unoffici relationship also sometim child accompani applic mayb adult childr
2651,##### NAME_INCOME_TYPE,name incom type
2652,"We can see that there are 4 categories with little amount of people in them: several high-income businessmen, 4 women and 1 man on maternity leave, and some unemployed/students. It is quite interesting that unemployed/students have quite a high income.
And of course, most of the people work.",see 4 categori littl amount peopl sever high incom businessmen 4 woman 1 man matern leav unemploy student quit interest unemploy student quit high incom cours peopl work
2653,"##### AMT_GOODS_PRICE
For consumer loans it is the price of the goods for which the loan is given",amt good price consum loan price good loan given
2654,So this means that only 278 loans have some other type. Let's fo deeper.,mean 278 loan type let fo deeper
2655,"We can see that most of the loans have the amount which is similar to the goods price, but there are some outliers.",see loan amount similar good price outlier
2656,##### NAME_HOUSING_TYPE,name hous type
2657,"##### Contact information
There are 6 features showing that client provided some contact information, let's see how many ways of contact clients usually provide.",contact inform 6 featur show client provid contact inform let see mani way contact client usual provid
2658,"Most clients provide 3 ways to contact them and usually minimus is 2, if we don't consider several people who left only 1.",client provid 3 way contact usual minimu 2 consid sever peopl left 1
2659,"# deliquencies

It is very important to see how many times clients was late with payments or defaulted his loans. I suppose info about his social circle is also important. I'll divide values into 2 groups: 0, 1 and more than 1.",deliqu import see mani time client late payment default loan suppos info social circl also import divid valu 2 group 0 1 1
2660,#### Continuous variables,continu variabl
2661,##### AMT_INCOME_TOTAL,amt incom total
2662,"We can see following things from the information above:
- income feature has some huge outliers. This could be due to rich individuals or due to errors in data;
- average income is almost similar for those who repay the loans and those who don't;
- if we leave only data within 90 percentile, it is almost normally distributed;
- log transformation also helps;",see follow thing inform incom featur huge outlier could due rich individu due error data averag incom almost similar repay loan leav data within 90 percentil almost normal distribut log transform also help
2663,##### AMT_CREDIT,amt credit
2664,"This feature shows the amount of the loan in question.
We can see following things from the information above:
- income feature has some outliers. Maybe mortgage?;
- average credit amoint is almost similar for those who repay the loans and those who don't;
- if we leave only data within 95 percentile, it is almost normally distributed;
- log transformation also helps;",featur show amount loan question see follow thing inform incom featur outlier mayb mortgag averag credit amoint almost similar repay loan leav data within 95 percentil almost normal distribut log transform also help
2665,##### DAYS_BIRTH,day birth
2666,We can see that age distribution is almost normal and most of the people are between 30 and 40 years.,see age distribut almost normal peopl 30 40 year
2667,##### DAYS_EMPLOYED,day employ
2668,"Ther was a strange value - 365243, it could mean empty values or some errors, so I replace it with zero.
A lot of people don't work, but let's look deeper into this.",ther strang valu 365243 could mean empti valu error replac zero lot peopl work let look deeper
2669,"Well, it seems that a lot of non-working people are pensioners, which is normal. As for working people - they seem to work for several years at one place.",well seem lot non work peopl pension normal work peopl seem work sever year one place
2670,Ther are so many features and so many possible angles from which we can analyze them. Let's see this for example:,ther mani featur mani possibl angl analyz let see exampl
2671,We can see that most of the loans are taken by working people with secondary education.,see loan taken work peopl secondari educ
2672,## Transforming and merging data,transform merg data
2673,"## Basic modelling, LGB",basic model lgb
2674,"This was EDA and basic feature engineering. I know that feature engineering and modelling could be much better, but decided to make EDA the main focus of this kernel. I'll do better feature engineering and modelling in the next one.",eda basic featur engin know featur engin model could much better decid make eda main focu kernel better featur engin model next one
2675,">[this kernel](https://www.kaggle.com/shivamb/homecreditrisk-extensive-eda-baseline-0-772) had provided with initial exploration ideas

>please suggest some of your methods to improve my exploration technique

> Successfully visualised all of the columns (220 in total).",kernel http www kaggl com shivamb homecreditrisk extens eda baselin 0 772 provid initi explor idea plea suggest method improv explor techniqu success visualis column 220 total
2676,## The idea of directory that I am working in,idea directori work
2677,## The data files present in that directory,data file present directori
2678,## Important imports,import import
2679,## The function to plot the distribution of the categorical values Horizontaly,function plot distribut categor valu horizontali
2680,## Function to get the distribution of the categories according to the target,function get distribut categori accord target
2681,## Function to explore the numeric data,function explor numer data
2682,## Reading the first data file,read first data file
2683,## Got the idea about its ,got idea
2684,## The number of rows and columns we have in the *application_train.csv*,number row column applic train csv
2685,## Target ,target
2686,"> Target variable (1 - client with payment difficulties: he/she had late payment more than X days on at least one of the first Y installments of the loan in our sample, 0 - all other cases)",target variabl 1 client payment difficulti late payment x day least one first instal loan sampl 0 case
2687,"> We see that the 8.07% (24,825) of the clients have difficulties while repayment of the loan",see 8 07 24 825 client difficulti repay loan
2688,## *NAME_CONTRACT_TYPE* :- IDENTIFICATION IF THE LOAN IS CASH OR REVOLVING,name contract type identif loan cash revolv
2689,"> 9.52% of the loan is the revolving loan
>90.5% of the loan is the cash loan",9 52 loan revolv loan 90 5 loan cash loan
2690,## *CODE_GENDER* :- Gender of the Client,code gender gender client
2691,> We see that there are 3 gender categories and may be the XNA is the null value,see 3 gender categori may xna null valu
2692,## *FLAG_OWN_CAR*:- Flag if the client owns a car,flag car flag client own car
2693,">66% do not own car while only 34% own car

",66 car 34 car
2694,> people with no cars have more repayment difficulties,peopl car repay difficulti
2695,## *FLAG_OWN_REALTY* :- Flag if client owns a house or a flat ,flag realti flag client own hous flat
2696,## *CNT_CHILDREN*:-Number of children the client has,cnt child number child client
2697,## *AMT_INCOME_TOTAL*:- Income of the Client,amt incom total incom client
2698,## *AMT_CREDIT*:- Credit Amount of Loan ,amt credit credit amount loan
2699,## *AMT_ANNUITY*:-Loan Annuity,amt annuiti loan annuiti
2700,## *AMT_GOODS_PRICE*:-For consumer loans it is the price of the goods for which the loan is given,amt good price consum loan price good loan given
2701,## *NAME_TYPE_SUITE*:-Who was accompanying client when he was applying for the loan,name type suit accompani client appli loan
2702,"## *NAME_INCOME_TYPE*:-Clients income type(Business, Maternity,...) ",name incom type client incom type busi matern
2703,## *NAME_EDUCATION_TYPE*:- Level of Highest Education the Client Achieved,name educ type level highest educ client achiev
2704,## *NAME_FAMILY_STATUS*:-Family Status of the client,name famili statu famili statu client
2705,"## *NAME_HOUSING_TYPE*:-What is the Housing Situation of the Client (renting, living with parents,...)",name hous type hous situat client rent live parent
2706,## *REGION_POPULATION_RELATIVE*:-Normalised Population of the Regions where Clients live (higher number more populated region),region popul rel normalis popul region client live higher number popul region
2707,## *DAYS_BIRTH*:-Client 's Age in days at the time of application,day birth client age day time applic
2709,## *DAYS_EMPLOYED*:-How many days before the application the client started current employement,day employ mani day applic client start current employ
2710,## *DAYS_REGISTRATION*:- How many days before the application did the client change his registration ,day registr mani day applic client chang registr
2711,## *OWN_AGE_CAR*:- Age of clients car,age car age client car
2712,## *FLAG_MOBIL*:- Did client provide mobile phone,flag mobil client provid mobil phone
2713,## *FLAG_EMP_PHONE*:- Did client provide Work phone,flag emp phone client provid work phone
2714,"## *FLAG_WORK_PHONE*:- Did client provide home phone
",flag work phone client provid home phone
2715,"
## *FLAG_CONT_MOBILE*:-Was mobile phone reachable",flag cont mobil mobil phone reachabl
2716,## *FLAG_PHONE*:- Did client provide home phone,flag phone client provid home phone
2717,## *FLAG_EMAIL*:- Did client provide email,flag email client provid email
2718,"
## *OCCUPATION_TYPE*:-What kind of occupation does the client have",occup type kind occup client
2719,## *CNT_FAM_MEMBERS*:- How many family member does the client have,cnt fam member mani famili member client
2720,## *REGION_RATING_CLIENT*:-Our(their) rating of the region where client lives,region rate client rate region client live
2721,"
## *REGION_RATING_CLIENT_W_CITY*:-Our rating of the region where client lives with taking city into account (1,2,3)
",region rate client w citi rate region client live take citi account 1 2 3
2722,"## *REGION_RATING_CLIENT_W_CITY*:-On which day of the week did the client apply for the loan
",region rate client w citi day week client appli loan
2723,"## *HOUR_APPR_PROCESS_START*:-Approximately at what hour did the client apply for the loan
",hour appr process start approxim hour client appli loan
2724,"## *REG_REGION_NOT_LIVE_REGION*:- Flag if client's permanent address does not match contact address (1=different, 0=same, at region level)",reg region live region flag client perman address match contact address 1 differ 0 region level
2725,"
## REG_REGION_NOT_WORK_REGION:- Flag if client's permanent address does not match work address (1=different, 0=same, at region level)",reg region work region flag client perman address match work address 1 differ 0 region level
2726,"## *LIVE_REGION_NOT_WORK_REGION*:-Flag if client's contact address does not match work address (1=different, 0=same, at region level)",live region work region flag client contact address match work address 1 differ 0 region level
2727,"## *REG_CITY_NOT_LIVE_CITY* :- Flag if client's permanent address does not match contact address (1=different, 0=same, at city level)",reg citi live citi flag client perman address match contact address 1 differ 0 citi level
2728,"## *REG_CITY_NOT_LIVE_CITY* :- Flag if client's permanent address does not match work address (1=different, 0=same, at city level)",reg citi live citi flag client perman address match work address 1 differ 0 citi level
2729,"
## *LIVE_CITY_NOT_WORK_CITY*:- Flag if client's contact address does not match work address (1=different, 0=same, at city level)",live citi work citi flag client contact address match work address 1 differ 0 citi level
2730,## *ORGANIZATION_TYPE*:- Type of organization where client works,organ type type organ client work
2731,"
## *EXT_SOURCE_1*:- Normalized score from external data source",ext sourc 1 normal score extern data sourc
2732,# *EXT_SOURCE_2* :-Normalized score from external data source,ext sourc 2 normal score extern data sourc
2733,## *EXT_SOURCE_3*:- Normalized score from external data source,ext sourc 3 normal score extern data sourc
2734,"## *APARTMENTS_AVG*:-Normalized information about building where the client lives, What is average (_AVG suffix), modus (_MODE suffix), median (_MEDI suffix) apartment size, common area, living area, age of building, number of elevators, number of entrances, state of the building, number of floor",apart avg normal inform build client live averag avg suffix modu mode suffix median medi suffix apart size common area live area age build number elev number entranc state build number floor
2735,"## *BASEMENTAREA_AV*:-Normalized information about building where the client lives, What is average (_AVG suffix), modus (_MODE suffix), median (_MEDI suffix) apartment size, common area, living area, age of building, number of elevators, number of entrances, state of the building, number of floor",basementarea av normal inform build client live averag avg suffix modu mode suffix median medi suffix apart size common area live area age build number elev number entranc state build number floor
2736,"## *YEARS_BEGINEXPLUATATION_AVG*:-Normalized information about building where the client lives, What is average (_AVG suffix), modus (_MODE suffix), median (_MEDI suffix) apartment size, common area, living area, age of building, number of elevators, number of entrances, state of the building, number of floor",year beginexpluat avg normal inform build client live averag avg suffix modu mode suffix median medi suffix apart size common area live area age build number elev number entranc state build number floor
2737,"## *YEARS_BUILD_AVG*:-Normalized information about building where the client lives, What is average (_AVG suffix), modus (_MODE suffix), median (_MEDI suffix) apartment size, common area, living area, age of building, number of elevators, number of entrances, state of the building, number of floor",year build avg normal inform build client live averag avg suffix modu mode suffix median medi suffix apart size common area live area age build number elev number entranc state build number floor
2738,">Normalized information about building where the client lives, What is average (_AVG suffix), modus (_MODE suffix), median (_MEDI suffix) apartment size, common area, living area, age of building, number of elevators, number of entrances, state of the building, number of floor
",normal inform build client live averag avg suffix modu mode suffix median medi suffix apart size common area live area age build number elev number entranc state build number floor
2739,## *OBS_30_CNT_SOCIAL_CIRCLE*:-How many observation of client's social surroundings with observable 30 DPD (days past due) default,ob 30 cnt social circl mani observ client social surround observ 30 dpd day past due default
2740,## *DEF_30_CNT_SOCIAL_CIRCLE*:-How many observation of client's social surroundings defaulted on 30 DPD (days past due) ,def 30 cnt social circl mani observ client social surround default 30 dpd day past due
2741,## *OBS_60_CNT_SOCIAL_CIRCLE*:-How many observation of client's social surroundings with observable 60 DPD (days past due) default,ob 60 cnt social circl mani observ client social surround observ 60 dpd day past due default
2742,## *DEF_60_CNT_SOCIAL_CIRCLE*:-How many observation of client's social surroundings defaulted on 60 (days past due) DPD,def 60 cnt social circl mani observ client social surround default 60 day past due dpd
2743,## *DAYS_LAST_PHONE_CHANGE* = How many days before application did client change phone,day last phone chang mani day applic client chang phone
2744,"> Did the client provide the list of documents (Document no.2,...Document no. 21)",client provid list document document 2 document 21
2745,## *AMT_REQ_CREDIT_BUREAU_HOUR*:-Number of enquiries to Credit Bureau about the client one hour before application,amt req credit bureau hour number enquiri credit bureau client one hour applic
2746,## *AMT_REQ_CREDIT_BUREAU_DAY*:-Number of enquiries to Credit Bureau about the client one day before application (excluding one hour before application),amt req credit bureau day number enquiri credit bureau client one day applic exclud one hour applic
2747,## *AMT_REQ_CREDIT_BUREAU_WEEK*:-Number of enquiries to Credit Bureau about the client one week before application (excluding one day before application),amt req credit bureau week number enquiri credit bureau client one week applic exclud one day applic
2748, ## *AMT_REQ_CREDIT_BUREAU_MON*:-Number of enquiries to Credit Bureau about the client one month before application (excluding one week before application),amt req credit bureau mon number enquiri credit bureau client one month applic exclud one week applic
2749,## *AMT_REQ_CREDIT_BUREAU_QRT*:-Number of enquiries to Credit Bureau about the client 3 month before application (excluding one month before application),amt req credit bureau qrt number enquiri credit bureau client 3 month applic exclud one month applic
2750,## *AMT_REQ_CREDIT_BUREAU_YEAR*:-Number of enquiries to Credit Bureau about the client one day year (excluding last 3 months before application),amt req credit bureau year number enquiri credit bureau client one day year exclud last 3 month applic
2751,## *bureau.csv*,bureau csv
2752,## getting a view of the schema,get view schema
2753,"## *SK_ID_CURR*:-ID of loan in our sample - one loan in our sample can have 0,1,2 or more related previous credits in credit bureau  ",sk id curr id loan sampl one loan sampl 0 1 2 relat previou credit credit bureau
2754,## *SK_BUREAU_ID*:-Recoded ID of previous Credit Bureau credit related to our loan (unique coding for each loan application),sk bureau id recod id previou credit bureau credit relat loan uniqu code loan applic
2755,## *CREDIT_ACTIVE*:-Status of the Credit Bureau (CB) reported credits,credit activ statu credit bureau cb report credit
2756,## *CREDIT_CURRENCY*:-Recoded currency of the Credit Bureau credit,credit currenc recod currenc credit bureau credit
2757,## *DAYS_CREDIT*:-How many days before current application did client apply for Credit Bureau credit,day credit mani day current applic client appli credit bureau credit
2758,## *CREDIT_DAY_OVERDUE*:-Number of days past due on CB credit at the time of application for related loan in our sample,credit day overdu number day past due cb credit time applic relat loan sampl
2759,## *DAYS_CREDIT_ENDDATE*:-Remaining duration of CB credit (in days) at the time of application in Home Credit,day credit enddat remain durat cb credit day time applic home credit
2760,## *DAYS_ENDDATE_FACT*:-Days since CB credit ended at the time of application in Home Credit (only for closed credit),day enddat fact day sinc cb credit end time applic home credit close credit
2761,"## ""AMT_CREDIT_MAX_OVERDUE"":-Maximal amount overdue on the Credit Bureau credit so far (at application date of loan in our sample)",amt credit max overdu maxim amount overdu credit bureau credit far applic date loan sampl
2762,## *CNT_CREDIT_PROLONG*:-How many times was the Credit Bureau credit prolonged,cnt credit prolong mani time credit bureau credit prolong
2763,## *AMT_CREDIT_SUM*:-Current credit amount for the Credit Bureau credit,amt credit sum current credit amount credit bureau credit
2764,## *AMT_CREDIT_SUM_DEBT*:-Current debt on Credit Bureau credit,amt credit sum debt current debt credit bureau credit
2765,## *AMT_CREDIT_SUM_LIMIT*:-Current credit limit of credit card reported in Credit Bureau,amt credit sum limit current credit limit credit card report credit bureau
2766,## *AMT_CREDIT_SUM_OVERDUE*:-Current amount overdue on Credit Bureau credit,amt credit sum overdu current amount overdu credit bureau credit
2767,## *AMT_CREDIT_SUM_OVERDUE*:-Current amount overdue on Credit Bureau credit,amt credit sum overdu current amount overdu credit bureau credit
2768,"## *CREDIT_TYPE*:-Type of Credit Bureau credit (Car, cash,...)",credit type type credit bureau credit car cash
2769,   ## *DAYS_CREDIT_UPDATE*:-How many days before loan application did last information about the Credit Bureau credit come,day credit updat mani day loan applic last inform credit bureau credit come
2770,## *AMT_ANNUITY*:-Annuity of the Credit Bureau credit,amt annuiti annuiti credit bureau credit
2771,   > *bureau_balance.csv*:-,bureau balanc csv
2772,## *SK_BUREAU_ID*:-Recoded ID of Credit Bureau credit (unique coding for each application) - use this to join to CREDIT_BUREAU table ,sk bureau id recod id credit bureau credit uniqu code applic use join credit bureau tabl
2773,## *MONTHS_BALANCE*:-Month of balance relative to application date (-1 means the freshest balance date),month balanc month balanc rel applic date 1 mean freshest balanc date
2774,"## *STATUS*:-Status of Credit Bureau loan during the month (active, closed, DPD0-30,… [C means closed, X means status unknown, 0 means no DPD, 1 means maximal did during month between 1-30, 2 means DPD 31-60,… 5 means DPD 120+ or sold or written off ] )",statu statu credit bureau loan month activ close dpd0 30 c mean close x mean statu unknown 0 mean dpd 1 mean maxim month 1 30 2 mean dpd 31 60 5 mean dpd 120 sold written
2775,> *POS_CASH_balance.csv*:-,po cash balanc csv
2776,"## *SK_ID_PREV *:-ID of previous credit in Home Credit related to loan in our sample. (One loan in our sample can have 0,1,2 or more previous loans in Home Credit)",sk id prev id previou credit home credit relat loan sampl one loan sampl 0 1 2 previou loan home credit
2777,## *SK_ID_CURR*:-ID of loan in our sample,sk id curr id loan sampl
2778,"## *MONTHS_BALANCE*:-Month of balance relative to application date (-1 means the information to the freshest monthly snapshot, 0 means the information at application - often it will be the same as -1 as many banks are not updating the information to Credit Bureau regularly )",month balanc month balanc rel applic date 1 mean inform freshest monthli snapshot 0 mean inform applic often 1 mani bank updat inform credit bureau regularli
2779,## *CNT_INSTALMENT*:-Term of previous credit (can change over time),cnt instal term previou credit chang time
2780,## *CNT_INSTALMENT_FUTURE*:-Installments left to pay on the previous credit,cnt instal futur instal left pay previou credit
2781,## *CNT_INSTALMENT_FUTURE*:-Installments left to pay on the previous credit,cnt instal futur instal left pay previou credit
2782,## *NAME_CONTRACT_STATUS*:-Contract status during the month,name contract statu contract statu month
2783,## *SK_DPD*:- DPD (days past due) during the month of previous credit,sk dpd dpd day past due month previou credit
2784,## *SK_DPD_DEF*:- DPD during the month with tolerance (debts with low loan amounts are ignored) of the previous credit,sk dpd def dpd month toler debt low loan amount ignor previou credit
2785,> credit_card_balance.csv,credit card balanc csv
2786,"## *SK_ID_PREV *:-ID of previous credit in Home credit related to loan in our sample. (One loan in our sample can have 0,1,2 or more previous loans in Home Credit)",sk id prev id previou credit home credit relat loan sampl one loan sampl 0 1 2 previou loan home credit
2787,## *SK_ID_CURR*:-ID of loan in our sample,sk id curr id loan sampl
2788,## *MONTHS_BALANCE*:-Month of balance relative to application date (-1 means the freshest balance date),month balanc month balanc rel applic date 1 mean freshest balanc date
2789,## *AMT_BALANCE*:-Balance during the month of previous credit,amt balanc balanc month previou credit
2790,## *AMT_CREDIT_LIMIT_ACTUAL*:-Credit card limit during the month of the previous credit,amt credit limit actual credit card limit month previou credit
2791,## *AMT_DRAWINGS_ATM_CURRENT*:-Amount drawing at ATM during the month of the previous credit,amt draw atm current amount draw atm month previou credit
2792,## *AMT_DRAWINGS_CURRENT*:-Amount drawing during the month of the previous credit,amt draw current amount draw month previou credit
2793,   ## *AMT_DRAWINGS_OTHER_CURRENT*:-Amount of other drawings during the month of the previous credit ,amt draw current amount draw month previou credit
2794,## *AMT_DRAWINGS_POS_CURRENT*:-Amount drawing or buying goods during the month of the previous credit,amt draw po current amount draw buy good month previou credit
2795,## *AMT_INST_MIN_REGULARITY*:-Minimal installment for this month of the previous credit,amt inst min regular minim instal month previou credit
2796,## *AMT_PAYMENT_CURRENT*:-How much did the client pay during the month on the previous credit,amt payment current much client pay month previou credit
2797,## *AMT_PAYMENT_TOTAL_CURRENT*:-How much did the client pay during the month in total on the previous credit,amt payment total current much client pay month total previou credit
2798,## *AMT_RECEIVABLE_PRINCIPAL*:-Amount receivable for principal on the previous credit,amt receiv princip amount receiv princip previou credit
2799,## *AMT_RECIVABLE*:-Amount receivable on the previous credit,amt reciv amount receiv previou credit
2800,## *AMT_TOTAL_RECEIVABLE*:-Total amount receivable on the previous credit,amt total receiv total amount receiv previou credit
2801,## *CNT_DRAWINGS_ATM_CURRENT*:-Number of drawings at ATM during this month on the previous credit,cnt draw atm current number draw atm month previou credit
2802,## *CNT_DRAWINGS_CURRENT*:-Number of drawings during this month on the previous credit,cnt draw current number draw month previou credit
2803,## *CNT_DRAWINGS_OTHER_CURRENT*:-Number of other drawings during this month on the previous credit,cnt draw current number draw month previou credit
2804,## *CNT_DRAWINGS_POS_CURRENT*:-Number of drawings for goods during this month on the previous credit,cnt draw po current number draw good month previou credit
2805,## *CNT_INSTALMENT_MATURE_CUM*:-Number of paid installments on the previous credit,cnt instal matur cum number paid instal previou credit
2806,"## *NAME_CONTRACT_STATUS*:-Contract status (active signed,...) on the previous credit",name contract statu contract statu activ sign previou credit
2807,   ## *SK_DPD*:-DPD (Days past due) during the month on the previous credit,sk dpd dpd day past due month previou credit
2808,## *SK_DPD_DEF*:-DPD (Days past due) during the month with tolerance (debts with low loan amounts are ignored) of the previous credit,sk dpd def dpd day past due month toler debt low loan amount ignor previou credit
2809,> Exploration of *previous_application.csv*,explor previou applic csv
2810,"## *SK_ID_PREV *:- ID of previous credit in Home credit related to loan in our sample. (One loan in our sample can have 0,1,2 or more previous loan applications in Home Credit, previous application could, but not necessarily have to lead to credit) ",sk id prev id previou credit home credit relat loan sampl one loan sampl 0 1 2 previou loan applic home credit previou applic could necessarili lead credit
2811,## *SK_ID_CURR*:- ID of loan in our sample,sk id curr id loan sampl
2812,"   ## *NAME_CONTRACT_TYPE*:-Contract product type (Cash loan, consumer loan [POS] ,...) of the previous application ",name contract type contract product type cash loan consum loan po previou applic
2813,## *AMT_ANNUITY*:-Annuity of previous application,amt annuiti annuiti previou applic
2814,## *AMT_APPLICATION*:-For how much credit did client ask on the previous application,amt applic much credit client ask previou applic
2815,"## *AMT_CREDIT*:-Final credit amount on the previous application. This differs from AMT_APPLICATION in a way that the AMT_APPLICATION is the amount for which the client initially applied for, but during our approval process he could have received different amount - AMT_CREDIT",amt credit final credit amount previou applic differ amt applic way amt applic amount client initi appli approv process could receiv differ amount amt credit
2816,   ## *AMT_DOWN_PAYMENT*:-Down payment on the previous application,amt payment payment previou applic
2817,## *AMT_GOODS_PRICE*:-Goods price of good that client asked for (if applicable) on the previous application,amt good price good price good client ask applic previou applic
2818,## *WEEKDAY_APPR_PROCESS_START*:-On which day of the week did the client apply for previous application,weekday appr process start day week client appli previou applic
2819,## *HOUR_APPR_PROCESS_START*:-Approximately at what day hour did the client apply for the previous application,hour appr process start approxim day hour client appli previou applic
2820,## *FLAG_LAST_APPL_PER_CONTRACT*:-Flag if it was last application for the previous contract. Sometimes by mistake of client or our clerk there could be more applications for one single contract,flag last appl per contract flag last applic previou contract sometim mistak client clerk could applic one singl contract
2821,   ## *NFLAG_LAST_APPL_IN_DAY*:-Flag if the application was the last application per day of the client. Sometimes clients apply for more applications a day. Rarely it could also be error in our system that one application is in the database twice,nflag last appl day flag applic last applic per day client sometim client appli applic day rare could also error system one applic databas twice
2822,## *NFLAG_MICRO_CASH*:-Flag Micro finance loan,nflag micro cash flag micro financ loan
2823,    - No column with name in the table,column name tabl
2824,## *RATE_DOWN_PAYMENT*:-Down payment rate normalized on previous credit,rate payment payment rate normal previou credit
2825,## *RATE_INTEREST_PRIMARY*:-Interest rate normalized on previous credit,rate interest primari interest rate normal previou credit
2826,## *RATE_INTEREST_PRIVILEGED*:-Interest rate normalized on previous credit,rate interest privileg interest rate normal previou credit
2827,## *NAME_CASH_LOAN_PURPOSE*:-Purpose of the cash loan,name cash loan purpos purpos cash loan
2828,"## *NAME_CONTRACT_STATUS*:-Contract status (approved, cancelled, ...) of previous application",name contract statu contract statu approv cancel previou applic
2829,## *DAYS_DECISION*:-Relative to current application when was the decision about previous application made,day decis rel current applic decis previou applic made
2830,## *NAME_PAYMENT_TYPE*:-Payment method that client chose to pay for the previous application,name payment type payment method client chose pay previou applic
2831,## *CODE_REJECT_REASON*:- Why was the previous application rejected,code reject reason previou applic reject
2832,## *NAME_TYPE_SUITE*:- Who accompanied client when applying for the previous application,name type suit accompani client appli previou applic
2833,## *NAME_CLIENT_TYPE*:-Was the client old or new client when applying for the previous application,name client type client old new client appli previou applic
2834,## *NAME_GOODS_CATEGORY*:- What kind of goods did the client apply for in the previous application,name good categori kind good client appli previou applic
2835,"## *NAME_PORTFOLIO*:- Was the previous application for CASH, POS, CAR, …",name portfolio previou applic cash po car
2836,## *NAME_PRODUCT_TYPE*:-Was the previous application x-sell o walk-in,name product type previou applic x sell walk
2837,## *CHANNEL_TYPE*:-Through which channel we acquired the client on the previous application,channel type channel acquir client previou applic
2838,## *SELLERPLACE_AREA*:-Selling area of seller place of the previous application,sellerplac area sell area seller place previou applic
2839,## *NAME_SELLER_INDUSTRY*:-The industry of the seller,name seller industri industri seller
2840,## *CNT_PAYMENT*:-Term of previous credit at application of the previous application,cnt payment term previou credit applic previou applic
2841,## *NAME_YIELD_GROUP*:-Grouped interest rate into small medium and high of the previous application,name yield group group interest rate small medium high previou applic
2842,   ## *PRODUCT_COMBINATION*:-Detailed product combination of the previous application,product combin detail product combin previou applic
2843,## *DAYS_FIRST_DRAWING*:-Relative to application date of current application when was the first disbursement of the previous application,day first draw rel applic date current applic first disburs previou applic
2844,## *DAYS_FIRST_DUE*:-Relative to application date of current application when was the first due supposed to be of the previous application,day first due rel applic date current applic first due suppos previou applic
2845,## *DAYS_LAST_DUE_1ST_VERSION*:-Relative to application date of current application when was the first due of the previous application,day last due 1st version rel applic date current applic first due previou applic
2846,## *DAYS_LAST_DUE*:-Relative to application date of current application when was the last due date of the previous application,day last due rel applic date current applic last due date previou applic
2847,## *DAYS_TERMINATION*:-Relative to application date of current application when was the expected termination of the previous application,day termin rel applic date current applic expect termin previou applic
2848,## *NFLAG_INSURED_ON_APPROVAL*:-Did the client requested insurance during the previous application,nflag insur approv client request insur previou applic
2849,> installments_payments.csv,instal payment csv
2850,"## *SK_ID_PREV *:-ID of previous credit in Home credit related to loan in our sample. (One loan in our sample can have 0,1,2 or more previous loans in Home Credit)",sk id prev id previou credit home credit relat loan sampl one loan sampl 0 1 2 previou loan home credit
2851,## *SK_ID_CURR*:-ID of loan in our sample,sk id curr id loan sampl
2852,## *NUM_INSTALMENT_VERSION*:-Version of installment calendar (0 is for credit card) of previous credit. Change of installment version from month to month signifies that some parameter of payment calendar has changed,num instal version version instal calendar 0 credit card previou credit chang instal version month month signifi paramet payment calendar chang
2853,## *NUM_INSTALMENT_NUMBER*:-On which installment we observe payment,num instal number instal observ payment
2854,## *DAYS_INSTALMENT*:-When the installment of previous credit was supposed to be paid (relative to application date of current loan),day instal instal previou credit suppos paid rel applic date current loan
2855,## *DAYS_ENTRY_PAYMENT*:-When was the installments of previous credit paid actually (relative to application date of current loan),day entri payment instal previou credit paid actual rel applic date current loan
2856,## *AMT_INSTALMENT*:-What was the prescribed installment amount of previous credit on this installment,amt instal prescrib instal amount previou credit instal
2857,## *AMT_PAYMENT*:-What the client actually paid on previous credit on this installment,amt payment client actual paid previou credit instal
2858,## *A brief visualisationof all the columns of this competitions* :),brief visualisationof column competit
2859,"# Introduction: Deep Learning with Embedding Layers


This notebook is intended for those who want an introduction into Embedding Layers with Keras. I choosed not to focus on describing the preprocessing nor the different methods, to merge all the table, but rather to focus more specificaly on how to get started in Embedding.

Embedding is a technique used to encode categorical features like One-Hot encoding or target encoding, it is a bit more difficult to implement but keras allow us to create a model pretty easily.

Embeddings help to generalize better when the data is sparse and statistics is unknown. Thus, it is especially useful for datasets with lots of high cardinality features, where other methods tend to overfit.

Why should we use Entity Embedding instead of One-Hot Encoding ? There are mutiple reasons for that :

*  One-Hot encoded vectors are high-dimensional and sparse. In this dataset we have a feature that represent an organization type (denoted: ORGANIZATION_TYPE) of 58 distinct value . This means that, when using one-hot encoding, this feature will be represented by a vector containing 58 integers. And 57 of these integers are zeros. In a big dataset or in NLP ( Natural Language Processing) when you have more than 2000 outcomes for a feature, this approach is not computationally efficient.


* The vectors of each embedding get updated while training the neural network. This allows us to visualize relationships between words or more generally speaking categories, but also between everything that can be turned into a vector through an embedding layer. Please look at the image below  that show how similarities between categories can be found in a multi-dimensional space.

![](https://cdn-images-1.medium.com/max/1000/1*sXNXYfAqfLUeiDXPCo130w.png)",introduct deep learn embed layer notebook intend want introduct embed layer kera choos focu describ preprocess differ method merg tabl rather focu specificali get start embed embed techniqu use encod categor featur like one hot encod target encod bit difficult implement kera allow u creat model pretti easili embed help gener better data spar statist unknown thu especi use dataset lot high cardin featur method tend overfit use entiti embed instead one hot encod mutipl reason one hot encod vector high dimension spar dataset featur repres organ type denot organ type 58 distinct valu mean use one hot encod featur repres vector contain 58 integ 57 integ zero big dataset nlp natur languag process 2000 outcom featur approach comput effici vector embed get updat train neural network allow u visual relationship word gener speak categori also everyth turn vector embed layer plea look imag show similar categori found multi dimension space http cdn imag 1 medium com max 1000 1 sxnxyfaqflueidxpco130w png
2860,# Prepare the data,prepar data
2861,There is **307511** lines in the train file and **48744** lines in the test files. We have 121 differents features ( I'm deliberating excluding **SK_ID_CURR** which act as an ID and the **TARGET** variable),307511 line train file 48744 line test file 121 differ featur deliber exclud sk id curr act id target variabl
2862,# Variable Type,variabl type
2863,# Label encode the categorical features,label encod categor featur
2864,# Create the network,creat network
2865,In order to create our embedding model we need to have a look at the spatiality of the cat features. We choose here to use Embedding only on cat features that present more than 2 outcomes otherwise it is count as a numeric value (0 or 1).,order creat embed model need look spatial cat featur choos use embed cat featur present 2 outcom otherwis count numer valu 0 1
2866,"We are including 13 features **out of 16 categorical features** into our Embedding.

We can see that our features have a reatively small number of outcomes except for **OCCUPATION_TYPE** and **ORGANIZATION_TYPE** which will be represented in a high dimensional spaces in our Embedding.

The first layer of our network is the embedding layer with the size of 3 ""CODE_GENDER"". The embedding-size defines the dimensionality in which we map the categorical variables (in a 3D spaces for instance). One good rule of thumb to use for the output is : 

**embedding size = min(50, number of categories/2)**",includ 13 featur 16 categor featur embed see featur reativ small number outcom except occup type organ type repres high dimension space embed first layer network embed layer size 3 code gender embed size defin dimension map categor variabl 3d space instanc one good rule thumb use output embed size min 50 number categori 2
2867,"In order for keras to know which features are going to be included into the Embedding layers we need to create a list containing for each feature the corresponding numpy array (**13** in total for us). The last element of the list will be our numerical features (**173**) and the categorical features that we decided not to include in the Embedding (**3**) for a total of **176** distinct features.
",order kera know featur go includ embed layer need creat list contain featur correspond numpi array 13 total u last element list numer featur 173 categor featur decid includ embed 3 total 176 distinct featur
2868,Let us go more specifically into this function : ,let u go specif function
2869,This list will be passed into the network. It is composed of 14 numpy arrays containing our categorical features that are going throught the Embedding Layers (**13 layers**). The last element of the list is a numpy array composed of the **173 numerics features added to the 3 categorical features that have at most 2 distinct outcomes**. ,list pas network compos 14 numpi array contain categor featur go throught embed layer 13 layer last element list numpi array compos 173 numer featur ad 3 categor featur 2 distinct outcom
2870,"# Prepare the data

In neural networks, it is a best practice to scale input data before use. Data scaling
makes the training of the network faster, memory efficient and yield accurate
forecast results. Neural networks only work with data usually between a specified range (1 to 1 or 0 to 1), it makes it necessary then that data is scaled down and normalized. 

Scaling can be as simple as taking the ratios (reciprocal normalization), computing the differences
(range normalization) or multiplicative normalization.
Normalization ensures that data is roughly uniformly distributed between the network inputs
and the outputs.",prepar data neural network best practic scale input data use data scale make train network faster memori effici yield accur forecast result neural network work data usual specifi rang 1 1 0 1 make necessari data scale normal scale simpl take ratio reciproc normal comput differ rang normal multipl normal normal ensur data roughli uniformli distribut network input output
2871,# Train the network,train network
2872,"We can see that the model is performing well with an **AVG AUC of 0.75 on CV5 out-of-fold ** and an **AUC of 0.748 on LB**. 

However I'm having difficulties to perform as well as Boosted Trees like XGBoost, Lgbm and Catboost. If anyone have any hint on how to improve this kernel, please let me know.",see model perform well avg auc 0 75 cv5 fold auc 0 748 lb howev difficulti perform well boost tree like xgboost lgbm catboost anyon hint improv kernel plea let know
2873,"# Introduction
Home Credit strives to broaden financial inclusion for the unbanked population by providing a positive and safe borrowing experience. In order to make sure this underserved population has a positive loan experience, Home Credit makes use of a variety of alternative data--including telco and transactional information--to predict their clients' repayment abilities.

While Home Credit is currently using various statistical and machine learning methods to make these predictions, they're challenging Kagglers to help them unlock the full potential of their data. Doing so will ensure that clients capable of repayment are not rejected and that loans are given with a principal, maturity, and repayment calendar that will empower their clients to be successful.",introduct home credit strive broaden financi inclus unbank popul provid posit safe borrow experi order make sure underserv popul posit loan experi home credit make use varieti altern data includ telco transact inform predict client repay abil home credit current use variou statist machin learn method make predict challeng kaggler help unlock full potenti data ensur client capabl repay reject loan given princip matur repay calendar empow client success
2874,Lets see what are the files we have to explore the data.,let see file explor data
2875,"## How files are sturtured and link between each files and their content is given in the below image, which helps us to understand probelm well.
",file sturtur link file content given imag help u understand probelm well
2876,![](https://storage.googleapis.com/kaggle-media/competitions/home-credit/home_credit.png),http storag googleapi com kaggl medium competit home credit home credit png
2877,Lets read all the fiels and have a glimpse of data.,let read fiel glimps data
2878,## Lets check the missing values in each file.,let check miss valu file
2879,you can also check missing values like this without need of function.,also check miss valu like without need function
2880,# Visuliazation,visuliaz
2881,"from the above image, most of the loans were repayed and less than 5000 were not payed.",imag loan repay le 5000 pay
2882,Little advanced visualization ,littl advanc visual
2883,"who accompanied client when appliying for the loan/application, and their repayment count given below.",accompani client appliy loan applic repay count given
2884,"more in pipe line, this is for beginners and all are basic level and self explanatory only.

if you like it, please upvote for me. 

Thank you : ) ",pipe line beginn basic level self explanatori like plea upvot thank
2885," In this notebook we will try to gain insight into a tree model based on the shap package. To understand why current feature importances calculated by lightGBM, Xgboost and other tree based models have issues read this article:[ Interpretable Machine Learning with XGBoost]( Interpretable Machine Learning with XGBoost). The shap library [https://github.com/slundberg/shap](https://github.com/slundberg/shap) can be used by going to the settings of the notebook (upper right corner,) and  ""add a custom package"" in the settings tab.
 
 The most important plot is the summary plot (below in this notebook), that shows the 30 most important features. For each feature a distribution is plotted on how the train samples influence the model outcome. The more red the dots, the higher the feature value, the more blue the lower the feature value.

In this case, the feature EXT_SOURCE_2 is the feature that has the most impact on the model output. Train samples with low EXT_SOURCE_2 have higher probability upon obtaining a loan. If the client has a high EXT_SOURCE_2 value, the probability of getting a loan is low. For the red blob on the left, we see that a lot clients are in this case.

In the dependence plot of EXT_SOURCE_2, we see that if this value is between 0 and 0.2 the model output is higher especially when CODE_GENDER is zero (the blue dots). The feature CODE_GENDER is automatically chosen by the shap dependence plot function.

In the cc_bal_CNT_DRAWINGS_ATM_CURRENT dependence plot we see a few outliers. Maybe we should remove them ? The are training samples with CODE_GENDER equal to 2. Are that transgenders ? Also dependence plot of SK_DPD_DEF show that most samples are zero except a few exceptions. Maybe we need some feature engineering here. Chances that the model influence of this feature does not generalise to the test set is very high. In other words, overfitting is likely. I would not recommend including this feature. As we can derive from the depence plot, the impact of leaving this feature out will be low on the model performance.",notebook tri gain insight tree model base shap packag understand current featur import calcul lightgbm xgboost tree base model issu read articl interpret machin learn xgboost interpret machin learn xgboost shap librari http github com slundberg shap http github com slundberg shap use go set notebook upper right corner add custom packag set tab import plot summari plot notebook show 30 import featur featur distribut plot train sampl influenc model outcom red dot higher featur valu blue lower featur valu case featur ext sourc 2 featur impact model output train sampl low ext sourc 2 higher probabl upon obtain loan client high ext sourc 2 valu probabl get loan low red blob left see lot client case depend plot ext sourc 2 see valu 0 0 2 model output higher especi code gender zero blue dot featur code gender automat chosen shap depend plot function cc bal cnt draw atm current depend plot see outlier mayb remov train sampl code gender equal 2 transgend also depend plot sk dpd def show sampl zero except except mayb need featur engin chanc model influenc featur generalis test set high word overfit like would recommend includ featur deriv depenc plot impact leav featur low model perform
2886,# Feature enginering based on [https://www.kaggle.com/ogrellier/good-fun-with-ligthgbm](https://www.kaggle.com/ogrellier/good-fun-with-ligthgbm),featur engin base http www kaggl com ogrelli good fun ligthgbm http www kaggl com ogrelli good fun ligthgbm
2887,"# Explaining the lightgbm model with shap
The advantage of using lightgbm over sklearn random forrest classifier is that lightGBM can deal with the Nan.",explain lightgbm model shap advantag use lightgbm sklearn random forrest classifi lightgbm deal nan
2888,There are CODER_GENDER = 2 ?,coder gender 2
2889,"Since CODE_GENDER does not appear in the test set, we can drop them from the train samples ?",sinc code gender appear test set drop train sampl
2890,If DAYS_EMPLOYED is a large positive number means the client is unemployed ? Maybe extraxt those with dummy variable ? It would applying for a loan unemployed lowers your approval from 8.6% downto 5.4%.,day employ larg posit number mean client unemploy mayb extraxt dummi variabl would appli loan unemploy lower approv 8 6 downto 5 4
2891,"# Visualize many predictions
To keep the browser happy we only visualize 1,000 individuals.",visual mani predict keep browser happi visual 1 000 individu
2892,# Prepare for submission,prepar submiss
2893,"# Feature selection
![](http://)Overlap (or misclassification rate) and ""probability of superiority"" have two good properties:
* As probabilities, they don't depend on units of measure, so they are comparable between studies.
* They are expressed in operational terms, so a reader has a sense of what practical effect the difference makes.

### Cohen's effect size
There is one other common way to express the difference between distributions.  Cohen's $d$ is the difference in means, standardized by dividing by the standard deviation.  Here's the math notation:
 
 $ d = \frac{\bar{x}_1 - \bar{x}_2} s $
 
where $s$ is the pooled standard deviation:

$s = \sqrt{\frac{n_1 s^2_1 + n_2 s^2_2}{n_1+n_2}}$

 Here's a function that computes it:",featur select http overlap misclassif rate probabl superior two good properti probabl depend unit measur compar studi express oper term reader sen practic effect differ make cohen effect size one common way express differ distribut cohen differ mean standard divid standard deviat math notat frac bar x 1 bar x 2 pool standard deviat sqrt frac n 1 2 1 n 2 2 2 n 1 n 2 function comput
2894,# Data preparation based on [fork-of-good-fun-with-ligthgbm-more-features](https://www.kaggle.com/cttsai/fork-of-good-fun-with-ligthgbm-more-features),data prepar base fork good fun ligthgbm featur http www kaggl com cttsai fork good fun ligthgbm featur
2895,# Explore the data,explor data
2896,# Impute the data,imput data
2897,# Upsample the minority class to match the majority class with SMOTE,upsampl minor class match major class smote
2898,# Prepare for submission,prepar submiss
2899,![](http://www.homecredit.net/~/media/Images/H/Home-Credit-Group/image-gallery/full/image-gallery-01-11-2016-b.png),http www homecredit net medium imag h home credit group imag galleri full imag galleri 01 11 2016 b png
2900,"- <a href='#1'>1. Introduction</a>  
- <a href='#2'>2. Retrieving the Data</a>
- <a href='#3'>3. Glimpse of Data</a>
- <a href='#4'> 4. Check for missing data</a>
- <a href='#5'>5. Data Exploration</a>
    - <a href='#5-1'>5.1 Distribution of AMT_CREDIT</a>
    - <a href='#5-2'>5.2 Distribution of AMT_INCOME_TOTAL</a>
    - <a href='#5-3'>5.3 Distribution of AMT_GOODS_PRICE</a>
    - <a href='#5-4'>5.4 Distribution of Name of type of the Suite</a>
    - <a href='#5-5'>5.5 Data is balanced or imbalanced</a>
    - <a href='#5-6'>5.6 Types of loan</a>
    - <a href='#5-7'>5.7 Purpose of loan</a>
    - <a href='#5-8'>5.8 Income sources of Applicant's who applied for loan</a>
    - <a href='#5-9'>5.9 Family Status of Applicant's who applied for loan</a>
    - <a href='#5-10'>5.10 Occupation of Applicant's who applied for loan</a>
    - <a href='#5-11'>5.11 Education of Applicant's who applied for loan</a>
    - <a href='#5-12'>5.12 For which types of house higher applicant's applied for loan ?</a>
    - <a href='#5-13'>5.13 Types of Organizations who applied for loan </a>
    - <a href='#5-14'>5.14 Exploration in terms of loan is repayed or not</a>
        - <a href='#5-14-1'>5.14.1 Income sources of Applicant's in terms of loan is repayed or not</a>
        - <a href='#5-14-2'>5.14.2 Family Status of Applicant's in terms of loan is repayed or not</a>
        - <a href='#5-14-3'>5.14.3 Occupation of Applicant's in terms of loan is repayed or not</a>
        - <a href='#5-14-4'>5.14.4 Education of Applicant's in terms of loan is repayed or not</a>
        - <a href='#5-14-5'>5.14.5 For which types of house higher applicant's applied for loan in terms of loan is repayed or not</a>
        - <a href='#5-14-6'>5.14.6 Types of Organizations in terms of loan is repayed or not</a>
        - <a href='#5-14-7'>5.14.7 Distribution of Name of type of the Suite in terms of loan is repayed or not</a>
    - <a href='#5-15'>5.15 Exploartion of previous application data</a>
        - <a href='#5-15-1'>5.15.1 Contract product type of previous application</a>
        - <a href='#5-15-2'>5.15.2 On which day highest number of clients applied in prevoies application</a>
        - <a href='#5-15-3'>5.15.3 Purpose of cash loan in previous application</a>
        - <a href='#5-15-4'>5.15.4 Contract was approved or not in previous application</a>
        - <a href='#5-15-5'>5.15.5 Payment method that client choose to pay for the previous application</a>
        - <a href='#5-15-6'>5.15.6 Why was the previous application rejected ?</a>
        - <a href='#5-15-7'>5.15.7 Who accompanied client when applying for the previous application</a>
        - <a href='#5-15-8'>5.15.8 Was the client old or new client when applying for the previous application</a>
        - <a href='#5-15-9'>5.15.9 What kind of goods did the client apply for in the previous application</a>
        - <a href='#5=15=10'>5.15.10 Was the previous application for CASH, POS, CAR, …</a>
        - <a href='#5-15-11'>5.15.11 Was the previous application x-sell or walk-in ?</a>
        - <a href='#5-15-12'>5.15.12 Top channels  through which they acquired the client on the previous application</a>
        - <a href='#5-15-13'>5.15.13 Top industry of the seller</a>
        - <a href='#5-15-14'>5.15.14 Grouped interest rate into small medium and high of the previous application</a>
        - <a href='#5-15-15'>5.15.15 Top Detailed product combination of the previous application</a>
        - <a href='#5-15-16'>5.15.16 Did the client requested insurance during the previous application</a>
- <a href='#6'>6. Pearson Correlation of features</a>
- <a href='#7'>7. Feature Importance using Random forest</a>",href 1 1 introduct href 2 2 retriev data href 3 3 glimps data href 4 4 check miss data href 5 5 data explor href 5 1 5 1 distribut amt credit href 5 2 5 2 distribut amt incom total href 5 3 5 3 distribut amt good price href 5 4 5 4 distribut name type suit href 5 5 5 5 data balanc imbalanc href 5 6 5 6 type loan href 5 7 5 7 purpos loan href 5 8 5 8 incom sourc applic appli loan href 5 9 5 9 famili statu applic appli loan href 5 10 5 10 occup applic appli loan href 5 11 5 11 educ applic appli loan href 5 12 5 12 type hous higher applic appli loan href 5 13 5 13 type organ appli loan href 5 14 5 14 explor term loan repay href 5 14 1 5 14 1 incom sourc applic term loan repay href 5 14 2 5 14 2 famili statu applic term loan repay href 5 14 3 5 14 3 occup applic term loan repay href 5 14 4 5 14 4 educ applic term loan repay href 5 14 5 5 14 5 type hous higher applic appli loan term loan repay href 5 14 6 5 14 6 type organ term loan repay href 5 14 7 5 14 7 distribut name type suit term loan repay href 5 15 5 15 exploart previou applic data href 5 15 1 5 15 1 contract product type previou applic href 5 15 2 5 15 2 day highest number client appli prevoi applic href 5 15 3 5 15 3 purpos cash loan previou applic href 5 15 4 5 15 4 contract approv previou applic href 5 15 5 5 15 5 payment method client choos pay previou applic href 5 15 6 5 15 6 previou applic reject href 5 15 7 5 15 7 accompani client appli previou applic href 5 15 8 5 15 8 client old new client appli previou applic href 5 15 9 5 15 9 kind good client appli previou applic href 5 15 10 5 15 10 previou applic cash po car href 5 15 11 5 15 11 previou applic x sell walk href 5 15 12 5 15 12 top channel acquir client previou applic href 5 15 13 5 15 13 top industri seller href 5 15 14 5 15 14 group interest rate small medium high previou applic href 5 15 15 5 15 15 top detail product combin previou applic href 5 15 16 5 15 16 client request insur previou applic href 6 6 pearson correl featur href 7 7 featur import use random forest
2901,# <a id='1'>1. Introduction</a>,id 1 1 introduct
2902,"Home Credit strives to broaden financial inclusion for the unbanked population by providing a positive and safe borrowing experience. In order to make sure this underserved population has a positive loan experience, Home Credit makes use of a variety of alternative data--including telco and transactional information--to predict their clients' repayment abilities.

While Home Credit is currently using various statistical and machine learning methods to make these predictions, they're challenging Kagglers to help them unlock the full potential of their data. Doing so will ensure that clients capable of repayment are not rejected and that loans are given with a principal, maturity, and repayment calendar that will empower their clients to be successful.",home credit strive broaden financi inclus unbank popul provid posit safe borrow experi order make sure underserv popul posit loan experi home credit make use varieti altern data includ telco transact inform predict client repay abil home credit current use variou statist machin learn method make predict challeng kaggler help unlock full potenti data ensur client capabl repay reject loan given princip matur repay calendar empow client success
2903, # <a id='2'>2. Retrieving the Data</a>,id 2 2 retriev data
2904,# <a id='3'>3. Glimpse of Data</a>,id 3 3 glimps data
2905,**application_train data**,applic train data
2906,**POS_CASH_balance data**,po cash balanc data
2907,**bureau_balance data**,bureau balanc data
2908,**previous_application data**,previou applic data
2909,**installments_payments data**,instal payment data
2910,**credit_card_balance data**,credit card balanc data
2911,**bureau data**,bureau data
2912,# <a id='4'> 4 Check for missing data</a>,id 4 4 check miss data
2913,**checking missing data in application_train **,check miss data applic train
2914,**checking missing data in POS_CASH_balance **,check miss data po cash balanc
2915,**checking missing data in bureau_balance **,check miss data bureau balanc
2916,**checking missing data in previous_application **,check miss data previou applic
2917,**checking missing data in installments_payments **,check miss data instal payment
2918,**checking missing data in credit_card_balance **,check miss data credit card balanc
2919,**checking missing data in bureau **,check miss data bureau
2920,# <a id='5'>5. Data Exploration</a>,id 5 5 data explor
2921,## <a id='5-1'>5.1 Distribution of AMT_CREDIT</a>,id 5 1 5 1 distribut amt credit
2922,## <a id='5-2'>5.2 Distribution of AMT_INCOME_TOTAL</a>,id 5 2 5 2 distribut amt incom total
2923,## <a id='5-3'>5.3 Distribution of AMT_GOODS_PRICE</a>,id 5 3 5 3 distribut amt good price
2924,## <a id='5-4'>5.4 Who accompanied client when applying for the  application</a>,id 5 4 5 4 accompani client appli applic
2925,## <a id='5-5'>5.5 Data is balanced or imbalanced</a>,id 5 5 5 5 data balanc imbalanc
2926,* As we can see data is highly imbalanced.,see data highli imbalanc
2927,## <a id='5-6'>5.6 Types of loan</a>,id 5 6 5 6 type loan
2928,"* **Rovolving loans :**  Arrangement which allows for the loan amount to be withdrawn, repaid, and redrawn again in any manner and any number of times, until the arrangement expires. Credit card loans and overdrafts are revolving loans. Also called evergreen loan",rovolv loan arrang allow loan amount withdrawn repaid redrawn manner number time arrang expir credit card loan overdraft revolv loan also call evergreen loan
2929,* Most of the loans are Cash loans which were taken by applicants. **90.5 %** loans are Cash loans.,loan cash loan taken applic 90 5 loan cash loan
2930,## <a id='5-7'>5.7 Purpose of loan</a>,id 5 7 5 7 purpos loan
2931,## <a id='5-8'>5.8 Income sources of Applicant's who applied for loan</a>,id 5 8 5 8 incom sourc applic appli loan
2932,* 51.6 % Applicants mentioned that they are working.  23.3 % are Commercial Associate and 18 % are Pensioner etc. ,51 6 applic mention work 23 3 commerci associ 18 pension etc
2933,## <a id='5-9'>5.9 Family Status of Applicant's who applied for loan</a>,id 5 9 5 9 famili statu applic appli loan
2934,* 63.9 % applicants are married. 14.8 % are single etc.,63 9 applic marri 14 8 singl etc
2935,## <a id='5-10'>5.10 Occupation of Applicant's who applied for loan</a>,id 5 10 5 10 occup applic appli loan
2936,"* **Top Applicant's who applied for loan :**
  * Laborers - Apprx. 55 K
  * Sales Staff - Approx. 32 K
  * Core staff - Approx. 28 K
  * Managers - Approx. 21 K
  * Drivers - Approx. 19 K",top applic appli loan labor apprx 55 k sale staff approx 32 k core staff approx 28 k manag approx 21 k driver approx 19 k
2937,## <a id='5-11'>5.11 Education of Applicant's who applied for loan</a>,id 5 11 5 11 educ applic appli loan
2938,* 71 % applicants have secondary and 24.3 % having higher education.,71 applic secondari 24 3 higher educ
2939,## <a id='5-12'>5.12 For which types of house higher applicant's applied for loan ?</a>,id 5 12 5 12 type hous higher applic appli loan
2940,"* Approx. 89 % peoples applied for loan, they mentioned type of house is **House / Appartment**.",approx 89 peopl appli loan mention type hous hous appart
2941,## <a id='5-13'>5.13 Types of Organizations who applied for loan </a>,id 5 13 5 13 type organ appli loan
2942,"* **Types of Organizations who applied for loan :**
  * Business Entity Type 3 - Approx. 68 K
  * XNA - Approx. 55 K
  * Self employed - Approx. 38 K
  * Others - Approx. 17 K
  * Medicine - Approx. 11 K
 ",type organ appli loan busi entiti type 3 approx 68 k xna approx 55 k self employ approx 38 k other approx 17 k medicin approx 11 k
2943,## <a id='5-14'>5.14 Exploration in terms of loan is repayed or not</a>,id 5 14 5 14 explor term loan repay
2944,## <a id='5-14-1'>5.14.1 Income sources of Applicant's in terms of loan is repayed or not in %</a>,id 5 14 1 5 14 1 incom sourc applic term loan repay
2945,## <a id='5-14-2'>5.14.2 Family Status of Applicant's in terms of loan is repayed or not in %</a>,id 5 14 2 5 14 2 famili statu applic term loan repay
2946,## <a id='5-14-3'>5.14.3 Occupation of Applicant's in terms of loan is repayed or not in %</a>,id 5 14 3 5 14 3 occup applic term loan repay
2947,## <a id='5-14-4'>5.14.4 Education of Applicant's in terms of loan is repayed or not in %</a>,id 5 14 4 5 14 4 educ applic term loan repay
2948,## <a id='5-14-5'>5.14.5 For which types of house higher applicant's applied for loan in terms of loan is repayed or not in %</a>,id 5 14 5 5 14 5 type hous higher applic appli loan term loan repay
2949,## <a id='5-14-6'>5.14.6 Types of Organizations in terms of loan is repayed or not in %</a>,id 5 14 6 5 14 6 type organ term loan repay
2950,## <a id='5-14-7'>5.14.7 Distribution of Name of type of the Suite in terms of loan is repayed or not in %</a>,id 5 14 7 5 14 7 distribut name type suit term loan repay
2951,# <a id='5-15'>5.15 Exploartion of previous application data</a>,id 5 15 5 15 exploart previou applic data
2952,## <a id='5-15-1'>5.15.1 Contract product type of previous application</a>,id 5 15 1 5 15 1 contract product type previou applic
2953,"* **Contract product type of previous application :**
  * Cash loans - 44.8 %
  * Consumer loans - 43.7 %
  * Rovolving loan - 11.6 %
  * XNA - 0.0207 %",contract product type previou applic cash loan 44 8 consum loan 43 7 rovolv loan 11 6 xna 0 0207
2954,## <a id='5-15-2'>5.15.2 On which day highest number of clients applied in prevoies application</a>,id 5 15 2 5 15 2 day highest number client appli prevoi applic
2955,"* What a coincedence, Approximately 15 % clients applied in each 5 days a week i.e, Tuesday, Wednesday, Monday, Friday and Thrusday. ",coinced approxim 15 client appli 5 day week e tuesday wednesday monday friday thrusday
2956,## <a id='5-15-3'>5.15.3 Purpose of cash loan in previous application</a>,id 5 15 3 5 15 3 purpos cash loan previou applic
2957,"* **Main purpose of the cash loan was  :**
  * XAP - 55 %
  * XNA - 41 %",main purpos cash loan xap 55 xna 41
2958,## <a id='5-15-4'>5.15.4 Contract was approved or not in previous application</a>,id 5 15 4 5 15 4 contract approv previou applic
2959,"* **Contract was approved or not in previous application :**
  * Approved : 62.1 % times
  * Cancelled : 18.9 % times
  * Refused : 17.4 % times 
  * Unused offer : 1.58 % times",contract approv previou applic approv 62 1 time cancel 18 9 time refus 17 4 time unus offer 1 58 time
2960,## <a id='5-15-5'>5.15.5 Payment method that client choose to pay for the previous application</a>,id 5 15 5 5 15 5 payment method client choos pay previou applic
2961,* As we can most of the payment(61.9 %) has done thorugh cash only. ,payment 61 9 done thorugh cash
2962,## <a id='5-15-6'>5.15.6 Why was the previous application rejected ?</a>,id 5 15 6 5 15 6 previou applic reject
2963,## <a id='5-15-7'>5.15.7 Who accompanied client when applying for the previous application</a>,id 5 15 7 5 15 7 accompani client appli previou applic
2964,"* **Who accompanied client when applying for the previous application :**
  * Unaccompanied : Approx. 60 % times
  * Family : Approx. 25 % times
  * Spouse, Partner : Approx. 8 %
  * Childrens : Approx. 4 %",accompani client appli previou applic unaccompani approx 60 time famili approx 25 time spous partner approx 8 child approx 4
2965,## <a id='5-15-8'>5.15.8 Was the client old or new client when applying for the previous application</a>,id 5 15 8 5 15 8 client old new client appli previou applic
2966,* Approximately 74 % was repeater clients who applied for previous application.,approxim 74 repeat client appli previou applic
2967,## <a id='5-15-9'>5.15.9 What kind of goods did the client apply for in the previous application</a>,id 5 15 9 5 15 9 kind good client appli previou applic
2968,"## <a id='5=15=10'>5.15.10 Was the previous application for CASH, POS, CAR, …</a>",id 5 15 10 5 15 10 previou applic cash po car
2969,## <a id='5-15-11'>5.15.11 Was the previous application x-sell or walk-in ?</a>,id 5 15 11 5 15 11 previou applic x sell walk
2970,## <a id='5-15-12'>5.15.12 Top channels  through which they acquired the client on the previous application</a>,id 5 15 12 5 15 12 top channel acquir client previou applic
2971,"* **Top channels  through which they acquired the client on the previous application :**
  * Credidit and cash offices : 43 % times
  * Country_wide : 30 % times
  * Stone : 13 % times",top channel acquir client previou applic credidit cash offic 43 time countri wide 30 time stone 13 time
2972,## <a id='5-15-13'>5.15.13 Top industry of the seller</a>,id 5 15 13 5 15 13 top industri seller
2973,## <a id='5-15-14'>5.15.14 Grouped interest rate into small medium and high of the previous application</a>,id 5 15 14 5 15 14 group interest rate small medium high previou applic
2974,## <a id='5-15-15'>5.15.15 Top Detailed product combination of the previous application</a>,id 5 15 15 5 15 15 top detail product combin previou applic
2975,## <a id='5-15-16'>5.15.16 Did the client requested insurance during the previous application</a>,id 5 15 16 5 15 16 client request insur previou applic
2976,# <a id='6'>6. Pearson Correlation of features</a>,id 6 6 pearson correl featur
2977,# <a id='7'>7. Feature Importance using Random forest</a>,id 7 7 featur import use random forest
2978,# More To Come. Stayed Tuned ,come stay tune
2979,"## latest updates
* created another crosstab function - this one for numeric variables
* crosstabs which have been suppressed are now included as a data source
* set up option to suppress particular crosstabs which have been created previously and slow down current kernel runs
* created a custom crosstab function
* the time it takes for sections to run is now recorded
* dataframe info gets outputed to csv files
* concatenated application train and test files
* just setting up my job at the moment",latest updat creat anoth crosstab function one numer variabl crosstab suppress includ data sourc set option suppress particular crosstab creat previous slow current kernel run creat custom crosstab function time take section run record datafram info get output csv file concaten applic train test file set job moment
2980,## application train and test data files,applic train test data file
2981,### application train,applic train
2982,### application test,applic test
2983,## concatenating application train and test files,concaten applic train test file
2984,### testing and learning (please ignore I am not doing anything great),test learn plea ignor anyth great
2986,"['- **application_{train|test}.csv**\n', '- **bureau.csv**\n', '- **bureau_balance.csv**\n', '- **previous_application**\n', '- **POS_CASH_balance.csv**\n', '- **credit_card_balance.csv**\n', '- **installments_payments.csv**\n', '\n', '# application_{train|test}.csv  \n', '1. 数据量及描述\n', '\t- train (307511,122) \n', '\t- test (48744,121) \n', '\t- 描述：所有贷款申请的静态数据，一行代表我们数据样本中的一笔贷款。 \n', '2. SK_ID_CURR\n', '\t- 含义：样本中贷款ID\n', '\t- 字段：10002/10003...\n', '3. TARGET\n', '\t- 含义：目标变量 (1代表客户在贷款的前Y个分期内至少有一笔迟交天数超过X天)\n', '\t- 字段：1/0 (test数据集无该列)\n', '4. NAME_CONTRACT_TYPE\n', '\t- 含义: 贷款是现金还是循环的标识 (循环贷是指客户将商品住房抵押给银行，就可获得一定的贷款额度，在房产抵押期限内客户可分次提款、循环使用，不超过可用额度单笔用款时，只需客户填写提款申请表，不用专门再次审批，一般1小时便可提取现金，等于随身有了一个安全又方便的流动大“金库”。)\n', '\t- 字段：Cash loans/Revolving loans\t\n', '5. CODE_GENDER\n', '\t- 含义：客户性别\n', '\t- 字段: F/M/**XNA**\n', '\t- 处理方法: **不做处理**\n', '6. FLAG_OWN_CAR\n', '\t- 含义：客户是否拥有汽车标识\n', '\t- 字段：N/Y\n', '7. FLAG_OWN_REALTY\n', '\t- 含义：客户是否拥有房屋或公寓标识\n', '\t- 字段：N/Y\n', '8. CNT_CHILDREN\n', '\t- 含义：客户拥有的孩子数目\n', '\t- 字段：0-12/14/19/20\n', '9. AMT_INCOME_TOTAL\n', '\t- 含义：客户年收入\n', '\t- 字段：25,650至117,000,000\n', '\t- 处理方法: **年收入超过100000000值(1人)替换为np.nan**\n', '10. AMT_CREDIT\n', '\t- 含义：贷款金额\n', '\t- 字段：45,000至4,050,000\n', '11. AMT_ANNUITY\n', '\t- 含义：贷款年金\n', '\t- 字段：1,616至258,026\n', '12. AMT_GOODS_PRICE\n', '\t- 含义：对于消费贷款，它是给予贷款的商品的价格\n', '\t- 字段：40,500至4,050,000\n', '13. NAME_TYPE_SUITE\n', '\t- 含义：谁在申请贷款时陪同客户\n', '\t- 字段：Unaccompanied/Family/Spouse, partner/children/Other_B/Other_A/Group of people\n', '14. NAME_INCOME_TYPE\n', '\t- 含义：客户收入类型\n', '\t- 字段：Working/Commercial associate/Pensioner/State servant/Unemployed/Student/Businessman/Maternity leave\n', '15. NAME_EDUCATION_TYPE\n', '\t- 含义：客户最高学历\n', '\t- 字段：Secondary/Higher education/Incomplete higher/Lower secondary/Academic degree\n', '16. NAME_FAMILY_STATUS\n', '\t- 含义：客户的家庭状况\n', '\t- 字段：Married/Single/Civil marriage/Separated/Widow/Unknown (我的理解是civil marriage指世俗婚姻或者民事婚姻，与教会承认而合法的婚姻相对。在欧美婚姻被合法承认有两种途径，一种由政府民事机构承认，一种由教会牧师承认。世俗婚姻即指前者。中国是只有世俗婚姻，而欧美在历史上曾经只有教会承认的婚姻，发展到现在则是两种婚姻并存。)\n', '17. NAME_HOUSING_TYPE\n', '\t- 含义：客户的住房情况\n', '\t- 字段：House apartment/With parents/Municipal apartment(市政公寓)/Rented apartment/Office apartment/Co-op apartment\n', '18. REGION_POPULATION_RELATIVE\n', '\t- 含义：客户所居住的区域的正常化人口（数字越大意味着客户居住在人口更稠密的地区）\n', '\t- 字段：0.00至0.07\n', '19. DAYS_BIRTH\n', '\t- 含义：客户出生日距离申请日天数\n', '\t- 字段：-25,229至-7,338 (20岁至69岁)\n', '\t- 处理方法: **取绝对值**\n', '20. DAYS_EMPLOYED\n', '\t- 含义：客户当前工作开始日期距离申请日天数\n', '\t- 字段：-17,912至**365,243(异常值)** \n', '\t- 处理方法: **365243全部替换为np.nan/取绝对值**\n', '21. DAYS_REGISTRATION\n', '\t- 含义：申请前几天客户改变了他的注册 (**什么样的注册?**)\n', '\t- 字段：-24,672至0\n', '\t- 处理方法: **取绝对值**\n', '22. DAYS_ID_PUBLISH\n', '\t- 含义：客户在申请前几天更改了申请贷款的身份证明文件\n', '\t- 字段：-7,197至-1,717\n', '\t- 处理方法: **取绝对值**\n', '23. OWN_CAR_AGE\n', '\t- 含义：客户车辆年限\n', '\t- 字段：0-91\n', '24. FLAG_MOBIL\n', '\t- 含义：客户是否提供移动电话号\n', '\t- 字段：1/0\n', '25. FLAG_EMP_PHONE\n', '\t- 含义：客户是否提供工作电话号\n', '\t- 字段：1/0\n', '26. FLAG_WORK_PHONE\n', '\t- 含义：客户是否提供家庭电话号\n', '\t- 字段：1/0\n', '27. FLAG_CONT_MOBILE\n', '\t- 含义：移动电话号是否可以拨通\n', '\t- 字段：1/0\n', '28. FLAG_PHONE\n', '\t- 含义：客户是否提供家庭电话号 (**跟FLAG_WORK_PHONE重复？**)\n', '\t- 字段：1/0\n', '\t- 处理方法: **删除**\n', '29. FLAG_EMAIL\n', '\t- 含义：客户是否提供邮件地址\n', '\t- 字段：1/0\n', '30. OCCUPATION_TYPE\n', '\t- 含义：客户职业类型\n', '\t- 字段：Laborers/Sales staff/Core Staff/Managers/Drivers/High Skill Tech Staff/Accountants/Medicine Staff/Security Staff/Cooking Staff/Cleaning Staff/Private service staff/Low-skill Laborers/Waiters barmen staff/Secretaries/Realty agents/HR staff/IT staff \n', '31. CNT_FAM_MEMBERS\n', '\t- 含义：客户拥有多少家庭成员\n', '\t- 字段：1-21\n', '32. REGION_RATING_CLIENT\n', '\t- 含义：我们对客户居住地区的评分\n', '\t- 字段：1/2/3\n', '33. REGION_RATING_CLIENT_W_CITY\n', '\t- 含义：我们对客户居住地区的评分(将城市因素考虑进去)\n', '\t- 字段：1/2/3/**-1(异常值)**\n', '\t- 处理方法: **去除-1行**\n', '34. WEEKDAY_APPR_PROCESS_START\n', '\t- 含义：客户在一周的第几天申请的贷款\n', '\t- 字段：MONDAY至SUNDAY\n', '35. HOUR_APPR_PROCESS_START\n', '\t- 含义：客户大约在几点申请的贷款\n', '\t- 字段：0-23\n', '36. REG_REGION_NOT_LIVE_REGION\n', '\t- 含义：客户永久地址不匹配客户联系地址标识(地区层级)\n', '\t- 字段：1/0\n', '37. REG_REGION_NOT_WORK_REGION\n', '\t- 含义：客户永久地址不匹配客户工作地址标识(地区层级)\n', '\t- 字段：1/0\n', '38. LIVE_REGION_NOT_WORK_REGION\n', '\t- 含义：客户联系地址不匹配客户工作地址标识(地区层级)\n', '\t- 字段：1/0\n', '39. REG_CITY_NOT_LIVE_CITY\n', '\t- 含义：客户永久地址不匹配客户联系地址标识(城市层级)\n', '\t- 字段：1/0\n', '40. REG_CITY_NOT_WORK_CITY\n', '\t- 含义：客户永久地址不匹配客户工作地址标识(城市层级)\n', '\t- 字段：1/0\n', '41. LIVE_CITY_NOT_WORK_CITY\n', '\t- 含义：客户联系地址不匹配客户工作地址标识(城市层级)\n', '\t- 字段：1/0\n', '42. ORGANIZATION_TYPE\n', '\t- 含义：客户工作机构类型\n', '\t- 字段：Business Entity Type 3/**XNA**/Self-employed/Other...\n', '\t- 处理方法: **XNA不做处理,把它当做一类**\n', '43. EXT_SOURCE_1\n', '\t- 含义：来自外部数据源的标准化分数 (**什么分数？**)\n', '\t- 字段：0-1\n', '44. EXT_SOURCE_2\n', '\t- 含义：来自外部数据源的标准化分数\n', '\t- 字段：0-1\n', '45. EXT_SOURCE_3\n', '\t- 含义：来自外部数据源的标准化分数\n', '\t- 字段：0-1\n', '46. APARTMENTS_AVG\n', '\t- 含义：**关于客户居住的建筑物的标准化信息**，平均值（_AVG后缀），众数（_MODE后缀），中位数（_MEDI后缀）；房屋面积均值? (**字段意义不明**) \n', '\t- 字段：0-1\n', '47. BASEMENTAREA_AVG\n', '\t- 含义：地下室区域面积均值\n', '\t- 字段：0-1\n', '48. YEARS_BEGINEXPLUATATION_AVG\n', '\t- 含义：开始建造年限均值\n', '\t- 字段：0-1\n', '49. YEARS_BUILD_AVG\n', '\t- 含义：建造年限均值\n', '\t- 字段：0-1\n', '50. COMMONAREA_AVG\n', '\t- 含义：公共区域面积均值\n', '\t- 字段：0-1\n', '51. ELEVATORS_AVG\n', '\t- 含义：电梯数均值\n', '\t- 字段：0-1\n', '52. ENTRANCES_AVG\n', '\t- 含义：入口数均值\n', '\t- 字段：0-1\n', '53. FLOORSMAX_AVG\n', '\t- 含义：最大楼层数均值\n', '\t- 字段：0-1\n', '54. FLOORSMIN_AVG\n', '\t- 含义：最小楼层数均值\n', '\t- 字段：0-1\n', '55. LANDAREA_AVG\n', '\t- 含义：土地面积均值\n', '\t- 字段：0-1\n', '56. LIVINGAPARTMENTS_AVG\n', '\t- 含义：生活公寓面积均值? (**字段意义不明**)\n', '\t- 字段：0-1\n', '57. LIVINGAREA_AVG\n', '\t- 含义：生活区域面积均值\n', '\t- 字段：0-1\n', '58. NONLIVINGAPARTMENTS_AVG (**字段意义不明**)\n', '\t- 含义：非生活公寓面积均值?\n', '\t- 字段：0-1\n', '59. NONLIVINGAREA_AVG\n', '\t- 含义：非生活区域面积均值\n', '\t- 字段：0-1\n', '60. APARTMENTS_MODE\n', '\t- 含义：公寓面积众数? (**字段意义不明**)\n', '\t- 字段：0-1\n', '61. BASEMENTAREA_MODE\n', '\t- 含义：地下室区域面积众数\n', '\t- 字段：0-1\n', '62. YEARS_BEGINEXPLUATATION_MODE\n', '\t- 含义：开始建造年限众数\n', '\t- 字段：0-1\n', '63. YEARS_BUILD_MODE\n', '\t- 含义：建造年限众数\n', '\t- 字段：0-1\n', '64. COMMONAREA_MODE\n', '\t- 含义：公共区域面积众数\n', '\t- 字段：0-1\n', '65. ELEVATORS_MODE\n', '\t- 含义：电梯数众数\n', '\t- 字段：0-1\n', '66. ENTRANCES_MODE\n', '\t- 含义：入口数众数\n', '\t- 字段：0-1\n', '67. FLOORSMAX_MODE\n', '\t- 含义：最大楼层数众数\n', '\t- 字段：0-1\n', '68. FLOORSMIN_MODE\n', '\t- 含义：最小楼层数众数\n', '\t- 字段：0-1\n', '69. LANDAREA_MODE\n', '\t- 含义：土地面积众数\n', '\t- 字段：0-1\n', '70. LIVINGAPARTMENTS_MODE\n', '\t- 含义：生活公寓面积众数?\n', '\t- 字段：0-1\n', '71. LIVINGAREA_MODE\n', '\t- 含义：生活区域面积众数\n', '\t- 字段：0-1\n', '72. NONLIVINGAPARTMENTS_MODE\n', '\t- 含义：非生活公寓面积众数？\n', '\t- 字段：0-1\n', '73. NONLIVINGAREA_MODE\n', '\t- 含义：非生活区域面积众数\n', '\t- 字段：0-1\n', '74. APARTMENTS_MEDI\n', '\t- 含义：公寓中位数\n', '\t- 字段：0-1\n', '75. BASEMENTAREA_MEDI\n', '\t- 含义：地下室面积中位数\n', '\t- 字段：0-1\n', '76. YEARS_BEGINEXPLUATATION_MEDI\n', '\t- 含义：开始建造年限中位数\n', '\t- 字段：0-1\n', '77. YEARS_BUILD_MEDI\n', '\t- 含义：建造年限中位数\n', '\t- 字段：0-1\n', '78. COMMONAREA_MEDI\n', '\t- 含义：公共区域中位数\n', '\t- 字段：0-1\n', '79. ELEVATORS_MEDI\n', '\t- 含义：电梯数中位数\n', '\t- 字段：0-1\n', '80. ENTRANCES_MEDI\n', '\t- 含义：入口数中位数\n', '\t- 字段：0-1\n', '81. FLOORSMAX_MEDI\n', '\t- 含义：最大楼层数中位数\n', '\t- 字段：0-1\n', '82. FLOORSMIN_MEDI\n', '\t- 含义：最小楼层数中位数\n', '\t- 字段：0-1\n', '83. LANDAREA_MEDI\n', '\t- 含义：土地面积中位数\n', '\t- 字段：0-1\n', '84. LIVINGAPARTMENTS_MEDI\n', '\t- 含义：生活公寓面积中位数?\n', '\t- 字段：0-1\n', '85. LIVINGAREA_MEDI\n', '\t- 含义：生活区域面积中位数\n', '\t- 字段：0-1\n', '86. NONLIVINGAPARTMENTS_MEDI\n', '\t- 含义：非生活公寓面积中位数?\n', '\t- 字段：0-1\n', '87. NONLIVINGAREA_MEDI\n', '\t- 含义：非生活区域面积中位数\n', '\t- 字段：0-1\n', '88. FONDKAPREMONT_MODE\n', '\t- 含义：账号类型众数\n', '\t- 字段：reg oper account 注册账号/reg oper spec account 注册标注账号/not specified 未注明/org spec account 组织标注账号\n', '89. HOUSETYPE_MODE\n', '\t- 含义：房屋类型众数\n', '\t- 字段：block of flats 公寓楼/specific housing/terraced house 排屋\n', '90. TOTALAREA_MODE\n', '\t- 含义：房屋总面积众数\n', '\t- 字段：0-1\n', '91. WALLSMATERIAL_MODE\n', '\t- 含义：墙壁材料众数\n', '\t- 字段：Panel 面板/Stone, brick 石头/Block/Wooden 木质/Mixed 混合/Monolithic 独石/Others\n', '92. EMERGENCYSTATE_MODE\n', '\t- 含义：紧急状态众数\n', '\t- 字段：No/Yes\n', '93. OBS_30_CNT_SOCIAL_CIRCLE\n', '\t- 含义：客户周围关系圈内有多少人可观察到欠交贷款30天\n', '\t- 字段：0-354 (**354疑似异常值**)\n', '\t- 处理方法: **大于100值替换为np.nan**\n', '94. DEF_30_CNT_SOCIAL_CIRCLE\n', '\t- 含义：客户周围关系圈内有多少人已欠交贷款30天 (**与上个变量区别是什么？**)\n', '\t- 字段：0-34\n', '95. OBS_60_CNT_SOCIAL_CIRCLE\n', '\t- 含义：客户周围关系圈内有多少人可观察到欠交贷款60天\n', '\t- 字段：0-351\n', '\t- 处理方法: **大于100值替换为np.nan**\n', '96. DEF_60_CNT_SOCIAL_CIRCLE\n', '\t- 含义：客户周围关系圈内有多少人已欠交贷款60天\n', '\t- 字段：0-24\n', '97. DAYS_LAST_PHONE_CHANGE\n', '\t- 含义：客户距离申请贷款改变电话号码天数\n', '\t- 字段：-4361-0\n', '\t- 处理方法: **取绝对值**\n', '98. FLAG_DOCUMENT_2\n', '\t- 含义：客户是否提供文件2\n', '\t- 字段：0/1\n', '99. FLAG_DOCUMENT_3\n', '\t- 含义：客户是否提供文件3\n', '\t- 字段：0/1\n', '100. FLAG_DOCUMENT_4\n', '\t- 含义：客户是否提供文件4\n', '\t- 字段：0/1\n', '101. FLAG_DOCUMENT_5\n', '\t- 含义：客户是否提供文件5\n', '\t- 字段：0/1\n', '102. FLAG_DOCUMENT_6\n', '\t- 含义：客户是否提供文件6\n', '\t- 字段：0/1\n', '103. FLAG_DOCUMENT_7\n', '\t- 含义：客户是否提供文件7\n', '\t- 字段：0/1\n', '104. FLAG_DOCUMENT_8\n', '\t- 含义：客户是否提供文件8\n', '\t- 字段：0/1\n', '105. FLAG_DOCUMENT_9\n', '\t- 含义：客户是否提供文件9\n', '\t- 字段：0/1\n', '106. FLAG_DOCUMENT_10\n', '\t- 含义：客户是否提供文件10\n', '\t- 字段：0/1\n', '107. FLAG_DOCUMENT_11\n', '\t- 含义：客户是否提供文件11\n', '\t- 字段：0/1\n', '108. FLAG_DOCUMENT_12\n', '\t- 含义：客户是否提供文件12\n', '\t- 字段：0/1\n', '109. FLAG_DOCUMENT_13\n', '\t- 含义：客户是否提供文件13\n', '\t- 字段：0/1\n', '110. FLAG_DOCUMENT_14\n', '\t- 含义：客户是否提供文件14\n', '\t- 字段：0/1\n', '111. FLAG_DOCUMENT_15\n', '\t- 含义：客户是否提供文件15\n', '\t- 字段：0/1\n', '112. FLAG_DOCUMENT_16\n', '\t- 含义：客户是否提供文件16\n', '\t- 字段：0/1\n', '113. FLAG_DOCUMENT_17\n', '\t- 含义：客户是否提供文件17\n', '\t- 字段：0/1\n', '114. FLAG_DOCUMENT_18\n', '\t- 含义：客户是否提供文件18\n', '\t- 字段：0/1\n', '115. FLAG_DOCUMENT_19\n', '\t- 含义：客户是否提供文件19\n', '\t- 字段：0/1\n', '116. FLAG_DOCUMENT_20\n', '\t- 含义：客户是否提供文件20\n', '\t- 字段：0/1\n', '117. FLAG_DOCUMENT_21\n', '\t- 含义：客户是否提供文件21\n', '\t- 字段：0/1\n', '118. AMT_REQ_CREDIT_BUREAU_HOUR\n', '\t- 含义：申请前一小时向信用局询问客户的情况次数\n', '\t- 字段：0/1/2/3/4\n', '119. AMT_REQ_CREDIT_BUREAU_DAY\n', '\t- 含义：申请前一天(排除前一小时)向信用局询问客户的情况次数\n', '\t- 字段：0-8\n', '120. AMT_REQ_CREDIT_BUREAU_WEEK\n', '\t- 含义：申请前一天(排除前一天)向信用局询问客户的情况次数\n', '\t- 字段：0-8\n', '121. AMT_REQ_CREDIT_BUREAU_MON\n', '\t- 含义：申请前一月(排除前一周)向信用局询问客户的情况次数\n', '\t- 字段：0-26\n', '122. AMT_REQ_CREDIT_BUREAU_QRT\n', '\t- 含义：申请前三月(排除前一月)向信用局询问客户的情况次数\n', '\t- 字段：0-261 (**261疑似异常值**)\n', '\t- 处理方法: **大于100值替换为np.nan**\n', '123. AMT_REQ_CREDIT_BUREAU_YEAR\n', '\t- 含义：申请前一年(排除前三月)向信用局询问客户的情况次数\n', '\t- 字段：0-25\n', '\n', '# bureau.csv\n', '1. 数据量及描述\n', '\t- (1716428,17) \n', '\t- 描述：所有客户之前由其他金融机构提供给信用局的信用报告（对于我们样本中有贷款的客户）。对于我们样本中的每笔贷款，客户在申请日期之前在信用局拥有的信贷数量与行数一样多。\n', '2. SK_ID_CURR\n', '\t- 含义：样本中贷款ID (样本中贷款可以有一至多笔在信用局相关的信贷记录)\n', '\t- 字段：100002/100003...\n', '3. SK_ID_BUREAU\n', '\t- 含义：与我们贷款相关的以前信用局信贷ID\n', '\t- 字段：5714462/5714463...\n', '4. CREDIT_ACTIVE\n', '\t- 含义：信用局记录在案的信贷的当前状态\n', '\t- 字段：Closed/Active/Sold/Bad Debt\n', '5. CREDIT_CURRENCY\n', '\t- 含义：信用局信贷的货币单位\n', '\t- 字段：currency 1/currency 2/currency 3/currency 4\n', '6. DAYS_CREDIT\n', '\t- 含义：客户申请信用局信贷距离当前申请天数\n', '\t- 字段：-2922至0\n', '\t- 处理方法: **取绝对值**\n', '7. CREDIT_DAY_OVERDUE\n', '\t- 含义：根据客户申请贷款的时间点来说，信用局信贷违约天数\n', '\t- 字段：0至2792\n', '8. DAYS_CREDIT_ENDDATE\n', '\t- 含义：根据客户申请贷款的时间点来说，在信用局信贷剩余的期限\n', '\t- 字段：-42,060至31,199 (**正数代表信贷未到期，负数代表信贷已经到期**)\n', '\t- 处理方法: **小于-20000值替换为np.nan (不可能在115年前该客户有贷款)**\n', '9. DAYS_ENDDATE_FACT\n', '\t- 含义：根据客户申请贷款的时间点来说，信用局信贷已经结束天数(仅限已结束的信贷)\n', '\t- 字段：-42,023至0 (42023=115年，肯定是错误的)\n', '\t- 处理方法: **小于-4000值替换为np.nan**\n', '10. AMT_CREDIT_MAX_OVERDUE\n', '\t- 含义：迄今为止，信用局信贷最高过期额度\n', '\t- 字段：0-**115,987,185.00(疑似异常值)** (0居多) \n', '\t- 处理方法: **大于10000000值替换为np.nan**\n', '11. CNT_CREDIT_PROLONG\n', '\t- 含义：信用局信贷延长了多少次\n', '\t- 字段：0-9 (0居多)\n', '12. AMT_CREDIT_SUM\n', '\t- 含义：信用局信贷的当前信用额度\n', '\t- 字段：0-**585,000,000(疑似异常值)** (1350000为95分位数，超过10000000有2183人)\n', '\t- 处理方法: **大于10000000值替换为np.nan**\n', '13. AMT_CREDIT_SUM_DEBT\n', '\t- 含义：信用局信贷的当前债务\n', '\t- 字段：-4,705,600.32至170,100,000 (**负值代表什么? 8418人小于0**)\n', '\t- 处理方法: **负值替换为0/大于50000000值替换为np.nan** \n', '14. AMT_CREDIT_SUM_LIMIT\n', '\t- 含义：信用局目前信用卡的信用额度\n', '\t- 字段：-586,406.11至4,705,600.32\n', '15. AMT_CREDIT_SUM_OVERDUE\n', '\t- 含义：信用局信贷当前逾期金额\n', '\t- 字段：0-3,756,681 (0居多)\n', '16. CREDIT_TYPE\n', '\t- 含义：信用局信贷类型\n', '\t- 字段：Consumer credit/Credit Card/Car loan/Mortgage/Microloan/Loan for business development...\n', '17. DAYS_CREDIT_UPDATE\n', '\t- 含义：最后一次来源于信用局的信息距离当今申请贷款天数\n', '\t- 字段：-41,947至372\n', '18. AMT_ANNUITY\n', '\t- 含义：信用局年度信用额度\n', '\t- 字段：0至**118,453,423(明显异常值)**\n', '\t- 处理方法: **超过10000000替换np.nan(24人)**\n', '\n', '# bureau_balance.csv\n', '1. 数据量及描述\n', '\t- (27299925,3) \n', '\t- 描述：信贷局以往信贷的每月金额。该表一行对应每月向信用局报告的信贷记录\n', '\t- 逻辑梳理：当今样本中贷款数(train) * 当前贷款相关的之前信用局的信贷数(bureau) * 之前信用局单笔信贷有记录月度信息的月数(bureau_balance)\n', '2. SK_BUREAU_ID\n', '\t- 含义：与我们贷款相关的以前信用局信贷ID\n', '\t- 字段：5714462/5714463...\n', '3. MONTHS_BALANCE\n', '\t- 含义：相对于申请日来说的月份个数(-1指最新的日期)\n', '\t- 字段：-96至0\n', '\t- 处理方法: **取绝对值**\n', '4. STATUS\n', '\t- 含义：在该月期间信用局的贷款状态\n', '\t- 字段：C means closed, X means status unknown, 0 means no DPD, 1 means maximal did during month between 1-30, 2 means DPD 31-60, 5 means DPD 120+ or sold or written off\n', '\n', '# previous_application.csv\n', '1. 数据量及描述\n', '\t- (1670214, 37)\n', '\t- 描述：所有在我们样本中申请贷款客户的之前所有在Home Credit申请贷款的记录。一行代表之前的一笔申请\n', '2. SK_ID_PREV \n', '\t- 含义：与我们现在申请贷款相关的之前在Home Credit的申请 (一笔贷款可能会有之前的多笔贷款)\n', '\t- 字段：1000983/1026910...\n', '3. SK_ID_CURR\n', '\t- 含义：样本中贷款ID\n', '\t- 字段：100002/100003...\n', '4. NAME_CONTRACT_TYPE\n', '\t- 含义：上一个申请的合同产品类型\n', '\t- 字段：Cash loans/Consumer loans/Revolving loans/**XNA**\n', '5. AMT_ANNUITY\n', '\t- 含义：贷款年金\n', '\t- 字段：0至418,058\n', '6. AMT_APPLICATION\n', '\t- 含义：客户在之前申请中申请了多少额度\n', '\t- 字段：0至6,905,160\n', '7. AMT_CREDIT\n', '\t- 含义：客户之前申请里最终的信贷额度\n', '\t- 字段：0至6,905,160\n', '8. AMT_DOWN_PAYMENT\n', '\t- 含义：先前申请的预付金\n', '\t- 字段：-0.90至3,060,045 (**-0.9异常值**)\n', '\t- 处理方法: **小于0数值替换为0**\n', '9. AMT_GOODS_PRICE\n', '\t- 含义：先前申请中客户想要的商品价格(如果有的话)\n', '\t- 字段：0至6,905,160\n', '10. WEEKDAY_APPR_PROCESS_START\n', '\t- 含义：先前申请中客户是在第几天申请的\n', '\t- 字段：MONDAY至SUNDAY\n', '11. HOUR_APPR_PROCESS_START\n', '\t- 含义：先前申请中客户大约在几点申请的贷款\n', '\t- 字段：0至23\n', '12. FLAG_LAST_APPL_PER_CONTRACT\n', '\t- 含义：如果它是上次合同的最后一次申请，则标记。 有时候，由于客户或我们的职员的错误，可能会有单一合同的多份申请\n', '\t- 字段：Y/N\n', '13. NFLAG_LAST_APPL_IN_DAY\n', '\t- 含义：是否是当天申请的最后一次申请。有时候客户一天会有多次申请，也有很低可能会发生由于系统错误导致的一次申请在数据库中出现两次\n', '\t- 字段：1/0\n', '14. NFLAG_MICRO_CASH\n', '\t- 含义：是否是小额贷款\n', '\t- 字段：**数据未有该字段**\n', '15. RATE_DOWN_PAYMENT\n', '\t- 含义：标准化的预付金\n', '\t- 字段：0.0至1.0\n', '16. RATE_INTEREST_PRIMARY\n', '\t- 含义：标准化利率\n', '\t- 字段：0.03至1.00\n', '17. RATE_INTEREST_PRIVILEGED\n', '\t- 含义：标准化利率 (**区别？**)\n', '\t- 字段：0.37至1.00\n', '18. NAME_CASH_LOAN_PURPOSE\n', '\t- 含义：现金贷款目的\n', '\t- 字段：**XAP/XNA**/Repairs/Other/Urgent Needs/Buying a used car/Building a house or an annex...\n', '\t- 处理方法: **不做处理**\n', '19. NAME_CONTRACT_STATUS\n', '\t- 含义：先前申请的合同状态\n', '\t- 字段：Approved/Canceled/Refused/Unused offer\n', '20. DAYS_DECISION\n', '\t- 含义：相对于当前申请，上次申请何时做出\n', '\t- 字段：-2,922至-1\n', '\t- 处理方法: **取绝对值**\n', '21. NAME_PAYMENT_TYPE\n', '\t- 含义：客户选择的付款方式\n', '\t- 字段：Cash through the bank/**XNA**/Non-cash from your account/Cashless from the account of the employer\n', '\t- 处理方法: **不做处理**\n', '22. CODE_REJECT_REASON\n', '\t- 含义：先前申请被拒绝原因\n', '\t- 字段：**XAP**/NC/**XNA**/LIMIT...\n', '\t- 处理方法: **不做处理**\n', '23. NAME_TYPE_SUITE\n', '\t- 含义：谁陪同客户去做的申请\n', '\t- 字段：Unaccompanied/Family/Spouse, partner...\n', '24. NAME_CLIENT_TYPE\n', '\t- 含义：是否新老客户\n', '\t- 字段：Repeater/New/Refreshed/**XNA**\n', '\t- 处理方法: **不做处理**\n', '25. NAME_GOODS_CATEGORY\n', '\t- 含义：客户先前申请的商品种类\n', '\t- 字段：**XNA**/Mobile/Consumer Electronics/Computers...\n', '\t- 处理方法: **不做处理**\n', '26. NAME_PORTFOLIO\n', '\t- 含义：之前的申请是为了CASH，POS，CAR\n', '\t- 字段：CASH/POS/CAR/Cards/**XNA**\n', '\t- 处理方法: **不做处理**\n', '27. NAME_PRODUCT_TYPE\n', '\t- 含义：先前申请是电子还是现场申请？(**?**)\n', '\t- 字段：**XNA**/x-sell/walk-in\n', '\t- 处理方法: **不做处理**\n', '28. CHANNEL_TYPE\n', '\t- 含义：我们通过何种渠道获得的客户\n', '\t- 字段：Credit and cash offices/Country-wide/Stone...\n', '29. SELLERPLACE_AREA\n', '\t- 含义：先前申请卖方的销售面积\n', '\t- 字段：-1至4,000,000 (**异常值**)\n', '\t- 处理方法: **小于0的值替换为0**\n', '30. NAME_SELLER_INDUSTRY\n', '\t- 含义：卖方行业\n', '\t- 字段：**XNA**/Consumer electronics/Connectivity/Furniture...\n', '\t- 处理方法: **不做处理** \n', '31. CNT_PAYMENT\n', '\t- 含义：申请时的先前信用期限\n', '\t- 字段：0至84\n', '32. NAME_YIELD_GROUP\n', '\t- 含义：利率分级\n', '\t- 字段：**XNA**/middle/high/low_normal/low_action\n', '\t- 处理方法: **不做处理** \n', '33. PRODUCT_COMBINATION\n', '\t- 含义：产品组合细节\n', '\t- 字段：Cash/POS household with interest/POS mobile with interest...\n', '34. DAYS_FIRST_DRAWING\n', '\t- 含义：相对于当前申请的申请日期，先前申请第一次支付时间\n', '\t- 字段：-2,922至**365,243**\n', '\t- 处理方法: **365243替换为np.nan/取绝对值**\n', '35. DAYS_FIRST_DUE\n', '\t- 含义：相对于当前申请的申请日期，先前申请第一次应该到期时间\n', '\t- 字段：-2,892至**365,243**\n', '\t- 处理方法: **365243替换为np.nan/取绝对值**\n', '36. DAYS_LAST_DUE_1ST_VERSION\n', '\t- 含义：相对于当前申请的申请日期，先前申请第一次实际到期时间\n', '\t- 字段：-2,801至**365,243**\n', '\t- 处理方法: **365243替换为np.nan/取绝对值**\n', '37. DAYS_LAST_DUE\n', '\t- 含义：相对于当前申请的申请日期，先前申请最后一次实际到期时间\n', '\t- 字段：-2,889至**365,243**\n', '\t- 处理方法: **365243替换为np.nan/取绝对值**\n', '38. DAYS_TERMINATION\n', '\t- 含义：相对于当前申请的申请日期，先前申请期望的结束时间\n', '\t- 字段：-2,874至**365,243**\n', '\t- 处理方法: **365243替换为np.nan/取绝对值**\n', '39. NFLAG_INSURED_ON_APPROVAL\n', '\t- 含义：客户是否在先前申请里申请了保险\n', '\t- 字段：0/1\n', '\n', '# POS_CASH_balance.csv\n', '1. 数据量及描述\n', '\t- (10001358, 8) \n', '\t- 描述：申请人先前申请的POS贷和现金贷的月度余额快照 (POS贷含义：短期资金需求的小微企业主和个体工商户，缺乏传统银行所需要的担保、抵质押物等条件，融资困难，凭借POS机进行贷款，利用交易流水换贷款授信)\n', '2. SK_ID_PREV \n', '\t- 含义：与我们现在申请贷款相关的之前在Home Credit的申请 (一笔贷款可能会有之前的多笔贷款)\n', '\t- 字段：1000983/1026910...\n', '3. SK_ID_CURR \n', '\t- 含义：样本中贷款ID\n', '\t- 字段：10002/10003...\n', '4. MONTHS_BALANCE\n', '\t- 含义：相对于申请日期的月份余额（-1表示最新月度快照的信息，0表示申请时的信息 - 通常与-1相同，因为许多银行没有定期向信用局更新信息）\n', '\t- 字段：-96至-1\n', '\t- 处理方法: **取绝对值** \n', '5. CNT_INSTALMENT\n', '\t- 含义：以前的信用期限（可以随时间变化）\n', '\t- 字段：1至92\n', '6. CNT_INSTALMENT_FUTURE\n', '\t- 含义：待付的分期付款期数\n', '\t- 字段：0至85\n', '7. NAME_CONTRACT_STATUS\n', '\t- 含义：该月期间的合同状态\n', '\t- 字段：Active/Completed/Signed/Demand/**XNA**...\n', '\t- 处理方法: **不做处理**\n', '8. SK_DPD\t\n', '    - 含义：该月期间逾期天数\n', '\t- 字段：0至4231 (0居多)\n', '9. SK_DPD_DEF\n', '\t- 含义：该月期间逾期天数 (低贷债务被忽略)\n', '\t- 字段：0至3595\n', '\n', '# credit_card_balance.csv\n', '1. 数据量及描述\n', '\t- (3840312, 23) \n', '\t- 描述：申请人先前信用卡的月度余额快照\n', '2. SK_ID_PREV \n', '\t- 含义：与我们现在申请贷款相关的之前在Home Credit的申请 (一笔贷款可能会有之前的多笔贷款)\n', '\t- 字段：1000983/1026910...\n', '3. SK_ID_CURR \n', '\t- 含义：样本中贷款ID\n', '\t- 字段：10002/10003...\n', '4. MONTHS_BALANCE\n', '\t- 含义：相对于申请日期的月份分数余额（-1表示最新月度快照的信息)\n', '\t- 字段：-96至-1\n', '\t- 处理方法: **取绝对值**\n', '5. AMT_BALANCE\n', '\t- 含义：先前信贷的月度余额\n', '\t- 字段：-420,250.18至1,505,902.19\n', '\t- 处理方法: **不做处理**\n', '6. AMT_CREDIT_LIMIT_ACTUAL\n', '\t- 含义：信用卡月度限额\n', '\t- 字段：0至1,350,000\n', '7. AMT_DRAWINGS_ATM_CURRENT\n', '\t- 含义：月度从ATM提取的金额\n', '\t- 字段：-6,827.31至2,115,000 (**负数的意思？**)\n', '\t- 处理方法: **小于0的值替换为np.nan**\n', '8. AMT_DRAWINGS_CURRENT\n', '\t- 含义：月度提取的金额\n', '\t- 字段：-6,211.62至2,287,098.31 \n', '\t- 处理方法: **小于0的值替换为np.nan**\n', '9. AMT_DRAWINGS_OTHER_CURRENT\n', '\t- 含义：月度其它提取的金额\n', '\t- 字段：0至1,529,847\n', '10. AMT_DRAWINGS_POS_CURRENT\n', '\t- 含义：月度提取金额或购买商品金额\n', '\t- 字段：0至2,239,274.16\n', '11. AMT_INST_MIN_REGULARITY\n', '\t- 含义：本月最小分期付款金额\n', '\t- 字段：0至202,882.01\n', '12. AMT_PAYMENT_CURRENT\n', '\t- 含义：客户月度付款金额\n', '\t- 字段：0至4,289,207.45\n', '13. AMT_PAYMENT_TOTAL_CURRENT\n', '\t- 含义：客户月度付款总金额\n', '\t- 字段：0至4,278,315.69\n', '14. AMT_RECEIVABLE_PRINCIPAL\n', '\t- 含义：收款的本金金额\n', '\t- 字段：-423,305.82至1,472,316.79\n', '15. AMT_RECIVABLE\n', '\t- 含义：收款的金额\n', '\t- 字段：-420,250.18至1,493,338.19\n', '16. AMT_TOTAL_RECEIVABLE\n', '\t- 含义：总共收款的金额\n', '\t- 字段：-420,250.18至1,493,338.19\n', '17. CNT_DRAWINGS_ATM_CURRENT\n', '\t- 含义：月度从ATM提取的次数\n', '\t- 字段：0-51\n', '18. CNT_DRAWINGS_CURRENT\n', '\t- 含义：月度提取的次数\n', '\t- 字段：0-165\n', '19. CNT_DRAWINGS_OTHER_CURRENT\n', '\t- 含义：月度其它提取的次数\n', '\t- 字段：0-12\n', '20. CNT_DRAWINGS_POS_CURRENT\n', '\t- 含义：月度用于商品提取的次数\n', '\t- 字段：0-165\n', '21. CNT_INSTALMENT_MATURE_CUM\n', '\t- 含义：已付的分期次数\n', '\t- 字段：0-120\n', '22. NAME_CONTRACT_STATUS\n', '\t- 含义：合同状态\n', '\t- 字段：Active/Completed/Signed...\n', '23. SK_DPD\n', '    - 含义：该月期间逾期天数\n', '\t- 字段：0至3260 (0居多)\n', '24. SK_DPD_DEF\n', '\t- 含义：该月期间逾期天数 (低贷债务被忽略)\n', '\t- 字段：0至3260\n', '\n', '# installments_payments.csv\n', '1. 数据量及描述\n', '\t- (13605401, 8) \n', '\t- 描述：先前信贷的还款历史；一次分期付款相当于我们样本中与贷款相关的一个先前Home Credit信贷的一次付款。\n', '2. SK_ID_PREV \n', '\t- 含义：与我们现在申请贷款相关的之前在Home Credit的申请 (一笔贷款可能会有之前的多笔贷款)\n', '\t- 字段：1000983/1026910...\n', '3. SK_ID_CURR\n', '\t- 含义：样本中贷款ID\n', '\t- 字段：10002/10003...\n', '4. NUM_INSTALMENT_VERSION\n', ' \t- 含义：先前信贷的分期日历版本。月之间分期日历版本的更改意味着付款日历的一些参数已经改变了\n', '\t- 字段：0至178\n', '5. NUM_INSTALMENT_NUMBER\n', ' \t- 含义：在哪一期我们观察付款\n', '\t- 字段：1-277\n', '6. DAYS_INSTALMENT\n', '\t- 含义：分期应该何时付款\n', '\t- 字段：-2,922至-1\n', '\t- 处理方法: **取绝对值**\n', '7. DAYS_ENTRY_PAYMENT\n', '\t- 含义：分期实际在何时付款\n', '\t- 字段：-4,921至-1\n', '\t- 处理方法: **取绝对值**\n', '8. AMT_INSTALMENT\n', '\t- 含义：本期中规定的分期金额是多少？\n', '\t- 字段：0至3,771,487.85\n', '9. AMT_PAYMENT\n', '\t- 含义：客户实际在此次分期中付款多少\n', '\t- 字段：0至3,771,487.85\n']",applic train test csv bureau csv bureau balanc csv previou applic po cash balanc csv credit card balanc csv instal payment csv applic train test csv 1 train 307511 122 test 48744 121 2 sk id curr id 10002 10003 3 target 1yx 1 0 test 4 name contract type 1 cash loan revolv loan 5 code gender f xna 6 flag car n 7 flag realti n 8 cnt child 0 12 14 19 20 9 amt incom total 25 650117 000 000 100000000 1 np nan 10 amt credit 45 0004 050 000 11 amt annuiti 1 616258 026 12 amt good price 40 5004 050 000 13 name type suit unaccompani famili spous partner child b group peopl 14 name incom type work commerci associ pension state servant unemploy student businessman matern leav 15 name educ type secondari higher educ incomplet higher lower secondari academ degre 16 name famili statu marri singl civil marriag separ widow unknown civil marriag 17 name hous type hous apart parent municip apart rent apart offic apart co op apart 18 region popul rel 0 000 07 19 day birth 25 229 7 338 2069 20 day employ 17 912 365 243 365243np nan 21 day registr 24 6720 22 day id publish 7 197 1 717 23 car age 0 91 24 flag mobil 1 0 25 flag emp phone 1 0 26 flag work phone 1 0 27 flag cont mobil 1 0 28 flag phone flag work phone 1 0 29 flag email 1 0 30 occup type labor sale staff core staff manag driver high skill tech staff account medicin staff secur staff cook staff clean staff privat servic staff low skill labor waiter barman staff secretari realti agent hr staff staff 31 cnt fam member 1 21 32 region rate client 1 2 3 33 region rate client w citi 1 2 3 1 1 34 weekday appr process start mondaysunday 35 hour appr process start 0 23 36 reg region live region 1 0 37 reg region work region 1 0 38 live region work region 1 0 39 reg citi live citi 1 0 40 reg citi work citi 1 0 41 live citi work citi 1 0 42 organ type busi entiti type 3 xna self employ xna 43 ext sourc 1 0 1 44 ext sourc 2 0 1 45 ext sourc 3 0 1 46 apart avg avg mode medi 0 1 47 basementarea avg 0 1 48 year beginexpluat avg 0 1 49 year build avg 0 1 50 commonarea avg 0 1 51 elev avg 0 1 52 entranc avg 0 1 53 floorsmax avg 0 1 54 floorsmin avg 0 1 55 landarea avg 0 1 56 livingapart avg 0 1 57 livingarea avg 0 1 58 nonlivingapart avg 0 1 59 nonlivingarea avg 0 1 60 apart mode 0 1 61 basementarea mode 0 1 62 year beginexpluat mode 0 1 63 year build mode 0 1 64 commonarea mode 0 1 65 elev mode 0 1 66 entranc mode 0 1 67 floorsmax mode 0 1 68 floorsmin mode 0 1 69 landarea mode 0 1 70 livingapart mode 0 1 71 livingarea mode 0 1 72 nonlivingapart mode 0 1 73 nonlivingarea mode 0 1 74 apart medi 0 1 75 basementarea medi 0 1 76 year beginexpluat medi 0 1 77 year build medi 0 1 78 commonarea medi 0 1 79 elev medi 0 1 80 entranc medi 0 1 81 floorsmax medi 0 1 82 floorsmin medi 0 1 83 landarea medi 0 1 84 livingapart medi 0 1 85 livingarea medi 0 1 86 nonlivingapart medi 0 1 87 nonlivingarea medi 0 1 88 fondkapremont mode reg oper account reg oper spec account specifi org spec account 89 housetyp mode block flat specif hous terrac hous 90 totalarea mode 0 1 91 wallsmateri mode panel stone brick block wooden mix monolith other 92 emergencyst mode ye 93 ob 30 cnt social circl 30 0 354 354 100np nan 94 def 30 cnt social circl 30 0 34 95 ob 60 cnt social circl 60 0 351 100np nan 96 def 60 cnt social circl 60 0 24 97 day last phone chang 4361 0 98 flag document 2 2 0 1 99 flag document 3 3 0 1 100 flag document 4 4 0 1 101 flag document 5 5 0 1 102 flag document 6 6 0 1 103 flag document 7 7 0 1 104 flag document 8 8 0 1 105 flag document 9 9 0 1 106 flag document 10 10 0 1 107 flag document 11 11 0 1 108 flag document 12 12 0 1 109 flag document 13 13 0 1 110 flag document 14 14 0 1 111 flag document 15 15 0 1 112 flag document 16 16 0 1 113 flag document 17 17 0 1 114 flag document 18 18 0 1 115 flag document 19 19 0 1 116 flag document 20 20 0 1 117 flag document 21 21 0 1 118 amt req credit bureau hour 0 1 2 3 4 119 amt req credit bureau day 0 8 120 amt req credit bureau week 0 8 121 amt req credit bureau mon 0 26 122 amt req credit bureau qrt 0 261 261 100np nan 123 amt req credit bureau year 0 25 bureau csv 1 1716428 17 2 sk id curr id 100002 100003 3 sk id bureau id 5714462 5714463 4 credit activ close activ sold bad debt 5 credit currenc currenc 1 currenc 2 currenc 3 currenc 4 6 day credit 29220 7 credit day overdu 02792 8 day credit enddat 42 06031 199 20000np nan 115 9 day enddat fact 42 0230 42023 115 4000np nan 10 amt credit max overdu 0 115 987 185 00 0 10000000np nan 11 cnt credit prolong 0 9 0 12 amt credit sum 0 585 000 000 135000095100000002183 10000000np nan 13 amt credit sum debt 4 705 600 32170 100 000 84180 0 50000000np nan 14 amt credit sum limit 586 406 114 705 600 32 15 amt credit sum overdu 0 3 756 681 0 16 credit type consum credit credit card car loan mortgag microloan loan busi develop 17 day credit updat 41 947372 18 amt annuiti 0 118 453 423 10000000np nan 24 bureau balanc csv 1 27299925 3 train bureau bureau balanc 2 sk bureau id id 5714462 5714463 3 month balanc 1 960 4 statu c mean close x mean statu unknown 0 mean dpd 1 mean maxim month 1 30 2 mean dpd 31 60 5 mean dpd 120 sold written previou applic csv 1 1670214 37 home credit 2 sk id prev home credit 1000983 1026910 3 sk id curr id 100002 100003 4 name contract type cash loan consum loan revolv loan xna 5 amt annuiti 0418 058 6 amt applic 06 905 160 7 amt credit 06 905 160 8 amt payment 0 903 060 045 0 9 00 9 amt good price 06 905 160 10 weekday appr process start mondaysunday 11 hour appr process start 023 12 flag last appl per contract n 13 nflag last appl day 1 0 14 nflag micro cash 15 rate payment 0 01 0 16 rate interest primari 0 031 00 17 rate interest privileg 0 371 00 18 name cash loan purpos xap xna repair urgent need buy use car build hous annex 19 name contract statu approv cancel refus unus offer 20 day decis 2 922 1 21 name payment type cash bank xna non cash account cashless account employ 22 code reject reason xap nc xna limit 23 name type suit unaccompani famili spous partner 24 name client type repeat new refresh xna 25 name good categori xna mobil consum electron comput 26 name portfolio cashposcar cash po car card xna 27 name product type xna x sell walk 28 channel type credit cash offic countri wide stone 29 sellerplac area 14 000 000 00 30 name seller industri xna consum electron connect furnitur 31 cnt payment 084 32 name yield group xna middl high low normal low action 33 product combin cash po household interest po mobil interest 34 day first draw 2 922 365 243 365243np nan 35 day first due 2 892 365 243 365243np nan 36 day last due 1st version 2 801 365 243 365243np nan 37 day last due 2 889 365 243 365243np nan 38 day termin 2 874 365 243 365243np nan 39 nflag insur approv 0 1 po cash balanc csv 1 10001358 8 po pospo 2 sk id prev home credit 1000983 1026910 3 sk id curr id 10002 10003 4 month balanc 10 1 96 1 5 cnt instal 192 6 cnt instal futur 085 7 name contract statu activ complet sign demand xna 8 sk dpd 04231 0 9 sk dpd def 03595 credit card balanc csv 1 3840312 23 2 sk id prev home credit 1000983 1026910 3 sk id curr id 10002 10003 4 month balanc 1 96 1 5 amt balanc 420 250 181 505 902 19 6 amt credit limit actual 01 350 000 7 amt draw atm current atm 6 827 312 115 000 0np nan 8 amt draw current 6 211 622 287 098 31 0np nan 9 amt draw current 01 529 847 10 amt draw po current 02 239 274 16 11 amt inst min regular 0202 882 01 12 amt payment current 04 289 207 45 13 amt payment total current 04 278 315 69 14 amt receiv princip 423 305 821 472 316 79 15 amt reciv 420 250 181 493 338 19 16 amt total receiv 420 250 181 493 338 19 17 cnt draw atm current atm 0 51 18 cnt draw current 0 165 19 cnt draw current 0 12 20 cnt draw po current 0 165 21 cnt instal matur cum 0 120 22 name contract statu activ complet sign 23 sk dpd 03260 0 24 sk dpd def 03260 instal payment csv 1 13605401 8 home credit 2 sk id prev home credit 1000983 1026910 3 sk id curr id 10002 10003 4 num instal version 0178 5 num instal number 1 277 6 day instal 2 922 1 7 day entri payment 4 921 1 8 amt instal 03 771 487 85 9 amt payment 03 771 487 85
2988,"**Hi Guys!**

For those, who want to play with **automated feature engineering** - please find below small example of how to do it with [featuretools](https://docs.featuretools.com/)

Featuretools is a framework to perform automated feature engineering. 
<br>It (at least it's stated that) excels at transforming transactional and relational datasets into feature matrices for machine learning.",hi guy want play autom featur engin plea find small exampl featuretool http doc featuretool com featuretool framework perform autom featur engin br least state excel transform transact relat dataset featur matric machin learn
2989,### Load Main table (Applications),load main tabl applic
2990,### Load previous applications table,load previou applic tabl
2991,## TABLE JOINING (FEATURE TOOLS),tabl join featur tool
2992,"An `EntitySet` is a collection of entities and the relationships between them. 

They are useful for preparing raw, structured datasets for feature engineering. 
<br>While many functions in Featuretools take `entities` and `relationships` as separate arguments,
<br>it is recommended to create an `EntitySet`, so you can more easily manipulate your data as needed.",entityset collect entiti relationship use prepar raw structur dataset featur engin br mani function featuretool take entiti relationship separ argument br recommend creat entityset easili manipul data need
2993,"In the call to `entity_from_dataframe`, we specified three important parameters

- The `index` parameter specifies the column that uniquely identifies rows in the dataframe
- The `time_index` parameter tells Featuretools when the data was ""created"".
- The `variable_types` parameter indicates that some columns should be interpreted as a Categorical variable, 
even though it just an integer in the underlying data.
",call entiti datafram specifi three import paramet index paramet specifi column uniqu identifi row datafram time index paramet tell featuretool data creat variabl type paramet indic column interpret categor variabl even though integ underli data
2994,"**Adding a Relationship**

With two entities in our entity set, we can add a relationship between them.

We want to relate these two entities by the columns called “SK_ID_CURR” in each entity. 
<br>Each application has multiple previous applications associated with it, 
<br>so it is called it the parent entity, while the previous applications  entity is known as the child entity. 
<br>When specifying relationships we list the variable in the parent entity first. Note that each ft.Relationship must denote a **one-to-many relationship** rather than a relationship which is one-to-one or many-to-many.",ad relationship two entiti entiti set add relationship want relat two entiti column call sk id curr entiti br applic multipl previou applic associ br call parent entiti previou applic entiti known child entiti br specifi relationship list variabl parent entiti first note ft relationship must denot one mani relationship rather relationship one one mani mani
2995,"**Feature primitives**

Feature primitives are the building blocks of Featuretools. They define individual computations that can be applied to raw datasets to create new features. Because a primitive only constrains the input and output data types, they can be applied across datasets and can stack to create new calculations.

**Why primitives?**

The space of potential functions that humans use to create a feature is expansive. By breaking common feature engineering calculations down into primitive components, we are able to capture the underlying structure of the features humans create today.

See [documentation](https://docs.featuretools.com/automated_feature_engineering/primitives.html) for further details",featur primit featur primit build block featuretool defin individu comput appli raw dataset creat new featur primit constrain input output data type appli across dataset stack creat new calcul primit space potenti function human use creat featur expans break common featur engin calcul primit compon abl captur underli structur featur human creat today see document http doc featuretool com autom featur engin primit html detail
2996,"**Handling time**

When performing feature engineering to learn a model to predict the future, 
<br>the value to predict will be associated with a time. 
In this case, it is paramount to only incorporate data prior to this `“cutoff time”` when calculating the feature values.

Featuretools is designed to take time into consideration when required. 
<br>By specifying a cutoff time, we can control what portions of the data are used when calculating features.

We can specify the time for each instance of the `target_entity` to calculate features. 
<br>The timestamp represents the last time data can be used for calculating features. This is specified using a dataframe of cutoff times. 

Read more [here](https://docs.featuretools.com/automated_feature_engineering/handling_time.html)",handl time perform featur engin learn model predict futur br valu predict associ time case paramount incorpor data prior cutoff time calcul featur valu featuretool design take time consider requir br specifi cutoff time control portion data use calcul featur specifi time instanc target entiti calcul featur br timestamp repres last time data use calcul featur specifi use datafram cutoff time read http doc featuretool com autom featur engin handl time html
2997,"**Running DFS with training windows**

Training windows are an extension of cutoff times: starting from the cutoff time and moving backwards through time, only data within that window of time will be used to calculate features. We will use events **within 2 month time window**",run df train window train window extens cutoff time start cutoff time move backward time data within window time use calcul featur use event within 2 month time window
2998,"**Hope this small example inspired you to try this approach yourself! 
<br>Have fun - add custom features, add more tables and relationships, gather hands on experience**

**Likes and comments are welcome :)**",hope small exampl inspir tri approach br fun add custom featur add tabl relationship gather hand experi like comment welcom
2999,"Here's how I load the data and reduce the memory usage of each dataframe.  I can save from 60% to 75% of memory usage on each dataframe.  
This method is inspired from this [kernel](https://www.kaggle.com/arjanso/reducing-dataframe-memory-size-by-65). I don't handle NANs at this point.  
Hope it helps.",load data reduc memori usag datafram save 60 75 memori usag datafram method inspir kernel http www kaggl com arjanso reduc datafram memori size 65 handl nan point hope help
3000,"This kernel shows how one can try to explain the predictions of a given boosted tree model using the lib SHAP https://github.com/slundberg/shap

The model is then retrained only with the best features to avoid fitting to noise and improve our LB score ;)

It is based on the best performing public script and some other variants: i.e. https://www.kaggle.com/jsaguiar/updated-0-792-lb-lightgbm-with-simple-features et al.",kernel show one tri explain predict given boost tree model use lib shap http github com slundberg shap model retrain best featur avoid fit nois improv lb score base best perform public script variant e http www kaggl com jsaguiar updat 0 792 lb lightgbm simpl featur et al
3001,## Using SHAP (SHapley Additive exPlanations),use shap shapley addit explan
3002,"## Select Best Features
To avoid fitting to noise and improve our LB score ;)",select best featur avoid fit nois improv lb score
3003,**Thank you everyone for showing your appreciation and support. It's my first gold medal in kernels and I hope to publish far better kernels than this in near future.**,thank everyon show appreci support first gold medal kernel hope publish far better kernel near futur
3004,## Blend with one rank weighted submission [0.8 LB],blend one rank weight submiss 0 8 lb
3005,"## Diversified blend [0.799 LB]


**The blending ingredients are taken from three different type of models.**",diversifi blend 0 799 lb blend ingredi taken three differ type model
3006,## Blending lowest correlated models,blend lowest correl model
3008,"# Home Credit Default Risk Competition

## Why I Wrote this Kernel

This was my first Kaggle competition, and through several months of practice, research, trial and error, as well as extensive exploration of forum advice and kernels written by expert Kagglers, I was able to build a featureset/model that earned a final private leaderboard score of `0.79506` as a solo submission, which translated to final rank of 561 out of 7,198 -- just inside the top 8%.

Along the way, I had to figure out from scratch several things that seasoned Kagglers probably take for granted. These included things like:
* Are my Pandas operations as efficient as can be?
* How do I keep my memory use from ballooning out of control?
* What kind of cross validation should I use? And how do I implement it?
* Choosing a learning rate and tuning hyperparameters.
* Deciding how many boosting rounds to use during training.
* Should I use only one, or multiple training rounds?
* If more than one, how do I blend predictions from each training round?
* And finally, exactly how do I create that CSV file that I'll need to submit?

I wanted to publish this kernel in order create an example (both for my future self and for other beginners) of what it looks like to implement all the steps (preprocessing --> feature engineering --> cross validation --> training --> prediction generation --> post processing) that are necessary to build a (somewhat) competitive prediction algorithm. In other words, I wanted to demonstrate what it looks like to *""put it all together.""*

This is just single model LightGBM kernel, without any stacking. I'm planning to dive much deeper into stacking/blending/etc. in my next competition : )

I am indebted to the generosity of the community at large, and in particular, to the wisdom and techniques shared by folks like  [Silogram](https://www.kaggle.com/psilogram), [olivier](https://www.kaggle.com/ogrellier), and [Laurae](https://www.kaggle.com/laurae2).

Although I still consider myself to be a beginner who has a lot more to learn, these folks and their generosity have inspired me to do what little I can to give back. I hope that certain aspects of my code below, such as how my cross-validation method includes target encoding in a way that (I believe) prevents data leak, will be useful to others in the community. 

Finally, I welcome any tips and or feedback on my approach and implementation that follows below. Having one's blindspots pointed out to them is the surest way to growth and improvement  : )",home credit default risk competit wrote kernel first kaggl competit sever month practic research trial error well extens explor forum advic kernel written expert kaggler abl build featureset model earn final privat leaderboard score 0 79506 solo submiss translat final rank 561 7 198 insid top 8 along way figur scratch sever thing season kaggler probabl take grant includ thing like panda oper effici keep memori use balloon control kind cross valid use implement choos learn rate tune hyperparamet decid mani boost round use train use one multipl train round one blend predict train round final exactli creat csv file need submit want publish kernel order creat exampl futur self beginn look like implement step preprocess featur engin cross valid train predict gener post process necessari build somewhat competit predict algorithm word want demonstr look like put togeth singl model lightgbm kernel without stack plan dive much deeper stack blend etc next competit indebt generos commun larg particular wisdom techniqu share folk like silogram http www kaggl com psilogram olivi http www kaggl com ogrelli laura http www kaggl com laurae2 although still consid beginn lot learn folk generos inspir littl give back hope certain aspect code cross valid method includ target encod way believ prevent data leak use other commun final welcom tip feedback approach implement follow one blindspot point surest way growth improv
3009,"## Summary of My Kernel

Single model LightGBM with 1,275 features in total. Solo submission. This was my best performing kernel, though not my final submission (more later on as to why this was the case). `0.79581` local CV score. `0.79524` private LB score. `0.79831` public LB score:


* I incorporated features from all seven of the data tables. I one-hot encoded categorical features from the bureau and previous application data tables. All other categorical features were left as categorical, and were ultimately target encoded. Some categorical features in the bureau and previous application tables were also target encoded.


* I tried my best to isolate unhelpful features and drop them. This proved to be time-consuming and it was soon clear to me that the process of vetting features one-by-one would not scale to the size of the competition's featureset. I had tried using LightGBM and SelectKBest feature importances to guide me, but found that they were more of a red herring than anything else, in that those importances did not reliably predict how a feature's absence would affect my local CV's ROC AUC score.


* I experimented with various types of scaling and normalization of numerical features, such as log-normalization, replacing NaN entries with 0, -1, -999999, etc., but found that none of these tweaks helped the performance of my LightGBM model. (This makes sense since LGBM is a tree-based model.)


* Several of my engineered features were simple aggregations, using mean, sum, min, max, etc. At the same time, I created a handful of bespoke features that made intuitive sense to me. 


* Stratified 5-fold CV for model selection.


* In addition to trying to add/remove features one-by-one, at times I tried adding/removing features in bulk (in the interest of saving time). It's not clear to me if one approach is necessarily better than the other all the time.

  I would find that there were times I would add one feature, see my CV score drop, then add several more features, re-tune my model parameters, see the CV score improve, then try and remove that first feature that had originally lowered my CV score, only to see the CV score fall after the feature was removed. This experience only strengthened my hunch that successful data science has an element of artfulness and intuition.
  

* I used LightGBM's built-in CV for feature selection and hyperparameter tuning. It trains and makes predictions on each validation fold in parallel, so much time is saved over running serial K-fold CV. Unfortunately, lightgbm.cv doesn't currently support the kind of preprocessing inside CV folds that would be necessary to properly perform target encoding during CV without leakage. (Believe me, I tried.)


* I found that using target encoding added just under 0.001 to my local CV and public LB scores. (Because lightgbm.cv doesn't support target encoding preprocessing for each fold, I had to use standard serial K-fold CV to do an apples-apples comparison of the performance of target encoding vs. merely using lightgbm's default categorical feature handling.)

  Interestingly, it ultimately turned out that the private LB score of my model that used target encoding (with number of boosting rounds determined by serial CV) was only just under 0.0001 better than that of my model that used LightGBM's default handling of categorical features (and had its number of boosting rounds determined by lightgbm.cv). If I had it to do over again, I'm not sure I'd use target encoding.
  

* To generate test set predictions, I trained my final model five times, each time using a different random seed for my LightGBM parameters, and generated five sets of test predictions. I used mean ranking to blend the sets of predictions. The number of boosting rounds was equal to 110% of the average round of highest score across each of the CV folds.

  Had I not used target encoding, I could have used lightgbm.cv and found the number of the actual single round where the average CV score across all folds was the highest. As it was, I had to settle for finding the round number of highest score for each of my five CV folds, and then take the average of those five round numbers.


* I didn't experiment with any other sort of ensemble methods such as blending different model types, or stacking. I am saving that for my next competition :)


### A Lesson I Learned About Overfitting to the Training Set:
As mentioned above, although this was my best performing kernel, it was only my second-to-final submission to the competition. In the final four days of the competition, I went on a spree of feature engineering/aggregation that more than doubled my feature count, arriving at a grand total of 2,782 features. This of course substantially decreased the speed of my data preprocessing and model training, but it did increase my local CV score from `0.79581` to `0.79665`. I figured the higher score was worth it and tagged that set of predictions as my final submission.

Unfortunately, while doing this raised my public LB score from `0.79831` to `0.80003`, it turned out that it would eventually lower my private LB score from `0.79524` to `0.79506`. Thankfully, this wasn't too huge a drop, and I was still able to achieve a bronze medal and score within the top 8% of the competition with my `0.79506` submission. 

In retrospect, although my local CV score did increase after adding all those extra features, I should have been suspicious because the difference between training and validation scores on each fold of my CV increased by nearly 50%. This was likely evidence that my model was now doing some serious overfitting in order to achieve that slight bump in local CV score. ",summari kernel singl model lightgbm 1 275 featur total solo submiss best perform kernel though final submiss later case 0 79581 local cv score 0 79524 privat lb score 0 79831 public lb score incorpor featur seven data tabl one hot encod categor featur bureau previou applic data tabl categor featur left categor ultim target encod categor featur bureau previou applic tabl also target encod tri best isol unhelp featur drop prove time consum soon clear process vet featur one one would scale size competit featureset tri use lightgbm selectkbest featur import guid found red her anyth el import reliabl predict featur absenc would affect local cv roc auc score experi variou type scale normal numer featur log normal replac nan entri 0 1 999999 etc found none tweak help perform lightgbm model make sen sinc lgbm tree base model sever engin featur simpl aggreg use mean sum min max etc time creat hand bespok featur made intuit sen stratifi 5 fold cv model select addit tri add remov featur one one time tri ad remov featur bulk interest save time clear one approach necessarili better time would find time would add one featur see cv score drop add sever featur tune model paramet see cv score improv tri remov first featur origin lower cv score see cv score fall featur remov experi strengthen hunch success data scienc element art intuit use lightgbm built cv featur select hyperparamet tune train make predict valid fold parallel much time save run serial k fold cv unfortun lightgbm cv current support kind preprocess insid cv fold would necessari properli perform target encod cv without leakag believ tri found use target encod ad 0 001 local cv public lb score lightgbm cv support target encod preprocess fold use standard serial k fold cv appl appl comparison perform target encod v mere use lightgbm default categor featur handl interestingli ultim turn privat lb score model use target encod number boost round determin serial cv 0 0001 better model use lightgbm default handl categor featur number boost round determin lightgbm cv sure use target encod gener test set predict train final model five time time use differ random seed lightgbm paramet gener five set test predict use mean rank blend set predict number boost round equal 110 averag round highest score across cv fold use target encod could use lightgbm cv found number actual singl round averag cv score across fold highest settl find round number highest score five cv fold take averag five round number experi sort ensembl method blend differ model type stack save next competit lesson learn overfit train set mention although best perform kernel second final submiss competit final four day competit went spree featur engin aggreg doubl featur count arriv grand total 2 782 featur cours substanti decreas speed data preprocess model train increas local cv score 0 79581 0 79665 figur higher score worth tag set predict final submiss unfortun rais public lb score 0 79831 0 80003 turn would eventu lower privat lb score 0 79524 0 79506 thank huge drop still abl achiev bronz medal score within top 8 competit 0 79506 submiss retrospect although local cv score increas ad extra featur suspici differ train valid score fold cv increas nearli 50 like evid model seriou overfit order achiev slight bump local cv score
3010,"## Characteristics of the Competition and its Dataset

This [competition](https://www.kaggle.com/c/home-credit-default-risk) ran for three months, from May 17 to August 29, 2018. The objective was to build an algorithm that could predict the likelihood that a loan applicant would eventually default on his or her loan. The training set contained various financial and personal information originally taken from the loan application profiles 307,511 previous Home Credit borrowers. The test set had 48,744 borrower records. The scoring metric was area under the ROC curve. Features were contained in seven different data tables. The largest table contained demographic information such as job type and gender, along with various numerical features that described a borrower's financial status, such as normalized credit rating scores. Each of the six supplementary data tables contained different kinds of detailed financial records. (e.g. credit card payment histories, loan payment histories recorded as recorded by the credit bureau, etc.)

As for the content of the dataset itself, several features were noticeably sparse, and it was clear to me early-on that I would need an algorithm that handled NaN entries as deftly as possible. Furthermore, some features had names and descriptions that were cryptic or vague at best. At times this made it tough to gain an intuition of how to best engineer new features. There were even two features that were incorrectly described as normalized even though they were clearly categorical (they contained word strings as entries). Finally, several features didn't always have missing values represented by np.nan. For some numerical features, the integer 365243 was equivalent to NaN. For certain categorical features, the strings 'XNA' or 'XAP' were used to denote missing entries. None of this information was included in the dataset's description, but was shared in the forum by Home Credit's liaison during the course of the competition.

These speedbumps added a certain element of challenge that helped to level the playing field. Namely, there seemed to be a larger than normal benefit to those teams taking the time to diligently explore and understand all the quirks and idiosyncrasies of the dataset. Simply having the most advanced stacking or ensembling methods would not be enough to guarantee victory in this competition.

Most importantly, however, the consensus amongst competitors was that Home Credit's team went above and beyond in curating the test set such that data leak was minimized as much as possible. Furthermore, Home Credit's representative was active and responsive on the forums throughout the duration of the competition, and was helpful in clearing up questions that competitors had regarding the dataset and its feature definitions.

At the time of competition's conclusion, 7,198 teams had submitted entries, making this the largest ever featured competition in Kaggle's history. The [top team](https://www.kaggle.com/c/home-credit-default-risk/discussion/64821) achieved a private leaderboard score of `0.80570`.",characterist competit dataset competit http www kaggl com c home credit default risk ran three month may 17 august 29 2018 object build algorithm could predict likelihood loan applic would eventu default loan train set contain variou financi person inform origin taken loan applic profil 307 511 previou home credit borrow test set 48 744 borrow record score metric area roc curv featur contain seven differ data tabl largest tabl contain demograph inform job type gender along variou numer featur describ borrow financi statu normal credit rate score six supplementari data tabl contain differ kind detail financi record e g credit card payment histori loan payment histori record record credit bureau etc content dataset sever featur notic spar clear earli would need algorithm handl nan entri deftli possibl furthermor featur name descript cryptic vagu best time made tough gain intuit best engin new featur even two featur incorrectli describ normal even though clearli categor contain word string entri final sever featur alway miss valu repres np nan numer featur integ 365243 equival nan certain categor featur string xna xap use denot miss entri none inform includ dataset descript share forum home credit liaison cours competit speedbump ad certain element challeng help level play field name seem larger normal benefit team take time dilig explor understand quirk idiosyncrasi dataset simpli advanc stack ensembl method would enough guarante victori competit importantli howev consensu amongst competitor home credit team went beyond curat test set data leak minim much possibl furthermor home credit repres activ respons forum throughout durat competit help clear question competitor regard dataset featur definit time competit conclus 7 198 team submit entri make largest ever featur competit kaggl histori top team http www kaggl com c home credit default risk discus 64821 achiev privat leaderboard score 0 80570
3011,"## My Journey Toward Participating in this Competition

I began this competition in early June of 2018, in order to complete my [final project](https://github.com/jamesdellinger/machine_learning_nanodegree_capstone_project) for Udacity's Machine Learning Engineer Nanodegree.

I spent about a month exploring the idiosyncracies of the main data table's (`application_{train|test}.csv`) 120 features, experimented with various single model predictors (Naive Bayes, Logistic Regression, AdaBoost, Multi-Layer Perceptron, and LightGBM), and alternately trained each of them on the table's full feature set, on just the top 30 features according to SelectKBest, and on a featureset where the dimensionality of the main table's numerical features was reduced using PCA. I compared the performance of each of these models using a simple 80%-20% train-validation set split. 

Ultimately, my LightGBM single model that was trained on all 120 main table features performed the best, with a local CV score (ROC AUC) of `0.76092`. This translated to a public leaderboard score of `0.74111`. 

At this point, I was pleased that I had put into practice several of the techniques and algorithms I had learned while completing the Machine Learning Engineer Nanodegree. However, I was by no means satisfied that I had done all I could do to build a kernel that was *competitive* on the Home Credit dataset. This spurred me to spend the remainder of the competition learning and applying new techniques and approaches.",journey toward particip competit began competit earli june 2018 order complet final project http github com jamesdelling machin learn nanodegre capston project udac machin learn engin nanodegre spent month explor idiosyncraci main data tabl applic train test csv 120 featur experi variou singl model predictor naiv bay logist regress adaboost multi layer perceptron lightgbm altern train tabl full featur set top 30 featur accord selectkbest featureset dimension main tabl numer featur reduc use pca compar perform model use simpl 80 20 train valid set split ultim lightgbm singl model train 120 main tabl featur perform best local cv score roc auc 0 76092 translat public leaderboard score 0 74111 point plea put practic sever techniqu algorithm learn complet machin learn engin nanodegre howev mean satisfi done could build kernel competit home credit dataset spur spend remaind competit learn appli new techniqu approach
3012,"## Credit Where Credit is Due

I believe that apprenticeship and discipleship are some of the most effective ways to learn, and I have certainly benefited from being an indirect ""apprentice"" of some very generous world-class Kagglers. They have initiated and participated in extended threads in the forums, where they patiently described the ins and outs of their approaches and implementations. They have also shared kernels that contain real, usable implementations of some of the most important techniques. These kernels are far more useful than the run-of-the-mill blog posts that tend to introduce and wax poetic about the philosophy behind some advanced technique (such as stacking), yet leave the uninitiated woefully unprepared for the nitty-gritty and the subtle, yet crucial, details of the technique's actual implementation. 

I am particularly indebted to:

* [Silogram](https://www.kaggle.com/psilogram), whose extensive comments and advice on this [thread](https://www.kaggle.com/c/home-credit-default-risk/discussion/58332#348689) helped me to learn about mean rank prediction blending, a heuristic for deciding the number of training rounds, the usefulness of LightGBM's built-in CV, and most importantly, underscored for me the importance of trusting my CV and not giving in to the temptation to overfit to the public leaderboard.


* [olivier](https://www.kaggle.com/ogrellier), whose three kernels showed me how to implement [target encoding](https://www.kaggle.com/ogrellier/python-target-encoding-for-categorical-features/notebook), the different ways I could create [aggregate](https://www.kaggle.com/ogrellier/home-credit-hyperopt-optimization) features from the various data tables, as well as the code for using Seaborn to plot LightGBM [feature importance](https://www.kaggle.com/ogrellier/good-fun-with-ligthgbm/code).


* [Laurae](https://www.kaggle.com/laurae2), whose [masterpiece of a website](https://sites.google.com/view/lauraepp/parameters) taught me more than I could have dreamed about LightGBM, its parameters, and how to tune them.


* [neptune-ml](https://neptune.ml/), whose [open solution](https://github.com/neptune-ml/open-solution-home-credit/blob/solution-5/notebooks/eda-application.ipynb) showed me several features that I could engineer and aggregate from the main data table's featureset.",credit credit due believ apprenticeship discipleship effect way learn certainli benefit indirect apprentic gener world class kaggler initi particip extend thread forum patient describ in out approach implement also share kernel contain real usabl implement import techniqu kernel far use run mill blog post tend introduc wax poetic philosophi behind advanc techniqu stack yet leav uniniti woefulli unprepar nitti gritti subtl yet crucial detail techniqu actual implement particularli indebt silogram http www kaggl com psilogram whose extens comment advic thread http www kaggl com c home credit default risk discus 58332 348689 help learn mean rank predict blend heurist decid number train round use lightgbm built cv importantli underscor import trust cv give temptat overfit public leaderboard olivi http www kaggl com ogrelli whose three kernel show implement target encod http www kaggl com ogrelli python target encod categor featur notebook differ way could creat aggreg http www kaggl com ogrelli home credit hyperopt optim featur variou data tabl well code use seaborn plot lightgbm featur import http www kaggl com ogrelli good fun ligthgbm code laura http www kaggl com laurae2 whose masterpiec websit http site googl com view lauraepp paramet taught could dream lightgbm paramet tune neptun ml http neptun ml whose open solut http github com neptun ml open solut home credit blob solut 5 notebook eda applic ipynb show sever featur could engin aggreg main data tabl featureset
3013,## I. Preprocessing and Feature Engineering,preprocess featur engin
3014,### Preprocessing Helper Functions,preprocess helper function
3015,###  1. Main Data Table `application_{train|test}.csv`,1 main data tabl applic train test csv
3016,### 2. Bureau Data Table `bureau.csv`,2 bureau data tabl bureau csv
3017,### 3. Bureau Balance Data Table `bureau_balance.csv`,3 bureau balanc data tabl bureau balanc csv
3018,### 4. Previous Application Data Table `previous_application.csv`,4 previou applic data tabl previou applic csv
3019,### 5. POS CASH Balance Data Table `POS_CASH_balance.csv`,5 po cash balanc data tabl po cash balanc csv
3020,### 6. Installments Payments Data Table `installments_payments.csv`,6 instal payment data tabl instal payment csv
3021,### 7. Credit Card Balance Data Table `credit_card_balance.csv`,7 credit card balanc data tabl credit card balanc csv
3022,### Engineer Features by Combining Features From Various Tables,engin featur combin featur variou tabl
3023,### Drop Unhelpful Features,drop unhelp featur
3024,### Executing All Preprocessing and Feature Engineering:,execut preprocess featur engin
3025,## II. Training & Cross-Validation,ii train cross valid
3026,### Displaying Feature Importances,display featur import
3027,### Target Encoding Helper Functions for Categorical Features,target encod helper function categor featur
3028,"### Train LightGBM Model

My method for training a LightGBM model. It's called below when I run serial CV and when I generate test predictions.",train lightgbm model method train lightgbm model call run serial cv gener test predict
3029,"### LightGBM Parameters

Commented-out hyperparameter values give an indication of the different values I tried on my journey to ending up at my best-performing combination. 

I first try to pick a combination of 'max_depth'/'num_leaves' that is deep/large enough for the dataset without overfitting. At the same time, I choose the highest possible learning rate (usually 0.2, 0.1, or 0.009) and then observe how the CV score changes as I adjust other hyperparameter values. 

""Highest possible learning rate"" means: a learning rate that gives the model the chance to run for enough boosting rounds so that I can confirm that the model is incrementally learning from round to round. If the learning rate is too high, the model's score won't steadily improve from round to round, and we won't be able to observe how tuning other hyperparameters affects overall performance. If the learning rate is too low, we'll be taking an unnecessarily long time to make these observations.

I like to tune hyperparameters one-by-one, and empirically observe how the CV score changes. Sometimes, but not that often, I will adjust pairs of hyperparameters together if I believe that the two hyperparameters have a unique interrelationship. I'm not a fan of computational tools like GridSearchCV. I find that simple, rote trial and error gives me a far stronger intuition about how the different hyperparameters are responding to my dataset and affecting CV score, in a much shorter amount of time than it takes GridSearchCV to finish churning through all the different hyperparamter combos while running on my laptop's CPU.

Learning the fundamentals of decision trees, LightGBM specifically, and what each of its hyperparameters purports to do helped me to begin to be able to make mental shortcuts of know what hyperparameters to tune, and when and how to tune them. The following two resources helped me immensely:

1. The paper on LightGBM: https://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree.pdf

2. Laurea's site that explains all hyperparameter values for LightGBM/XGBoost: https://sites.google.com/view/lauraepp/parameters

After tuning the various hyperparameters at the highest possible learning rate, such that I can confirm improvements in performance, I then lower my learning rate to the value that will maximize my model's performance. This will be the learning rate I use when training and making predictions.",lightgbm paramet comment hyperparamet valu give indic differ valu tri journey end best perform combin first tri pick combin max depth num leav deep larg enough dataset without overfit time choos highest possibl learn rate usual 0 2 0 1 0 009 observ cv score chang adjust hyperparamet valu highest possibl learn rate mean learn rate give model chanc run enough boost round confirm model increment learn round round learn rate high model score steadili improv round round abl observ tune hyperparamet affect overal perform learn rate low take unnecessarili long time make observ like tune hyperparamet one one empir observ cv score chang sometim often adjust pair hyperparamet togeth believ two hyperparamet uniqu interrelationship fan comput tool like gridsearchcv find simpl rote trial error give far stronger intuit differ hyperparamet respond dataset affect cv score much shorter amount time take gridsearchcv finish churn differ hyperparamt combo run laptop cpu learn fundament decis tree lightgbm specif hyperparamet purport help begin abl make mental shortcut know hyperparamet tune tune follow two resourc help immens 1 paper lightgbm http paper nip cc paper 6907 lightgbm highli effici gradient boost decis tree pdf 2 laurea site explain hyperparamet valu lightgbm xgboost http site googl com view lauraepp paramet tune variou hyperparamet highest possibl learn rate confirm improv perform lower learn rate valu maxim model perform learn rate use train make predict
3030,"### Perform Cross Validation Using LightGBM's Built-In CV (all folds run in parallel)

The advantages of using LightGBM's built-in CV are that it not only trains all folds in parallel, but that it also keeps track of the *average* ROC AUC score *across all folds* for each boosting round. This allows me to know the exact round when the average ROC AUC score across all folds was at its maximum.

Unfortunately, lightgbm.cv doesn't support the kind of preprocessing of data within its folds that would be necessary to perform target encoding in a way that doesn't lead to data leakage. Nonetheless, with enough tuning of the parameters related to LightGBM's default handling of categorical features (in particular, reducing 'max_cat_threshold' from 32 to 4), I was able to get the CV score of lightgbm.cv to within 0.0002 below the CV score of my serial CV that computed the average best score across all folds.

Due to its much more rapid training time, I found it helpful to use lightgbm.cv while adding/dropping/engineering new features and tuning hyperparameters. By enabling 'verbose_eval', lightgbm.cv gives me the clearest possible picture of how my choice of learning rate is affecting how my model learns overall. This was invaluable in helping me to decide on a good temporary learning rate to use when tuning all the other hyperparameters, as well as finding the optimal learning rate (low enough, but not too low) to use for final training and test prediction generation. 

For this competition, I ultimately chose to use target encoding, which required me to implement standard, serial stratified K-Fold CV to get my final local CV score. I explain more about that decision below. ",perform cross valid use lightgbm built cv fold run parallel advantag use lightgbm built cv train fold parallel also keep track averag roc auc score across fold boost round allow know exact round averag roc auc score across fold maximum unfortun lightgbm cv support kind preprocess data within fold would necessari perform target encod way lead data leakag nonetheless enough tune paramet relat lightgbm default handl categor featur particular reduc max cat threshold 32 4 abl get cv score lightgbm cv within 0 0002 cv score serial cv comput averag best score across fold due much rapid train time found help use lightgbm cv ad drop engin new featur tune hyperparamet enabl verbos eval lightgbm cv give clearest possibl pictur choic learn rate affect model learn overal invalu help decid good temporari learn rate use tune hyperparamet well find optim learn rate low enough low use final train test predict gener competit ultim chose use target encod requir implement standard serial stratifi k fold cv get final local cv score explain decis
3031,"### Stratified 5-Fold Cross Validation (performed serially, fold-by-fold, using target encoding)

Categorical feature target encoding is performed five times (for each of the five validation folds). This is necessary in order to prevent data leakage.

### Why I decided to use target encoding:
Performing serial cross validation is obviously much slower than using LightGBM's built-in CV, which trains and predicts on all five folds in parallel. However, after making the apples-to-apples comparison of performing serial CV, first using target encoding, and then again using LightGBM's default handling of categorical features, I found that I got a higher (by `0.001`) CV score when using target encoding. This was enough for me to conclude that accepting the limitations of not using lightgbm.cv was worth it as a tradeoff for the higher score I got using target encoding vs. not doing target encoding, all other conditions being held constant.

The single biggest trade-off, of course, is that using serial CV probably does a slightly worse job of approximating the true ideal number of boosting rounds for training my model, in that I can only know the *average round* of best score, as opposed to *the single round* when the *average score across* all 5 folds was best.

### However, knowing what I know now:
As I mentioned in my comments at the beginning of this kernel, it would ultimately turn out that training a predictor using LightGBM's default categorical feature handling, where the number of boosting rounds was determined by the results of running lightgbm.cv, would result in a private LB score only just `0.0001` lower than the private LB score earned by my model that used target encoding and had its number of boosting rounds determined by the results of running serial CV. 

My conclusion is that with the proper hyperparameter tuning, there may not be much advantage to using target encoding with LightGBM, at least not for the Home Credit competition's dataset.",stratifi 5 fold cross valid perform serial fold fold use target encod categor featur target encod perform five time five valid fold necessari order prevent data leakag decid use target encod perform serial cross valid obvious much slower use lightgbm built cv train predict five fold parallel howev make appl appl comparison perform serial cv first use target encod use lightgbm default handl categor featur found got higher 0 001 cv score use target encod enough conclud accept limit use lightgbm cv worth tradeoff higher score got use target encod v target encod condit held constant singl biggest trade cours use serial cv probabl slightli wors job approxim true ideal number boost round train model know averag round best score oppos singl round averag score across 5 fold best howev know know mention comment begin kernel would ultim turn train predictor use lightgbm default categor featur handl number boost round determin result run lightgbm cv would result privat lb score 0 0001 lower privat lb score earn model use target encod number boost round determin result run serial cv conclus proper hyperparamet tune may much advantag use target encod lightgbm least home credit competit dataset
3032,"### List Features in Order of LightGBM Importance (importance_type='split')

I tried using this to get the top 20/50/100 features of lowest importance, and then remove them from my dataset and see if my local CV score increased. However, doing this never improved my CV score.",list featur order lightgbm import import type split tri use get top 20 50 100 featur lowest import remov dataset see local cv score increas howev never improv cv score
3033,## III. Generating Test Set Predictions,iii gener test set predict
3034,"<h1 align=""center""> Default Classification </h1>
<img src=""https://storage.googleapis.com/kaggle-organizations/1536/thumbnail.png%3Fr=93"" width=400 height=200>

<h2> Introduction: </h2>
In this project we will analyze what factors affect whether an individual will be able to repay a loan or not. We will explore several statistical techniques to find better ways to to analyze the data and for our model to learn from those statistical techniques in a more effective way. First, we will focus on the Exploratory Data Analysis (EDA) aspect since I want to have a better understanding of what the data is telling us, then we will determine if the dataset is highly imbalanced and we will proceed with several techniques as of how to deal with these types of datasets. One note before we start this project, I will be giving much slower updates than usual because currently I am taking a statistical course so in the future I could come with more approachable techniques so as to how solve and analyze complex datasets. Since this statistics course is taking a bit of a toll of my time, updates for this project will be more slow than usual. Let's start with our analysis! <br> <br>

This project will be focus into three phases:
<ul>
    <li><b>Extensive Exploratory Analysis:</b> There is a vast amount of data so in this phase it will take me some time to dig down into all the important features that I consider to be important. </li>
    <li> <b>Preprocessing the Data: </b> This is the most important aspect of how accurate our models will be, using the right techniques to scale and transform some of the missing values is essential.</li>
    <li><b>Implementation of the Model:</b> Decide which predictive model will work best in this scenario.  </li>
    </ul>
    
  <br>  
 <b> A note from the author: </b> I am a bit busy with my studies but I will have some free time to dedicate to this project however, I want to take my time to provide to the Kaggle community a good quality work so this might take some time. Have a great day everyone and enjoy the parts of this analysis that are already published!

<h2> Oulline: (To be Updated) </h2>
",h1 align center default classif h1 img src http storag googleapi com kaggl organ 1536 thumbnail png 3fr 93 width 400 height 200 h2 introduct h2 project analyz factor affect whether individu abl repay loan explor sever statist techniqu find better way analyz data model learn statist techniqu effect way first focu exploratori data analysi eda aspect sinc want better understand data tell u determin dataset highli imbalanc proceed sever techniqu deal type dataset one note start project give much slower updat usual current take statist cours futur could come approach techniqu solv analyz complex dataset sinc statist cours take bit toll time updat project slow usual let start analysi br br project focu three phase ul li b extens exploratori analysi b vast amount data phase take time dig import featur consid import li li b preprocess data b import aspect accur model use right techniqu scale transform miss valu essenti li li b implement model b decid predict model work best scenario li ul br b note author b bit busi studi free time dedic project howev want take time provid kaggl commun good qualiti work might take time great day everyon enjoy part analysi alreadi publish h2 oullin updat h2
3035,"<h3> Several Distributions: </h3>
In this section we will analyze several distribution to see if those distributions are <b> right-skewed </b>, <b> symetric (normal distribution) </b> or <b> left-skewed </b>. We will use the <b>norm</b> function from scipy to determine whether those distributions can fit our model perfectly. The more normal the distribution the better for our model to handle ouliers or values that are more than two standard deviations away from the mean. So how do we know if the distribution is fitting perfectly our model? If the dots don't fit perfectly the line or if somehow it deviates in such an extreme way we could assume that our model is not fitting perfectly. <b> Is there a way to make the distribution fit more normally? </b> One alternative is to use a natural logarithm in order to reduce the distance for extreme values which will improve how values fit to our model. See how the values deviate less in the second norm subplots.",h3 sever distribut h3 section analyz sever distribut see distribut b right skew b b symetr normal distribut b b left skew b use b norm b function scipi determin whether distribut fit model perfectli normal distribut better model handl oulier valu two standard deviat away mean know distribut fit perfectli model dot fit perfectli line somehow deviat extrem way could assum model fit perfectli b way make distribut fit normal b one altern use natur logarithm order reduc distanc extrem valu improv valu fit model see valu deviat le second norm subplot
3036,"<h3> Gender Analysis: </h3>

<h4> Summary: </h4>
<ul>
<li> <b>Housing Type: </b> Most of the housing type by both genders are house/apartment.</li>
<li><b> Income Type: </b> The top income types for both genders are currently working and commercial associate. </li>
<li><b>Correlation between income and credit: </b> There is a slight positive correlation between between income and credit (the higher the income, the higher the loan).</li>
<li><b> Income distributions: </b> Males tend to have a slightly higher income distribution compared to female. </li>
</ul>",h3 gender analysi h3 h4 summari h4 ul li b hous type b hous type gender hous apart li li b incom type b top incom type gender current work commerci associ li li b correl incom credit b slight posit correl incom credit higher incom higher loan li li b incom distribut b male tend slightli higher incom distribut compar femal li ul
3037,"### Continue Here forhe Gender Analysis:
<ul>
<li> Look for more insightfu variables that we can use mainly in the y and x variables to evaluate the discrepancies between Gender and determine whether there is more risk for females to default on loans. </li>
<li> Find the distribution of genders and see how many of each gender defaulted on a loan (Only available with the training dataset. </li>
</ul>",continu forh gender analysi ul li look insightfu variabl use mainli x variabl evalu discrep gender determin whether risk femal default loan li li find distribut gender see mani gender default loan avail train dataset li ul
3038,"### Material Analysis: (Next Phase of the Project)
---> Description Coming Soon",materi analysi next phase project descript come soon
3039,"['![app](https://i.imgur.com/jKtMJgg.jpg)\n', '\n', 'The missing data within a dataset can often provide insight into the issue at hand. We can look at the structure of the missing values - which features are affected, which records are affected, and differences between groups. We can also use the missing data as a feature itself by counting missing values or transforming them. In this report I explore some patterns and suggest one way to improve your predictive.\n', '\n', '## Patterns\n', ""First let's look at the overall pattern of missing data. The [missingno](https://github.com/ResidentMario/missingno) package by [Aleksey Bilogur](https://www.kaggle.com/residentmario) is the perfect tool here. Looking at a sample of data for all columns we see a group of columns where the missing values appear correlated.\n"", '\n']",app http imgur com jktmjgg jpg miss data within dataset often provid insight issu hand look structur miss valu featur affect record affect differ group also use miss data featur count miss valu transform report explor pattern suggest one way improv predict pattern first let look overal pattern miss data missingno http github com residentmario missingno packag aleksey bilogur http www kaggl com residentmario perfect tool look sampl data column see group column miss valu appear correl
3040,"['Zooming in on the middle columns we see they deal mostly with information about the building where the client lives. It appears there are many applicants who leave blank the information for their housing. We can think about why that might be the case or how it might inform our model.\n', '\n', ""I'll sort the data this time to better see the proportions.""]",zoom middl column see deal mostli inform build client live appear mani applic leav blank inform hous think might case might inform model sort data time better see proport
3041,['The dendrogram view shows how missing values are related across columns by using hierarchical clustering. Pretty cool! '],dendrogram view show miss valu relat across column use hierarch cluster pretti cool
3042,"['## Comparison of Completed Applications\n', '\n', ""With an idea of the overall picture, let's now focus on the large group of applications with missing house data. Is there a difference in mean default rates between those with house information and those without?""]",comparison complet applic idea overal pictur let focu larg group applic miss hous data differ mean default rate hous inform without
3043,"[""There appears to be a difference. Viewed one way, borrowers with incomplete applications are ~30% more likely to default. You may want to include this information in your model. I found it helpful to add a binary feature called 'no_housing_info'. The application is flagged if it has more than 45 blanks. You could also create three classes to account for the applications with some housing data (which may denote apartment dwellers). \n"", '\n', '\n', '## Statistical Significance\n', ""To be thorough, I looked at statistical significance of the difference in default rates between groups. I used a [G-test](https://en.wikipedia.org/wiki/G-test) which is similar to Pearson's chi-squared test. Either one should work in this case, but I generally prefer the G-test.""]",appear differ view one way borrow incomplet applic 30 like default may want includ inform model found help add binari featur call hous info applic flag 45 blank could also creat three class account applic hous data may denot apart dweller statist signific thorough look statist signific differ default rate group use g test http en wikipedia org wiki g test similar pearson chi squar test either one work case gener prefer g test
3044,"['""If p is low, the null must go."" The p-value here is 1e-114 which is pretty much 0. So we can reject the null hypothesis with only a small probability of [Type 1 error](https://en.wikipedia.org/wiki/Type_I_and_type_II_errors). In other words, the difference in default ratios between the two groups is not due to random chance. \n', '\n', 'Good luck!']",p low null must go p valu 1e 114 pretti much 0 reject null hypothesi small probabl type 1 error http en wikipedia org wiki type type ii error word differ default ratio two group due random chanc good luck
3045,"<h3>Feature importance evaluation</h3>

In this notebook we will be using the [Shap library](https://github.com/slundberg/shap) to evaluate the importance of features from [LightGBM with selected features](https://www.kaggle.com/ogrellier/lighgbm-with-selected-features/code) on the <b>full dataset</b>. LightGBM standard feature importance will also be used to comparison.

Shap (Shapley Additive explanations) combine [game theory](https://en.wikipedia.org/wiki/Game_theory) with local explanations to create a consistent indicator for feature importance in complex models. Shap value measures how much each feature contributes, either positively or negatively, to a default. For more information:

* [Medium article](https://medium.com/civis-analytics/demystifying-black-box-models-with-shap-value-analysis-3e20b536fc80) by Peter Cooman 
* [Original paper](http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf)
* [Detailed paper on Tree ensembles](https://arxiv.org/pdf/1802.03888.pdf)",h3 featur import evalu h3 notebook use shap librari http github com slundberg shap evalu import featur lightgbm select featur http www kaggl com ogrelli lighgbm select featur code b full dataset b lightgbm standard featur import also use comparison shap shapley addit explan combin game theori http en wikipedia org wiki game theori local explan creat consist indic featur import complex model shap valu measur much featur contribut either posit neg default inform medium articl http medium com civi analyt demystifi black box model shap valu analysi 3e20b536fc80 peter cooman origin paper http paper nip cc paper 7062 unifi approach interpret model predict pdf detail paper tree ensembl http arxiv org pdf 1802 03888 pdf
3046,"<b>LightGBM Model</b>

The original script used Sklearn to train the model in five folds and then averaged the predictions and feature importance. This notebook will use the lgb.cv method to perform cross-validation in nfolds and then train a model on full data using the optimal iteration (boost round) found. We will be using the same parameters and features.",b lightgbm model b origin script use sklearn train model five fold averag predict featur import notebook use lgb cv method perform cross valid nfold train model full data use optim iter boost round found use paramet featur
3047,"<b>Running the model</b>

Just removed the timers and added a function to reduce dataframe's memory usage.",b run model b remov timer ad function reduc datafram memori usag
3048,"<h3>Shap analysis</h3>

Now we have all we need to start evaluating each feature performance. The first graph shows the 20 most important features according to the sum of shap magnitudes over all samples. The color represents the feature value (red high, blue low) and the x-axis the impact on model output.",h3 shap analysi h3 need start evalu featur perform first graph show 20 import featur accord sum shap magnitud sampl color repres featur valu red high blue low x axi impact model output
3049,Let's plot the feature importance (by gain) to compare:,let plot featur import gain compar
3050,"<b>Individual data points</b>

It's also possible to visualize individual data points and how features contribute to the model output:",b individu data point b also possibl visual individu data point featur contribut model output
3051,"<b>Dependence plot</b>

We can use the dependence_plot to compare the feature value (x axis) and the shap values for that feature (y axis). The coloring represents the value for an automatically selected feature so we can understand the iteraction effect. Let's plot some of the most important features:",b depend plot b use depend plot compar featur valu x axi shap valu featur axi color repres valu automat select featur understand iteract effect let plot import featur
3052,"<b>Bureau and bureau_balance</b>

Let's plot the 20 most important features from bureau.csv and bureau_balance.csv:",b bureau bureau balanc b let plot 20 import featur bureau csv bureau balanc csv
3053,"<b>Previous Applications</b>

Most important features from previous_application.csv",b previou applic b import featur previou applic csv
3054,"<b>Previous applications: balances and payments</b>

Most important features from credit_card_balance, POS_CASH_balance and installments_payments.",b previou applic balanc payment b import featur credit card balanc po cash balanc instal payment
3055,"<h3>Shap values and feature importance for all features</h3>

To conclude, we will sum the shap magnitude for each feature and print it with the feature importance by gain and by number of splits.",h3 shap valu featur import featur h3 conclud sum shap magnitud featur print featur import gain number split
3056,"Hi there, this notebook aims to provide visualization about different features distribution and observing abnormal values in the dataset. 

# Table of Contents
1. Target Distribution
2. application_{train|test}.csv
    * 2.1 NaN Count
    * 2.2 [Feature Generation] Adding IS_NAN features for each column.
    * 2.3 The importance of the missing values
    * 2.4. Features' Distribution
    * 2.5 Are the distributions making sense?
3. bureau.csv
4. ...pending

# 1. Target Distribution


",hi notebook aim provid visual differ featur distribut observ abnorm valu dataset tabl content 1 target distribut 2 applic train test csv 2 1 nan count 2 2 featur gener ad nan featur column 2 3 import miss valu 2 4 featur distribut 2 5 distribut make sen 3 bureau csv 4 pend 1 target distribut
3057,"The target distribution is imbalanced, which indicates the company has already done a great job (some direct feature, like external scoring, in the dataset might not be that helpful)",target distribut imbalanc indic compani alreadi done great job direct featur like extern score dataset might help
3058,"# 2. application_{train|test}.csv> 
##  2.1 NaN Count
There are lots of NaN values in the dataset (also as discussed in the forum, the organizer also filled in some missing data with magic values). Need to handle them carefully.",2 applic train test csv 2 1 nan count lot nan valu dataset also discus forum organ also fill miss data magic valu need handl care
3059,## 2.2 [Feature Generation] Adding IS_NAN features for each column.,2 2 featur gener ad nan featur column
3060,## 2.3 The importance of the missing values,2 3 import miss valu
3061,## 2.4 Features' Distribution,2 4 featur distribut
3062,"## 2.5 Are the distributions making sense?
### 2.5.1 DAYS_EMPLOYED
How many days before the application the person started current employment

Discussed in the forum, For DAYS_xxx columns, **365243 means missing value**.
* The original distribution:",2 5 distribut make sen 2 5 1 day employ mani day applic person start current employ discus forum day xxx column 365243 mean miss valu origin distribut
3063,"If the magic number is removed, the distribution:",magic number remov distribut
3064,"### 2.5.2 AMT_INCOME_TOTAL
Income of the client.

There are a huge number at the right of the plot (1.170000e+08):",2 5 2 amt incom total incom client huge number right plot 1 170000e 08
3065,The plot makes more sense if we remove that data point:,plot make sen remov data point
3066,"### 2.5.3 AMT_REQ_CREDIT_BUREAU_QRT
Number of enquiries to Credit Bureau about the client 3 month before application (excluding one month before application)

Why were there 261 enquireies about a application within 2 months? 4 calls a day?",2 5 3 amt req credit bureau qrt number enquiri credit bureau client 3 month applic exclud one month applic 261 enquirei applic within 2 month 4 call day
3067,Removing that data point:,remov data point
3068,"### 2.5.4 Normalized information about building where the client lives

Why those Normalized information got many 0s and 1s? such as this one:",2 5 4 normal inform build client live normal inform got mani 0 1 one
3069,"### 2.5.5 OBS_30_CNT_SOCIAL_CIRCLE
How many observation of client's social surroundings with observable 30 DPD (days past due) default

Is it normal to have over 350 social surroundings overations?",2 5 5 ob 30 cnt social circl mani observ client social surround observ 30 dpd day past due default normal 350 social surround over
3070,"# 3 bureau.csv

Pending",3 bureau csv pend
3071,"Thanks for reading, this is my first attempt to try making a relatively complete EDA & visualization. Please let me know if you have any suggestions, I am desired to learn new things.",thank read first attempt tri make rel complet eda visual plea let know suggest desir learn new thing
3072,"# Preprocessing
## Solution 3",preprocess solut 3
3073,"[Martin Kotek (Competition Host): ""Value 365243 denotes infinity in DAYS variables in the datasets, therefore you can consider them NA values. Also XNA/XAP denote NA values.""](https://www.kaggle.com/c/home-credit-default-risk/discussion/57247)",martin kotek competit host valu 365243 denot infin day variabl dataset therefor consid na valu also xna xap denot na valu http www kaggl com c home credit default risk discus 57247
3074,"# Feature Engineering
## Solution 3
### Hand crafted features",featur engin solut 3 hand craft featur
3075,### Aggregation features,aggreg featur
3076,"## Solution 4 
### Hand crafted features
* diff features",solut 4 hand craft featur diff featur
3077,* unemployed,unemploy
3078,* age binns,age binn
3079,"# Feature Engineering
## Solution 3",featur engin solut 3
3080,## Aggregations,aggreg
3081,# Solution 4,solut 4
3082,## per id aggregations,per id aggreg
3083,## Per id k last installment information,per id k last instal inform
3084,## per id dynamic ,per id dynam
3085,"### helper functions
These functions are just helpers :)",helper function function helper
3086,### Aggregations,aggreg
3087,"## Solution 4
### Hand crafted features",solut 4 hand craft featur
3088,"## Solution 5

### Hand crafted features",solut 5 hand craft featur
3089,### Last loan features,last loan featur
3090,### Trend features,trend featur
3091,"# Feature Engineering
## Solution 3",featur engin solut 3
3092,### Aggregations,aggreg
3093,"## Solution 4
### Hand crafted features",solut 4 hand craft featur
3094,Bayesian Optimzation code copied here for display purposes,bayesian optimz code copi display purpos
3095,CSV files readers and enhancers,csv file reader enhanc
3096,Hyperopt helper object,hyperopt helper object
3097,"Read files, create full dataset and optimize",read file creat full dataset optim
3099,"this is folked kernel with japanese comment for education

# ホーム・クレジット社 債務不履行リスク - データ探索 + 基本モデル

融資履歴が少なかったり無かったりするために、多くの人々が融資を受けるのに苦労しています。  
そして、残念なことに、このような人々は、怪しげな金貸し屋によってしばしばカモにされます。  
ホームクレジット社は、ポジティブで安全な借入経験を提供することによって、  
銀行口座を持たない人々のためのファイナンシャル・インクルージョン (貧困層に正規の金融取引ができるように改善する解決策を提供すること) を広めるために努力しています。  
この金銭的に不利な人々がポジティブな借入経験を持つことを確実にするために、ホームクレジットは電話や取引情報を含むさまざまな代替データを利用しています。
そして顧客の返済能力を予測しています。

ホームクレジット社は現在、これらの予測を行うためにさまざまな統計的方法や機械学習方法を使用していますが、  
ホームクレジット社は社の持つデータの潜在能力を最大限に発揮するためにKagglersに挑戦を挑みました。  
このコンペにより、返済能力のある顧客が無事借入できること、そして顧客がより確実に返済完了できるような借入額、完済日、返済スケジュールを提供することが可能となるでしょう。

これはホーム・クレジット社債務不履行データについてのデータ探索と基本モデルについての簡単なノートブックです。  
**Contents**   
1. Dataset Preparation    
2. Exploration - Applications Train  
&nbsp;&nbsp;&nbsp;&nbsp; 2.1 Snapshot - Application Train    
&nbsp;&nbsp;&nbsp;&nbsp; 2.2 Distribution of Target Variable    
&nbsp;&nbsp;&nbsp;&nbsp; 2.3 Gender and Contract Type Distribution and Target Variable    
&nbsp;&nbsp;&nbsp;&nbsp; 2.4 Own Realty and Own Car  - Distribution with Target Variable  
&nbsp;&nbsp;&nbsp;&nbsp; 2.5 Suit Type and Income Type    
&nbsp;&nbsp;&nbsp;&nbsp; 2.6 Family Statue and Housing Type   
&nbsp;&nbsp;&nbsp;&nbsp; 2.7 Education Type and Income Type   
&nbsp;&nbsp;&nbsp;&nbsp; 2.8.1 Organization Type and Occupation Type   
&nbsp;&nbsp;&nbsp;&nbsp; 2.8.2 Walls Material, Foundation and House Type   
&nbsp;&nbsp;&nbsp;&nbsp; 2.9 Amount Credit Distribution    
&nbsp;&nbsp;&nbsp;&nbsp; 2.10 Amount Annuity Distribution  
&nbsp;&nbsp;&nbsp;&nbsp; 2.11 Amount Goods Price   
&nbsp;&nbsp;&nbsp;&nbsp; 2.12 Amount Region Population Relative    
&nbsp;&nbsp;&nbsp;&nbsp; 2.13 Days Birth   
&nbsp;&nbsp;&nbsp;&nbsp; 2.14 Days Employed    
&nbsp;&nbsp;&nbsp;&nbsp; 2.15 Num Days Registration  
&nbsp;&nbsp;&nbsp;&nbsp; 2.15 Count of Family Members  
3. Exploration - Bureau Data  
&nbsp;&nbsp;&nbsp;&nbsp; 3.1 Snapshot - Bureau Data    
4. Exploration - Bureau Balance Data  
&nbsp;&nbsp;&nbsp;&nbsp; 4.1 Snapshot - Bureau Balance Data     
5. Exploration - Credit Card Balance Data   
&nbsp;&nbsp;&nbsp;&nbsp; 5.1 Snapshot - Credit Card Balance Data   
6. Exploration - POS Cash Balance Data   
&nbsp;&nbsp;&nbsp;&nbsp; 6.1 Snapshot - POS Cash Balance Data   
7. Exploration - Previous Application Data   
&nbsp;&nbsp;&nbsp;&nbsp; 7.1 Snapshot - Previous Application Data  
&nbsp;&nbsp;&nbsp;&nbsp; 7.2 Contract Status Distribution - Previous Applications  
&nbsp;&nbsp;&nbsp;&nbsp; 7.3 Suite Type Distribution - Previous Application    
&nbsp;&nbsp;&nbsp;&nbsp; 7.4 Client Type Distribution  - Previous Application    
&nbsp;&nbsp;&nbsp;&nbsp; 7.5 Channel Type Distribution - Previous Applications  
7. Exploration - Installation Payments  
&nbsp;&nbsp;&nbsp;&nbsp; 8.1 Snapshot of Installation Payments  
9. Baseline Model  
&nbsp;&nbsp;&nbsp;&nbsp; 9.1 Dataset Preparation  
&nbsp;&nbsp;&nbsp;&nbsp; 9.2 Label Encoding     
&nbsp;&nbsp;&nbsp;&nbsp; 9.3 Validation Sets Preparation    
&nbsp;&nbsp;&nbsp;&nbsp; 9.4 Model Fitting    
&nbsp;&nbsp;&nbsp;&nbsp; 9.5 Feature Importance    
&nbsp;&nbsp;&nbsp;&nbsp; 9.6 Prediction 



## 1. Dataset Preparation ",folk kernel japanes comment educ kaggler content 1 dataset prepar 2 explor applic train nbsp nbsp nbsp nbsp 2 1 snapshot applic train nbsp nbsp nbsp nbsp 2 2 distribut target variabl nbsp nbsp nbsp nbsp 2 3 gender contract type distribut target variabl nbsp nbsp nbsp nbsp 2 4 realti car distribut target variabl nbsp nbsp nbsp nbsp 2 5 suit type incom type nbsp nbsp nbsp nbsp 2 6 famili statu hous type nbsp nbsp nbsp nbsp 2 7 educ type incom type nbsp nbsp nbsp nbsp 2 8 1 organ type occup type nbsp nbsp nbsp nbsp 2 8 2 wall materi foundat hous type nbsp nbsp nbsp nbsp 2 9 amount credit distribut nbsp nbsp nbsp nbsp 2 10 amount annuiti distribut nbsp nbsp nbsp nbsp 2 11 amount good price nbsp nbsp nbsp nbsp 2 12 amount region popul rel nbsp nbsp nbsp nbsp 2 13 day birth nbsp nbsp nbsp nbsp 2 14 day employ nbsp nbsp nbsp nbsp 2 15 num day registr nbsp nbsp nbsp nbsp 2 15 count famili member 3 explor bureau data nbsp nbsp nbsp nbsp 3 1 snapshot bureau data 4 explor bureau balanc data nbsp nbsp nbsp nbsp 4 1 snapshot bureau balanc data 5 explor credit card balanc data nbsp nbsp nbsp nbsp 5 1 snapshot credit card balanc data 6 explor po cash balanc data nbsp nbsp nbsp nbsp 6 1 snapshot po cash balanc data 7 explor previou applic data nbsp nbsp nbsp nbsp 7 1 snapshot previou applic data nbsp nbsp nbsp nbsp 7 2 contract statu distribut previou applic nbsp nbsp nbsp nbsp 7 3 suit type distribut previou applic nbsp nbsp nbsp nbsp 7 4 client type distribut previou applic nbsp nbsp nbsp nbsp 7 5 channel type distribut previou applic 7 explor instal payment nbsp nbsp nbsp nbsp 8 1 snapshot instal payment 9 baselin model nbsp nbsp nbsp nbsp 9 1 dataset prepar nbsp nbsp nbsp nbsp 9 2 label encod nbsp nbsp nbsp nbsp 9 3 valid set prepar nbsp nbsp nbsp nbsp 9 4 model fit nbsp nbsp nbsp nbsp 9 5 featur import nbsp nbsp nbsp nbsp 9 6 predict 1 dataset prepar
3100,"## 2. データ探索: Application (ローン申込書)

## 2.1 Application Train の概観

Application データは全ローン申込書の統計情報からなり、各行が1つのローンを表す。",2 applic 2 1 applic train applic 1
3101,"> 307,511件のローンのデータがあり、列数は122です。

## 2.2 目的変数の分布
目的変数
- 1: 支払が困難なクライアント = クライアントが最初のY回の分割払いの内に少なくとも一回でX日以上延滞していた場合
- 0: それ以外の場合",307 511122 2 2 1 yx 0
3102,"> - 目的変数は約282k (85%) が 0 で、わずか24kが 1 です。

## 2.3 どの性別、どの契約タイプがローンを申し込んでいるか
- 性別: クライアントの性別  
- 契約タイプ: ローンがキャッシュかリボ払いか  

### 2.3.1 性別・契約タイプの分布",282k 85 0 24k 1 2 3 2 3 1
3103,"> 性別については、女性が多く (202448) 男性は少ない (105059)。  
> 契約タイプについてはキャッシュが主でリボ払いは約29kとかなり少ない。

### 2.3.2 性別・契約タイプと目的変数との関係",202448 105059 29k 2 3 2
3104,## 2.4. 土地所有・車所有,2 4
3105,"## 2.5 同伴者・収入形態
- 同伴者 (NAME_TYPE_SUITE): 借入申請書提出時に同伴した人物

### 2.5.1 同伴者・収入形態の値",2 5 name type suit 2 5 1
3106,"> 同伴者のトップ3は同伴者なし (250k)、家族、夫婦である。
> 収入形態は8タイプがありトップは:  
    - Working Class労働階級 (158K)
    - Pensiner 年金受給者 (55K)同伴者

### 2.5.2 同伴者・収入形態と目的変数との関係",3 250k 8 work class 158k pensin 55k 2 5 2
3107,"## 2.6. 婚姻状況・住居

### 2.6.1 婚姻状況・住居の値",2 6 2 6 1
3108,"> - 既婚の顧客が最も多く (約196k) 独身がそれに続く。
> - 住居は多くが ""一軒家/アパート"" で85%を占め、両親と同居、公営住宅が続く。

### 2.6.2 婚姻状況・住居と目的変数との関係",196k 85 2 6 2
3109,"## 2.7. 教育

### 2.7.1 教育の分布",2 7 2 7 1
3110,"> 多くの割合が中等教育に占められ (218k)、高等教育 (75k) がそれに続く。

### 2.7.2 教育と目的変数との関係",218k 75k 2 7 2
3111,"## 2.8. 組織・業種
-  組織: クライアントが働いている組織
-  業種: クライアントの業種

### 2.8.1 組織・業種の分布",2 8 2 8 1
3112,"> 申請者の中で多い業種は労働者 (55k)、販売員 (32k)、コアスタッフ (28k)  
> 多い組織は第3種法人が最多で67kを占める

### 2.8.2 組織・業種と目的変数との関係",55k 32k 28k 367k 2 8 2
3113,"### 2.8.3. 壁の種類・基礎の種類・家屋の種類の分布
変数の意味がよくわかりません…",2 8 3
3114,"> - 平屋が150kでほとんどを占め、特殊家屋、テラスハウスは1500以下である。
> - 壁はパネル、石・レンガがほぼ同数で120k近くを占める。

### 2.8.4 壁の種類・基礎の種類・家屋の種類と目的変数との関係",150k1500 120k 2 8 4
3115,## 2.9. 借金額の分布,2 9
3116,"## 2.10 年金の分布
- 年金: ローン年金 (って何?)",2 10
3117,"## 2.11 商品価格の分布
- 商品価格: ローンを組む目的である商品の価格",2 11
3118,"## 2.12 相対地域人口分布の分布
- 相対地域人口: 正規化されたクライアントが住んでいる地域の人口 (クライアントがより人工の多い地域に住んでいることを表す) ",2 12
3119,"## 2.13 年齢の分布
- 年齢: クライアントが借入申請日の何日前に生まれたか",2 13
3120,"## 2.14 雇用日数の分布
- 雇用日数: クライアントが借入申請日の何日前から現在の仕事を始めたか",2 14
3121,"## 2.15 登録日の分布
- 登録日: クライアントが借入申請日の何日前に登録情報を更新したか",2 15
3122,## 2.16 家族人数,2 16
3123,"## 3. 信用情報機関データのデータ探索

信用情報機関によって報告されている顧客の過去の他の金融機関で借入履歴。  
顧客の借入申込日以前の借入回数と同じ行数の借入情報が含まれる。  

## 3.1 信用情報機関データの概観",3 3 1
3124,"## 4. 信用情報機関残高のデータ探索

信用情報機関の過去の借入の月間残高。   
このテーブルには、過去の借入についての各月の残高が1行ずつ記録されています。  
テーブルの各列には、ある借入のx月 (借入申請からxヶ月前) の債務状況の情報が含まれます。  

## 4.1 信用情報機関残高データの概観",4 1 x x 4 1
3125,"## 5. クレジットカード残高のデータ探索

借入申請者の持つホーム・クレジット社製クレジットカードの各月の残高情報。  
このテーブルには、借入申請者の持つホーム・クレジット社製クレジットカード (消費者金融・キャッシュローン) の各月の残高が1行ずつ記録されています。  
テーブルの各列には、あるクレジットカードのx月 (借入申請からxヶ月前) の債務状況の情報が含まれます。 

## 5.1 クレジットカード残高データの概観",5 1 x x 5 1
3126,"## 6. POSキャッシュ残高のデータ探索

借入申請者の持つホーム・クレジットに関する過去のPOSとキャッシュローンの各月の残高情報。  
テーブルの各列には、あるローンのx月 (借入申請からxヶ月前) の債務状況の情報が含まれます。 
このテーブルには、ローン の各月の残高が1行ずつ記録されています。  
(訳注: よくわかりませんでした)

## 6.1 POSキャッシュ残高データの概観",6 po po x x 1 6 1 po
3127,"## 7. 過去の借入申請書のデータ探索

## 7.1  過去の借入申請書データの概観",7 7 1
3128,"## 7.2 過去の借入申請書の契約状況の分布
- 契約状況:  受理、拒否...",7 2
3129,">-  多くの人が過去に申請が受理されている (62%)。一方で19%がキャンセル、17%が拒否となっている。

## 7.3 過去の借入申請書の同伴者の分布",62 19 17 7 3
3130,">- 過去の申請書の同伴者の多くが同伴者なしであり (60%)、家族がそれに続く (25%)。

## 7.4 過去の借入申請書の顧客タイプ",60 25 7 4
3131,">- 過去の借入申請者の74%がリピーターで18%が新規、8%が再登録? (refreshed)である。

## 7.5 チャネルタイプ
- チャネルタイプ: どの方法で借入申請書を受け取ったか",74 18 8 refresh 7 5
3132,"## 8. 分割支払のデータ探索
## 8.1 分割支払データの概観",8 8 1
3133,"## 9. ベースライン・モデル

### 9.1 前処理",9 9 1
3134,"### 9.2 カテゴリ変数の処理

より良い処理をしたければOliverの素晴らしいkernelを見に行ってください: https://www.kaggle.com/ogrellier/good-fun-with-ligthgbm ",9 2 oliverkernel http www kaggl com ogrelli good fun ligthgbm
3135,### 9.3 データセットを一箇所にまとめる,9 3
3136,### 9.4 validationデータの作成,9 4 valid
3137,### 9.5 モデル (Light GBM)の学習,9 5 light gbm
3138,### 9.6 特徴量の重要度,9 6
3139,### 9.7 推定,9 7
3140,"[""There are many different method's to select the important features from a dataset. In this notebook I will show a quick way to select important features with the use of Boruta.\n"", '\n', 'Boruta tries to find all relevant features that carry information to make an accurate classification. You can read more about Boruta [here](http://danielhomola.com/2015/05/08/borutapy-an-all-relevant-feature-selection-method/)\n', '\n', ""Let's start by doing all necessary imports.""]",mani differ method select import featur dataset notebook show quick way select import featur use boruta boruta tri find relev featur carri inform make accur classif read boruta http danielhomola com 2015 05 08 borutapi relev featur select method let start necessari import
3141,"[""Next we load only the 'application_train' data as this is to demonstrate Boruta only. ""]",next load applic train data demonstr boruta
3142,['All categorical values will be one-hot encoded.'],categor valu one hot encod
3143,['Get all feature names from the dataset'],get featur name dataset
3144,['Replace all missing values with the Mean.'],replac miss valu mean
3145,['Get the final dataset *X* and labels *Y*'],get final dataset x label
3146,['Next we setup the *RandomForrestClassifier* as the estimator to use for Boruta. The *max_depth* of the tree is advised on the Boruta Github page to be between 3 to 7.'],next setup randomforrestclassifi estim use boruta max depth tree advis boruta github page 3 7
3147,"[""Next we setup Boruta. It uses the *scikit-learn* interface as much as possible so we can use *fit(X, y), transform(X), fit_transform(X, y)*. I'll let it run for a maximum of *max_iter = 50* iterations. With *perc = 90* a threshold is specified. The lower the threshold the more features will be selected. I usually use a percentage between 80 and 90. ""]",next setup boruta use scikit learn interfac much possibl use fit x transform x fit transform x let run maximum max iter 50 iter perc 90 threshold specifi lower threshold featur select usual use percentag 80 90
3148,['After Boruta has run we can transform our dataset.'],boruta run transform dataset
3149,['And we create a list of the feature names if we would like to use them at a later stage.'],creat list featur name would like use later stage
3150,"['So I hope you enjoyed my very first Kaggle Kernel :-)\n', 'Let me know if you have any feedback or suggestions.']",hope enjoy first kaggl kernel let know feedback suggest
3153,[' # Exploratory analysis on credit risk analysis datasets'],exploratori analysi credit risk analysi dataset
3154,"['In the below notebook you will learn how to do exploratory analysis for any credit risk analysis problem. The dataset that we have taken is from the famous competition problem of Home Credit.\n', '\n', '**About Home Credit**\n', 'Home Credit strives to broaden financial inclusion for the unbanked population by providing a positive and safe borrowing experience. They wish to find out which customer is likely to default so that they can accordingly decide the customers they wish to lend to']",notebook learn exploratori analysi credit risk analysi problem dataset taken famou competit problem home credit home credit home credit strive broaden financi inclus unbank popul provid posit safe borrow experi wish find custom like default accordingli decid custom wish lend
3155,"[""- <a href='#1'>1. Importing necessary libraries and packages and reading files</a>  \n"", ""- <a href='#2'>2. Handling non-numerical variables</a>\n"", ""- <a href='#3'>3. Aligning Training and Testing Data</a>\n"", ""- <a href='#4'> 4. Handling missing values (using Iterative Imputer)</a>\n"", ""- <a href='#5'>5. Outlier Detection (using Isolation Forest)</a>\n"", ""  - <a href='#5-1'>5.1 Anomaly detection</a>\n"", ""- <a href='#6'>6. Missing data in application_train</a>\n"", ""- <a href='#7'>7. Duplicate data in application_train</a>\n"", ""- <a href='#8'>8. Checking for data imbalance</a>\n"", ""- <a href='#9'>9. Exploratory Data Analysis for application_train by visualisation</a>\n"", ""   - <a href='#9.1'>9.1. Distribution of income</a>\n"", ""   - <a href='#9.2'>9.2. Distribution of credit</a>\n"", ""   - <a href='#9.3'>9.3. Distribution of loan types</a>\n"", ""   - <a href='#9.4'>9.4. Distribution of NAME_INCOME_TYPE</a>\n"", ""   - <a href='#9.5'>9.5. Distribution of NAME_TYPE_SUITE</a>\n"", ""   - <a href='#9.6'>9.6. Distribution of NAME_EDUCATION_TYPE</a>\n"", ""   - <a href='#9.7'>9.7. Effect of marital status on ability to pay back loans</a>\n"", ""   - <a href='#9.8'>9.8. Distribution of NAME_HOUSING_TYPE</a>\n"", ""   - <a href='#9.9'>9.9. Distribution of Age</a>\n"", ""   - <a href='#9.10'>9.10. Effect of OCCUPATION_TYPE on default \n"", ""- <a href='#10'>10. Preparation of Data</a>\n"", ""   - <a href='#10.1'>10.1. Feature Engineering of Application data </a>\n"", ""   - <a href='#10.2'>10.2 Using Bureau Data</a>\n"", ""   - <a href='#10.3'>10.3. Using Previous Application Data</a>\n"", ""   - <a href='#10.4'>10.4. Using POS_CASH_balance data</a>\n"", ""   - <a href='#10.5'>10.5 Using installments_payments data</a>\n"", ""   - <a href='#10.6'>10.6. Using Credit card balance data </a>\n"", ""- <a href='#11'>11. Dividing data into train, valid and test   </a>\n"", ""- <a href='#12'>12. Feature Selection using Information Value and Weight of Evidence </a>\n"", ""- <a href='#13'> 13. Data Imputation before applying machine learning algorithms</a>\n"", ""- <a href='#14'>14. Applying Machine Learning Algorithms </a>\n"", ""  - <a href='#14.1'>14.1. Applying Logistic Regression</a>\n"", ""  - <a href='#14.2'>14.2. Applying XGBoost </a>\n"", ""  - <a href='#14.3'>14.3. Applying CATBOOST</a>\n"", ""  - <a href='#14.4'>14.4. Applying LightGBM</a>\n"", ""  - <a href='#14.5'>14.5. Applying RandomForest</a>\n"", ""- <a href='#15'> 15. Evaluating machine learning algorithms accuracy on training and testing sets </a>\n"", ""- <a href='#16'> 16. Optimising selected machine learning model further by choosing best hyperparameters </a>\n"", ""- <a href='#17'> 17. Final Predictions </a>""]",href 1 1 import necessari librari packag read file href 2 2 handl non numer variabl href 3 3 align train test data href 4 4 handl miss valu use iter imput href 5 5 outlier detect use isol forest href 5 1 5 1 anomali detect href 6 6 miss data applic train href 7 7 duplic data applic train href 8 8 check data imbal href 9 9 exploratori data analysi applic train visualis href 9 1 9 1 distribut incom href 9 2 9 2 distribut credit href 9 3 9 3 distribut loan type href 9 4 9 4 distribut name incom type href 9 5 9 5 distribut name type suit href 9 6 9 6 distribut name educ type href 9 7 9 7 effect marit statu abil pay back loan href 9 8 9 8 distribut name hous type href 9 9 9 9 distribut age href 9 10 9 10 effect occup type default href 10 10 prepar data href 10 1 10 1 featur engin applic data href 10 2 10 2 use bureau data href 10 3 10 3 use previou applic data href 10 4 10 4 use po cash balanc data href 10 5 10 5 use instal payment data href 10 6 10 6 use credit card balanc data href 11 11 divid data train valid test href 12 12 featur select use inform valu weight evid href 13 13 data imput appli machin learn algorithm href 14 14 appli machin learn algorithm href 14 1 14 1 appli logist regress href 14 2 14 2 appli xgboost href 14 3 14 3 appli catboost href 14 4 14 4 appli lightgbm href 14 5 14 5 appli randomforest href 15 15 evalu machin learn algorithm accuraci train test set href 16 16 optimis select machin learn model choos best hyperparamet href 17 17 final predict
3156,"['## <a id=""1""> 1. Importing necessary libraries and packages and reading files</a>']",id 1 1 import necessari librari packag read file
3157,"['## <a id=""2""> 2. Handling non-numerical variables</a>\n', 'Machines can understand only numbers. Hence let us convert all non-numeric columns into numbers. Categorical variables will be converted into dummy columns , ordinal variables are converted into numbers by mapping and variables which are non-numeric and cannot be converted into numbers will be dropped from the model.']",id 2 2 handl non numer variabl machin understand number henc let u convert non numer column number categor variabl convert dummi column ordin variabl convert number map variabl non numer convert number drop model
3158,"['### <a id=""3""> 3. Aligning Training and Testing Data</a>\n', '\n', 'There need to be the same features (columns) in both the training and testing data.\n', 'One-hot encoding has created more columns in the training data because there were some categorical variables\n', 'with categories not represented in the testing data. To remove the columns in the training data that are not in the testing\n', 'data, we need to `align` the dataframes. First we extract the target column from the training data (because this is not in\n', 'the testing data but we need to keep this information). When we do the align, we must make sure to set `axis = 1` \n', 'to align the dataframes based on the columns and not on the rows!\n']",id 3 3 align train test data need featur column train test data one hot encod creat column train data categor variabl categori repres test data remov column train data test data need align datafram first extract target column train data test data need keep inform align must make sure set axi 1 align datafram base column row
3159,['The training and testing datasets now have the same features which is required for machine learning. The number of features has grown significantly due to one-hot encoding. At some point we probably will want to try [dimensionality reduction (removing features that are not relevant)](https://en.wikipedia.org/wiki/Dimensionality_reduction) to reduce the size of the datasets.'],train test dataset featur requir machin learn number featur grown significantli due one hot encod point probabl want tri dimension reduct remov featur relev http en wikipedia org wiki dimension reduct reduc size dataset
3160,"['### <a id=""4""> 4. Handling missing values (using Iterative Imputer) prior to outlier detection </a>\n', '\n', 'We need to handle our missing values before we can do any kind of outlier detection.\n', ""There are many ways to handle missing values. We can use fillna() and replace missing values with data's mean, median or most frequent value. The approach that we shall use below will be Iterative Imputer. Iterative imputer will consider the missing variable to be the dependent variable and all the other features will be independent variables. So there will be a regression and the independent variables will be used gto determine the dependent variable (which is the missing feature).""]",id 4 4 handl miss valu use iter imput prior outlier detect need handl miss valu kind outlier detect mani way handl miss valu use fillna replac miss valu data mean median frequent valu approach shall use iter imput iter imput consid miss variabl depend variabl featur independ variabl regress independ variabl use gto determin depend variabl miss featur
3161,"['###  <a id=""5"">5. OUTLIERS DETECTION</a>\n', '\n', 'In statistics, an outlier is an observation point that is distant from other observations. There are many ways for outlier detection. \n', '\n', '**Visual methods to spot and remove outliers**\n', '1. Box-plot\n', '2. Scatter plots\n', '\n', '**Outliers detection and removal using mathematical function**\n', '1. Z-score: Threshold of -3 to 3 is taken, and any point with z score not in this range is removed as an outlier.\n', '2. IQR Score : This works similar to a box plot and z - score in the sense that a threshold IQR value is defined. IQR is the first quartile subtracted from the third quartile. Any point below the threshold IQR is removed. \n', '\n', '\n', '**Clustering methods for outlier detection**\n', '1. DBScan clustering (making clusters around data points). Minimum number of points are required to be in a cluster. There will be points that do not belong to any cluster or else points which are single in an entire cluster. So we can remove such noise points.  \n', '2. Isolation Forest: Isolation Forest will output the predictions for each data point in an array. If the result is -1, it means that this specific data point is an outlier. If the result is 1, then it means that the data point is not an outlier\n', '\n', 'Here we shall use Isolation Forest method because it can handle missing values well and does not require scaling of inputs. ']",id 5 5 outlier detect statist outlier observ point distant observ mani way outlier detect visual method spot remov outlier 1 box plot 2 scatter plot outlier detect remov use mathemat function 1 z score threshold 3 3 taken point z score rang remov outlier 2 iqr score work similar box plot z score sen threshold iqr valu defin iqr first quartil subtract third quartil point threshold iqr remov cluster method outlier detect 1 dbscan cluster make cluster around data point minimum number point requir cluster point belong cluster el point singl entir cluster remov nois point 2 isol forest isol forest output predict data point array result 1 mean specif data point outlier result 1 mean data point outlier shall use isol forest method handl miss valu well requir scale input
3162,"[""### <a id='5.1'> 5.1. Anomaly detection </a>\n"", '\n', 'Though we have removed outliers using Isolation Forest we will still see the data once to check for any anomalies. Isolation Forest or any outlier detection method assumes that outlier is a point which is in minority and does not resemble the other majority points. However sometimes some abberation points are too many in numbers too. Let us see if there is any such anamoly that we find']",id 5 1 5 1 anomali detect though remov outlier use isol forest still see data check anomali isol forest outlier detect method assum outlier point minor resembl major point howev sometim abber point mani number let u see anamoli find
3163,"['**Univariate outliers detection**\n', '\n', '**Negative numbers:**\n', 'DAYS_BIRTH, DAYS_EMPLOYED, DAYS_REGISTRATION, DAYS_ID_PUBLISH, DAYS_LAST_PHONE_CHANGE\n', '\n', 'Numbers are negative since they are taken relative to the date of application. So we need to change them to positive.\n', '\n', '**Maximum value discrepancy**\n', 'DAYS_EMPLOYED: 365243 days(over 1000 years)\n', 'OWN_CAR_AGE: 91 Years\n']",univari outlier detect neg number day birth day employ day registr day id publish day last phone chang number neg sinc taken rel date applic need chang posit maximum valu discrep day employ 365243 day 1000 year car age 91 year
3164,['As we can see there is anomaly in Days_employed as it is highly unlikely that a person will be employed for 1000 years.'],see anomali day employ highli unlik person employ 1000 year
3165,['The data now looks nice and clean.'],data look nice clean
3166,"['## <a id=""6"">6. Missing data in application_train</a>']",id 6 6 miss data applic train
3167,"['## <a id=""7"">7. Duplicate data in application_train </a>']",id 7 7 duplic data applic train
3168,"['## <a id=""8"">8. Checking for data imbalance</a>']",id 8 8 check data imbal
3169,['We see that the class is clearly imbalanced with cases of default as very low compared to overall cases. So we need to balance the data when we use Machine learning models.'],see class clearli imbalanc case default low compar overal case need balanc data use machin learn model
3170,"['## <a id=""9"">  9. Exploratory Data Analysis for application_train by visualisation </a>']",id 9 9 exploratori data analysi applic train visualis
3171,"['### <a id=""9.1""> 9.1. Distribution of income</a>\n']",id 9 1 9 1 distribut incom
3172,"['The distribution is right skewed and there are extreme values, we can apply log distribution.']",distribut right skew extrem valu appli log distribut
3173,['People with high income tend to not default'],peopl high incom tend default
3174,['We see that income variable gets normal distribution when it is log transformed. '],see incom variabl get normal distribut log transform
3175,"['### <a id=""9.2"">9.2. Distribution of credit</a>\n']",id 9 2 9 2 distribut credit
3176,['People who take more credit default less.'],peopl take credit default le
3177,"['### <a id=""9.3""> 9.3. Distribution of loan types</a>']",id 9 3 9 3 distribut loan type
3178,['More people are interested to take cash loans than revolving loans.'],peopl interest take cash loan revolv loan
3179,"['### <a id=""9.4""> 9.4. Distribution of NAME_INCOME_TYPE</a>\n']",id 9 4 9 4 distribut name incom type
3180,"['### <a id=""9.5""> 9.5. Distribution of NAME_TYPE_SUITE</a>']",id 9 5 9 5 distribut name type suit
3181,['Who accompanied the person while taking the loan? '],accompani person take loan
3182,['Most people are unaccompanied.'],peopl unaccompani
3183,"['### <a id=""9.6"">9.6. Distribution of NAME_EDUCATION_TYPE</a>\n']",id 9 6 9 6 distribut name educ type
3184,['People with a degree are able to pay back mostly.'],peopl degre abl pay back mostli
3185,"['### <a id=""9.7""> 9.7. Effect of marital status on ability to pay back loans</a>']",id 9 7 9 7 effect marit statu abil pay back loan
3186,"['### <a id=""9.8""> 9.8. Distribution of NAME_HOUSING_TYPE</a>\n']",id 9 8 9 8 distribut name hous type
3187,"['People in office apartment, co-op apartment almost never default.']",peopl offic apart co op apart almost never default
3188,"['### <a id=""9.9""> 9.9. Distribution of AGE</a>\n']",id 9 9 9 9 distribut age
3189,"['### <a id=""9.10"">9.10. Effect of OCCUPATION_TYPE on default </a>']",id 9 10 9 10 effect occup type default
3190,['Highly skilled people more likely to pay back and low skilled not so likely to pay back loans'],highli skill peopl like pay back low skill like pay back loan
3191,"['## <a id=""10""> 10. Combining other tables to extract their data </a> \n', '\n', 'There are many tables apart from application_train. Due to memory and space restrictions on Kaggle, I am unable to describe them here. But one can easily look up the description on the competition data page. We need to extract information from these ']",id 10 10 combin tabl extract data mani tabl apart applic train due memori space restrict kaggl unabl describ one easili look descript competit data page need extract inform
3192,"[""<a id='3.4.1'></a>\n"", '<h3> 10.1 Feature Engineering of Application data </h3>']",id 3 4 1 h3 10 1 featur engin applic data h3
3193,"[""<a id='10.2'></a>\n"", '<h3> 10.2 Using Bureau Data </h3>']",id 10 2 h3 10 2 use bureau data h3
3194,"[""<a id='10.2.1'></a>\n"", '<h3> 10.2.1. Feature Engineering of Bureau Data </h3>']",id 10 2 1 h3 10 2 1 featur engin bureau data h3
3195,"[""<a id='10.3'></a>\n"", '<h3> 10.3 Using Previous Application Data </h3>']",id 10 3 h3 10 3 use previou applic data h3
3196,"[""<a id='10.4'></a>\n"", '<h3> 10.4. Using POS_CASH_balance data </h3>']",id 10 4 h3 10 4 use po cash balanc data h3
3197,"[""<a id='10.5'></a>\n"", '<h3> 10.5. Using installments_payments data</h3>']",id 10 5 h3 10 5 use instal payment data h3
3198,"[""<a id='10.6'></a>\n"", '<h3> 10.6. Using Credit card balance data </h3>']",id 10 6 h3 10 6 use credit card balanc data h3
3199,"[""<h3> <a id='11'> 11. Dividing data into train, valid and test </a> </h3> ""]",h3 id 11 11 divid data train valid test h3
3200,"[""`X=application_bureau_prev.drop(columns=['TARGET'])\n"", ""y=application_bureau_prev['TARGET']`\n"", '\n', '`from sklearn.model_selection import train_test_split\n', 'X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)`']",x applic bureau prev drop column target applic bureau prev target sklearn model select import train test split x train x test train test train test split x test size 0 2 random state 1
3201,"[""<h3> <a id='12'> 12. Feature Selection using Information Value and Weight of Evidence </a> </h3>""]",h3 id 12 12 featur select use inform valu weight evid h3
3202,"['There are many methods for feature selection. Some of them include feature importance using XGBoost and RandomForest. Other methods are forward or backward elimination and Boruta. Here we use one of the most common methods for feature selection, Information value and weight of evidence to determine the feature selection for credit risk analysis.\n', '\n', '\n', '**Code to calculate IV and WOE**\n', '\n', 'max_bin = 20\n', '\n', 'force_bin = 3\n', '\n', '#Define a binning function\n', '\n', 'def mono_bin(Y, X, n = max_bin):\n', '\n', '    df1 = pd.DataFrame({""X"": X, ""Y"": Y})\n', ""    justmiss = df1[['X','Y']][df1.X.isnull()]\n"", ""    notmiss = df1[['X','Y']][df1.X.notnull()]\n"", '    r = 0\n', '    while np.abs(r) < 1:\n', '        try:\n', '            d1 = pd.DataFrame({""X"": notmiss.X, ""Y"": notmiss.Y, ""Bucket"": pd.qcut(notmiss.X, n)})\n', ""            d2 = d1.groupby('Bucket', as_index=True)\n"", '            r, p = stats.spearmanr(d2.mean().X, d2.mean().Y)\n', '            n = n - 1 \n', '        except Exception as e:\n', '            n = n - 1\n', '\n', '    if len(d2) == 1:\n', '        n = force_bin         \n', '        bins = algos.quantile(notmiss.X, np.linspace(0, 1, n))\n', '        if len(np.unique(bins)) == 2:\n', '            bins = np.insert(bins, 0, 1)\n', '            bins[1] = bins[1]-(bins[1]/2)\n', '        d1 = pd.DataFrame({""X"": notmiss.X, ""Y"": notmiss.Y, ""Bucket"": pd.cut(notmiss.X, np.unique(bins),include_lowest=True)}) \n', ""        d2 = d1.groupby('Bucket', as_index=True)\n"", '    \n', '    d3 = pd.DataFrame({},index=[])\n', '    d3[""MIN_VALUE""] = d2.min().X\n', '    d3[""MAX_VALUE""] = d2.max().X\n', '    d3[""COUNT""] = d2.count().Y\n', '    d3[""EVENT""] = d2.sum().Y\n', '    d3[""NONEVENT""] = d2.count().Y - d2.sum().Y\n', '    d3=d3.reset_index(drop=True)\n', '    \n', '    if len(justmiss.index) > 0:\n', ""        d4 = pd.DataFrame({'MIN_VALUE':np.nan},index=[0])\n"", '        d4[""MAX_VALUE""] = np.nan\n', '        d4[""COUNT""] = justmiss.count().Y\n', '        d4[""EVENT""] = justmiss.sum().Y\n', '        d4[""NONEVENT""] = justmiss.count().Y - justmiss.sum().Y\n', '        d3 = d3.append(d4,ignore_index=True)\n', '    \n', '    d3[""EVENT_RATE""] = d3.EVENT/d3.COUNT\n', '    d3[""NON_EVENT_RATE""] = d3.NONEVENT/d3.COUNT\n', '    d3[""DIST_EVENT""] = d3.EVENT/d3.sum().EVENT\n', '    d3[""DIST_NON_EVENT""] = d3.NONEVENT/d3.sum().NONEVENT\n', '    d3[""WOE""] = np.log(d3.DIST_EVENT/d3.DIST_NON_EVENT)\n', '    d3[""IV""] = (d3.DIST_EVENT-d3.DIST_NON_EVENT)*np.log(d3.DIST_EVENT/d3.DIST_NON_EVENT)\n', '    d3[""VAR_NAME""] = ""VAR""\n', ""    d3 = d3[['VAR_NAME','MIN_VALUE', 'MAX_VALUE', 'COUNT', 'EVENT', 'EVENT_RATE', 'NONEVENT', 'NON_EVENT_RATE', 'DIST_EVENT','DIST_NON_EVENT','WOE', 'IV']]       \n"", '    d3 = d3.replace([np.inf, -np.inf], 0)\n', '    d3.IV = d3.IV.sum()\n', '    \n', '    return(d3)\n', '\n', '\n', 'def char_bin(Y, X):\n', '        \n', '    df1 = pd.DataFrame({""X"": X, ""Y"": Y})\n', ""    justmiss = df1[['X','Y']][df1.X.isnull()]\n"", ""    notmiss = df1[['X','Y']][df1.X.notnull()]    \n"", ""    df2 = notmiss.groupby('X',as_index=True)\n"", '    \n', '    d3 = pd.DataFrame({},index=[])\n', '    d3[""COUNT""] = df2.count().Y\n', '    d3[""MIN_VALUE""] = df2.sum().Y.index\n', '    d3[""MAX_VALUE""] = d3[""MIN_VALUE""]\n', '    d3[""EVENT""] = df2.sum().Y\n', '    d3[""NONEVENT""] = df2.count().Y - df2.sum().Y\n', '    \n', '    if len(justmiss.index) > 0:\n', ""        d4 = pd.DataFrame({'MIN_VALUE':np.nan},index=[0])\n"", '        d4[""MAX_VALUE""] = np.nan\n', '        d4[""COUNT""] = justmiss.count().Y\n', '        d4[""EVENT""] = justmiss.sum().Y\n', '        d4[""NONEVENT""] = justmiss.count().Y - justmiss.sum().Y\n', '        d3 = d3.append(d4,ignore_index=True)\n', '    \n', '    d3[""EVENT_RATE""] = d3.EVENT/d3.COUNT\n', '    d3[""NON_EVENT_RATE""] = d3.NONEVENT/d3.COUNT\n', '    d3[""DIST_EVENT""] = d3.EVENT/d3.sum().EVENT\n', '    d3[""DIST_NON_EVENT""] = d3.NONEVENT/d3.sum().NONEVENT\n', '    d3[""WOE""] = np.log(d3.DIST_EVENT/d3.DIST_NON_EVENT)\n', '    d3[""IV""] = (d3.DIST_EVENT-d3.DIST_NON_EVENT)*np.log(d3.DIST_EVENT/d3.DIST_NON_EVENT)\n', '    d3[""VAR_NAME""] = ""VAR""\n', ""    d3 = d3[['VAR_NAME','MIN_VALUE', 'MAX_VALUE', 'COUNT', 'EVENT', 'EVENT_RATE', 'NONEVENT', 'NON_EVENT_RATE', 'DIST_EVENT','DIST_NON_EVENT','WOE', 'IV']]      \n"", '    d3 = d3.replace([np.inf, -np.inf], 0)\n', '    d3.IV = d3.IV.sum()\n', '    d3 = d3.reset_index(drop=True)\n', '    \n', '    return(d3)\n', '\n', '-------------------------------------------------------------------------------------------------------------------\n', '\n', 'def data_vars(df1, target):\n', '    \n', '    stack = traceback.extract_stack()\n', '    filename, lineno, function_name, code = stack[-2]\n', ""    vars_name = re.compile(r'\\((.*?)\\).*$').search(code).groups()[0]\n"", '    final = (re.findall(r""[\\w\']+"", vars_name))[-1]\n', '    \n', '    x = df1.dtypes.index\n', '    count = -1\n', '    \n', '    for i in x:\n', '        if i.upper() not in (final.upper()):\n', '            if np.issubdtype(df1[i], np.number) and len(Series.unique(df1[i])) > 2:\n', '                conv = mono_bin(target, df1[i])\n', '                conv[""VAR_NAME""] = i\n', '                count = count + 1\n', '            else:\n', '                conv = char_bin(target, df1[i])\n', '                conv[""VAR_NAME""] = i            \n', '                count = count + 1\n', '                \n', '            if count == 0:\n', '                iv_df = conv\n', '            else:\n', '                iv_df = iv_df.append(conv,ignore_index=True)\n', '    \n', ""    iv = pd.DataFrame({'IV':iv_df.groupby('VAR_NAME').IV.max()})\n"", '    iv = iv.reset_index()\n', '    return(iv_df,iv)\n', '    \n', '-------------------------------------------------------------------------------------------------------------------\n', '\n', '`import pandas as pd\n', 'import numpy as np\n', 'import pandas.core.algorithms as algos\n', 'from pandas import Series\n', 'import scipy.stats.stats as stats\n', 'import re\n', 'import traceback\n', 'import string\n', 'final_iv, IV = data_vars(X_train, y_train)\n', 'IV`\n', '\n', '**Output of IV**\n', '\n', 'Index         | VAR_NAME | IV\n', '----------------------|-----------|---------------\n', ' Logistic Regression with Selected features  |    AMT_ANNUITY  | 4.050335e-04\n', ' Random Forest with Selected features  |    AMT_CREDIT |    2.415783e-03  \n', ' LightGBM with Selected features |    AMT_GOODS_PRICE |    3.591973e-02\n', ' CATBoost with Selected features |    AMT_INCOME_TOTAL |    2.504913e-03\n', ' XGBoost with Selected features |    AMT_REQ_CREDIT_BUREAU_DAY |    1.289777e-02\n', '\n', '\n', 'In case of Information value, predictions with information value < 0.02 are useless for predictions, so we will only consider columns with IV > 0.02.\n', '\n', ""`list_of_columns=IV[IV['IV'] > 0.02]['VAR_NAME'].to_list()\n"", 'print(len(list_of_columns))`\n', '\n', '63\n', '\n', 'We find that only 63 columns are efficient in predicting the default by a customer. Hence we shall only consider those columns \n', '\n', '`X_train_selected_features=X_train[list_of_columns]\n', 'X_test_selected_features=X_test[list_of_columns]\n', ""X_train_selected_features['SK_ID_CURR']=X_train['SK_ID_CURR']\n"", ""X_test_selected_features['SK_ID_CURR']=X_test['SK_ID_CURR']`\n"", '\n', '`application_bureau_prev_test_selected_features=application_bureau_prev_test[list_of_columns]\n', ""application_bureau_prev_test_selected_features['SK_ID_CURR']=application_bureau_prev_test['SK_ID_CURR']`""]",mani method featur select includ featur import use xgboost randomforest method forward backward elimin boruta use one common method featur select inform valu weight evid determin featur select credit risk analysi code calcul iv woe max bin 20 forc bin 3 defin bin function def mono bin x n max bin df1 pd datafram x x justmiss df1 x df1 x isnul notmiss df1 x df1 x notnul r 0 np ab r 1 tri d1 pd datafram x notmiss x notmiss bucket pd qcut notmiss x n d2 d1 groupbi bucket index true r p stat spearmanr d2 mean x d2 mean n n 1 except except e n n 1 len d2 1 n forc bin bin algo quantil notmiss x np linspac 0 1 n len np uniqu bin 2 bin np insert bin 0 1 bin 1 bin 1 bin 1 2 d1 pd datafram x notmiss x notmiss bucket pd cut notmiss x np uniqu bin includ lowest true d2 d1 groupbi bucket index true d3 pd datafram index d3 min valu d2 min x d3 max valu d2 max x d3 count d2 count d3 event d2 sum d3 nonev d2 count d2 sum d3 d3 reset index drop true len justmiss index 0 d4 pd datafram min valu np nan index 0 d4 max valu np nan d4 count justmiss count d4 event justmiss sum d4 nonev justmiss count justmiss sum d3 d3 append d4 ignor index true d3 event rate d3 event d3 count d3 non event rate d3 nonev d3 count d3 dist event d3 event d3 sum event d3 dist non event d3 nonev d3 sum nonev d3 woe np log d3 dist event d3 dist non event d3 iv d3 dist event d3 dist non event np log d3 dist event d3 dist non event d3 var name var d3 d3 var name min valu max valu count event event rate nonev non event rate dist event dist non event woe iv d3 d3 replac np inf np inf 0 d3 iv d3 iv sum return d3 def char bin x df1 pd datafram x x justmiss df1 x df1 x isnul notmiss df1 x df1 x notnul df2 notmiss groupbi x index true d3 pd datafram index d3 count df2 count d3 min valu df2 sum index d3 max valu d3 min valu d3 event df2 sum d3 nonev df2 count df2 sum len justmiss index 0 d4 pd datafram min valu np nan index 0 d4 max valu np nan d4 count justmiss count d4 event justmiss sum d4 nonev justmiss count justmiss sum d3 d3 append d4 ignor index true d3 event rate d3 event d3 count d3 non event rate d3 nonev d3 count d3 dist event d3 event d3 sum event d3 dist non event d3 nonev d3 sum nonev d3 woe np log d3 dist event d3 dist non event d3 iv d3 dist event d3 dist non event np log d3 dist event d3 dist non event d3 var name var d3 d3 var name min valu max valu count event event rate nonev non event rate dist event dist non event woe iv d3 d3 replac np inf np inf 0 d3 iv d3 iv sum d3 d3 reset index drop true return d3 def data var df1 target stack traceback extract stack filenam lineno function name code stack 2 var name compil r search code group 0 final findal r w var name 1 x df1 dtype index count 1 x upper final upper np issubdtyp df1 np number len seri uniqu df1 2 conv mono bin target df1 conv var name count count 1 el conv char bin target df1 conv var name count count 1 count 0 iv df conv el iv df iv df append conv ignor index true iv pd datafram iv iv df groupbi var name iv max iv iv reset index return iv df iv import panda pd import numpi np import panda core algorithm algo panda import seri import scipi stat stat stat import import traceback import string final iv iv data var x train train iv output iv index var name iv logist regress select featur amt annuiti 4 050335e 04 random forest select featur amt credit 2 415783e 03 lightgbm select featur amt good price 3 591973e 02 catboost select featur amt incom total 2 504913e 03 xgboost select featur amt req credit bureau day 1 289777e 02 case inform valu predict inform valu 0 02 useless predict consid column iv 0 02 list column iv iv iv 0 02 var name list print len list column 63 find 63 column effici predict default custom henc shall consid column x train select featur x train list column x test select featur x test list column x train select featur sk id curr x train sk id curr x test select featur sk id curr x test sk id curr applic bureau prev test select featur applic bureau prev test list column applic bureau prev test select featur sk id curr applic bureau prev test sk id curr
3203,"[""## <a id ='13'> 13. Data Imputation before applying machine learning algorithms </a>""]",id 13 13 data imput appli machin learn algorithm
3204,"[""There are many ways to handle missing values. We can use fillna() and replace missing values with data's mean, median or most frequent value. The approach that we shall use below will be Iterative Imputer. Iterative imputer will consider the missing variable to be the dependent varibale and all the other features will be independent variables. Then it will apply regression and the independent variables will be used to determine the dependent variable (which is the missing feature).\n"", '\n', '`imputer = IterativeImputer(BayesianRidge())\n', 'X_train_imputed = pd.DataFrame(imputer.fit_transform(X_train_selected_features))\n', 'X_train_imputed.columns = X_train_selected_features.columns`\n', '\n', '`imputer = IterativeImputer(BayesianRidge())\n', 'application_bureau_prev_test_selected_features_subset1=application_bureau_prev_test_selected_features.iloc[:, np.r_[62,0:30]]\n', 'app_bur_prev_test_imputed_subset1 = pd.DataFrame(imputer.fit_transform(application_bureau_prev_test_selected_features_subset1))\n', 'app_bur_prev_test_imputed_subset1.columns = application_bureau_prev_test_selected_features_subset1.columns`\n', '\n', '`application_bureau_prev_test_selected_features_subset2=application_bureau_prev_test_selected_features.iloc[:, np.r_[31:63]]\n', 'app_bur_prev_test_imputed_subset2 = pd.DataFrame(imputer.fit_transform(application_bureau_prev_test_selected_features_subset2))\n', 'app_bur_prev_test_imputed_subset2.columns = application_bureau_prev_test_selected_features_subset2.columns`\n', '\n', ""`app_bur_prev_test_imputed=pd.merge(app_bur_prev_test_imputed_subset1, app_bur_prev_test_imputed_subset2, on= 'SK_ID_CURR')`\n"", '\n', '`imputer = IterativeImputer(BayesianRidge())\n', 'X_test_imputed = pd.DataFrame(imputer.fit_transform(X_test_selected_features))\n', 'X_test_imputed.columns = X_test_selected_features.columns`\n', '\n', '`print(X_test_imputed.shape)\n', 'print(X_train_imputed.shape)\n', 'print(app_bur_prev_test_imputed.shape)`\n', 'Output\n', '\n', '(61503, 63)\n', '\n', '(246008, 63)\n', '\n', '(48744, 62)\n', '\n', '**Align the training and testing dataframes, keep only columns present in both dataframes**\n', '\n', 'We see above that the number of columns in test and training set are not same.\n', '\n', ""`X_train_imputed, app_bur_prev_test_imputed  = app_bur_prev_test_imputed.align(X_train_imputed, join = 'inner', axis = 1)\n"", ""X_train_imputed,X_test_imputed= X_train_imputed.align(X_test_imputed, join = 'inner', axis = 1)`\n""]",mani way handl miss valu use fillna replac miss valu data mean median frequent valu approach shall use iter imput iter imput consid miss variabl depend varibal featur independ variabl appli regress independ variabl use determin depend variabl miss featur imput iterativeimput bayesianridg x train imput pd datafram imput fit transform x train select featur x train imput column x train select featur column imput iterativeimput bayesianridg applic bureau prev test select featur subset1 applic bureau prev test select featur iloc np r 62 0 30 app bur prev test imput subset1 pd datafram imput fit transform applic bureau prev test select featur subset1 app bur prev test imput subset1 column applic bureau prev test select featur subset1 column applic bureau prev test select featur subset2 applic bureau prev test select featur iloc np r 31 63 app bur prev test imput subset2 pd datafram imput fit transform applic bureau prev test select featur subset2 app bur prev test imput subset2 column applic bureau prev test select featur subset2 column app bur prev test imput pd merg app bur prev test imput subset1 app bur prev test imput subset2 sk id curr imput iterativeimput bayesianridg x test imput pd datafram imput fit transform x test select featur x test imput column x test select featur column print x test imput shape print x train imput shape print app bur prev test imput shape output 61503 63 246008 63 48744 62 align train test datafram keep column present datafram see number column test train set x train imput app bur prev test imput app bur prev test imput align x train imput join inner axi 1 x train imput x test imput x train imput align x test imput join inner axi 1
3205,"[""## <a id='14'> 14. Applying Machine Learning Algorithms (using cross validation) </a>""]",id 14 14 appli machin learn algorithm use cross valid
3206,"['**14.1. Applying Logistic Regression**\n', '\n', '`from sklearn.linear_model import LogisticRegression\n', ""lr_clf = LogisticRegression(random_state = 0, class_weight='balanced')\n"", 'lr_clf.fit(X_train_imputed, y_train)\n', 'from sklearn.model_selection import cross_val_predict\n', 'from sklearn.model_selection import cross_val_score\n', 'y_train_pred_lr=cross_val_predict(lr_clf, X_train_imputed, y_train, cv=3)\n', ""print('Accuracy on Training set:',cross_val_score(lr_clf, X_train_imputed,y_train, cv=3, scoring='accuracy'))\n"", ""print('Accuracy on Test set:',cross_val_score(lr_clf, X_test_imputed,y_test, cv=3, scoring='accuracy'))`\n"", '\n', '*Accuracy on Training set: [0.66357739 0.68726373 0.6783615 ],\n', 'Accuracy on Test set: [0.66164277 0.66167504 0.65356098]*\n', '\n', '-----------------------------------------------------------------------------------------------------------------------\n', '\n', '**14.2. Applying XGBoost**\n', '\n', '`#Scale_pos_weight if set to sum(negative instances)/ sum(negative instances) will take care of imbalanced data in the dataset\n', 'scale_pos_weight_value=y_train.value_counts().values.tolist()[0]/y_train.value_counts().values.tolist()[1]\n', 'from xgboost import XGBClassifier\n', 'XGB_clf = XGBClassifier(scale_pos_weight=scale_pos_weight_value)\n', 'XGB_clf.fit(X_train_imputed, y_train)\n', ""print('Accuracy on Training set:',cross_val_score(XGB_clf, X_train_imputed, y_train, cv=3, scoring='accuracy'))\n"", ""print('Accuracy on Test set:',cross_val_score(XGB_clf, X_test_imputed, y_test, cv=3, scoring='accuracy'))`\n"", '\n', '*Accuracy on Training set: [0.74863421 0.75014024 0.74932319],\n', 'Accuracy on Test set: [0.81938347 0.82391103 0.82102439]*\n', '\n', '-----------------------------------------------------------------------------------------------------------------------\n', '\n', '**14.3. Applying CATBOOST**\n', '\n', '`cols_numeric = X_train_imputed.select_dtypes([np.number]).columns\n', 'cols_categorical=X_train_imputed.columns.difference(cols_numeric)\n', '#We find that there are no categorical columns.\n', 'from catboost import CatBoostClassifier\n', 'CatBoost_clf=CatBoostClassifier(scale_pos_weight=scale_pos_weight_value)\n', ""#CatBoost_clf=CatBoostRegressor(iterations=50, depth=3, learning_rate=0.1, loss_function='RMSE')\n"", 'CatBoost_clf.fit(X_train_imputed, y_train)\n', ""print('Accuracy on Training set:',cross_val_score(CatBoost_clf, X_train_imputed, y_train, cv=3, scoring='accuracy'))\n"", ""print('Accuracy on Test set:',cross_val_score(CatBoost_clf, X_test_imputed, y_test, cv=3, scoring='accuracy'))`\n"", '\n', '*Accuracy on Training set: [0.76207258 0.76210336 0.75900588],\n', 'Accuracy on Test set: [0.81211589 0.8081557  0.81302439]*\n', '\n', '-----------------------------------------------------------------------------------------------------------------------\n', '\n', '**14.4. Applying LightGBM**\n', '\n', '`import lightgbm as lgb\n', 'LightGBM_clf=lgb.LGBMClassifier(scale_pos_weight=scale_pos_weight_value)\n', 'LightGBM_clf.fit(X_train_imputed, y_train)\n', ""print('Accuracy on Training set:',cross_val_score(LightGBM_clf, X_train_imputed, y_train, cv=3, scoring='accuracy'))\n"", ""print('Accuracy on Test set:',cross_val_score(LightGBM_clf, X_test_imputed, y_test, cv=3, scoring='accuracy'))`\n"", '\n', '*Accuracy on Training set: [0.70613629 0.70886076 0.70815346],\n', 'Accuracy on Test set: [0.75158521 0.74947564 0.75639024]*\n', '\n', '-----------------------------------------------------------------------------------------------------------------------\n', '\n', '**14.5. Applying RandomForest**\n', '\n', ""`#class_weight = 'balanced' ensures that RandomForest works well on imbalanced datasets.\n"", 'from sklearn.ensemble import RandomForestClassifier\n', 'rf_clf = RandomForestClassifier(n_estimators = 10, random_state = 0, n_jobs=-1, class_weight=""balanced"")\n', 'rf_clf.fit(X_train_imputed, y_train)\n', ""print('Accuracy on Training set:',cross_val_score(rf_clf, X_train_imputed, y_train, cv=3, scoring='accuracy'))\n"", ""print('Accuracy on Test set:',cross_val_score(rf_clf, X_test_imputed, y_test, cv=3, scoring='accuracy'))`\n"", '\n', '*Accuracy on Training set: [0.86872348 0.86855077 0.8684654 ],\n', 'Accuracy on Test set: [0.86878841 0.86829667 0.8684878 ]*']",14 1 appli logist regress sklearn linear model import logisticregress lr clf logisticregress random state 0 class weight balanc lr clf fit x train imput train sklearn model select import cross val predict sklearn model select import cross val score train pred lr cross val predict lr clf x train imput train cv 3 print accuraci train set cross val score lr clf x train imput train cv 3 score accuraci print accuraci test set cross val score lr clf x test imput test cv 3 score accuraci accuraci train set 0 66357739 0 68726373 0 6783615 accuraci test set 0 66164277 0 66167504 0 65356098 14 2 appli xgboost scale po weight set sum neg instanc sum neg instanc take care imbalanc data dataset scale po weight valu train valu count valu tolist 0 train valu count valu tolist 1 xgboost import xgbclassifi xgb clf xgbclassifi scale po weight scale po weight valu xgb clf fit x train imput train print accuraci train set cross val score xgb clf x train imput train cv 3 score accuraci print accuraci test set cross val score xgb clf x test imput test cv 3 score accuraci accuraci train set 0 74863421 0 75014024 0 74932319 accuraci test set 0 81938347 0 82391103 0 82102439 14 3 appli catboost col numer x train imput select dtype np number column col categor x train imput column differ col numer find categor column catboost import catboostclassifi catboost clf catboostclassifi scale po weight scale po weight valu catboost clf catboostregressor iter 50 depth 3 learn rate 0 1 loss function rmse catboost clf fit x train imput train print accuraci train set cross val score catboost clf x train imput train cv 3 score accuraci print accuraci test set cross val score catboost clf x test imput test cv 3 score accuraci accuraci train set 0 76207258 0 76210336 0 75900588 accuraci test set 0 81211589 0 8081557 0 81302439 14 4 appli lightgbm import lightgbm lgb lightgbm clf lgb lgbmclassifi scale po weight scale po weight valu lightgbm clf fit x train imput train print accuraci train set cross val score lightgbm clf x train imput train cv 3 score accuraci print accuraci test set cross val score lightgbm clf x test imput test cv 3 score accuraci accuraci train set 0 70613629 0 70886076 0 70815346 accuraci test set 0 75158521 0 74947564 0 75639024 14 5 appli randomforest class weight balanc ensur randomforest work well imbalanc dataset sklearn ensembl import randomforestclassifi rf clf randomforestclassifi n estim 10 random state 0 n job 1 class weight balanc rf clf fit x train imput train print accuraci train set cross val score rf clf x train imput train cv 3 score accuraci print accuraci test set cross val score rf clf x test imput test cv 3 score accuraci accuraci train set 0 86872348 0 86855077 0 8684654 accuraci test set 0 86878841 0 86829667 0 8684878
3207,"[""## <a id='15'> 15. Evaluating machine learning algorithms accuracy on training and testing sets </a>\n"", 'This perfromance is without tuning any hyperparameters and without any optimisation']",id 15 15 evalu machin learn algorithm accuraci train test set perfrom without tune hyperparamet without optimis
3208,"['Model         | Train Accuracy | Test AUC\n', '----------------------|-----------|---------------\n', ' Logistic Regression with Selected features  |    0.66  | 0.66\n', ' Random Forest with Selected features  |    0.86 |    0.86  \n', ' LightGBM with Selected features |    0.71 |    0.75\n', ' CATBoost with Selected features |    0.76 |    0.81\n', ' XGBoost with Selected features |    0.75 |    0.82\n', ' \n', ' \n']",model train accuraci test auc logist regress select featur 0 66 0 66 random forest select featur 0 86 0 86 lightgbm select featur 0 71 0 75 catboost select featur 0 76 0 81 xgboost select featur 0 75 0 82
3209,"['Since we get the best performance in Random Forest, let us try to enhance Random Forest by tuning hyperparameters']",sinc get best perform random forest let u tri enhanc random forest tune hyperparamet
3210,"[""## <a id='16'> 16. Optimising selected machine learning model further by choosing best hyperparameters </a>""]",id 16 16 optimis select machin learn model choos best hyperparamet
3211,"[""Since we got best performance with Random Forest, let's try to optimise it further by using the correct parameters. We shall use GridSearchCV or RandomizedSearchCV to choose the best parameters.\n"", '\n', '**First choose the range of hyperparameters using RandomizedSearchCV.**\n', '\n', '`from sklearn.model_selection import RandomizedSearchCV`\n', '\n', '`#Hyperparameters`\n', '\n', '`n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n', ""max_features = ['auto', 'sqrt']\n"", 'max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n', 'max_depth.append(None)\n', 'min_samples_split = [ 10, 25, 50, 100]\n', 'min_samples_leaf = [1, 2, 4,10,20,30]\n', 'bootstrap = [True, False]`\n', '\n', '`#Create the random grid`\n', '\n', ""`random_grid = {'n_estimators': n_estimators,\n"", ""               'max_features': max_features,\n"", ""               'max_depth': max_depth,\n"", ""               'min_samples_split': min_samples_split,\n"", ""               'min_samples_leaf': min_samples_leaf,\n"", ""               'bootstrap': bootstrap}`\n"", '\n', '`from sklearn.ensemble import RandomForestClassifier\n', 'rf = RandomForestClassifier(class_weight=""balanced"")\n', 'rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 10, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n', '#Fit the random search model\n', 'rf_random.fit(X_train_imputed, y_train)\n', 'rf_random.best_params_`\n', '\n', '\n', '**Random_grid Best Parameters output**\n', '\n', '`{\n', ""    'bootstrap': [True],\n"", ""    'max_depth': [100],\n"", ""    'max_features': [2],\n"", ""    'min_samples_leaf': [ 4],\n"", ""    'min_samples_split': [10],\n"", ""    'n_estimators': [200]\n"", '}`\n', '\n', '**GridSearchCV**\n', '\n', 'After getting the best parameters from RandomSearchCV , we have understood the range for hyperparameters. For example the number of trees\\estimators to be used should be in the range of 200, maximum features should be in range of 2 and so on.\n', 'Now let us test which hyperparameter to use by using GridSearchCV. We will include the parameters in the range as found by random_grid output.\n', '\n', '`from sklearn.model_selection import GridSearchCV`\n', '\n', '`#Create the parameter grid based on the results of random search`\n', '\n', '`param_grid = {\n', ""    'bootstrap': [True],\n"", ""    'max_depth': [80, 90, 100, 110],\n"", ""    'max_features': [2, 3],\n"", ""    'min_samples_leaf': [3, 4, 5],\n"", ""    'min_samples_split': [8, 10, 12],\n"", ""    'n_estimators': [100, 200, 300, 1000]\n"", '}`\n', '\n', '`#Create a RandomForets based model\n', 'rf = RandomForestRegressor()`\n', '\n', '`#Instantiate the grid search model\n', 'grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n', '                          cv = 3, n_jobs = -1, verbose = 2)\n', 'grid_search.best_params_`\n', '\n', '**Output of grid search best parameters**\n', '\n', '`max_depth= 20,\n', "" max_features= 'sqrt',\n"", ' min_samples_leaf= 5,\n', ' min_samples_split= 40,\n', ' n_estimators= 200`\n', ' \n', 'Fitting the parameters obtained by GridSearchCV on the RandomForest classification and rechecking the accuracy obtained on training and testing set.\n', '\n', '`from sklearn.ensemble import RandomForestClassifier\n', 'rf_clf_grid = RandomForestClassifier(random_state = 0, n_jobs=-1, class_weight=""balanced"",bootstrap= True,\n', ' max_depth= 20,\n', "" max_features= 'sqrt',\n"", ' min_samples_leaf= 5,\n', ' min_samples_split= 40,\n', ' n_estimators= 200)\n', 'rf_clf_grid.fit(X_train_imputed, y_train)`\n', '\n', ""`print('Accuracy on Training set:',cross_val_score(rf_clf_grid, X_train_imputed, y_train, cv=3, scoring='accuracy'))\n"", ""print('Accuracy on Test set:',cross_val_score(rf_clf_grid, X_test_imputed, y_test, cv=3, scoring='accuracy'))`\n"", '\n', '*Accuracy on Training set: [0.88676162 0.88575888 0.88496622]\n', 'Accuracy on Test set: [0.89396156 0.89868787 0.89497561]*\n', '\n', '\n', 'Note that the accuracy obtained via GridsearchCV and RandomizedSearchCV is more than the accuracy obtained without hyperparameters tuning.']",sinc got best perform random forest let tri optimis use correct paramet shall use gridsearchcv randomizedsearchcv choos best paramet first choos rang hyperparamet use randomizedsearchcv sklearn model select import randomizedsearchcv hyperparamet n estim int x x np linspac start 200 stop 2000 num 10 max featur auto sqrt max depth int x x np linspac 10 110 num 11 max depth append none min sampl split 10 25 50 100 min sampl leaf 1 2 4 10 20 30 bootstrap true fals creat random grid random grid n estim n estim max featur max featur max depth max depth min sampl split min sampl split min sampl leaf min sampl leaf bootstrap bootstrap sklearn ensembl import randomforestclassifi rf randomforestclassifi class weight balanc rf random randomizedsearchcv estim rf param distribut random grid n iter 10 cv 3 verbos 2 random state 42 n job 1 fit random search model rf random fit x train imput train rf random best param random grid best paramet output bootstrap true max depth 100 max featur 2 min sampl leaf 4 min sampl split 10 n estim 200 gridsearchcv get best paramet randomsearchcv understood rang hyperparamet exampl number tree estim use rang 200 maximum featur rang 2 let u test hyperparamet use use gridsearchcv includ paramet rang found random grid output sklearn model select import gridsearchcv creat paramet grid base result random search param grid bootstrap true max depth 80 90 100 110 max featur 2 3 min sampl leaf 3 4 5 min sampl split 8 10 12 n estim 100 200 300 1000 creat randomforet base model rf randomforestregressor instanti grid search model grid search gridsearchcv estim rf param grid param grid cv 3 n job 1 verbos 2 grid search best param output grid search best paramet max depth 20 max featur sqrt min sampl leaf 5 min sampl split 40 n estim 200 fit paramet obtain gridsearchcv randomforest classif recheck accuraci obtain train test set sklearn ensembl import randomforestclassifi rf clf grid randomforestclassifi random state 0 n job 1 class weight balanc bootstrap true max depth 20 max featur sqrt min sampl leaf 5 min sampl split 40 n estim 200 rf clf grid fit x train imput train print accuraci train set cross val score rf clf grid x train imput train cv 3 score accuraci print accuraci test set cross val score rf clf grid x test imput test cv 3 score accuraci accuraci train set 0 88676162 0 88575888 0 88496622 accuraci test set 0 89396156 0 89868787 0 89497561 note accuraci obtain via gridsearchcv randomizedsearchcv accuraci obtain without hyperparamet tune
3212,"[""## <a id='17'> 17. Final predictions </a>""]",id 17 17 final predict
3213,"['Fitting the parameters found out by using GridSearchCV and predicting the outputs by fitting the best machine learning model.\n', '\n', '`rf_clf_grid.fit(X_train_imputed, y_train)\n', 'predictions_grid=rf_clf_grid.predict(app_bur_prev_test_imputed)`\n', '\n', 'Saving the results in csv files \n', '\n', '`predictions_grid_df=pd.DataFrame(data={""SK_ID_CURR"":app_bur_prev_test_imputed[""SK_ID_CURR""],""TARGET"":predictions_grid}) \n', ""predictions_grid_df['SK_ID_CURR'] = predictions_grid_df['SK_ID_CURR'].astype(int)\n"", 'predictions_grid_df.to_csv(path_or_buf=""predictions_grid_df.csv"",index=False)`']",fit paramet found use gridsearchcv predict output fit best machin learn model rf clf grid fit x train imput train predict grid rf clf grid predict app bur prev test imput save result csv file predict grid df pd datafram data sk id curr app bur prev test imput sk id curr target predict grid predict grid df sk id curr predict grid df sk id curr astyp int predict grid df csv path buf predict grid df csv index fals
3214,"# HOW TO INTERPRET BUREAU DATA

This table talks about the Loan data of each unique customer with all financial institutions other than Home Credit
For each unique SK_ID_CURR we have multiple SK_ID_BUREAU Id's, each being a unique loan transaction from other financial institutions availed by the same customer and reported to the bureau. ",interpret bureau data tabl talk loan data uniqu custom financi institut home credit uniqu sk id curr multipl sk id bureau id uniqu loan transact financi institut avail custom report bureau
3215,"# EXAMPLE OF BUREAU TRANSACTIONS 

- In the example below customer with SK_ID_CURR = 100001 had  7 credit transactions before the current application. ",exampl bureau transact exampl custom sk id curr 100001 7 credit transact current applic
3216,# UNDERSTANDING OF VARIABLES ,understand variabl
3217,"CREDIT_ACTIVE - Current status of a Loan - Closed/ Active (2 values)

CREDIT_CURRENCY - Currency in which the transaction was executed -  Currency1, Currency2, Currency3, Currency4 
                                        ( 4 values)
                                        
CREDIT_DAY_OVERDUE -  Number of overdue days 

CREDIT_TYPE -  Consumer Credit, Credit card, Mortgage, Car loan, Microloan, Loan for working capital replemishment, 
                             Loan for Business development, Real estate loan, Unkown type of laon, Another type of loan. 
                             Cash loan, Loan for the purchase of equipment, Mobile operator loan, Interbank credit, 
                             Loan for purchase of shares ( 15 values )

DAYS_CREDIT -   Number of days ELAPSED since customer applied for CB credit with respect to current application 
Interpretation - Are these loans evenly spaced time intervals? Are they concentrated within a same time frame?


DAYS_CREDIT_ENDDATE - Number of days the customer CREDIT is valid at the time of application 
CREDIT_DAY_OVERDUE - Number of days the customer CREDIT is past the end date at the time of application

AMT_CREDIT_SUM -  Total available credit for a customer 
AMT_CREDIT_SUM_DEBT -  Total amount yet to be repayed
AMT_CREDIT_SUM_LIMIT -   Current Credit that has been utilized 
AMT_CREDIT_SUM_OVERDUE - Current credit payment that is overdue 
CNT_CREDIT_PROLONG - How many times was the Credit date prolonged 

# NOTE: 
For a given loan transaction 
 'AMT_CREDIT_SUM' =  'AMT_CREDIT_SUM_DEBT' +'AMT_CREDIT_SUM_LIMIT'



AMT_ANNUITY -  Annuity of the Credit Bureau data
DAYS_CREDIT_UPDATE -  Number of days before current application when last CREDIT UPDATE was received 
DAYS_ENDDATE_FACT -    Days since CB credit ended at the time of application 
AMT_CREDIT_MAX_OVERDUE - Maximum Credit amount overdue at the time of application 
",credit activ current statu loan close activ 2 valu credit currenc currenc transact execut currency1 currency2 currency3 currency4 4 valu credit day overdu number overdu day credit type consum credit credit card mortgag car loan microloan loan work capit replemish loan busi develop real estat loan unkown type laon anoth type loan cash loan loan purchas equip mobil oper loan interbank credit loan purchas share 15 valu day credit number day elaps sinc custom appli cb credit respect current applic interpret loan evenli space time interv concentr within time frame day credit enddat number day custom credit valid time applic credit day overdu number day custom credit past end date time applic amt credit sum total avail credit custom amt credit sum debt total amount yet repay amt credit sum limit current credit util amt credit sum overdu current credit payment overdu cnt credit prolong mani time credit date prolong note given loan transact amt credit sum amt credit sum debt amt credit sum limit amt annuiti annuiti credit bureau data day credit updat number day current applic last credit updat receiv day enddat fact day sinc cb credit end time applic amt credit max overdu maximum credit amount overdu time applic
3218,# FEATURE ENGINEERING WITH BUREAU CREDIT ,featur engin bureau credit
3219,# FEATURE 1 - NUMBER OF PAST LOANS PER CUSTOMER ,featur 1 number past loan per custom
3220,# FEATURE 2 - NUMBER OF TYPES OF PAST LOANS PER CUSTOMER ,featur 2 number type past loan per custom
3221,"# FEATURE 3 - AVERAGE NUMBER OF PAST LOANS PER TYPE PER CUSTOMER

# Is the Customer diversified in taking multiple types of Loan or Focused on a single type of loan
",featur 3 averag number past loan per type per custom custom diversifi take multipl type loan focus singl type loan
3222,# FEATURE 4 - % OF ACTIVE LOANS FROM BUREAU DATA ,featur 4 activ loan bureau data
3223,"# FEATURE 5

# AVERAGE NUMBER OF DAYS BETWEEN SUCCESSIVE PAST APPLICATIONS FOR EACH CUSTOMER 

# How often did the customer take credit in the past? Was it spaced out at regular time intervals - a signal of good financial planning OR were the loans concentrated around a smaller time frame - indicating potential financial trouble?
",featur 5 averag number day success past applic custom often custom take credit past space regular time interv signal good financi plan loan concentr around smaller time frame indic potenti financi troubl
3224,"# FEATURE 6  

# % of LOANS PER CUSTOMER WHERE END DATE FOR CREDIT IS PAST

 # INTERPRETING CREDIT_DAYS_ENDDATE 
 
 #  NEGATIVE VALUE - Credit date was in the past at time of application( Potential Red Flag !!! )
 
 # POSITIVE VALUE - Credit date is in the future at time of application ( Potential Good Sign !!!!)
 
 # NOTE : This is not the same as % of Active loans since Active loans 
 # can have Negative and Positive values for DAYS_CREDIT_ENDDATE",featur 6 loan per custom end date credit past interpret credit day enddat neg valu credit date past time applic potenti red flag posit valu credit date futur time applic potenti good sign note activ loan sinc activ loan neg posit valu day credit enddat
3225,"# FEATURE 7 

# AVERAGE NUMBER OF DAYS IN WHICH CREDIT EXPIRES IN FUTURE -INDICATION OF CUSTOMER DELINQUENCY IN FUTURE??",featur 7 averag number day credit expir futur indic custom delinqu futur
3226,"# FEATURE 8 - DEBT OVER CREDIT RATIO 
# The Ratio of Total Debt to Total Credit for each Customer 
# A High value may be a red flag indicative of potential default",featur 8 debt credit ratio ratio total debt total credit custom high valu may red flag indic potenti default
3227,"# FEATURE 9 - OVERDUE OVER DEBT RATIO 
#  What fraction of total Debt is overdue per customer?
# A high value could indicate a potential DEFAULT ",featur 9 overdu debt ratio fraction total debt overdu per custom high valu could indic potenti default
3228,# FEATURE 10 - AVERAGE NUMBER OF LOANS PROLONGED ,featur 10 averag number loan prolong
3229,"![](https://www.homecredit.ph/files/copy-hc-logo.png)

# DNN classifier in Tensorflow

This kernel will build a DNN classifier for the Home Credit Default Risk competition. The challenge here (as always!) is to try and match the performance of the LightGBM/XGBoost classifiers which always seems tricky for NNs for this kind of problem.

A lot of the feature engineering going into the model is from my previous kernel [here](https://www.kaggle.com/shep312/lightgbm-with-weighted-averages-dropout-771), so I will focus more on the NN graph development here.

### Contents

1. [Load and process data](#load)
    1. [Check nulls](#nulls)
    2. [Identify categoricals](#cats)
    3. [Scaling](#scale)
2. [Building the graph](#graph)
3. [Training the NN](#train)
4. [Analysis and submission](#submit)",http www homecredit ph file copi hc logo png dnn classifi tensorflow kernel build dnn classifi home credit default risk competit challeng alway tri match perform lightgbm xgboost classifi alway seem tricki nn kind problem lot featur engin go model previou kernel http www kaggl com shep312 lightgbm weight averag dropout 771 focu nn graph develop content 1 load process data load 1 check null null 2 identifi categor cat 3 scale scale 2 build graph graph 3 train nn train 4 analysi submiss submit
3230,"## 1. Load and process data <a name=""load""></a>

First step is to load all the different .csvs into memory.",1 load process data name load first step load differ csv memori
3231,Since they are in disparate .csv's next I need to merge them into a single use-able dataframe:,sinc dispar csv next need merg singl use abl datafram
3232,"Before I do any futher processing, extract the target variable for training later.",futher process extract target variabl train later
3233,"### 1.1 Check nulls <a name=""nulls""></a>

The data set has a a few variables containing a lot of nulls. Drop any features that are over x% null, then fill with 0. This obviously isn't a great method and provides room for improvement later on.",1 1 check null name null data set variabl contain lot null drop featur x null fill 0 obvious great method provid room improv later
3234,"### 1.2 Identify categorical variables <a name=""cats""></a>

Categorical variables will be important in this model - there are a lot of them and a few that have high cardinality. 

I will be creating embeddings to encode them for use in the model later, but for now just make a note of them and their positions.",1 2 identifi categor variabl name cat categor variabl import model lot high cardin creat embed encod use model later make note posit
3235,"### 1.3 Scaling <a name=""scale""></a>

Next data processing step is to scale the features so they don't get unfairly weighted against each other.",1 3 scale name scale next data process step scale featur get unfairli weight
3236,"## 2. Building the graph <a name=""graph""></a>

Now the data is in decent shape, build the NN. 

First step, however, is to re-separate the competition train and test sets. I'll then further split the training set down to provide a hold out validation set.",2 build graph name graph data decent shape build nn first step howev separ competit train test set split train set provid hold valid set
3237,Set the parameters for the network:,set paramet network
3238,Graph itself. Note that there is an embedding step first where each categorical feature is embedded and attached to the continuous input features.,graph note embed step first categor featur embed attach continu input featur
3239,"## 3. Training the NN <a name=""train""></a>

Time to train the network. We know that only 8% of the targets are positive, so I will be upsampling positives for the gradient descent batches. I'm going to go with even representation in the training batches, but this isn't necessarily going to get the best score so may need optimising.",3 train nn name train time train network know 8 target posit upsampl posit gradient descent batch go go even represent train batch necessarili go get best score may need optimis
3240,"So an OK performance, but not matching the gradient boosted results yet. There's a lot of avenues to improve, however, including but not limited to:

- **Optimising number of nodes / width**: Currently I've only used a small number of nodes in the hidden layers, limiting the complexity of the model. More nodes should lead to better performance, but I found that it started to overfit. There is probably a way to regularise the network to allow it to get more complex
- **Number of layers**: Currently at 3 hidden layers deep, could be a better number
- **Upsampling of positives**: Reduction in the upsampling of postive samples could be tuned
- **Other hyperparameters**: Learning rate, batch size etc. could be tuned",ok perform match gradient boost result yet lot avenu improv howev includ limit optimis number node width current use small number node hidden layer limit complex model node lead better perform found start overfit probabl way regularis network allow get complex number layer current 3 hidden layer deep could better number upsampl posit reduct upsampl postiv sampl could tune hyperparamet learn rate batch size etc could tune
3241,"## 4. Analysis and Submission <a name=""submit""></a>

Lets have a look at some summarising plots, then submit the results. 

Training curves & the ROC curve:",4 analysi submiss name submit let look summaris plot submit result train curv roc curv
3242,...and a precision-recall curve and the confusion matrix:,precis recal curv confus matrix
3243,"# Home Credit Default Risk - Exploration + Baseline Model

Many people struggle to get loans due to insufficient or non-existent credit histories. And, unfortunately, this population is often taken advantage of by untrustworthy lenders. Home Credit strives to broaden financial inclusion for the unbanked population by providing a positive and safe borrowing experience. In order to make sure this underserved population has a positive loan experience, Home Credit makes use of a variety of alternative data--including telco and transactional information--to predict their clients' repayment abilities.

While Home Credit is currently using various statistical and machine learning methods to make these predictions, they're challenging Kagglers to help them unlock the full potential of their data. Doing so will ensure that clients capable of repayment are not rejected and that loans are given with a principal, maturity, and repayment calendar that will empower their clients to be successful.

This is a simple notebook on exploration and baseline model of home credit default risk data 

**Contents**   
[1. Dataset Preparation](#1)    
[2. Exploration - Applications Train](#2)  
&nbsp;&nbsp;&nbsp;&nbsp; [2.1 Snapshot - Application Train](#2.1)    
&nbsp;&nbsp;&nbsp;&nbsp; [2.2 Distribution of Target Variable](#2.2)    
&nbsp;&nbsp;&nbsp;&nbsp; [2.3 Applicant's Gender Type](#2.3)    
&nbsp;&nbsp;&nbsp;&nbsp; [2.4 Family Status of Applicants who takes the loan](#2.4)  
&nbsp;&nbsp;&nbsp;&nbsp; [2.5 Does applicants own Real Estate or Car](#2.5)    
&nbsp;&nbsp;&nbsp;&nbsp; [2.6 Suite Type and Income Type of Applicants](#2.6)   
&nbsp;&nbsp;&nbsp;&nbsp; [2.7 Applicants Contract Type](#2.7)   
&nbsp;&nbsp;&nbsp;&nbsp; [2.8 Education Type and Occupation Type](#2.8)   
&nbsp;&nbsp;&nbsp;&nbsp; [2.9 Organization Type and Occupation Type](#2.9)   
&nbsp;&nbsp;&nbsp;&nbsp; [2.10 Walls Material, Foundation and House Type](#2.10)   
&nbsp;&nbsp;&nbsp;&nbsp; [2.11 Amount Credit Distribution](#2.11)    
&nbsp;&nbsp;&nbsp;&nbsp; [2.12 Amount Annuity Distribution - Distribution](#2.12)  
&nbsp;&nbsp;&nbsp;&nbsp; [2.13 Amount Goods Price - Distribution](#2.13)   
&nbsp;&nbsp;&nbsp;&nbsp; [2.14 Amount Region Population Relative](#2.14)    
&nbsp;&nbsp;&nbsp;&nbsp; [2.15 Days Birth - Distribution](#2.15)   
&nbsp;&nbsp;&nbsp;&nbsp; [2.16 Days Employed - Distribution](#2.16)    
&nbsp;&nbsp;&nbsp;&nbsp; [2.17 Distribution of Num Days Registration](#2.17)  
&nbsp;&nbsp;&nbsp;&nbsp; [2.18 Applicants Number of Family Members](#2.18)  
&nbsp;&nbsp;&nbsp;&nbsp; [2.19 Applicants Number of Children](#2.19)  
[3. Exploration - Bureau Data](#3)  
&nbsp;&nbsp;&nbsp;&nbsp; [3.1 Snapshot - Bureau Data](#3)    
[4. Exploration - Bureau Balance Data](#4)  
&nbsp;&nbsp;&nbsp;&nbsp; [4.1 Snapshot - Bureau Balance Data](#3)     
[5. Exploration - Credit Card Balance Data](#5)   
&nbsp;&nbsp;&nbsp;&nbsp; [5.1 Snapshot - Credit Card Balance Data](#3)   
[6. Exploration - POS Cash Balance Data](#6)   
&nbsp;&nbsp;&nbsp;&nbsp; [6.1 Snapshot - POS Cash Balance Data](#3)   
[7. Exploration - Previous Application Data](#7)   
&nbsp;&nbsp;&nbsp;&nbsp; [7.1 Snapshot - Previous Application Data](#7.1)  
&nbsp;&nbsp;&nbsp;&nbsp; [7.2 Contract Status Distribution - Previous Applications](#7.2)  
&nbsp;&nbsp;&nbsp;&nbsp; [7.3 Suite Type Distribution - Previous Application](#7.3)    
&nbsp;&nbsp;&nbsp;&nbsp; [7.4 Client Type Distribution  - Previous Application](#7.4)    
&nbsp;&nbsp;&nbsp;&nbsp; [7.5 Channel Type Distribution - Previous Applications](#7.5)  
[8. Exploration - Installation Payments](#8)  
&nbsp;&nbsp;&nbsp;&nbsp; [8.1 Snapshot of Installation Payments](#3)  
[9. Baseline Model](#9)  
&nbsp;&nbsp;&nbsp;&nbsp; [9.1 Dataset Preparation](#9.1)  
&nbsp;&nbsp;&nbsp;&nbsp; [9.2 Handelling Categorical Features](#9.2)     
&nbsp;&nbsp;&nbsp;&nbsp; [9.3 Create Flat Dataset](#9.3)     
&nbsp;&nbsp;&nbsp;&nbsp; [9.4 Validation Sets Preparation](#9.4)    
&nbsp;&nbsp;&nbsp;&nbsp; [9.5 Model Fitting](#9.5)    
&nbsp;&nbsp;&nbsp;&nbsp; [9.6 Feature Importance](#9.6)    
&nbsp;&nbsp;&nbsp;&nbsp; [9.7 Prediction](#9.7)   



## <a id=""1"">1. Dataset Preparation </a>",home credit default risk explor baselin model mani peopl struggl get loan due insuffici non exist credit histori unfortun popul often taken advantag untrustworthi lender home credit strive broaden financi inclus unbank popul provid posit safe borrow experi order make sure underserv popul posit loan experi home credit make use varieti altern data includ telco transact inform predict client repay abil home credit current use variou statist machin learn method make predict challeng kaggler help unlock full potenti data ensur client capabl repay reject loan given princip matur repay calendar empow client success simpl notebook explor baselin model home credit default risk data content 1 dataset prepar 1 2 explor applic train 2 nbsp nbsp nbsp nbsp 2 1 snapshot applic train 2 1 nbsp nbsp nbsp nbsp 2 2 distribut target variabl 2 2 nbsp nbsp nbsp nbsp 2 3 applic gender type 2 3 nbsp nbsp nbsp nbsp 2 4 famili statu applic take loan 2 4 nbsp nbsp nbsp nbsp 2 5 applic real estat car 2 5 nbsp nbsp nbsp nbsp 2 6 suit type incom type applic 2 6 nbsp nbsp nbsp nbsp 2 7 applic contract type 2 7 nbsp nbsp nbsp nbsp 2 8 educ type occup type 2 8 nbsp nbsp nbsp nbsp 2 9 organ type occup type 2 9 nbsp nbsp nbsp nbsp 2 10 wall materi foundat hous type 2 10 nbsp nbsp nbsp nbsp 2 11 amount credit distribut 2 11 nbsp nbsp nbsp nbsp 2 12 amount annuiti distribut distribut 2 12 nbsp nbsp nbsp nbsp 2 13 amount good price distribut 2 13 nbsp nbsp nbsp nbsp 2 14 amount region popul rel 2 14 nbsp nbsp nbsp nbsp 2 15 day birth distribut 2 15 nbsp nbsp nbsp nbsp 2 16 day employ distribut 2 16 nbsp nbsp nbsp nbsp 2 17 distribut num day registr 2 17 nbsp nbsp nbsp nbsp 2 18 applic number famili member 2 18 nbsp nbsp nbsp nbsp 2 19 applic number child 2 19 3 explor bureau data 3 nbsp nbsp nbsp nbsp 3 1 snapshot bureau data 3 4 explor bureau balanc data 4 nbsp nbsp nbsp nbsp 4 1 snapshot bureau balanc data 3 5 explor credit card balanc data 5 nbsp nbsp nbsp nbsp 5 1 snapshot credit card balanc data 3 6 explor po cash balanc data 6 nbsp nbsp nbsp nbsp 6 1 snapshot po cash balanc data 3 7 explor previou applic data 7 nbsp nbsp nbsp nbsp 7 1 snapshot previou applic data 7 1 nbsp nbsp nbsp nbsp 7 2 contract statu distribut previou applic 7 2 nbsp nbsp nbsp nbsp 7 3 suit type distribut previou applic 7 3 nbsp nbsp nbsp nbsp 7 4 client type distribut previou applic 7 4 nbsp nbsp nbsp nbsp 7 5 channel type distribut previou applic 7 5 8 explor instal payment 8 nbsp nbsp nbsp nbsp 8 1 snapshot instal payment 3 9 baselin model 9 nbsp nbsp nbsp nbsp 9 1 dataset prepar 9 1 nbsp nbsp nbsp nbsp 9 2 handel categor featur 9 2 nbsp nbsp nbsp nbsp 9 3 creat flat dataset 9 3 nbsp nbsp nbsp nbsp 9 4 valid set prepar 9 4 nbsp nbsp nbsp nbsp 9 5 model fit 9 5 nbsp nbsp nbsp nbsp 9 6 featur import 9 6 nbsp nbsp nbsp nbsp 9 7 predict 9 7 id 1 1 dataset prepar
3244,"## <a href=""2"">2. Exploration of Applications Data </a>

### <a href=""2.1"">2.1 Snapshot of Application Train</a>

Application data consists of static data for all applications and every row represents one loan.",href 2 2 explor applic data href 2 1 2 1 snapshot applic train applic data consist static data applic everi row repres one loan
3245,"> There are total 307,511 rows which contains the information of loans and there are 122 variables. 

> The target variable defines if the client had payment difficulties meaning he/she had late payment more than X days on at least one of the first Y installments of the loan. Such case is marked as 1 while other all other cases as 0.

### <a href=""2.2"">2.2 Distribution of Target Variable </a>

The target variable defines weather the loan was repayed or not. Let us look at what is the distribution of loan repayment in the training dataset. ",total 307 511 row contain inform loan 122 variabl target variabl defin client payment difficulti mean late payment x day least one first instal loan case mark 1 case 0 href 2 2 2 2 distribut target variabl target variabl defin weather loan repay let u look distribut loan repay train dataset
3246,"> The target variable is slightly imbalance with the majority of loans has the target equals to 0 which indicates that individuals did not had any problems in paying installments in given time. There are about 91% loans which is equal to about 282K with target = 0, While only 9% of the total loans (about 24K applicants) in this dataset involved the applicants having problems in repaying the loan / making installments.  

### <a href=""2.3"">2.3 Gender Type of Applicants </a>",target variabl slightli imbal major loan target equal 0 indic individu problem pay instal given time 91 loan equal 282k target 0 9 total loan 24k applic dataset involv applic problem repay loan make instal href 2 3 2 3 gender type applic
3247,"> In the applicant's data women have applied for a larger majority of loans which is almost the double as the men. In total, there are about 202,448 loan applications filed by females in contrast to about 105,059 applications filed by males. However, a larger percentage (about 10% of the total) of men had the problems in paying the loan or making installments within time as compared to women applicants (about 7%). 

### <a href=""2.4"">2.4 Family Status of Applicants </a>",applic data woman appli larger major loan almost doubl men total 202 448 loan applic file femal contrast 105 059 applic file male howev larger percentag 10 total men problem pay loan make instal within time compar woman applic 7 href 2 4 2 4 famili statu applic
3248,"> Married people have applied for a larger number of loan applications about 196K, However, people having Civil Marriage has the highest percentage (about 10%) of loan problems and challenges. 

### <a href=""2.5"">2.5. Does applicants own Real Estate or Car ?</a>",marri peopl appli larger number loan applic 196k howev peopl civil marriag highest percentag 10 loan problem challeng href 2 5 2 5 applic real estat car
3249,"> About 70% of the applicants own Real Estate, while only 34% of applicants own Car who had applied for the loan in the past years. However, a higher percentage of people having payment difficulties was observed with applicants which did not owned Car or which did not owned Real Estate. 

### <a href=""2.6"">2.6 Suite Type and Income Type of Applicants </a>",70 applic real estat 34 applic car appli loan past year howev higher percentag peopl payment difficulti observ applic own car own real estat href 2 6 2 6 suit type incom type applic
3250,"> Top 3 Type Suites which applies for loan are the houses which are:  
    - Unaccompanined (about 248K applicants) 
    - Family (about 40K applicants)  
    - Spouse, partner (about 11K applicants)    
> The income type of people who applies for loan include about 8 categroes, top ones are : 
    - Working Class (158K)
    - Commercial Associate (71K)
    - Pensiner (55K)

### <a id=""2.6.1"">2.6.1 How does Target Varies with Suite and Income Type of Applicants </a>",top 3 type suit appli loan hous unaccompanin 248k applic famili 40k applic spous partner 11k applic incom type peopl appli loan includ 8 categro top one work class 158k commerci associ 71k pensin 55k id 2 6 1 2 6 1 target vari suit incom type applic
3251,"> We see that Applicants having Income Types : Maternity Leaves and UnEmployed has the highest percentage (about 40% and 36% approx) of Target = 1 ie. having more payment problems, while Pensioners have the least (about 5.3%). 

### <a id=""2.7"">2.7. Applicant's Contract Type</a>",see applic incom type matern leav unemploy highest percentag 40 36 approx target 1 ie payment problem pension least 5 3 id 2 7 2 7 applic contract type
3252,"> Cash loans with about 278K loans contributes to a majorty of total lonas in this dataset. Revolving loans has significantly lesser number equal to about 29K as compared to Cash loans. 

### <a id=""2.8"">2.8 Education Type and Housing Type </a>",cash loan 278k loan contribut majorti total lona dataset revolv loan significantli lesser number equal 29k compar cash loan id 2 8 2 8 educ type hous type
3253,"> A large number of applications (218K) are filed by people having secondary education followed by people with Higher Education with 75K applications. Applicants living in House / apartments has the highest number of loan apllications equal to 272K. While we see that the applicants with Lower Secondary education status has the highest percentage of payment related problems. Also, Applicants living in apartments or living with parents also shows the same trend. ",larg number applic 218k file peopl secondari educ follow peopl higher educ 75k applic applic live hous apart highest number loan apllic equal 272k see applic lower secondari educ statu highest percentag payment relat problem also applic live apart live parent also show trend
3254,"### <a id=""2.9"">2.9. Which Organization and Occupation Type applies for loan and which repays </a>",id 2 9 2 9 organ occup type appli loan repay
3255,"> Top Applicant's who applied for loan : Laborers - Approx 55 K, Sales Staff - Approx 32 K, Core staff - Approx 28 K. Entity Type 3 type organizations have filed maximum number of loans equal to approx 67K

### <a id=""2.9.1"">2.9.1 Target Variable with respect to Organization and Occupation Type </a>",top applic appli loan labor approx 55 k sale staff approx 32 k core staff approx 28 k entiti type 3 type organ file maximum number loan equal approx 67k id 2 9 1 2 9 1 target variabl respect organ occup type
3256,"### <a id=""2.10"">2.10 Walls Material, Foundation, and House Type </a>",id 2 10 2 10 wall materi foundat hous type
3257,"> - ""Blocks and Flats"" related house types have filed the largest number of loan applications equal to about 150K, rest of the other categories : Specific Housing and Terraced house have less than 1500 applications. Similarly houses having Panel and Stone Brick type walls material have filed the largest applciations close to 120K combined. 

### <a id=""2.10.1"">2.10.1 Target Variable with respect to Walls Material, Fondkappremont, House Type </a>",block flat relat hous type file largest number loan applic equal 150k rest categori specif hous terrac hous le 1500 applic similarli hous panel stone brick type wall materi file largest applciat close 120k combin id 2 10 1 2 10 1 target variabl respect wall materi fondkappremont hous type
3258,"### <a id=""2.11"">2.11. Distribution of Amount Credit </a>",id 2 11 2 11 distribut amount credit
3259,"### <a id=""2.12"">2.12 Distribution of Amount AMT_ANNUITY </a>",id 2 12 2 12 distribut amount amt annuiti
3260,"### <a id=""2.13"">2.13 Distribution of Amount AMT_GOODS_PRICE </a>",id 2 13 2 13 distribut amount amt good price
3261,"### <a id=""2.14"">2.14 Distribution of Amount REGION_POPULATION_RELATIVE </a>",id 2 14 2 14 distribut amount region popul rel
3262,"### <a id=""2.15"">2.15 Distribution of Amount DAYS_BIRTH </a>",id 2 15 2 15 distribut amount day birth
3263,"### <a id=""2.16"">2.16 Distribution of Amount DAYS_EMPLOYED </a>",id 2 16 2 16 distribut amount day employ
3264,"### <a id=""2.17"">2.17 Distribution of Number of Days for Registration</a>",id 2 17 2 17 distribut number day registr
3265,"### <a id=""2.18"">2.18 How many Family Members does the applicants has </a>",id 2 18 2 18 mani famili member applic
3266,"> Most of the applicants who applied for loan had 2 family members in total

### <a id=""2.19""> 2.19 How many Children does the applicants have </a>",applic appli loan 2 famili member total id 2 19 2 19 mani child applic
3267,"> A large majority of applicants did not had children when they applied for loan

## <a id=""3"">3. Exploration of Bureau Data</a>

All client's previous credits provided by other financial institutions that were reported to Credit Bureau (for clients who have a loan in our sample). For every loan in our sample, there are as many rows as number of credits the client had in Credit Bureau before the application date.

### <a id=""3.1"">3.1 Snapshot of Bureau Data</a>",larg major applic child appli loan id 3 3 explor bureau data client previou credit provid financi institut report credit bureau client loan sampl everi loan sampl mani row number credit client credit bureau applic date id 3 1 3 1 snapshot bureau data
3268,"## <a id=""4"">4. Exploration of Bureau Balance Data</a>

Monthly balances of previous credits in Credit Bureau. This table has one row for each month of history of every previous credit reported to Credit Bureau – i.e the table has (#loans in sample * # of relative previous credits * # of months where we have some history observable for the previous credits) rows.

### <a id=""4.1"">4.1 Snapshot of Bureau Balance Data</a>",id 4 4 explor bureau balanc data monthli balanc previou credit credit bureau tabl one row month histori everi previou credit report credit bureau e tabl loan sampl rel previou credit month histori observ previou credit row id 4 1 4 1 snapshot bureau balanc data
3269,"## <a id=""5"">5. Exploration of Credit Card Balance</a>

Monthly balance snapshots of previous credit cards that the applicant has with Home Credit.This table has one row for each month of history of every previous credit in Home Credit (consumer credit and cash loans) related to loans in our sample – i.e. the table has (#loans in sample * # of relative previous credit cards * # of months where we have some history observable for the previous credit card) rows.

### <a id=""5.1"">5.1 Snapshot of Credit Card Balance</a>",id 5 5 explor credit card balanc monthli balanc snapshot previou credit card applic home credit tabl one row month histori everi previou credit home credit consum credit cash loan relat loan sampl e tabl loan sampl rel previou credit card month histori observ previou credit card row id 5 1 5 1 snapshot credit card balanc
3270,"## <a id=""6"">6. Exploration of POS CASH Balance Data</a>

Monthly balance snapshots of previous POS (point of sales) and cash loans that the applicant had with Home Credit. This table has one row for each month of history of every previous credit in Home Credit (consumer credit and cash loans) related to loans in our sample – i.e. the table has (#loans in sample * # of relative previous credits * # of months in which we have some history observable for the previous credits) rows.

### <a id=""6.1"">6.1 Snapshot of POS CASH Balance Data</a>",id 6 6 explor po cash balanc data monthli balanc snapshot previou po point sale cash loan applic home credit tabl one row month histori everi previou credit home credit consum credit cash loan relat loan sampl e tabl loan sampl rel previou credit month histori observ previou credit row id 6 1 6 1 snapshot po cash balanc data
3271,"## <a id=""7"">7. Exploration of Prev Application</a>

### <a id=""7.1"">7.1 Snapshot of Prev Application</a>",id 7 7 explor prev applic id 7 1 7 1 snapshot prev applic
3272,"### <a id=""7.2"">7.2 Contract Status Distribution in Previously Filed Applications</a>",id 7 2 7 2 contract statu distribut previous file applic
3273,"> - A large number of people (about 62%) had their previous applications approved, while about 19% of them had cancelled and other 17% were resued. 

### <a id=""7.3"">7.3 Suite Type Distribution of Previous Applications</a>",larg number peopl 62 previou applic approv 19 cancel 17 resu id 7 3 7 3 suit type distribut previou applic
3274,"> - A majority of applicants had previous applications having Unaccompanied Suite Type (about 60%) followed by Family related suite type (about 25%)

### <a id=""7.4"">7.4 Client Type of Previous Applications</a>",major applic previou applic unaccompani suit type 60 follow famili relat suit type 25 id 7 4 7 4 client type previou applic
3275,"> - About 74% of the previous applications were Repeater Clients, while only 18% are new. About 8% are refreshed. 

### <a id=""7.5"">7.5 Channel Type - Previous Applications </a>",74 previou applic repeat client 18 new 8 refresh id 7 5 7 5 channel type previou applic
3276,"## <a id=""8"">8. Exploration of Installation Payments </a>
### <a id=""8.1"">8.1 Snapshot of Installation Payments </a>",id 8 8 explor instal payment id 8 1 8 1 snapshot instal payment
3277,"## <a id=""9"">9. Baseline Model </a>

### <a id=""9.1"">9.1 Dataset Preparation</a>",id 9 9 baselin model id 9 1 9 1 dataset prepar
3278,"### <a id=""9.2"">9.2 Handelling Categorical Features</a>",id 9 2 9 2 handel categor featur
3279,"### <a id=""9.3"">9.3 Feature Engineering</a>",id 9 3 9 3 featur engin
3280,"### <a id=""9.3.1"">9.3.1 Feature Engineering - Previous Applications</a>

Credits to excellent kernel shared by Olivier for more ideas: https://www.kaggle.com/ogrellier/good-fun-with-ligthgbm 
",id 9 3 1 9 3 1 featur engin previou applic credit excel kernel share olivi idea http www kaggl com ogrelli good fun ligthgbm
3281,"### <a id=""9.3.2"">9.3.2 Feature Engineering - Bureau Data</a>",id 9 3 2 9 3 2 featur engin bureau data
3282,"### <a id=""9.3.3"">9.3.3 Feature Engineering - Previous Installments</a>",id 9 3 3 9 3 3 featur engin previou instal
3283,"### <a id=""9.3.4"">9.3.4 Feature Engineering - Pos Cash Balance</a>",id 9 3 4 9 3 4 featur engin po cash balanc
3284,"### <a id=""9.3.5"">9.3.5 Feature Engineering - Credit Card Balance </a>",id 9 3 5 9 3 5 featur engin credit card balanc
3285,"### <a id=""9.3.6"">9.3.6 Prepare Final Train and Test data</a>",id 9 3 6 9 3 6 prepar final train test data
3286,"### <a id=""9.4"">9.4 Create Validation Sets</a>",id 9 4 9 4 creat valid set
3287,"### <a id=""9.5"">9.5 Fit the Model</a>",id 9 5 9 5 fit model
3288,"### <a id=""9.6"">9.6 Feature Importance </a>",id 9 6 9 6 featur import
3289,"### <a id=""9.7"">9.7 Predict</a>",id 9 7 9 7 predict
3290,Thanks for viewing. ,thank view
3291,"# Universal Blender
## Part 2. Potions 101.

Inspired by Henk van Veen 
https://www.kdnuggets.com/2015/06/ensembles-kaggle-data-science-competition-p3.html

In this part I go by the book and try all the blending techniques mentioned by Henk. This part shows that stacking and blending offers a lot of ways to be creative and overfit. :) I think I might have made a mistake when chosing holdout, so I have not achieved a higher score on test, despite the good results on cross validation. Try it on your own, maybe you will be luckier than me. If you spot a mistake in the code, I will much appreciate if you point it out in the comments.",univers blender part 2 potion 101 inspir henk van veen http www kdnugget com 2015 06 ensembl kaggl data scienc competit p3 html part go book tri blend techniqu mention henk part show stack blend offer lot way creativ overfit think might made mistak chose holdout achiev higher score test despit good result cross valid tri mayb luckier spot mistak code much appreci point comment
3292,"### Data load
Note we won't touch train set at all. Only holdout and test sets.",data load note touch train set holdout test set
3293,First check and drop any overcorrelated results:,first check drop overcorrel result
3294,Let's drop lightGBM:,let drop lightgbm
3295,"## Avergaing
Simply average all probabilities, in this case a single confident classifier may overrun several non-confident classifiers.",averga simpli averag probabl case singl confid classifi may overrun sever non confid classifi
3296,That has not gone well; pass.,gone well pas
3297,"## Weighted vote
Same as linear regression with constraint on weights >0.",weight vote linear regress constraint weight 0
3298,"Better than sole xgb, but not for the leader board.",better sole xgb leader board
3299,"## Ranked logistic regression
Here we do two things: 
1. substitute rank for probability in every column
2. train simple logistic regression based on the ranks",rank logist regress two thing 1 substitut rank probabl everi column 2 train simpl logist regress base rank
3300,"**Once again we'll snoop the test, to get a better score on LB.** Clean solution would rank holdout set, train on it. Then match values from test set to ranks in holdout and use them to predict target. ",snoop test get better score lb clean solut would rank holdout set train match valu test set rank holdout use predict target
3301,Worse than xgb.,wors xgb
3302,"## Quadratic linear stacking
Same as above, but we add quadratic features of the ranked dataset:",quadrat linear stack add quadrat featur rank dataset
3303,Awful!,aw
3304,"## Logistic regression with ranks and PCA
1. Use PCA to reduce dimension of original holdout and test data
2. add the reduced dimensions to the ranked datesets,
3. train simple logistic regression based on the ranks + reduced dimensions.",logist regress rank pca 1 use pca reduc dimens origin holdout test data 2 add reduc dimens rank dateset 3 train simpl logist regress base rank reduc dimens
3305,**Lorgistic regression with linear stretch on pca + rank features:**,lorgist regress linear stretch pca rank featur
3306,"**That is somewhat promising, but can we do better?
**Actually scored 0.788**",somewhat promis better actual score 0 788
3307,"## Feature-Weighted Linear Stacking
https://arxiv.org/pdf/0911.0460.pdf

As far as I understand it is what it sounds like. Generate meta features for the dataset, then multiply them by predictions. And build linear regression on top of it.",featur weight linear stack http arxiv org pdf 0911 0460 pdf far understand sound like gener meta featur dataset multipli predict build linear regress top
3308,Nope.,nope
3309,## LightGBM with ranks and PCA,lightgbm rank pca
3310,"That is very promising, but actually it just overfits and scores 0.788. Ok, as we deal with overfit, lets try to train a bunch of classifiers and then train logistic regreesion on their outcome. As we can only use holdout set, we will have to find weights with CV.",promis actual overfit score 0 788 ok deal overfit let tri train bunch classifi train logist regrees outcom use holdout set find weight cv
3311,## Dragons,dragon
3312,"""Here be dragons. With 7 heads. Standing on top of 30 other dragons."" Henk van Veen ",dragon 7 head stand top 30 dragon henk van veen
3313,"Let’s say you want to do 2-fold stacking:

* Split the train set in 2 parts: train_a and train_b
* Fit a first-stage model on train_a and create predictions for train_b
* Fit the same model on train_b and create predictions for train_a
* Finally fit the model on the entire train set and create predictions for the test set.
Now train a second-stage stacker model on the probabilities from the first-stage model(s) (using CV).",let say want 2 fold stack split train set 2 part train train b fit first stage model train creat predict train b fit model train b creat predict train final fit model entir train set creat predict test set train second stage stacker model probabl first stage model use cv
3314,"**Yet again it scored 0.788.** That is dissapointing. Perhaps, there is a flaw in implementation. For instance, I finish by using lgbm only. Maybe, that is not the best choise and a bunch of weaker estimators have to be blended/stacked instead. I also suspect that the hold out is not representative of the test set. The fact that the score is stuck at 0.788 hints that there is a simple mistake somewhere. Anyway, I hope this will serve as starter guide for creative ensamblimg and blending. Have fun and good luck! I'll go and retrain the whole damn thing, yet again; with different holdout.

PS: One other thing, that I is worth trying is to train a blender on the validation sets in the models training loop. This will make the code more cumbersome, but allow to train models on the whole set without need for holdout.",yet score 0 788 dissapoint perhap flaw implement instanc finish use lgbm mayb best chois bunch weaker estim blend stack instead also suspect hold repres test set fact score stuck 0 788 hint simpl mistak somewher anyway hope serv starter guid creativ ensamblimg blend fun good luck go retrain whole damn thing yet differ holdout p one thing worth tri train blender valid set model train loop make code cumbersom allow train model whole set without need holdout
3315,"# Universal Blender: XGB+CatB+LGB
## Part 1. Staging.
This is an attempt to build a universal blender frame, that collects holdout, crossval and train predictions, so that more advanced stacking and blending techinques can be used. 

This realisation prepares stage to blend Ridge, DNN, XGB, CatBoost and LGBM, but you can add or drop any models you want. The code could have been more elegant with a estimator class, but it is not :). If you know how to make it better, please share.

It also includes Data Builder function with memory optimisation that decreases memory usage by 70%. The optimized pickle dump can be used instead of building the dataframe from scratch.
The kernel is submitted in debug mode. State debag = False in oof_regression_stacker to get real results. Chose number of folds carefully as XGBoost and CatBoost take forever to train.

**Note:** Dataloader applies MinMaxscaler on the dataset that includes train and test data, therefore a leak from test to train occures. It is beneficial for leaderboard score, but should be avoided in real projects. To get a real life example how it can mess up your results please watch Caltech lecture, Puzzle 4, 52:00 : https://www.youtube.com/watch?v=EZBUDG12Nr0&index=17&list=PLD63A284B7615313A",univers blender xgb catb lgb part 1 stage attempt build univers blender frame collect holdout crossval train predict advanc stack blend techinqu use realis prepar stage blend ridg dnn xgb catboost lgbm add drop model want code could eleg estim class know make better plea share also includ data builder function memori optimis decreas memori usag 70 optim pickl dump use instead build datafram scratch kernel submit debug mode state debag fals oof regress stacker get real result chose number fold care xgboost catboost take forev train note dataload appli minmaxscal dataset includ train test data therefor leak test train occur benefici leaderboard score avoid real project get real life exampl mess result plea watch caltech lectur puzzl 4 52 00 http www youtub com watch v ezbudg12nr0 index 17 list pld63a284b7615313a
3316,"## Blending:
### Estimators:
#### Ridge regression",blend estim ridg regress
3317,"#### Simple DNN
Please thank its author:
https://www.kaggle.com/tottenham/10-fold-simple-dnn-with-rank-gauss

works surprisingly fast.",simpl dnn plea thank author http www kaggl com tottenham 10 fold simpl dnn rank gauss work surprisingli fast
3318,"#### LightGBM
the best of the batch, fast and convenient to use.",lightgbm best batch fast conveni use
3319,"#### XGBoost 
has a nasty feature that it takes only DMatrix as arguments therefore predict method has to be wrapend into a function.",xgboost nasti featur take dmatrix argument therefor predict method wrapend function
3320,"#### Catboost
watch out, it has predict and predict_proba methods. predict_proba should be used; it returns 2d array, that has to be flattened.",catboost watch predict predict proba method predict proba use return 2d array flatten
3321,"##  Data part
### Data loader:",data part data loader
3322,"## Data Builder:
### Service functions:",data builder servic function
3323,"### Memory reducer:
Thanks **You Guillaume Martin** for the Awesome Memory Optimizer!
https://www.kaggle.com/gemartin/load-data-reduce-memory-usage",memori reduc thank guillaum martin awesom memori optim http www kaggl com gemartin load data reduc memori usag
3324,"### Data builder function:
From lighgbm-with-selected-features",data builder function lighgbm select featur
3325,## Collecting data:,collect data
3326,# Blender call:,blender call
3327,"# Interpreting a LightGBM model

This notebook explores the patterns identified by a straightforward LightGBM model. Many thanks to Olivier and the Good_fun_with_LigthGBM kernel where all the LightGBM training code is from. This notebook is just meant to extend that kernel and examine it using individualized feature importances. You will need to scroll a bit to get to the pretty pictures :)",interpret lightgbm model notebook explor pattern identifi straightforward lightgbm model mani thank olivi good fun ligthgbm kernel lightgbm train code notebook meant extend kernel examin use individu featur import need scroll bit get pretti pictur
3328,## Build the dataset,build dataset
3329,"## Train the LightGBM model

Here we just use a simple train/validation split.",train lightgbm model use simpl train valid split
3330,"# Explain the model

SHAP values are fair allocation of credit among the features and have theoretical garuntees about consistency from game theory (so you can trust them). There is a high speed algorithm to compute SHAP values for LightGBM (and XGBoost and CatBoost), so they are particularly helpful when interpreting predictions from gradient boosting tree models. While typical feature importances are for the whole dataset (and often [have consistency problems](https://towardsdatascience.com/interpretable-machine-learning-with-xgboost-9ec80d148d27) these values are computed for every single prediction, which opens up new ways to understand the model. For more details check out the [docs](https://github.com/slundberg/shap).**",explain model shap valu fair alloc credit among featur theoret garunte consist game theori trust high speed algorithm comput shap valu lightgbm xgboost catboost particularli help interpret predict gradient boost tree model typic featur import whole dataset often consist problem http towardsdatasci com interpret machin learn xgboost 9ec80d148d27 valu comput everi singl predict open new way understand model detail check doc http github com slundberg shap
3331,### Summarize the feature imporances with a bar chart,summar featur impor bar chart
3332,"### Summarize the feature importances with a density scatter plot

Just looking a single number hides a lot of information about the model. So instead we plot the SHAP values for each feature for every sample on the x-axis and then let them pil up when there is not space to show density. If we then color each dot by the value of the original feature we can see how a low feature value or high feature value effects the model output. For example EXT_SOURCE_2 is the most important feature (because they are sorted by mean abolute SHAP value) and the red colored dots (samples) have low SHAP values with the blue colored dots have high SHAP values. Since for a logistic regression model the SHAP values are in log odds, this color spread means low values of EXT_SOURCE_2 raise the log odds output of the model significantly.

Note that some features like SK_DPD_DEF are not important for most people, but have a very large impact for a subset of the people in the data set. This highlights how a globally important feature is not nessecarrily the most importance feature for each person.",summar featur import densiti scatter plot look singl number hide lot inform model instead plot shap valu featur everi sampl x axi let pil space show densiti color dot valu origin featur see low featur valu high featur valu effect model output exampl ext sourc 2 import featur sort mean abolut shap valu red color dot sampl low shap valu blue color dot high shap valu sinc logist regress model shap valu log odd color spread mean low valu ext sourc 2 rais log odd output model significantli note featur like sk dpd def import peopl larg impact subset peopl data set highlight global import featur nessecarrili import featur person
3333,"### Investigate the dependence of the model on each feature

The summary plot above gives a lot of information. But we can learn more by examining a single feature and how it impacts the model's predicition across all the samples. To do this we plot the SHAP value for a feature on the y-axis and the original value of the feature on the x-axis. Doing this for all samples for EXT_SOURCE_2 shows a fairly linear relationship that flattens near high values. Note that the impact of EXT_SOURCE_2 on the model output is different for different people that have the same value of EXT_SOURCE_2, as shown by the vertical spread of the dots at a single point on the x axis. This is because of interaction effects (and would disappear if the trees were all depth 1 in the LightGBM model). To highlight what interactions may be driving this vertical spread shap colors the dots by another feature that seems to explain some of the dispersion (see the docs for more details on getting the exact interaction effects). For EXT_SOURCE_2 we see gender seems to have some effect.",investig depend model featur summari plot give lot inform learn examin singl featur impact model predicit across sampl plot shap valu featur axi origin valu featur x axi sampl ext sourc 2 show fairli linear relationship flatten near high valu note impact ext sourc 2 model output differ differ peopl valu ext sourc 2 shown vertic spread dot singl point x axi interact effect would disappear tree depth 1 lightgbm model highlight interact may drive vertic spread shap color dot anoth featur seem explain dispers see doc detail get exact interact effect ext sourc 2 see gender seem effect
3334,"## Plot the SHAP dependence plots for the top 20 features

Here you can scroll through the top 20 features and see lots of interesting patterns and interactions. Like how having high EXT_SOURCE_1 is even more important if you also have a small DAYS_BIRTH magnitude (see the EXT_SOURCE_1 plot).",plot shap depend plot top 20 featur scroll top 20 featur see lot interest pattern interact like high ext sourc 1 even import also small day birth magnitud see ext sourc 1 plot
3335,"**Before you begin please upvote the original authors. Its all there effort not mine.**
**Links to original kernels-->**
1.[Lightgbm with simple features by jsaguiar](http://www.kaggle.com/jsaguiar/lightgbm-with-simple-features-0-785-lb)
2.[tidy_xgb -all tables by kxx](http://www.kaggle.com/kailex/tidy-xgb-all-tables-0-782/code)",begin plea upvot origin author effort mine link origin kernel 1 lightgbm simpl featur jsaguiar http www kaggl com jsaguiar lightgbm simpl featur 0 785 lb 2 tidi xgb tabl kxx http www kaggl com kailex tidi xgb tabl 0 782 code
3336,"- <a href='#1'>Prepare</a>  
- <a href='#2'>Feature Selection</a>
    - <a href='#2-1'>1. Filter</a>
        - <a href='#2-1-1'>1.1 Pearson Correlation</a>
        - <a href='#2-1-2'>1.2 Chi-2</a>
    - <a href='#2-2'>2. Wrapper</a>
    - <a href='#2-3'>3. Embeded</a>
        - <a href='#2-3-1'>3.1 Logistics Regression L1</a>
        - <a href='#2-3-2'>3.2 Random Forest</a>
        - <a href='#2-3-3'>3.3 LightGBM</a>
- <a href='#3'>Summary</a>",href 1 prepar href 2 featur select href 2 1 1 filter href 2 1 1 1 1 pearson correl href 2 1 2 1 2 chi 2 href 2 2 2 wrapper href 2 3 3 embed href 2 3 1 3 1 logist regress l1 href 2 3 2 3 2 random forest href 2 3 3 3 3 lightgbm href 3 summari
3337,# <a id='1'>Prepare</a>,id 1 prepar
3338,### Stratified Sampling (ratio = 0.1),stratifi sampl ratio 0 1
3339,### Impute missing values,imput miss valu
3340,### Deal with Categorical features: OneHotEncoding,deal categor featur onehotencod
3341,### Feature matrix and target,featur matrix target
3342,"# <a id='2'>Feature Selection</a>
- select **100** features from 226
- **xxx_support**: list to represent select this feature or not
- **xxx_feature**: the name of selected features",id 2 featur select select 100 featur 226 xxx support list repres select featur xxx featur name select featur
3343,"## <a id='2-1'>1 Filter</a>
- documentation for **SelectKBest**: http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html

###  <a id='2-1-1'>1.1 Pearson Correlation</a>
**Note**
- Normalization: no
- Impute missing values: yes",id 2 1 1 filter document selectkbest http scikit learn org stabl modul gener sklearn featur select selectkbest html id 2 1 1 1 1 pearson correl note normal imput miss valu ye
3344,"###  <a id='2-1-2'>1.2 Chi-2</a>

**Note**
- Normalization: MinMaxScaler (values should be bigger than 0)
- Impute missing values: yes",id 2 1 2 1 2 chi 2 note normal minmaxscal valu bigger 0 imput miss valu ye
3345,"## <a id='2-2'>2. Wrapper</a>
- documentation for **RFE**: http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html

**Note**
- Normalization: depend on the used model; yes for LR
- Impute missing values: depend on the used model; yes for LR
",id 2 2 2 wrapper document rfe http scikit learn org stabl modul gener sklearn featur select rfe html note normal depend use model ye lr imput miss valu depend use model ye lr
3346,"## <a id='2-3'>3. Embeded</a>
- documentation for **SelectFromModel**: http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel.html
###  <a id='2-3-1'>3.1 Logistics Regression L1</a>
**Note**
- Normalization: Yes
- Impute missing values: Yes",id 2 3 3 embed document selectfrommodel http scikit learn org stabl modul gener sklearn featur select selectfrommodel html id 2 3 1 3 1 logist regress l1 note normal ye imput miss valu ye
3347,"###  <a id='2-3-2'>3.2 Random Forest</a>
**Note**
- Normalization: No
- Impute missing values: Yes",id 2 3 2 3 2 random forest note normal imput miss valu ye
3348,"###  <a id='2-3-3'>3.3 LightGBM</a>
**Note**
- Normalization: No
- Impute missing values: No",id 2 3 3 3 3 lightgbm note normal imput miss valu
3349,# <a id='3'>Summary</a>,id 3 summari
3350,"- <a href='#0'>0. Introduction</a>  
- <a href='#1'>1. Get the Data</a>
- <a href='#2'>2. Check the Data</a>
- <a href='#3'> 3. Explore the data</a>
    - <a href='#3-1'>3.1 Categorical features</a>
    - <a href='#3-2'>3.2 Numerical features</a>
    - <a href='#3-3'>3.3 Categorical features by label</a>
    - <a href='#3-4'>3.4 Numerical features by label</a>
    - <a href='#3-5'>3.5 Correlation Matrix</a>
- <a href='#4'> 4. A further exploration on application table</a>
    - <a href='#4-1'>4.1 Impute missing values</a>
    - <a href='#4-2'>4.2 Create more feature</a>
    - <a href='#4-3'>4.3 Train model</a>
    - <a href='#4-4'>4.4 Feature importance</a>
    - <a href='#4-5'>4.5 Prediction</a>",href 0 0 introduct href 1 1 get data href 2 2 check data href 3 3 explor data href 3 1 3 1 categor featur href 3 2 3 2 numer featur href 3 3 3 3 categor featur label href 3 4 3 4 numer featur label href 3 5 3 5 correl matrix href 4 4 explor applic tabl href 4 1 4 1 imput miss valu href 4 2 4 2 creat featur href 4 3 4 3 train model href 4 4 4 4 featur import href 4 5 4 5 predict
3351,## <a id='0'>0. Introduction</a>,id 0 0 introduct
3352," [Home Credit](http://www.homecredit.net/[](http://) is an international non-bank financial institution founded in 1997 in the Czech Republic. The company operates in 14 countries and focuses on lending primarily to people with little or no credit history. 

Home Credit strives to broaden financial inclusion for the unbanked population by providing a positive and safe borrowing experience. In order to make sure this underserved population has a positive loan experience, Home Credit makes use of a variety of alternative data--including telco and transactional information--**to predict their clients' repayment abilities.**

While Home Credit is currently using various statistical and machine learning methods to make these predictions, **they're challenging Kagglers to help them unlock the full potential of their data**. Doing so will ensure that clients capable of repayment are not rejected and that loans are given with a principal, maturity, and repayment calendar that will empower their clients to be successful.
![](http://www.homecredit.net/~/media/Images/H/Home-Credit-Group/image-gallery/full/image-gallery-01-11-2016-b.png)",home credit http www homecredit net http intern non bank financi institut found 1997 czech republ compani oper 14 countri focus lend primarili peopl littl credit histori home credit strive broaden financi inclus unbank popul provid posit safe borrow experi order make sure underserv popul posit loan experi home credit make use varieti altern data includ telco transact inform predict client repay abil home credit current use variou statist machin learn method make predict challeng kaggler help unlock full potenti data ensur client capabl repay reject loan given princip matur repay calendar empow client success http www homecredit net medium imag h home credit group imag galleri full imag galleri 01 11 2016 b png
3353,> ## <a id='1'>1. Get the data</a>,id 1 1 get data
3354,"This file contains descriptions for the columns in the various data files.

<img src=""https://storage.googleapis.com/kaggle-media/competitions/home-credit/home_credit.png"" width=""800""></img>",file contain descript column variou data file img src http storag googleapi com kaggl medium competit home credit home credit png width 800 img
3355,"## <a id='2'>2. Check the data</a>
### 2.1 application train / test",id 2 2 check data 2 1 applic train test
3356,### 2.2 POS_CASH_balance,2 2 po cash balanc
3357,### 2.3 bureau,2 3 bureau
3358,### 2.4 bureau_balance,2 4 bureau balanc
3359,### 2.5 credit_card_balance,2 5 credit card balanc
3360,### 2.6 previous_application,2 6 previou applic
3361,### 2.7 installments_payments,2 7 instal payment
3362,"## <a id='3'>3. Explore the data</a>

### <a id='3-1'>3.1 Categorical features</a>
#### Label",id 3 3 explor data id 3 1 3 1 categor featur label
3363,### Occupation Type,occup type
3364,#### Gender,gender
3365,### Income Type,incom type
3366,### House Type,hous type
3367,"### <a id='3-2'>3.2 Numerical features</a>
#### Credit Amount",id 3 2 3 2 numer featur credit amount
3368,#### Annuity Amount,annuiti amount
3369,### Days employed,day employ
3370,"### <a id='3-3'>3.3 Categorical features by label</a>
#### Gender",id 3 3 3 3 categor featur label gender
3371,#### Education Type,educ type
3372,"### <a id='3-4'>3.4 Numerical features by label</a>
#### EXT_SOURCE_1",id 3 4 3 4 numer featur label ext sourc 1
3373,#### EXT_SOURCE_2,ext sourc 2
3374,#### EXT_SOURCE_3,ext sourc 3
3375, ### <a id='3-5'>3.5 Correlation Matrix</a>,id 3 5 3 5 correl matrix
3376," ## <a id='4'>4 A further exploration on application table</a>
 ### <a id='4-1'>4.1 Impute missing values</a>",id 4 4 explor applic tabl id 4 1 4 1 imput miss valu
3377,"### split categorical, discrete and numerical features",split categor discret numer featur
3378,### convert categorical using LabelEncoder,convert categor use labelencod
3379,"### impute missing values
- for categorical and discrete features: use **'mode'** in SimpleImputer
- for continuous features: use [MICEImputer](http://scikit-learn.org/dev/modules/generated/sklearn.impute.MICEImputer.html) with **median** as initial strategy ",imput miss valu categor discret featur use mode simpleimput continu featur use miceimput http scikit learn org dev modul gener sklearn imput miceimput html median initi strategi
3380, ### <a id='4-2'>4.2 Create more features</a>,id 4 2 4 2 creat featur
3381,### Term: total credit / annuity,term total credit annuiti
3382,### OVER_EXPECT_CREDIT: actual credit larger than goods price,expect credit actual credit larger good price
3383,### MEAN_BUILDING_SCORE_TOTAL: the sum of all building AVG score,mean build score total sum build avg score
3384,### FLAG_DOCUMENT_TOTAL: the total number of provided document,flag document total total number provid document
3385,### AMT_REQ_CREDIT_BUREAU_TOTAL: the total number of enquiries,amt req credit bureau total total number enquiri
3386,### BIRTH_EMPLOTED_INTERVEL: the days between born and employed,birth emplot intervel day born employ
3387,"### <a id='4-3'>4.3 Train model</a>

**application**: 'binary' for binary classification

**num_iterations**: number of boosting iterations/trees, **n_estimators** in sklearn

**learning_rate**

**num_leaves**: number of leaves in one tree

**feature_fraction**: part of features used for each iteration

**bagging_fraction**: part of data used for each iteration

**lambda_l1/lambda_l2**: L1/L2 regularization

**min_split_gain**: the minimun gain to perform a split

**early_stopping_round**: if the validation metric can't improve for n rounds, stop iteration

**categorical_feature**: LightGBM API can deal with categorical feature automatically, **but we need transform string into integer**",id 4 3 4 3 train model applic binari binari classif num iter number boost iter tree n estim sklearn learn rate num leav number leav one tree featur fraction part featur use iter bag fraction part data use iter lambda l1 lambda l2 l1 l2 regular min split gain minimun gain perform split earli stop round valid metric improv n round stop iter categor featur lightgbm api deal categor featur automat need transform string integ
3388,### <a id='4-4'>4.4 Feature importance</a>,id 4 4 4 4 featur import
3389,### <a id='4-5'>4.5 Prediction</a>,id 4 5 4 5 predict
3391,"### Step 1: parameters to be tuned
**Note**: values for parameters should make sense, e.g.: 'num_leaves' needs to be a integer and 'feature_fraction' should between 0 and 1",step 1 paramet tune note valu paramet make sen e g num leav need integ featur fraction 0 1
3392,"### Step 2: Set the range for each parameter
**Gentle reminder**: try to make the range as narrow as possible",step 2 set rang paramet gentl remind tri make rang narrow possibl
3393,### Step 3: Bayesian Optimization: Maximize,step 3 bayesian optim maxim
3394,### Step 4: Get the parameters,step 4 get paramet
3395,"### Put all together
**Note**: It is just a demo. To get a better result, you should increase initial rounds, optimization rounds and n_estimators",put togeth note demo get better result increas initi round optim round n estim
3396,"**Introduction**
Real world datasets commonly show the particularity to have a number of samples of a given class under-represented compared to other classes. This imbalance gives rise to the “class imbalance” problem (or “curse of imbalanced datasets”) which is the problem of learning a concept from the class that has a small number of samples.
The class imbalance problem has been encountered in multiple areas such as telecommu- nication managements, bioinformatics, fraud detection, and medical diagnosis, and has been considered one of the top 10 problems in data mining and pattern recognition. Imbalanced data substantially compromises the learning process, since most of the standard machine learning algorithms expect balanced class dis- tribution or an equal misclassification cost. Related Inferential Statistics post showing bootstrap permutation ECDF plots: https://www.kaggle.com/tini9911/data-wrangling-eda-inferential-statistics-ml-model",introduct real world dataset commonli show particular number sampl given class repres compar class imbal give rise class imbal problem cur imbalanc dataset problem learn concept class small number sampl class imbal problem encount multipl area telecommu nicat manag bioinformat fraud detect medic diagnosi consid one top 10 problem data mine pattern recognit imbalanc data substanti compromis learn process sinc standard machin learn algorithm expect balanc class di tribut equal misclassif cost relat inferenti statist post show bootstrap permut ecdf plot http www kaggl com tini9911 data wrangl eda inferenti statist ml model
3397,"Here I will work on some techniques to handle highly unbalanced datasets, with a focus on resampling. The Home Credit Risk Prediction competition, is a classic problem of unbalanced classes, since Credit Loan in risk can be considered unusual cases when considering all clients. Other classic examples of unbalanced classes are the detection of financial fraud and attacks on computer networks.
Let's see how unbalanced the dataset is:",work techniqu handl highli unbalanc dataset focu resampl home credit risk predict competit classic problem unbalanc class sinc credit loan risk consid unusu case consid client classic exampl unbalanc class detect financi fraud attack comput network let see unbalanc dataset
3398,<h1> Resampling,h1 resampl
3399,"A widely adopted technique for dealing with highly unbalanced datasets is called resampling. It consists of removing samples from the majority class (under-sampling) and / or adding more examples from the minority class (over-sampling).
Despite the advantage of balancing classes, these techniques also have their weaknesses (there is no free lunch). The simplest implementation of over-sampling is to duplicate random records from the minority class, which can cause overfitting. In under-sampling, the simplest technique involves removing random records from the majority class, which can cause loss of information.
Let's implement a basic example, which uses the DataFrame.sample method to get random samples each class:",wide adopt techniqu deal highli unbalanc dataset call resampl consist remov sampl major class sampl ad exampl minor class sampl despit advantag balanc class techniqu also weak free lunch simplest implement sampl duplic random record minor class caus overfit sampl simplest techniqu involv remov random record major class caus loss inform let implement basic exampl use datafram sampl method get random sampl class
3400,**Random under-sampling**,random sampl
3401,**Random over-sampling**,random sampl
3402,**Python imbalanced-learn module **,python imbalanc learn modul
3403,"A number of more sophisticated resapling techniques have been proposed in the scientific literature.
For example, we can cluster the records of the majority class, and do the under-sampling by removing records from each cluster, thus seeking to preserve information. In over-sampling, instead of creating exact copies of the minority class records, we can introduce small variations into those copies, creating more diverse synthetic samples.
Let's apply some of these resampling techniques, using the Python library imbalanced-learn. It is compatible with scikit-learn and is part of scikit-learn-contrib projects.",number sophist resapl techniqu propos scientif literatur exampl cluster record major class sampl remov record cluster thu seek preserv inform sampl instead creat exact copi minor class record introduc small variat copi creat diver synthet sampl let appli resampl techniqu use python librari imbalanc learn compat scikit learn part scikit learn contrib project
3404,"For ease of visualization, let's create a small unbalanced sample dataset using the make_classification method:",ea visual let creat small unbalanc sampl dataset use make classif method
3405,"We will also create a 2-dimensional plot function, plot_2d_space, to see the data distribution:",also creat 2 dimension plot function plot 2d space see data distribut
3406,"Because the dataset has many dimensions (features) and our graphs will be 2D, we will reduce the size of the dataset using Principal Component Analysis (PCA):",dataset mani dimens featur graph 2d reduc size dataset use princip compon analysi pca
3407,**Random under-sampling and over-sampling with imbalanced-learn **,random sampl sampl imbalanc learn
3408,**Under-sampling: Tomek links **¶,sampl tomek link
3409,"Tomek links are pairs of very close instances, but of opposite classes. Removing the instances of the majority class of each pair increases the space between the two classes, facilitating the classification process.
In the code below, we'll use ratio='majority' to resample the majority class.",tomek link pair close instanc opposit class remov instanc major class pair increas space two class facilit classif process code use ratio major resampl major class
3410,**Under-sampling: Cluster Centroids** ,sampl cluster centroid
3411,"This technique performs under-sampling by generating centroids based on clustering methods. The data will be previously grouped by similarity, in order to preserve information.
In this example we will pass the {0: 10} dict for the parameter ratio, to preserve 10 elements from the majority class (0), and all minority class (1) .",techniqu perform sampl gener centroid base cluster method data previous group similar order preserv inform exampl pas 0 10 dict paramet ratio preserv 10 element major class 0 minor class 1
3412,**Over-sampling: SMOTE **,sampl smote
3413,"SMOTE (Synthetic Minority Oversampling TEchnique) consists of synthesizing elements for the minority class, based on those that already exist. It works randomly picingk a point from the minority class and computing the k-nearest neighbors for this point. The synthetic points are added between the chosen point and its neighbors.
We'll use ratio='minority' to resample the minority class.",smote synthet minor oversampl techniqu consist synthes element minor class base alreadi exist work randomli picingk point minor class comput k nearest neighbor point synthet point ad chosen point neighbor use ratio minor resampl minor class
3414,**Over-sampling followed by under-sampling **,sampl follow sampl
3415,"Now, we will do a combination of over-sampling and under-sampling, using the SMOTE and Tomek links techniques:",combin sampl sampl use smote tomek link techniqu
3416,<h1>Deploying Machine Learning Model over Resampled Dataset ,h1 deploy machin learn model resampl dataset
3417,<h1> Random Forest Classifier ,h1 random forest classifi
3418," ##  **Introducation**

Home Credit strives to broaden financial inclusion for the unbanked population by providing a positive and safe borrowing experience. In order to make sure this underserved population has a positive loan experience, Home Credit makes use of a variety of alternative data--including telco and transactional information--to predict their clients' repayment abilities.

While Home Credit is currently using various statistical and machine learning methods to make these predictions, they're challenging Kagglers to help them unlock the full potential of their data. Doing so will ensure that clients capable of repayment are not rejected and that loans are given with a principal, maturity, and repayment calendar that will empower their clients to be successful.
",introduc home credit strive broaden financi inclus unbank popul provid posit safe borrow experi order make sure underserv popul posit loan experi home credit make use varieti altern data includ telco transact inform predict client repay abil home credit current use variou statist machin learn method make predict challeng kaggler help unlock full potenti data ensur client capabl repay reject loan given princip matur repay calendar empow client success
3419, ## Importing The Dataset,import dataset
3420,## Data Description,data descript
3421,"appliction_train & application_test

This is the main table, broken into two files for Train (with TARGET) and Test (without TARGET).
Static data for all applications. One row represents one loan in our data sample.

bureau

All client's previous credits provided by other financial institutions that were reported to Credit Bureau (for clients who have a loan in our sample).
For every loan in our sample, there are as many rows as number of credits the client had in Credit Bureau before the application date.

bureau_balance

Monthly balances of previous credits in Credit Bureau.
This table has one row for each month of history of every previous credit reported to Credit Bureau – i.e the table has (#loans in sample * # of relative previous credits * # of months where we have some history observable for the previous credits) rows.

POS_CASH_balance

Monthly balance snapshots of previous POS (point of sales) and cash loans that the applicant had with Home Credit.
This table has one row for each month of history of every previous credit in Home Credit (consumer credit and cash loans) related to loans in our sample – i.e. the table has (#loans in sample * # of relative previous credits * # of months in which we have some history observable for the previous credits) rows.

credit_card_balance

Monthly balance snapshots of previous credit cards that the applicant has with Home Credit.
This table has one row for each month of history of every previous credit in Home Credit (consumer credit and cash loans) related to loans in our sample – i.e. the table has (#loans in sample * # of relative previous credit cards * # of months where we have some history observable for the previous credit card) rows.

previous_application

All previous applications for Home Credit loans of clients who have loans in our sample.
There is one row for each previous application related to loans in our data sample.

installments_payments

Repayment history for the previously disbursed credits in Home Credit related to the loans in our sample.
There is a) one row for every payment that was made plus b) one row each for missed payment.
One row is equivalent to one payment of one installment OR one installment corresponding to one payment of one previous Home Credit credit related to loans in our sample.

",applict train applic test main tabl broken two file train target test without target static data applic one row repres one loan data sampl bureau client previou credit provid financi institut report credit bureau client loan sampl everi loan sampl mani row number credit client credit bureau applic date bureau balanc monthli balanc previou credit credit bureau tabl one row month histori everi previou credit report credit bureau e tabl loan sampl rel previou credit month histori observ previou credit row po cash balanc monthli balanc snapshot previou po point sale cash loan applic home credit tabl one row month histori everi previou credit home credit consum credit cash loan relat loan sampl e tabl loan sampl rel previou credit month histori observ previou credit row credit card balanc monthli balanc snapshot previou credit card applic home credit tabl one row month histori everi previou credit home credit consum credit cash loan relat loan sampl e tabl loan sampl rel previou credit card month histori observ previou credit card row previou applic previou applic home credit loan client loan sampl one row previou applic relat loan data sampl instal payment repay histori previous disburs credit home credit relat loan sampl one row everi payment made plu b one row miss payment one row equival one payment one instal one instal correspond one payment one previou home credit credit relat loan sampl
3422,![image.png](attachment:image.png),imag png attach imag png
3423,"2. ### Number of records and Features in the datasets
2.1. ** Let Examine the Applaication Train DataSet**",2 number record featur dataset 2 1 let examin applaic train dataset
3424,### Identifying Numerical and Categorical Features,identifi numer categor featur
3425,###  Function for  find out Numerical and categeical Variables,function find numer categ variabl
3426,### Identifying Missing Value Present in Application Train Dataset,identifi miss valu present applic train dataset
3427,### The above given figure which clear say about the which features has missing value and precentage of missing value avaiable in application_train DataSet,given figur clear say featur miss valu precentag miss valu avaiabl applic train dataset
3428,### Let us Examine application_test data set,let u examin applic test data set
3429,### identifying the Catergical and numberical variables ,identifi caterg number variabl
3430,### identifying the missing values,identifi miss valu
3431,### Let Exmaine bureau dataset,let exmain bureau dataset
3432,### identifying the catergical and numnerical features  ,identifi caterg numner featur
3433,### identying the missing data,identi miss data
3434,### Exmaine the bureau_balance DataSet,exmain bureau balanc dataset
3435,## identifying Catergical and numerical variables,identifi caterg numer variabl
3436,### identifying the missing value in bureau_balance,identifi miss valu bureau balanc
3437,### No missing data in Bureau Balance Dataset,miss data bureau balanc dataset
3438,## Exmaine the POS_CASH_balance DataSet,exmain po cash balanc dataset
3439,### identifying the Catergical and numerical variables ,identifi caterg numer variabl
3440,### identifying the missing values,identifi miss valu
3441,### Examine the credit_card_balance dataset,examin credit card balanc dataset
3442,### identifying the Categerical and numerical Variable ,identifi categer numer variabl
3443,### identifying the missing value in credit_card_balance dataset,identifi miss valu credit card balanc dataset
3444,### Exmaine the previous_application Dataset,exmain previou applic dataset
3445, ### identifying the catergical and numerical variable in previous application Data set.,identifi caterg numer variabl previou applic data set
3446,### identifying the missing value in previous_application,identifi miss valu previou applic
3447,### Exmaine the installments_payments dataset,exmain instal payment dataset
3448,### identifying the categerical and numerical Variable ,identifi categer numer variabl
3449,### identifying the missing value in installments_payments,identifi miss valu instal payment
3450,## Checking the Imbalance of Target Variable,check imbal target variabl
3451,"It is evident that  many customer are able to pay the loan back i.e Only 91.9% of the total customer are repaying the loan.
We need to drill down more to get better insights from the data and see which categories of the customer are not able to pay back loan.

We will try to check the repayer and defualter rate by using the different features of the dataset. 
Some of the features being Gender,Education,Employment_type,etc. First let us understand the different types of features.",evid mani custom abl pay loan back e 91 9 total custom repay loan need drill get better insight data see categori custom abl pay back loan tri check repay defualt rate use differ featur dataset featur gender educ employ type etc first let u understand differ type featur
3452,"## Types Of Features

### Categorical Features:
A categorical variable is one that has two or more categories and each value in that feature can be categorised by them.For example, 
Name Education Type is a categorical variable having Five categories. Now we cannot sort or give any ordering to such variables. They are also known as Nominal Variables.

Categorical Features in the dataset: Education_type,occupation_type,Contract_type

### Analysing The Features
***Eduaction-----> Categorical Feature***",type featur categor featur categor variabl one two categori valu featur categoris exampl name educ type categor variabl five categori sort give order variabl also known nomin variabl categor featur dataset educ type occup type contract type analys featur eduact categor featur
3453,### the proof is  edvined by looking at the above given plot and groupby function is clearly customer with education of secondary/secondary special has high  count where not able to pay loan back.,proof edvin look given plot groupbi function clearli custom educ secondari secondari special high count abl pay loan back
3454,## Analysis based on Code  Gender ,analysi base code gender
3455,### its clear that  by looking at the above given plot and groupby function is clearly customer based on code gender type female  has high  count where not able to pay loan back compare to male.,clear look given plot groupbi function clearli custom base code gender type femal high count abl pay loan back compar male
3456,### Analysis based on INCOME TYPE,analysi base incom type
3457,### its clear that  by looking at the above given plot and groupby function is clearly customer based on code income type and the working  has high  count where not able to pay loan back compare to all other,clear look given plot groupbi function clearli custom base code incom type work high count abl pay loan back compar
3458,### Analysis based on OCCUPATION TYPE,analysi base occup type
3459,###  its clear that  by looking at the above given plot and groupby function is clearly customer based on occupation  type. the working  has high  count where not able to pay loan back compare to all other.,clear look given plot groupbi function clearli custom base occup type work high count abl pay loan back compar
3460,### Analysis Based on FAMILY STATUS,analysi base famili statu
3461,### its clear that  by looking at the above given plot and groupby function is clearly customer based on code Family type and the Married customer  has high  count where not able to pay loan back compare to all other.,clear look given plot groupbi function clearli custom base code famili type marri custom high count abl pay loan back compar
3462,"
### Analysis based HOUSING TYPE",analysi base hous type
3463,### its clear that  by looking at the above given plot and groupby function is clearly customer based on Housing type and the house type customer  has high  count where not able to pay loan back compare to all other.,clear look given plot groupbi function clearli custom base hous type hous type custom high count abl pay loan back compar
3464,### Analysis based on TYPE SUITE,analysi base type suit
3465,### its clear that  by looking at the above given plot and groupby function is clearly customer based on Suite type and the unaccompanied customer  has high  count where not able to pay loan back compare to all other.,clear look given plot groupbi function clearli custom base suit type unaccompani custom high count abl pay loan back compar
3466,### Analysis Based on ORGANIZATION TYPE,analysi base organ type
3467,### its clear that  by looking at the above given plot and groupby function is clearly customer based on Organization type and the Business type 3 customer  has high  count where not able to pay loan back compare to all other.,clear look given plot groupbi function clearli custom base organ type busi type 3 custom high count abl pay loan back compar
3468,### Analysis based on FLAG OWN CAR,analysi base flag car
3469,### its clear that  by looking at the above given plot and groupby function is clearly customer based on owning car type and the customer with no car  has high  count where not able to pay loan back compare to all other.,clear look given plot groupbi function clearli custom base own car type custom car high count abl pay loan back compar
3470,### Analysis based on FLAG_OWN_REALTY,analysi base flag realti
3471,### its clear that  by looking at the above given plot and groupby function is clearly customer based on owning Reality type and the customer has Reality  has high  count where not able to pay loan back compare to all other.,clear look given plot groupbi function clearli custom base own realiti type custom realiti high count abl pay loan back compar
3472,### Analysis based on NAME_CONTRACT_TYPE,analysi base name contract type
3473,### its clear that  by looking at the above given plot and groupby function is clearly customer based on Contract type and the customer with cash loans  has high  count where not able to pay loan back compare to all other.,clear look given plot groupbi function clearli custom base contract type custom cash loan high count abl pay loan back compar
3474,### Analysis based on WEEKDAY_APPR_PROCESS_START,analysi base weekday appr process start
3475,"### its clear that  by looking at the above given plot and groupby function is clearly customer based application start on days and the customer registed  on tuesday  has high  count were not able to pay loan back compare to all other.

### Analysis based House Type Mode",clear look given plot groupbi function clearli custom base applic start day custom regist tuesday high count abl pay loan back compar analysi base hous type mode
3476,### its clear that  by looking at the above given plot and groupby function is clearly customer based Housetype Mode and the customer registed  on Block of flats has high  count where not able to pay loan back compare to all other.,clear look given plot groupbi function clearli custom base housetyp mode custom regist block flat high count abl pay loan back compar
3477,### Analysis Based on EMERGENCYSTATE MODE ,analysi base emergencyst mode
3478,### its clear that  by looking at the above given plot and groupby function is clearly customer based Emergency state Mode and the customer has No  has high  count where not able to pay loan back compare to all other.,clear look given plot groupbi function clearli custom base emerg state mode custom high count abl pay loan back compar
3479,"### ANALYSIS THE NUMBERICAL FEATURES 


#### ANALYZSIS BASED ON  COUNT CHILDREN ",analysi number featur analyzsi base count child
3480,its clear that  by looking at the above given plots and groupby function is clearly customer based count of children and the customer with   has No  children has high  count and customer with more than  8   50% of non payer based on it own count where not able to pay loan back compare to all other.,clear look given plot groupbi function clearli custom base count child custom child high count custom 8 50 non payer base count abl pay loan back compar
3481,### if we see  the above given distrbution plot it clear say major distrbuiton  for amouth annuity is from 0 to 75000 and amount of anual income is from 0 to 1000000,see given distrbut plot clear say major distrbuiton amouth annuiti 0 75000 amount anual incom 0 1000000
3482,"#### Analysis based on   REGION_RATING_CLIENT , REGION_RATING_CLIENT_W_CITY', HOUR_APPR_PROCESS_STAR',
  REG_REGION_NOT_LIVE_REGION', 'REG_REGION_NOT_WORK_REGION',
  LIVE_REGION_NOT_WORK_REGION', 'REG_CITY_NOT_LIVE_CITY
 REG_CITY_NOT_WORK_CITY', 'LIVE_CITY_NOT_WORK_CITY'",analysi base region rate client region rate client w citi hour appr process star reg region live region reg region work region live region work region reg citi live citi reg citi work citi live citi work citi
3483,if we look at the above plot it is clear that customer register in city but not work in city based analysis we know that customer not city has high count.,look plot clear custom regist citi work citi base analysi know custom citi high count
3484,### By looking at the above given we can clear get id of which loan Repayer vs NOn Payer  all features list in above plot,look given clear get id loan repay v non payer featur list plot
3485,1. 1. ### Analysis Based on EXter Source Types,1 1 analysi base exter sourc type
3486,### Analysis based Averages values,analysi base averag valu
3487,### Checking the  Correlation Between The Features for Application Train Dataset,check correl featur applic train dataset
3488,### If look at the above given plot is clear that all AVG featuers are high correleted values by seeing this plot we can easy find out the coorelated features,look given plot clear avg featuer high correlet valu see plot easi find coorel featur
3489,#### **By see above given two  corelation plot  we can easy find out  the most corelated featuers along with they corelated values.  and all kind of analysis are done for the application train dataset.,see given two corel plot easi find corel featuer along corel valu kind analysi done applic train dataset
3490,"## EDA of Bureau Data

All client's previous credits provided by other financial institutions that were reported to Credit Bureau (for clients who have a loan in our sample). For every loan in our sample, there are as many rows as number of credits the client had in Credit Bureau before the application date.",eda bureau data client previou credit provid financi institut report credit bureau client loan sampl everi loan sampl mani row number credit client credit bureau applic date
3491,### Merging the bureau dataset along with application train dataset to do more analysis,merg bureau dataset along applic train dataset analysi
3492,"### Analysis Based on CREDIT ACTIVE, CREDIT_CURRENCY, CREDIT TYPE",analysi base credit activ credit currenc credit type
3493,### BASED on above  given plot it clear that the Credit type consumer credit has high count of non payer of loan and credit currency customer with currency 1 has high count of non payer and based credit active customer with in group of closed customer has high count of non payer,base given plot clear credit type consum credit high count non payer loan credit currenc custom currenc 1 high count non payer base credit activ custom group close custom high count non payer
3494,### ABOVE given plots show distributions,given plot show distribut
3495,### EAD for bureau balance Dataset,ead bureau balanc dataset
3496,"### based  type its clear that status c, o,x has high count in bureau balnace dataset",base type clear statu c x high count bureau balnac dataset
3497,### EDA for previous application dataset,eda previou applic dataset
3498,### By looking at the above given plot its clear that which types has high count in each feature so we can easy out the root cause of the problem.,look given plot clear type high count featur easi root caus problem
3499,### analyzing the numerical features disturbion in previous application dataset,analyz numer featur disturbion previou applic dataset
3500,## those are few EAD i have done please  vote for me which help my movitvation to increase to do a lot of work if there any imporvment can be done means please say in comments References This notebook has been created based on great work done solving the House Credit defult Risk competition and other sources.,ead done plea vote help movitv increas lot work imporv done mean plea say comment refer notebook creat base great work done solv hous credit defult risk competit sourc
3501,"# Introduction: Automated Feature Engineering Basics

In this notebook, we will walk through applying automated feature engineering to the [Home Credit Default Risk dataset](https://www.kaggle.com/c/home-credit-default-risk) using the featuretools library. [Featuretools](https://docs.featuretools.com/) is an open-source Python package for automatically creating new features from multiple tables of structured, related data. It is ideal tool for problems such as the Home Credit Default Risk competition where there are several related tables that need to be combined into a single dataframe for training (and one for testing). 

## Feature Engineering

The objective of [feature engineering](https://en.wikipedia.org/wiki/Feature_engineering) is to create new features (alos called explantory variables or predictors) to represent as much information from an entire dataset in one table.  Typically, this process is done by hand using pandas operations such as `groupby`, `agg`, or `merge` and can be very tedious. Moreover, manual feature engineering is limited both by human time constraints and imagination: we simply cannot conceive of every possible feature that will be useful. (For an example of using manual feature engineering, check out [part one](https://www.kaggle.com/willkoehrsen/introduction-to-manual-feature-engineering) and [part two](https://www.kaggle.com/willkoehrsen/introduction-to-manual-feature-engineering-p2) applied to this competition). The importance of creating the proper features cannot be overstated because a machine learning model can only learn from the data we give to it. Extracting as much information as possible from the available datasets is crucial to creating an effective solution.

[Automated feature engineering](https://towardsdatascience.com/automated-feature-engineering-in-python-99baf11cc219) aims to help the data scientist with the problem of feature creation by automatically building hundreds or thousands of new features from a dataset. Featuretools - the only library for automated feature engineering at the moment - will not replace the data scientist, but it will allow her to focus on more valuable parts of the machine learning pipeline, such as delivering robust models into production. 

Here we will touch on the concepts of automated feature engineering with featuretools and show how to implement it for the Home Credit Default Risk competition. We will stick to the basics so we can get the ideas down and then build upon this foundation in later work when we customize featuretools. We will work with a subset of the data because this is a computationally intensive job that is outside the capabilities of the Kaggle kernels. I took the work done in this notebook and ran the methods on the entire dataset with the results [available here](https://www.kaggle.com/willkoehrsen/home-credit-default-risk-feature-tools). At the end of this notebook, we'll look at the features themselves, as well as the results of modeling with different combinations of hand designed and automatically built features. 

If you are new to this competition, I suggest checking out [this post to get started](https://www.kaggle.com/willkoehrsen/start-here-a-gentle-introduction). For a good take on why features are so important, here's a [blog post](https://www.featurelabs.com/blog/secret-to-data-science-success/) by one of the developers of Featuretools. ",introduct autom featur engin basic notebook walk appli autom featur engin home credit default risk dataset http www kaggl com c home credit default risk use featuretool librari featuretool http doc featuretool com open sourc python packag automat creat new featur multipl tabl structur relat data ideal tool problem home credit default risk competit sever relat tabl need combin singl datafram train one test featur engin object featur engin http en wikipedia org wiki featur engin creat new featur alo call explantori variabl predictor repres much inform entir dataset one tabl typic process done hand use panda oper groupbi agg merg tediou moreov manual featur engin limit human time constraint imagin simpli conceiv everi possibl featur use exampl use manual featur engin check part one http www kaggl com willkoehrsen introduct manual featur engin part two http www kaggl com willkoehrsen introduct manual featur engin p2 appli competit import creat proper featur overst machin learn model learn data give extract much inform possibl avail dataset crucial creat effect solut autom featur engin http towardsdatasci com autom featur engin python 99baf11cc219 aim help data scientist problem featur creation automat build hundr thousand new featur dataset featuretool librari autom featur engin moment replac data scientist allow focu valuabl part machin learn pipelin deliv robust model product touch concept autom featur engin featuretool show implement home credit default risk competit stick basic get idea build upon foundat later work custom featuretool work subset data comput intens job outsid capabl kaggl kernel took work done notebook ran method entir dataset result avail http www kaggl com willkoehrsen home credit default risk featur tool end notebook look featur well result model differ combin hand design automat built featur new competit suggest check post get start http www kaggl com willkoehrsen start gentl introduct good take featur import blog post http www featurelab com blog secret data scienc success one develop featuretool
3502,"# Problem

The Home Credit Default Risk competition is a supervised classification machine learning task. The objective is to use historical financial and socioeconomic data to predict whether or not an applicant will be able to repay a loan. This is a standard supervised classification task:

* __Supervised__: The labels are included in the training data and the goal is to train a model to learn to predict the labels from the features
* __Classification__: The label is a binary variable, 0 (will repay loan on time), 1 (will have difficulty repaying loan)

## Dataset

The data is provided by [Home Credit](http://www.homecredit.net/about-us.aspx), a service dedicated to provided lines of credit (loans) to the unbanked population. 

There are 7 different data files:

* __application_train/application_test__: the main training and testing data with information about each loan application at Home Credit. Every loan has its own row and is identified by the `SK_ID_CURR`. The training application data comes with the `TARGET` with indicating 0: the loan was repaid and 1: the loan was not repaid. 
* __bureau__: data concerning client's previous credits from other financial institutions. Each previous credit has its own row in bureau and is identified by the `SK_ID_BUREAU`, Each loan in the application data can have multiple previous credits.
* __bureau_balance__: monthly data about the previous credits in bureau. Each row is one month of a previous credit, and a single previous credit can have multiple rows, one for each month of the credit length. 
* __previous_application__: previous applications for loans at Home Credit of clients who have loans in the application data. Each current loan in the application data can have multiple previous loans. Each previous application has one row and is identified by the feature `SK_ID_PREV`. 
* __POS_CASH_BALANCE__: monthly data about previous point of sale or cash loans clients have had with Home Credit. Each row is one month of a previous point of sale or cash loan, and a single previous loan can have many rows.
* __credit_card_balance__: monthly data about previous credit cards clients have had with Home Credit. Each row is one month of a credit card balance, and a single credit card can have many rows.
* __installments_payment__: payment history for previous loans at Home Credit. There is one row for every made payment and one row for every missed payment. 

The diagram below (provided by Home Credit) shows how the tables are related. This will be very useful when we need to define relationships in featuretools. 

![image](https://storage.googleapis.com/kaggle-media/competitions/home-credit/home_credit.png)

### Read in Data and Create Small Datasets

We will read in the full dataset, sort by the `SK_ID_CURR` and keep only the first 1000 rows to make the calculations feasible. Later we can convert to a script and run with the entire datasets.",problem home credit default risk competit supervis classif machin learn task object use histor financi socioeconom data predict whether applic abl repay loan standard supervis classif task supervis label includ train data goal train model learn predict label featur classif label binari variabl 0 repay loan time 1 difficulti repay loan dataset data provid home credit http www homecredit net u aspx servic dedic provid line credit loan unbank popul 7 differ data file applic train applic test main train test data inform loan applic home credit everi loan row identifi sk id curr train applic data come target indic 0 loan repaid 1 loan repaid bureau data concern client previou credit financi institut previou credit row bureau identifi sk id bureau loan applic data multipl previou credit bureau balanc monthli data previou credit bureau row one month previou credit singl previou credit multipl row one month credit length previou applic previou applic loan home credit client loan applic data current loan applic data multipl previou loan previou applic one row identifi featur sk id prev po cash balanc monthli data previou point sale cash loan client home credit row one month previou point sale cash loan singl previou loan mani row credit card balanc monthli data previou credit card client home credit row one month credit card balanc singl credit card mani row instal payment payment histori previou loan home credit one row everi made payment one row everi miss payment diagram provid home credit show tabl relat use need defin relationship featuretool imag http storag googleapi com kaggl medium competit home credit home credit png read data creat small dataset read full dataset sort sk id curr keep first 1000 row make calcul feasibl later convert script run entir dataset
3503,"We'll join the train and test set together but add a separate column identifying the set. This is important because we are going to want to apply the same exact procedures to each dataset. It's safest to just join them together and treat them as a single dataframe. 

(I'm not sure if this is allowing data leakage into the train set and if these feature creation operations should be applied separately. Any thoughts would be much appreciated!)",join train test set togeth add separ column identifi set import go want appli exact procedur dataset safest join togeth treat singl datafram sure allow data leakag train set featur creation oper appli separ thought would much appreci
3504,"# Featuretools Basics

[Featuretools](https://docs.featuretools.com/#minute-quick-start) is an open-source Python library for automatically creating features out of a set of related tables using a technique called [deep feature synthesis](http://www.jmaxkanter.com/static/papers/DSAA_DSM_2015.pdf). Automated feature engineering, like many topics in machine learning, is a complex subject built upon a foundation of simpler ideas. By going through these ideas one at a time, we can build up our understanding of how featuretools which will later allow for us to get the most out of it.

There are a few concepts that we will cover along the way:

* [Entities and EntitySets](https://docs.featuretools.com/loading_data/using_entitysets.html)
* [Relationships between tables](https://docs.featuretools.com/loading_data/using_entitysets.html#adding-a-relationship)
* [Feature primitives](https://docs.featuretools.com/automated_feature_engineering/primitives.html): aggregations and transformations
* [Deep feature synthesis](https://docs.featuretools.com/automated_feature_engineering/afe.html)

# Entities and Entitysets

An entity is simply a table or in Pandas, a `dataframe`. The observations are in the rows and the features in the columns. An entity in featuretools must have a unique index where none of the elements are duplicated.  Currently, only `app`, `bureau`, and `previous` have unique indices (`SK_ID_CURR`, `SK_ID_BUREAU`, and `SK_ID_PREV` respectively). For the other dataframes, we must pass in `make_index = True` and then specify the name of the index. Entities can also have time indices where each entry is identified by a unique time. (There are not datetimes in any of the data, but there are relative times, given in months or days, that we could consider treating as time variables).

An [EntitySet](https://docs.featuretools.com/loading_data/using_entitysets.html) is a collection of tables and the relationships between them. This can be thought of a data structute with its own methods and attributes. Using an EntitySet allows us to group together multiple tables and manipulate them much quicker than individual tables. 

First we'll make an empty entityset named clients to keep track of all the data.",featuretool basic featuretool http doc featuretool com minut quick start open sourc python librari automat creat featur set relat tabl use techniqu call deep featur synthesi http www jmaxkant com static paper dsaa dsm 2015 pdf autom featur engin like mani topic machin learn complex subject built upon foundat simpler idea go idea one time build understand featuretool later allow u get concept cover along way entiti entityset http doc featuretool com load data use entityset html relationship tabl http doc featuretool com load data use entityset html ad relationship featur primit http doc featuretool com autom featur engin primit html aggreg transform deep featur synthesi http doc featuretool com autom featur engin afe html entiti entityset entiti simpli tabl panda datafram observ row featur column entiti featuretool must uniqu index none element duplic current app bureau previou uniqu indic sk id curr sk id bureau sk id prev respect datafram must pas make index true specifi name index entiti also time indic entri identifi uniqu time datetim data rel time given month day could consid treat time variabl entityset http doc featuretool com load data use entityset html collect tabl relationship thought data structut method attribut use entityset allow u group togeth multipl tabl manipul much quicker individu tabl first make empti entityset name client keep track data
3505,"Now we define each entity, or table of data. We need to pass in an index if the data has one or `make_index = True` if not. Featuretools will automatically infer the types of variables, but we can also change them if needed. For intstance, if we have a categorical variable that is represented as an integer we might want to let featuretools know the right type.",defin entiti tabl data need pas index data one make index true featuretool automat infer type variabl also chang need intstanc categor variabl repres integ might want let featuretool know right type
3506,"# Relationships

Relationships are a fundamental concept not only in featuretools, but in any relational database. The best way to think of a one-to-many relationship is with the analogy of parent-to-child. A parent is a single individual, but can have mutliple children. The children can then have multiple children of their own. In a _parent table_, each individual has a single row. Each individual in the parent table can have multiple rows in the _child table_. 

As an example, the `app` dataframe has one row for each client  (`SK_ID_CURR`) while the `bureau` dataframe has multiple previous loans (`SK_ID_PREV`) for each parent (`SK_ID_CURR`). Therefore, the `bureau` dataframe is the child of the `app` dataframe. The `bureau` dataframe in turn is the parent of `bureau_balance` because each loan has one row in `bureau` but multiple monthly records in `bureau_balance`. ",relationship relationship fundament concept featuretool relat databas best way think one mani relationship analog parent child parent singl individu mutlipl child child multipl child parent tabl individu singl row individu parent tabl multipl row child tabl exampl app datafram one row client sk id curr bureau datafram multipl previou loan sk id prev parent sk id curr therefor bureau datafram child app datafram bureau datafram turn parent bureau balanc loan one row bureau multipl monthli record bureau balanc
3507,"The `SK_ID_CURR` ""100002"" has one row in the parent table and multiple rows in the child. 

Two tables are linked via a shared variable. The `app` and `bureau` dataframe are linked by the `SK_ID_CURR` variable while the `bureau` and `bureau_balance` dataframes are linked with the `SK_ID_BUREAU`. Defining the relationships is relatively straightforward, and the diagram provided by the competition is helpful for seeing the relationships. For each relationship, we need to specify the parent variable and the child variable. Altogether, there are a total of 6 relationships between the tables. Below we specify all six relationships and then add them to the EntitySet.",sk id curr 100002 one row parent tabl multipl row child two tabl link via share variabl app bureau datafram link sk id curr variabl bureau bureau balanc datafram link sk id bureau defin relationship rel straightforward diagram provid competit help see relationship relationship need specifi parent variabl child variabl altogeth total 6 relationship tabl specifi six relationship add entityset
3508,"Slightly advanced note: we need to be careful to not create a [diamond graph](https://en.wikipedia.org/wiki/Diamond_graph) where there are multiple paths from a parent to a child. If we directly link `app` and `cash` via `SK_ID_CURR`; `previous` and `cash` via `SK_ID_PREV`; and `app` and `previous` via `SK_ID_CURR`, then we have created two paths from `app` to `cash`. This results in ambiguity, so the approach we have to take instead is to link `app` to `cash` through `previous`. We establish a relationship between `previous` (the parent) and `cash` (the child) using `SK_ID_PREV`. Then we establish a relationship between `app` (the parent) and `previous` (now the child) using `SK_ID_CURR`. Then featuretools will be able to create features on `app` derived from both `previous` and `cash` by stacking multiple primitives. ",slightli advanc note need care creat diamond graph http en wikipedia org wiki diamond graph multipl path parent child directli link app cash via sk id curr previou cash via sk id prev app previou via sk id curr creat two path app cash result ambigu approach take instead link app cash previou establish relationship previou parent cash child use sk id prev establish relationship app parent previou child use sk id curr featuretool abl creat featur app deriv previou cash stack multipl primit
3509,"All entities in the entity can be related to each other. In theory this allows us to calculate features for any of the entities, but in practice, we will only calculate features for the `app` dataframe since that will be used for training/testing. ",entiti entiti relat theori allow u calcul featur entiti practic calcul featur app datafram sinc use train test
3510,"# Feature Primitives

A [feature primitive](https://docs.featuretools.com/automated_feature_engineering/primitives.html) is an operation applied to a table or a set of tables to create a feature. These represent simple calculations, many of which we already use in manual feature engineering, that can be stacked on top of each other to create complex features. Feature primitives fall into two categories:

* __Aggregation__: function that groups together child datapoints for each parent and then calculates a statistic such as mean, min, max, or standard deviation. An example is calculating the maximum previous loan amount for each client. An aggregation works across multiple tables using relationships between tables.
* __Transformation__: an operation applied to one or more columns in a single table. An example would be taking the absolute value of a column, or finding the difference between two columns in one table.

A list of the available features primitives in featuretools can be viewed below.",featur primit featur primit http doc featuretool com autom featur engin primit html oper appli tabl set tabl creat featur repres simpl calcul mani alreadi use manual featur engin stack top creat complex featur featur primit fall two categori aggreg function group togeth child datapoint parent calcul statist mean min max standard deviat exampl calcul maximum previou loan amount client aggreg work across multipl tabl use relationship tabl transform oper appli one column singl tabl exampl would take absolut valu column find differ two column one tabl list avail featur primit featuretool view
3511,"# Deep Feature Synthesis

Deep Feature Synthesis (DFS) is the process featuretools uses to make new features. DFS stacks feature primitives to form features with a ""depth"" equal to the number of primitives. For example, if we take the maximum value of a client's previous loans (say `MAX(previous.loan_amount)`), that is a ""deep feature"" with a depth of 1. To create a feature with a depth of two, we could stack primitives by taking the maximum value of a client's average montly payments per previous loan (such as `MAX(previous(MEAN(installments.payment)))`). The [original paper on automated feature engineering using deep feature synthesis](https://dai.lids.mit.edu/wp-content/uploads/2017/10/DSAA_DSM_2015.pdf) is worth a read. 

To perform DFS in featuretools, we use the `dfs`  function passing it an `entityset`, the `target_entity` (where we want to make the features), the `agg_primitives` to use, the `trans_primitives` to use and the `max_depth` of the features. Here we will use the default aggregation and transformation primitives,  a max depth of 2, and calculate primitives for the `app` entity. Because this process is computationally expensive, we can run the function using `features_only = True` to return only a list of the features and not calculate the features themselves. This can be useful to look at the resulting features before starting an extended computation.",deep featur synthesi deep featur synthesi df process featuretool use make new featur df stack featur primit form featur depth equal number primit exampl take maximum valu client previou loan say max previou loan amount deep featur depth 1 creat featur depth two could stack primit take maximum valu client averag montli payment per previou loan max previou mean instal payment origin paper autom featur engin use deep featur synthesi http dai lid mit edu wp content upload 2017 10 dsaa dsm 2015 pdf worth read perform df featuretool use df function pas entityset target entiti want make featur agg primit use tran primit use max depth featur use default aggreg transform primit max depth 2 calcul primit app entiti process comput expens run function use featur true return list featur calcul featur use look result featur start extend comput
3512,### DFS with Default Primitives,df default primit
3513,"If you are interested in running this call on the entire dataset and making the features, I wrote a script [for that here](https://www.kaggle.com/willkoehrsen/feature-engineering-using-feature-tools). Unfortunately, this will not run in a Kaggle kernel due to the computational expense of the operation. Using a computer with 64GB of ram, this function call took around 24 hours (I don't think I'm technically breaking the rules of my university's high powered computing center). I have made the entire dataset available [here](https://www.kaggle.com/willkoehrsen/home-credit-default-risk-feature-tools/data) in the file called `feature_matrix.csv`. 

To generate a subset of the features, run the code cell below.",interest run call entir dataset make featur wrote script http www kaggl com willkoehrsen featur engin use featur tool unfortun run kaggl kernel due comput expens oper use comput 64gb ram function call took around 24 hour think technic break rule univers high power comput center made entir dataset avail http www kaggl com willkoehrsen home credit default risk featur tool data file call featur matrix csv gener subset featur run code cell
3514,"### DFS with Selected Aggregation Primitives

With featuretools, we were able to go from 121 original features to almost 1700 in a few lines of code.  When I did feature engineering by hand, it took about 12 hours to create a comparable size dataset. However, while we get a lot of features in featuretools, this function call is not very well-informed. We simply used the default aggregations without thinking about which ones are ""important"" for the problem. We end up with a lot of features, but they are probably not all relevant to the problem. Too many irrelevant features can decrease performance by drowning out the important features (related to the [curse of dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality))

The next call we make will specify a smaller set of features. We still are not using much domain knowledge, but this feature set will be more manageable. The next step from here is improving the features we actually build and performing feature selection.",df select aggreg primit featuretool abl go 121 origin featur almost 1700 line code featur engin hand took 12 hour creat compar size dataset howev get lot featur featuretool function call well inform simpli use default aggreg without think one import problem end lot featur probabl relev problem mani irrelev featur decreas perform drown import featur relat cur dimension http en wikipedia org wiki cur dimension next call make specifi smaller set featur still use much domain knowledg featur set manag next step improv featur actual build perform featur select
3515,"That ""only"" gives us 884 features (and takes about 12 hours to run on the complete dataset). ",give u 884 featur take 12 hour run complet dataset
3516,"## Notes on Basic Implementation

These calls represent only a [small fraction of the ability of featuretools](https://docs.featuretools.com/guides/tuning_dfs.html). We did not specify the variable types when creating entities, did not use the relative time variables, and didn't touch on [custom primitives](https://docs.featuretools.com/guides/advanced_custom_primitives.html) or seed features or interesting values! Nonetheless, in this notebook, we were able to learn the basic foundations which will allow us to more effective use the tool as we learn how it works.  Now, let's take a look at some of the features we have built and modeling results.",note basic implement call repres small fraction abil featuretool http doc featuretool com guid tune df html specifi variabl type creat entiti use rel time variabl touch custom primit http doc featuretool com guid advanc custom primit html seed featur interest valu nonetheless notebook abl learn basic foundat allow u effect use tool learn work let take look featur built model result
3517,"# Results

To determine whether our basic implementation of featuretools was useful, we can look at several results:

* Cross validation scores and public leaderboard scores using several different sets of features.
* Correlations: both between the features and the `TARGET`, and between features themselves
* Feature importances: determined by a gradient boosting machine model
",result determin whether basic implement featuretool use look sever result cross valid score public leaderboard score use sever differ set featur correl featur target featur featur import determin gradient boost machin model
3518,"## Feature Performance Experiments

To compare a number of different feature sets for the machine learning task, I set up several experiments.. In order to isolate the effect of the features, the same model was used to test a number of different feature sets. The model (which can be viewed in the appendix) is a basic LightGBM algorithm using 5-fold cross validation for training and evaluation. First, we establish a control dataset, and then we carry out a series of experiments and present the results.

* Control: using only data from the `application` dataset
* Test One: manual feature engineering using only the `application`, `bureau` and `bureau_balance` data
* Test Two: manual feature engineering using all datasets
* Test Three: featuretools default features (in the `feature_matrix`)
* Test Four: featuretools specified features (in the `feature_matrix_spec`)
* Test Five: featuretools specified features combined with manual feature engineering 

The number of features is after one-hot encoding, the validation receiver operating characteristic area under the curve (ROC AUC) is calculated using 5-fold cross validation, the test ROC AUC is from the public leaderboard, and the time spent designing is my best estimate of how long it took to make the dataset! 

| Test    | Number of Features | Validation ROC AUC | Test ROC AUC | Time Spent |
|---------|--------------------|--------------------|--------------|--------|
| Control | 241                |           0.760         |     0.745         |       0.25 hours  |
| One     | 421                |       0.766             |      0.757        |        8 hours        |
| Two     |      1465             |          0.785          |         0.783     |                 12 hours |
| Three   | 1803               |      0.784              |       0.777       |               1 hour
| Four    | 1156               |         0.786           |        0.779      |                 1.25 hours |
| Five    |  1624                  |           0. 787        |      0.782        |                    13.25 hours |


It's hard to say which set is exactly the best (although I trust the cross validation scores more than the public leaderboard) but there are huge discrepancies is the time for development. The specified featuretools dataset was able to achieve nearly the same performance as the hand engineered features on the test set with 8% of the time invested. It's clear that featuretools delivered value on this problem, but it still did not leave us without a job. The vital role of the data scientist now comes down to choosing the correct set of primitives and selecting the best features from among all the candidates. ",featur perform experi compar number differ featur set machin learn task set sever experi order isol effect featur model use test number differ featur set model view appendix basic lightgbm algorithm use 5 fold cross valid train evalu first establish control dataset carri seri experi present result control use data applic dataset test one manual featur engin use applic bureau bureau balanc data test two manual featur engin use dataset test three featuretool default featur featur matrix test four featuretool specifi featur featur matrix spec test five featuretool specifi featur combin manual featur engin number featur one hot encod valid receiv oper characterist area curv roc auc calcul use 5 fold cross valid test roc auc public leaderboard time spent design best estim long took make dataset test number featur valid roc auc test roc auc time spent control 241 0 760 0 745 0 25 hour one 421 0 766 0 757 8 hour two 1465 0 785 0 783 12 hour three 1803 0 784 0 777 1 hour four 1156 0 786 0 779 1 25 hour five 1624 0 787 0 782 13 25 hour hard say set exactli best although trust cross valid score public leaderboard huge discrep time develop specifi featuretool dataset abl achiev nearli perform hand engin featur test set 8 time invest clear featuretool deliv valu problem still leav u without job vital role data scientist come choos correct set primit select best featur among candid
3519,"## Correlations

Next we can look at correlations within the data. When we look at correlations with the target, we need to be careful about the [multiple comparisons problem](https://towardsdatascience.com/the-multiple-comparisons-problem-e5573e8b9578): if we make a ton of features, some are likely to be correlated with the target simply because of random noise. Using correlations is fine as a first approximation for identifying ""good features"", but it is not a rigorous feature selection method.  

Also, based on examining some of the features, it seems there might be issues with [collinearity between features](https://en.wikipedia.org/wiki/Multicollinearity) made by featuretools. Features that are highly correlated with one another can diminish interpretability and generalization performance on the test set. In an ideal scenario, we would have a set of independent features, but that rarely occurs in practice. If there are very highly correlated varibables, we might want to think about removing some of them.

For the correlations, we will focus on the `feature_matrix_spec`, the features we made by specifying the primitives. The same analysis could be applied to the default feature set. These correlations were calculated using the entire training section of the feature matrix.",correl next look correl within data look correl target need care multipl comparison problem http towardsdatasci com multipl comparison problem e5573e8b9578 make ton featur like correl target simpli random nois use correl fine first approxim identifi good featur rigor featur select method also base examin featur seem might issu collinear featur http en wikipedia org wiki multicollinear made featuretool featur highli correl one anoth diminish interpret gener perform test set ideal scenario would set independ featur rare occur practic highli correl varib might want think remov correl focu featur matrix spec featur made specifi primit analysi could appli default featur set correl calcul use entir train section featur matrix
3520,### Correlations with the Target,correl target
3521,"Several of the features created by featuretools are among the most correlated with the `TARGET` (in terms of absolute magnitude). However, that does not mean they are necessarily ""important"". 

",sever featur creat featuretool among correl target term absolut magnitud howev mean necessarili import
3522,"### Visualize Distribution of Correlated Variables

One way we can look at the resulting features and their relation to the target is with a kernel density estimate plot. This shows the distribution of a single variable, and can be thought of as a smoothed histogram. To show the effect of a categorical variable on the distribution of a numeric variable, we can color the plot by th value of the categorical variable. In the plot below, we show the distribution of two of the newly created features, colored by the value of the target. 

First, we read in some of the feature matrix using the `nrows` argument of pandas `read_csv` function. This ensures we will not read in the entire 2 GB file. ",visual distribut correl variabl one way look result featur relat target kernel densiti estim plot show distribut singl variabl thought smooth histogram show effect categor variabl distribut numer variabl color plot th valu categor variabl plot show distribut two newli creat featur color valu target first read featur matrix use nrow argument panda read csv function ensur read entir 2 gb file
3523,"The correlation between this feature and the target is extremely weak and could be only noise. Trying to interpret this feature is difficult, but my best guess is: a client's maximum value of average number of atm drawings per month on previous credit card loans. (We are only using a sample of the features, so this might not be representative of the entire dataset).",correl featur target extrem weak could nois tri interpret featur difficult best guess client maximum valu averag number atm draw per month previou credit card loan use sampl featur might repres entir dataset
3524,"Another area to investigate is highly correlated features, known as collinear features. We can look for pairs of correlated features and potentially remove any above a threshold.",anoth area investig highli correl featur known collinear featur look pair correl featur potenti remov threshold
3525,#### Collinear Features,collinear featur
3526,These variables all have a 0.99 correlation with each other which is nearly perfectly positively linear. Including them all in the model is unnecessary because it would be encoding redundant information. We would probably want to remove some of these highly correlated variables in order to help the model learn and generalize better. ,variabl 0 99 correl nearli perfectli posit linear includ model unnecessari would encod redund inform would probabl want remov highli correl variabl order help model learn gener better
3527,"## Feature Importances

The feature importances returned by a tree-based model [represent the reduction in impurity](https://stackoverflow.com/questions/15810339/how-are-feature-importances-in-randomforestclassifier-determined) from including the feature in the model. While the absolute value of the importances can be difficult to interpret, looking at the relative value of the importances allows us to compare the relevance of features. Although we want to be careful about placing too much value on the feature importances, they can be a useful method for dimensionality reduction and understanding the model.",featur import featur import return tree base model repres reduct impur http stackoverflow com question 15810339 featur import randomforestclassifi determin includ featur model absolut valu import difficult interpret look rel valu import allow u compar relev featur although want care place much valu featur import use method dimension reduct understand model
3528,"The most important feature created by featuretools was the maximum number of days before current application that the client applied for a loan at another institution. (This feature is originally recorded as negative, so the maximum value would be closest to zero).",import featur creat featuretool maximum number day current applic client appli loan anoth institut featur origin record neg maximum valu would closest zero
3529,We can calculate the number of top 100 features that were made by featuretools. ,calcul number top 100 featur made featuretool
3530,Let's write a short function to visualize the 15 most important features. ,let write short function visual 15 import featur
3531,"The most important feature created by featuretools was `MAX(bureau.DAYS_CREDIT)`. `DAYS_CREDIT` represents the number of days before the current application at Home Credit that the applicant applied for a loan at another credit institution. The maximum of this value (over the previous loans) is therefore represented by this feature. We also see several important features with a depth of two such as `MEAN(previous_app.MIN(installments.AMT_PAYMENT))` which is the average over a client's loans of the minimum value of previous credit application installment payments. 

Feature importances can be used for dimensionality reduction. They can also be used to help us better understand a problem. For example, we could use the most important features in order to concentrate on these aspects of a client when evaluating a potential loan. Let's look at the number of features with 0 importance which almost certainly can be removed from the featureset. ",import featur creat featuretool max bureau day credit day credit repres number day current applic home credit applic appli loan anoth credit institut maximum valu previou loan therefor repres featur also see sever import featur depth two mean previou app min instal amt payment averag client loan minimum valu previou credit applic instal payment featur import use dimension reduct also use help u better understand problem exampl could use import featur order concentr aspect client evalu potenti loan let look number featur 0 import almost certainli remov featureset
3532,"## Remove Low Importance Features

Feature selection is an entire topic by itself, but one thing we can do is remove any features that have only a single unique value or are all null. Featuretools has a default method for doing this available in the `selection` module.",remov low import featur featur select entir topic one thing remov featur singl uniqu valu null featuretool default method avail select modul
3533,"## Align Train and Test Sets

We also want to make sure the train and test sets have the same exact features. We can first one-hot encode the data (we'll have to do this anyway for our model) and then align the dataframes on the columns.",align train test set also want make sure train test set exact featur first one hot encod data anyway model align datafram column
3534,Removing the low information features and aligning the dataframes has left us with 1689 features! Feature selection will certainly play an important role when using featuretools. ,remov low inform featur align datafram left u 1689 featur featur select certainli play import role use featuretool
3535," # Conclusions

In this notebook we went through a basic implementation of using automated feature engineering with featuretools for the Home Credit Default Risk dataset. Although we did not use the advanced functionality of featuretools, we still were able to create useful features that improved the model's performance in cross validation and on the test set. Moreover, automated feature engineering took a fraction of the time spent manual feature engineering while delivering comparable results. 

__Even the default set of features in featuretools was able to achieve similar performance to hand-engineered features in less than 10% of the time.__
__Featuretools demonstrably adds value when included in a data scientist's toolbox.__

The next steps are to take advantage of the advanced functionality in featuretools combined with domain knowledge to create a more useful set of features. We will look explore [tuning featuretools in an upcoming notebook](https://www.kaggle.com/willkoehrsen/intro-to-tuning-automated-feature-engineering)!",conclus notebook went basic implement use autom featur engin featuretool home credit default risk dataset although use advanc function featuretool still abl creat use featur improv model perform cross valid test set moreov autom featur engin took fraction time spent manual featur engin deliv compar result even default set featur featuretool abl achiev similar perform hand engin featur le 10 time featuretool demonstr add valu includ data scientist toolbox next step take advantag advanc function featuretool combin domain knowledg creat use set featur look explor tune featuretool upcom notebook http www kaggl com willkoehrsen intro tune autom featur engin
3536,"## Appendix: GBM Model (Used Across Feature Sets)
```python
def model(features, test_features, encoding = 'ohe', n_folds = 5):
    
    """"""Train and test a light gradient boosting model using
    cross validation. 
    
    Parameters
    --------
        features (pd.DataFrame): 
            dataframe of training features to use 
            for training a model. Must include the TARGET column.
        test_features (pd.DataFrame): 
            dataframe of testing features to use
            for making predictions with the model. 
        encoding (str, default = 'ohe'): 
            method for encoding categorical variables. Either 'ohe' for one-hot encoding or 'le' for integer label encoding
            n_folds (int, default = 5): number of folds to use for cross validation
        
    Return
    --------
        submission (pd.DataFrame): 
            dataframe with `SK_ID_CURR` and `TARGET` probabilities
            predicted by the model.
        feature_importances (pd.DataFrame): 
            dataframe with the feature importances from the model.
        valid_metrics (pd.DataFrame): 
            dataframe with training and validation metrics (ROC AUC) for each fold and overall.
        
    """"""
    
    # Extract the ids
    train_ids = features['SK_ID_CURR']
    test_ids = test_features['SK_ID_CURR']
    
    # Extract the labels for training
    labels = features['TARGET']
    
    # Remove the ids and target
    features = features.drop(columns = ['SK_ID_CURR', 'TARGET'])
    test_features = test_features.drop(columns = ['SK_ID_CURR'])
    
    
    # One Hot Encoding
    if encoding == 'ohe':
        features = pd.get_dummies(features)
        test_features = pd.get_dummies(test_features)
        
        # Align the dataframes by the columns
        features, test_features = features.align(test_features, join = 'inner', axis = 1)
        
        # No categorical indices to record
        cat_indices = 'auto'
    
    # Integer label encoding
    elif encoding == 'le':
        
        # Create a label encoder
        label_encoder = LabelEncoder()
        
        # List for storing categorical indices
        cat_indices = []
        
        # Iterate through each column
        for i, col in enumerate(features):
            if features[col].dtype == 'object':
                # Map the categorical features to integers
                features[col] = label_encoder.fit_transform(np.array(features[col].astype(str)).reshape((-1,)))
                test_features[col] = label_encoder.transform(np.array(test_features[col].astype(str)).reshape((-1,)))

                # Record the categorical indices
                cat_indices.append(i)
    
    # Catch error if label encoding scheme is not valid
    else:
        raise ValueError(""Encoding must be either 'ohe' or 'le'"")
        
    print('Training Data Shape: ', features.shape)
    print('Testing Data Shape: ', test_features.shape)
    
    # Extract feature names
    feature_names = list(features.columns)
    
    # Convert to np arrays
    features = np.array(features)
    test_features = np.array(test_features)
    
    # Create the kfold object
    k_fold = KFold(n_splits = n_folds, shuffle = False, random_state = 50)
    
    # Empty array for feature importances
    feature_importance_values = np.zeros(len(feature_names))
    
    # Empty array for test predictions
    test_predictions = np.zeros(test_features.shape[0])
    
    # Empty array for out of fold validation predictions
    out_of_fold = np.zeros(features.shape[0])
    
    # Lists for recording validation and training scores
    valid_scores = []
    train_scores = []
    
    # Iterate through each fold
    for train_indices, valid_indices in k_fold.split(features):
        
        # Training data for the fold
        train_features, train_labels = features[train_indices], labels[train_indices]
        # Validation data for the fold
        valid_features, valid_labels = features[valid_indices], labels[valid_indices]
        
        # Create the model
        model = lgb.LGBMClassifier(n_estimators=10000, boosting_type = 'goss',
				   objective = 'binary', 
                                   class_weight = 'balanced', learning_rate = 0.05, 
                                   reg_alpha = 0.1, reg_lambda = 0.1, n_jobs = -1, random_state = 50)
        
        # Train the model
        model.fit(train_features, train_labels, eval_metric = 'auc',
                  eval_set = [(valid_features, valid_labels), (train_features, train_labels)],
                  eval_names = ['valid', 'train'], categorical_feature = cat_indices,
                  early_stopping_rounds = 100, verbose = 200)
        
        # Record the best iteration
        best_iteration = model.best_iteration_
        
        # Record the feature importances
        feature_importance_values += model.feature_importances_ / k_fold.n_splits
        
        # Make predictions
        test_predictions += model.predict_proba(test_features, num_iteration = best_iteration)[:, 1] / k_fold.n_splits
        
        # Record the out of fold predictions
        out_of_fold[valid_indices] = model.predict_proba(valid_features, num_iteration = best_iteration)[:, 1]
        
        # Record the best score
        valid_score = model.best_score_['valid']['auc']
        train_score = model.best_score_['train']['auc']
        
        valid_scores.append(valid_score)
        train_scores.append(train_score)
        
        # Clean up memory
        gc.enable()
        del model, train_features, valid_features
        gc.collect()
        
    # Make the submission dataframe
    submission = pd.DataFrame({'SK_ID_CURR': test_ids, 'TARGET': test_predictions})
    
    # Make the feature importance dataframe
    feature_importances = pd.DataFrame({'feature': feature_names, 'importance': feature_importance_values})
    
    # Overall validation score
    valid_auc = roc_auc_score(labels, out_of_fold)
    
    # Add the overall scores to the metrics
    valid_scores.append(valid_auc)
    train_scores.append(np.mean(train_scores))
    
    # Needed for creating dataframe of validation scores
    fold_names = list(range(n_folds))
    fold_names.append('overall')
    
    # Dataframe of validation scores
    metrics = pd.DataFrame({'fold': fold_names,
                            'train': train_scores,
                            'valid': valid_scores}) 
    
    return submission, feature_importances, metrics
```",appendix gbm model use across featur set python def model featur test featur encod ohe n fold 5 train test light gradient boost model use cross valid paramet featur pd datafram datafram train featur use train model must includ target column test featur pd datafram datafram test featur use make predict model encod str default ohe method encod categor variabl either ohe one hot encod le integ label encod n fold int default 5 number fold use cross valid return submiss pd datafram datafram sk id curr target probabl predict model featur import pd datafram datafram featur import model valid metric pd datafram datafram train valid metric roc auc fold overal extract id train id featur sk id curr test id test featur sk id curr extract label train label featur target remov id target featur featur drop column sk id curr target test featur test featur drop column sk id curr one hot encod encod ohe featur pd get dummi featur test featur pd get dummi test featur align datafram column featur test featur featur align test featur join inner axi 1 categor indic record cat indic auto integ label encod elif encod le creat label encod label encod labelencod list store categor indic cat indic iter column col enumer featur featur col dtype object map categor featur integ featur col label encod fit transform np array featur col astyp str reshap 1 test featur col label encod transform np array test featur col astyp str reshap 1 record categor indic cat indic append catch error label encod scheme valid el rais valueerror encod must either ohe le print train data shape featur shape print test data shape test featur shape extract featur name featur name list featur column convert np array featur np array featur test featur np array test featur creat kfold object k fold kfold n split n fold shuffl fals random state 50 empti array featur import featur import valu np zero len featur name empti array test predict test predict np zero test featur shape 0 empti array fold valid predict fold np zero featur shape 0 list record valid train score valid score train score iter fold train indic valid indic k fold split featur train data fold train featur train label featur train indic label train indic valid data fold valid featur valid label featur valid indic label valid indic creat model model lgb lgbmclassifi n estim 10000 boost type go object binari class weight balanc learn rate 0 05 reg alpha 0 1 reg lambda 0 1 n job 1 random state 50 train model model fit train featur train label eval metric auc eval set valid featur valid label train featur train label eval name valid train categor featur cat indic earli stop round 100 verbos 200 record best iter best iter model best iter record featur import featur import valu model featur import k fold n split make predict test predict model predict proba test featur num iter best iter 1 k fold n split record fold predict fold valid indic model predict proba valid featur num iter best iter 1 record best score valid score model best score valid auc train score model best score train auc valid score append valid score train score append train score clean memori gc enabl del model train featur valid featur gc collect make submiss datafram submiss pd datafram sk id curr test id target test predict make featur import datafram featur import pd datafram featur featur name import featur import valu overal valid score valid auc roc auc score label fold add overal score metric valid score append valid auc train score append np mean train score need creat datafram valid score fold name list rang n fold fold name append overal datafram valid score metric pd datafram fold fold name train train score valid valid score return submiss featur import metric
3537,"# Introduction: Automated Hyperparameter Tuning

In this notebook, we will talk through a complete example of using automated hyperparameter tuning to optimize a machine learning model. In particular, we will use Bayesian Optimization and the Hyperopt library to tune the hyperparameters of a gradient boosting machine. 

__Additional Notebooks__ 

If you haven't checked out my other work on this problem, here is a complete list of the notebooks I have completed so far:

* [A Gentle Introduction](https://www.kaggle.com/willkoehrsen/start-here-a-gentle-introduction)
* [Manual Feature Engineering Part One](https://www.kaggle.com/willkoehrsen/introduction-to-manual-feature-engineering)
* [Manual Feature Engineering Part Two](https://www.kaggle.com/willkoehrsen/introduction-to-manual-feature-engineering-p2)
* [Introduction to Automated Feature Engineering](https://www.kaggle.com/willkoehrsen/automated-feature-engineering-basics)
* [Advanced Automated Feature Engineering](https://www.kaggle.com/willkoehrsen/tuning-automated-feature-engineering-exploratory)
* [Feature Selection](https://www.kaggle.com/willkoehrsen/introduction-to-feature-selection)
* [Intro to Model Tuning: Grid and Random Search](https://www.kaggle.com/willkoehrsen/intro-to-model-tuning-grid-and-random-search)
* [Automated Model Tuning](https://www.kaggle.com/willkoehrsen/automated-model-tuning)

There are four approaches to tuning the hyperparameters of a machine learning model

1. __Manual__: select hyperparameters based on intuition/experience/guessing, train the model with the hyperparameters, and score on the validation data. Repeat process until you run out of patience or are satisfied with the results.
2. __Grid Search__: set up a grid of hyperparameter values and for each combination, train a model and score on the validation data. In this approach, every single combination of hyperparameters values is tried which can be very inefficient!
3. __Random search__: set up a grid of hyperparameter values and select random combinations to train the model and score. The number of search iterations is set based on time/resources.
4. __Automated Hyperparameter Tuning__: use methods such as gradient descent, Bayesian Optimization, or evolutionary algorithms to conduct a guided search for the best hyperparameters.

These are listed in general order of least to most efficient. While we already conquered 2 and 3 [in this notebook](https://www.kaggle.com/willkoehrsen/intro-to-model-tuning-grid-and-random-search) (we didn't even try method 1), we have yet to take on automated hyperparameter tuning. There are a number of methods to do this including genetic programming, Bayesian optimization, and gradient based methods. Here we will focus only on Bayesian optimization, using the Tree Parzen Esimator (don't worry, you don't need to understand this in detail) in the [Hyperopt open-source Python library](https://hyperopt.github.io/hyperopt/).

For a little more background (we'll cover everything you need below), [here is an introductory article](https://towardsdatascience.com/an-introductory-example-of-bayesian-optimization-in-python-with-hyperopt-aae40fff4ff0) on Bayesian optimization, and [here is an article on automated hyperparameter tuning](https://towardsdatascience.com/automated-machine-learning-hyperparameter-tuning-in-python-dfda59b72f8a) using Bayesian optimization. Here we'll get right into automated hyperparameter tuning, so for the necessary background on model tuning, refer to [this kernel](https://www.kaggle.com/willkoehrsen/intro-to-model-tuning-grid-and-random-search)

## Bayesian Optimization Primer

The problem with grid and random search is that these are __uninformed methods__ because they do not use the past results from different values of hyperparameters in the objective function (remember the objective function takes in the hyperparameters and returns the model cross validation score). We record the results of the objective function for each set of hyperparameters, but the algorithms do not select the next hyperparameter values from this information. Intuitively, if we have the past results, we should  use them to reason about what hyperparameter values work the best and choose the next values wisely to try and spend more iterations evaluating promising values. Evaluating hyperparameters in the objective function is very time-consuming, and the __concept of Bayesian optimization is to limit calls to the evaluation function by choosing the next hyperparameter values based on the previous results.__ This allows the algorithm to spend __more time evaluating promising hyperparameter values and less time in low-scoring regions of the hyperparameter space__. For example, consider the image below:

![](https://raw.githubusercontent.com/WillKoehrsen/hyperparameter-optimization/master/images/random_forest_hypothetical.png)

If you were choosing the next number of trees to try for the random forest, where would you concentrate your search? Probably around 100 trees because that is where the lowest errors have tended to occur (imagine this is a problem where we want to minimize the error). In effect, you have just done Bayesian hyperparameter optimization in your head! You formed a probability model of the error as a function of the hyperparameters and then selected the next hyperparameter values by maximizing the probability of a low error. Bayesian optimization works by building a surrogate function (in the form of a probability model) of the objective function $P(\text{score} | \text{hyperparameters}$. The surrogate function is much cheaper to evaluate than the objective, so the algorithm chooses the next values to try in the objective based on maximizing a criterion on the surrogate (usually expected improvement), exactly what you would have done with respect to the image above. 

The surrogate function is based on past evaluation results - pairs of (score, hyperparameter) records - and is continually updated with each objective function evaluation. Bayesian optimization therefore uses Bayesian reasoning: form an initial model (called a prior) and then update it with more evidence. The idea is that as the data accumulates, the surrogate function gets closer and closer to the objective function, and the hyperparameter values that are the best in the surrogate function will also do the best in the objective function. Bayesian optimization methods differ in the algorithm used to build the surrogate function and choose the next hyperparameter values to try. Some of the common choices are Gaussian Process (implemented in Spearmint), Random Forest Regression (in SMAC), and the Tree Parzen Estimator (TPE) in Hyperopt (technical details can be found in this article, although they won't be necessary to use the methods).

### Four Part of Bayesian Optimization

Bayesian hyperparameter optimization requires the same four parts as we implemented in grid and random search:

1. __Objective Function__: takes in an input (hyperparameters) and returns a score to minimize or maximize (the cross validation score)
2. __Domain space__: the range of input values (hyperparameters) to evaluate
3. __Optimization Algorithm__: the method used to construct the surrogate function and choose the next values to evaluate
4. __Results__: score, value pairs that the algorithm uses to build the surrogate function

The only differences are that now our objective function will return a score to minimize (this is just convention in the field of optimization), our domain space will be probability distributions rather than a hyperparameter grid, and the optimization algorithm will be an __informed method__ that uses past results to choose the next hyperparameter values to evaluate. 

## Hyperopt

Hyperopt is an open-source Python library the implements Bayesian Optimization using the Tree Parzen Estimator algorithm to construct the surrogate function and select the next hyperparameter values to evaluate in the objective function. There are a number of other libraries such as Spearmint (Guassian process surrogate function) and SMAC (random forest regression surrogate function) sharing the same problem structure. The four parts of an optimization problem that we develop here will apply to all the libraries with only a change in syntax. Morevoer, the optimization methods as applied to the Gradient Boosting Machine will translate to other machine learning models or any problem where we have to minimize a function.

### Gradient Boosting Machine

We will use the gradient booosting machine (GBM) as our model to tune in the LightGBM library. The GBM is our choice of model because it performs extremely well for these types of problems (as shown on the leaderboard) and because the performance is heavily dependent on the choice of hyperparameter values. 

For more details of the Gradient Boosting Machine (GBM), check out this high-level blog post, or this in depth technical article.

### Cross Validation with Early Stopping

As with random and grid search, we will evaluate each set of hyperparameters using 5 fold cross validation on the training data. The GBM model will be trained with early stopping, where estimators are added to the ensemble until the validation score has not decrease for 100 iterations (estimators added). 

Cross validation and early stopping will be implemented using the LightGBM `cv` function. We will use 5 folds and 100 early stopping rounds. 

#### Dataset and Approach

As before, we will work with a limited section of the data - 10000 observations for training and 6000 observations for testing. This will allow the optimization within the notebook to finish in a reasonable amount of time. Later in the notebook, I'll present results from 1000 iterations of Bayesian hyperparameter optimization on the reduced dataset and we then will see if these results translate to a full dataset (from [this kernel](https://www.kaggle.com/jsaguiar/updated-0-792-lb-lightgbm-with-simple-features)). The functions developed here can be taken and run on any dataset, or used with any machine learning model (just with minor changes in the details) and working with a smaller dataset will allow us to learn all of the concepts. I am currently running 500 iterations of Bayesian hyperparameter optimization on a complete dataset and will make the results available when the search is completed. ",introduct autom hyperparamet tune notebook talk complet exampl use autom hyperparamet tune optim machin learn model particular use bayesian optim hyperopt librari tune hyperparamet gradient boost machin addit notebook check work problem complet list notebook complet far gentl introduct http www kaggl com willkoehrsen start gentl introduct manual featur engin part one http www kaggl com willkoehrsen introduct manual featur engin manual featur engin part two http www kaggl com willkoehrsen introduct manual featur engin p2 introduct autom featur engin http www kaggl com willkoehrsen autom featur engin basic advanc autom featur engin http www kaggl com willkoehrsen tune autom featur engin exploratori featur select http www kaggl com willkoehrsen introduct featur select intro model tune grid random search http www kaggl com willkoehrsen intro model tune grid random search autom model tune http www kaggl com willkoehrsen autom model tune four approach tune hyperparamet machin learn model 1 manual select hyperparamet base intuit experi guess train model hyperparamet score valid data repeat process run patienc satisfi result 2 grid search set grid hyperparamet valu combin train model score valid data approach everi singl combin hyperparamet valu tri ineffici 3 random search set grid hyperparamet valu select random combin train model score number search iter set base time resourc 4 autom hyperparamet tune use method gradient descent bayesian optim evolutionari algorithm conduct guid search best hyperparamet list gener order least effici alreadi conquer 2 3 notebook http www kaggl com willkoehrsen intro model tune grid random search even tri method 1 yet take autom hyperparamet tune number method includ genet program bayesian optim gradient base method focu bayesian optim use tree parzen esim worri need understand detail hyperopt open sourc python librari http hyperopt github io hyperopt littl background cover everyth need introductori articl http towardsdatasci com introductori exampl bayesian optim python hyperopt aae40fff4ff0 bayesian optim articl autom hyperparamet tune http towardsdatasci com autom machin learn hyperparamet tune python dfda59b72f8a use bayesian optim get right autom hyperparamet tune necessari background model tune refer kernel http www kaggl com willkoehrsen intro model tune grid random search bayesian optim primer problem grid random search uninform method use past result differ valu hyperparamet object function rememb object function take hyperparamet return model cross valid score record result object function set hyperparamet algorithm select next hyperparamet valu inform intuit past result use reason hyperparamet valu work best choos next valu wise tri spend iter evalu promis valu evalu hyperparamet object function time consum concept bayesian optim limit call evalu function choos next hyperparamet valu base previou result allow algorithm spend time evalu promis hyperparamet valu le time low score region hyperparamet space exampl consid imag http raw githubusercont com willkoehrsen hyperparamet optim master imag random forest hypothet png choos next number tree tri random forest would concentr search probabl around 100 tree lowest error tend occur imagin problem want minim error effect done bayesian hyperparamet optim head form probabl model error function hyperparamet select next hyperparamet valu maxim probabl low error bayesian optim work build surrog function form probabl model object function p text score text hyperparamet surrog function much cheaper evalu object algorithm choos next valu tri object base maxim criterion surrog usual expect improv exactli would done respect imag surrog function base past evalu result pair score hyperparamet record continu updat object function evalu bayesian optim therefor use bayesian reason form initi model call prior updat evid idea data accumul surrog function get closer closer object function hyperparamet valu best surrog function also best object function bayesian optim method differ algorithm use build surrog function choos next hyperparamet valu tri common choic gaussian process implement spearmint random forest regress smac tree parzen estim tpe hyperopt technic detail found articl although necessari use method four part bayesian optim bayesian hyperparamet optim requir four part implement grid random search 1 object function take input hyperparamet return score minim maxim cross valid score 2 domain space rang input valu hyperparamet evalu 3 optim algorithm method use construct surrog function choos next valu evalu 4 result score valu pair algorithm use build surrog function differ object function return score minim convent field optim domain space probabl distribut rather hyperparamet grid optim algorithm inform method use past result choos next hyperparamet valu evalu hyperopt hyperopt open sourc python librari implement bayesian optim use tree parzen estim algorithm construct surrog function select next hyperparamet valu evalu object function number librari spearmint guassian process surrog function smac random forest regress surrog function share problem structur four part optim problem develop appli librari chang syntax morevo optim method appli gradient boost machin translat machin learn model problem minim function gradient boost machin use gradient booost machin gbm model tune lightgbm librari gbm choic model perform extrem well type problem shown leaderboard perform heavili depend choic hyperparamet valu detail gradient boost machin gbm check high level blog post depth technic articl cross valid earli stop random grid search evalu set hyperparamet use 5 fold cross valid train data gbm model train earli stop estim ad ensembl valid score decreas 100 iter estim ad cross valid earli stop implement use lightgbm cv function use 5 fold 100 earli stop round dataset approach work limit section data 10000 observ train 6000 observ test allow optim within notebook finish reason amount time later notebook present result 1000 iter bayesian hyperparamet optim reduc dataset see result translat full dataset kernel http www kaggl com jsaguiar updat 0 792 lb lightgbm simpl featur function develop taken run dataset use machin learn model minor chang detail work smaller dataset allow u learn concept current run 500 iter bayesian hyperparamet optim complet dataset make result avail search complet
3538,"With the background details out of the way, let's get started with Bayesian optimization applied to automated hyperparameter tuning! ",background detail way let get start bayesian optim appli autom hyperparamet tune
3539,The code below reads in the data and creates a smaller version for training and a set for testing. We can only use the training data __a single time__ when we evaluate the final model. Hyperparameter tuning must be done on the training data using cross validation!,code read data creat smaller version train set test use train data singl time evalu final model hyperparamet tune must done train data use cross valid
3540,"### Baseline Model 

First we can create a model with the default value of hyperparameters and score it using cross validation with early stopping. Using the `cv` LightGBM function requires creating a `Dataset`.",baselin model first creat model default valu hyperparamet score use cross valid earli stop use cv lightgbm function requir creat dataset
3541,Now we can evaluate the baseline model on the testing data.,evalu baselin model test data
3542,"# Objective Function

The first part to write is the objective function which takes in a set of hyperparameter values and returns the cross validation score on the training data. An objective function in Hyperopt must return either a single real value to minimize, or a dictionary with a key ""loss"" with the score to minimize (and a key ""status"" indicating if the run was successful or not). 

Optimization is typically about minimizing a value, and because our metric is Receiver Operating Characteristic Area Under the Curve (ROC AUC) where higher is better, the objective function will return $1 - \text{ROC AUC Cross Validation}$. The algorithm will try to drive this value as low as possible (raising the ROC AUC) by choosing the next hyperparameters based on the past results. 

The complete objective function is shown below. As with random and grid search, we write to a `csv` file on each call of the function in order to track results as the search progress and so we have a saved record of the search. (The `subsample` and `boosting_type` logic will be explained when we get to the domain). ",object function first part write object function take set hyperparamet valu return cross valid score train data object function hyperopt must return either singl real valu minim dictionari key loss score minim key statu indic run success optim typic minim valu metric receiv oper characterist area curv roc auc higher better object function return 1 text roc auc cross valid algorithm tri drive valu low possibl rais roc auc choos next hyperparamet base past result complet object function shown random grid search write csv file call function order track result search progress save record search subsampl boost type logic explain get domain
3543,"# Domain 

Specifying the domain (called the space in Hyperopt) is a little trickier than in grid search. In Hyperopt, and other Bayesian optimization frameworks, the domian is not a discrete grid but instead has probability distributions for each hyperparameter. For each hyperparameter, we will use the same limits as with the grid, but instead of being defined at each point, the domain represents probabilities for each hyperparameter. This will probably become clearer in the code and the images!",domain specifi domain call space hyperopt littl trickier grid search hyperopt bayesian optim framework domian discret grid instead probabl distribut hyperparamet hyperparamet use limit grid instead defin point domain repres probabl hyperparamet probabl becom clearer code imag
3544,"First we will go through an example of the learning rate. We are using a log-uniform space for the learning rate defined from 0.005 to 0.5. The log - uniform distribution has the values evenly placed in logarithmic space rather than linear space. This is useful for variables that differ over several orders of magnitude such as the learning rate. For example, with a log-uniform distribution, there will be an equal chance of drawing a value from 0.005 to 0.05 and from 0.05 to 0.5 (in linear space far more values would be drawn from the later since the linear distance is much larger. The logarithmic space is exactly the same - a factor of 10).",first go exampl learn rate use log uniform space learn rate defin 0 005 0 5 log uniform distribut valu evenli place logarithm space rather linear space use variabl differ sever order magnitud learn rate exampl log uniform distribut equal chanc draw valu 0 005 0 05 0 05 0 5 linear space far valu would drawn later sinc linear distanc much larger logarithm space exactli factor 10
3545,We can visualize the learning rate by drawing 10000 samples from the distribution.,visual learn rate draw 10000 sampl distribut
3546,The number of leaves on the other hand is a discrete uniform distribution.,number leav hand discret uniform distribut
3547,"### Conditional Domain

In Hyperopt, we can use nested conditional statements to indicate hyperparameters that depend on other hyperparameters. For example, the ""goss"" `boosting_type` cannot use subsampling, so when we set up the `boosting_type` categorical variable, we have to set the subsample to 1.0 while for the other boosting types it's a float between 0.5 and 1.0.",condit domain hyperopt use nest condit statement indic hyperparamet depend hyperparamet exampl go boost type use subsampl set boost type categor variabl set subsampl 1 0 boost type float 0 5 1 0
3548,"We need to set both the boosting_type and subsample as top-level keys in the parameter dictionary. We can use the Python dict.get method with a default value of 1.0. This means that if the key is not present in the dictionary, the value returned will be the default (1.0).",need set boost type subsampl top level key paramet dictionari use python dict get method default valu 1 0 mean key present dictionari valu return default 1 0
3549,"The gbm cannot use the nested dictionary so we need to set the `boosting_type` and `subsample` as top level keys. 

Nested conditionals allow us to use a different set of hyperparameters depending on other hyperparameters. For example, we can explore different models with completely different sets of hyperparameters by using nested conditionals. The only requirement is that the first nested statement must be based on a choice hyperparameter (the choice could be the type of model).",gbm use nest dictionari need set boost type subsampl top level key nest condit allow u use differ set hyperparamet depend hyperparamet exampl explor differ model complet differ set hyperparamet use nest condit requir first nest statement must base choic hyperparamet choic could type model
3550,"## Complete Bayesian Domain

Now we can define the entire domain. Each variable needs to have a label and a few parameters specifying the type and extent of the distribution. For the variables such as boosting type that are categorical, we use the choice variable. Other variables types include quniform, loguniform, and uniform. For the complete list, check out the documentation for Hyperopt. Altogether there are 10 hyperparameters to optimize. ",complet bayesian domain defin entir domain variabl need label paramet specifi type extent distribut variabl boost type categor use choic variabl variabl type includ quniform loguniform uniform complet list check document hyperopt altogeth 10 hyperparamet optim
3551,"### Example of Sampling from the Domain

Let's sample from the domain (using the conditional logic) to see the result of each draw. Every time we run this code, the results will change. (Again notice that we need to assign the top level keys to the keywords understood by the GBM).",exampl sampl domain let sampl domain use condit logic see result draw everi time run code result chang notic need assign top level key keyword understood gbm
3552,"Let's test the objective function with the domain to make sure it works. (Every time the `of_connection` line is run, the `outfile` will be overwritten, so use a different name for each trial to save the results.)",let test object function domain make sure work everi time connect line run outfil overwritten use differ name trial save result
3553,"# Optimization Algorithm

The optimization algorithm is the method for constructing the surrogate function (probability model) and selecting the next set of hyperparameters to evaluate in the objective function. Hyperopt has two choices: random search and Tree Parzen Estimator. 

The technical details of TPE can be found in [this article](https://papers.nips.cc/paper/4443-algorithms-for-hyper-parameter-optimization.pdf) and a conceptual explanation is in [this article](https://towardsdatascience.com/a-conceptual-explanation-of-bayesian-model-based-hyperparameter-optimization-for-machine-learning-b8172278050f). Although this is the most technical part of Bayesian hyperparameter optimization, defining the algorithm in Hyperopt is simple. ",optim algorithm optim algorithm method construct surrog function probabl model select next set hyperparamet evalu object function hyperopt two choic random search tree parzen estim technic detail tpe found articl http paper nip cc paper 4443 algorithm hyper paramet optim pdf conceptu explan articl http towardsdatasci com conceptu explan bayesian model base hyperparamet optim machin learn b8172278050f although technic part bayesian hyperparamet optim defin algorithm hyperopt simpl
3554,"# Results History
The final part is the history of objective function evaluations. Although Hyperopt internally keeps track of the results for the algorithm to use, if we want to monitor the results and have a saved copy of the search, we need to store the results ourselves. Here, we are using two methods to make sure we capture all the results:

1. A `Trials` object that stores the dictionary returned from the objective function
2. Adding a line to a csv file every iteration.

The csv file option also lets us monitor the results of an on-going experiment. Although do not use Excel to open the file while training is on-going. Instead check the results using `tail results/out_file.csv` from bash or open the file in Sublime Text or Notepad.",result histori final part histori object function evalu although hyperopt intern keep track result algorithm use want monitor result save copi search need store result use two method make sure captur result 1 trial object store dictionari return object function 2 ad line csv file everi iter csv file option also let u monitor result go experi although use excel open file train go instead check result use tail result file csv bash open file sublim text notepad
3555,"The `Trials` object will hold everything returned from the objective function in the `.results` attribute. We can use this after the search is complete to inspect the results, but an easier method is to read in the `csv` file because that will already be in a dataframe.",trial object hold everyth return object function result attribut use search complet inspect result easier method read csv file alreadi datafram
3556,"# Automated Hyperparameter Optimization in Practice

We have all four parts we need to run the optimization. To run Bayesian optimization we use the `fmin` function (a good reminder that we need a metric to minimize!) ",autom hyperparamet optim practic four part need run optim run bayesian optim use fmin function good remind need metric minim
3557,`fmin` takes the four parts defined above as well as the maximum number of iterations `max_evals`. ,fmin take four part defin well maximum number iter max eval
3558,"The `best` object holds only the hyperparameters that returned the lowest loss in the objective function. Although this is ultimately what we are after, if we want to understand how the search progresses, we need to inspect the `Trials` object or the `csv` file. For example, we can sort the `results` returned from the objective function by the lowest loss:",best object hold hyperparamet return lowest loss object function although ultim want understand search progress need inspect trial object csv file exampl sort result return object function lowest loss
3559,An easier method is to read in the csv file since this will be a dataframe. ,easier method read csv file sinc datafram
3560,"The function below takes in the results, trains a model on the training data, and evalutes on the testing data. It returns a dataframe of hyperparameters from the search. 

Saving the results to a csv file converts the dictionary of hyperparameters to a string. We need to map this back to a dictionary using `ast.literal_eval`. ",function take result train model train data evalut test data return datafram hyperparamet search save result csv file convert dictionari hyperparamet string need map back dictionari use ast liter eval
3561,"## Continue Optimization

Hyperopt can continue searching where a previous search left off if we pass in a `Trials` object that already has results. The algorithms used in Bayesian optimization are black-box optimizers because they have no internal state. All they need is the previous results of objective function evaluations (the input values and loss) and they can build up the surrogate function and select the next values to evaluate in the objective function. This means that any search can be continued as long as we have the history in a `Trials` object. ",continu optim hyperopt continu search previou search left pas trial object alreadi result algorithm use bayesian optim black box optim intern state need previou result object function evalu input valu loss build surrog function select next valu evalu object function mean search continu long histori trial object
3562,"To save the `Trials` object so it can be read in later for more training, we can use the `json` format. ",save trial object read later train use json format
3563,"To start the training from where it left off, simply load in the `Trials` object and pass it to an instance of `fmin`. (You might even be able to tweak the hyperparameter distribution and continue searching with the `Trials` object because the algorithm does not maintain an internal state. Someone should check this and let me know in the comments!).",start train left simpli load trial object pas instanc fmin might even abl tweak hyperparamet distribut continu search trial object algorithm maintain intern state someon check let know comment
3564,"## Next Steps

Now that we have developed all the necessary parts for automated hyperparameter tuning using Bayesian optimization, we can apply these to any dataset or any machine learning method. The functions taken here can be put in a script and run a full dataset. Next, we will go through results from 1000 evaluations on a reduced size dataset to see how the search progresses. We can then compare these results to random search to see how a method that uses __reasoning__ about past results differs from a method that does not. 

After examining the tuning results from the reduced dataset, we will take the best performing hyperparameters and see if these translate to a full dataset, the features from the `[Updated 0.792 LB] LightGBM with Simple Features`(https://www.kaggle.com/jsaguiar/updated-0-792-lb-lightgbm-with-simple-features) kernel (I did not develop these features and want to give credit to the numerous people, including [Aguiar](https://www.kaggle.com/jsaguiar) and [olivier](https://www.kaggle.com/ogrellier),  who have worked on these features. Please check out their [kernels](https://www.kaggle.com/ogrellier/lighgbm-with-selected-features)!). We saw in the random and grid search notebook that the best hyperparameter values from the small datasets do not necessarily perform well on the full datasets. I am currently running the Bayesian Hyperparameter optimization for 500 iterations on the features referenced above and will make the results publicly available when the search is finished. For now, we will turn to the 1000 trials from the smaller dataset. These results can be generated by running the cell below, but I can't guarantee if this will finish within the kernel time limit! ",next step develop necessari part autom hyperparamet tune use bayesian optim appli dataset machin learn method function taken put script run full dataset next go result 1000 evalu reduc size dataset see search progress compar result random search see method use reason past result differ method examin tune result reduc dataset take best perform hyperparamet see translat full dataset featur updat 0 792 lb lightgbm simpl featur http www kaggl com jsaguiar updat 0 792 lb lightgbm simpl featur kernel develop featur want give credit numer peopl includ aguiar http www kaggl com jsaguiar olivi http www kaggl com ogrelli work featur plea check kernel http www kaggl com ogrelli lighgbm select featur saw random grid search notebook best hyperparamet valu small dataset necessarili perform well full dataset current run bayesian hyperparamet optim 500 iter featur referenc make result publicli avail search finish turn 1000 trial smaller dataset result gener run cell guarante finish within kernel time limit
3565,"# Search Results 

Next we will go through the results from 1000 search iterations on the reduced dataset. We will look at the scores, the distribution of hyperparameter values tried, the evolution of values over time, and compare the hyperparameters values to those from random search.

After examining the search results, we will use the optimized hyperparameters (at least optimized for the smaller dataset) to make predictions on a full dataset. These can then be submitted to the competition to see how well the methods do on a small sample of the data.",search result next go result 1000 search iter reduc dataset look score distribut hyperparamet valu tri evolut valu time compar hyperparamet valu random search examin search result use optim hyperparamet least optim smaller dataset make predict full dataset submit competit see well method small sampl data
3566,## Learning Rate Distribution ,learn rate distribut
3567,"We can see that the Bayesian search did worse in cross validation but then found hyperparameter values that did better on the test set! We will have to see if these results translate to the acutal competition data. First though, we can get all the scores in a dataframe in order to plot them over the course of training.",see bayesian search wors cross valid found hyperparamet valu better test set see result translat acut competit data first though get score datafram order plot cours train
3568,We can also find the best scores for plotting the best hyperparameter values.,also find best score plot best hyperparamet valu
3569,"Below is the code showing the progress of scores versus the iteration. For random search we do not expect to see a pattern, but for Bayesian optimization, we expect to see the scores increasing with the search as more promising hyperparameter values are tried.",code show progress score versu iter random search expect see pattern bayesian optim expect see score increas search promis hyperparamet valu tri
3570,"Sure enough, we see that the Bayesian hyperparameter optimization scores increase as the search continues. This shows that more promising values (at least on the cross validation reduced dataset) were tried as the search progressed. Random search does record a better score, but the results do not improve over the course of the search. In this case, it looks like if we were to continue searching with Bayesian optimization, we would eventually reach higher scores on the cross vadidation data. 

For fun, we can make the same plot in Altair.",sure enough see bayesian hyperparamet optim score increas search continu show promis valu least cross valid reduc dataset tri search progress random search record better score result improv cours search case look like continu search bayesian optim would eventu reach higher score cross vadid data fun make plot altair
3571,"Same chart, just in a different library for practice! 

## Learning Rate Distribution

Next we can start plotting the distributions of hyperparameter values searched. We expect random search to align with the search domain, while the Bayesian hyperparameter optimization should tend to focus on more promising values, wherever those happen to be in the search domain.

The dashed vertical lines indicate the ""optimal"" value of the hyperparameter.",chart differ librari practic learn rate distribut next start plot distribut hyperparamet valu search expect random search align search domain bayesian hyperparamet optim tend focu promis valu wherev happen search domain dash vertic line indic optim valu hyperparamet
3572,"## Distribution of all Numeric Hyperparameters

We can make the same chart now for all of the hyperparameters. For each setting, we plot the values tried by random search and bayesian optimization, as well as the sampling distirbution.",distribut numer hyperparamet make chart hyperparamet set plot valu tri random search bayesian optim well sampl distirbut
3573,"## Evolution of Search

An interesting series of plots to make is the evolution of the hyperparameters over the search. This can show us what values the Bayesian optimization tended to focus on. The average cross validation score continued to improve throughout Bayesian optimization, indicating that ""more promising"" values of the hyperparameters were being evaluated and maybe a longer search would prove useful (or there could be a plateau in the validation scores with a longer search).",evolut search interest seri plot make evolut hyperparamet search show u valu bayesian optim tend focu averag cross valid score continu improv throughout bayesian optim indic promis valu hyperparamet evalu mayb longer search would prove use could plateau valid score longer search
3574,The final plot is just a bar chart of the `boosting_type`. ,final plot bar chart boost type
3575,"The Bayes optimization spent many more iterations using the `dart` boosting type than would be expected from a uniform distribution. We can use information such as this in further hyperparameter tuning. For example, we could use the distributions from Bayesian hyperparameter optimization to make a more focused hyperparameter grid for grid or even random search. 

![](http://)For this chart, we can also make it in Altair for the practice.",bay optim spent mani iter use dart boost type would expect uniform distribut use inform hyperparamet tune exampl could use distribut bayesian hyperparamet optim make focus hyperparamet grid grid even random search http chart also make altair practic
3576,"## Applied to Full Dataset

Now, we can take the best hyperparameters found from 1000 iterations of Bayesian hyperparameter optimization on the smaller dataset and apply these to a full dataset of features from the `[Updated 0.792 LB] LightGBM with Simple Features`(https://www.kaggle.com/jsaguiar/updated-0-792-lb-lightgbm-with-simple-features) kernel. The best hyperparameters from the smaller dataset will not necessarily be the best on the full dataset (because the small dataset does nto perfectly represent the entire data), but we can at least try them out. We will train a model using the optimal hyperparameters from Bayesian optimization using early stopping to determine the number of estimators. ",appli full dataset take best hyperparamet found 1000 iter bayesian hyperparamet optim smaller dataset appli full dataset featur updat 0 792 lb lightgbm simpl featur http www kaggl com jsaguiar updat 0 792 lb lightgbm simpl featur kernel best hyperparamet smaller dataset necessarili best full dataset small dataset nto perfectli repres entir data least tri train model use optim hyperparamet bayesian optim use earli stop determin number estim
3577,### Random Search on the Full Dataset,random search full dataset
3578,Then we can make predictions on the test data. The predictions are saved to a csv file that can be submitted to the competition.,make predict test data predict save csv file submit competit
3579,Submitting these to the competition results in a score of __0.787__ which compares to the original score from the kernel of __0.792__,submit competit result score 0 787 compar origin score kernel 0 792
3580,### Bayesian Optimization on the Full Dataset,bayesian optim full dataset
3581,"Submitting these to the competition results in a score of __0.792__. So the Bayesian optimization outperforms the random search based on 1000 iterations on a reduced sized dataset. I wouldn't put too much weight into these results because we saw that random search actually yielded a higher cross validation score. Nonetheless, Bayesian hyperparameter optimization can be an effective technique for automated machine learning model tuning.",submit competit result score 0 792 bayesian optim outperform random search base 1000 iter reduc size dataset put much weight result saw random search actual yield higher cross valid score nonetheless bayesian hyperparamet optim effect techniqu autom machin learn model tune
3582,"# Conclusions

Bayesian optimization is one method for automated hyperparameter tuning. Automated hyperparameter tuning aims to find the best hyperparameter values for a machine learning model on a given dataset with no input from the data scientist beyond initial set-up required. Bayesian optimization uses Bayesian reasoning to build a probability model of the objecitve function $P(\text{score} | \text{hyperparameters})$ which is then used to select the next hyperparameter values to evaluate. The concept is to use more search iterations evaluating promising hyperparameter values by reasoning from the past results. This is an intuitive method of hyperparamter optimization that works in much the same way a human does to get better at any situation: learn from past experiences! If everything works as expected, Bayesian hyperparameter optimization can result in:

* Better generalization performance on the test set
* Fewer iterations than random or grid search require

Even if Bayesian optimization (or other automated hyperparameter tuning methods) does not deliver on the above points in all situations, it is a useful skill to master as a data scientist. In the future, data scientists are not going to spend valuable time tweaking model hyperparameters, and knowing methods for accomplishing this automatically will go a long way in your career or studies! Feel free to take this code and apply it to any dataset or to a different machine learning model. I am currently running these methods on a full dataset and will share the results when they are completed. I look forward to the next notebook! 

As always, I welcome feedback and constructive criticism. I write for Towards Data Science at https://medium.com/@williamkoehrsen/ and can be reached on Twitter at https://twitter.com/koehrsen_will. Thanks for reading to the end and keep making the data science community such a wonderful place for learning and sharing, 

Will",conclus bayesian optim one method autom hyperparamet tune autom hyperparamet tune aim find best hyperparamet valu machin learn model given dataset input data scientist beyond initi set requir bayesian optim use bayesian reason build probabl model objecitv function p text score text hyperparamet use select next hyperparamet valu evalu concept use search iter evalu promis hyperparamet valu reason past result intuit method hyperparamt optim work much way human get better situat learn past experi everyth work expect bayesian hyperparamet optim result better gener perform test set fewer iter random grid search requir even bayesian optim autom hyperparamet tune method deliv point situat use skill master data scientist futur data scientist go spend valuabl time tweak model hyperparamet know method accomplish automat go long way career studi feel free take code appli dataset differ machin learn model current run method full dataset share result complet look forward next notebook alway welcom feedback construct critic write toward data scienc http medium com williamkoehrsen reach twitter http twitter com koehrsen thank read end keep make data scienc commun wonder place learn share
3583,"# Clean Manual Feature Engineering

The purpose of this notebook is to clean up the manual feature engineering I had scattered over several other kernels. We will implement the complete manual feature engineering and then test the results.

Update August 7: __After some modifications, this can now run in a kernel!__ The features themselves are available at https://www.kaggle.com/willkoehrsen/home-credit-manual-engineered-features under `clean_manual.csv`. The feature importances for these features in a gradient boosting model are also available at the same link with the name `fi_clean_manual.csv`. 

### Roadmap

Our plan of action is as follows.We have to be very careful about memory usage in the kernels, which affects the order of operations:

1. Define functions:
    * `agg_numeric`
    * `agg_categorical`
    * `agg_child` 
    * `agg_grandchild`
 2. Add in domain knowledge features to `app`
 3. Work through the `bureau` and `bureau_balance` data
     * Add in hand built features
     * Aggregate both using the appropriate functions
     * Merge with `app` and delete the dataframes
4. Work through `previous`, `installments`, `cash`, and `credit`
    * Add in hand built features
    * Aggregate using the appropriate functions
    * Merge with `app` and delete the dataframes
5. Modeling using a Gradient Boosting Machine
    * Train model on training data using best hyperparameters from random search notebook
    * Make predictions and submit


",clean manual featur engin purpos notebook clean manual featur engin scatter sever kernel implement complet manual featur engin test result updat august 7 modif run kernel featur avail http www kaggl com willkoehrsen home credit manual engin featur clean manual csv featur import featur gradient boost model also avail link name fi clean manual csv roadmap plan action follow care memori usag kernel affect order oper 1 defin function agg numer agg categor agg child agg grandchild 2 add domain knowledg featur app 3 work bureau bureau balanc data add hand built featur aggreg use appropri function merg app delet datafram 4 work previou instal cash credit add hand built featur aggreg use appropri function merg app delet datafram 5 model use gradient boost machin train model train data use best hyperparamet random search notebook make predict submit
3584,"# Numeric Aggregation Function

The following function aggregates all the numeric variables in a child dataframe at the parent level. That is, for each parent, gather together (group) all of their children, and calculate the aggregations statistics across the children. The function also removes any columns that share the exact same values (which might happen using `count`). ",numer aggreg function follow function aggreg numer variabl child datafram parent level parent gather togeth group child calcul aggreg statist across child function also remov column share exact valu might happen use count
3585,"# Categorical Aggregation Function

Much like the numerical aggregation function, the `agg_categorical` function works on a child dataframe to aggregate statistics at the parent level. This can work with any child of `app` and might even be extensible to other problems with only minor changes in syntax.",categor aggreg function much like numer aggreg function agg categor function work child datafram aggreg statist parent level work child app might even extens problem minor chang syntax
3586,"# Combined Aggregation Function

We can put these steps together into a function that will handle a child dataframe. The function will take care of both the numeric and categorical variables and will return the result of merging the two dataframes. ",combin aggreg function put step togeth function handl child datafram function take care numer categor variabl return result merg two datafram
3587,"This function can be applied to both `bureau` and `previous` because these are direct children of `app`. For the children of the children, we will need to take an additional aggregation step. ",function appli bureau previou direct child app child child need take addit aggreg step
3588,"# Aggregate Grandchild Data Tables

Several of the tables (`bureau_balance, cash, credit_card`, and `installments`) are children of the child dataframes. In other words, these are grandchildren of the main `app` data table. To aggregate these tables, they must first be aggregated at the parent level (which is on a per loan basis) and then at the grandparent level (which is on the client basis). For example, in the `bureau_balance` dataframe, there is monthly information on the loans in `bureau`. To get this data into the `app` dataframe will first require grouping the monthly information for each loan and then grouping the loans for each client. 

Hopefully, the nomenclature does not get too confusing, but here's a rounddown:

* __grandchild__: the child of a child data table, for instance, `bureau_balance`. For every row in the child table, there can be multiple rows in the grandchild. 
* __parent__: the parent table of the grandchild that links the grandchild to the grandparent. For example, the `bureau` dataframe is the parent of the `bureau_balance` dataframe in this situation. `bureau` is in turn the child of the `app` dataframe. `bureau_balance` can be connected to `app` through `bureau`.
* __grandparent__: the parent of the parent of the grandchild, in this problem the `app` dataframe. The end goal is to aggregate the information in the grandchild into the grandparent. This will be done in two stages: first at the parent (loan) level and then at the grandparent (client) level
* __parent variable__: the variable linking the grandchild to the parent. For the `bureau` and `bureau_balance` data this is `SK_ID_BUREAU` which uniquely identifies each previous loan
* __grandparent variable__: the variable linking the parent to the grandparent. This is `SK_ID_CURR` which uniquely identifies each client in `app`.

### Aggregating Grandchildren Function

We can take the individual steps required for aggregating a grandchild dataframe at the grandparent level in a function. These are:

1. Aggregate the numeric variables at the parent (the loan, `SK_ID_BUREAU` or `SK_ID_PREV`) level.
2. Merge with the parent of the grandchild to get the grandparent variable in the data (for example `SK_ID_CURR`)
3. Aggregate the numeric variables at the grandparent (the client, `SK_ID_CURR`) level. 
4. Aggregate the categorical variables at the parent level.
5. Merge the aggregated data with the parent to get the grandparent variable
6. Aggregate the categorical variables at the grandparent level
7. Merge the numeric and categorical dataframes on the grandparent varible
8. Remove the columns with all duplicated values.
9. The resulting dataframe should now have one row for every grandparent (client) observation
10. Merge with the main dataframe (`app`) on the grandparent variable (`SK_ID_CURR`). 

This function can be applied to __all 4 grandchildren__ without the need for hard-coding in specific variables. ",aggreg grandchild data tabl sever tabl bureau balanc cash credit card instal child child datafram word grandchild main app data tabl aggreg tabl must first aggreg parent level per loan basi grandpar level client basi exampl bureau balanc datafram monthli inform loan bureau get data app datafram first requir group monthli inform loan group loan client hope nomenclatur get confus rounddown grandchild child child data tabl instanc bureau balanc everi row child tabl multipl row grandchild parent parent tabl grandchild link grandchild grandpar exampl bureau datafram parent bureau balanc datafram situat bureau turn child app datafram bureau balanc connect app bureau grandpar parent parent grandchild problem app datafram end goal aggreg inform grandchild grandpar done two stage first parent loan level grandpar client level parent variabl variabl link grandchild parent bureau bureau balanc data sk id bureau uniqu identifi previou loan grandpar variabl variabl link parent grandpar sk id curr uniqu identifi client app aggreg grandchild function take individu step requir aggreg grandchild datafram grandpar level function 1 aggreg numer variabl parent loan sk id bureau sk id prev level 2 merg parent grandchild get grandpar variabl data exampl sk id curr 3 aggreg numer variabl grandpar client sk id curr level 4 aggreg categor variabl parent level 5 merg aggreg data parent get grandpar variabl 6 aggreg categor variabl grandpar level 7 merg numer categor datafram grandpar varibl 8 remov column duplic valu 9 result datafram one row everi grandpar client observ 10 merg main datafram app grandpar variabl sk id curr function appli 4 grandchild without need hard code specif variabl
3589,"# Putting it Together

Now that we have the individual pieces of semi-automated feature engineering, we need to put them together. There are two functions that can handle the children and the grandchildren data tables:

1. `agg_child(df, parent_var, df_name)`: aggregate the numeric and categorical variables of a child dataframe at the parent level. For example, the `previous` dataframe is a child of the `app` dataframe that must be aggregated for each client. 
2. `agg_grandchild(df, parent_df, parent_var, grandparent_var, df_name)`: aggregate the numeric and categorical variables of a grandchild dataframe at the grandparent level. For example, the `bureau_balance` dataframe is the grandchild of the `app` dataframe with `bureau` as the parent. 

For each of the children dataframes of `app`, (`previous` and `bureau`), we will use the first function and merge the result into the `app` on the parent variable, `SK_ID_CURR`. For the four grandchild dataframes, we will use the second function, which returns a single dataframe that can then be merged into app on `SK_ID_CURR`. ",put togeth individu piec semi autom featur engin need put togeth two function handl child grandchild data tabl 1 agg child df parent var df name aggreg numer categor variabl child datafram parent level exampl previou datafram child app datafram must aggreg client 2 agg grandchild df parent df parent var grandpar var df name aggreg numer categor variabl grandchild datafram grandpar level exampl bureau balanc datafram grandchild app datafram bureau parent child datafram app previou bureau use first function merg result app parent variabl sk id curr four grandchild datafram use second function return singl datafram merg app sk id curr
3590,"## Hand-Built Features

Along the way, we will add in hand-built features to the datasets. These have come from my own ideas (probably not very optimal) and from the community.

First we will add in ""domain knowledge"" features to the `app` dataframe. These were developed based on work done in other kernels (both from the community and my own work)",hand built featur along way add hand built featur dataset come idea probabl optim commun first add domain knowledg featur app datafram develop base work done kernel commun work
3591,"### Hand-Built Features for other Dataframes

We can also add in hand built features for the other dataframes. Since these are not the main dataframe, these features will end up being aggregated in different ways. These will be added as we go through the tables.",hand built featur datafram also add hand built featur datafram sinc main datafram featur end aggreg differ way ad go tabl
3592,"#### Aggregate the bureau data

First add the loan rate for previous loans at other institutions.",aggreg bureau data first add loan rate previou loan institut
3593,"#### Aggregate the bureau balance

Now we turn to the `bureau_balance` dataframe. We will make a column indicating whether a loan was past due for the month or whether the payment was on time.",aggreg bureau balanc turn bureau balanc datafram make column indic whether loan past due month whether payment time
3594,"## Merge with the main dataframe

The individual dataframes can all be merged into the main `app` dataframe. Merging is much quicker if done on any index, so it's good practice to first set the index to the variable on which we will merge. In each case, we use a `left` join so that all the observations in `app` are kept even if they are not present in the other dataframes (which occurs because not every client has previous records at Home Bureau or other credit institutions). After each step of mergning, we remove the dataframe from memory in order to hopefully let the kernel continue to run.

The final result is one dataframe with a single row for each client that can be used for training a machine learning model. ",merg main datafram individu datafram merg main app datafram merg much quicker done index good practic first set index variabl merg case use left join observ app kept even present datafram occur everi client previou record home bureau credit institut step mergn remov datafram memori order hope let kernel continu run final result one datafram singl row client use train machin learn model
3595,"#### Aggregate previous loans at Home Credit

We will add in two domain features, first the loan rate and then the difference between the amount applied for and the amount awarded.",aggreg previou loan home credit add two domain featur first loan rate differ amount appli amount award
3596,`AMT_DIFFERENCE` is the difference between what was given to the client and what the client requested on previous loans at Home Credit.,amt differ differ given client client request previou loan home credit
3597,"#### Aggregate Installments Data

The installments table has each installment (payment) for previous loans at Home Credit. We can create a column indicating whether or not a loan was late.",aggreg instal data instal tabl instal payment previou loan home credit creat column indic whether loan late
3598,`LOW_PAYMENT` represents a payment that was less than the prescribed amount. ,low payment repres payment le prescrib amount
3599,"#### Aggregate Cash previous loans

The next dataframe is the `cash` which has monthly information on previous cash loans at Home Credit. We can create a column indicating if the loan was overdue for the month. ",aggreg cash previou loan next datafram cash monthli inform previou cash loan home credit creat column indic loan overdu month
3600,`INSTALLMENTS_PAID` is meant to represent the number of already paid (or I guess missed) installments by subtracting the future installments from the total installments.,instal paid meant repres number alreadi paid guess miss instal subtract futur instal total instal
3601,"#### Aggregate Credit previous loans

The last dataframe is `credit` which has previous credit card loans at Home Credit. We can make a column indicating whether the balance is greater than the credit limit, a column showing whether or not the balance was cleared (equal to 0), whether or not the payment was below the prescribed amount, and whether or not the payment was behind. Then we aggregate as with the other grandchildren.",aggreg credit previou loan last datafram credit previou credit card loan home credit make column indic whether balanc greater credit limit column show whether balanc clear equal 0 whether payment prescrib amount whether payment behind aggreg grandchild
3602,"__This is usually the point at which the kernel fails.__ To try and alleviate the problem, I have added a pause of 10 minutes.",usual point kernel fail tri allevi problem ad paus 10 minut
3603,__Update August 7__: The kernel can now run!,updat august 7 kernel run
3604,"# Modeling

After all the hard work, now we get to test our features! We will use a model with the hyperparameters from random search that are documented in another notebook. 

The final model scores __0.792__ when uploaded to the competition.",model hard work get test featur use model hyperparamet random search document anoth notebook final model score 0 792 upload competit
3605,"## Feature Importances

Now we can see if all that time was worth it! In the code below, we find the most important features and show them in a plot and dataframe.",featur import see time worth code find import featur show plot datafram
3606,"# Conclusions

This code is a little too much to run in the Kaggle kernels. However, the features themselves are available at https://www.kaggle.com/willkoehrsen/home-credit-manual-engineered-features under `clean_manual.csv`. The feature importances for these features in a gradient boosting model are also available at the same link with the name `fi_clean_manual.csv`. 

This notebook is meant to serve as a clean version of the manual feature engineering I had scattered across several other notebooks. We were able to build a complete set of __ features that scored 0.792 on the public leaderboard__. Further hyperparameter tuning might improve the performance. For additional feature engineering, we will probably want to turn to more technical operations such as treating this as a time-series problem. Since we have relative time information (relative to the current loan at Home Credit), it's possible to find the most recent information and also trends over time. These can be useful because changes in behavior might inform us as to whether or not a client will be able to repay a loan! 

Thanks for reading and as always, I welcome feedback and constructive criticism. I'll see you in the next notebook.

Best,

Will",conclus code littl much run kaggl kernel howev featur avail http www kaggl com willkoehrsen home credit manual engin featur clean manual csv featur import featur gradient boost model also avail link name fi clean manual csv notebook meant serv clean version manual featur engin scatter across sever notebook abl build complet set featur score 0 792 public leaderboard hyperparamet tune might improv perform addit featur engin probabl want turn technic oper treat time seri problem sinc rel time inform rel current loan home credit possibl find recent inform also trend time use chang behavior might inform u whether client abl repay loan thank read alway welcom feedback construct critic see next notebook best
3607,"# Introduction: Hyperparameter Tuning using Grid and Random Search

In this notebook, we will explore two methods for hyperparameter tuning a machine learning model. [In contrast](https://machinelearningmastery.com/difference-between-a-parameter-and-a-hyperparameter/) to model __parameters__ which are learned during training, model __hyperparameters__ are set by the data scientist ahead of training and control implementation aspects of the model. The weights learned during training of a linear regression model are parameters while the number of trees in a random forest is a model hyperparameter because this is set by the data scientist. Hyperparameters can be thought of as model settings. These settings need to be tuned for each problem because the best model hyperparameters for one particular dataset will __not be__ the best across all datasets. The process of [hyperparameter tuning (also called hyperparameter optimization)](https://en.wikipedia.org/wiki/Hyperparameter_optimization) means finding the combination of hyperparameter values for a machine learning model that performs the best - as measured on a validation dataset - for a problem. 

(__Quick Note__: a lot of data scientists use the terms _parameters_ and _hyperparameters_ interchangeably to refer to the model settings. While this is technically incorrect, it's pretty common practice and it's usually possible to tell when they are referring to parameters learned during training versus hyperparameters. I'll try to stick to using model hyperparameters or model settings and I'll  point out when I'm talking about a parameter that is learned during training. If you're still confused, [this article](https://machinelearningmastery.com/difference-between-a-parameter-and-a-hyperparameter/) may help you out!)

__Additional Notebooks__ 

If you haven't checked out my other work on this problem, here is a complete list of the notebooks I have completed so far:

* [A Gentle Introduction](https://www.kaggle.com/willkoehrsen/start-here-a-gentle-introduction)
* [Manual Feature Engineering Part One](https://www.kaggle.com/willkoehrsen/introduction-to-manual-feature-engineering)
* [Manual Feature Engineering Part Two](https://www.kaggle.com/willkoehrsen/introduction-to-manual-feature-engineering-p2)
* [Introduction to Automated Feature Engineering](https://www.kaggle.com/willkoehrsen/automated-feature-engineering-basics)
* [Advanced Automated Feature Engineering](https://www.kaggle.com/willkoehrsen/tuning-automated-feature-engineering-exploratory)
* [Feature Selection](https://www.kaggle.com/willkoehrsen/introduction-to-feature-selection)
* [Intro to Model Tuning: Grid and Random Search](https://www.kaggle.com/willkoehrsen/intro-to-model-tuning-grid-and-random-search)
* [Automated Model Tuning](https://www.kaggle.com/willkoehrsen/automated-model-tuning)

There are several approaches to hyperparameter tuning

1. __Manual__: select hyperparameters based on intuition/experience/guessing, train the model with the hyperparameters, and score on the validation data. Repeat process until you run out of patience or are satisfied with the results. 
2. __Grid Search__: set up a grid of hyperparameter values and for each combination, train a model and score on the validation data. In this approach, every single combination of hyperparameters values is tried which can be very inefficient!
3. __Random search__: set up a grid of hyperparameter values and select _random_ combinations to train the model and score. The number of search iterations is set based on time/resources. 
4. __Automated Hyperparameter Tuning__: use methods such as gradient descent, Bayesian Optimization, or evolutionary algorithms to conduct a guided search for the best hyperparameters.

(This [Wikipedia Article](https://en.wikipedia.org/wiki/Hyperparameter_optimization) provides a good high-level overview of tuning options with links for more details)

In this notebook, we will implement approaches 2 and 3 for a Gradient Boosting Machine Learning Model. In a future notebook, we will implement automated hyperparameter tuning using Bayesian optimization, specifically the Hyperopt library. If you want to get an idea of how automated hyperparameter tuning is done, check out [this article](https://towardsdatascience.com/automated-machine-learning-hyperparameter-tuning-in-python-dfda59b72f8a). 

## Model: Gradient Boosting Machine 

The [Gradient Boosting Machine (GBM)](https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/) has recently emerged as one of the top machine learning models. The GBM is extremely effective on structured data - where the information is in rows and columns - and medium sized datasets - where there are at most a few million observations. We will focus on this model because it is currently the top performing method for most competitions on Kaggle and because the performance is highly dependent on the hyperparameter choices. The basics you need to know about the GBM are that it is an ensemble method that works by training many individual learners, almost always decision trees. However, unlike in a random forest where the trees are trained in __parallel__, in a GBM, the trees are trained __sequentially__ with each tree learning from the mistakes of the previous ones. The hundreds or thousands of weak learners are combined to make a single strong ensemble learner with the contributions of each individual learned during training using Gradient Descent (the weights of the individual trees would therefore be a model _parameter_). 

The GBM [has many hyperparameters to tune](http://lightgbm.readthedocs.io/en/latest/Parameters.html) that control both the overall ensemble (such as the learning rate) and the individual decision trees (such as the number of leaves in the tree or the maximum depth of the tree). It is difficult to know which combination of hyperparameters will work best based only on theory because there are complex interactions between hyperparameters. Hence the need for hyperparameter tuning: the only way to find the optimal hyperparameter values is to try many different combinations on a dataset!

We will use the implementation of the Gradient Boosting Machine in the [LightGBM library](http://lightgbm.readthedocs.io/en/latest/). This is a much faster (and some say more accurate) implementation than that available in Scikit-Learn.

For more details of the Gradient Boosting Machine (GBM), check out this [high-level blog post](http://blog.kaggle.com/2017/01/23/a-kaggle-master-explains-gradient-boosting/), or this [in depth technical article.](https://brage.bibsys.no/xmlui/bitstream/handle/11250/2433761/16128_FULLTEXT.pdf) 

### Getting Started

With the necessary background out of the way, let's get started. For this notebook, we will work with a subset of the data consisting of 10000 rows. Hyperparameter tuning is extremely computationally expensive and working with the full dataset in a Kaggle Kernel would not be feasible for more than a few search iterations. However, the same ideas that we will implement here can be applied to the full dataset and while this notebook is specifically aimed at the GBM, the methods can be applied for any machine learning model. 

To ""test"" the tuning results, we will save some of the training data, 6000 rows, as a separate testing set. When we do hyperparameter tuning, it's crucial to __not tune the hyperparameters on the testing data__. We can only use the testing data __a single time__ when we evaluate the final model that has been tuned on the validation data. To actually test our methods from this notebook, we would need to train the best model on all of the training data, make predictions on the actual testing data, and then submit our answers to the competition. ",introduct hyperparamet tune use grid random search notebook explor two method hyperparamet tune machin learn model contrast http machinelearningmasteri com differ paramet hyperparamet model paramet learn train model hyperparamet set data scientist ahead train control implement aspect model weight learn train linear regress model paramet number tree random forest model hyperparamet set data scientist hyperparamet thought model set set need tune problem best model hyperparamet one particular dataset best across dataset process hyperparamet tune also call hyperparamet optim http en wikipedia org wiki hyperparamet optim mean find combin hyperparamet valu machin learn model perform best measur valid dataset problem quick note lot data scientist use term paramet hyperparamet interchang refer model set technic incorrect pretti common practic usual possibl tell refer paramet learn train versu hyperparamet tri stick use model hyperparamet model set point talk paramet learn train still confus articl http machinelearningmasteri com differ paramet hyperparamet may help addit notebook check work problem complet list notebook complet far gentl introduct http www kaggl com willkoehrsen start gentl introduct manual featur engin part one http www kaggl com willkoehrsen introduct manual featur engin manual featur engin part two http www kaggl com willkoehrsen introduct manual featur engin p2 introduct autom featur engin http www kaggl com willkoehrsen autom featur engin basic advanc autom featur engin http www kaggl com willkoehrsen tune autom featur engin exploratori featur select http www kaggl com willkoehrsen introduct featur select intro model tune grid random search http www kaggl com willkoehrsen intro model tune grid random search autom model tune http www kaggl com willkoehrsen autom model tune sever approach hyperparamet tune 1 manual select hyperparamet base intuit experi guess train model hyperparamet score valid data repeat process run patienc satisfi result 2 grid search set grid hyperparamet valu combin train model score valid data approach everi singl combin hyperparamet valu tri ineffici 3 random search set grid hyperparamet valu select random combin train model score number search iter set base time resourc 4 autom hyperparamet tune use method gradient descent bayesian optim evolutionari algorithm conduct guid search best hyperparamet wikipedia articl http en wikipedia org wiki hyperparamet optim provid good high level overview tune option link detail notebook implement approach 2 3 gradient boost machin learn model futur notebook implement autom hyperparamet tune use bayesian optim specif hyperopt librari want get idea autom hyperparamet tune done check articl http towardsdatasci com autom machin learn hyperparamet tune python dfda59b72f8a model gradient boost machin gradient boost machin gbm http machinelearningmasteri com gentl introduct gradient boost algorithm machin learn recent emerg one top machin learn model gbm extrem effect structur data inform row column medium size dataset million observ focu model current top perform method competit kaggl perform highli depend hyperparamet choic basic need know gbm ensembl method work train mani individu learner almost alway decis tree howev unlik random forest tree train parallel gbm tree train sequenti tree learn mistak previou one hundr thousand weak learner combin make singl strong ensembl learner contribut individu learn train use gradient descent weight individu tree would therefor model paramet gbm mani hyperparamet tune http lightgbm readthedoc io en latest paramet html control overal ensembl learn rate individu decis tree number leav tree maximum depth tree difficult know combin hyperparamet work best base theori complex interact hyperparamet henc need hyperparamet tune way find optim hyperparamet valu tri mani differ combin dataset use implement gradient boost machin lightgbm librari http lightgbm readthedoc io en latest much faster say accur implement avail scikit learn detail gradient boost machin gbm check high level blog post http blog kaggl com 2017 01 23 kaggl master explain gradient boost depth technic articl http brage bibsi xmlui bitstream handl 11250 2433761 16128 fulltext pdf get start necessari background way let get start notebook work subset data consist 10000 row hyperparamet tune extrem comput expens work full dataset kaggl kernel would feasibl search iter howev idea implement appli full dataset notebook specif aim gbm method appli machin learn model test tune result save train data 6000 row separ test set hyperparamet tune crucial tune hyperparamet test data use test data singl time evalu final model tune valid data actual test method notebook would need train best model train data make predict actual test data submit answer competit
3608,"Below we read in the data and separate into a training set of 10000 observations and a ""testing set"" of 6000 observations. After creating the testing set, we cannot do any hyperparameter tuning with it! ",read data separ train set 10000 observ test set 6000 observ creat test set hyperparamet tune
3609,"We will also use only the numeric features to reduce the number of dimensions which will help speed up the hyperparameter search. Again, this is something we would not want to do on a real problem, but for demonstration purposes, it will allow us to see the concepts in practice (rather than waiting days/months for the search to finish).",also use numer featur reduc number dimens help speed hyperparamet search someth would want real problem demonstr purpos allow u see concept practic rather wait day month search finish
3610,"# Cross Validation

To evaluate each combination of hyperparameter values, we need to score them on a validation set. The hyperparameters __can not be tuned on the testing data__. We can only use the testing data __once__ when we evaluate the final model. The testing data is meant to serve as an estimate of the model performance when deployed on real data, and therefore we do not want to optimize our model to the testing data because that will not give us a fair estimate of the actual performance. The correct approach is therefore to use a validation set. However, instead of splitting the valuable training data into a separate training and validation set, we use [KFold cross validation](https://www.youtube.com/watch?v=TIgfjmp-4BA). In addition to preserving training data, this should give us a better estimate of generalization performance on the test set than using a single validation set (since then we are probably overfitting to that validation set). The performance of each set of hyperparameters is determined by Receiver Operating Characteristic Area Under the Curve (ROC AUC) from the cross-validation.

In this example, we will use 5-fold cross validation which means training and testing the model with each set of hyperparameter values 5 times to assess performance. Part of the reason why hyperparameter tuning is so time-consuming is because of the use of cross validation. If we have a [large enough training set, we can probably get away with just using a single separate validation set](https://www.coursera.org/lecture/deep-neural-network/train-dev-test-sets-cxG1s), but cross validation is a safer method to avoid overfitting. 

To implement KFold cross validation, we will use the LightGBM cross validation function, `cv`, because this allows us to use a critical technique for training a GBM, early stopping. (For other machine learning models where we do not need to use early stopping, we can use the Scikit-Learn functions `RandomizedSearchCV` or `GridSearchCV`.)

## Early Stopping

One of the most important hyperparameters in a Gradient Boosting Machine is the number of estimators (the number of decision trees trained sequentially). We could set this as another hyperparameter in our search, but there's a better method: [early stopping](https://en.wikipedia.org/wiki/Early_stopping). Early stopping means training until the validation error does not decrease for a specified number of iterations. In the case of the GBM, this means training more decision trees, and in this example, we will use early stopping with 100 rounds, meaning that the training will continue until validation error has not decreased for 100 rounds. Then, the number of estimators that yielded the best score on the validation data will be chosen as the number of estimators to use in the final model.

The concept of early stopping is commonly applied to the GBM and to deep neural networks so it's a great technique to understand. This is one of many forms of regularization that aims to improve generalization performance on the testing set by not overfitting to the training data. If we keep adding estimators, the training error will always decrease because the capacity of the model increases. Although this might seem positive, it means that the model will start to memorize the training data and then will not perform well on new testing data. The __variance__ of the model increases as we continue adding estimators because the model starts to rely too heavily on the training data (high variance means overfitting).

Early stopping is simple to implement with the LightGBM library in the cross validation function. We simply need to pass in the number of early stopping rounds.

### Example of Cross Validation and Early Stopping 

To use the `cv` function, we first need to make a LightGBM `dataset`. ",cross valid evalu combin hyperparamet valu need score valid set hyperparamet tune test data use test data evalu final model test data meant serv estim model perform deploy real data therefor want optim model test data give u fair estim actual perform correct approach therefor use valid set howev instead split valuabl train data separ train valid set use kfold cross valid http www youtub com watch v tigfjmp 4ba addit preserv train data give u better estim gener perform test set use singl valid set sinc probabl overfit valid set perform set hyperparamet determin receiv oper characterist area curv roc auc cross valid exampl use 5 fold cross valid mean train test model set hyperparamet valu 5 time ass perform part reason hyperparamet tune time consum use cross valid larg enough train set probabl get away use singl separ valid set http www coursera org lectur deep neural network train dev test set cxg1 cross valid safer method avoid overfit implement kfold cross valid use lightgbm cross valid function cv allow u use critic techniqu train gbm earli stop machin learn model need use earli stop use scikit learn function randomizedsearchcv gridsearchcv earli stop one import hyperparamet gradient boost machin number estim number decis tree train sequenti could set anoth hyperparamet search better method earli stop http en wikipedia org wiki earli stop earli stop mean train valid error decreas specifi number iter case gbm mean train decis tree exampl use earli stop 100 round mean train continu valid error decreas 100 round number estim yield best score valid data chosen number estim use final model concept earli stop commonli appli gbm deep neural network great techniqu understand one mani form regular aim improv gener perform test set overfit train data keep ad estim train error alway decreas capac model increas although might seem posit mean model start memor train data perform well new test data varianc model increas continu ad estim model start reli heavili train data high varianc mean overfit earli stop simpl implement lightgbm librari cross valid function simpli need pas number earli stop round exampl cross valid earli stop use cv function first need make lightgbm dataset
3611,"We have to pass in a set of hyperparameters to the cross validation, so we will use the default hyperparameters in LightGBM. In the `cv` call, the `num_boost_round` is set to 10,000 (`num_boost_round` is the same as `n_estimators`), but this number won't actually be reached because we are using early stopping. As a reminder, the metric we are using is Receiver Operating Characteristic Area Under the Curve (ROC AUC).

The code below carries out both cross validation with 5 folds and early stopping with 100 early stopping rounds. ",pas set hyperparamet cross valid use default hyperparamet lightgbm cv call num boost round set 10 000 num boost round n estim number actual reach use earli stop remind metric use receiv oper characterist area curv roc auc code carri cross valid 5 fold earli stop 100 earli stop round
3612,"The `cv_results` is a dictionary with lists for the `metric` mean and the `metric` standard deviation. The last entry (index of -1) contains the best performing score. The length of each list in the dictionary will be the ""optimal"" number of estimators to train.",cv result dictionari list metric mean metric standard deviat last entri index 1 contain best perform score length list dictionari optim number estim train
3613,"We can use this result as a baseline model to beat. To find out how well the model does on our ""test"" data, we will retrain it on all the training data with the best number of estimators found during cross validation with early stopping.",use result baselin model beat find well model test data retrain train data best number estim found cross valid earli stop
3614,This is the baseline score _before hyperparameter tuning_. The only difference we made from the default model was using early stopping to set the number of estimators (which by default is 100). ,baselin score hyperparamet tune differ made default model use earli stop set number estim default 100
3615,"## Hyperparameter Tuning Implementation

Now we have the basic framework in place: we will use cross validation to determine the performance of model hyperparameters and early stopping with the GBM so we do not have to tune the number of estimators. The basic strategy for both grid and random search is simple: for each hyperparameter value combination, evaluate the cross validation score and record the results along with the hyperparameters. Then, at the end of searching, choose the hyperparameters that yielded the highest cross-validation score, train the model on all the training data, and make predictions on the test data.

# Four parts of Hyperparameter tuning

It's helpful to think of hyperparameter tuning as having four parts (these four parts also will form the basis of Bayesian Optimization):

1. Objective function: a function that takes in hyperparameters and returns a score we are trying to minimize or maximize
2. Domain: the set of hyperparameter values over which we want to search. 
3. Algorithm: method for selecting the next set of hyperparameters to evaluate in the objective function.
4. Results history: data structure containing each set of hyperparameters and the resulting score from the objective function.

Switching from grid to random search to Bayesian optimization will only require making minor modifications to these four parts. 

## Objective Function

The objective function takes in hyperparameters and outputs a value representing a score. Traditionally in optimization, this is a score to minimize, but here our score will be the ROC AUC which of course we want to maximize. Later, when we get to Bayesian Optimization, we will have to use a value to minimize, so we can take $1 - \text{ROC AUC}$ as the score. What occurs in the middle of the objective function will vary according to the problem, but for this problem, we will use cross validation with the specified model hyperparameters to get the cross-validation ROC AUC. This score will then be used to select the best model hyperparameter values. 

In addition to returning the value to maximize, our objective function will return the hyperparameters and the iteration of the search. These results will let us go back and inspect what occurred during a search. The code below implements a simple objective function which we can use for both grid and random search.",hyperparamet tune implement basic framework place use cross valid determin perform model hyperparamet earli stop gbm tune number estim basic strategi grid random search simpl hyperparamet valu combin evalu cross valid score record result along hyperparamet end search choos hyperparamet yield highest cross valid score train model train data make predict test data four part hyperparamet tune help think hyperparamet tune four part four part also form basi bayesian optim 1 object function function take hyperparamet return score tri minim maxim 2 domain set hyperparamet valu want search 3 algorithm method select next set hyperparamet evalu object function 4 result histori data structur contain set hyperparamet result score object function switch grid random search bayesian optim requir make minor modif four part object function object function take hyperparamet output valu repres score tradit optim score minim score roc auc cours want maxim later get bayesian optim use valu minim take 1 text roc auc score occur middl object function vari accord problem problem use cross valid specifi model hyperparamet get cross valid roc auc score use select best model hyperparamet valu addit return valu maxim object function return hyperparamet iter search result let u go back inspect occur search code implement simpl object function use grid random search
3616,"# Domain

The domain, or search space, is all the possible values for all the hyperparameters that we want to search over. For random and grid search, the domain is a hyperparameter grid and usually takes the form of a dictionary with the keys being the hyperparameters and the values lists of values for each hyperparameter.

## Hyperparameters for GBM

To see which settings we can tune, let's make a model and print it out. You can also refer to the [LightGBM documentation](http://lightgbm.readthedocs.io/en/latest/Parameters.html) for the description of all the hyperparameters.",domain domain search space possibl valu hyperparamet want search random grid search domain hyperparamet grid usual take form dictionari key hyperparamet valu list valu hyperparamet hyperparamet gbm see set tune let make model print also refer lightgbm document http lightgbm readthedoc io en latest paramet html descript hyperparamet
3617,"Some of these we do not need to tune such as `silent`, `objective`, `random_state`, and `n_jobs`, and we will use early stopping to determine perhaps the most important hyperparameter, the number of individual learners trained, `n_estimators` (also referred to as `num_boost_rounds` or the number of iterations). Some of the hyperparameters do not need to be tuned if others are: for example, `min_child_samples` and `min_child_weight` both limit the complexity of individual decision trees by adjusting the minimum leaf observation requirements and therefore we will only adjust one. However, there are still many hyperparameters to optimize, and we will choose 10 to tune. 

Choosing a hyperparameter grid is probably the most difficult part of hyperparameter tuning: it's nearly impossible ahead of time to say which values of hyperparameters will work well and the optimal settings will depend on the dataset. Moreover, the hyperparameters have complex interactions with each other which means that just tuning one at a time doesn't work because when we start changing other hyperparameters that will affect the one we just tuned! 

If we have prior experience with a model, we might know where the best values for the hyperparameters typically lie, or what a good search space is. However, if we don't have much experience, we can simply define a large search space and hope that the best values are in there somewhere. Typically, when first using a method, I define a wide search space centered around the default values. Then, if I see that some values of hyperparameters tend to work better, I can concentrate the search around those values. 

A complete grid for the 10 hyperparameter is defined below. Each of the values in the dicionary must be a list, so we use `list` combined with `range`, `np.linspace`, and `np.logspace` to define the range of values for each hyperparameter. ",need tune silent object random state n job use earli stop determin perhap import hyperparamet number individu learner train n estim also refer num boost round number iter hyperparamet need tune other exampl min child sampl min child weight limit complex individu decis tree adjust minimum leaf observ requir therefor adjust one howev still mani hyperparamet optim choos 10 tune choos hyperparamet grid probabl difficult part hyperparamet tune nearli imposs ahead time say valu hyperparamet work well optim set depend dataset moreov hyperparamet complex interact mean tune one time work start chang hyperparamet affect one tune prior experi model might know best valu hyperparamet typic lie good search space howev much experi simpli defin larg search space hope best valu somewher typic first use method defin wide search space center around default valu see valu hyperparamet tend work better concentr search around valu complet grid 10 hyperparamet defin valu dicionari must list use list combin rang np linspac np logspac defin rang valu hyperparamet
3618,"One aspect to note is that if `boosting_type` is `goss`, then we cannot use `subsample` (which refers to training on only a fraction of the rows in the training data, a technique known as [stochastic gradient boosting](https://en.wikipedia.org/wiki/Gradient_boosting#Stochastic_gradient_boosting)). Therefore, we will need a line of logic in our algorithm that sets the `subsample` to 1.0 (which means use all the rows) if `boosting_type=goss`. As an example below, if we randomly select a set of hyperparameters, and the boosting type is ""goss"", then we set the `subsample` to 1.0.",one aspect note boost type go use subsampl refer train fraction row train data techniqu known stochast gradient boost http en wikipedia org wiki gradient boost stochast gradient boost therefor need line logic algorithm set subsampl 1 0 mean use row boost type go exampl randomli select set hyperparamet boost type go set subsampl 1 0
3619,"The `boosting_type` and `is_unbalance` domains are pretty simple because these are categorical variables. For the hyperparameters that must be integers (`num_leaves`, `min_child_samples`), we use `range(start, stop, [step])` which returns a range of numbers from start to stop spaced by step (or 1 if not specified). `range` always returns integers, which means that if we want evenly spaced values that can be fractions, we need to use `np.linspace(start, stop, [num])`.  This works the same way except the third argument is the number of values (by default 100).

Finally, `np.logspace(start, stop, [num = 100], [base = 10.0])` returns values evenly spaced on a logarithmic scale. According to the [the docs](https://docs.scipy.org/doc/numpy/reference/generated/numpy.logspace.html) ""In linear space, the sequence starts at $base^{start}$ (base to the power of start) and ends with $base ^{stop}$ "" This is useful for values that differ over several orders of magnitude such as the learning rate.",boost type unbal domain pretti simpl categor variabl hyperparamet must integ num leav min child sampl use rang start stop step return rang number start stop space step 1 specifi rang alway return integ mean want evenli space valu fraction need use np linspac start stop num work way except third argument number valu default 100 final np logspac start stop num 100 base 10 0 return valu evenli space logarithm scale accord doc http doc scipi org doc numpi refer gener numpi logspac html linear space sequenc start base start base power start end base stop use valu differ sever order magnitud learn rate
3620,"### Learning Rate Domain

The learning rate domain is from 0.005 to 0.5. Using a logarithmic uniform distribution allows us to create a domain where there are as many values from 0.005 to 0.05 as from 0.05 to 0.5. In a linear space, there would be far more values from 0.05 to 0.5 because this represents a larger distance in linear space but in logarithmic space each of these two intervals is the same width because they are multiples of 10 of each other. (Think about going from 1 to 10 and then from 10 to 100. On a logarithmic scale, these intervals are the same size, but on a linear scale the latter is 10 times the size of the former). In other words, a logarithmic uniform distribution lets us sample more evenly from a domain that varies over several orders of magnitude. 

If that's a little confusing, perhaps the graph above makes it clearer. We can also do a sanity check to make sure the spacing is correct by counting the number of values in each interval.",learn rate domain learn rate domain 0 005 0 5 use logarithm uniform distribut allow u creat domain mani valu 0 005 0 05 0 05 0 5 linear space would far valu 0 05 0 5 repres larger distanc linear space logarithm space two interv width multipl 10 think go 1 10 10 100 logarithm scale interv size linear scale latter 10 time size former word logarithm uniform distribut let u sampl evenli domain vari sever order magnitud littl confus perhap graph make clearer also saniti check make sure space correct count number valu interv
3621,"As an example of a simple domain, the `num_leaves` is a uniform distribution. This means values are evenly spaced on a linear scale.",exampl simpl domain num leav uniform distribut mean valu evenli space linear scale
3622,"# Algorithm for selecting next values

Although we don't generally think of them as such, both grid and random search are algorithms. In the case of grid search, we input the domain and the algorithm selects the next value for each hyperparameter in an ordered sequence. The only requirement of grid search is that it tries every combination in a grid once (and only once). For random search, we input the domain and each time the algorithm gives us a random combination of hyperparameter values to try. There are no requirements for random search other than that the next values are selected at random. 

We will implement these algorithms very shortly, as soon as we cover the final part of hyperparameter tuning.",algorithm select next valu although gener think grid random search algorithm case grid search input domain algorithm select next valu hyperparamet order sequenc requir grid search tri everi combin grid random search input domain time algorithm give u random combin hyperparamet valu tri requir random search next valu select random implement algorithm shortli soon cover final part hyperparamet tune
3623,"# Results History

The results history is a data structure that contains the hyperparameter combinations and the resulting score on the objective function. When we get to Bayesian Optimization, the model actually _uses the past results to decide on the next hyperparmeters_ to evaluate. Random and grid search are _uninformed_ methods that do not use the past history, but we still need the history so we can find out which hyperparameters worked the best! 

A dataframe is a useful data structure to hold the results.",result histori result histori data structur contain hyperparamet combin result score object function get bayesian optim model actual use past result decid next hyperparmet evalu random grid search uninform method use past histori still need histori find hyperparamet work best datafram use data structur hold result
3624,"# Grid Search Implementation

Grid search is best described as exhuastive guess and check. We have a problem: find the hyperparameters that result in the best cross validation score, and a set of values to try in the hyperparameter grid - the domain. The grid search method for finding the answer is to try all combinations of values in the domain and hope that the best combination is  in the grid (in reality, we will never know if we found the best settings unless we have an infinite hyperparameter grid which would then require an infinite amount of time to run).

Grid search suffers from one limiting problem: it is extremely computationally expensive because we have to perform cross validation with every single combination of hyperparameters in the grid! Let's see how many total hyperparameter settings there are in our simple little grid we developed.",grid search implement grid search best describ exhuast guess check problem find hyperparamet result best cross valid score set valu tri hyperparamet grid domain grid search method find answer tri combin valu domain hope best combin grid realiti never know found best set unless infinit hyperparamet grid would requir infinit amount time run grid search suffer one limit problem extrem comput expens perform cross valid everi singl combin hyperparamet grid let see mani total hyperparamet set simpl littl grid develop
3625,"Until Kaggle upgrades the kernels to quantum computers, we are not going to be able to run evan a fraction of the combinations! Let's assume 100 seconds per evaluation and see how many years this would take:",kaggl upgrad kernel quantum comput go abl run evan fraction combin let assum 100 second per evalu see mani year would take
3626,"I think we're going to need a better approach! Before we discuss alternatives, let's walk through how we would actually use this grid and evaluate all the hyperparameters.

The code below shows the ""algorithm"" for grid search. First, we [unpack the values](https://www.geeksforgeeks.org/packing-and-unpacking-arguments-in-python/) in the hyperparameter grid (which is a Python dictionary) using the line `keys, values = zip(*param_grid.items())`.  The key line is `for v in itertools.product(*values)` where we iterate through all the possible combinations of values in the hyperparameter grid one at a time.  For each combination of values, we create a dictionary `hyperparameters = dict(zip(keys, v))` and then pass these to the objective function defined earlier. The objective function returns the cross validation score from the hyperparameters which we record in the dataframe. This process is repeated for each and every combination of hyperparameter values. By using `itertools.product` (from [this Stack Overflow Question and Answer](https://codereview.stackexchange.com/questions/171173/list-all-possible-permutations-from-a-python-dictionary-of-lists)), we create a [generator](http://book.pythontips.com/en/latest/generators.html) rather than allocating a list of all possible combinations which would be far too large to hold in memory. ",think go need better approach discus altern let walk would actual use grid evalu hyperparamet code show algorithm grid search first unpack valu http www geeksforgeek org pack unpack argument python hyperparamet grid python dictionari use line key valu zip param grid item key line v itertool product valu iter possibl combin valu hyperparamet grid one time combin valu creat dictionari hyperparamet dict zip key v pas object function defin earlier object function return cross valid score hyperparamet record datafram process repeat everi combin hyperparamet valu use itertool product stack overflow question answer http codereview stackexchang com question 171173 list possibl permut python dictionari list creat gener http book pythontip com en latest gener html rather alloc list possibl combin would far larg hold memori
3627,"Normally, in grid search, we do not limit the number of evaluations. The number of evaluations is set by the total combinations in the hyperparameter grid (or the number of years we are willing to wait!). So the lines 

```
        if i > MAX_EVALS:
            break
```

would not be used in actual grid search. Here we will run grid search for 5 iterations just as an example. The results returned will show us the validation score (ROC AUC), the hyperparameters, and the iteration sorted by best performing combination of hyperparameter values.",normal grid search limit number evalu number evalu set total combin hyperparamet grid number year will wait line max eval break would use actual grid search run grid search 5 iter exampl result return show u valid score roc auc hyperparamet iter sort best perform combin hyperparamet valu
3628,"Now, since we have the best hyperparameters, we can evaluate them on our ""test"" data (remember not the real test data)!",sinc best hyperparamet evalu test data rememb real test data
3629,"It's interesting that the model scores better on the test set than in cross validation. Usually the opposite happens (higher on cross validation than on test) because the model is tuned to the validation data. In this case, the better performance is probably due to small size of the test data and we get very lucky (although this probably does not translate to the actual competition data). ",interest model score better test set cross valid usual opposit happen higher cross valid test model tune valid data case better perform probabl due small size test data get lucki although probabl translat actual competit data
3630,"To get a sense of how grid search works, we can look at the progression of hyperparameters that were evaluated.",get sen grid search work look progress hyperparamet evalu
3631,"Look at the `subsample` and the `is_unbalance` because these are the only hyperparameters that change. In fact, the effect of  changing these values is so small that validation scores literally did not change across runs (indicating this small of a change has no effect on the model). This is grid search trying every single value in the grid! No matter how small the increment between subsequent values of a hyperparameter, it will try them all. Clearly, we are going to need a more efficient approach if we want to find better hyperparameters in a reasonable amount of time. ",look subsampl unbal hyperparamet chang fact effect chang valu small valid score liter chang across run indic small chang effect model grid search tri everi singl valu grid matter small increment subsequ valu hyperparamet tri clearli go need effici approach want find better hyperparamet reason amount time
3632,"#### Application

If you want to run this on the entire dataset feel free to take these functions and put them in a script. However, I would advise against using grid search unless you have a very small hyperparameter grid because this is such as exhaustive method! 
Later, we will look at results from 1000 iterations of grid and random search run on the same small subset of data as we used above. I have not tried to run any form of grid search on the full data (and probably will not try this method).",applic want run entir dataset feel free take function put script howev would advis use grid search unless small hyperparamet grid exhaust method later look result 1000 iter grid random search run small subset data use tri run form grid search full data probabl tri method
3633,"# Random Search

Random search is surprisingly efficient compared to grid search. Although grid search will find the optimal value of hyperparameters (assuming they are in your grid) eventually, random search will usually find a ""close-enough"" value in far fewer iterations. [This great paper explains why this is so](http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf): grid search spends too much time evaluating unpromising regions of the hyperparameter search space because it has to evaluate every single combination in the grid. Random search in contrast, does a better job of exploring the search space and therefore can usually find a good combination of hyperparameters in far fewer iterations. 

As [this article](https://medium.com/rants-on-machine-learning/smarter-parameter-sweeps-or-why-grid-search-is-plain-stupid-c17d97a0e881) lays out, random search should probably be the first hyperparameter optimization method tried because of its effectiveness. Even though it's an _uninformed_ method (meaning it does not rely on past evaluation results), random search can still usually find better values than the default and is simple to run.

Random search can also be thought of as an algorithm: randomly select the next set of hyperparameters from the grid! We can build a dictionary of hyperparameters by selecting one random value for each hyperparameter as follows (again accounting for subsampling):",random search random search surprisingli effici compar grid search although grid search find optim valu hyperparamet assum grid eventu random search usual find close enough valu far fewer iter great paper explain http www jmlr org paper volume13 bergstra12a bergstra12a pdf grid search spend much time evalu unpromis region hyperparamet search space evalu everi singl combin grid random search contrast better job explor search space therefor usual find good combin hyperparamet far fewer iter articl http medium com rant machin learn smarter paramet sweep grid search plain stupid c17d97a0e881 lay random search probabl first hyperparamet optim method tri effect even though uninform method mean reli past evalu result random search still usual find better valu default simpl run random search also thought algorithm randomli select next set hyperparamet grid build dictionari hyperparamet select one random valu hyperparamet follow account subsampl
3634,"Next, we define the `random_search` function. This takes the same general structure as `grid_search` except for the method used to select the next hyperparameter values. Moreover, random search is always run with a limit on the number of search iterations.",next defin random search function take gener structur grid search except method use select next hyperparamet valu moreov random search alway run limit number search iter
3635,"We can also evaluate the best random search model on the ""test"" data.",also evalu best random search model test data
3636,"Finally, we can view the random search sequence of hyperparameters.",final view random search sequenc hyperparamet
3637,"This time we see hyperparameter values that are all over the place, almost as if they had been selected at random! Random search will do a much better job than grid search of exploring the search domain (for the same number of iterations). If we have a limited time to evaluate hyperparameters, random search is a better option than grid search for exactly this reason.

### Stacking Random and Grid Search

One option for a smarter implementation of hyperparameter tuning is to combine random search and grid search: 

1. Use random search with  a large hyperparameter grid 
2. Use the results of random search to build a focused hyperparameter grid around the best performing hyperparameter values.
3. Run grid search on the reduced hyperparameter grid. 
4. Repeat grid search on more focused grids until maximum computational/time budget is exceeded.

In a later notebook (upcoming), we will look at methods that use the past evaluation results to pick the next hyperparameter values to try in the objective function. These methods (including [Bayesian optimization](https://sigopt.com/static/pdf/SigOpt_Bayesian_Optimization_Primer.pdf)) are essentially doing what we would do in the strategy outlined above: adjust the next values tried in the search from the previous results. The overall objective of these _informed methods_ is to limit evaluations of the objective function by reasoning about the next values to try based on past evaluation results. These algorithms are therefore able to save time by evaluating more promising values of hyperparameters. This is a really cool topic and [Bayesian optimization](http://gpss.cc/gpmc17/slides/LancasterMasterclass_1.pdf) is fascinating so stay tuned for this upcoming notebook. ",time see hyperparamet valu place almost select random random search much better job grid search explor search domain number iter limit time evalu hyperparamet random search better option grid search exactli reason stack random grid search one option smarter implement hyperparamet tune combin random search grid search 1 use random search larg hyperparamet grid 2 use result random search build focus hyperparamet grid around best perform hyperparamet valu 3 run grid search reduc hyperparamet grid 4 repeat grid search focus grid maximum comput time budget exceed later notebook upcom look method use past evalu result pick next hyperparamet valu tri object function method includ bayesian optim http sigopt com static pdf sigopt bayesian optim primer pdf essenti would strategi outlin adjust next valu tri search previou result overal object inform method limit evalu object function reason next valu tri base past evalu result algorithm therefor abl save time evalu promis valu hyperparamet realli cool topic bayesian optim http gps cc gpmc17 slide lancastermasterclass 1 pdf fascin stay tune upcom notebook
3638,"## Next Steps

We can now take these random and grid search functions and use them on the complete dataset or any dataset of our choosing. These search methods are very expensive, so expect the hyperparameter tuning to take a while.(I am currently running this script on a full set of features for 500 iterations and will make the results public when they are available. )

For now, we will turn to implementing random and grid search on the reduced dataset for 1000 iterations just to compare the results (I took the code below and already ran it because even with the small dataset, it takes a very long time. The results are available as part of the data in this kernel). 

## Writing to File to Monitor Progress

When we run these searches for a long time, it's natural to want to track the performance while the search is going on. We can print information to the command prompt, but this will grow cluttered after 1000 iterations and the results will be gone if we close the command prompt. A better solution (although not perfect) is to write a line to a csv (comma separated value) file on each iteration. Then, we can look at the file to track progress while the searching is running, and eventually, have the entire results saved when the search is complete.

### Extremely Important Note about Checking Files

When you want to check the csv file, __do not open it in Excel while the search is ongoing__. This will cause a permission error in Python and the search will be terminated. Instead, you can view the end of the file by typing `tail out_file.csv` from Bash where `out_file.csv` is the name of the file being written to. There are also some text editors, such as notepad or Sublime Text, where you can open the results safely while the search is occurring. However, __do not use Excel to open a file that is being written to in Python__. This is a mistake I've made several times so you do not have to! ",next step take random grid search function use complet dataset dataset choos search method expens expect hyperparamet tune take current run script full set featur 500 iter make result public avail turn implement random grid search reduc dataset 1000 iter compar result took code alreadi ran even small dataset take long time result avail part data kernel write file monitor progress run search long time natur want track perform search go print inform command prompt grow clutter 1000 iter result gone close command prompt better solut although perfect write line csv comma separ valu file iter look file track progress search run eventu entir result save search complet extrem import note check file want check csv file open excel search ongo caus permiss error python search termin instead view end file type tail file csv bash file csv name file written also text editor notepad sublim text open result safe search occur howev use excel open file written python mistak made sever time
3639,"Below is the code we need to run before the search. This creates the csv file, opens a connection, writes the header (column names), and then closes the connection. This will overwrite any information currently in the `out_file`, so change to a new file name every time you want to start a new search.",code need run search creat csv file open connect write header column name close connect overwrit inform current file chang new file name everi time want start new search
3640,"Now we must slightly modify `random_search` and `grid_search` to write to this file every time. We do this by opening a connection, this time using the `""a""` option for append (the first time we used the `""w""` option for write) and writing a line with the desired information (which in this case is the cross validation score, the hyperparameters, and the number of the iteration). Then we close the connection until the function is called again.",must slightli modifi random search grid search write file everi time open connect time use option append first time use w option write write line desir inform case cross valid score hyperparamet number iter close connect function call
3641,"To run these functions for 1000 iterations (or however many you choose) uncomment the cell below. Otherwise, I have run these functions on the reduced dataset and attached the results to this kernel.",run function 1000 iter howev mani choos uncom cell otherwis run function reduc dataset attach result kernel
3642,"# Results on Limited Data

We can examine 1000 search iterations of the above functions on the reduced dataset. Later, we can try the hyperparameters that worked the best for the small versions of the data on a complete dataset to see if the best hyperparameters translate when increasing the size of the data 30 times! The 1000 search iterations were not run in a kernel, although they might be able to finish (no guarantees) in the 12 hour time limit. 

First we can find out which method returned the best results. ",result limit data examin 1000 search iter function reduc dataset later tri hyperparamet work best small version data complet dataset see best hyperparamet translat increas size data 30 time 1000 search iter run kernel although might abl finish guarante 12 hour time limit first find method return best result
3643,"When we save the results to a csv, for some reason the dictionaries are saved as strings. Therefore we need to convert them back to dictionaries after reading in the results using the `ast.literal_eval` function.",save result csv reason dictionari save string therefor need convert back dictionari read result use ast liter eval function
3644,Now let's make a function to parse the results from the hyperparameter searches. This returns a dataframe where each column is a hyperparameter and each row has one search result (so taking the dictionary of hyperparameters and mapping it into a row in a dataframe).,let make function par result hyperparamet search return datafram column hyperparamet row one search result take dictionari hyperparamet map row datafram
3645,"# Visualizations

Visualizations are both enjoyable to make, and can give us an intuitive look into a technique. Here we will make a few simple plots using matplotlib, seaborn, and Altair! __Unfortunately, the Altair visualizations do not show up when the notebook is rendered. To view the Altair figures, you'll have to run the notebook yourself!__",visual visual enjoy make give u intuit look techniqu make simpl plot use matplotlib seaborn altair unfortun altair visual show notebook render view altair figur run notebook
3646,"First we can plot the validation scores versus the iteration. Here we will use the [Altair](https://altair-viz.github.io/) visualization library to make some plots! First, we need to put our data into a long format dataframe.",first plot valid score versu iter use altair http altair viz github io visual librari make plot first need put data long format datafram
3647,"Below, we make the same plot using seaborn because the Altair visualizations do not show up in the rendered notebook. ",make plot use seaborn altair visual show render notebook
3648,"The grid cross validation score increases over time. This indicates that whatever hyperparameters are changing in grid search are gradually increasing the score. The random cross validation scores on the other hand are all over the place as expected. This grid search appears to be stuck in a relatively low-performing region of the search space, and because it is constrained to try all the values in the grid, it is not able to try significantly different hyperparameter values that would perform better (as occurs in random search). The random search method does a very good job of exploring the search space as we will see when we look at the hyperparameter values searched. ",grid cross valid score increas time indic whatev hyperparamet chang grid search gradual increas score random cross valid score hand place expect grid search appear stuck rel low perform region search space constrain tri valu grid abl tri significantli differ hyperparamet valu would perform better occur random search random search method good job explor search space see look hyperparamet valu search
3649,"## Distribution of Search Values

We can show the distribution of search values for random search (grid search is very uninteresting). Even though we expect these to be _random_, it's always a good idea to check our code both quantitatively and visually. ",distribut search valu show distribut search valu random search grid search uninterest even though expect random alway good idea check code quantit visual
3650,"The boosting type should be evenly distributed for random search. 

Again, we have to remake this chart in seaborn to have the visualization appear in the rendered notebook (if anyone knows how to address this issue, please tell me in the comments!)",boost type evenli distribut random search remak chart seaborn visual appear render notebook anyon know address issu plea tell comment
3651,"Next, for the numeric hyperparameters, we will plot both the sampling distribution (the hyperparameter grid) and the results from random search in a kernel density estimate (KDE) plot. (The grid search results are completely uninteresting). As random search is just drawing random values, we would expect the random search distribution to align with the sampling grid (although it won't be perfectly aligned because of the limited number of searches). 

As an example, below we plot the distribution of learning rates from both the sampling distribution and the random search results. The vertical dashed line indicates the optimal value found from random search.",next numer hyperparamet plot sampl distribut hyperparamet grid result random search kernel densiti estim kde plot grid search result complet uninterest random search draw random valu would expect random search distribut align sampl grid although perfectli align limit number search exampl plot distribut learn rate sampl distribut random search result vertic dash line indic optim valu found random search
3652,The following code repeats this plot for all the of the numeric hyperparameters. ,follow code repeat plot numer hyperparamet
3653,"## Sequence of Search Values

Finally, we can plot the sequence of search values against the iteration for random search. Clearly there will not be any order, but this can let us visualize what happens in a random search!

The star indicates the best value of the hyperparameter that was found.",sequenc search valu final plot sequenc search valu iter random search clearli order let u visual happen random search star indic best valu hyperparamet found
3654,## Score versus Hyperparameters,score versu hyperparamet
3655,"As a final plot, we can show the score versus the value of each hyperparameter. We need to keep in mind that the hyperparameters are not changed one at a time, so if there are relationships between the values and the score, they do not mean that particular hyperparameter is influencing the score. However, we might be able to identify values of hyperparameters that seem more promising. Mostly these plots are for my own interest, to see if there are any trends! ",final plot show score versu valu hyperparamet need keep mind hyperparamet chang one time relationship valu score mean particular hyperparamet influenc score howev might abl identifi valu hyperparamet seem promis mostli plot interest see trend
3656,"We want to avoid placing too much emphasis on any of these relationships because we were not changing one hyperparameter at a time (although we could carry out experiments where we only change one hyperparameter and observes the effects on the score) and so the trends are not due solely to the single hyperparameter we show. If we could plot this in higher dimensions, it might be interesting to see if there are more promising regions of the search space but here we are limited to one dimension (a single hyperparameter versus the score).  If we want to observe the effects of one hyperparameter on the cross validation score, we could alter only that hyperparameter while holding all the others constant. However, the hyperparameters do not act by themselves and there are complex interactions between the model settings.",want avoid place much emphasi relationship chang one hyperparamet time although could carri experi chang one hyperparamet observ effect score trend due sole singl hyperparamet show could plot higher dimens might interest see promis region search space limit one dimens singl hyperparamet versu score want observ effect one hyperparamet cross valid score could alter hyperparamet hold other constant howev hyperparamet act complex interact model set
3657,"# Testing Results on Full Data

We can take the best hyperparameters found from the 1000 iterations of random search on the reduced training data and try these on an entire training dataset. Here, we will use the features from the `[Updated 0.792 LB] LightGBM with Simple Features
`(https://www.kaggle.com/jsaguiar/updated-0-792-lb-lightgbm-with-simple-features) kernel (I did not develop these features and want to give credit to the numerous people, including [Aguiar](https://www.kaggle.com/jsaguiar) and [olivier](https://www.kaggle.com/ogrellier),  who have worked on these features. Please check out their [kernels](https://www.kaggle.com/ogrellier/lighgbm-with-selected-features)!). 

The code below uses the best random search hyperparameters to build a model, train on the full features from `[Updated 0.792 LB] LightGBM with Simple Features
`(https://www.kaggle.com/jsaguiar/updated-0-792-lb-lightgbm-with-simple-features), and test on the testing features. The test data is the actual competition data, so we can then submit these and see how well the score translates to a full dataset! ",test result full data take best hyperparamet found 1000 iter random search reduc train data tri entir train dataset use featur updat 0 792 lb lightgbm simpl featur http www kaggl com jsaguiar updat 0 792 lb lightgbm simpl featur kernel develop featur want give credit numer peopl includ aguiar http www kaggl com jsaguiar olivi http www kaggl com ogrelli work featur plea check kernel http www kaggl com ogrelli lighgbm select featur code use best random search hyperparamet build model train full featur updat 0 792 lb lightgbm simpl featur http www kaggl com jsaguiar updat 0 792 lb lightgbm simpl featur test test featur test data actual competit data submit see well score translat full dataset
3658,First we will test the cross validation score using the best model hyperparameter values from random search. This can give us an idea of the generalization error on the test set. We will delete the number of estimators found from the smaller dataset and use early stopping to find the best number of decision trees to train. ,first test cross valid score use best model hyperparamet valu random search give u idea gener error test set delet number estim found smaller dataset use earli stop find best number decis tree train
3659,"The public leaderboard score is only calculated on 10% of the test data, so the cross validation score might actually give us a better idea of how the model will perform on the full test set. Usually we expect the cross validation score to be higher than on the testing data, but because of the small size of the testing data, this might be reversed for this problem.

Next, we will make predictions on the test data that can be submitted to the competition. ",public leaderboard score calcul 10 test data cross valid score might actual give u better idea model perform full test set usual expect cross valid score higher test data small size test data might revers problem next make predict test data submit competit
3660,"The score when submitting to the test competition is __0.782__. The original score from the kernel where I got these features was 0.792, so we can conclude that the results from random search on the smaller dataset to not translate to a full dataset. I currently am running random search with 500 iterations on the full dataset, and will make those results publicly available when the search is complete! ",score submit test competit 0 782 origin score kernel got featur 0 792 conclud result random search smaller dataset translat full dataset current run random search 500 iter full dataset make result publicli avail search complet
3661,"## Model Tuning Next Steps

From here, we might want to take the functions we wrote and apply them to a complete dataset. The results are likely to be different because we were only using a random subset of the training data. However, this will take much longer (300000+ observations instead of 10000). I'm currently running the random search on the full dataset from the Kernel referenced above, and will see how the results turn out. (Sampling some of the observations is not inherently negative, and it can help us get reasonable answers in a much shorter time frame. However, if we are using such a small portion of the data that is not representative of the entire dataset, then we should not expect the tuning to translate to the full dataset.)

In an upcoming notebook, we will turn to automated hyperparameter tuning, in particular, Bayesian Optimization. We will implement automated optimization of machine learning hyperparameters step-by-step using the Hyperopt open-source Python library. I'll provide the link here as soon as this notebook is finished, but if you want to get an idea of Bayesian optimization, you can check out [this introductory article](https://towardsdatascience.com/an-introductory-example-of-bayesian-optimization-in-python-with-hyperopt-aae40fff4ff0), or [this article on automated hyperparameter tuning](https://towardsdatascience.com/automated-machine-learning-hyperparameter-tuning-in-python-dfda59b72f8a). These topics are pretty neat and it's incredible that they are available in an easy-to-use format for anyone to take advantage of. I'll see you in the next notebook! ",model tune next step might want take function wrote appli complet dataset result like differ use random subset train data howev take much longer 300000 observ instead 10000 current run random search full dataset kernel referenc see result turn sampl observ inher neg help u get reason answer much shorter time frame howev use small portion data repres entir dataset expect tune translat full dataset upcom notebook turn autom hyperparamet tune particular bayesian optim implement autom optim machin learn hyperparamet step step use hyperopt open sourc python librari provid link soon notebook finish want get idea bayesian optim check introductori articl http towardsdatasci com introductori exampl bayesian optim python hyperopt aae40fff4ff0 articl autom hyperparamet tune http towardsdatasci com autom machin learn hyperparamet tune python dfda59b72f8a topic pretti neat incred avail easi use format anyon take advantag see next notebook
3662,"# Conclusions

Model tuning is the process of finding the best machine learning model hyperparameters for a particular problem. Random and grid search are two uniformed methods for hyperparameter tuning that search by selecting hyperparameter values from a grid domain. 
The four parts of hyperparameter tuning are:

1. Objective function: takes in hyperparameters and returns the cross validation score we want to maximize or minimize
2. Domain of hyperparameters: values over which we want to search
3. Algorithm: method for selecting the next hyperparameter values to evaluate in the objective function
4. Results: history of hyperparameters and cross validation scores

These four parts apply to grid and random search as well as to Bayesian optimization, a form of automated hyperparameter tuning. In this notebook, we implemented both random and grid search on a reduced dataset, inspected the results, and tried to translate the optimal hyperparameters to a full dataset (from [this kernel](https://www.kaggle.com/jsaguiar/updated-0-792-lb-lightgbm-with-simple-features)). As a small note, it's important to remember that we tune the hyperparameters to the training data - using cross validation - so the hyperparameter values we find are only optimal for the training data. Although the best hyperparameters from the smaller dataset did not work that well on the full dataset, we were still able to see the ideas behind these two tuning methods. Moreover, we can take the functions developed here and apply them to any dataset or to any machine learning model, not just the gradient boosting machine. 

Random search turns out to work pretty well in practice (because it is good at exploring the search domain), but it still is not a reasoning method because it does not use past evaluation results to choose the next hyperparameter values. A better approach would be to use the past results to reason about the best values to try next in the objective function, especially because as we saw, evaluating the objective function is time-consuming! In future work, we will look at [implementing automated hyperparameter tuning](https://towardsdatascience.com/automated-machine-learning-hyperparameter-tuning-in-python-dfda59b72f8a) using Bayesian optimization. 

Hyperparameter tuning is a crucial part of the machine learning pipeline because the performance of a model can depend strongly on the choices of the hyperparameter values. Random and grid search are two decent methods to start tuning a model (at least they are better than manual tuning) and are important tools to have in the data science skillset. Thanks for reading and I'll see you in the next notebook!

As always, I welcome feedback and constructive criticism. I write for Towards Data Science at https://medium.com/@williamkoehrsen/ and can be reached on Twitter at https://twitter.com/koehrsen_will

Will",conclus model tune process find best machin learn model hyperparamet particular problem random grid search two uniform method hyperparamet tune search select hyperparamet valu grid domain four part hyperparamet tune 1 object function take hyperparamet return cross valid score want maxim minim 2 domain hyperparamet valu want search 3 algorithm method select next hyperparamet valu evalu object function 4 result histori hyperparamet cross valid score four part appli grid random search well bayesian optim form autom hyperparamet tune notebook implement random grid search reduc dataset inspect result tri translat optim hyperparamet full dataset kernel http www kaggl com jsaguiar updat 0 792 lb lightgbm simpl featur small note import rememb tune hyperparamet train data use cross valid hyperparamet valu find optim train data although best hyperparamet smaller dataset work well full dataset still abl see idea behind two tune method moreov take function develop appli dataset machin learn model gradient boost machin random search turn work pretti well practic good explor search domain still reason method use past evalu result choos next hyperparamet valu better approach would use past result reason best valu tri next object function especi saw evalu object function time consum futur work look implement autom hyperparamet tune http towardsdatasci com autom machin learn hyperparamet tune python dfda59b72f8a use bayesian optim hyperparamet tune crucial part machin learn pipelin perform model depend strongli choic hyperparamet valu random grid search two decent method start tune model least better manual tune import tool data scienc skillset thank read see next notebook alway welcom feedback construct critic write toward data scienc http medium com williamkoehrsen reach twitter http twitter com koehrsen
3663,"# Introduction: Feature Selection

In this notebook we will apply feature engineering to the manual engineered features built in two previous kernels. We will reduce the number of features using several methods and then we will test the performance of the features using a fairly basic gradient boosting machine model. 

The main takeaways from this notebook are:

* Going from 1465 total features to 536 and an AUC ROC of 0.783 on the public leaderboard
* A further optional step to go to 342 features and an AUC ROC of 0.782

The full set of features was built in [Part One](https://www.kaggle.com/willkoehrsen/introduction-to-manual-feature-engineering) and [Part Two](https://www.kaggle.com/willkoehrsen/introduction-to-manual-feature-engineering-p2) of Manual Feature Engineering

We will use three methods for feature selection:

1. Remove collinear features
2. Remove features with greater than a threshold percentage of missing values
3. Keep only the most relevant features using feature importances from a model

We will also take a look at an example of applying PCA although we will not use this method for feature reduction. ",introduct featur select notebook appli featur engin manual engin featur built two previou kernel reduc number featur use sever method test perform featur use fairli basic gradient boost machin model main takeaway notebook go 1465 total featur 536 auc roc 0 783 public leaderboard option step go 342 featur auc roc 0 782 full set featur built part one http www kaggl com willkoehrsen introduct manual featur engin part two http www kaggl com willkoehrsen introduct manual featur engin p2 manual featur engin use three method featur select 1 remov collinear featur 2 remov featur greater threshold percentag miss valu 3 keep relev featur use featur import model also take look exampl appli pca although use method featur reduct
3664,Standard imports for data science work. The LightGBM library is used for the gradient boosting machine.,standard import data scienc work lightgbm librari use gradient boost machin
3665,"* `train_bureau` is the training features built manually using the `bureau` and `bureau_balance` data
* `train_previous` is the training features built manually using the `previous`, `cash`, `credit`, and `installments` data

We first will see how many features we built over the manual engineering process. Here we use a couple of set operations to find the columns that are only in the `bureau`, only in the `previous`, and in both dataframes, indicating that there are `original` features from the `application` dataframe. Here we are working with a small subset of the data in order to not overwhelm the kernel. This code has also been run on the full dataset (we will take a look at some of the results).",train bureau train featur built manual use bureau bureau balanc data train previou train featur built manual use previou cash credit instal data first see mani featur built manual engin process use coupl set oper find column bureau previou datafram indic origin featur applic datafram work small subset data order overwhelm kernel code also run full dataset take look result
3666,That gives us the number of features in each dataframe. Now we want to combine the data without creating any duplicate rows. ,give u number featur datafram want combin data without creat duplic row
3667,"Next we want to one-hot encode the dataframes. This doesn't give the full features since we are only working with a sample of the data and this will not create as many columns as one-hot encoding the entire dataset would. Doing this to the full dataset results in 1465 features.

An important note in the code cell is where we __align the dataframes by the columns.__ This ensures we have the same columns in the training and testing datasets.",next want one hot encod datafram give full featur sinc work sampl data creat mani column one hot encod entir dataset would full dataset result 1465 featur import note code cell align datafram column ensur column train test dataset
3668,"When we do this to the full dataset, we get __1465__ features. ",full dataset get 1465 featur
3669,"### Admit and Correct Mistakes!

When doing manual feature engineering, I accidentally created some columns derived from the client id, `SK_ID_CURR`. As this is a unique identifier for each client, it should not have any predictive power, and we would not want to build a model trained on this ""feature"". Let's remove any columns built on the `SK_ID_CURR`.",admit correct mistak manual featur engin accident creat column deriv client id sk id curr uniqu identifi client predict power would want build model train featur let remov column built sk id curr
3670,"After applying this to the full dataset, we end up with __1416 __ features. More features might seem like a good thing, and they can be if they help our model learn. However, irrelevant features, highly correlated features, and missing values can prevent the model from learning and decrease generalization performance on the testing data. Therefore, we perform feature selection to keep only the most useful variables.

We will start feature selection by focusing on collinear variables.",appli full dataset end 1416 featur featur might seem like good thing help model learn howev irrelev featur highli correl featur miss valu prevent model learn decreas gener perform test data therefor perform featur select keep use variabl start featur select focus collinear variabl
3671,"# Remove Collinear Variables

Collinear variables are those which are highly correlated with one another. These can decrease the model's availablility to learn, decrease model interpretability, and decrease generalization performance on the test set. Clearly, these are three things we want to increase, so removing collinear variables is a useful step. We will establish an admittedly arbitrary threshold for removing collinear variables, and then remove one out of any pair of variables that is above that threshold. 

The code below identifies the highly correlated variables based on the absolute magnitude of the Pearson correlation coefficient being greater than 0.9. Again, this is not entirely accurate since we are dealing with such a limited section of the data. This code is for illustration purposes, but if we read in the entire dataset, it would work (if the kernels allowed it)! 

This code is adapted from [work by Chris Albon](https://chrisalbon.com/machine_learning/feature_selection/drop_highly_correlated_features/).",remov collinear variabl collinear variabl highli correl one anoth decreas model availablil learn decreas model interpret decreas gener perform test set clearli three thing want increas remov collinear variabl use step establish admittedli arbitrari threshold remov collinear variabl remov one pair variabl threshold code identifi highli correl variabl base absolut magnitud pearson correl coeffici greater 0 9 entir accur sinc deal limit section data code illustr purpos read entir dataset would work kernel allow code adapt work chri albon http chrisalbon com machin learn featur select drop highli correl featur
3672,### Identify Correlated Variables,identifi correl variabl
3673,#### Drop Correlated Variables,drop correl variabl
3674,"Applying this on the entire dataset __results in 538  collinear features__ removed.  

This has reduced the number of features singificantly, but it is likely still too many. At this point, we'll read in the full dataset after removing correlated variables for further feature selection.

The full datasets (after removing correlated variables) are available in `m_train_combined.csv` and `m_test_combined.csv`.",appli entir dataset result 538 collinear featur remov reduc number featur singificantli like still mani point read full dataset remov correl variabl featur select full dataset remov correl variabl avail train combin csv test combin csv
3675,"### Read in Full Dataset

Now we are ready to move on to the full set of features. These were built by applying the above steps to the entire `train_bureau` and `train_previous` files (you can do the same if you want and have the computational resources)!",read full dataset readi move full set featur built appli step entir train bureau train previou file want comput resourc
3676,"# Remove Missing Values

A relatively simple choice of feature selection is removing missing values. Well, it seems simple, at least until we have to decide what percentage of missing values is the minimum threshold for removing a column. Like many choices in machine learning, there is no right answer, and not even a general rule of thumb for making this choice. In this implementation, if any columns have greater than 75% missing values, they will be removed. 

Most models (including those in Sk-Learn) cannot handle missing values, so we will have to fill these in before machine learning. The Gradient Boosting Machine ([at least in LightGBM](https://github.com/Microsoft/LightGBM/blob/master/docs/Advanced-Topics.rst)) can handle missing values. Imputing missing values always makes me a little uncomfortable because we are adding information that actually isn't in the dataset. Since we are going to be evaluating several models (in a later notebook), we will have to use some form of imputation. For now, we will focus on removing columns above the threshold.",remov miss valu rel simpl choic featur select remov miss valu well seem simpl least decid percentag miss valu minimum threshold remov column like mani choic machin learn right answer even gener rule thumb make choic implement column greater 75 miss valu remov model includ sk learn handl miss valu fill machin learn gradient boost machin least lightgbm http github com microsoft lightgbm blob master doc advanc topic rst handl miss valu imput miss valu alway make littl uncomfort ad inform actual dataset sinc go evalu sever model later notebook use form imput focu remov column threshold
3677,"Let's drop the columns, one-hot encode the dataframes, and then align the columns of the dataframes.",let drop column one hot encod datafram align column datafram
3678,"# Feature Selection through Feature Importances

The next method we can employ for feature selection is to use the feature importances of a model. Tree-based models (and consequently ensembles of trees) can determine an ""importance"" for each feature by measuring the reduction in impurity for including the feature in the model. I'm not really sure what that means (any explanations would be welcome) and the absolute value of the importance can be difficult to interpret. However, the relative value of the importances can be used as an approximation of the ""relevance"" of different features in a model. Moreover, we can use the feature importances to remove features that the model does not consider important. 

One method for doing this automatically is the [Recursive Feature Elimination method](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html) in Scikit-Learn. This accepts an estimator (one that either returns feature weights such as a linear regression, or feature importances such as a random forest) and a desired number of features. In then fits the model repeatedly on the data and iteratively removes the lowest importance features until the desired number of features is left. This means we have another arbitrary hyperparameter to use in out pipeline: the number of features to keep! 

Instead of doing this automatically, we can perform our own feature removal by first removing all zero importance features from the model. If this leaves too many features, then we can consider removing the features with the lowest importance. We will use a Gradient Boosted Model from the LightGBM library to assess feature importances. If you're used to the Scikit-Learn library, the LightGBM library has an API that makes deploying the model very similar to using a Scikit-Learn model. ",featur select featur import next method employ featur select use featur import model tree base model consequ ensembl tree determin import featur measur reduct impur includ featur model realli sure mean explan would welcom absolut valu import difficult interpret howev rel valu import use approxim relev differ featur model moreov use featur import remov featur model consid import one method automat recurs featur elimin method http scikit learn org stabl modul gener sklearn featur select rfe html scikit learn accept estim one either return featur weight linear regress featur import random forest desir number featur fit model repeatedli data iter remov lowest import featur desir number featur left mean anoth arbitrari hyperparamet use pipelin number featur keep instead automat perform featur remov first remov zero import featur model leav mani featur consid remov featur lowest import use gradient boost model lightgbm librari ass featur import use scikit learn librari lightgbm librari api make deploy model similar use scikit learn model
3679,"Since the LightGBM model does not need missing values to be imputed, we can directly `fit` on the training data. We will use Early Stopping to determine the optimal number of iterations and run the model twice, averaging the feature importances to try and avoid overfitting to a certain set of features.",sinc lightgbm model need miss valu imput directli fit train data use earli stop determin optim number iter run model twice averag featur import tri avoid overfit certain set featur
3680,"We see that one of our features made it into the top 5 most important! That's a good sign for all of our hard work making the features. It also looks like many of the features we made have literally 0 importance. For the gradient boosting machine, features with 0 importance are not used at all to make any splits. Therefore, we can remove these features from the model with no effect on performance (except for faster training). ",see one featur made top 5 import good sign hard work make featur also look like mani featur made liter 0 import gradient boost machin featur 0 import use make split therefor remov featur model effect perform except faster train
3681,Let's remove the features that have zero importance.,let remov featur zero import
3682,"At this point, we can re-run the model to see if it identifies any more features with zero importance. In a way, we are implementing our own form of recursive feature elimination. Since we are repeating work, we should probably put the zero feature importance identification code in a function.",point run model see identifi featur zero import way implement form recurs featur elimin sinc repeat work probabl put zero featur import identif code function
3683,"There are now no 0 importance features left (I guess we should have expected this). If we want to remove more features, we will have to start with features that have a non-zero importance. One way we could do this is by retaining enough features to account for a threshold percentage of importance, such as 95%. At this point, let's keep enough features to account for 95% of the importance. Again, this is an arbitrary decision! ",0 import featur left guess expect want remov featur start featur non zero import one way could retain enough featur account threshold percentag import 95 point let keep enough featur account 95 import arbitrari decis
3684,"We can keep only the features needed for 95% importance. This step seems to me to have the greatest chance of harming the model's learning ability, so rather than changing the original dataset, we will make smaller copies. Then, we can test both versions of the data to see if the extra feature removal step is worthwhile. ",keep featur need 95 import step seem greatest chanc harm model learn abil rather chang origin dataset make smaller copi test version data see extra featur remov step worthwhil
3685,"# Test New Featuresets

The last step of feature removal we did seems like it may have the potential to hurt the model the most. Therefore we want to test the effect of this removal. To do that, we can use a standard model and change the features. 

We will use a fairly standard LightGBM model, similar to the one we used for feature selection. The main difference is this model uses five-fold cross validation for training and we  use it to make predictions. There's a lot of code here, but that's because I included documentation and a few extras (such as feature importances) that aren't strictly necessary. For now, understanding the entire model isn't critical, just know that we are using the same model with two different datasets to see which one performs the best.",test new featureset last step featur remov seem like may potenti hurt model therefor want test effect remov use standard model chang featur use fairli standard lightgbm model similar one use featur select main differ model use five fold cross valid train use make predict lot code includ document extra featur import strictli necessari understand entir model critic know use model two differ dataset see one perform best
3686,"### Test ""Full"" Dataset

This is the expanded dataset. To recap the process to make this dataset we:

* Removed collinear features as measured by the correlation coefficient greater than 0.9
* Removed any columns with greater than 80% missing values in the train or test set
* Removed all features with non-zero feature importances",test full dataset expand dataset recap process make dataset remov collinear featur measur correl coeffici greater 0 9 remov column greater 80 miss valu train test set remov featur non zero featur import
3687,The full features after feature selection score __0.783__ when submitted to the public leaderboard. ,full featur featur select score 0 783 submit public leaderboard
3688,"### Test ""Small"" Dataset

The small dataset requires one additional step over the ful l dataset:

* Keep only features needed to reach 95% cumulative importance in the gradient boosting machine",test small dataset small dataset requir one addit step ful l dataset keep featur need reach 95 cumul import gradient boost machin
3689,The smaller featureset scores __0.782__ when submitted to the public leaderboard.,smaller featureset score 0 782 submit public leaderboard
3690,"# Other Options for Dimensionality Reduction

We only covered a small portion of the techniques used for feature selection/dimensionality reduction. There are many other methods such as:

* PCA: Principle Components Analysis (PCA)
* ICA: Independent Components Analysis (ICA)
* Manifold learning: [also called non-linear dimensionality reduction](https://stats.stackexchange.com/questions/247907/what-is-the-difference-between-manifold-learning-and-non-linear-dimensionality-r)

PCA is a great method for reducing the number of features provided that you do not care about model interpretability. It projects the original set of features onto a lower dimension, in the process, eliminating any physical representation behind the features. Here's a pretty thorough introduction to the math for anyone interested. PCA also assumes that the data is Gaussian distributed, which may not be the case, especially when dealing with real-world human generated data. 

ICA representations also obscure any physical meaning behind the variables and presevere the most ""independent"" dimensions of the data (which is different than the dimensions with the most variance). 

Manifold learning is more often used for low-dimensional visualizations (such as with T-SNE or LLE) rather than for dimensionality reduction for a classifier. These methods are heavily dependent on several hyperparameters and are not deterministic which means that there is no way to apply it to new data (in other words you cannot `fit` it to the training data and then separately `transform` the testing data). The learned representation of a dataset will change every time you apply manifold learning so it is not generally a stable method for feature selection.",option dimension reduct cover small portion techniqu use featur select dimension reduct mani method pca principl compon analysi pca ica independ compon analysi ica manifold learn also call non linear dimension reduct http stat stackexchang com question 247907 differ manifold learn non linear dimension r pca great method reduc number featur provid care model interpret project origin set featur onto lower dimens process elimin physic represent behind featur pretti thorough introduct math anyon interest pca also assum data gaussian distribut may case especi deal real world human gener data ica represent also obscur physic mean behind variabl presever independ dimens data differ dimens varianc manifold learn often use low dimension visual sne lle rather dimension reduct classifi method heavili depend sever hyperparamet determinist mean way appli new data word fit train data separ transform test data learn represent dataset chang everi time appli manifold learn gener stabl method featur select
3691,"## PCA Example

We can go through a quick example to show how PCA is implemented. Without going through too many details, PCA finds a new set of axis (the principal components) that maximize the amount of variance captured in the data. The original data is then projected down onto these principal components. The idea is that we can use fewer principal components than the original number of features while still capturing most of the variance. PCA is implemented in Scikit-Learn in the same way as preprocessing methods. We can either select the number of new components, or the fraction of variance we want explained in the data. If we pass in no argument, the number of principal components will be the same as the number of original features. We can then use the `variance_explained_ratio_` to determine the number of components needed for different threshold of variance retained.",pca exampl go quick exampl show pca implement without go mani detail pca find new set axi princip compon maxim amount varianc captur data origin data project onto princip compon idea use fewer princip compon origin number featur still captur varianc pca implement scikit learn way preprocess method either select number new compon fraction varianc want explain data pas argument number princip compon number origin featur use varianc explain ratio determin number compon need differ threshold varianc retain
3692,We only need a few prinicipal components to account for the majority of variance in the data. We can use the first two principal components to visualize the entire dataset. We will color the datapoints by the value of the target to see if using two principal components clearly separates the classes.,need prinicip compon account major varianc data use first two princip compon visual entir dataset color datapoint valu target see use two princip compon clearli separ class
3693,"Even though we have accounted for most of the variance, that does not mean the pca decomposition makes the problem of identifying loans repaid vs not repaid any easier. PCA does not consider the value of the label when projecting the features to a lower dimension. Feel free to try a classifier on top of this data, but when I have done so, I noticed that it was not very accurate. ",even though account varianc mean pca decomposit make problem identifi loan repaid v repaid easier pca consid valu label project featur lower dimens feel free tri classifi top data done notic accur
3694,"# Conclusions

In this notebook we employed a number of feature selection methods. These methods are necessary to reduce the number of features to increase model interpretability, decrease model runtime, and increase generalization performance on the test set. The methods of feature selection we used are:

1. Remove highly collinear variables as measured by a correlation coefficient greater than 0.9
2. Remove any columns with more than 75% missing values.
3. Remove any features with a zero importance as determined by a gradient boosting machine.
4. (Optional) keep only enough features to account for 95% of the importance in the gradient boosting machine.

Using the first three methods, we reduced the number of features from __1465__ to __536__ with a 5-fold cv AUC ROC score of 0.7838 and a public leaderboard score of 0.783.

After applying the fourth method, we end up with 342 features with a 5-fold cv AUC SCORE of 0.7482 and a public leaderboard score of 0.782. 

Going forward, we might actually want to add _more_ features except this time, instead of naively applying aggregations, think about what features are actually important from a domain point of view. There are a number of kernels that have created useful features that we can add to our set here to improve performance. The process of feature engineering - feature selection is iterative, and it may require several more passes before we get it completely right! ",conclus notebook employ number featur select method method necessari reduc number featur increas model interpret decreas model runtim increas gener perform test set method featur select use 1 remov highli collinear variabl measur correl coeffici greater 0 9 2 remov column 75 miss valu 3 remov featur zero import determin gradient boost machin 4 option keep enough featur account 95 import gradient boost machin use first three method reduc number featur 1465 536 5 fold cv auc roc score 0 7838 public leaderboard score 0 783 appli fourth method end 342 featur 5 fold cv auc score 0 7482 public leaderboard score 0 782 go forward might actual want add featur except time instead naiv appli aggreg think featur actual import domain point view number kernel creat use featur add set improv perform process featur engin featur select iter may requir sever pas get complet right
3695,"# Introduction: Manual Feature Engineering (part two)

In this notebook we will expand on the [Introduction to Manual Feature Engineering](https://www.kaggle.com/willkoehrsen/introduction-to-manual-feature-engineering/output) notebook. We will use the aggregation and value counting functions developed in that notebook in order to incorporate information from the `previous_application`, `POS_CASH_balance`, `installments_payments`, and `credit_card_balance` data files. We already used the information from the `bureau` and `bureau_balance` in the previous notebook and were able to improve our competition score compared to using only the `application` data. After running a model with the features included here, performance does increase, but we run into issues with an explosion in the number of features! I'm working on a notebook of feature selection, but for this notebook we will continue building up a rich set of data for our model. 

The definitions of the four additional data files are:

* previous_application (called `previous`): previous applications for loans at Home Credit of clients who have loans in the application data. Each current loan in the application data can have multiple previous loans. Each previous application has one row and is identified by the feature SK_ID_PREV.
* POS_CASH_BALANCE (called `cash`): monthly data about previous point of sale or cash loans clients have had with Home Credit. Each row is one month of a previous point of sale or cash loan, and a single previous loan can have many rows.
* credit_card_balance (called `credit`): monthly data about previous credit cards clients have had with Home Credit. Each row is one month of a credit card balance, and a single credit card can have many rows.
* installments_payment (called `installments`): payment history for previous loans at Home Credit. There is one row for every made payment and one row for every missed payment.",introduct manual featur engin part two notebook expand introduct manual featur engin http www kaggl com willkoehrsen introduct manual featur engin output notebook use aggreg valu count function develop notebook order incorpor inform previou applic po cash balanc instal payment credit card balanc data file alreadi use inform bureau bureau balanc previou notebook abl improv competit score compar use applic data run model featur includ perform increas run issu explos number featur work notebook featur select notebook continu build rich set data model definit four addit data file previou applic call previou previou applic loan home credit client loan applic data current loan applic data multipl previou loan previou applic one row identifi featur sk id prev po cash balanc call cash monthli data previou point sale cash loan client home credit row one month previou point sale cash loan singl previou loan mani row credit card balanc call credit monthli data previou credit card client home credit row one month credit card balanc singl credit card mani row instal payment call instal payment histori previou loan home credit one row everi made payment one row everi miss payment
3696,"# Functions 

We spent quite a bit of time developing two functions in the previous notebook:

* `agg_numeric`: calculate aggregation statistics (`mean`, `count`, `max`, `min`) for numeric variables.
* `agg_categorical`: compute counts and normalized counts of each category in a categorical variable.

Together, these two functions can extract information about both the numeric and categorical data in a dataframe. Our general approach will be to apply both of these functions to the dataframes, grouping by the client id, `SK_ID_CURR`. For the `POS_CASH_balance`, `credit_card_balance`, and `installment_payments`, we can first group by the `SK_ID_PREV`, the unique id for the previous loan. Then we will group the resulting dataframe by the `SK_ID_CURR` to calculate the aggregation statistics for each client across all of their previous loans. If that's a little confusing, I'd suggest heading back to the [first feature engineering notebook](https://www.kaggle.com/willkoehrsen/introduction-to-manual-feature-engineering/output).**",function spent quit bit time develop two function previou notebook agg numer calcul aggreg statist mean count max min numer variabl agg categor comput count normal count categori categor variabl togeth two function extract inform numer categor data datafram gener approach appli function datafram group client id sk id curr po cash balanc credit card balanc instal payment first group sk id prev uniqu id previou loan group result datafram sk id curr calcul aggreg statist client across previou loan littl confus suggest head back first featur engin notebook http www kaggl com willkoehrsen introduct manual featur engin output
3697,"## Function to Aggregate Numeric Data

This groups data by the `group_var` and calculates `mean`, `max`, `min`, and `sum`. It will only be applied to numeric data by default in pandas.",function aggreg numer data group data group var calcul mean max min sum appli numer data default panda
3698,"### Function to Calculate Categorical Counts

This function calculates the occurrences (counts) of each category in a categorical variable for each client. It also calculates the normed count, which is the count for a category divided by the total counts for all categories in a categorical variable. ",function calcul categor count function calcul occurr count categori categor variabl client also calcul norm count count categori divid total count categori categor variabl
3699,"### Function for KDE Plots of Variable

We also made a function that plots the distribution of variable colored by the value of `TARGET` (either 1 for did not repay the loan or 0 for did repay the loan). We can use this function to visually examine any new variables we create. This also calculates the correlation cofficient of the variable with the target which can be used as an approximation of whether or not the created variable will be useful. ",function kde plot variabl also made function plot distribut variabl color valu target either 1 repay loan 0 repay loan use function visual examin new variabl creat also calcul correl coffici variabl target use approxim whether creat variabl use
3700,"# Function to Convert Data Types

This will help reduce memory usage by using more efficient types for the variables. For example `category` is often a better type than `object` (unless the number of unique categories is close to the number of rows in the dataframe).",function convert data type help reduc memori usag use effici type variabl exampl categori often better type object unless number uniqu categori close number row datafram
3701,Let's deal with one dataframe at a time. First up is the `previous_applications`. This has one row for every previous loan a client had at Home Credit. A client can have multiple previous loans which is why we need to aggregate statistics for each client.,let deal one datafram time first previou applic one row everi previou loan client home credit client multipl previou loan need aggreg statist client
3702,### previous_application,previou applic
3703,We can join the calculated dataframe to the main training dataframe using a merge. Then we should delete the calculated dataframes to avoid using too much of the kernel memory.,join calcul datafram main train datafram use merg delet calcul datafram avoid use much kernel memori
3704,"We are going to have to be careful about calculating too many features. We don't want to overwhelm the model with too many irrelevant features or features with too many missing values. In the previous notebook, we removed any features with more than 75% missing values. To be consistent, we will apply that same logic here. ",go care calcul mani featur want overwhelm model mani irrelev featur featur mani miss valu previou notebook remov featur 75 miss valu consist appli logic
3705,## Function to Calculate Missing Values,function calcul miss valu
3706,# Applying to More Data,appli data
3707,### Function to Aggregate Stats at the Client Level,function aggreg stat client level
3708,## Monthly Cash Data,monthli cash data
3709,## Monthly Credit Data,monthli credit data
3710,### Installment Payments,instal payment
3711," #### Save All Newly Calculated Features
 
 Unfortunately, saving all the created features does not work in a Kaggle notebook. You will have to run the code on your personal machine. I have run the code and uploaded the [entire datasets here](https://www.kaggle.com/willkoehrsen/home-credit-manual-engineered-features). I plan on doing some feature selection and uploading reduced versions of the datasets. Right now, they are slightly to big to handle in Kaggle notebooks or scripts. .",save newli calcul featur unfortun save creat featur work kaggl notebook run code person machin run code upload entir dataset http www kaggl com willkoehrsen home credit manual engin featur plan featur select upload reduc version dataset right slightli big handl kaggl notebook script
3712,## Modeling,model
3713,"# Introduction: Manual Feature Engineering

If you are new to this competition, I highly suggest checking out [this notebook](https://www.kaggle.com/willkoehrsen/start-here-a-gentle-introduction/) to get started.

In this notebook, we will explore making features by hand for the Home Credit Default Risk competition. In an earlier notebook, we used only the `application` data in order to build a model. The best model we made from this data achieved a score on the leaderboard around 0.74. In order to better this score, we will have to include more information from the other dataframes. Here, we will look at using information from the `bureau` and `bureau_balance` data. The definitions of these data files are:

* bureau: information about client's previous loans with other financial institutions reported to Home Credit. Each previous loan has its own row.
* bureau_balance: monthly information about the previous loans. Each month has its own row.

Manual feature engineering can be a tedious process (which is why we use automated feature engineering with featuretools!) and often relies on domain expertise. Since I have limited domain knowledge of loans and what makes a person likely to default, I will instead concentrate of getting as much info as possible into the final training dataframe. The idea is that the model will then pick up on which features are important rather than us having to decide that. Basically, our approach is to make as many features as possible and then give them all to the model to use! Later, we can perform feature reduction using the feature importances from the model or other techniques such as PCA. 

The process of manual feature engineering will involve plenty of Pandas code, a little patience, and a lot of great practice manipulation data. Even though automated feature engineering tools are starting to be made available, feature engineering will still have to be done using plenty of data wrangling for a little while longer. ",introduct manual featur engin new competit highli suggest check notebook http www kaggl com willkoehrsen start gentl introduct get start notebook explor make featur hand home credit default risk competit earlier notebook use applic data order build model best model made data achiev score leaderboard around 0 74 order better score includ inform datafram look use inform bureau bureau balanc data definit data file bureau inform client previou loan financi institut report home credit previou loan row bureau balanc monthli inform previou loan month row manual featur engin tediou process use autom featur engin featuretool often reli domain expertis sinc limit domain knowledg loan make person like default instead concentr get much info possibl final train datafram idea model pick featur import rather u decid basic approach make mani featur possibl give model use later perform featur reduct use featur import model techniqu pca process manual featur engin involv plenti panda code littl patienc lot great practic manipul data even though autom featur engin tool start made avail featur engin still done use plenti data wrangl littl longer
3714,"## Example: Counts of a client's previous loans

To illustrate the general process of manual feature engineering, we will first simply get the count of a client's previous loans at other financial institutions. This requires a number of Pandas operations we will make heavy use of throughout the notebook:

* `groupby`: group a dataframe by a column. In this case we will group by the unique client, the `SK_ID_CURR` column
* `agg`: perform a calculation on the grouped data such as taking the mean of columns. We can either call the function directly (`grouped_df.mean()`) or use the `agg` function together with a list of transforms (`grouped_df.agg([mean, max, min, sum])`)
* `merge`: match the aggregated statistics to the appropriate client. We need to merge the original training data with the calculated stats on the `SK_ID_CURR` column which will insert `NaN` in any cell for which the client does not have the corresponding statistic

We also use the (`rename`) function quite a bit specifying the columns to be renamed as a dictionary. This is useful in order to keep track of the new variables we create.

This might seem like a lot, which is why we'll eventually write a function to do this process for us. Let's take a look at implementing this by hand first. ",exampl count client previou loan illustr gener process manual featur engin first simpli get count client previou loan financi institut requir number panda oper make heavi use throughout notebook groupbi group datafram column case group uniqu client sk id curr column agg perform calcul group data take mean column either call function directli group df mean use agg function togeth list transform group df agg mean max min sum merg match aggreg statist appropri client need merg origin train data calcul stat sk id curr column insert nan cell client correspond statist also use renam function quit bit specifi column renam dictionari use order keep track new variabl creat might seem like lot eventu write function process u let take look implement hand first
3715,Scroll all the way to the right to see the new column. ,scroll way right see new column
3716,"## Assessing Usefulness of New Variable with r value

To determine if the new variable is useful, we can calculate the Pearson Correlation Coefficient (r-value) between this variable and the target. This measures the strength of a linear relationship between two variables and ranges from -1 (perfectly negatively linear) to +1 (perfectly positively linear). The r-value is not best measure of the ""usefulness"" of a new variable, but it can give a first approximation of whether a variable will be helpful to a machine learning model. The larger the r-value of a variable with respect to the target, the more a change in this variable is likely to affect the value of the target. Therefore, we look for the variables with the greatest absolute value r-value relative to the target.

We can also visually inspect a relationship with the target using the Kernel Density Estimate (KDE) plot. ",ass use new variabl r valu determin new variabl use calcul pearson correl coeffici r valu variabl target measur strength linear relationship two variabl rang 1 perfectli neg linear 1 perfectli posit linear r valu best measur use new variabl give first approxim whether variabl help machin learn model larger r valu variabl respect target chang variabl like affect valu target therefor look variabl greatest absolut valu r valu rel target also visual inspect relationship target use kernel densiti estim kde plot
3717,"### Kernel Density Estimate Plots

The kernel density estimate plot shows the distribution of a single variable (think of it as a smoothed histogram). To see the different in distributions dependent on the value of a categorical variable, we can color the distributions differently according to the category. For example, we can show the kernel density estimate of the `previous_loan_count` colored by whether the `TARGET` = 1 or 0. The resulting KDE will show any significant differences in the distribution of the variable between people who did not repay their loan (`TARGET == 1`) and the people who did (`TARGET == 0`). This can serve as an indicator of whether a variable will be 'relevant' to a machine learning model. 

We will put this plotting functionality in a function to re-use for any variable. ",kernel densiti estim plot kernel densiti estim plot show distribut singl variabl think smooth histogram see differ distribut depend valu categor variabl color distribut differ accord categori exampl show kernel densiti estim previou loan count color whether target 1 0 result kde show signific differ distribut variabl peopl repay loan target 1 peopl target 0 serv indic whether variabl relev machin learn model put plot function function use variabl
3718,We can test this function using the `EXT_SOURCE_3` variable which we [found to be one of the most important variables ](https://www.kaggle.com/willkoehrsen/start-here-a-gentle-introduction) according to a Random Forest and Gradient Boosting Machine. ,test function use ext sourc 3 variabl found one import variabl http www kaggl com willkoehrsen start gentl introduct accord random forest gradient boost machin
3719,"Now for the new variable we just made, the number of previous loans at other institutions.",new variabl made number previou loan institut
3720,"From this it's difficult to tell if this variable will be important. The correlation coefficient is extremely weak and there is almost no noticeable difference in the distributions. 

Let's move on to make a few more variables from the bureau dataframe. We will take the mean, min, and max of every numeric column in the bureau dataframe.",difficult tell variabl import correl coeffici extrem weak almost notic differ distribut let move make variabl bureau datafram take mean min max everi numer column bureau datafram
3721,"## Aggregating Numeric Columns

To account for the numeric information in the `bureau` dataframe, we can compute statistics for all the numeric columns. To do so, we `groupby` the client id, `agg` the grouped dataframe, and merge the result back into the training data. The `agg` function will only calculate the values for the numeric columns where the operation is considered valid. We will stick to using `'mean', 'max', 'min', 'sum'` but any function can be passed in here. We can even write our own function and use it in an `agg` call. ",aggreg numer column account numer inform bureau datafram comput statist numer column groupbi client id agg group datafram merg result back train data agg function calcul valu numer column oper consid valid stick use mean max min sum function pas even write function use agg call
3722,"We need to create new names for each of these columns. The following code makes new names by appending the stat to the name. Here we have to deal with the fact that the dataframe has a multi-level index. I find these confusing and hard to work with, so I try to reduce to a single level index as quickly as possible.",need creat new name column follow code make new name append stat name deal fact datafram multi level index find confus hard work tri reduc singl level index quickli possibl
3723,Now we simply merge with the training data as we did before.,simpli merg train data
3724,"### Correlations of Aggregated Values with Target

We can calculate the correlation of all new values with the target. Again, we can use these as an approximation of the variables which may be important for modeling. ",correl aggreg valu target calcul correl new valu target use approxim variabl may import model
3725,"In the code below, we sort the correlations by the magnitude (absolute value) using the `sorted` Python function. We also make use of an anonymous `lambda` function, another important Python operation that is good to know. ",code sort correl magnitud absolut valu use sort python function also make use anonym lambda function anoth import python oper good know
3726,"None of the new variables have a significant correlation with the TARGET. We can look at the KDE plot of the highest correlated variable, `bureau_DAYS_CREDIT_mean`, with the target in  in terms of absolute magnitude correlation. ",none new variabl signific correl target look kde plot highest correl variabl bureau day credit mean target term absolut magnitud correl
3727,"The definition of this column is: ""How many days before current application did client apply for Credit Bureau credit"". My interpretation is this is the number of days that the previous loan was applied for before the application for a loan at Home Credit. Therefore, a larger negative number indicates the loan was further before the current loan application. We see an extremely weak positive relationship between the average of this variable and the target meaning that clients who applied for loans further in the past potentially are more likely to repay loans at Home Credit. With a correlation this weak though, it is just as likely to be noise as a signal. 

#### The Multiple Comparisons Problem

When we have lots of variables, we expect some of them to be correlated just by pure chance, a [problem known as multiple comparisons](https://towardsdatascience.com/the-multiple-comparisons-problem-e5573e8b9578). We can make hundreds of features, and some will turn out to be corelated with the target simply because of random noise in the data. Then, when our model trains, it may overfit to these variables because it thinks they have a relationship with the target in the training set, but this does not necessarily generalize to the test set. There are many considerations that we have to take into account when making features! ",definit column mani day current applic client appli credit bureau credit interpret number day previou loan appli applic loan home credit therefor larger neg number indic loan current loan applic see extrem weak posit relationship averag variabl target mean client appli loan past potenti like repay loan home credit correl weak though like nois signal multipl comparison problem lot variabl expect correl pure chanc problem known multipl comparison http towardsdatasci com multipl comparison problem e5573e8b9578 make hundr featur turn corel target simpli random nois data model train may overfit variabl think relationship target train set necessarili gener test set mani consider take account make featur
3728,"## Function for Numeric Aggregations

Let's encapsulate all of the previous work into a function. This will allow us to compute aggregate stats for numeric columns across any dataframe. We will re-use this function when we want to apply the same operations for other dataframes.",function numer aggreg let encapsul previou work function allow u comput aggreg stat numer column across datafram use function want appli oper datafram
3729,"To make sure the function worked as intended, we should compare with the aggregated dataframe we constructed by hand. ",make sure function work intend compar aggreg datafram construct hand
3730,"If we go through and inspect the values, we do find that they are equivalent. We will be able to reuse this function for calculating numeric stats for other dataframes. Using functions allows for consistent results and decreases the amount of work we have to do in the future! 

### Correlation Function

Before we move on, we can also make the code to calculate correlations with the target into a function.",go inspect valu find equival abl reus function calcul numer stat datafram use function allow consist result decreas amount work futur correl function move also make code calcul correl target function
3731,"## Categorical Variables

Now we move from the numeric columns to the categorical columns. These are discrete string variables, so we cannot just calculate statistics such as mean 
and max which only work with numeric variables. Instead, we will rely on calculating value counts of each category within each categorical variable. As an example, if we have the following dataframe:

| SK_ID_CURR | Loan type |
|------------|-----------|
| 1          | home      |
| 1          | home      |
| 1          | home      |
| 1          | credit    |
| 2          | credit    |
| 3          | credit    |
| 3          | cash      |
| 3          | cash      |
| 4          | credit    |
| 4          | home      |
| 4          | home      |

we will use this information counting the number of loans in each category for each client. 

| SK_ID_CURR | credit count | cash count | home count | total count |
|------------|--------------|------------|------------|-------------|
| 1          | 1            | 0          | 3          | 4           |
| 2          | 1            | 0          | 0          | 1           |
| 3          | 1            | 2          | 0          | 3           |
| 4          | 1            | 0          | 2          | 3           |


Then we can normalize these value counts by the total number of occurences of that categorical variable for that observation (meaning that the normalized counts must sum to 1.0 for each observation).

| SK_ID_CURR | credit count | cash count | home count | total count | credit count norm | cash count norm | home count norm |
|------------|--------------|------------|------------|-------------|-------------------|-----------------|-----------------|
| 1          | 1            | 0          | 3          | 4           | 0.25              | 0               | 0.75            |
| 2          | 1            | 0          | 0          | 1           | 1.00              | 0               | 0               |
| 3          | 1            | 2          | 0          | 3           | 0.33              | 0.66            | 0               |
| 4          | 1            | 0          | 2          | 3           | 0.33              | 0               | 0.66            |

Hopefully, encoding the categorical variables this way will allow us to capture the information they contain. If anyone has a better idea for this process, please let me know in the comments!
We will now go through this process step-by-step. At the end, we will wrap up all the code into one function to be re-used for many dataframes.",categor variabl move numer column categor column discret string variabl calcul statist mean max work numer variabl instead reli calcul valu count categori within categor variabl exampl follow datafram sk id curr loan type 1 home 1 home 1 home 1 credit 2 credit 3 credit 3 cash 3 cash 4 credit 4 home 4 home use inform count number loan categori client sk id curr credit count cash count home count total count 1 1 0 3 4 2 1 0 0 1 3 1 2 0 3 4 1 0 2 3 normal valu count total number occur categor variabl observ mean normal count must sum 1 0 observ sk id curr credit count cash count home count total count credit count norm cash count norm home count norm 1 1 0 3 4 0 25 0 0 75 2 1 0 0 1 1 00 0 0 3 1 2 0 3 0 33 0 66 0 4 1 0 2 3 0 33 0 0 66 hope encod categor variabl way allow u captur inform contain anyon better idea process plea let know comment go process step step end wrap code one function use mani datafram
3732,First we one-hot encode a dataframe with only the categorical columns (`dtype == 'object'`).,first one hot encod datafram categor column dtype object
3733,"The `sum` columns represent the count of that category for the associated client and the `mean` represents the normalized count. One-hot encoding makes the process of calculating these figures very easy!

We can use a similar function as before to rename the columns. Again, we have to deal with the multi-level index for the columns. We iterate through the first level (level 0) which is the name of the categorical variable appended with the value of the category (from one-hot encoding). Then we iterate  stats we calculated for each client. We will rename the column with the level 0 name appended with the stat. As an example, the column with `CREDIT_ACTIVE_Active` as level 0 and `sum` as level 1 will become `CREDIT_ACTIVE_Active_count`. ",sum column repres count categori associ client mean repres normal count one hot encod make process calcul figur easi use similar function renam column deal multi level index column iter first level level 0 name categor variabl append valu categori one hot encod iter stat calcul client renam column level 0 name append stat exampl column credit activ activ level 0 sum level 1 becom credit activ activ count
3734,"The sum column records the counts and the mean column records the normalized count. 

We can merge this dataframe into the training data.",sum column record count mean column record normal count merg datafram train data
3735,"### Function to Handle Categorical Variables

To make the code more efficient, we can now write a function to handle the categorical variables for us. This will take the same form as the `agg_numeric` function in that it accepts a dataframe and a grouping variable. Then it will calculate the counts and normalized counts of each category for all categorical variables in the dataframe.",function handl categor variabl make code effici write function handl categor variabl u take form agg numer function accept datafram group variabl calcul count normal count categori categor variabl datafram
3736,"### Applying Operations to another dataframe

We will now turn to the bureau balance dataframe. This dataframe has monthly information about each client's previous loan(s) with other financial institutions. Instead of grouping this dataframe by the `SK_ID_CURR` which is the client id, we will first group the dataframe by the `SK_ID_BUREAU` which is the id of the previous loan. This will give us one row of the dataframe for each loan. Then, we can group by the `SK_ID_CURR` and calculate the aggregations across the loans of each client. The final result will be a dataframe with one row for each client, with stats calculated for their loans.",appli oper anoth datafram turn bureau balanc datafram datafram monthli inform client previou loan financi institut instead group datafram sk id curr client id first group datafram sk id bureau id previou loan give u one row datafram loan group sk id curr calcul aggreg across loan client final result datafram one row client stat calcul loan
3737,"First, we can calculate the value counts of each status for each loan. Fortunately, we already have a function that does this for us! ",first calcul valu count statu loan fortun alreadi function u
3738,"Now we can handle the one numeric column. The `MONTHS_BALANCE` column has the ""months of balance relative to application date."" This might not necessarily be that important as a numeric variable, and in future work we might want to consider this as a time variable. For now, we can just calculate the same aggregation statistics as previously. ",handl one numer column month balanc column month balanc rel applic date might necessarili import numer variabl futur work might want consid time variabl calcul aggreg statist previous
3739,"The above dataframes have the calculations done on each _loan_. Now we need to aggregate these for each _client_. We can do this by merging the dataframes together first and then since all the variables are numeric, we just need to aggregate the statistics again, this time grouping by the `SK_ID_CURR`. ",datafram calcul done loan need aggreg client merg datafram togeth first sinc variabl numer need aggreg statist time group sk id curr
3740,"To recap, for the `bureau_balance` dataframe we:

1. Calculated numeric stats grouping by each loan
2. Made value counts of each categorical variable grouping by loan
3. Merged the stats and the value counts on the loans
4. Calculated numeric stats for the resulting dataframe grouping by the client id

The final resulting dataframe has one row for each client, with statistics calculated for all of their loans with monthly balance information. 

Some of these variables are a little confusing, so let's try to explain a few:

* `client_bureau_balance_MONTHS_BALANCE_mean_mean`: For each loan calculate the mean value of `MONTHS_BALANCE`. Then for each client, calculate the mean of this value for all of their loans. 
* `client_bureau_balance_STATUS_X_count_norm_sum`: For each loan, calculate the number of occurences of `STATUS` == X divided by the number of total `STATUS` values for the loan. Then, for each client, add up the values for each loan. ",recap bureau balanc datafram 1 calcul numer stat group loan 2 made valu count categor variabl group loan 3 merg stat valu count loan 4 calcul numer stat result datafram group client id final result datafram one row client statist calcul loan monthli balanc inform variabl littl confus let tri explain client bureau balanc month balanc mean mean loan calcul mean valu month balanc client calcul mean valu loan client bureau balanc statu x count norm sum loan calcul number occur statu x divid number total statu valu loan client add valu loan
3741,We will hold off on calculating the correlations until we have all the variables together in one dataframe. ,hold calcul correl variabl togeth one datafram
3742,"# Putting the Functions Together

We now have all the pieces in place to take the information from the previous loans at other institutions and the monthly payments information about these loans and put them into the main training dataframe. Let's do a reset of all the variables and then use the functions we built to do this from the ground up. This demonstrate the benefit of using functions for repeatable workflows! ",put function togeth piec place take inform previou loan institut monthli payment inform loan put main train datafram let reset variabl use function built ground demonstr benefit use function repeat workflow
3743,### Counts of Bureau Dataframe,count bureau datafram
3744,### Aggregated Stats of Bureau Dataframe,aggreg stat bureau datafram
3745,### Value counts of Bureau Balance dataframe by loan,valu count bureau balanc datafram loan
3746,### Aggregated stats of Bureau Balance dataframe by loan,aggreg stat bureau balanc datafram loan
3747,### Aggregated Stats of Bureau Balance by Client,aggreg stat bureau balanc client
3748,## Insert Computed Features into Training Data,insert comput featur train data
3749,"# Feature Engineering Outcomes

After all that work, now we want to take a look at the variables we have created. We can look at the percentage of missing values, the correlations of variables with the target, and also the correlation of variables with the other variables. The correlations between variables can show if we have collinear varibles, that is, variables that are highly correlated with one another. Often, we want to remove one in a pair of collinear variables because having both variables would be redundant. We can also use the percentage of missing values to remove features with a substantial majority of values that are not present. __Feature selection__ will be an important focus going forward, because reducing the number of features can help the model learn during training and also generalize better to the testing data. The ""curse of dimensionality"" is the name given to the issues caused by having too many features (too high of a dimension). As the number of variables increases, the number of datapoints needed to learn the relationship between these variables and the target value increases exponentially. 

Feature selection is the process of removing variables to help our model to learn and generalize better to the testing set. The objective is to remove useless/redundant variables while preserving those that are useful. There are a number of tools we can use for this process, but in this notebook we will stick to removing columns with a high percentage of missing values and variables that have a high correlation with one another. Later we can look at using the feature importances returned from models such as the `Gradient Boosting Machine` or `Random Forest` to perform feature selection.",featur engin outcom work want take look variabl creat look percentag miss valu correl variabl target also correl variabl variabl correl variabl show collinear varibl variabl highli correl one anoth often want remov one pair collinear variabl variabl would redund also use percentag miss valu remov featur substanti major valu present featur select import focu go forward reduc number featur help model learn train also gener better test data cur dimension name given issu caus mani featur high dimens number variabl increas number datapoint need learn relationship variabl target valu increas exponenti featur select process remov variabl help model learn gener better test set object remov useless redund variabl preserv use number tool use process notebook stick remov column high percentag miss valu variabl high correl one anoth later look use featur import return model gradient boost machin random forest perform featur select
3750,"## Missing Values

An important consideration is the missing values in the dataframe. Columns with too many missing values might have to be dropped. ",miss valu import consider miss valu datafram column mani miss valu might drop
3751,"We see there are a number of columns with a high percentage of missing values. There is no well-established threshold for removing missing values, and the best course of action depends on the problem. Here, to reduce the number of features, we will remove any columns in either the training or the testing data that have greater than 90% missing values.",see number column high percentag miss valu well establish threshold remov miss valu best cours action depend problem reduc number featur remov column either train test data greater 90 miss valu
3752,"Before we remove the missing values, we will find the missing value percentages in the testing data. We'll then remove any columns with greater than 90% missing values in either the training or testing data.
Let's now read in the testing data, perform the same operations, and look at the missing values in the testing data. We already have calculated all the counts and aggregation statistics, so we only need to merge the testing data with the appropriate data. ",remov miss valu find miss valu percentag test data remov column greater 90 miss valu either train test data let read test data perform oper look miss valu test data alreadi calcul count aggreg statist need merg test data appropri data
3753,## Calculate Information for Testing Data,calcul inform test data
3754,"We need to align the testing and training dataframes, which means matching up the columns so they have the exact same columns. This shouldn't be an issue here, but when we one-hot encode variables, we need to align the dataframes to make sure they have the same columns.",need align test train datafram mean match column exact column issu one hot encod variabl need align datafram make sure column
3755,"The dataframes now have the same columns (with the exception of the `TARGET` column in the training data). This means we can use them in a machine learning model which needs to see the same columns in both the training and testing dataframes.

Let's now look at the percentage of missing values in the testing data so we can figure out the columns that should be dropped.",datafram column except target column train data mean use machin learn model need see column train test datafram let look percentag miss valu test data figur column drop
3756,We ended up removing no columns in this round because there are no columns with more than 90% missing values. We might have to apply another feature selection method to reduce the dimensionality. ,end remov column round column 90 miss valu might appli anoth featur select method reduc dimension
3757,At this point we will save both the training and testing data. I encourage anyone to try different percentages for dropping the missing columns and compare the outcomes. ,point save train test data encourag anyon tri differ percentag drop miss column compar outcom
3758,"## Correlations

First let's look at the correlations of the variables with the target. We can see in any of the variables we created have a greater correlation than those already present in the training data (from `application`). ",correl first let look correl variabl target see variabl creat greater correl alreadi present train data applic
3759,"The highest correlated variable with the target (other than the `TARGET` which of course has a correlation of 1), is a variable we created. However, just because the variable is correlated does not mean that it will be useful, and we have to remember that if we generate hundreds of new variables, some are going to be correlated with the target simply because of random noise. 

Viewing the correlations skeptically, it does appear that several of the newly created variables may be useful. To assess the ""usefulness"" of variables, we will look at the feature importances returned by the model. For curiousity's sake (and because we already wrote the function) we can make a kde plot of two of the newly created variables.",highest correl variabl target target cours correl 1 variabl creat howev variabl correl mean use rememb gener hundr new variabl go correl target simpli random nois view correl skeptic appear sever newli creat variabl may use ass use variabl look featur import return model curious sake alreadi wrote function make kde plot two newli creat variabl
3760,"This variable represents the average number of monthly records per loan for each client. For example, if a client had three previous loans with 3, 4, and 5 records in the monthly data, the value of this variable for them would be 4. Based on the distribution, clients with a greater number of average monthly records per loan were more likely to repay their loans with Home Credit. Let's not read too much into this value, but it could indicate that clients who have had more previous credit history are generally more likely to repay a loan.",variabl repres averag number monthli record per loan client exampl client three previou loan 3 4 5 record monthli data valu variabl would 4 base distribut client greater number averag monthli record per loan like repay loan home credit let read much valu could indic client previou credit histori gener like repay loan
3761,Well this distribution is all over the place. This variable represents the number of previous loans with a `CREDIT_ACTIVE` value of `Active` divided by the total number of previous loans for a client. The correlation here is so weak that I do not think we should draw any conclusions! ,well distribut place variabl repres number previou loan credit activ valu activ divid total number previou loan client correl weak think draw conclus
3762,"### Collinear Variables

We can calculate not only the correlations of the variables with the target, but also the correlation of each variable with every other variable. This will allow us to see if there are highly collinear variables that should perhaps be removed from the data. 

Let's look for any variables that have a greather than 0.8 correlation with other variables.",collinear variabl calcul correl variabl target also correl variabl everi variabl allow u see highli collinear variabl perhap remov data let look variabl greather 0 8 correl variabl
3763,"For each of these pairs of highly correlated variables, we only want to remove one of the variables. The following code creates a set of variables to remove by only adding one of each pair. ",pair highli correl variabl want remov one variabl follow code creat set variabl remov ad one pair
3764,We can remove these columns from both the training and the testing datasets. We will have to compare performance after removing these variables with performance keeping these variables (the raw csv files we saved earlier).,remov column train test dataset compar perform remov variabl perform keep variabl raw csv file save earlier
3765,"# Modeling 

To actually test the performance of these new datasets, we will try using them for machine learning! Here we will use a function I developed in another notebook to compare the features (the raw version with the highly correlated variables removed). We can run this kind of like an experiment, and the control will be the performance of just the `application` data in this function when submitted to the competition. I've already recorded that performance, 
so we can list out our control and our two test conditions:

__For all datasets, use the model shown below (with the exact hyperparameters).__

* control: only the data in the `application` files. 
* test one: the data in the `application` files with all of the data recorded from the `bureau` and `bureau_balance` files
* test two: the data in the `application` files with all of the data recorded from the `bureau` and `bureau_balance` files with highly correlated variables removed. ",model actual test perform new dataset tri use machin learn use function develop anoth notebook compar featur raw version highli correl variabl remov run kind like experi control perform applic data function submit competit alreadi record perform list control two test condit dataset use model shown exact hyperparamet control data applic file test one data applic file data record bureau bureau balanc file test two data applic file data record bureau bureau balanc file highli correl variabl remov
3766,"### Control

The first step in any experiment is establishing a control. For this we will use the function defined above (that implements a Gradient Boosting Machine model) and the single main data source (`application`). ",control first step experi establish control use function defin implement gradient boost machin model singl main data sourc applic
3767,"Fortunately, once we have taken the time to write a function, using it is simple (if there's a central theme in this notebook, it's use functions to make things simpler and reproducible!). The function above returns a `submission` dataframe we can upload to the competition, a `fi` dataframe of feature importances, and a `metrics` dataframe with validation and test performance. ",fortun taken time write function use simpl central theme notebook use function make thing simpler reproduc function return submiss datafram upload competit fi datafram featur import metric datafram valid test perform
3768,"The control slightly overfits because the training score is higher than the validation score. We can address this in later notebooks when we look at regularization (we already perform some regularization in this model by using `reg_lambda` and `reg_alpha` as well as early stopping). 

We can visualize the feature importance with another function, `plot_feature_importances`. The feature importances may be useful when it's time for feature selection. ",control slightli overfit train score higher valid score address later notebook look regular alreadi perform regular model use reg lambda reg alpha well earli stop visual featur import anoth function plot featur import featur import may use time featur select
3769,__The control scores 0.745 when submitted to the competition.__,control score 0 745 submit competit
3770,"### Test One

Let's conduct the first test. We will just need to pass in the data to the function, which does most of the work for us.",test one let conduct first test need pas data function work u
3771,"Based on these numbers, the engineered features perform better than the control case. However, we will have to submit the predictions to the leaderboard before we can say if this better validation performance transfers to the testing data. ",base number engin featur perform better control case howev submit predict leaderboard say better valid perform transfer test data
3772,"Examining the feature improtances, it looks as if a few of the feature we constructed are among the most important. Let's find the percentage of the top 100 most important features that we made in this notebook. However, rather than just compare to the original features, we need to compare to the _one-hot encoded_ original features. These are already recorded for us in `fi` (from the original data). ",examin featur improt look featur construct among import let find percentag top 100 import featur made notebook howev rather compar origin featur need compar one hot encod origin featur alreadi record u fi origin data
3773,Over half of the top 100 features were made by us! That should give us confidence that all the hard work we did was worthwhile. ,half top 100 featur made u give u confid hard work worthwhil
3774,__Test one scores 0.759 when submitted to the competition.__,test one score 0 759 submit competit
3775,"### Test Two

That was easy, so let's do another run! Same as before but with the highly collinear variables removed. ",test two easi let anoth run highli collinear variabl remov
3776,"These results are better than the control, but slightly lower than the raw features. ",result better control slightli lower raw featur
3777,__Test Two scores 0.753 when submitted to the competition.__,test two score 0 753 submit competit
3778,"# Results

After all that work, we can say that including the extra information did improve performance! The model is definitely not optimized to our data, but we still had a noticeable improvement over the original dataset when using the calculated features. Let's officially summarize the performances:

| __Experiment__ | __Train AUC__ | __Validation AUC__ | __Test AUC__  |
|------------|-------|------------|-------|
| __Control__    | 0.815 | 0.760      | 0.745 |
| __Test One__   | 0.837 | 0.767      | 0.759 |
| __Test Two__   | 0.826 | 0.765      | 0.753 |


(Note that these scores may change from run to run of the notebook. I have not observed that the general ordering changes however.)

All of our hard work translates to a small improvement of 0.014 ROC AUC over the original testing data. Removing the highly collinear variables slightly decreases performance so we will want to consider a different method for feature selection. Moreover, we can say that some of the features we built are among the most important as judged by the model. 

In a competition such as this, even an improvement of this size is enough to move us up 100s of spots on the leaderboard. By making numerous small improvements such as in this notebook, we can gradually achieve better and better performance. I encourage others to use the results here to make their own improvements, and I will continue to document the steps I take to help others. 

## Next Steps

Going forward, we can now use the functions we developed in this notebook on the other datasets. There are still 4 other data files to use in our model! In the next notebook, we will incorporate the information from these other data files (which contain information on previous loans at Home Credit) into our training data. Then we can build the same model and run more experiments to determine the effect of our feature engineering. There is plenty more work to be done in this competition, and plenty more gains in performance to be had! I'll see you in the next notebook.",result work say includ extra inform improv perform model definit optim data still notic improv origin dataset use calcul featur let offici summar perform experi train auc valid auc test auc control 0 815 0 760 0 745 test one 0 837 0 767 0 759 test two 0 826 0 765 0 753 note score may chang run run notebook observ gener order chang howev hard work translat small improv 0 014 roc auc origin test data remov highli collinear variabl slightli decreas perform want consid differ method featur select moreov say featur built among import judg model competit even improv size enough move u 100 spot leaderboard make numer small improv notebook gradual achiev better better perform encourag other use result make improv continu document step take help other next step go forward use function develop notebook dataset still 4 data file use model next notebook incorpor inform data file contain inform previou loan home credit train data build model run experi determin effect featur engin plenti work done competit plenti gain perform see next notebook
3779,"# Introduction: Random vs Bayesian Optimization Model Tuning

In this notebook, we will compare random search and Bayesian optimization hyperparameter tuning methods implemented in two previous notebooks.

* [Intro to Model Tuning: Grid and Random Search](https://www.kaggle.com/willkoehrsen/intro-to-model-tuning-grid-and-random-search)
* [Automated Model Tuning](https://www.kaggle.com/willkoehrsen/automated-model-tuning)

In those notebooks we saw results of the methods applied to a limited dataset (10000 observations) but here we will explore results on a complete dataset with 700 + features.  The results in this notebook are from 500 iterations of random search and 400 iterations of Bayesian Optimization (these took about 5 days to run each). We will thoroughly explore the results both visually and statistically, and then implement the best hyperparameter values on a full set of features. After all the hard work in the random search and Bayesian optimization notebooks, now we get to have some fun! 

# Roadmap

Our plan of action is as follows:

1. High Level Overview
    * Which method did best? 
2. Examine distribution of scores
    * Are there trends over the course of the search?
3. Explore hyperparameter values
    * Look at values over the course of the search
    * Identify correlations between hyperparameters and the score
4. Perform ""meta"" machine learning using these results
    * Fit a linear regression to results and look at coefficients
5. Train a model on the full set of features using the best performing values
    * Try best results from both random search and bayesian optimization
6.  Lay out next steps
    * How can we use these results for this _and other_ problems? 
    * Are there better methods for hyperparameter optimization
    
At each step, we will use plenty of figures and statistics to explore the data. This will be a fun notebook (even though it may not land you at the top of the leaderboard)! 

## Recap 

In the respective notebooks, we examined we performed 1000 iterations of random search and Bayesian optimization on a reduced sample of the dataset (10000 rows). We compared the cross-validation ROC AUC on the training data, the score on a ""testing set"" (6000 observations) and the score on the real test set when submitted to the competition leaderboard. Results are below:

| Method                               | Cross Validation Score | Test Score (on 6000 Rows) | Submission to Leaderboard | Iterations to best score |
|--------------------------------------|------------------------|---------------------------|---------------------------|--------------------------|
| Random Search                        | 0.73110                | 0.73274                   | 0.782                     | 996                      |
| Bayesian Hyperparameter Optimization | 0.73448                | 0.73069                   | 0.792                     | 596                      ",introduct random v bayesian optim model tune notebook compar random search bayesian optim hyperparamet tune method implement two previou notebook intro model tune grid random search http www kaggl com willkoehrsen intro model tune grid random search autom model tune http www kaggl com willkoehrsen autom model tune notebook saw result method appli limit dataset 10000 observ explor result complet dataset 700 featur result notebook 500 iter random search 400 iter bayesian optim took 5 day run thoroughli explor result visual statist implement best hyperparamet valu full set featur hard work random search bayesian optim notebook get fun roadmap plan action follow 1 high level overview method best 2 examin distribut score trend cours search 3 explor hyperparamet valu look valu cours search identifi correl hyperparamet score 4 perform meta machin learn use result fit linear regress result look coeffici 5 train model full set featur use best perform valu tri best result random search bayesian optim 6 lay next step use result problem better method hyperparamet optim step use plenti figur statist explor data fun notebook even though may land top leaderboard recap respect notebook examin perform 1000 iter random search bayesian optim reduc sampl dataset 10000 row compar cross valid roc auc train data score test set 6000 observ score real test set submit competit leaderboard result method cross valid score test score 6000 row submiss leaderboard iter best score random search 0 73110 0 73274 0 782 996 bayesian hyperparamet optim 0 73448 0 73069 0 792 596
3780,"__Take these with some skepticism because they were performed on a very small subset of the data!__ 

For more rigorous results, we will turn to the evaluation metrics from running __500 iterations (with random search)__ and __400+ iterations (with Bayesian Optimization)__ on a full training dataset with about 700 features (the features are from [this notebook](https://www.kaggle.com/jsaguiar/updated-0-792-lb-lightgbm-with-simple-features) by [Aguiar](https://www.kaggle.com/jsaguiar)). These iterations took around 6 days on a machine with 128 GB of RAM so they will not run in a kernel! The Bayesian Optimization method is still running and I will update the results as they finish.

__In this notebook  we will focus only on the results and building the best model, so for the explanations of the methods, refer to the previous notebooks! __

# Overall Results

First, let's start with the most basic question: which model produced the highest cross validation ROC AUC score (using 5 folds) on the training dataset?",take skeptic perform small subset data rigor result turn evalu metric run 500 iter random search 400 iter bayesian optim full train dataset 700 featur featur notebook http www kaggl com jsaguiar updat 0 792 lb lightgbm simpl featur aguiar http www kaggl com jsaguiar iter took around 6 day machin 128 gb ram run kernel bayesian optim method still run updat result finish notebook focu result build best model explan method refer previou notebook overal result first let start basic question model produc highest cross valid roc auc score use 5 fold train dataset
3781,"Well, there you go! __Random search slightly outperformed  Bayesian optimization and found a higher cross validation model in far fewer iterations.__ However, as we will shortly see, this does not mean random search is the better hyperparameter optimization method. 

When submitted to the competition (at the end of this notebook):

* __Random search results scored 0.790__
* __Bayesian optimization results scored 0.791__

What were the best model hyperparameters from both methods?

####  Random Search best Hyperparameters",well go random search slightli outperform bayesian optim found higher cross valid model far fewer iter howev shortli see mean random search better hyperparamet optim method submit competit end notebook random search result score 0 790 bayesian optim result score 0 791 best model hyperparamet method random search best hyperparamet
3782,#### Bayesian Optimization best Hyperparameters,bayesian optim best hyperparamet
3783,"If we compare the individual values, we actually see that they are fairly close together when we consider the entire search grid! 

## Distribution of Scores

Let's plot the distribution of scores for both models in a kernel density estimate plot.",compar individu valu actual see fairli close togeth consid entir search grid distribut score let plot distribut score model kernel densiti estim plot
3784,"Bayesian optimization did not produce the highest individual score, but it did tend to spend more time evaluating ""better"" values of hyperparameters. __Random search got lucky and found the best values but Bayesian optimization tended to ""concentrate"" on better-scoring values__. That's pretty much what we expect: random search does a good job of exploring the search space which means it will probably happen upon a high-scoring set of values (if the space is not extremely high-dimensional) while Bayesian optimization will tend to focus on a set of values that yield higher scores. __If all you wanted was the conclusion, then you're probably good to go. If you really enjoy making plots and doing exploratory data analysis and want to gain a better understanding of how these methods work, then read on!__ In the next few sections, we will thoroughly explore these results.

Our plan for going through the results is as follows:

* Distribution of scores
    * Overall distribution
    * Score versus the iteration (did scores improve as search progressed)
* Distribution of hyperparameters
    * Overall distribution including the hyperparameter grid for a reference
    * Hyperparameters versus iteration to look at _evolution_ of values
* Hyperparameter values versus the score
    * Do scores improve with certain values of hyperparameters (correlations)
    * 3D plots looking at effects of 2 hyperparameters at a time on the score
* Additional Plots
    * Time to run each evaluation for Bayesian optimization
    * Correlation heatmaps of hyperparameters with score
    
There will be all sorts of plots: heatmaps, 3D scatterplots, density plots, bar charts (hey even bar charts can be helpful!)

After going through the results, we will do a little meta-machine learning, and implement the best model on the full set of features.",bayesian optim produc highest individu score tend spend time evalu better valu hyperparamet random search got lucki found best valu bayesian optim tend concentr better score valu pretti much expect random search good job explor search space mean probabl happen upon high score set valu space extrem high dimension bayesian optim tend focu set valu yield higher score want conclus probabl good go realli enjoy make plot exploratori data analysi want gain better understand method work read next section thoroughli explor result plan go result follow distribut score overal distribut score versu iter score improv search progress distribut hyperparamet overal distribut includ hyperparamet grid refer hyperparamet versu iter look evolut valu hyperparamet valu versu score score improv certain valu hyperparamet correl 3d plot look effect 2 hyperparamet time score addit plot time run evalu bayesian optim correl heatmap hyperparamet score sort plot heatmap 3d scatterplot densiti plot bar chart hey even bar chart help go result littl meta machin learn implement best model full set featur
3785,"# Distribution of Scores

We already saw the kernel density estimate plot, so let's go on to a bar plot. First we'll get the data in a long format.",distribut score alreadi saw kernel densiti estim plot let go bar plot first get data long format
3786,"Keep in mind that random search ran for more iterations (as of now). Even so, we can see that Bayesian Optimization tended to produce much more higher cross validation scores. Let's look at the statistical averages:",keep mind random search ran iter even see bayesian optim tend produc much higher cross valid score let look statist averag
3787,"If we are going by mean, then Bayesian optimization is the clear winner. If we go by high score, then random search just wins out. ",go mean bayesian optim clear winner go high score random search win
3788,"## Score versus Iteration

Now, to see if either method improves over the course of the search, we need to plot the score as a function of the iteration. ",score versu iter see either method improv cours search need plot score function iter
3789,"Again keeping in mind that Bayesian optimization has not yet finished, we can see a clear upward trend for this method and no trend whatsoever for random search. 

### Linear Regression of Scores versus Iteration

To show that Bayesian optimization improves over time, we can regress the score by the iteration. Then, we can use this to extrapolate into the future, __a wildly inappropriate technique in this case, but fun nonetheless!__

Here we use `np.polyfit` with a degree of 1 for the linear regression (you can compare the results with `LinearRegression`  from `sklearn.linear_model`.",keep mind bayesian optim yet finish see clear upward trend method trend whatsoev random search linear regress score versu iter show bayesian optim improv time regress score iter use extrapol futur wildli inappropri techniqu case fun nonetheless use np polyfit degre 1 linear regress compar result linearregress sklearn linear model
3790,The random search slope is basically zero. ,random search slope basic zero
3791,"The Bayesian slope is about 15 times greater than that of random search! What happens if we say run these methods for 10,000 iterations?",bayesian slope 15 time greater random search happen say run method 10 000 iter
3792,"Incredible! I told you this was wildly inappropriate. Nonetheless, the slope does indicate that Bayesian optimization ""learns"" the hyperparameter values that do better over time. It then concentrates on evaluating these rather than spending time exploring other values as does random search. This means it can get stuck in a local optimum and can tend to __exploit__ values rather than continue to __explore__.

Now we will move on to the actual values of the hyperparameters.",incred told wildli inappropri nonetheless slope indic bayesian optim learn hyperparamet valu better time concentr evalu rather spend time explor valu random search mean get stuck local optimum tend exploit valu rather continu explor move actual valu hyperparamet
3793,"# Hyperparameter Values

For each hyperparameter, we will plot the values tried by both searches as well as the reference distribution (which was the same in both cases, just a grid for random and distributions for Bayesian). We would expect the random search to almost exactly match the reference - it will converge on the reference given enough iterations.

First, we will process the results into a dataframe where each column is one hyperparameter. Saving the file converted the dictionary into a string, so we use `ast.literal_eval` to convert back to a dictionary before adding as a row in the dataframe.",hyperparamet valu hyperparamet plot valu tri search well refer distribut case grid random distribut bayesian would expect random search almost exactli match refer converg refer given enough iter first process result datafram column one hyperparamet save file convert dictionari string use ast liter eval convert back dictionari ad row datafram
3794,Next we define the hyperparameter grid that was used (the same ranges applied in both searches).,next defin hyperparamet grid use rang appli search
3795,"# Distributions of Search Values

Below are the kernel density estimate plots for each hyperparameter. The dashed vertical lines indicate the ""optimal"" value found in the respective searches. 

We start with the learning rate:",distribut search valu kernel densiti estim plot hyperparamet dash vertic line indic optim valu found respect search start learn rate
3796,"Even though the search domain extended from 0.005 to 0.2, both optimal values clustered around a lower value. Perhaps this tells us we should concentrate further searches in this area below 0.02?

That code was a little tedious, so let's write a function that makes the same code for any hyperparameter (feel free to pick your own colors!).",even though search domain extend 0 005 0 2 optim valu cluster around lower valu perhap tell u concentr search area 0 02 code littl tediou let write function make code hyperparamet feel free pick color
3797,"We can do this for all of the hyperparameters. These results can be used to inform further searches. They can even be used to define a grid search over a concentrated region. The problem with grid search is the insane compuational and time costs involved, and a smaller hyperparameter grid will help immensely! ",hyperparamet result use inform search even use defin grid search concentr region problem grid search insan compuat time cost involv smaller hyperparamet grid help immens
3798,"The `reg_alpha` and `reg_lambda` best scores seem to complement one another for Bayesian optimization. In other words, if either `reg_lambda` or `reg_alpha` is high (say greater than 0.5), then the other should be low (below 0.5). These hyperparameters control a penalty placed on the weights of the trees and thus are meant to control overfitting. It might make sense if only one needs to be high then.",reg alpha reg lambda best score seem complement one anoth bayesian optim word either reg lambda reg alpha high say greater 0 5 low 0 5 hyperparamet control penalti place weight tree thu meant control overfit might make sen one need high
3799,"### Boosting Type

The boosting type deserves its own section because it is a categorical variable, and because as we will see, it has an outsized effect on model performance. First, let's calculate statistics grouped by boosting type for each search method.",boost type boost type deserv section categor variabl see outsiz effect model perform first let calcul statist group boost type search method
3800,"In both search methods, the `gbdt` (gradient boosted decision tree) and `dart` (dropout meets additive regression tree) do much better than `goss` (gradient based one-sided sampling). `gbdt` does the best on average (and for the max), so it might make sense to use that method in the future! Let's view the results as a barchart:",search method gbdt gradient boost decis tree dart dropout meet addit regress tree much better go gradient base one side sampl gbdt best averag max might make sen use method futur let view result barchart
3801,"__`gbdt` (or `dart`) it should be! Notice that random search tried `gbdt` about the same number of times as the other two (since it selected with no reasoning) while Bayesian optimization tried `gbdt` much more often. __

Since `gbdt` supports `subsample` (using on a sample of the observations to train on in every tree) we can plot the distribution of `subsample` where `boosting_type=='gbdt'`. We also show the reference distribution.",gbdt dart notic random search tri gbdt number time two sinc select reason bayesian optim tri gbdt much often sinc gbdt support subsampl use sampl observ train everi tree plot distribut subsampl boost type gbdt also show refer distribut
3802,There is a significant disagreement between the two methods on the optimal value for `subsample`. Perhaps we would want to leave this as a wide distribution in any further searches (although some subsampling does look to be beneficial).,signific disagr two method optim valu subsampl perhap would want leav wide distribut search although subsampl look benefici
3803,"Finally, we can look at the instance of `is_unbalance`, a hyperparameter that tells LightGBM whether or not to treat the problem as unbalance classification.",final look instanc unbal hyperparamet tell lightgbm whether treat problem unbal classif
3804,"__According to the average score, it pretty much does not matter if this hyperparameter is `True` or `False`.__ To be honest, I'm not sure what difference this is supposed to make, so anyone who wants can fill me in!",accord averag score pretti much matter hyperparamet true fals honest sure differ suppos make anyon want fill
3805,"# Hyperparameters versus Iteration

Next we will take a look at the __evolution__ of the Bayesian search (random search shows no pattern as expected) by graphing the values versus the iteration. This can inform us the direction in which the search was heading in terms of where the values tended to cluster. Given these graphs, we might then be able to extrapolate values that lead to even higher scores (or maybe not, _extrapolation is dangerous_!)

The black star in the plots below signifies the best scoring value.",hyperparamet versu iter next take look evolut bayesian search random search show pattern expect graph valu versu iter inform u direct search head term valu tend cluster given graph might abl extrapol valu lead even higher score mayb extrapol danger black star plot signifi best score valu
3806,"We want to be careful about placing too much value in these results, because remember, the Bayesian optimization could have found a local minimum of the cross validation loss that it is exploting. Moreover, the trends here are generally pretty small. It is encouraging that the best value was found close to the end of the search indicating cross validation scores were continuing to improve. 

Next, we can look at the values of the score as a function of the hyperparameter values. This is again a dangerous area! ",want care place much valu result rememb bayesian optim could found local minimum cross valid loss explot moreov trend gener pretti small encourag best valu found close end search indic cross valid score continu improv next look valu score function hyperparamet valu danger area
3807,"# Plots of Hyperparameters vs Score

![](http://)These next plots show the value of a single hyperparameter versus the score. We want to avoid placing too much emphasis on these graphs, because we are not changing one hyperparameter at a time. Therefore, if there are trends, it might not be solely due to the single hyperparameter we show. A truly accurate grid would be 10-dimensional and show the values of __all__ hyperparameters and the resulting score. If we could understand a __10-dimensional__ graph, then we might be able to figure out the optimal combination of hyperparameters! ",plot hyperparamet v score http next plot show valu singl hyperparamet versu score want avoid place much emphasi graph chang one hyperparamet time therefor trend might sole due singl hyperparamet show truli accur grid would 10 dimension show valu hyperparamet result score could understand 10 dimension graph might abl figur optim combin hyperparamet
3808,"__The only clear distinction is that the score decreases as the learning rate increases.__ Of course, we cannot say whether that is due to the learning rate itself, or some other factor (we will look at the interplay between the learning rate and the number of esimators shortly). The learning rate domain was on a logarithmic scale, so it's most accurate for the plot to be as well (unfortunately I cannot get this to work yet).",clear distinct score decreas learn rate increas cours say whether due learn rate factor look interplay learn rate number esim shortli learn rate domain logarithm scale accur plot well unfortun get work yet
3809,Now for the next four hyperparameters versus the score.,next four hyperparamet versu score
3810,"There are not any strong trends here. Next we will try to look at two hyperparameters simultaneously versus the score in a 3-dimensional plot. This makes sense for hyperparameters that work in concert, such as the learning rate and the number of esimators or the two regularization values.",strong trend next tri look two hyperparamet simultan versu score 3 dimension plot make sen hyperparamet work concert learn rate number esim two regular valu
3811,"## 3D Plots 

To try and examine the simultaneous effects of hyperparameters, we can make 3D plots with 2 hyperparameters and the score. A truly accurate plot would be 10-D (one for each hyperparameter) but in this case we will stick to 3 dimensions. 3D plots can be made in matplotlib by import `Axes3D` and specifying the `3d` projection in a call to `.add_subplot`",3d plot tri examin simultan effect hyperparamet make 3d plot 2 hyperparamet score truli accur plot would 10 one hyperparamet case stick 3 dimens 3d plot made matplotlib import axes3d specifi 3d project call add subplot
3812,First up is `reg_alpha` and `reg_lambda`. These control the amount of regularization on each decision tree and help to prevent overfitting to the training data.,first reg alpha reg lambda control amount regular decis tree help prevent overfit train data
3813,"It's a little difficult to tell much from this plot. If we look at the best values and then look at the plot, we can see that scores do tend to be higher around 0.9 for `reg_alpha`and 0.2 for `reg_lambda`.  Later, we'll make the same plot for the Bayesian Optimization for comparison.",littl difficult tell much plot look best valu look plot see score tend higher around 0 9 reg alpha 0 2 reg lambda later make plot bayesian optim comparison
3814,The next plot is learning rate and number of estimators versus the score. __Remember that the number of estimators was selected using early stopping for 100 rounds with 5-fold cross validation__. The number of estimators __was not__ a hyperparameter in the grid that we searched over. Early stopping is a more efficient method of finding the best number of estimators than including it in a search (based on my limited experience)!,next plot learn rate number estim versu score rememb number estim select use earli stop 100 round 5 fold cross valid number estim hyperparamet grid search earli stop effici method find best number estim includ search base limit experi
3815,Here there appears to be a clear trend: a lower learning rate leads to higher values! What does the plot of just learning rate versus number of estimators look like?,appear clear trend lower learn rate lead higher valu plot learn rate versu number estim look like
3816,"This plot is very easy to interpret: the lower the learning rate, the more estimators that will be trained. From our knowledge of the model, this makes sense: each individual decision trees contribution is lessened as the learning rate is decreased leading to a need for more decision trees in the ensemble. Moreover, from the previous graphs, it appears that decreasing the learning rate increases the model score.",plot easi interpret lower learn rate estim train knowledg model make sen individu decis tree contribut lessen learn rate decreas lead need decis tree ensembl moreov previou graph appear decreas learn rate increas model score
3817,"### Function for 3D plotting

Any time you write code more than twice, it should be encoded into a function! That's what the next code block is for: putting this code into a function that we can use many times! This function can be used for __any__ 3d plotting needs.",function 3d plot time write code twice encod function next code block put code function use mani time function use 3d plot need
3818,The bayesian optimization results are close in trend to those from random search: lower learning rate leads to higher cross validation scores.,bayesian optim result close trend random search lower learn rate lead higher cross valid score
3819,"Again, we probably want one of the regularization values to be high and the other to be low. This must help to ""balance"" the model between bias and variance. ",probabl want one regular valu high low must help balanc model bia varianc
3820,"# Correlations between Hyperparameters and Score

Time for another dangerous act: finding correlations between the hyperparameters and the score. These are not going to be accurate because again, we are not varying one value at a time! Nonetheless, we may discover useful insight about the Gradient Boosting Machine model.",correl hyperparamet score time anoth danger act find correl hyperparamet score go accur vari one valu time nonetheless may discov use insight gradient boost machin model
3821,### Correlations for Random Search,correl random search
3822,"As expected, the `learning_rate` has one of the greatest correlations with the score. The `subsample` rate might be affected by the fact that 1/3 of the time this was set to 1.0.",expect learn rate one greatest correl score subsampl rate might affect fact 1 3 time set 1 0
3823,### Correlations for Bayesian Optimization,correl bayesian optim
3824,"The `learning_rate` again appears to be moderately correlated with the score. This should tell us again that a lower learning rate tends to co-occur with a higher cross-validation score, but not that this is nexessarily the cause of the higher score. ",learn rate appear moder correl score tell u lower learn rate tend co occur higher cross valid score nexessarili caus higher score
3825,"## Correlation Heatmap

Now we can make a heatmap of the correlations. I enjoy heatmaps and thankfully, they are not very difficult to make in `seaborn`.",correl heatmap make heatmap correl enjoy heatmap thank difficult make seaborn
3826,That's a lot of plot for not very much code! We can see that the number of estimators and the learning rate have the greatest magnitude correlation (ignoring subsample which is influenced by the boosting type).,lot plot much code see number estim learn rate greatest magnitud correl ignor subsampl influenc boost type
3827,Feel free to use this code for your own heatmaps! (Also send me color recommendations because I am not great at picking out a palette).,feel free use code heatmap also send color recommend great pick palett
3828,"# Meta-Machine Learning

So we have a labeled set of data: the hyperparameter values and the resulting score. Clearly, the next step is to use these for machine learning? Yes, here we will perform _meta-machine learning_ by fitting an estimator on top of the hyperparameter values and the scores. This is a supervised regression problem, and although we can use any method for learning the data, here we will stick to a linear regression. This will let us examine the coefficients on each hyperparameter and will help reduce overfitting. ",meta machin learn label set data hyperparamet valu result score clearli next step use machin learn ye perform meta machin learn fit estim top hyperparamet valu score supervis regress problem although use method learn data stick linear regress let u examin coeffici hyperparamet help reduc overfit
3829,"If we wanted, we could treat this as _another optimization problem_ and try to maximize the linear regression in terms of the score! However, for now I think we have done enough optimization. 

It's time to move on to implementing the best hyperparameter values from random and Bayesian optimization on the full dataset.",want could treat anoth optim problem tri maxim linear regress term score howev think done enough optim time move implement best hyperparamet valu random bayesian optim full dataset
3830,"# Implementation

The full set of features on which these results come are from [this notebook](https://www.kaggle.com/jsaguiar/updated-0-792-lb-lightgbm-with-simple-features) by [Aguiar](https://www.kaggle.com/jsaguiar)). Here, we will load in the same features, train on the full training features and make predictions on the testing data. These can then be uploaded to the competition.",implement full set featur result come notebook http www kaggl com jsaguiar updat 0 792 lb lightgbm simpl featur aguiar http www kaggl com jsaguiar load featur train full train featur make predict test data upload competit
3831,First we need to format the data and extract the labels.,first need format data extract label
3832,We can also save the features to later use for plotting feature importances.,also save featur later use plot featur import
3833,### Random Search,random search
3834,### Bayesian Optimization,bayesian optim
3835,"### Competition Results

* __Random search results scored 0.790__
* __Bayesian optimization results scored 0.791__

If we go by best score on the public leaderboard, Bayesian Optimization wins! However, the public leaderboard is based only on 10% of the test data, so it's possible this is a result of overfitting to this particular subset of the testing data. Overall, I would say the complete results suggest that both methods produce similar outcomes especially when run for enough iterations. Either method is better than hand-tuning! ",competit result random search result score 0 790 bayesian optim result score 0 791 go best score public leaderboard bayesian optim win howev public leaderboard base 10 test data possibl result overfit particular subset test data overal would say complet result suggest method produc similar outcom especi run enough iter either method better hand tune
3836,"#### Feature Importances

As a final step, we can compare the feature importances between the models from the best hyperparameters. It would be interesting to see if the hyperparameter values has an effect on the feature importances.",featur import final step compar featur import model best hyperparamet would interest see hyperparamet valu effect featur import
3837,"The feature importances look to be relatively stable across hyperparameter values. This is what I expected, but at the same time, we can see that the _absolute magnitude_ of the importances differs significantly but not the _relative ordering_.",featur import look rel stabl across hyperparamet valu expect time see absolut magnitud import differ significantli rel order
3838,"# Conclusions

Random search narrowly beat out Bayesian optimization in terms of finding the hyperparameter values that resulted in the highest cross validation ROC AUC. That single number does not tell the whole story though as the Bayesian method average ROC AUC was much higher than that of random search. We expect this to be the case because Bayesian optimization should focus on higher scoring values based on the surrogate model of the objective function it constructs. Morevoer, this tells us Bayesian optimization is a valuable technique, but random search can still happen upon better values in fewer search iterations if we are lucky. 

* Random search slightly outperformed Bayesian optimization in terms of cv ROC AUC 
* Bayesian optimization average scores were much higher than random search indicating it spends more time evaluating ""better"" hyperparameters
* Bayesian scored 0.791 when submitted and random search scored 0.790 indicating that with enough iterations, the methods deliver similar results
* Boosting type ""gdbt"" did much better than ""goss"" with ""dart"" nearly as good
* A lower learning rate resulted in higher model scores: lower than 0.02 looks to be optimal
* `reg_alpha` and `reg_lambda` should complement one another: if one is high (above 0.5), than the other should be lower (below 0.5)
* Some subsampling appears to increase the model scores
* The other hyperparameters either did not have a significant effect, or their effects are intertwined and hence could not be disentangled in this study

Feel free to build upon these results! I'm curious if the best hyperparameters for this dataset will translate to other datasets, either for this problem, or for vastly different data science problems. The best way to find out is to try them! 

If you're looking for more work on this problem, I have a series of notebooks documenting my work:

__Additional Notebooks__ 

* [A Gentle Introduction](https://www.kaggle.com/willkoehrsen/start-here-a-gentle-introduction)
* [Manual Feature Engineering Part One](https://www.kaggle.com/willkoehrsen/introduction-to-manual-feature-engineering)
* [Manual Feature Engineering Part Two](https://www.kaggle.com/willkoehrsen/introduction-to-manual-feature-engineering-p2)
* [Introduction to Automated Feature Engineering](https://www.kaggle.com/willkoehrsen/automated-feature-engineering-basics)
* [Advanced Automated Feature Engineering](https://www.kaggle.com/willkoehrsen/tuning-automated-feature-engineering-exploratory)
* [Feature Selection](https://www.kaggle.com/willkoehrsen/introduction-to-feature-selection)
* [Intro to Model Tuning: Grid and Random Search](https://www.kaggle.com/willkoehrsen/intro-to-model-tuning-grid-and-random-search)
* [Automated Model Tuning](https://www.kaggle.com/willkoehrsen/automated-model-tuning)

Thanks for reading and feel free to share any constructive criticism or feedback. 

Best,

Will",conclus random search narrowli beat bayesian optim term find hyperparamet valu result highest cross valid roc auc singl number tell whole stori though bayesian method averag roc auc much higher random search expect case bayesian optim focu higher score valu base surrog model object function construct morevo tell u bayesian optim valuabl techniqu random search still happen upon better valu fewer search iter lucki random search slightli outperform bayesian optim term cv roc auc bayesian optim averag score much higher random search indic spend time evalu better hyperparamet bayesian score 0 791 submit random search score 0 790 indic enough iter method deliv similar result boost type gdbt much better go dart nearli good lower learn rate result higher model score lower 0 02 look optim reg alpha reg lambda complement one anoth one high 0 5 lower 0 5 subsampl appear increas model score hyperparamet either signific effect effect intertwin henc could disentangl studi feel free build upon result curiou best hyperparamet dataset translat dataset either problem vastli differ data scienc problem best way find tri look work problem seri notebook document work addit notebook gentl introduct http www kaggl com willkoehrsen start gentl introduct manual featur engin part one http www kaggl com willkoehrsen introduct manual featur engin manual featur engin part two http www kaggl com willkoehrsen introduct manual featur engin p2 introduct autom featur engin http www kaggl com willkoehrsen autom featur engin basic advanc autom featur engin http www kaggl com willkoehrsen tune autom featur engin exploratori featur select http www kaggl com willkoehrsen introduct featur select intro model tune grid random search http www kaggl com willkoehrsen intro model tune grid random search autom model tune http www kaggl com willkoehrsen autom model tune thank read feel free share construct critic feedback best
3839,"# Introduction: Tuning Automated Feature Engineering

In this notebook we will expand upon the [basic automated feature engineering](https://www.kaggle.com/willkoehrsen/applied-automated-feature-engineering-basics) applied to the Home Credit Default Risk competition. We will explore a few different methods for improving the set of features and incorporating domain knowledge into the final dataset. These methods include:

* Properly representing variable types
* Creating and using time variables
* Setting interesting values of variables
* Creating seed features
* Building custom primitives

Reading through the discussion around this competition and working through some of the top kernels, intricate feature engineering is a must. Using the default feature primitives in the basic notebook did improve our score, but to do better we will need some more advanced methods. 

This will be more as an exploration of the capabilities of featuretools than a complete implementation. I'm still working on figuring out the most useful features to build by reading through other kernels, finding features, and figuring out how to recreate and build upon those in featuretools. Any ideas would be much appreciated! 

This work draws heavily on the [featuretools documentation](https://docs.featuretools.com/) and the [featuretools GitHub repository](https://github.com/Featuretools/featuretools). ",introduct tune autom featur engin notebook expand upon basic autom featur engin http www kaggl com willkoehrsen appli autom featur engin basic appli home credit default risk competit explor differ method improv set featur incorpor domain knowledg final dataset method includ properli repres variabl type creat use time variabl set interest valu variabl creat seed featur build custom primit read discus around competit work top kernel intric featur engin must use default featur primit basic notebook improv score better need advanc method explor capabl featuretool complet implement still work figur use featur build read kernel find featur figur recreat build upon featuretool idea would much appreci work draw heavili featuretool document http doc featuretool com featuretool github repositori http github com featuretool featuretool
3840,"### Read in Data and Create Smaller Datasets

We will limit the data to 1000 rows because automated feature engineering is computationally intensive work. Later we can refactor this code into functions and put it in a script to run on a more powerful machine. ",read data creat smaller dataset limit data 1000 row autom featur engin comput intens work later refactor code function put script run power machin
3841,"# Properly Representing Variable Types

There are a number of columns in the `app` dataframe that are represented as integers but are really discrete variables that can only take on a limited number of features. Some of these are Boolean flags (only 1 or 0) and two columns are ordinal (ordered discrete). To tell featuretools to treat these as Boolean variables, we need to pass in the correct datatype using a dictionary mapping {`variable_name`: `variable_type`}. ",properli repres variabl type number column app datafram repres integ realli discret variabl take limit number featur boolean flag 1 0 two column ordin order discret tell featuretool treat boolean variabl need pas correct datatyp use dictionari map variabl name variabl type
3842,There are also two ordinal variables in the `app` data: the rating of the region with and without the city. ,also two ordin variabl app data rate region without citi
3843,The previous data also has two Boolean variables. ,previou data also two boolean variabl
3844,"# Time Variables

Time can be a crucial factor in many datasets because behaviors change over time and therefore we want to make features to reflect this. For example, a client might be taking out larger and larger loans over time which could be an indicator that they are about to default or they could have a run of missed payments but then get back on track.

There are no explicit datetimes in the data, but there are relative time offsets. All the time offset are measured from the current application at Home Credit and are measured in months or days. For example, in `bureau`, the `DAYS_CREDIT` column represents ""How many days before current application did client apply for Credit Bureau credit"". (Credit Bureau refers to any other credit organization besides Home Credit). Although we do not know the actual application date, if we assume a starting application date that is the same for all clients, then we can convert the `MONTHS_BALANCE` into a datetime. This can then be treated as a relative time that we can use to find trends or identify the most recent value of a variable. ",time variabl time crucial factor mani dataset behavior chang time therefor want make featur reflect exampl client might take larger larger loan time could indic default could run miss payment get back track explicit datetim data rel time offset time offset measur current applic home credit measur month day exampl bureau day credit column repres mani day current applic client appli credit bureau credit credit bureau refer credit organ besid home credit although know actual applic date assum start applic date client convert month balanc datetim treat rel time use find trend identifi recent valu variabl
3845,"### Replace Outliers

There are a number of day offsets that are recorded as 365243. Reading through discussions, others replaced this number with `np.nan`. If we don't do this, Pandas will not be able to convert into a timedelta and throws an error that the number is too large. The following code has been adapted from a script on [GitHub](https://github.com/JYLFamily/Home_Credit_Default_Risk/blob/master/20180603/FeaturesV2/ApplicationTestFeatures.py).",replac outlier number day offset record 365243 read discus other replac number np nan panda abl convert timedelta throw error number larg follow code adapt script github http github com jylfamili home credit default risk blob master 20180603 featuresv2 applicationtestfeatur py
3846,First we can establish an arbitrary date and then convert the time offset in months into a Pandas `timedelta` object. ,first establish arbitrari date convert time offset month panda timedelta object
3847," These four columns represent different offsets:

* `DAYS_CREDIT`: Number of days before current application at Home Credit client applied for loan at other financial institution. We will call this the application date, `bureau_credit_application_date` and make it the `time_index` of the entity. 
* `DAYS_CREDIT_ENDDATE`: Number of days of credit remaining at time of client's application at Home Credit. We will call this the ending date, `bureau_credit_end_date`
* `DAYS_ENDDATE_FACT`: For closed credits, the number of days before current application at Home Credit that credit at other financial institution ended. We will call this the closing date, `bureau_credit_close_date`. 
* `DAYS_CREDIT_UPDATE`: Number of days before current application at Home Credit that the most recent information about the previous credit arrived. We will call this the update date, `bureau_credit_update_date`. 

If we were doing manual feature engineering, we might want to create new columns such as by subtracting `DAYS_CREDIT_ENDDATE` from `DAYS_CREDIT` to get the planned length of the loan in days, or subtracting `DAYS_CREDIT_ENDDATE` from `DAYS_ENDDATE_FACT` to find the number of days the client paid off the loan early. However, in this notebook we will not make any features by hand, but rather let featuretools develop useful features for us.

To make date columns from the `timedelta`, we simply add the offset to the start date. ",four column repres differ offset day credit number day current applic home credit client appli loan financi institut call applic date bureau credit applic date make time index entiti day credit enddat number day credit remain time client applic home credit call end date bureau credit end date day enddat fact close credit number day current applic home credit credit financi institut end call close date bureau credit close date day credit updat number day current applic home credit recent inform previou credit arriv call updat date bureau credit updat date manual featur engin might want creat new column subtract day credit enddat day credit get plan length loan day subtract day credit enddat day enddat fact find number day client paid loan earli howev notebook make featur hand rather let featuretool develop use featur u make date column timedelta simpli add offset start date
3848,"### Plot for a sanity check

To make sure the conversion went as planned, let's make a plot showing the distribution of loan lengths.",plot saniti check make sure convers went plan let make plot show distribut loan length
3849,"It looks as if there are a number of loans that are unreasonably long. Reading through the discussions, other people had noticed this as well. At this point, we will just leave in the outliers. We also will drop the time offset columns.",look number loan unreason long read discus peopl notic well point leav outlier also drop time offset column
3850,"#### Bureau Balance

The bureau balance dataframe has a `MONTHS_BALANCE` column that we can use as a months offset. The resulting column of dates can be used as a `time_index`.",bureau balanc bureau balanc datafram month balanc column use month offset result column date use time index
3851,"#### Previous Applications

The `previous` dataframe holds previous applications at Home Credit. There are a number of time offset columns in this dataset:

* `DAYS_DECISION`: number of days before current application at Home Credit that decision was made about previous application. This will be the `time_index` of the data.
* `DAYS_FIRST_DRAWING`: number of days before current application at Home Credit that first disbursement was made
* `DAYS_FIRST_DUE`: number of days before current application at Home Credit that first due was suppoed to be
* `DAYS_LAST_DUE_1ST_VERSION`: number of days before current application at Home Credit that first was??
* `DAYS_LAST_DUE`: number of days before current application at Home Credit of last due date of previous application
* `DAYS_TERMINATION`: number of days before current application at Home Credit of expected termination

Let's convert all these into timedeltas in a loop and then make time columns.",previou applic previou datafram hold previou applic home credit number time offset column dataset day decis number day current applic home credit decis made previou applic time index data day first draw number day current applic home credit first disburs made day first due number day current applic home credit first due suppo day last due 1st version number day current applic home credit first day last due number day current applic home credit last due date previou applic day termin number day current applic home credit expect termin let convert timedelta loop make time column
3852,"#### Previous Credit and Cash

The `credit_card_balance` and `POS_CASH_balance` each have a `MONTHS_BALANCE` column with the month offset. This is the number of months before the current application at Home Credit of the previous application record. These will represent the `time_index` of the data. ",previou credit cash credit card balanc po cash balanc month balanc column month offset number month current applic home credit previou applic record repres time index data
3853,"#### Installments Payments 

The `installments_payments` data contains information on each payment made on the previous loans at Home Credit. It has two date offset columns:

* `DAYS_INSTALMENT`: number of days before current application at Home Credit that previous installment was supposed to be paid
* `DAYS_ENTRY_PAYMENT`: number of days before current application at Home Credit that previous installment was actually paid

By now the process should be familiar: convert to timedeltas and then make time columns. The DAYS_INSTALMENT will serve as the `time_index`. ",instal payment instal payment data contain inform payment made previou loan home credit two date offset column day instal number day current applic home credit previou instal suppos paid day entri payment number day current applic home credit previou instal actual paid process familiar convert timedelta make time column day instal serv time index
3854,"# Applying Featuretools

We can now start making features using the time columns. We will create an entityset named clients much as before, but now we have time variables that we can use. ",appli featuretool start make featur use time column creat entityset name client much time variabl use
3855,"### Entities

When creating the entities, we specify the `index`, the `time_index` (if present), and the `variable_types` (if they need to be specified). ",entiti creat entiti specifi index time index present variabl type need specifi
3856,"### Relationships

Not surprisingly, the relationships between tables has not changed since the previous implementation. ",relationship surprisingli relationship tabl chang sinc previou implement
3857,"## Time Features

Let's look at some of the time features we can make from the new time variables. Because these times are relative and not absolute, we are only interested in values that show change over time, such as trend or cumulative sum. We would not want to calculate values like the year or month since we choose an arbitrary starting date. 

Throughout this notebook, we will pass in a `chunk_size` to the `dfs` call which specifies the number of rows (if an integer) or the fraction or rows to use in each chunk (if a float). This can help to optimize the `dfs` procedure, and the `chunk_size` can have a [significant effect on the run time](https://docs.featuretools.com/guides/performance.html). Here we will use a chunk size equal to the number of rows in the data so all the results will be calculated in one pass. We also want to avoid making any features with the testing data, so we pass in `ignore_entities = [app_test]`.",time featur let look time featur make new time variabl time rel absolut interest valu show chang time trend cumul sum would want calcul valu like year month sinc choos arbitrari start date throughout notebook pas chunk size df call specifi number row integ fraction row use chunk float help optim df procedur chunk size signific effect run time http doc featuretool com guid perform html use chunk size equal number row data result calcul one pas also want avoid make featur test data pas ignor entiti app test
3858,Let's visualize one of these new variables. We can look at the trend in credit size over time. A positive value indicates that the loan size for the client is increasing over time. ,let visual one new variabl look trend credit size time posit valu indic loan size client increas time
3859,"# Interesting Values

Another method we can use in featuretools is ""interesting values."" Specifying interesting values will calculate new features conditioned on values of existing features. For example, we can create new features that are conditioned on the value of `NAME_CONTRACT_STATUS` in the `previous` dataframe. Each stat will be calculated for the specified interesting values which can be useful when we know that there are certain indicators that are of greater importance in the data.  ",interest valu anoth method use featuretool interest valu specifi interest valu calcul new featur condit valu exist featur exampl creat new featur condit valu name contract statu previou datafram stat calcul specifi interest valu use know certain indic greater import data
3860,"To use interesting values, we assign them to the variable and then specify the `where_primitives` in the `dfs` call. ",use interest valu assign variabl specifi primit df call
3861,"One of the features is `MEAN(previous.CNT_PAYMENT WHERE NAME_CONTRACT_STATUS = Approved)`. This shows the average ""term of previous credit"" on previous loans conditioned on the previous loan being approved. We can compare the distribution of this feature to the `MEAN(previous.CNT_PAYMENT WHERE NAME_CONTRACT_STATUS = Canceled)` to see how these loans differ.",one featur mean previou cnt payment name contract statu approv show averag term previou credit previou loan condit previou loan approv compar distribut featur mean previou cnt payment name contract statu cancel see loan differ
3862,"Based on the most important features returned by a model, we can create new interesting features. This is one area where we can apply domain knowledge to feature creation.",base import featur return model creat new interest featur one area appli domain knowledg featur creation
3863,"# Seed Features

An additional extension to the default aggregations and transformations is to use [seed features](https://docs.featuretools.com/automated_feature_engineering/dfs_usage_tips.html#specifying-list-of-aggregation-functions). These are user defined features that we provide to deep feature synthesis that can then be built on top of where possible. 

As an example, we can create a seed feature that determines whether or not a payment was late. This time when we make the `dfs` function call, we need to pass in the `seed_features` argument.",seed featur addit extens default aggreg transform use seed featur http doc featuretool com autom featur engin df usag tip html specifi list aggreg function user defin featur provid deep featur synthesi built top possibl exampl creat seed featur determin whether payment late time make df function call need pas seed featur argument
3864,Another seed feature we can use is whether or not a previous loan at another institution was past due. ,anoth seed featur use whether previou loan anoth institut past due
3865,"# Create Custom Feature Primitives

If we are not satisfied with the existing primitives in featuretools, we [can write our own](https://docs.featuretools.com/automated_feature_engineering/primitives.html#defining-custom-primitives). This is an extremely powerful method that lets us expand the capabilities of featuretools. 

### NormalizedModeCount and LongestSeq

As an example, we will make three features, building on code from the [featuretools GitHub](https://github.com/Featuretools/featuretools). These will be aggregation primitives, where the function takes in an array of values and returns a single value.  The first, `NormalizedModeCount`, builds upon the `Mode` function by returning the fraction of total observations in a categorical feature that the model makes up. In other words, for a client with 5 total `bureau_balance` observations where 4 of the `STATUS` were `X`, the value of the `NormalizedModeCount` would be 0.8. The idea is to record not only the most common value, but also the relative frequency of the most common value compared to all observations.  

The second custom feature will record the longest consecutive run of a discrete variable. `LongestSeq` takes in an array of discrete values and returns the element that appears the most consecutive times. Because entities in the entityset are sorted by the `time_index`, this will return the value that occurs the most number of times in a row with respect to time. 



",creat custom featur primit satisfi exist primit featuretool write http doc featuretool com autom featur engin primit html defin custom primit extrem power method let u expand capabl featuretool normalizedmodecount longestseq exampl make three featur build code featuretool github http github com featuretool featuretool aggreg primit function take array valu return singl valu first normalizedmodecount build upon mode function return fraction total observ categor featur model make word client 5 total bureau balanc observ 4 statu x valu normalizedmodecount would 0 8 idea record common valu also rel frequenc common valu compar observ second custom featur record longest consecut run discret variabl longestseq take array discret valu return element appear consecut time entiti entityset sort time index return valu occur number time row respect time
3866,"These features could be completely useless, or they may be helpful. Only building a model and training it with the features will help us determine the answer. 

### MostRecent

The final custom feature will be `MOSTRECENT`. This simply returns the most recent value of a discrete variable with respect to time columns in a dataframe. When we create an entity, featuretools will [sort the entity](https://github.com/Featuretools/featuretools/blob/master/featuretools/entityset/entity.py) by the `time_index`. Therefore, the built-in aggregation primitive `LAST` calculates the most recent value based on the time index. However, in cases where there are multiple different time columns, it might be useful to know the most recent value with respect to all of the times. To build the custom feature primitive, I adapted the existing `TREND` primitive ([code here](https://github.com/Featuretools/featuretools/blob/master/featuretools/primitives/aggregation_primitives.py)). ",featur could complet useless may help build model train featur help u determin answer mostrec final custom featur mostrec simpli return recent valu discret variabl respect time column datafram creat entiti featuretool sort entiti http github com featuretool featuretool blob master featuretool entityset entiti py time index therefor built aggreg primit last calcul recent valu base time index howev case multipl differ time column might use know recent valu respect time build custom featur primit adapt exist trend primit code http github com featuretool featuretool blob master featuretool primit aggreg primit py
3867,"To test whether this function works as intended, we can compare the most recent variable of `CREDIT_TYPE` ordered by two different dates. ",test whether function work intend compar recent variabl credit type order two differ date
3868,"For client 100002, the most recent type of credit was `Credit card` if we order by the application date, but `Consumer credit` if we order by the end date of the loan. Whether this is actually useful knowledge is hard to say! 

",client 100002 recent type credit credit card order applic date consum credit order end date loan whether actual use knowledg hard say
3869,"# Putting it all Together

Finally, we can run deep feature synthesis with the time variables, with the correct specified categorical variables, with the interesting features, with the seed features, and with the custom features. To actually run this on the entire dataset, we can take the code here, put it in a script, and then use more computational resources. ",put togeth final run deep featur synthesi time variabl correct specifi categor variabl interest featur seed featur custom featur actual run entir dataset take code put script use comput resourc
3870,We will now do the same operation applied to the test set. Doing the calculations separately should prevent leakage from the testing data into the training data.,oper appli test set calcul separ prevent leakag test data train data
3871,"## Remove Features

[Feature selection](https://en.wikipedia.org/wiki/Feature_selection) is an entire topic to itself. However, one thing we can do is use the built-in featuretools [selection function to remove](https://docs.featuretools.com/generated/featuretools.selection.remove_low_information_features.html) columns that only have one unique value or have all null values. ",remov featur featur select http en wikipedia org wiki featur select entir topic howev one thing use built featuretool select function remov http doc featuretool com gener featuretool select remov low inform featur html column one uniqu valu null valu
3872,"When we're done, we probably want to save the results to a csv. We want to be careful because the index of the dataframe is the identifying column, so we should keep the index. We also should align the training and testing dataframes to make sure they have the same columns.",done probabl want save result csv want care index datafram identifi column keep index also align train test datafram make sure column
3873,"# Conclusions 

In this notebook we explored some of the advanced functionality in featuretools including:

* Time Variables: allow us to track trends over time 
* Interesting Variables: condition new features on values of existing features
* Seed Features: define new features manually that featuretools will then build on top of
* Custom feature primitives: design any transformation or aggregation feature that can incorporate domain knowledge

We can use these methods to encode domain knowledge about a problem into our features or create features based on what others have found useful. The next step from here would be to run the script on the entire dataset, then use the features for modeling. We could use the feature importances from the model to determine the most relevant features, perform feature selection, and then go through another round of feature synthesis with a new set of of primitives, seed features, and interesting features. As with many aspects of machine learning, feature creation is largely an empirical and iterative procedure. ",conclus notebook explor advanc function featuretool includ time variabl allow u track trend time interest variabl condit new featur valu exist featur seed featur defin new featur manual featuretool build top custom featur primit design transform aggreg featur incorpor domain knowledg use method encod domain knowledg problem featur creat featur base other found use next step would run script entir dataset use featur model could use featur import model determin relev featur perform featur select go anoth round featur synthesi new set primit seed featur interest featur mani aspect machin learn featur creation larg empir iter procedur
3874,"# Home Credit Default Risk: Random Forest & K-Fold Cross Validation
This notebook shows a simple random forest approach to the Home Credit Default Risk problem. A K-Fold cross validation is used to avoid overfitting.",home credit default risk random forest k fold cross valid notebook show simpl random forest approach home credit default risk problem k fold cross valid use avoid overfit
3875,"## Loans data model

It's good to keep in mind Home Credit loans data model to know how to join the different tables.",loan data model good keep mind home credit loan data model know join differ tabl
3876,![Home Credit loans data model](https://storage.googleapis.com/kaggle-media/competitions/home-credit/home_credit.png),home credit loan data model http storag googleapi com kaggl medium competit home credit home credit png
3877,## Read data,read data
3878,"
## Features selection

The feature selection has been done using the most important features resulting of Will Koehrsen [automated features generation](https://www.kaggle.com/willkoehrsen/applied-automated-feature-engineering-basics/notebook). Thanks to him for his awesome notebook and introduction to [Featuretools](https://www.featuretools.com/) and [Deep Feature Synthesis](http://www.jmaxkanter.com/static/papers/DSAA_DSM_2015.pdf) method.

First, let's aggregate the features coming from the credit bureau file in the main loans table.",featur select featur select done use import featur result koehrsen autom featur gener http www kaggl com willkoehrsen appli autom featur engin basic notebook thank awesom notebook introduct featuretool http www featuretool com deep featur synthesi http www jmaxkant com static paper dsaa dsm 2015 pdf method first let aggreg featur come credit bureau file main loan tabl
3879,"Next, let's generate the features coming from the installments data. To do so:
* We compute first the minimum of installment payments in the previous applications table. The rational is to catch the loans where one payment has been missed or was very low.
* Then we merge the aggregates of all minimum installment payments across all previous loans.",next let gener featur come instal data comput first minimum instal payment previou applic tabl ration catch loan one payment miss low merg aggreg minimum instal payment across previou loan
3880,"Finally, let's add all the base features from the main loan table which don't need aggregation. We fill the missing values with their average.",final let add base featur main loan tabl need aggreg fill miss valu averag
3881,## Random Forest model,random forest model
3882,## K-Fold cross validation,k fold cross valid
3883,"Contents
- <a href='#1'>1. Read dataset</a>
    - <a href='#1_1'>1.1. Read dataset</a>
    - <a href='#1_2'>1.2. Check null data</a>
    - <a href='#1_2'>1.3. Make meta dataframe</a>
- <a href='#2'>2. EDA - application train</a>
    - <a href='#2_1'>2.1. Object feature</a>
        - <a href='#2_1_1'>2.1.1 Contract type</a>
        - <a href='#2_1_2'>2.1.2. Gender</a>
        - <a href='#2_1_3'>2.1.3. Do you have an own car?</a>
        - <a href='#2_1_4'>2.1.4. Do you have own realty?</a>
        - <a href='#2_1_5'>2.1.5. Suite type</a>
        - <a href='#2_1_6'>2.1.6. Income type</a>
        - <a href='#2_1_7'>2.1.7 Contract type </a>
        - <a href='#2_1_8'>2.1.8. 2.8 Family status</a>
        - <a href='#2_1_9'>2.1.9. Housing type</a>
        - <a href='#2_1_10'>2.1.10. Occupation type</a>
        - <a href='#2_1_11'>2.1.11. Process start (weekday)</a>
        - <a href='#2_1_12'>2.1.12. Organization type</a>
        - <a href='#2_1_13'>2.1.13. FONDKAPREMONT </a>
        - <a href='#2_1_14'>2.1.14. House type</a>
        - <a href='#2_1_15'>2.1.15. Wall material</a>
        - <a href='#2_1_16'>2.1.16. Emergency</a>
    - <a href='#2_2'>2.2. Int feature</a>
        - <a href='#2_2_1'>2.2.1 Count of children</a>
        - <a href='#2_2_2'>2.2.2. Mobil</a>
        - <a href='#2_2_3'>2.2.3. EMP Phone</a>
        - <a href='#2_2_4'>2.2.4. Work phone</a>
        - <a href='#2_2_5'>2.2.5. Cont mobile</a>
        - <a href='#2_2_6'>2.2.6. Phone</a>
        - <a href='#2_2_7'>2.2.7 Region Rating Client</a>
        - <a href='#2_2_8'>2.2.8. Region Rating Client With City</a>
        - <a href='#2_2_9'>2.2.9. Hour Appr Process Start</a>
        - <a href='#2_2_10'>2.2.10. Register region and not live region</a>
        - <a href='#2_2_11'>2.2.11. Register region and not work region</a>
        - <a href='#2_2_12'>2.2.12. Live region and not work region</a>
        - <a href='#2_2_13'>2.2.13. Register city and not live city</a>
        - <a href='#2_2_14'>2.2.14. Register city and not work city</a>
        - <a href='#2_2_15'>2.2.15. Live city and not work city</a>
        - <a href='#2_2_16'>2.2.16. Heatmap for int features</a>
        - <a href='#2_2_17'>2.2.17. More analysis for int features which have correlation with target</a>
        - <a href='#2_2_18'>2.2.18. linear regression analysis on the high correlated feature combinations</a> 
- <a href='#3'>3. EDA - Bureau</a>
    - <a href='#3_1'>3.1. Read and check data</a>
    - <a href='#3_2'>3.2. Merge with application_train</a>
    - <a href='#3_3'>3.3. Analysis on object feature</a>
        - <a href='#3_3_1'>3.3.1. Credit active</a>
        - <a href='#3_3_2'>3.3.2. Credit currency</a>
        - <a href='#3_3_3'>3.3.3. Credit type</a>
    - <a href='#3_4'>3.4. Analysis on int feature</a>
        - <a href='#3_4_1'>3.4.1. Credit day</a>
        - <a href='#3_4_2'>3.4.2. Credit day overdue</a>
        - <a href='#3_4_3'>3.4.3. Credit day prolong</a>
    - <a href='#3_5'>3.5. Analysis on float feature</a>
        - <a href='#3_5_1'>3.5.1 Amount credit sum</a>
        - <a href='#3_5_2'>3.5.2 Amount credit sum debt</a>
        - <a href='#3_5_3'>3.5.3 Amount credit sum limit</a>
        - <a href='#3_5_4'>3.5.4 Amount credit sum overdue</a>",content href 1 1 read dataset href 1 1 1 1 read dataset href 1 2 1 2 check null data href 1 2 1 3 make meta datafram href 2 2 eda applic train href 2 1 2 1 object featur href 2 1 1 2 1 1 contract type href 2 1 2 2 1 2 gender href 2 1 3 2 1 3 car href 2 1 4 2 1 4 realti href 2 1 5 2 1 5 suit type href 2 1 6 2 1 6 incom type href 2 1 7 2 1 7 contract type href 2 1 8 2 1 8 2 8 famili statu href 2 1 9 2 1 9 hous type href 2 1 10 2 1 10 occup type href 2 1 11 2 1 11 process start weekday href 2 1 12 2 1 12 organ type href 2 1 13 2 1 13 fondkapremont href 2 1 14 2 1 14 hous type href 2 1 15 2 1 15 wall materi href 2 1 16 2 1 16 emerg href 2 2 2 2 int featur href 2 2 1 2 2 1 count child href 2 2 2 2 2 2 mobil href 2 2 3 2 2 3 emp phone href 2 2 4 2 2 4 work phone href 2 2 5 2 2 5 cont mobil href 2 2 6 2 2 6 phone href 2 2 7 2 2 7 region rate client href 2 2 8 2 2 8 region rate client citi href 2 2 9 2 2 9 hour appr process start href 2 2 10 2 2 10 regist region live region href 2 2 11 2 2 11 regist region work region href 2 2 12 2 2 12 live region work region href 2 2 13 2 2 13 regist citi live citi href 2 2 14 2 2 14 regist citi work citi href 2 2 15 2 2 15 live citi work citi href 2 2 16 2 2 16 heatmap int featur href 2 2 17 2 2 17 analysi int featur correl target href 2 2 18 2 2 18 linear regress analysi high correl featur combin href 3 3 eda bureau href 3 1 3 1 read check data href 3 2 3 2 merg applic train href 3 3 3 3 analysi object featur href 3 3 1 3 3 1 credit activ href 3 3 2 3 3 2 credit currenc href 3 3 3 3 3 3 credit type href 3 4 3 4 analysi int featur href 3 4 1 3 4 1 credit day href 3 4 2 3 4 2 credit day overdu href 3 4 3 3 4 3 credit day prolong href 3 5 3 5 analysi float featur href 3 5 1 3 5 1 amount credit sum href 3 5 2 3 5 2 amount credit sum debt href 3 5 3 3 5 3 amount credit sum limit href 3 5 4 3 5 4 amount credit sum overdu
3884,# <a id='1'>1. Read dataset</a>,id 1 1 read dataset
3885,## <a id='1_1'>1.1. Read dataset</a>,id 1 1 1 1 read dataset
3886,## <a id='1_2'>1.2. Check null data</a>,id 1 2 1 2 check null data
3887,"- With msno library, we could see the blanks in the dataset. Check null data in application train.",msno librari could see blank dataset check null data applic train
3888,## <a id='1_3'>1.3. Make meta dataframe</a>,id 1 3 1 3 make meta datafram
3889,"- There are 3 data types(float64, int64, object) in application_train dataframe.",3 data type float64 int64 object applic train datafram
3890,"- Before starting EDA, It would be useful to make meta dataframe which include the information of dtype, level, response rate and role of each features. ",start eda would use make meta datafram includ inform dtype level respons rate role featur
3891,## <a id='1_4'>1.4. Check imbalance of target</a>,id 1 4 1 4 check imbal target
3892,"- Checking the imbalance of dataset is important. If imbalanced, we need to select more technical strategy to make a model.",check imbal dataset import imbalanc need select technic strategi make model
3893,"- As you can see, target is imbalanced.
- This fact makes this competition diffcult to solve. But, no pain, no gain. After this competition, we could learn many things! Enjoy!",see target imbalanc fact make competit diffcult solv pain gain competit could learn mani thing enjoy
3894,# <a id='2'>2. EDA - application_train </a>,id 2 2 eda applic train
3895,## <a id='2_1'>2.1. Object feature</a>,id 2 1 2 1 object featur
3896,- I want to draw two count bar plot for each object and int features. One contain the each count of responses and other contain the percent on target.,want draw two count bar plot object int featur one contain count respons contain percent target
3897,"- Sometimes, null data itself can be important feature. So, I want to compare the change when using null data as feature and not using nulll data as feature.",sometim null data import featur want compar chang use null data featur use nulll data featur
3898,### <a id='2_1_1'>2.1.1. Contract type</a>,id 2 1 1 2 1 1 contract type
3899,**REMIND:  repay == 0 and not repay == 1**,remind repay 0 repay 1
3900,"- Most contract type of clients is Cash loans. 
- Not repayment rate is higher in cash loans (~8%) than in revolving loans(~5%).",contract type client cash loan repay rate higher cash loan 8 revolv loan 5
3901,### <a id='2_1_2'>2.1.2. Gender</a>,id 2 1 2 2 1 2 gender
3902,"- The number of female clients is almoist double the number of male clients.
- Males have a higher chance of not returning their loans (~10%), comparing with women(~7%).",number femal client almoist doubl number male client male higher chanc return loan 10 compar woman 7
3903,### <a id='2_1_3'>2.1.3. Do you have an own car?</a>,id 2 1 3 2 1 3 car
3904,"- The clients that owns a car are higher than no-car clients by a factor of two times. 
- The Not-repayment percent is similar. (Own: ~7%, Not-own: ~8%)",client own car higher car client factor two time repay percent similar 7 8
3905,### <a id='2_1_4'>2.1.4. Do you have own realty?</a>,id 2 1 4 2 1 4 realti
3906,"- T he clients that owns a realty almost a half of the ones that doesn't own realty. 
- Both categories have not-repayment rate, about ~8%.",client own realti almost half one realti categori repay rate 8
3907,### <a id='2_1_5'>2.1.5. Suite type</a>,id 2 1 5 2 1 5 suit type
3908,"- Most suite type of clients are 'Unaccompanied', followed by Family, Spouse, children.
- When considering null data, there is no change the order.
- Other_B and Other_A have higher not-repayment rate than others.",suit type client unaccompani follow famili spous child consid null data chang order b higher repay rate other
3909,### <a id='2_1_6'>2.1.6. Income type</a>,id 2 1 6 2 1 6 incom type
3910,"- Most of the clients get income from working. 
- The number of Student, Unemployed, Bussnessman and Maternity leave are very few.
- When unemployed and maternity leave, there is  high probability of not-repayment.",client get incom work number student unemploy bussnessman matern leav unemploy matern leav high probabl repay
3911,### <a id='2_1_7'>2.1.7. Education type</a>,id 2 1 7 2 1 7 educ type
3912,"- Clients with secondary education type are most numerous, followed by higher education, incomplete higher.
- Clients with Lower secondary have the highest not-repayment rate(~10%).",client secondari educ type numer follow higher educ incomplet higher client lower secondari highest repay rate 10
3913,### <a id='2_1_8'>2.1.8. Family status</a>,id 2 1 8 2 1 8 famili statu
3914,"- Most of clients  for loans are married followed by single/not married, civial marriage.
- Civil marriage have almost 10% ratio of not returning loans followed by single/notmarried(9.9%), separate(8%).",client loan marri follow singl marri civial marriag civil marriag almost 10 ratio return loan follow singl notmarri 9 9 separ 8
3915,### <a id='2_1_9'>2.1.9. Housing type</a>,id 2 1 9 2 1 9 hous type
3916,"- Clients with house/apartment are most numerous, followed by With parents, Municipal apartment.
- When Rented apartment and live with parents, clients have somewhat high not-repayment ratio. (~12%)
",client hous apart numer follow parent municip apart rent apart live parent client somewhat high repay ratio 12
3917,### <a id='2_1_10'>2.1.10. Occupation type</a>,id 2 1 10 2 1 10 occup type
3918,"- When not considering null data, Majority of clients are laborers, sales staff, core staff, drivers. But with considering null data, null data(I think it would be 'not want to repond' or 'no job', 'not in category') are most numerous.
- However, not-repayment rate is low for null data. Low-skill labor is the most high not-repayment rate (~17%) in both plot.",consid null data major client labor sale staff core staff driver consid null data null data think would want repond job categori numer howev repay rate low null data low skill labor high repay rate 17 plot
3919,### <a id='2_1_11'>2.1.11. Process start (weekday)</a>,id 2 1 11 2 1 11 process start weekday
3920,"- The number of process for weekend is less than other days. That's because Weekend is weekend.
- There are no big changes between not-repayment rate of all days.
- Day is not important factor for repayment.",number process weekend le day weekend weekend big chang repay rate day day import factor repay
3921,### <a id='2_1_12'>2.1.12. Organization type</a>,id 2 1 12 2 1 12 organ type
3922,"- The most frequent case of organization is Bussiness Entity Type 3 followed XNA and self-employ.
- The Transport: type 3 has the highest not repayment rate(~16%), Industry: type 13(~13.5%).",frequent case organ bussi entiti type 3 follow xna self employ transport type 3 highest repay rate 16 industri type 13 13 5
3923,### <a id='2_1_13'>2.1.13. FONDKAPREMONT</a>,id 2 1 13 2 1 13 fondkapremont
3924,"- Actually, I don't know exact meaning of this feature FONDKAPREMONT_MODE.
- Anyway, when considering null data, nul data has the highest count and not-repayment rate.",actual know exact mean featur fondkapremont mode anyway consid null data nul data highest count repay rate
3925,### <a id='2_1_14'>2.1.14. House type</a>,id 2 1 14 2 1 14 hous type
3926,"- When considering null data, null data and block of flats are two-top. 
- But, specific housing and terraced house have higher not-repayment rate than block of flats. 
- null data has the highest not-repayment rate(~9%).",consid null data null data block flat two top specif hous terrac hous higher repay rate block flat null data highest repay rate 9
3927,### <a id='2_1_15'>2.1.15. Wall material</a>,id 2 1 15 2 1 15 wall materi
3928,"- There are over 150,000 null data for WALLSMATERIAL_MODE. 
- Clients with Wooden have higher than 9% not repayment rate.",150 000 null data wallsmateri mode client wooden higher 9 repay rate
3929,### <a id='2_1_16'>2.1.16. Emergency</a>,id 2 1 16 2 1 16 emerg
3930,"- For emergency state, there is also many null data. 
- If clients is in an emergency state, not-repayment rate(~10%) is higher than not in an emergency state.
- null is also high not-repayment rate(~-10%).",emerg state also mani null data client emerg state repay rate 10 higher emerg state null also high repay rate 10
3931,## <a id='2_2'>2.2. Int feature</a>,id 2 2 2 2 int featur
3932,- Let's do similar analysis for int features.,let similar analysi int featur
3933,### <a id='2_2_1'>2.2.1. Count of children</a>,id 2 2 1 2 2 1 count child
3934,"- Most clients with no children requested loan. 
- Clients with 9, 11 have 100% not-repayment rate. the each count of those cases is 2 and 1.
- Except 9, 11, Clients with 6 children has high not-repayment rate.",client child request loan client 9 11 100 repay rate count case 2 1 except 9 11 client 6 child high repay rate
3935,### <a id='2_2_2'>2.2.2. Mobil</a>,id 2 2 2 2 2 2 mobil
3936,- There are no clients without mobil(maybe mobile).,client without mobil mayb mobil
3937,### <a id='2_2_3'>2.2.3. EMP Phone</a>,id 2 2 3 2 2 3 emp phone
3938,"- Most clients(82%) have EPM Phone.
- The gap between the not-repayment percent is about 3%.",client 82 epm phone gap repay percent 3
3939,### <a id='2_2_4'>2.2.3. Work Phone</a>,id 2 2 4 2 2 3 work phone
3940,- Most clients(80%) don't have work phone.,client 80 work phone
3941,### <a id='2_2_5'>2.2.5. Cont mobile</a>,id 2 2 5 2 2 5 cont mobil
3942,- Clients who chose 'no' for CONT_MOBILE FALG is very few.(574),client chose cont mobil falg 574
3943,### <a id='2_2_6'>2.2.6. Phone</a>,id 2 2 6 2 2 6 phone
3944,- Most clients(72%) don't have work phone.,client 72 work phone
3945,### <a id='2_2_7'>2.2.7. Region Rating Client</a>,id 2 2 7 2 2 7 region rate client
3946,"- Clients who chose 2 for REGION_RATING_CLIENT is numerous, followed by 3, 1.
- For not-repayment, the order is 3, 2, 1.",client chose 2 region rate client numer follow 3 1 repay order 3 2 1
3947,### <a id='2_2_8'>2.2.8. Region Rating Client With City</a>,id 2 2 8 2 2 8 region rate client citi
3948,"- Clients who chose 2 for REGION_RATING_CLIENT with city is numerous, followed by 3, 1.
- For not-repayment, the order is 3, 2, 1.",client chose 2 region rate client citi numer follow 3 1 repay order 3 2 1
3949,### <a id='2_2_9'>2.2.9. Hour Appr Process Start</a>,id 2 2 9 2 2 9 hour appr process start
3950,- The most busy hour for Appr Process Start is a range from 10:00 to 13:00.,busi hour appr process start rang 10 00 13 00
3951,### <a id='2_2_10'>2.2.10. Register region and not live region</a>,id 2 2 10 2 2 10 regist region live region
3952,- 98.5% of clients registered their region but not live in the region.,98 5 client regist region live region
3953,### <a id='2_2_11'>2.2.11. Register region and not work region</a>,id 2 2 11 2 2 11 regist region work region
3954,- 95% of clients registered their region but not work in the region.,95 client regist region work region
3955,### <a id='2_2_12'>2.2.12. Live region and not work region</a>,id 2 2 12 2 2 12 live region work region
3956,- 95.9% of clients lives in their region but don't work in the region.,95 9 client live region work region
3957,"- For 3 questions about region(10, 11, 12), the not-repayment percent is similar for each case.",3 question region 10 11 12 repay percent similar case
3958,### <a id='2_2_13'>2.2.13. Register city and not live city</a>,id 2 2 13 2 2 13 regist citi live citi
3959,"- 92.1% of clients registered city and don't live in the city.
- Unlike region, city could be good information. Because the difference of the not-repayment percent between 'yes' and 'no' is higher than region case(2.2.10, 2.2.11, 2.2.12)",92 1 client regist citi live citi unlik region citi could good inform differ repay percent ye higher region case 2 2 10 2 2 11 2 2 12
3960,### <a id='2_2_14'>2.2.14. Register city and not work city</a>,id 2 2 14 2 2 14 regist citi work citi
3961,"- 78% of clients registered city and don't work in the city.
- If client is this case, the not-repayment rate is about 10%.",78 client regist citi work citi client case repay rate 10
3962,### <a id='2_2_15'>2.2.15. Live city and not work city</a>,id 2 2 15 2 2 15 live citi work citi
3963,"- 82% of clients registered city and don't work in the city.
- If client is this case, the not-repayment rate is about 10%.",82 client regist citi work citi client case repay rate 10
3964,### <a id='2_2_16'>2.2.16. Flag document</a>,id 2 2 16 2 2 16 flag document
3965,"- Document 2: 13 clients chose 1 and not-repayment rate is high, about 30%.
- Document 4: 25 clients chose 1 and not-repayment rate is 0. all the clients who chose 1 repaid.
- Document 10: 7 clients chose 1 and not-repayment rate is 0. all the clients who chose 1 repaid.
- Document 12: 2 clients chose 1 and not-repayment rate is 0. all the clients who chose 1 repaid.",document 2 13 client chose 1 repay rate high 30 document 4 25 client chose 1 repay rate 0 client chose 1 repaid document 10 7 client chose 1 repay rate 0 client chose 1 repaid document 12 2 client chose 1 repay rate 0 client chose 1 repaid
3966,### <a id='2_2_16'>2.2.16. Heatmap for int features</a>,id 2 2 16 2 2 16 heatmap int featur
3967,- Let's see the correlations between the int features. Heatmap helps us to see this easily.,let see correl int featur heatmap help u see easili
3968,"- There are some combinations with high correlation coefficient.
- FLAG_DOCUMENT_6 and FLAG_EMP_PHONE
- DAYS_BIRTH and FLAG_EMP_PHONE
- DAYS_EMPLOYED and FLAG_EMP_PHONE
- In follow section, we will look those features more deeply using linear regression plot with seaborn.",combin high correl coeffici flag document 6 flag emp phone day birth flag emp phone day employ flag emp phone follow section look featur deepli use linear regress plot seaborn
3969,### <a id='2_2_17'>2.2.17. More analysis for int features which have correlation with target</a>,id 2 2 17 2 2 17 analysi int featur correl target
3970,"- At first, find the int features which have high correlation with target.",first find int featur high correl target
3971,"- DAYS_BIRTH is some high correlation with target.
- With dividing 365(year) and applying abs(), we can see DAYS_BIRTH in the unit of year(AGE).",day birth high correl target divid 365 year appli ab see day birth unit year age
3972,"- As you can see, The younger, The higher not-repayment probability.
- The older, The lower not-repayment probability.",see younger higher repay probabl older lower repay probabl
3973,### <a id='2_2_18'>2.2.18. linear regression analysis on the high correlated feature combinations</a>,id 2 2 18 2 2 18 linear regress analysi high correl featur combin
3974,"- With lmplot from seaborn, we can draw linear regression plot very easily. Thanks!",lmplot seaborn draw linear regress plot easili thank
3975,"- With gaussian kde density represented by color and linear regression plot, we can see that there are many clients who have EMP Phone and chose document 6.",gaussian kde densiti repres color linear regress plot see mani client emp phone chose document 6
3976,"- With gaussian kde density represented by color and linear regression plot, we can see that the younger people tend to have EMP phone.",gaussian kde densiti repres color linear regress plot see younger peopl tend emp phone
3977,"- With gaussian kde density represented by color and linear regression plot, we can see that clients with shorter employed tend to have EMP phone. (simiar result compared to FLAG_EMP_PHONE vs DAYS_BIRTH)",gaussian kde densiti repres color linear regress plot see client shorter employ tend emp phone simiar result compar flag emp phone v day birth
3978,## <a id='2_3'>2.3. float feature</a>,id 2 3 2 3 float featur
3979,- Let's move on float features.,let move float featur
3980,### <a id='2_3_1'>2.3.1. Heatmap for float features</a>,id 2 3 1 2 3 1 heatmap float featur
3981,- Let us draw the heatmap of float features.,let u draw heatmap float featur
3982,"- There are some features which have  some high correlation with target. In follow section, we will find them and analyze them.
- There are many feature combinations which have high correlation value(larger than 0.9).
- Let's find the combinations.",featur high correl target follow section find analyz mani featur combin high correl valu larger 0 9 let find combin
3983,### <a id='2_3_2'>2.3.2. More analysis for int features which have correlation with target</a>,id 2 3 2 2 3 2 analysi int featur correl target
3984,- Let's find the float features which are highly correlated with target.,let find float featur highli correl target
3985,"- The simple kde plot(kernel density estimation plot) shows that the distribution of repay and not-repay is different for EXT_SOURCE_1.
- EXT_SOURCE_1 can be good feature.",simpl kde plot kernel densiti estim plot show distribut repay repay differ ext sourc 1 ext sourc 1 good featur
3986,"- Not as much as EXT_SOURCE_1 do, EXT_SOURCE_2 shows different distribution for each repay and not-repay.",much ext sourc 1 ext sourc 2 show differ distribut repay repay
3987,"- EXX_SOUCE_3 has similar trend with EXT_SOURCE_1.
- EXT_SOURCE_3 can be good feature.",exx souc 3 similar trend ext sourc 1 ext sourc 3 good featur
3988,### <a id='2_3_3'>2.3.3. linear regression analysis on the high correlated feature combinations</a>,id 2 3 3 2 3 3 linear regress analysi high correl featur combin
3989,"- Using corr() and numpy boolean technique with triu(), we could obtain the correlation matrix without replicates.",use corr numpi boolean techniqu triu could obtain correl matrix without replic
3990,"- There are 60 combinations which have larger correlation values than 0.95.
- Let's draw the regplot for all combinations with splitting the target.",60 combin larger correl valu 0 95 let draw regplot combin split target
3991,"- After looking these 56 plots, I found som combinations in which the distribution for repay and not-repay is a bit different.
- Let's see this with single and multi variable kde plot.
- It is nice to use log-operation on features. With log-operation, we can analyze the distribution more easily.",look 56 plot found som combin distribut repay repay bit differ let see singl multi variabl kde plot nice use log oper featur log oper analyz distribut easili
3992,"- The mutivariate kde plot of not-repay is broader than one of repay.
- For both CNT_60_SOCIAL_CIRCLE and OBS_30_CNT_SOCIAL_CIRCLE, the distribution of each repay and not-repay is a bit different. Log-operation helps us to see them easily.",mutivari kde plot repay broader one repay cnt 60 social circl ob 30 cnt social circl distribut repay repay bit differ log oper help u see easili
3993,"- This case is similar with previous case.
- The mutivariate kde plot of not-repay is broader than one of repay.
- For both CNT_60_SOCIAL_CIRCLE and OBS_30_CNT_SOCIAL_CIRCLE, the distribution of each repay and not-repay is a bit different. Log-operation helps us to see them easily.",case similar previou case mutivari kde plot repay broader one repay cnt 60 social circl ob 30 cnt social circl distribut repay repay bit differ log oper help u see easili
3994,# <a id='3'>3. EDA - Bureau </a>,id 3 3 eda bureau
3995,- Bureau data contains the information of previous loan history of clients from other company.,bureau data contain inform previou loan histori client compani
3996,## <a id='3_1'>3.1. Read and check data</a>,id 3 1 3 1 read check data
3997,## <a id='3_2'>3.2. Merge with application_train</a>,id 3 2 3 2 merg applic train
3998,- A client can have several loans so that merge with bureau data can explode the row of application train.,client sever loan merg bureau data explod row applic train
3999,## <a id='3_3'>3.3. Analysis on object feature</a>,id 3 3 3 3 analysi object featur
4000,### <a id='3_3_1'>3.3.1 Credit active</a>,id 3 3 1 3 3 1 credit activ
4001,"- Most credit type of clients is 'Closed', 'Active'. 
- If credit type is finished in the state of bad dept, the not-repayment rate is some high.(20%)",credit type client close activ credit type finish state bad dept repay rate high 20
4002,### <a id='3_3_2'>3.3.2 Credit currency</a>,id 3 3 2 3 3 2 credit currenc
4003,"- 99.9% of clients chose currency 1.
- By the way, the not-repayment rate is high at currency 3.",99 9 client chose currenc 1 way repay rate high currenc 3
4004,### <a id='3_3_3'>3.3.3 Credit type</a>,id 3 3 3 3 3 3 credit type
4005,"- Clients with consumer credit is most numerous, followed by credit card.
- If clients requested loan for the purchase of equipment, the not-repayment rate is high.(23.5%). Next is microloan(20.6%).",client consum credit numer follow credit card client request loan purchas equip repay rate high 23 5 next microloan 20 6
4006,## <a id='3_4'>3.4. Analysis on int feature</a>,id 3 4 3 4 analysi int featur
4007,### <a id='3_4_1'>3.4.1 Credit day</a>,id 3 4 1 3 4 1 credit day
4008,"- There are 2 general(not linear) trends we can see.
- The shorter credit days, the more not-repayment.
- The larger credit days, the more repayment.",2 gener linear trend see shorter credit day repay larger credit day repay
4009,### <a id='3_4_2'>3.4.2 Credit day overdue</a>,id 3 4 2 3 4 2 credit day overdu
4010,- It is hard to see the trend for now. Let's remove the samples. (overdue < 200),hard see trend let remov sampl overdu 200
4011,"- As you can see, repay have a litter more right-skewed distribution.
- To see more deeply, Let's divide the overdue feature into several groups. ",see repay litter right skew distribut see deepli let divid overdu featur sever group
4012,"- The clients with short overdue days(<30) is most numerous.
- B group has the highest not-repayment rate (19%), followed by C, D, E. A group is the lowest.",client short overdu day 30 numer b group highest repay rate 19 follow c e group lowest
4013,- KDE plot with samples which have overdue larger than 30 shows that the distribution of clients who repaid is larger than that of not-repay clients.,kde plot sampl overdu larger 30 show distribut client repaid larger repay client
4014,### <a id='3_4_3'>3.4.3 Credit day prolong</a>,id 3 4 3 3 4 3 credit day prolong
4015,- There are no clients who have prolong larger than 3.,client prolong larger 3
4016,## <a id='3_5'>3.5. Analysis on float feature</a>,id 3 5 3 5 analysi float featur
4017,### <a id='3_5_1'>3.5.1 Amount credit sum</a>,id 3 5 1 3 5 1 amount credit sum
4018,"- As you can see, if credit is lower than 2,000,000, the distribution of each repay and not-repay is similar.
- But, if credit is larger than 2,000,000, the distribution of each repay and not-repay is different. Many clients who have very high(> 10,000,000) credit repaid.",see credit lower 2 000 000 distribut repay repay similar credit larger 2 000 000 distribut repay repay differ mani client high 10 000 000 credit repaid
4019,### <a id='3_5_2'>3.5.2 Amount credit sum debt</a>,id 3 5 2 3 5 2 amount credit sum debt
4020,"- AMT_CREDIT_SUM_DEBT shows similar trend compared to AMT_CREDIT_SUM.
- Many clients with high dept(> 50,000,000) repaid.",amt credit sum debt show similar trend compar amt credit sum mani client high dept 50 000 000 repaid
4021,### <a id='3_5_3'>3.5.3 Amount credit sum limit</a>,id 3 5 3 3 5 3 amount credit sum limit
4022,"- In rough way, the repay clients have high CREDIT_SUM_LIMIT.
- Is it possible to have minus credit sum limit??",rough way repay client high credit sum limit possibl minu credit sum limit
4023,### <a id='3_5_4'>3.5.4 Amount credit sum overdue</a>,id 3 5 4 3 5 4 amount credit sum overdu
4024,"- Likewise, there are many repay-clients who have large credit sum overdue.",likewis mani repay client larg credit sum overdu
4025,"**Want to find out how to do the feature enigeering step automatically in a concise and compact tutorial, applying it on a binary classification problem using lightGBM?---look no further **

After finding out that data scientists created a tool that ""replaces"" data-scientists I had to try it out. Thank you [https://docs.featuretools.com/](http://)! Feature-engineering is tiresome, and takes the biggest amount of time do it. What if we can make it a one liner. Well now it seems we can. Even more appropriately we will be working on Home Credit Default Risk. A set of datasets where all of them are in a relationship with one-another and from all of them some information should be extracted. Featuretools makes it easy! Our goal in the end is simple. Predict whether the customer will default or not.",want find featur enig step automat concis compact tutori appli binari classif problem use lightgbm look find data scientist creat tool replac data scientist tri thank http doc featuretool com http featur engin tiresom take biggest amount time make one liner well seem even appropri work home credit default risk set dataset relationship one anoth inform extract featuretool make easi goal end simpl predict whether custom default
4026,"Load in the data, NOTE: datasets are huge, working on them will be computationally costly. In order to avoid it we can introduce some limited sample size.",load data note dataset huge work comput costli order avoid introduc limit sampl size
4027,If we merge datasets now we can perofrm neccesary operations and seperate them later.,merg dataset perofrm neccesari oper seper later
4028,**NOTE** This NaN handling is just for the sake of it. It is by no-means complete and there are lot of them underneath (function is built that shows us percentage). But there is a specific way that GBM (light and xBGM) handle missing values. So even tough it would be better we want to focus on algortihm and automatic feature engineering!,note nan handl sake mean complet lot underneath function built show u percentag specif way gbm light xbgm handl miss valu even tough would better want focu algortihm automat featur engin
4029,"**NOTE** Even tough it is automatic, we can incorporate some manual features. IF we know some domain specific information.",note even tough automat incorpor manual featur know domain specif inform
4030,"***Featuretools*** is an open-source Python library for automatically creating features out of a set of related tables using a technique called deep feature synthesis. Automated feature engineering, like many topics in machine learning, is a complex subject built upon a foundation of simpler ideas. By going through these ideas one at a time, we can build up our understanding of how featuretools which will later allow for us to get the most out of it.

There are a few concepts that we will cover along the way:

1.  Entities and EntitySets
2. Relationships between tables
3. Feature primitives: aggregations and transformations
4. Deep feature synthesis",featuretool open sourc python librari automat creat featur set relat tabl use techniqu call deep featur synthesi autom featur engin like mani topic machin learn complex subject built upon foundat simpler idea go idea one time build understand featuretool later allow u get concept cover along way 1 entiti entityset 2 relationship tabl 3 featur primit aggreg transform 4 deep featur synthesi
4031,**2. Relationships betweeen the sets**,2 relationship betweeen set
4032,**Feature primitives** Basically which functions are we going to use to create features. Since we did not specify it we will be using standard ones (check doc) There is a option to define own ones or to just select some of the standards.,featur primit basic function go use creat featur sinc specifi use standard one check doc option defin one select standard
4033,**Label encoding** Making it machine readable,label encod make machin readabl
4034,**NaN imputation** will be skipped in this tutorial.,nan imput skip tutori
4035,Let us split the variables one more time.,let u split variabl one time
4036,"**Train** the model, predict, etc.",train model predict etc
