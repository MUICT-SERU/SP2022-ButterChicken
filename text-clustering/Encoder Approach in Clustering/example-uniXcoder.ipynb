{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare UniXcoder source file\n",
    "import os\n",
    "from urllib import request\n",
    "unixcoder_url = 'https://raw.githubusercontent.com/microsoft/CodeBERT/master/UniXcoder/unixcoder.py'\n",
    "\n",
    "if not (os.path.exists('./unixcoder.py')):\n",
    "    with request.urlopen(unixcoder_url) as response, open('unixcoder.py', 'wb') as out_file:\n",
    "        data = response.read()\n",
    "        out_file.write(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UniXcoder(\n",
       "  (model): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(51416, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(1026, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(10, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): RobertaPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=51416, bias=False)\n",
       "  (lsm): LogSoftmax(dim=-1)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch    \n",
    "from unixcoder import UniXcoder\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = UniXcoder(\"microsoft/unixcoder-base\")\n",
    "model.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Encoder-only Mode\n",
    "\n",
    "For encoder-only mode, we give an example of code search.\n",
    "\n",
    "## 1) Code and NL Embeddings\n",
    "\n",
    "Here, we give an example to obtain code fragment embedding from CodeBERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 768])\n",
      "tensor([[ 8.6533e-01, -1.9796e+00, -8.6849e-01,  4.2652e-01, -5.3695e-01,\n",
      "         -1.5521e-01,  5.3771e-01,  3.4200e-01,  3.6306e-01, -3.9392e-01,\n",
      "         -1.1816e+00,  2.6010e+00, -7.7133e-01,  1.8441e+00,  2.3645e+00,\n",
      "         -8.0971e-01,  1.9955e+00,  6.4072e-01, -2.4817e-01, -2.6337e+00,\n",
      "         -1.1199e+00, -2.3594e+00, -7.5279e-01,  1.1628e-01,  1.6286e+00,\n",
      "         -4.5879e-02, -1.4457e+00,  7.8070e-01,  7.7836e-01, -5.6094e-01,\n",
      "          2.2112e+00, -7.8929e-01, -2.3475e+00,  1.7342e+00, -8.8963e-01,\n",
      "         -6.9369e-01,  2.4841e+00,  4.9074e-01, -8.5599e-01,  1.0941e-01,\n",
      "         -1.4604e+00,  2.3402e+00, -1.8161e+00, -2.9290e+00, -3.9770e+00,\n",
      "         -4.7137e-01,  1.7589e+00,  2.5646e+00, -4.2621e-01, -8.0802e-02,\n",
      "         -2.9151e+00,  1.6699e+00, -1.9765e+00, -8.8960e-01,  8.9094e-01,\n",
      "         -6.7859e-01, -4.2710e-01, -2.1378e+00, -2.3407e+00, -4.9259e-01,\n",
      "          1.6661e+00, -1.4553e+00, -4.7143e-01,  1.7414e+00, -1.4021e+00,\n",
      "         -3.6193e-01, -2.2502e+00, -9.5057e-01, -2.3749e+00,  6.7465e-02,\n",
      "         -9.8630e-01, -7.7291e-01, -2.3313e+00, -1.9852e+00,  1.5710e+00,\n",
      "         -1.6094e+00, -2.6803e-01,  2.6429e-02, -4.9423e-01, -1.1881e+00,\n",
      "         -4.3671e+00,  3.6084e-02,  1.6481e+00,  7.6896e-01, -2.2784e+00,\n",
      "         -8.9302e-01,  7.6355e-01, -1.4323e+00, -1.7019e+00, -3.3756e+00,\n",
      "          7.5928e-01, -3.7095e-01, -8.6089e-01,  2.1288e-01, -4.8852e-01,\n",
      "         -5.8477e-01, -4.7743e-01, -1.1124e+00,  9.4385e-01,  4.4986e-01,\n",
      "          8.8278e-01, -1.2682e+00, -1.0734e+00, -5.0031e+00, -1.3651e-01,\n",
      "          1.6317e+00,  2.6095e+00,  1.4500e+00,  2.1283e+00,  9.8562e-01,\n",
      "         -6.4999e-01, -2.4928e+00,  3.9803e-01,  1.6497e+00, -2.4540e+00,\n",
      "         -1.2010e-01,  3.6969e-01,  3.2260e+00,  1.0488e+00,  1.6079e+00,\n",
      "          5.1230e-02, -8.2776e-01, -2.7712e-01, -8.1665e-01,  1.2216e+00,\n",
      "          2.2626e+00,  5.8629e-01,  1.8330e+00, -1.8594e+00, -6.7486e-01,\n",
      "         -2.9770e-01, -1.4904e+00, -1.8952e-01,  3.7281e-01,  2.5490e+00,\n",
      "         -3.5865e+00,  7.6946e-02,  1.5938e-01,  2.6871e+00, -1.0076e-01,\n",
      "         -4.7741e-01,  2.7635e+00,  1.6578e+00,  1.5435e+00,  1.4152e-01,\n",
      "         -6.9205e-01, -2.3796e+00, -4.2272e-01,  1.0949e+00,  1.8627e+00,\n",
      "         -2.9699e+00, -8.3375e-01, -2.0909e+00, -3.0734e+00,  1.8948e+00,\n",
      "          7.4057e-01, -3.5740e+00,  2.6625e-01,  2.8922e+00,  8.7425e-01,\n",
      "         -2.1935e+00,  5.0644e-01, -1.7126e+00, -7.2971e-01,  5.0957e+00,\n",
      "          1.0711e+00,  9.1476e-01, -1.5729e+00, -2.3783e+00, -5.1781e-01,\n",
      "         -8.4463e-04,  1.4461e+00, -4.5946e-01, -1.8899e+00,  1.6712e+00,\n",
      "         -4.3924e-01, -3.2047e-01,  2.6099e-01,  2.1750e+00, -8.7994e-01,\n",
      "          5.9681e-01,  2.1475e+00,  8.0813e-01,  2.4080e+00,  5.8878e-01,\n",
      "         -1.0541e+00, -1.2985e+00,  1.1474e+00,  1.0200e+00, -9.2665e-01,\n",
      "          1.8708e+00, -5.8330e-01,  2.8271e+00,  4.7924e-01, -1.1762e+00,\n",
      "         -1.5221e+00, -1.9213e+00,  2.1211e+00,  7.8631e-01,  1.3683e+00,\n",
      "         -1.3232e+00,  5.1992e-01,  4.5145e-01,  4.8065e-01, -3.8292e-01,\n",
      "          2.9903e+00,  2.1630e+00,  8.8401e-01, -4.4036e-01,  2.7951e-01,\n",
      "         -1.4615e+00,  2.9297e-01,  4.4157e-01, -1.6591e+00, -7.0851e-02,\n",
      "          1.7456e+00, -2.5399e-01,  2.2914e+00,  6.1646e-01,  1.5350e+00,\n",
      "          5.4857e-01, -1.9781e+00,  3.6409e+00, -3.6094e+00, -1.3872e+00,\n",
      "         -3.1542e+00,  1.8863e+00,  1.4781e+00, -6.2330e-01,  6.8705e-01,\n",
      "         -2.9921e-01,  6.7187e-01, -8.7066e-01,  1.8712e+00, -7.8887e-01,\n",
      "         -4.5481e-01,  3.0124e+00, -1.0727e+00, -2.5037e+00,  1.0462e+00,\n",
      "          9.4948e-02,  1.8802e+00,  7.9786e-02, -8.3761e-01, -1.6686e+00,\n",
      "          4.5976e-01,  1.2183e-01,  1.9651e+00,  3.1637e+00,  1.8581e+00,\n",
      "          1.2011e+00,  1.7087e+00,  2.4795e+00, -1.3058e+00, -2.2082e-01,\n",
      "         -9.7837e-01, -5.9851e-01, -4.1465e-01,  5.3013e-01, -1.8790e+00,\n",
      "          1.2956e-01,  3.0678e+00,  2.2996e+00,  1.3984e-01,  1.7384e+00,\n",
      "         -9.4576e-01,  2.9731e-01,  1.0073e+00, -1.8723e+00, -2.6245e+00,\n",
      "          2.4883e+00,  1.2625e+00,  1.6069e+00,  2.7757e+00, -1.1868e+00,\n",
      "          4.5585e+00,  1.8439e+00, -1.3947e+00,  1.1803e-01, -5.9164e-01,\n",
      "         -5.3891e-01, -6.4977e-01, -2.1538e+00,  1.6207e-03,  1.3014e+00,\n",
      "         -1.3717e+00,  1.3189e+00, -2.5527e+00, -2.1349e-01, -1.0274e+00,\n",
      "          1.6072e+00,  1.5219e+00,  2.2671e-01,  2.8007e-01,  1.7929e+00,\n",
      "         -3.5017e-01,  3.0812e-01, -3.6473e-02, -1.0446e+00,  7.2995e-01,\n",
      "          3.3016e+00, -6.8710e-01, -9.2392e-01, -6.9455e-01, -1.7919e+00,\n",
      "         -3.6050e-01, -7.6518e-01,  7.7713e-02,  2.0953e+00, -1.8401e+00,\n",
      "          1.2078e+00, -2.8348e-02,  8.5488e-01,  6.0134e-02,  4.5298e-01,\n",
      "          8.1379e-01,  1.5397e+00,  9.0122e-01, -1.8710e-01,  8.8635e-01,\n",
      "          1.9935e+00, -8.6843e-01,  1.4459e+00, -7.8360e-01, -1.3131e+00,\n",
      "          1.4339e+00,  1.5508e+00, -1.9167e+00,  1.0708e+00, -2.6021e+00,\n",
      "         -3.2115e+00, -7.3727e-01,  1.5973e+00, -3.4530e-01, -1.3524e+00,\n",
      "          8.0773e-01, -2.4317e+00,  1.1786e-01, -4.5069e-01, -1.9596e+00,\n",
      "         -1.8529e+00,  1.7978e+00,  3.3062e+00,  1.0482e+00,  3.0625e+00,\n",
      "         -1.8172e+00, -1.7491e+00,  1.6452e+00,  2.3509e+00, -1.0910e+00,\n",
      "          4.4588e+00, -3.9524e+00, -1.0016e+00,  1.1959e-02, -3.3176e-01,\n",
      "         -7.0906e-01, -1.3792e+00, -2.6217e-01,  1.9470e+00, -1.9929e+00,\n",
      "         -1.9104e+00,  3.0809e+00,  5.9398e+00,  3.4074e+00,  3.1277e+00,\n",
      "          2.2578e+00,  1.2538e+00,  8.9262e-01,  1.1161e+00, -2.3135e-01,\n",
      "          4.8995e-01, -4.5361e-01,  1.0180e+00, -2.3327e-01,  1.7317e+00,\n",
      "          1.1097e+00, -7.3151e-01, -7.8843e-01,  1.6248e+00, -4.9050e-01,\n",
      "         -2.0954e+00,  1.1009e+00,  4.8284e-01, -2.6718e-01,  2.2595e+00,\n",
      "          1.0920e+00,  1.8582e+00, -1.7376e-01, -1.1475e-01, -1.1515e+00,\n",
      "         -5.2181e-01,  1.4267e-01, -2.1767e+00, -2.3535e+00, -3.4999e+00,\n",
      "         -5.0195e-01,  3.8530e+00,  7.5294e-01,  8.8543e-01, -9.7280e-01,\n",
      "         -9.4020e-01, -2.0962e+00,  2.9626e+00, -1.6750e+00,  1.9114e-02,\n",
      "         -1.8565e+00, -1.8600e+00,  1.1990e-01, -1.3710e-01, -1.2912e+00,\n",
      "         -7.5155e-01,  8.9784e-01, -1.1661e+00,  4.7234e-01, -2.6635e+00,\n",
      "         -1.9436e-01,  3.9821e+00, -1.1534e+00,  1.1378e+00,  1.0223e+00,\n",
      "         -8.1626e-02,  2.0543e+00,  9.0213e-01,  8.8088e-01,  1.3845e+00,\n",
      "         -6.0124e-01, -8.4403e-01,  2.1341e-01,  2.7125e-01,  8.6327e-01,\n",
      "          7.5735e-01,  5.3838e-01, -9.4080e-01,  1.6509e+00,  1.9516e+00,\n",
      "         -1.4890e-01,  1.1405e+00, -1.1336e+00,  1.9752e-01,  1.5183e+00,\n",
      "          1.2735e+00, -3.7795e-01,  1.4665e+00, -1.0437e+00,  6.4951e-01,\n",
      "          1.9781e+00,  1.7280e+00, -9.9077e-01, -7.7442e-01,  3.6293e-01,\n",
      "          4.4741e-01, -8.9763e-02,  1.4460e-01,  1.8498e-01, -1.6132e+00,\n",
      "          1.6969e-02,  1.4408e-01, -5.9247e-01,  1.2433e+00, -1.5539e+00,\n",
      "         -5.0649e-01, -4.9254e+00, -3.6030e-02,  7.1625e-01,  2.2614e+00,\n",
      "          1.3858e+00,  6.1051e-01, -1.1697e+00, -8.2037e-01,  1.7391e+00,\n",
      "          7.8424e-01, -1.4823e+00, -1.9801e+00,  5.7443e-02, -2.0917e+00,\n",
      "         -4.0722e-01, -6.9664e-01, -6.2578e-01, -2.0278e-01, -2.4859e+00,\n",
      "         -3.4334e+00, -3.5998e+00,  1.1663e+00, -1.9142e+00,  2.1345e+00,\n",
      "         -4.0112e-01,  2.3311e+00, -1.0246e+00,  1.6932e+00, -4.3411e-01,\n",
      "          1.0373e+00, -1.3833e+00, -7.9660e-01, -1.7153e+00, -5.1743e-01,\n",
      "          2.4753e+00,  5.9121e-01,  1.4010e+00,  1.1947e+00, -6.3277e-01,\n",
      "          8.0882e-01,  5.5221e-01,  8.8730e-02,  9.6511e-01, -1.8576e-01,\n",
      "         -1.3922e+00,  5.1404e-01, -2.0511e-01, -4.5490e-01, -5.8501e-01,\n",
      "         -1.8881e-01, -4.7180e+00,  2.2569e+00, -2.9924e-01,  5.0827e-01,\n",
      "          1.8820e+00, -1.3360e+00,  3.4777e+00,  1.0219e+00, -2.2013e+00,\n",
      "         -3.4042e-01, -5.4942e-01, -2.3600e-01, -1.4957e+00, -3.9568e-01,\n",
      "          3.8032e-01, -1.2628e+00, -5.7947e-01, -3.0660e-01,  1.2096e+00,\n",
      "          2.6991e+00, -3.0874e-01,  1.4444e+00, -1.4881e+00,  2.0968e+00,\n",
      "          4.9322e-01,  2.2176e+00, -1.7833e+00,  1.8839e+00,  4.7263e-01,\n",
      "         -1.1250e+00,  1.8908e+00,  6.8065e-01, -1.1801e+00,  3.2459e-01,\n",
      "          7.6122e-02,  7.9372e-02, -1.5290e-01,  1.9483e+00, -1.1500e-01,\n",
      "          2.9453e-01,  7.8471e-01,  1.3337e+00,  1.4303e+00,  2.7406e+00,\n",
      "          5.7153e-01, -2.7634e-01,  1.7624e+00,  2.5725e-01, -2.2329e+00,\n",
      "         -2.1888e-01, -2.3203e+00,  3.1098e+00, -1.6923e-01, -3.2978e-01,\n",
      "          6.8278e-01, -7.4393e-01, -1.4427e+00,  1.6879e+00, -1.6014e+00,\n",
      "         -3.7360e+00, -4.7950e-01, -1.7664e+00,  1.3118e+00, -5.6726e-01,\n",
      "          1.5030e+00,  3.3084e+00, -1.0599e+00, -1.3962e+00, -1.2981e+00,\n",
      "          2.2579e+00, -4.9229e-01,  1.8309e+00, -1.9065e+00, -1.4791e+00,\n",
      "          8.1300e-01,  9.3415e-01, -5.2093e-01, -1.8433e+00,  1.4265e+00,\n",
      "         -4.1582e-01,  3.2170e+00, -2.6674e+00, -1.0673e-01, -1.2714e+00,\n",
      "          5.8092e-01, -1.3989e+00,  4.2112e-01, -1.5246e+00,  1.1610e+00,\n",
      "          2.2908e+00,  1.0594e+00,  2.6969e-01, -7.1869e-01, -8.9951e-01,\n",
      "          1.5509e+00, -1.3034e+00,  1.4789e+00,  9.0344e-01,  1.6565e+00,\n",
      "         -2.0864e-01,  8.3394e-01, -3.3061e-01, -1.0458e+00, -6.2172e-01,\n",
      "          1.9149e-01, -2.5827e+00, -3.3938e+00, -1.4336e+00,  8.6271e-01,\n",
      "         -2.9192e-01, -1.6747e+00, -1.6500e+00, -4.4059e-02,  2.5917e-01,\n",
      "          1.2447e+00,  5.7744e-01,  1.7346e+00,  2.4549e+00, -2.1901e+00,\n",
      "          6.9811e-01, -3.0469e-01, -7.8563e-02,  1.5446e+00,  2.7629e+00,\n",
      "          1.0252e+00, -1.3028e+00, -2.0092e+00, -7.3426e-01,  1.9086e-01,\n",
      "         -1.6745e+00, -1.4322e+00, -8.9498e-01,  1.4268e+00, -5.0477e-01,\n",
      "          2.1767e+00, -3.6158e+00,  1.2784e+00, -7.1187e-02, -4.8697e-01,\n",
      "         -1.9088e+00,  8.1315e-01, -2.2029e-01,  8.1296e-01,  4.6821e-01,\n",
      "          9.3522e-01,  1.1393e+00,  5.2134e-01,  1.0435e+00,  4.7905e-01,\n",
      "         -1.3381e+00,  1.1178e+00,  2.7336e-01, -2.6955e+00, -5.5288e-01,\n",
      "         -1.1877e+00, -1.4858e+00,  2.9520e+00,  9.8130e-01, -2.9186e+00,\n",
      "         -3.2787e-01, -1.5786e+00, -3.2005e+00, -6.2856e-01, -4.3005e+00,\n",
      "          4.0263e-01,  1.1887e+00, -2.9987e-01,  5.8845e-01,  1.0715e+00,\n",
      "         -8.4485e-01, -2.5873e-02,  9.7451e-01, -5.7812e-01,  2.4621e+00,\n",
      "          1.9372e+00, -3.5044e-01, -1.0036e+00, -6.3512e-01, -2.0758e+00,\n",
      "          2.3379e-02,  7.5186e-02, -9.2373e-01, -1.1802e+00,  1.6543e+00,\n",
      "          7.4129e-01, -1.5639e+00, -9.8197e-01, -2.2138e+00, -4.8779e-01,\n",
      "          5.9923e-01, -1.5731e+00,  6.2771e-01,  6.9869e-01,  2.4210e+00,\n",
      "          1.0076e+00, -5.9613e-01,  1.1802e+00,  9.3243e-01, -2.5028e+00,\n",
      "         -8.9874e-02, -2.6221e+00, -3.5344e+00, -2.5881e+00,  4.3919e-01,\n",
      "         -5.7207e-01,  2.7829e+00, -1.1192e+00,  1.1609e+00, -2.8175e-01,\n",
      "          1.1480e+00, -1.8932e-01,  1.5810e+00, -1.1571e+00, -3.7559e-01,\n",
      "          7.3000e-02, -1.4276e+00,  1.2068e+00, -3.2719e+00,  9.0357e-01,\n",
      "          2.2374e+00, -2.2292e+00, -1.7050e-01, -7.9845e-01, -1.4693e-01,\n",
      "          2.9546e-01, -2.7518e-01,  1.4507e+00,  2.8816e-01,  3.5947e-01,\n",
      "         -5.0299e-01,  3.0598e+00,  9.4410e-01,  2.8979e-01, -1.0988e-01,\n",
      "          1.4499e+00, -4.5743e-01,  8.5185e-01,  8.0965e-02, -2.1382e-01,\n",
      "         -1.2765e+00, -9.2484e-02,  1.4956e+00, -1.8240e+00,  1.9133e+00,\n",
      "         -2.9188e+00,  1.2555e+00, -1.9953e+00, -1.9795e+00,  1.7279e+00,\n",
      "          6.4590e-01, -5.2771e-02,  2.4965e-01,  2.3964e-02,  5.9997e-02,\n",
      "          2.5659e+00,  3.6533e+00,  2.0301e+00]], grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Encode maximum function\n",
    "func = \"def f(a,b): if a>b: return a else return b\"\n",
    "tokens_ids = model.tokenize([func],max_length=512,mode=\"<encoder-only>\")\n",
    "source_ids = torch.tensor(tokens_ids).to(device)\n",
    "tokens_embeddings,max_func_embedding = model(source_ids)\n",
    "\n",
    "# Encode minimum function\n",
    "func = \"def f(a,b): if a<b: return a else return b\"\n",
    "tokens_ids = model.tokenize([func],max_length=512,mode=\"<encoder-only>\")\n",
    "source_ids = torch.tensor(tokens_ids).to(device)\n",
    "tokens_embeddings,min_func_embedding = model(source_ids)\n",
    "\n",
    "# Encode NL\n",
    "nl = \"return maximum value\"\n",
    "tokens_ids = model.tokenize([nl],max_length=512,mode=\"<encoder-only>\")\n",
    "source_ids = torch.tensor(tokens_ids).to(device)\n",
    "tokens_embeddings,nl_embedding = model(source_ids)\n",
    "\n",
    "print(max_func_embedding.shape)\n",
    "print(max_func_embedding)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Similarity between code and NL\n",
    "\n",
    "Now, we calculate cosine similarity between NL and two functions. Although the difference of two functions is only a operator (< and >), UniXcoder can distinguish them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 768])\n",
      "tensor([[0.3002]], grad_fn=<ViewBackward0>)\n",
      "tensor([[0.1881]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Normalize embedding\n",
    "norm_max_func_embedding = torch.nn.functional.normalize(max_func_embedding, p=2, dim=1)\n",
    "norm_min_func_embedding = torch.nn.functional.normalize(min_func_embedding, p=2, dim=1)\n",
    "norm_nl_embedding = torch.nn.functional.normalize(nl_embedding, p=2, dim=1)\n",
    "print(norm_max_func_embedding.shape)\n",
    "\n",
    "max_func_nl_similarity = torch.einsum(\"ac,bc->ab\",norm_max_func_embedding,norm_nl_embedding)\n",
    "min_func_nl_similarity = torch.einsum(\"ac,bc->ab\",norm_min_func_embedding,norm_nl_embedding)\n",
    "\n",
    "print(max_func_nl_similarity)\n",
    "print(min_func_nl_similarity)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Decoder-only Mode\n",
    "\n",
    "For decoder-only mode, we give an example of **code completion**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\"\"\n",
    "def f(data,file_path):\n",
    "    # write json data into file_path in python language\n",
    "\"\"\"\n",
    "tokens_ids = model.tokenize([context],max_length=512,mode=\"<decoder-only>\")\n",
    "source_ids = torch.tensor(tokens_ids).to(device)\n",
    "prediction_ids = model.generate(source_ids, decoder_only=True, beam_size=3, max_length=128)\n",
    "predictions = model.decode(prediction_ids)\n",
    "print(context+predictions[0][0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Encoder-Decoder Mode\n",
    "\n",
    "For encoder-decoder mode, we give two examples including: function name prediction, API recommendation, code summarization.\n",
    "\n",
    "## 1) Function Name Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\"\"\n",
    "def <mask0>(data,file_path):\n",
    "    data = json.dumps(data)\n",
    "    with open(file_path, 'w') as f:\n",
    "        f.write(data)\n",
    "\"\"\"\n",
    "tokens_ids = model.tokenize([context],max_length=512,mode=\"<encoder-decoder>\")\n",
    "source_ids = torch.tensor(tokens_ids).to(device)\n",
    "prediction_ids = model.generate(source_ids, decoder_only=False, beam_size=3, max_length=128)\n",
    "predictions = model.decode(prediction_ids)\n",
    "print([x.replace(\"<mask0>\",\"\").strip() for x in predictions[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slothPete7773's example\n",
    "context = \"\"\"\n",
    "def <mask0>(url, file_path):\n",
    "    data = request.urlopen(url)\n",
    "    data = data.read()\n",
    "    with open(file_path, 'w') as f:\n",
    "        f.write(data)\n",
    "\"\"\"\n",
    "tokens_ids = model.tokenize([context],max_length=512,mode=\"<encoder-decoder>\")\n",
    "source_ids = torch.tensor(tokens_ids).to(device)\n",
    "prediction_ids = model.generate(source_ids, decoder_only=False, beam_size=3, max_length=128)\n",
    "predictions = model.decode(prediction_ids)\n",
    "print([x.replace(\"<mask0>\",\"\").strip() for x in predictions[0]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) API Recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\"\"\n",
    "def write_json(data,file_path):\n",
    "    data = <mask0>(data)\n",
    "    with open(file_path, 'w') as f:\n",
    "        f.write(data)\n",
    "\"\"\"\n",
    "tokens_ids = model.tokenize([context],max_length=512,mode=\"<encoder-decoder>\")\n",
    "source_ids = torch.tensor(tokens_ids).to(device)\n",
    "prediction_ids = model.generate(source_ids, decoder_only=False, beam_size=3, max_length=128)\n",
    "predictions = model.decode(prediction_ids)\n",
    "print([x.replace(\"<mask0>\",\"\").strip() for x in predictions[0]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Code Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 5, 2, 317, 121, 19, 729, 2250, 181, 2317, 126, 636, 130, 824, 181, 902, 953, 317, 377, 869, 385, 3192, 132, 17284, 126, 636, 127, 317, 377, 918, 2717, 126, 824, 181, 902, 130, 464, 205, 784, 880, 412, 144, 317, 394, 412, 132, 1112, 126, 636, 127, 317, 2]]\n"
     ]
    }
   ],
   "source": [
    "context = \"\"\"\n",
    "# <mask0>\n",
    "def write_json(data,file_path):\n",
    "    data = json.dumps(data)\n",
    "    with open(file_path, 'w') as f:\n",
    "        f.write(data)\n",
    "\"\"\"\n",
    "tokens_ids = model.tokenize([context],max_length=512,mode=\"<encoder-decoder>\")\n",
    "print(tokens_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,     5,     2,   317,   121,    19,   729,  2250,   181,  2317,\n",
      "           126,   636,   130,   824,   181,   902,   953,   317,   377,   869,\n",
      "           385,  3192,   132, 17284,   126,   636,   127,   317,   377,   918,\n",
      "          2717,   126,   824,   181,   902,   130,   464,   205,   784,   880,\n",
      "           412,   144,   317,   394,   412,   132,  1112,   126,   636,   127,\n",
      "           317,     2]])\n"
     ]
    }
   ],
   "source": [
    "source_ids = torch.tensor(tokens_ids).to(device)\n",
    "print(source_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 128])\n",
      "tensor([[[  19, 3813, 3798,  508, 1012,    0,    0,    0,    0,    0,    0,\n",
      "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "             0,    0,    0,    0,    0,    0,    0],\n",
      "         [  19, 3813, 3192,  508, 1012,    0,    0,    0,    0,    0,    0,\n",
      "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "             0,    0,    0,    0,    0,    0,    0],\n",
      "         [  19, 3813,  434, 3192, 1012,    0,    0,    0,    0,    0,    0,\n",
      "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "             0,    0,    0,    0,    0,    0,    0]]])\n"
     ]
    }
   ],
   "source": [
    "prediction_ids = model.generate(source_ids, decoder_only=False, beam_size=3, max_length=128)\n",
    "print(prediction_ids.shape)\n",
    "print(prediction_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['<mask0> Write JSON to file', '<mask0> Write json to file', '<mask0> Write a json file']]\n"
     ]
    }
   ],
   "source": [
    "predictions = model.decode(prediction_ids)\n",
    "# print(predictions.shape)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Write JSON to file', 'Write json to file', 'Write a json file']\n"
     ]
    }
   ],
   "source": [
    "print([x.replace(\"<mask0>\",\"\").strip() for x in predictions[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9 (tags/v3.10.9:1dd9be6, Dec  6 2022, 20:01:21) [MSC v.1934 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bf5f69b3c1e18f509bc3e5784158a91a8d5962c26b685adcaa8318167674423f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
