{"grandmaster": {"correct_pairs_summarize": {"ml": 294, "es": 2512, "es-processed": 2132}, "incorrect_pairs_items": [{"originalMarkdown": "", "originalCode": "import numpy as np\nimport pandas as pd\n\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\nfrom sklearn.cross_validation import train_test_split\n\nimport matplotlib.pyplot as plt\n%matplotlib inline \n\nimport math\n\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\ndef rmsle(y, y_pred):\n    assert len(y) == len(y_pred)\n    to_sum = [(math.log(y_pred[i] + 1) - math.log(y[i] + 1)) ** 2.0 for i,pred in enumerate(y_pred)]\n    return (sum(to_sum) * (1.0/len(y))) ** 0.5\n#Source: https://www.kaggle.com/marknagelberg/rmsle-function\n#LOAD DATA\nprint(\"Loading data...\")\ntrain = pd.read_table(\"../input/train.tsv\")\ntest = pd.read_table(\"../input/test.tsv\")\nprint(train.shape)\nprint(test.shape)\n#HANDLE MISSING VALUES\nprint(\"Handling missing values...\")\ndef handle_missing(dataset):\n    dataset.category_name.fillna(value=\"missing\", inplace=True)\n    dataset.brand_name.fillna(value=\"missing\", inplace=True)\n    dataset.item_description.fillna(value=\"missing\", inplace=True)\n    return (dataset)\n\ntrain = handle_missing(train)\ntest = handle_missing(test)\nprint(train.shape)\nprint(test.shape)\ntrain.head(3)\n#PROCESS CATEGORICAL DATA\nprint(\"Handling categorical variables...\")\nle = LabelEncoder()\n\nle.fit(np.hstack([train.category_name, test.category_name]))\ntrain.category_name = le.transform(train.category_name)\ntest.category_name = le.transform(test.category_name)\n\nle.fit(np.hstack([train.brand_name, test.brand_name]))\ntrain.brand_name = le.transform(train.brand_name)\ntest.brand_name = le.transform(test.brand_name)\ndel le\n\ntrain.head(3)\n\n#PROCESS TEXT: RAW\nprint(\"Text to seq process...\")\nfrom keras.preprocessing.text import Tokenizer\nraw_text = np.hstack([train.item_description.str.lower(), train.name.str.lower()])\n\nprint(\"   Fitting tokenizer...\")\ntok_raw = Tokenizer()\ntok_raw.fit_on_texts(raw_text)\nprint(\"   Transforming text to seq...\")\n\ntrain[\"seq_item_description\"] = tok_raw.texts_to_sequences(train.item_description.str.lower())\ntest[\"seq_item_description\"] = tok_raw.texts_to_sequences(test.item_description.str.lower())\ntrain[\"seq_name\"] = tok_raw.texts_to_sequences(train.name.str.lower())\ntest[\"seq_name\"] = tok_raw.texts_to_sequences(test.name.str.lower())\ntrain.head(3)\n#SEQUENCES VARIABLES ANALYSIS\nmax_name_seq = np.max([np.max(train.seq_name.apply(lambda x: len(x))), np.max(test.seq_name.apply(lambda x: len(x)))])\nmax_seq_item_description = np.max([np.max(train.seq_item_description.apply(lambda x: len(x)))\n                                   , np.max(test.seq_item_description.apply(lambda x: len(x)))])\nprint(\"max name seq \"+str(max_name_seq))\nprint(\"max item desc seq \"+str(max_seq_item_description))\ntrain.seq_name.apply(lambda x: len(x)).hist()\ntrain.seq_item_description.apply(lambda x: len(x)).hist()\n#EMBEDDINGS MAX VALUE\n#Base on the histograms, we select the next lengths\nMAX_NAME_SEQ = 10\nMAX_ITEM_DESC_SEQ = 75\nMAX_TEXT = np.max([np.max(train.seq_name.max())\n                   , np.max(test.seq_name.max())\n                  , np.max(train.seq_item_description.max())\n                  , np.max(test.seq_item_description.max())])+2\nMAX_CATEGORY = np.max([train.category_name.max(), test.category_name.max()])+1\nMAX_BRAND = np.max([train.brand_name.max(), test.brand_name.max()])+1\nMAX_CONDITION = np.max([train.item_condition_id.max(), test.item_condition_id.max()])+1\n#SCALE target variable\ntrain[\"target\"] = np.log(train.price+1)\ntarget_scaler = MinMaxScaler(feature_range=(-1, 1))\ntrain[\"target\"] = target_scaler.fit_transform(train.target.reshape(-1,1))\npd.DataFrame(train.target).hist()\n#EXTRACT DEVELOPTMENT TEST\ndtrain, dvalid = train_test_split(train, random_state=123, train_size=0.99)\nprint(dtrain.shape)\nprint(dvalid.shape)\n#KERAS DATA DEFINITION\nfrom keras.preprocessing.sequence import pad_sequences\n\ndef get_keras_data(dataset):\n    X = {\n        'name': pad_sequences(dataset.seq_name, maxlen=MAX_NAME_SEQ)\n        ,'item_desc': pad_sequences(dataset.seq_item_description, maxlen=MAX_ITEM_DESC_SEQ)\n        ,'brand_name': np.array(dataset.brand_name)\n        ,'category_name': np.array(dataset.category_name)\n        ,'item_condition': np.array(dataset.item_condition_id)\n        ,'num_vars': np.array(dataset[[\"shipping\"]])\n    }\n    return X\n\nX_train = get_keras_data(dtrain)\nX_valid = get_keras_data(dvalid)\nX_test = get_keras_data(test)\n#KERAS MODEL DEFINITION\nfrom keras.layers import Input, Dropout, Dense, BatchNormalization, Activation, concatenate, GRU, Embedding, Flatten, BatchNormalization\nfrom keras.models import Model\nfrom keras.callbacks import ModelCheckpoint, Callback, EarlyStopping\nfrom keras import backend as K\n\ndef get_callbacks(filepath, patience=2):\n    es = EarlyStopping('val_loss', patience=patience, mode=\"min\")\n    msave = ModelCheckpoint(filepath, save_best_only=True)\n    return [es, msave]\n\ndef rmsle_cust(y_true, y_pred):\n    first_log = K.log(K.clip(y_pred, K.epsilon(), None) + 1.)\n    second_log = K.log(K.clip(y_true, K.epsilon(), None) + 1.)\n    return K.sqrt(K.mean(K.square(first_log - second_log), axis=-1))\n\ndef get_model():\n    #params\n    dr_r = 0.15\n    \n    #Inputs\n    name = Input(shape=[X_train[\"name\"].shape[1]], name=\"name\")\n    item_desc = Input(shape=[X_train[\"item_desc\"].shape[1]], name=\"item_desc\")\n    brand_name = Input(shape=[1], name=\"brand_name\")\n    category_name = Input(shape=[1], name=\"category_name\")\n    item_condition = Input(shape=[1], name=\"item_condition\")\n    num_vars = Input(shape=[X_train[\"num_vars\"].shape[1]], name=\"num_vars\")\n    \n    #Embeddings layers\n    emb_name = Embedding(MAX_TEXT, 50)(name)\n    emb_item_desc = Embedding(MAX_TEXT, 50)(item_desc)\n    emb_brand_name = Embedding(MAX_BRAND, 10)(brand_name)\n    emb_category_name = Embedding(MAX_CATEGORY, 10)(category_name)\n    emb_item_condition = Embedding(MAX_CONDITION, 5)(item_condition)\n    \n    #rnn layer\n    rnn_layer1 = GRU(16) (emb_item_desc)\n    rnn_layer2 = GRU(8) (emb_name)\n    \n    #main layer\n    main_l = concatenate([\n        Flatten() (emb_brand_name)\n        , Flatten() (emb_category_name)\n        , Flatten() (emb_item_condition)\n        , rnn_layer1\n        , rnn_layer2\n        , num_vars\n    ])\n    main_l = Dropout(dr_r) (Dense(128) (main_l))\n    main_l = Dropout(dr_r) (Dense(64) (main_l))\n    \n    #output\n    output = Dense(1, activation=\"linear\") (main_l)\n    \n    #model\n    model = Model([name, item_desc, brand_name\n                   , category_name, item_condition, num_vars], output)\n    model.compile(loss=\"mse\", optimizer=\"adam\", metrics=[\"mae\", rmsle_cust])\n    \n    return model\n\n    \nmodel = get_model()\nmodel.summary()\n    \n5\n#FITTING THE MODEL\nBATCH_SIZE = 10000\nepochs = 3\n\nmodel = get_model()\nmodel.fit(X_train, dtrain.target, epochs=epochs, batch_size=BATCH_SIZE\n          , validation_data=(X_valid, dvalid.target)\n          , verbose=1)\n#EVLUEATE THE MODEL ON DEV TEST: What is it doing?\nval_preds = model.predict(X_valid)\nval_preds = target_scaler.inverse_transform(val_preds)\nval_preds = np.exp(val_preds)+1\n\n#mean_absolute_error, mean_squared_log_error\ny_true = np.array(dvalid.price.values)\ny_pred = val_preds[:,0]\nv_rmsle = rmsle(y_true, y_pred)\nprint(\" RMSLE error on dev test: \"+str(v_rmsle))\n#CREATE PREDICTIONS\npreds = model.predict(X_test, batch_size=BATCH_SIZE)\npreds = target_scaler.inverse_transform(preds)\npreds = np.exp(preds)-1\n\nsubmission = test[[\"test_id\"]]\nsubmission[\"price\"] = preds\nsubmission.to_csv(\"./myNNsubmission.csv\", index=False)\nsubmission.price.hist()\n", "bm25Code": []}, {"originalMarkdown": "## Model training", "originalCode": "# Reference: https://www.kaggle.com/code/artgor/segmentation-in-pytorch-using-convenient-tools\n\nrunner.train(\n    model=model,\n    criterion=criterion,\n    optimizer=optimizer,\n    scheduler=scheduler,\n    loaders=loaders,\n    callbacks=[DiceCallback(), EarlyStoppingCallback(patience=5, min_delta=0.001)],\n    logdir=logdir,\n    num_epochs=num_epochs,\n    verbose=True\n)\nutils.plot_metrics(\n    logdir=logdir, \n    # specify which metrics we want to plot\n    metrics=[\"loss\", \"dice\", 'lr', '_base/lr']\n)", "bm25Code": "# Reference: https://www.kaggle.com/code/artgor/cactalyst\n\n# model runner\nrunner = SupervisedRunner()\n\n# model training\nrunner.train(\n    model=model,\n    criterion=criterion,\n    optimizer=optimizer,\n    scheduler=scheduler, \n    loaders=loaders,\n    logdir=logdir,\n    callbacks=[\n        OneCycleLR(\n            cycle_len=num_epochs, \n            div_factor=3,\n            increase_fraction=0.3,\n            momentum_range=(0.95, 0.85))\n    ],\n    num_epochs=num_epochs,\n    verbose=False\n)\n# plotting training progress\nUtilsFactory.plot_metrics(logdir=logdir)"}, {"originalMarkdown": "#### Bivariate KDE distribution plot", "originalCode": "# Reference: https://www.kaggle.com/code/tarunpaparaju/lanl-earthquake-prediction-fresh-eda\n\nplot = sns.jointplot(x=app_entropies, y=targets, kind='kde', color='magenta')\nplot.set_axis_labels('app_entropy', 'time_to_failure', fontsize=16)\nplt.show()", "bm25Code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/lanl-earthquake-prediction-fresh-eda\n\nplot = sns.jointplot(x=perm_entropies, y=targets, kind='kde', color='orangered')\nplot.set_axis_labels('perm_entropy', 'time_to_failure', fontsize=16)\nplt.show()"}, {"originalMarkdown": "#### Bivariate KDE distribution plot", "originalCode": "# Reference: https://www.kaggle.com/code/tarunpaparaju/lanl-earthquake-prediction-fresh-eda\n\nplot = sns.jointplot(x=higuchi_fds, y=targets, kind='kde', color='crimson')\nplot.set_axis_labels('higuchi_fd', 'time_to_failure', fontsize=16)\nplt.show()", "bm25Code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/lanl-earthquake-prediction-fresh-eda\n\nplot = sns.jointplot(x=perm_entropies, y=targets, kind='kde', color='orangered')\nplot.set_axis_labels('perm_entropy', 'time_to_failure', fontsize=16)\nplt.show()"}, {"originalMarkdown": "The KDE plot has highest density (darkness) around a line with negative slope.#### Bivariate hexplot", "originalCode": "# Reference: https://www.kaggle.com/code/tarunpaparaju/lanl-earthquake-prediction-fresh-eda\n\nplot = sns.jointplot(x=higuchi_fds, y=targets, kind='hex', color='crimson')\nplot.set_axis_labels('higuchi_fd', 'time_to_failure', fontsize=16)\nplt.show()", "bm25Code": "# Reference: https://www.kaggle.com/code/tarunpaparaju/lanl-earthquake-prediction-fresh-eda\n\nplot = sns.jointplot(x=app_entropies, y=targets, kind='hex', color='magenta')\nplot.set_axis_labels('app_entropy', 'time_to_failure', fontsize=16)\nplt.show()"}, {"originalMarkdown": "# Data", "originalCode": "# Reference: https://www.kaggle.com/code/iafoss/severstal-fast-ai-256x256-crops\n\nclass SegmentationLabelList(SegmentationLabelList):\n    def open(self, fn): return open_mask(fn, div=True)\n    \nclass SegmentationItemList(SegmentationItemList):\n    _label_cls = SegmentationLabelList\n\n# Setting transformations on masks to False on test set\ndef transform(self, tfms:Optional[Tuple[TfmList,TfmList]]=(None,None), **kwargs):\n    if not tfms: tfms=(None,None)\n    assert is_listy(tfms) and len(tfms) == 2\n    self.train.transform(tfms[0], **kwargs)\n    self.valid.transform(tfms[1], **kwargs)\n    kwargs['tfm_y'] = False # Test data has no labels\n    if self.test: self.test.transform(tfms[1], **kwargs)\n    return self\nfastai.data_block.ItemLists.transform = transform\n\ndef open_mask(fn:PathOrStr, div:bool=True, convert_mode:str='L', cls:type=ImageSegment,\n        after_open:Callable=None)->ImageSegment:\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\", UserWarning)\n        #generate empty mask if file doesn't exist\n        x = PIL.Image.open(fn).convert(convert_mode) \\\n          if Path(fn).exists() \\\n          else PIL.Image.fromarray(np.zeros((sz,sz)).astype(np.uint8))\n    if after_open: x = after_open(x)\n    x = pil2tensor(x,np.float32)\n    return cls(x)\ndf = pd.read_csv(HARD_NEGATIVE)\ndf['index'] = df.index\ndf.plot(x='index', y='pixels', kind = 'line');\nplt.yscale('log')\nstats = ([0.396,0.396,0.396], [0.179,0.179,0.179])\n#check https://www.kaggle.com/iafoss/256x256-images-with-defects for stats\n\n#the code below eliminates sharing patches of the same image across folds\nimg_p = set([p.stem[:-2] for p in Path(TRAIN).ls()])\n#select 12000 of the most difficult negative exaples\nneg = list(pd.read_csv(HARD_NEGATIVE).head(12000).fname)\nneg = [Path(TRAIN_N)/f for f in neg]\nimg_n = set([p.stem for p in neg])\nimg_set = img_p | img_n\nimg_p_list = sorted(img_p)\nimg_n_list = sorted(img_n)\nimg_list = img_p_list + img_n_list\nkf = KFold(n_splits=nfolds, shuffle=True, random_state=SEED)\n\ndef get_data(fold):\n    #split with making sure that crops of the same original image \n    #are not shared between folds, so additional training and validation \n    #could be done on full images later\n    valid_idx = list(kf.split(list(range(len(img_list)))))[fold][1]\n    valid = set([img_list[i] for i in valid_idx])\n    valid_idx = []\n    for i,p in enumerate(Path(TRAIN).ls() + neg):\n        if p.stem[:-2] in valid: valid_idx.append(i)\n            \n    # Create databunch\n    sl = SegmentationItemList.from_folder(TRAIN)\n    sl.items = np.array((list(sl.items) + neg))\n    data = (sl.split_by_idx(valid_idx)\n        .label_from_func(lambda x : str(x).replace('/images', '/masks'), classes=[0,1,2,3,4])\n        .transform(get_transforms(xtra_tfms=dihedral()), size=sz, tfm_y=True)\n        .databunch(path=Path('.'), bs=bs)\n        .normalize(stats))\n    return data\n\n# Display some images with masks\nget_data(0).show_batch()\n@dataclass\nclass CSVLogger(LearnerCallback):\n    def __init__(self, learn, filename= 'history'):\n        self.learn = learn\n        self.path = self.learn.path/f'{filename}.csv'\n        self.file = None\n\n    @property\n    def header(self):\n        return self.learn.recorder.names\n\n    def read_logged_file(self):\n        return pd.read_csv(self.path)\n\n    def on_train_begin(self, metrics_names: StrList, **kwargs: Any) -> None:\n        self.path.parent.mkdir(parents=True, exist_ok=True)\n        e = self.path.exists()\n        self.file = self.path.open('a')\n        if not e: self.file.write(','.join(self.header) + '\\n')\n\n    def on_epoch_end(self, epoch: int, smooth_loss: Tensor, last_metrics: MetricsList, **kwargs: Any) -> bool:\n        self.write_stats([epoch, smooth_loss] + last_metrics)\n\n    def on_train_end(self, **kwargs: Any) -> None:\n        self.file.flush()\n        self.file.close()\n\n    def write_stats(self, stats: TensorOrNumList) -> None:\n        stats = [str(stat) if isinstance(stat, int) else f'{stat:.6f}'\n                 for name, stat in zip(self.header, stats)]\n        str_stats = ','.join(stats)\n        self.file.write(str_stats + '\\n')", "bm25Code": "# Reference: https://www.kaggle.com/code/cdeotte/one-feature-model-0-930\n\nplt.figure(figsize=(20,5))\nres = 1000; let = ['A','B','C','D','E','F','G','H','I','J']\nplt.plot(range(0,test.shape[0],res),test.signal[0::res])\nfor i in range(5): plt.plot([i*500000,i*500000],[-5,12.5],'r')\nfor j in range(21): plt.plot([j*100000,j*100000],[-5,12.5],'r:')\nfor k in range(4): plt.text(k*500000+200000,10,str(k+1),size=20)\nfor k in range(10): plt.text(k*100000+40000,7,let[k],size=16)\nplt.xlabel('Row',size=16); plt.ylabel('Channels Open',size=16); \nplt.title('Test Data Signal - 4 batches - 10 subsamples',size=20)\nplt.show()"}, {"originalMarkdown": "## Exploration", "originalCode": "# Reference: https://www.kaggle.com/code/xhlulu/exploration-and-preprocessing-for-keras-224x224\n\nlabel_df = pd.read_csv('../input/train.csv')\nsubmission_df = pd.read_csv('../input/sample_submission.csv')\nlabel_df.head()\nlabel_df['Id'].describe()\n# Display the most frequent ID (without counting new_whale)\nlabel_df['Id'].value_counts()[1:16].plot(kind='bar')\ndef display_samples(df, columns=4, rows=3):\n    fig=plt.figure(figsize=(5*columns, 3*rows))\n\n    for i in range(columns*rows):\n        image_path = df.loc[i,'Image']\n        image_id = df.loc[i,'Id']\n        img = cv2.imread(f'../input/train/{image_path}')\n        fig.add_subplot(rows, columns, i+1)\n        plt.title(image_id)\n        plt.imshow(img)\n\ndisplay_samples(label_df)", "bm25Code": "# Reference: https://www.kaggle.com/code/xhlulu/reducing-image-sizes-to-32x32\n\nlabel_df = pd.read_csv('../input/train.csv')\nsubmission_df = pd.read_csv('../input/sample_submission.csv')\nlabel_df.head()\nlabel_df['category_id'].value_counts()[1:16].plot(kind='bar')\ndef display_samples(df, columns=4, rows=3):\n    fig=plt.figure(figsize=(5*columns, 3*rows))\n\n    for i in range(columns*rows):\n        image_path = df.loc[i,'file_name']\n        image_id = df.loc[i,'category_id']\n        img = cv2.imread(f'../input/train_images/{image_path}')\n        fig.add_subplot(rows, columns, i+1)\n        plt.title(image_id)\n        plt.imshow(img)\n\ndisplay_samples(label_df)"}, {"originalMarkdown": "", "originalCode": "import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport lightgbm as lgb\nfrom sklearn.model_selection import KFold\nfrom sklearn import model_selection, preprocessing, metrics\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport shap\nimport os\nprint(os.listdir(\"../input\"))\nfrom sklearn import preprocessing\nimport xgboost as xgb\nimport gc\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\n\n# Load data\ntrain = pd.read_csv('../input/cat-in-the-dat/train.csv')\ntest = pd.read_csv('../input/cat-in-the-dat/test.csv')\n\nprint(train.shape)\nprint(test.shape)\n%%time\n\n# Subset\ntarget = train['target']\ntrain_id = train['id']\ntest_id = test['id']\ntrain.drop(['target', 'id'], axis=1, inplace=True)\ntest.drop('id', axis=1, inplace=True)\n\nprint(train.shape)\nprint(test.shape)\nfeatures = test.columns\ntrain['target'] = 0\ntest['target'] = 1\ntrain_test = pd.concat([train, test], axis =0)\ntarget = train_test['target'].values\ndel train, test\ngc.collect()\n# Label Encoding\nfor f in features:\n    lbl = preprocessing.LabelEncoder()\n    lbl.fit(list(train_test[f].values) )\n    train_test[f] = lbl.transform(list(train_test[f].values))\ntrain, test = model_selection.train_test_split(train_test, test_size=0.33, random_state=42, shuffle=True)\ndel train_test\ngc.collect()\ntrain_y = train['target'].values\ntest_y = test['target'].values\ndel train['target'], test['target']\ngc.collect()\ntrain = lgb.Dataset(train, label=train_y)\ntest = lgb.Dataset(test, label=test_y)\nparam = {'num_leaves': 50,\n         'min_data_in_leaf': 30, \n         'objective':'binary',\n         'max_depth': 5,\n         'learning_rate': 0.001,\n         \"min_child_samples\": 20,\n         \"boosting\": \"gbdt\",\n         \"feature_fraction\": 0.9,\n         \"bagging_freq\": 1,\n         \"bagging_fraction\": 0.9 ,\n         \"bagging_seed\": 44,\n         \"metric\": 'auc',\n         \"verbosity\": -1}\nnum_round = 50000\nclf = lgb.train(param, train, num_round, valid_sets = [train, test], verbose_eval=50, early_stopping_rounds = 500)\n", "bm25Code": []}, {"originalMarkdown": "Let's see city_id, merchant_category_id,  state_id, subsector_id.", "originalCode": "# Reference: https://www.kaggle.com/code/gpreda/elo-eda-and-prediction\n\nplot_bar(get_categories(new_merchant_trans_df,'city_id'), \n             'City ID distribution', 'City ID', 'Number of records','brown')\nplot_bar(get_categories(new_merchant_trans_df,'merchant_category_id'), \n             'Merchant category ID distribution', 'Merchant category ID', 'Number of records','green')\nplot_bar(get_categories(new_merchant_trans_df,'state_id'), \n             'State ID distribution', 'State ID', 'Number of records','darkblue')\nplot_bar(get_categories(new_merchant_trans_df,'subsector_id'), \n             'Subsector ID distribution', 'Subsector ID', 'Number of records','darkgreen')", "bm25Code": "# Reference: https://www.kaggle.com/code/gpreda/elo-eda-and-prediction\n\nplot_bar(get_categories(historical_trans_df,'city_id'), \n             'City ID distribution', 'City ID', 'Number of records','lightblue')\nplot_bar(get_categories(historical_trans_df,'merchant_category_id'), \n             'Merchant Cateogory ID distribution', 'Merchant Category ID', 'Number of records','lightgreen')\nplot_bar(get_categories(historical_trans_df,'state_id'), \n             'State ID distribution', 'State ID', 'Number of records','brown')\nplot_bar(get_categories(historical_trans_df,'subsector_id'), \n             'Subsector ID distribution', 'Subsector ID', 'Number of records','orange')"}, {"originalMarkdown": "## Feature importance", "originalCode": "# Reference: https://www.kaggle.com/code/gpreda/elo-eda-and-prediction\n\n##plot the feature importance\nlogger.info(\"Feature importance plot\")\ncols = (feature_importance_df[[\"Feature\", \"importance\"]]\n        .groupby(\"Feature\")\n        .mean()\n        .sort_values(by=\"importance\", ascending=False)[:1000].index)\nbest_features = feature_importance_df.loc[feature_importance_df.Feature.isin(cols)]\nplt.figure(figsize=(14,26))\nsns.barplot(x=\"importance\", y=\"Feature\", data=best_features.sort_values(by=\"importance\",ascending=False))\nplt.title('LightGBM Features (averaged over folds)')\nplt.tight_layout()\nplt.savefig('lgbm_importances.png')", "bm25Code": "# Reference: https://www.kaggle.com/code/vbmokin/nfl-feature-importance-xgb-lgbm-linreg\n\nX = train\nz = target\n#%% split training set to validation set\nXtrain, Xval, Ztrain, Zval = train_test_split(X, z, test_size=0.2, random_state=0)\ntrain_set = lgbm.Dataset(Xtrain, Ztrain, silent=False)\nvalid_set = lgbm.Dataset(Xval, Zval, silent=False)\nparams = {\n        'boosting_type':'gbdt',\n        'objective': 'regression',\n        'num_leaves': 31,\n        'learning_rate': 0.05,\n        'max_depth': -1,\n        'subsample': 0.8,\n        'bagging_fraction' : 1,\n        'max_bin' : 5000 ,\n        'bagging_freq': 20,\n        'colsample_bytree': 0.6,\n        'metric': 'rmse',\n        'min_split_gain': 0.5,\n        'min_child_weight': 1,\n        'min_child_samples': 10,\n        'scale_pos_weight':1,\n        'zero_as_missing': True,\n        'seed':0,        \n    }\n\nmodelL = lgbm.train(params, train_set = train_set, num_boost_round=1000,\n                   early_stopping_rounds=50,verbose_eval=10, valid_sets=valid_set)\nfig =  plt.figure(figsize = (25,30))\naxes = fig.add_subplot(111)\nlgbm.plot_importance(modelL,ax = axes,height = 0.5)\nplt.show();plt.close()\nfeature_score = pd.DataFrame(train.columns, columns = ['feature']) \nfeature_score['score_lgb'] = modelL.feature_importance()"}]}, "master": {"correct_pairs_summarize": {"ml": 405, "es": 3572, "es-processed": 3007}, "incorrect_pairs_items": [{"originalMarkdown": "", "originalCode": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline", "bm25Code": []}, {"originalMarkdown": "", "originalCode": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.ensemble import RandomForestRegressor\n%matplotlib inline\nimport seaborn as sns", "bm25Code": []}, {"originalMarkdown": "", "originalCode": "import pandas as pd\nimport numpy as np\nfrom scipy.optimize import curve_fit\n\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\nimport matplotlib.pyplot as plt\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import log_loss\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom lightgbm import LGBMRegressor\n\n\nimport xgboost as xgb\n\nfrom tqdm.notebook import tqdm\n\ndef exponential(x, a, k, b):\n    return a*np.exp(x*k) + b\n\ndef funclog(x, a, b,c):\n    return a*np.log(b+x)+c\n\ndef rmse( yt, yp ):\n    return np.sqrt( np.mean( (yt-yp)**2 ) )\n\ndef pinball(y_true, y_pred, tao=0.5 ):\n    return np.max( [(y_true - y_pred)*tao, (y_pred - y_true)*(1 - tao) ], axis=0 ) \n\ndef calc_metric( df ):\n    tmp = df.copy()\n    tmp['m0'] = pinball( tmp['TargetValue'].values, tmp['q05'].values , 0.05 )\n    tmp['m1'] = pinball( tmp['TargetValue'].values, tmp['q50'].values , 0.50 )\n    tmp['m2'] = pinball( tmp['TargetValue'].values, tmp['q95'].values , 0.95 )\n    tmp['q'] = tmp['Weight']*(tmp['m0']+tmp['m1']+tmp['m2']) / 3\n    return tmp['q'].mean()\ntrain = pd.read_csv('../input/covid19-global-forecasting-week-5/train.csv')\n\ntrain['Date'] = pd.to_datetime( train['Date'] )\nmindate  = str(train['Date'].min())[:10]\nmaxdate  = str(train['Date'].max())[:10]\ntestdate = str( train['Date'].max() + pd.Timedelta(days=1) )[:10]\nprint( mindate, maxdate, testdate )\n\ntrain['County'] = train['County'].fillna('N')\ntrain['Province_State'] = train['Province_State'].fillna('N')\ntrain['Country_Region'] = train['Country_Region'].fillna('N')\ntrain['geo'] = train['Country_Region'] + '-' + train['Province_State'] + '-' + train['County']\n\nprint(train.shape)\ntrain['dedup'] = pd.factorize( train['geo'] + '-' + train['Target'] + '-' + train['Date'].apply(str) + '-' + train['Population'].apply(str) )[0]\ntrain.drop_duplicates(subset =\"dedup\", keep = 'first', inplace = True)\ndel train['dedup']\nprint(train.shape)\n\ntrain.sort_values( ['geo','Date'], inplace=True )\n\ntrain.head(5)\ntest = pd.read_csv('../input/covid19-global-forecasting-week-5/test.csv')\n\ntest['Date'] = pd.to_datetime( test['Date'] )\n#testdate = str( test['Date'].max() + pd.Timedelta(days=1) )[:10]\n#print( maxdate, testdate )\n\ntest['County'] = test['County'].fillna('N')\ntest['Province_State'] = test['Province_State'].fillna('N')\ntest['Country_Region'] = test['Country_Region'].fillna('N')\ntest['geo'] = test['Country_Region'] + '-' + test['Province_State'] + '-' + test['County']\n\nprint(test.shape)\ntest.sort_values( ['geo','Date'], inplace=True )\n\nprint(test.head())\ntrain = pd.concat( (train, test.loc[test.Date>=testdate]) , sort=False )\ntrain.sort_values( ['geo','Date'], inplace=True )\n\ntrain.loc[ (train.Date=='2020-04-24')&(train.geo=='Spain-N-N')&(train.Target=='ConfirmedCases'), 'TargetValue' ] = 6000\ntrain.loc[ (train.Date=='2020-04-29')&(train.geo=='France-N-N')&(train.Target=='ConfirmedCases'), 'TargetValue' ] = 1843.5\ntrain.loc[ (train.Date=='2020-04-22')&(train.geo=='France-N-N')&(train.Target=='ConfirmedCases'), 'TargetValue' ] = 2522\ntrain0 = train.loc[ train.Target == 'ConfirmedCases' ].copy()\ntrain1 = train.loc[ train.Target == 'Fatalities' ].copy()\nfor t in [train0,train1]:\n    t['q05'] = 0\n    t['q50'] = 0\n    t['q95'] = 0\n\ntrain0 = train0.loc[ (train0.Date  >='2020-03-01') ].copy()\ntrain1 = train1.loc[ (train1.Date  >='2020-03-01') ].copy()\n    \ntest0 = test.loc[ test.Target == 'ConfirmedCases' ].copy()\ntest1 = test.loc[ test.Target == 'Fatalities' ].copy()\ntrain0.shape, test0.shape\n!ls -l ../input/covid-w5-worldometer-scraper/train_oldformat.csv\nDF = pd.read_csv('../input/covid-w5-worldometer-scraper/train_oldformat.csv')\nDF['Date'] = pd.to_datetime( DF['Date'] )\n#testdate = str( test['Date'].max() + pd.Timedelta(days=1) )[:10]\n#print( maxdate, testdate )\n\nDF['County'] = DF['County'].fillna('N')\nDF['Province_State'] = DF['Province_State'].fillna('N')\nDF['Country_Region'] = DF['Country_Region'].fillna('N')\nDF['geo'] = DF['Country_Region'] + '-' + DF['Province_State'] + '-' + DF['County']\nDF\nDF0 = train0.loc[ train0.Date == '2020-05-11' ].copy()\nDF0['ypred'] = DF0[['geo','Date']].merge( DF.loc[DF.Target=='ConfirmedCases'], on=['geo','Date'] , how='left' )['TargetValue'].values\n\nDF1 = train1.loc[ train1.Date == '2020-05-11' ].copy()\nDF1['ypred'] = DF1[['geo','Date']].merge( DF.loc[DF.Target=='Fatalities'], on=['geo','Date'] , how='left' )['TargetValue'].values\n\n\ntrain0['ypred'] = train0['TargetValue'].values\ntrain0['mpred'] = train0.groupby('geo')['TargetValue'].rolling(7).mean().values\ntrain0['Hstd']  = np.clip(train0['ypred'] - train0['mpred'], 0, 9999999999)\ntrain0['Lstd']  = np.clip(train0['ypred'] - train0['mpred'], -9999999999, 0)\n\ntrain0.loc[ train0.Date>='2020-04-27' ,'ypred'] = np.nan\ntrain0.loc[ train0.Date>='2020-04-27' ,'mpred'] = np.nan\ntrain0.loc[ train0.Date>='2020-04-27' ,'Hstd']  = np.nan\ntrain0.loc[ train0.Date>='2020-04-27' ,'Lstd']  = np.nan\n\ntrain0['Hstd']  = train0.groupby('geo')['Hstd'].rolling(28).std().values\ntrain0['Lstd']  = train0.groupby('geo')['Lstd'].rolling(28).std().values\n\ntrain0['Lstd']  = train0.groupby('geo')['Lstd'].fillna( method='ffill' )\ntrain0['Hstd']  = train0.groupby('geo')['Hstd'].fillna( method='ffill' )\ntrain0['ypred'] = train0.groupby('geo')['ypred'].fillna( method='ffill' )\ntrain0['mpred'] = train0.groupby('geo')['mpred'].fillna( method='ffill' )\n\ntrain0['q50'] = train0['TargetValue'].values\ntrain0.loc[ train0.Date>='2020-04-27' ,'q50']  = np.nan\ntrain0['q05'] = train0['q50']\ntrain0['q95'] = train0['q50']\nimport statsmodels.api as sm\n\ncount = 1\nfor valday in [\n    '2020-04-27',\n    '2020-04-28',\n    '2020-04-29',\n    '2020-04-30',\n    '2020-05-01',\n    '2020-05-02',\n    '2020-05-03',\n    '2020-05-04',\n    '2020-05-05',\n    '2020-05-06',\n    '2020-05-07',\n    '2020-05-08',\n    '2020-05-09',\n    '2020-05-10',\n    ]:\n    \n    for i in np.arange(1,13,1):\n        train0['lag1'+str(i)] = train0.groupby('geo')['q50'].shift(i)\n    train0['std1']= train0.groupby('geo')['q50'].shift(1).rolling(7).std()\n    train0['std2']= train0.groupby('geo')['q50'].shift(1).rolling(14).std()\n    train0['std2']= train0.groupby('geo')['q50'].shift(1).rolling(21).std()\n    train0['std2']= train0.groupby('geo')['q50'].shift(1).rolling(28).std()\n    \n    \n    TRAIN = train0.loc[ (train0.Date  <'2020-04-27')&(train0.Date >='2020-04-01') ].copy()\n    VALID = train0.loc[ (train0.Date ==valday) ].copy()\n    \n    features = TRAIN.columns[9:]\n    features = [f for f in features if f not in ['geo','ForecastId','q05','q50','q95','ypred','qstd','Lstd','Hstd','mpred','flag']  ]\n\n    if valday == '2020-04-27':        \n        model05 = sm.QuantReg(TRAIN['q50'], TRAIN[features]).fit(q=0.05)\n        model50 = sm.QuantReg(TRAIN['q50'], TRAIN[features]).fit(q=0.50)\n        model95 = sm.QuantReg(TRAIN['q50'], TRAIN[features]).fit(q=0.95)\n        \n    #break\n    VALID['q05'] = model05.predict( VALID[features] ) - VALID['Lstd']*np.clip(0.25*count,0,3.5)\n    VALID['q50'] = model50.predict( VALID[features] )\n    VALID['q95'] = model95.predict( VALID[features] ) + VALID['Hstd']*np.clip(0.25*count,0,3.5)\n    \n    VALID.loc[ VALID.q50<0  ,'q50'] = 0\n    VALID.loc[ VALID.q50<VALID.q05 ,'q05'] = VALID.loc[ VALID.q50<VALID.q05 ,'q50']\n    VALID.loc[ VALID.q50>VALID.q95 ,'q95'] = VALID.loc[ VALID.q50>VALID.q95 ,'q50']\n\n    VALID['q05'] = VALID['q05']/(1.02**count)\n    VALID['q50'] = VALID['q50']\n    VALID['q95'] = VALID['q95']*(1.02**count)\n    \n    VALID.loc[ VALID.q05<0  ,'q05'] = 0\n    VALID.loc[ VALID.q50<0  ,'q50'] = 0\n    VALID.loc[ VALID.q95<0  ,'q95'] = 0\n    \n    train0.loc[ (train0.Date ==valday),'q05'] = VALID['q05']\n    train0.loc[ (train0.Date ==valday),'q50'] = VALID['q50']\n    train0.loc[ (train0.Date ==valday),'q95'] = VALID['q95']\n   \n    print( calc_metric( VALID ), valday )\n    count+=1\n\nTMP0 = train0.loc[ (train0.Date>='2020-04-27')&(train0.Date<='2020-05-10') ].copy()\nprint( calc_metric( TMP0 ) )  \n\ntrain0['ypred'] = train0['TargetValue'].values\ntrain0['mpred'] = train0.groupby('geo')['TargetValue'].rolling(7).mean().values\ntrain0['Hstd']  = np.clip(train0['ypred'] - train0['mpred'], 0, 9999999999)\ntrain0['Lstd']  = np.clip(train0['ypred'] - train0['mpred'], -9999999999, 0)\n\n# train0.loc[ train0.Date>='2020-04-27' ,'ypred'] = np.nan\n# train0.loc[ train0.Date>='2020-04-27' ,'mpred'] = np.nan\n# train0.loc[ train0.Date>='2020-04-27' ,'Hstd']  = np.nan\n# train0.loc[ train0.Date>='2020-04-27' ,'Lstd']  = np.nan\n\ntrain0['Hstd']  = train0.groupby('geo')['Hstd'].rolling(28).std().values\ntrain0['Lstd']  = train0.groupby('geo')['Lstd'].rolling(28).std().values\n\ntrain0['Lstd']  = train0.groupby('geo')['Lstd'].fillna( method='ffill' )\ntrain0['Hstd']  = train0.groupby('geo')['Hstd'].fillna( method='ffill' )\ntrain0['ypred'] = train0.groupby('geo')['ypred'].fillna( method='ffill' )\ntrain0['mpred'] = train0.groupby('geo')['mpred'].fillna( method='ffill' )\n\ntrain0['q50'] = train0['TargetValue'].values\n#train0.loc[ train0.Date>='2020-04-27' ,'q50']  = np.nan\ntrain0['q05'] = train0['q50']\ntrain0['q95'] = train0['q50']\n\n\ncount = 1\nfor valday in [\n    '2020-05-11',\n    '2020-05-12',\n    '2020-05-13',\n    '2020-05-14',\n    '2020-05-15',\n    '2020-05-16',\n    '2020-05-17',\n    '2020-05-18',\n    '2020-05-19',\n    '2020-05-20',\n    '2020-05-21',\n    '2020-05-22',\n    '2020-05-23',\n    '2020-05-24',\n    '2020-05-25',\n    '2020-05-26',\n    '2020-05-27',\n    '2020-05-28',\n    '2020-05-29',\n    '2020-05-30',\n    '2020-05-31',\n    '2020-06-01',\n    '2020-06-02',\n    '2020-06-03',\n    '2020-06-04',\n    '2020-06-05',\n    '2020-06-06',\n    '2020-06-07',\n    '2020-06-08',\n    '2020-06-09',\n    '2020-06-10',\n]:\n    for i in np.arange(1,13,1):\n        train0['lag1'+str(i)] = train0.groupby('geo')['q50'].shift(i)#.fillna(0)\n    train0['std1']= train0.groupby('geo')['q50'].shift(1).rolling(7).std()\n    train0['std2']= train0.groupby('geo')['q50'].shift(1).rolling(14).std()\n    train0['std2']= train0.groupby('geo')['q50'].shift(1).rolling(21).std()\n    train0['std2']= train0.groupby('geo')['q50'].shift(1).rolling(28).std()\n    TRAIN = train0.loc[ (train0.Date  <'2020-05-11')&(train0.Date >='2020-04-01') ].copy()\n    VALID = train0.loc[ (train0.Date ==valday) ].copy()\n    \n    features = TRAIN.columns[9:]\n    features = [f for f in features if f not in ['geo','ForecastId','q05','q50','q95','ypred','qstd','Lstd','Hstd','mpred','flag']  ]\n\n    if valday == '2020-05-11':        \n        model05 = sm.QuantReg(TRAIN['q50'], TRAIN[features]).fit(q=0.05)\n        model50 = sm.QuantReg(TRAIN['q50'], TRAIN[features]).fit(q=0.50)\n        model95 = sm.QuantReg(TRAIN['q50'], TRAIN[features]).fit(q=0.95)\n        \n    #break\n    VALID['q05'] = model05.predict( VALID[features] ) - VALID['Lstd']*np.clip(0.25*count,0,3.5)\n    VALID['q50'] = model50.predict( VALID[features] )\n    VALID['q95'] = model95.predict( VALID[features] ) + VALID['Hstd']*np.clip(0.25*count,0,3.5)\n    \n    VALID.loc[ VALID.q50<0  ,'q50'] = 0\n    VALID.loc[ VALID.q50<VALID.q05 ,'q05'] = VALID.loc[ VALID.q50<VALID.q05 ,'q50']\n    VALID.loc[ VALID.q50>VALID.q95 ,'q95'] = VALID.loc[ VALID.q50>VALID.q95 ,'q50']\n\n    VALID['q05'] = VALID['q05']/(1.02**count)\n    VALID['q50'] = VALID['q50']\n    VALID['q95'] = VALID['q95']*(1.02**count)\n    \n    VALID.loc[ VALID.q05<0  ,'q05'] = 0\n    VALID.loc[ VALID.q50<0  ,'q50'] = 0\n    VALID.loc[ VALID.q95<0  ,'q95'] = 0\n    \n    train0.loc[ (train0.Date ==valday),'q05'] = VALID['q05']\n    train0.loc[ (train0.Date ==valday),'q50'] = VALID['q50']\n    train0.loc[ (train0.Date ==valday),'q95'] = VALID['q95']\n   \n    print( calc_metric( VALID ), valday )\n    count+=1\nTMP0B = train0.loc[ (train0.Date>='2020-05-11') ].copy()\nTMP0B.shape\n\ntrain1['ypred'] = train1['TargetValue'].values\ntrain1['mpred'] = train1.groupby('geo')['TargetValue'].rolling(7).mean().values\ntrain1['Hstd']  = np.clip(train1['ypred'] - train1['mpred'], 0, 9999999999)\ntrain1['Lstd']  = np.clip(train1['ypred'] - train1['mpred'], -9999999999, 0)\n\ntrain1.loc[ train1.Date>='2020-04-27' ,'ypred'] = np.nan\ntrain1.loc[ train1.Date>='2020-04-27' ,'mpred'] = np.nan\ntrain1.loc[ train1.Date>='2020-04-27' ,'Hstd']  = np.nan\ntrain1.loc[ train1.Date>='2020-04-27' ,'Lstd']  = np.nan\n\ntrain1['Hstd']  = train1.groupby('geo')['Hstd'].rolling(28).std().values\ntrain1['Lstd']  = train1.groupby('geo')['Lstd'].rolling(28).std().values\n\ntrain1['Lstd']  = train1.groupby('geo')['Lstd'].fillna( method='ffill' )\ntrain1['Hstd']  = train1.groupby('geo')['Hstd'].fillna( method='ffill' )\ntrain1['ypred'] = train1.groupby('geo')['ypred'].fillna( method='ffill' )\ntrain1['mpred'] = train1.groupby('geo')['mpred'].fillna( method='ffill' )\n\ntrain1['q50'] = train1['TargetValue'].values\ntrain1.loc[ train1.Date>='2020-04-27' ,'q50']  = np.nan\ntrain1['q05'] = train1['q50']\ntrain1['q95'] = train1['q50']\ntrain1['q50_cases'] = train1[['geo','Date']].merge( train0[['geo','Date','q50']], on=['geo','Date'], how='left' )['q50'].values\ntrain1.iloc[-60:,5:25]\ncount = 1\nfor valday in [\n    '2020-04-27',\n    '2020-04-28',\n    '2020-04-29',\n    '2020-04-30',\n    '2020-05-01',\n    '2020-05-02',\n    '2020-05-03',\n    '2020-05-04',\n    '2020-05-05',\n    '2020-05-06',\n    '2020-05-07',\n    '2020-05-08',\n    '2020-05-09',\n    '2020-05-10',\n    ]:\n    \n    for i in np.arange(1,13,1):\n        train1['lag1'+str(i)] = train1.groupby('geo')['q50'].shift(i)\n        train1['lagCases1'+str(i)] = train1.groupby('geo')['q50_cases'].shift(i)\n    train1['std1']= train1.groupby('geo')['q50'].shift(1).rolling(7).std()\n    train1['std2']= train1.groupby('geo')['q50'].shift(1).rolling(14).std()\n    train1['std2']= train1.groupby('geo')['q50'].shift(1).rolling(21).std()\n    train1['std2']= train1.groupby('geo')['q50'].shift(1).rolling(28).std()\n    \n    TRAIN = train1.loc[ (train1.Date  <'2020-04-27')&(train1.Date >='2020-04-01') ].copy()\n    VALID = train1.loc[ (train1.Date ==valday) ].copy()\n    \n    features = TRAIN.columns[9:]\n    features = [f for f in features if f not in ['geo','ForecastId','q05','q50','q95','ypred','qstd','Lstd','Hstd','mpred','flag']  ]\n\n    if valday == '2020-04-27':        \n        model05 = sm.QuantReg(TRAIN['q50'], TRAIN[features]).fit(q=0.05)\n        model50 = sm.QuantReg(TRAIN['q50'], TRAIN[features]).fit(q=0.50)\n        model95 = sm.QuantReg(TRAIN['q50'], TRAIN[features]).fit(q=0.95)\n        \n    #break\n    VALID['q05'] = model05.predict( VALID[features] ) - VALID['Lstd']*np.clip(0.01*count,0,3.5)\n    VALID['q50'] = model50.predict( VALID[features] )\n    VALID['q95'] = model95.predict( VALID[features] ) + VALID['Hstd']*np.clip(0.01*count,0,3.5)\n    \n    VALID.loc[ VALID.q50<0  ,'q50'] = 0\n    VALID.loc[ VALID.q50<VALID.q05 ,'q05'] = VALID.loc[ VALID.q50<VALID.q05 ,'q50']\n    VALID.loc[ VALID.q50>VALID.q95 ,'q95'] = VALID.loc[ VALID.q50>VALID.q95 ,'q50']\n\n    VALID['q05'] = VALID['q05']/(1.001**count)\n    VALID['q50'] = VALID['q50']\n    VALID['q95'] = VALID['q95']*(1.001**count)\n    \n    VALID.loc[ VALID.q05<0  ,'q05'] = 0\n    VALID.loc[ VALID.q50<0  ,'q50'] = 0\n    VALID.loc[ VALID.q95<0  ,'q95'] = 0\n    \n    train1.loc[ (train1.Date ==valday),'q05'] = VALID['q05']\n    train1.loc[ (train1.Date ==valday),'q50'] = VALID['q50']\n    train1.loc[ (train1.Date ==valday),'q95'] = VALID['q95']\n   \n    print( calc_metric( VALID ), valday )\n    count+=1\n\nTMP1 = train1.loc[ (train1.Date>='2020-04-27')&(train1.Date<='2020-05-10') ].copy()\nprint( calc_metric( TMP1 ) )\ntrain1.iloc[-60:,5:25]\ntrain1['ypred'] = train1['TargetValue'].values\ntrain1['mpred'] = train1.groupby('geo')['TargetValue'].rolling(7).mean().values\ntrain1['Hstd']  = np.clip(train1['ypred'] - train1['mpred'], 0, 9999999999)\ntrain1['Lstd']  = np.clip(train1['ypred'] - train1['mpred'], -9999999999, 0)\n\n# train1.loc[ train1.Date>='2020-04-27' ,'ypred'] = np.nan\n# train1.loc[ train1.Date>='2020-04-27' ,'mpred'] = np.nan\n# train1.loc[ train1.Date>='2020-04-27' ,'Hstd']  = np.nan\n# train1.loc[ train1.Date>='2020-04-27' ,'Lstd']  = np.nan\n\ntrain1['Hstd']  = train1.groupby('geo')['Hstd'].rolling(28).std().values\ntrain1['Lstd']  = train1.groupby('geo')['Lstd'].rolling(28).std().values\n\ntrain1['Lstd']  = train1.groupby('geo')['Lstd'].fillna( method='ffill' )\ntrain1['Hstd']  = train1.groupby('geo')['Hstd'].fillna( method='ffill' )\ntrain1['ypred'] = train1.groupby('geo')['ypred'].fillna( method='ffill' )\ntrain1['mpred'] = train1.groupby('geo')['mpred'].fillna( method='ffill' )\n\ntrain1['q50'] = train1['TargetValue'].values\n#train1.loc[ train1.Date>='2020-04-27' ,'q50']  = np.nan\ntrain1['q05'] = train1['q50']\ntrain1['q95'] = train1['q50']\n\ncount = 1\nfor valday in [\n    '2020-05-11',\n    '2020-05-12',\n    '2020-05-13',\n    '2020-05-14',\n    '2020-05-15',\n    '2020-05-16',\n    '2020-05-17',\n    '2020-05-18',\n    '2020-05-19',\n    '2020-05-20',\n    '2020-05-21',\n    '2020-05-22',\n    '2020-05-23',\n    '2020-05-24',\n    '2020-05-25',\n    '2020-05-26',\n    '2020-05-27',\n    '2020-05-28',\n    '2020-05-29',\n    '2020-05-30',\n    '2020-05-31',\n    '2020-06-01',\n    '2020-06-02',\n    '2020-06-03',\n    '2020-06-04',\n    '2020-06-05',\n    '2020-06-06',\n    '2020-06-07',\n    '2020-06-08',\n    '2020-06-09',\n    '2020-06-10',\n    ]:\n    \n    for i in np.arange(1,13,1):\n        train1['lag1'+str(i)] = train1.groupby('geo')['q50'].shift(i)\n        train1['lagCases1'+str(i)] = train1.groupby('geo')['q50_cases'].shift(i)\n    train1['std1']= train1.groupby('geo')['q50'].shift(1).rolling(7).std()\n    train1['std2']= train1.groupby('geo')['q50'].shift(1).rolling(14).std()\n    train1['std2']= train1.groupby('geo')['q50'].shift(1).rolling(21).std()\n    train1['std2']= train1.groupby('geo')['q50'].shift(1).rolling(28).std()\n    \n    TRAIN = train1.loc[ (train1.Date  <'2020-05-11')&(train1.Date >='2020-04-01') ].copy()\n    VALID = train1.loc[ (train1.Date ==valday) ].copy()\n    \n    features = TRAIN.columns[9:]\n    features = [f for f in features if f not in ['geo','ForecastId','q05','q50','q95','ypred','qstd','Lstd','Hstd','mpred','flag']  ]\n\n    if valday == '2020-05-11':        \n        model05 = sm.QuantReg(TRAIN['q50'], TRAIN[features]).fit(q=0.05)\n        model50 = sm.QuantReg(TRAIN['q50'], TRAIN[features]).fit(q=0.50)\n        model95 = sm.QuantReg(TRAIN['q50'], TRAIN[features]).fit(q=0.95)\n        \n    #break\n    VALID['q05'] = model05.predict( VALID[features] ) - VALID['Lstd']*np.clip(0.01*count,0,3.5)\n    VALID['q50'] = model50.predict( VALID[features] )\n    VALID['q95'] = model95.predict( VALID[features] ) + VALID['Hstd']*np.clip(0.01*count,0,3.5)\n    \n    VALID.loc[ VALID.q50<0  ,'q50'] = 0\n    VALID.loc[ VALID.q50<VALID.q05 ,'q05'] = VALID.loc[ VALID.q50<VALID.q05 ,'q50']\n    VALID.loc[ VALID.q50>VALID.q95 ,'q95'] = VALID.loc[ VALID.q50>VALID.q95 ,'q50']\n\n    VALID['q05'] = VALID['q05']/(1.001**count)\n    VALID['q50'] = VALID['q50']\n    VALID['q95'] = VALID['q95']*(1.001**count)\n    \n    VALID.loc[ VALID.q05<0  ,'q05'] = 0\n    VALID.loc[ VALID.q50<0  ,'q50'] = 0\n    VALID.loc[ VALID.q95<0  ,'q95'] = 0\n    \n    train1.loc[ (train1.Date ==valday),'q05'] = VALID['q05']\n    train1.loc[ (train1.Date ==valday),'q50'] = VALID['q50']\n    train1.loc[ (train1.Date ==valday),'q95'] = VALID['q95']\n   \n    print( calc_metric( VALID ), valday )\n    count+=1\n    \nTMP1B = train1.loc[ (train1.Date>='2020-05-11') ].copy()\nTMP1B.shape    ", "bm25Code": []}, {"originalMarkdown": "**Boxplot**", "originalCode": "# Reference: https://www.kaggle.com/code/jsaguiar/complete-eda-time-series-with-plotly\n\nitem_sum = df.groupby(['item', 'date'])['sales'].sum()\ntraces = []\n\nfor i in range(1, 51):\n    s = item_sum[i].to_frame().reset_index()\n    trace = go.Box(y= s.sales, name= 'Item {}'.format(i), jitter=0.8, whiskerwidth=0.2, marker=dict(size=2), line=dict(width=1))\n    traces.append(trace)\n\nlayout = go.Layout(\n    title='Sales BoxPlot for each item',\n    yaxis=dict(\n        autorange=True, showgrid=True, zeroline=True,\n        gridcolor='rgb(233,233,233)', zerolinecolor='rgb(255, 255, 255)',\n        zerolinewidth=2, gridwidth=1\n    ),\n    margin=dict(l=40, r=30, b=80, t=100), showlegend=False,\n)\n\nfig = go.Figure(data=traces, layout=layout)\niplot(fig)", "bm25Code": "# Reference: https://www.kaggle.com/code/jsaguiar/complete-eda-time-series-with-plotly\n\nstore_sum = df.groupby(['store', 'date'])['sales'].sum()\ntraces = []\n\nfor i in range(1, 11):\n    s = store_sum[i].to_frame().reset_index()\n    trace = go.Box(y= s.sales, name= 'Store {}'.format(i), jitter=0.8, whiskerwidth=0.2, marker=dict(size=2), line=dict(width=1))\n    traces.append(trace)\n\nlayout = go.Layout(\n    title='Sales BoxPlot for each store',\n    yaxis=dict(\n        autorange=True, showgrid=True, zeroline=True,\n        gridcolor='rgb(233,233,233)', zerolinecolor='rgb(255, 255, 255)',\n        zerolinewidth=2, gridwidth=1\n    ),\n    margin=dict(l=40, r=30, b=80, t=100), showlegend=False,\n)\n\nfig = go.Figure(data=traces, layout=layout)\niplot(fig)"}, {"originalMarkdown": "## Training", "originalCode": "# Reference: https://www.kaggle.com/code/hmendonca/aptos19-regressor-fastai-oversampling-tta\n\nlearn.lr_find(end_lr=0.5)\nlearn.recorder.plot(suggestion=True)\n# train head first\nlearn.fit_one_cycle(2, max_lr=5e-3, div_factor=15)\nlearn.save('stage-1')\nlearn.recorder.plot_losses()\n# unfreeze and search appropriate learning rate for full training\nlearn.unfreeze()\nlearn.lr_find(start_lr=1e-10, wd=1e-3)\nlearn.recorder.plot(suggestion=True)\n# train all layers\nlearn.fit_one_cycle(6, max_lr=slice(1e-5, 1e-3), div_factor=10, wd=1e-3)\nlearn.save('stage-2')\nlearn.recorder.plot_losses()\n# schedule of the lr (left) and momentum (right) that the 1cycle policy uses\nlearn.recorder.plot_lr(show_moms=True)\n# kappa scores\nlearn.recorder.plot_metrics()\n# reload best model so far and look for a new learning rate\nlearn.load('bestmodel')\nlearn.lr_find(start_lr=1e-10)\nlearn.recorder.plot(suggestion=True)\n# train all layers, now with some weight decay\nlearn.fit_one_cycle(12, max_lr=slice(2e-6, 2e-4), div_factor=20)\nlearn.save('stage-3')\nlearn.recorder.plot_losses()\n# # schedule of the lr (left) and momentum (right) that the 1cycle policy uses\n# learn.recorder.plot_lr(show_moms=True)", "bm25Code": "# Reference: https://www.kaggle.com/code/kashnitsky/amazon-product-reviews-classification-with-ulmfit\n\n%%time\nlearn = language_model_learner(data_lm, drop_mult=0.3, arch=AWD_LSTM)\n%%time\nlearn.lr_find(start_lr = slice(10e-7, 10e-5), end_lr=slice(0.1, 10))\nlearn.recorder.plot(skip_end=10, suggestion=True)\nbest_lm_lr = 3e-3 #learn.recorder.min_grad_lr\n# best_lm_lr\n%%time\nlearn.fit_one_cycle(1, best_lm_lr)\nlearn.unfreeze()\n%%time\nlearn.fit_one_cycle(5, best_lm_lr)"}, {"originalMarkdown": "", "originalCode": "from PIL import Image, ImageDraw, ImageFont\nfrom os import listdir\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline", "bm25Code": []}, {"originalMarkdown": "# See sample image", "originalCode": "# Reference: https://www.kaggle.com/code/bulentsiyah/dogs-vs-cats-classification-vgg16-fine-tuning\n\nsample = random.choice(filenames)\nimage = load_img(\"../input/train/train/\"+sample)\nplt.imshow(image)", "bm25Code": "# Reference: https://www.kaggle.com/code/kmader/qbi-illumination-correction\n\nnp.random.seed(2019)\nxx = np.stack([np.arange(5)]*5, -1)\nyy = xx.T\nbins_sample_8bit = np.linspace(0, 255, 8)\nsample_img = (25*(xx+yy)+np.random.uniform(-10, 10, size=(5, 5))).astype('uint8')\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(6,3))\nsns.heatmap(sample_img, annot=True,fmt='02d', ax=ax1, cmap='viridis')\nax2.hist(sample_img.ravel(), bins_sample_8bit, label='Original', alpha=1)"}, {"originalMarkdown": "", "originalCode": "# Reference: https://www.kaggle.com/code/bulentsiyah/dogs-vs-cats-classification-vgg16-fine-tuning\n\nloss, accuracy = model.evaluate_generator(validation_generator, total_validate//batch_size, workers=12)\nprint(\"Test: accuracy = %f  ;  loss = %f \" % (accuracy, loss))\ndef plot_model_history(model_history, acc='acc', val_acc='val_acc'):\n    fig, axs = plt.subplots(1,2,figsize=(15,5))\n    axs[0].plot(range(1,len(model_history.history[acc])+1),model_history.history[acc])\n    axs[0].plot(range(1,len(model_history.history[val_acc])+1),model_history.history[val_acc])\n    axs[0].set_title('Model Accuracy')\n    axs[0].set_ylabel('Accuracy')\n    axs[0].set_xlabel('Epoch')\n    axs[0].set_xticks(np.arange(1,len(model_history.history[acc])+1),len(model_history.history[acc])/10)\n    axs[0].legend(['train', 'val'], loc='best')\n    axs[1].plot(range(1,len(model_history.history['loss'])+1),model_history.history['loss'])\n    axs[1].plot(range(1,len(model_history.history['val_loss'])+1),model_history.history['val_loss'])\n    axs[1].set_title('Model Loss')\n    axs[1].set_ylabel('Loss')\n    axs[1].set_xlabel('Epoch')\n    axs[1].set_xticks(np.arange(1,len(model_history.history['loss'])+1),len(model_history.history['loss'])/10)\n    axs[1].legend(['train', 'val'], loc='best')\n    plt.show()\n    \nplot_model_history(history)\nY_val = validate_df['category']\ny_pred =  model.predict_generator(validation_generator)\nthreshold = 0.5\ny_final = np.where(y_pred > threshold, 1,0)\ny_final.size\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix\n# Predict the values from the validation dataset\n\n# compute the confusion matrix\nconfusion_mtx = confusion_matrix(Y_val, y_final) \n# plot the confusion matrix\nf,ax = plt.subplots(figsize=(8, 8))\nsns.heatmap(confusion_mtx, annot=True, linewidths=0.01,cmap=\"Greens\",linecolor=\"gray\", fmt= '.1f',ax=ax)\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.title(\"Confusion Matrix\")\nplt.show()\nfrom sklearn.metrics import classification_report\n\n# Generate a classification report\nreport = classification_report(Y_val, y_final, target_names=['0','1'])\n\nprint(report)", "bm25Code": []}, {"originalMarkdown": "## Model training", "originalCode": "# Reference: https://www.kaggle.com/code/ryches/turbo-charging-andrew-s-pytorch\n\nrunner.train(\n    model=model,\n    criterion=criterion,\n    optimizer=optimizer,\n    scheduler=scheduler,\n    loaders=loaders,\n    callbacks=[DiceCallback(), EarlyStoppingCallback(patience=5, min_delta=0.001)],\n    logdir=logdir,\n    num_epochs=num_epochs,\n    verbose=True\n)\nutils.plot_metrics(\n    logdir=logdir, \n    # specify which metrics we want to plot\n    metrics=[\"loss\", \"dice\", 'lr', '_base/lr']\n)\nencoded_pixels = []\nloaders = {\"infer\": valid_loader}\nrunner.infer(\n    model=model,\n    loaders=loaders,\n    callbacks=[\n        CheckpointCallback(\n            resume=f\"{logdir}/checkpoints/best.pth\"),\n        InferCallback()\n    ],\n)\nvalid_masks = []\nprobabilities = np.zeros((2220, 350, 525), dtype = np.float32)\nfor i, (batch, output) in enumerate(tqdm.tqdm(zip(\n        valid_dataset, runner.callbacks[0].predictions[\"logits\"]))):\n    image, mask = batch\n    for m in mask:\n        if m.shape != (350, 525):\n            m = cv2.resize(m, dsize=(525, 350), interpolation=cv2.INTER_LINEAR)\n        valid_masks.append(m)\n\n    for j, probability in enumerate(output):\n        if probability.shape != (350, 525):\n            probability = cv2.resize(probability, dsize=(525, 350), interpolation=cv2.INTER_LINEAR)\n        probabilities[i * 4 + j, :, :] = probability\nimport gc\ntorch.cuda.empty_cache()\ngc.collect()", "bm25Code": "# Reference: https://www.kaggle.com/code/gogo827jz/deeper-efficientnet-b7\n\n# Need this line so Google will recite some incantations\n# for Turing to magically load the model onto the TPU\nwith strategy.scope():\n    enet = efn.EfficientNetB7(\n        input_shape=(512, 512, 3),\n        weights='imagenet',\n        include_top=False\n    )\n    x = enet.output\n    x1 = tf.keras.layers.GlobalMaxPooling2D()(x)\n    x2 = tf.keras.layers.GlobalAveragePooling2D()(x)\n    x = tf.keras.layers.Concatenate()([x1, x2])\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Dropout(0.5)(x)\n    x = tf.keras.layers.Dense(512, activation='elu')(x)\n    y = tf.keras.layers.Dense(len(CLASSES), activation='softmax')(x)\n    \n    model = tf.keras.Model(inputs=enet.input, outputs=y)\n        \nmodel.compile(\n    optimizer=tf.keras.optimizers.Adam(lr=0.0001),\n    loss = 'sparse_categorical_crossentropy',\n    metrics=['sparse_categorical_accuracy']\n)\nmodel.summary()\ntf.keras.utils.plot_model(\n    model,\n    to_file='DeepEfficientNetB7.png',\n    show_shapes=False,\n    show_layer_names=True,\n    rankdir='TB',\n    expand_nested=False,\n    dpi=96\n)"}, {"originalMarkdown": "", "originalCode": "%matplotlib inline\nimport pandas as pd\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = 'all'\npd.set_option('display.max_columns', 99)\npd.set_option('display.max_rows', 99)\nimport os\nimport numpy as np\nfrom tqdm import tqdm\nimport datetime as dt\nimport matplotlib.pyplot as plt\nplt.rcParams['figure.figsize'] = [16, 10]\nplt.rcParams['font.size'] = 14\nimport seaborn as sns\nsns.set_palette(sns.color_palette('tab20', 20))\n\nimport plotly.express as px\nimport plotly.graph_objects as go\ndef to_log(x):\n    return np.log(x + 1)\n\n\ndef to_exp(x):\n    return np.exp(x) - 1\n\nstart = dt.datetime.now()\n\nlb_periods = {\n    1: ('2020-03-26', '2020-04-23'),\n    2: ('2020-04-02', '2020-04-30'),\n    3: ('2020-04-09', '2020-05-07'),\n    4: ('2020-04-16', '2020-05-14')\n}\ndef get_competition_data(week):\n    train = pd.read_csv(f'../input/covid19-global-forecasting-week-{week}/train.csv')\n    test = pd.read_csv(f'../input/covid19-global-forecasting-week-{week}/test.csv')\n    \n    if 'Province/State' in test.columns:\n        test = test.rename(columns={'Province/State': 'Province_State', 'Country/Region': 'Country_Region'})\n        train = train.rename(columns={'Province/State': 'Province_State', 'Country/Region': 'Country_Region'})\n    train['Location'] = train['Country_Region'] + '-' + train['Province_State'].fillna('')\n    test['Location'] = test['Country_Region'] + '-' + test['Province_State'].fillna('')\n    train = train[['Date', 'Location', 'ConfirmedCases', 'Fatalities']]\n    return train, test\ndef get_actual(skip_nova_scotia=False):\n    #     actual, _ = get_competition_data(week=4)\n    actual = pd.read_csv(f'../input/covid-19-forecasting-ongoing-data-updates/train.csv')\n    actual.ConfirmedCases = actual.ConfirmedCases.clip(0, None)\n    actual.Fatalities = actual.Fatalities.clip(0, None)\n    actual['Location'] = actual['Country_Region'] + '-' + actual['Province_State'].fillna('')\n    actual = actual[['Date', 'Location', 'ConfirmedCases', 'Fatalities']]\n    if skip_nova_scotia:\n        actual = actual[actual.Location != 'Canada-Nova Scotia']\n    return actual\nactual = get_actual(skip_nova_scotia=False)\nactual[actual.Location=='Canada-Nova Scotia'].tail(10)\nactual[actual.Location=='US-New York'].tail(40)\ndef get_submissions(week):\n    submission_path = f'../input/covid19-global-forecasting-submissions/week_{week}'\n    submission_files = os.listdir(submission_path)\n    submissions_list = []\n\n    for f in tqdm(submission_files):\n        submission = pd.read_csv(os.path.join(submission_path, f))\n        submission.insert(0, 'SubmissionId', int(f[:-4]))\n        submissions_list.append(submission)\n\n    submissions = pd.concat(submissions_list, ignore_index=True, sort=False)\n    \n    submissions = submissions[['SubmissionId', 'ForecastId', 'ConfirmedCases', 'Fatalities']]\n    \n    submissions.ConfirmedCases = submissions.ConfirmedCases.clip(0, None)\n    submissions.Fatalities = submissions.Fatalities.clip(0, None)\n    \n    _, test = get_competition_data(week)\n    submissions = submissions.merge(test, on='ForecastId', how='left')\n    \n    submissions = submissions.loc[submissions.Date >= lb_periods[week][0]]\n    \n    actual = get_actual()\n    submissions = submissions.merge(actual, how='left', on=['Date', 'Location'], suffixes=['', 'Actual'])\n    \n    return submissions\n# actual, _ = get_competition_data(week=4)\nactual = get_actual()\nprint(f'Actual last day: {actual.Date.max()}')\nactual.describe()\nweek = 1\nsubmissions = get_submissions(week)\n\n\nsubmissions.head()\nsubmissions.shape\ndef add_errors(submissions):\n    submissions.loc[:,'FatalitiesSLE'] = (to_log(submissions.Fatalities) - to_log(submissions.FatalitiesActual)) ** 2\n    submissions.loc[:,'ConfirmedCasesSLE'] = (to_log(submissions.ConfirmedCases) - to_log(submissions.ConfirmedCasesActual)) ** 2\n    return submissions\n\ndef calculate_lb(submissions):\n    lb = submissions[['SubmissionId', 'FatalitiesSLE', 'ConfirmedCasesSLE']].groupby('SubmissionId').mean().reset_index()\n    lb.loc[:, 'FatalatiesRMSLE'] = np.sqrt(lb['FatalitiesSLE'])\n    lb.loc[:, 'ConfirmedCasesRMSLE'] = np.sqrt(lb['ConfirmedCasesSLE'])\n    lb.loc[:, 'RMSLE'] = (lb['FatalatiesRMSLE'] + lb['ConfirmedCasesRMSLE']) / 2.0\n    lb = lb.sort_values(by='RMSLE')\n    lb['Rank'] = np.arange(len(lb))\n    return lb\nsubmissions = add_errors(submissions)\n\nlb = calculate_lb(submissions)\nsubmissions = submissions.merge(lb[['SubmissionId', 'RMSLE', 'Rank']], on='SubmissionId')\nsubmissions.head()\nlb.head()\ndef get_ensemble(submissions, k=10):\n    submissions['LogCC'] = to_log(submissions.ConfirmedCases)\n    submissions['LogF'] = to_log(submissions.Fatalities)\n\n    ensemble = submissions[submissions.Rank < k].groupby(['Date', 'Location'])[['LogCC', 'LogF']].mean()\n    ensemble['ConfirmedCases'] = to_exp(ensemble.LogCC)\n    ensemble['Fatalities'] = to_exp(ensemble.LogF)\n    ensemble = ensemble.reset_index()\n\n    ensemble = ensemble.merge(actual, how='left', on=['Date', 'Location'], suffixes=['', 'Actual'])\n    ensemble = add_errors(ensemble)\n    return ensemble\ndef calculate_lb_and_ensemble(week, top_ranks=10):\n    submissions = get_submissions(week)\n    submissions = add_errors(submissions)\n\n    lb = calculate_lb(submissions)\n    submissions = submissions.merge(lb[['SubmissionId', 'RMSLE', 'Rank']], on='SubmissionId')\n\n    ens = get_ensemble(submissions, k=10)\n    np.sqrt((ens.FatalitiesSLE.mean() + ens.ConfirmedCasesSLE.mean() ) / 2.0)\n\n    daily_error = submissions[submissions.Rank < top_ranks].groupby(['SubmissionId', 'Date']).mean().reset_index()\n    daily_error['Daily RMSLE'] = np.sqrt(0.5 * daily_error.FatalitiesSLE + 0.5 * daily_error.ConfirmedCasesSLE)\n    daily_error['LB Score'] = '#' + daily_error.Rank.astype(str) + ' - ' + daily_error.RMSLE.round(5).astype(str) + ' - ' + daily_error.SubmissionId.astype(str)\n    daily_error = daily_error.sort_values(by=['Rank', 'Date'])\n    fig = px.line(daily_error, x='Date', y='Daily RMSLE', color='LB Score')\n    _ = fig.update_layout(\n        title_text=f'COVID-19 Daily Prediction Error (Week {week})'\n    )\n\n    return submissions, lb, ens, daily_error, fig\n    ", "bm25Code": []}]}, "expert": {"correct_pairs_summarize": {"ml": 658, "es": 9187, "es-processed": 7191}, "incorrect_pairs_items": [{"originalMarkdown": "", "originalCode": "!pip install meteocalc\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport lightgbm as lgb\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import KFold, StratifiedKFold, GroupKFold\nfrom tqdm import tqdm_notebook as tqdm\nimport datetime\nfrom meteocalc import feels_like, Temp\nfrom sklearn import metrics\nimport gc\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))", "bm25Code": []}, {"originalMarkdown": "", "originalCode": "# Operating system\nimport sys\nimport os\nfrom pathlib import Path\nos.environ[\"OMP_NUM_THREADS\"] = \"1\"\n\n# math\nimport numpy as np\nimport pandas as pd\nfrom numpy import arange\nimport math\nfrom numpy import linalg as LA\nfrom scipy import stats\n\n# plots\nimport matplotlib.pyplot as plt\nfrom matplotlib.axes import Axes\nfrom matplotlib import animation, rc\nimport matplotlib.colors as colors\nimport plotly.graph_objects as go\nfrom tqdm import tqdm, tqdm_notebook\ntqdm.pandas()\nfrom datetime import timezone, datetime, timedelta\n\n# ML\nimport sklearn\nimport h5py\nimport sklearn.metrics\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.cluster import DBSCAN\nfrom sklearn import metrics\nfrom sklearn.datasets.samples_generator import make_blobs\nfrom sklearn.preprocessing import StandardScaler\n\n# Lyft SDK\n!pip install lyft-dataset-sdk\nfrom lyft_dataset_sdk.utils.map_mask import MapMask\nfrom lyft_dataset_sdk.lyftdataset import LyftDataset\nfrom lyft_dataset_sdk.utils.geometry_utils import view_points, box_in_image, BoxVisibility\nfrom lyft_dataset_sdk.utils.geometry_utils import view_points, transform_matrix\nfrom lyft_dataset_sdk.utils.data_classes import LidarPointCloud, Box, Quaternion\n\nDATA_PATH = './'\nos.system('rm -f data && ln -s /kaggle/input/3d-object-detection-for-autonomous-vehicles/train_data data')\nos.system('rm  -f images && ln -s /kaggle/input/3d-object-detection-for-autonomous-vehicles/train_images images')\nos.system('rm  -f maps && ln -s /kaggle/input/3d-object-detection-for-autonomous-vehicles/train_maps maps')\nos.system('rm  -f lidar && ln -s /kaggle/input/3d-object-detection-for-autonomous-vehicles/train_lidar lidar')\n\nDEBUG = True\ndef log(message):\n    if(DEBUG == True):\n        time_string = datetime.now().strftime('%Y-%m-%d-%H-%M-%S.%f')\n        print(time_string + ' : ', message )\nLYFT = LyftDataset(data_path=DATA_PATH, json_path=DATA_PATH + 'data', verbose=True)\n# Get object size\nwlhs = np.array([ann['size'] for ann in  LYFT.sample_annotation])", "bm25Code": []}, {"originalMarkdown": "# Dataset", "originalCode": "# Reference: https://www.kaggle.com/code/demesgal/train-inference-gpu-baseline-tta\n\nDATA_ROOT_PATH = '../input/alaska2-image-steganalysis'\n\ndef onehot(size, target):\n    vec = torch.zeros(size, dtype=torch.float32)\n    vec[target] = 1.\n    return vec\n\nclass DatasetRetriever(Dataset):\n\n    def __init__(self, kinds, image_names, labels, transforms=None):\n        super().__init__()\n        self.kinds = kinds\n        self.image_names = image_names\n        self.labels = labels\n        self.transforms = transforms\n\n    def __getitem__(self, index: int):\n        kind, image_name, label = self.kinds[index], self.image_names[index], self.labels[index]\n        image = cv2.imread(f'{DATA_ROOT_PATH}/{kind}/{image_name}', cv2.IMREAD_COLOR)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n        image /= 255.0\n        if self.transforms:\n            sample = {'image': image}\n            sample = self.transforms(**sample)\n            image = sample['image']\n            \n        target = onehot(4, label)\n        return image, target\n\n    def __len__(self) -> int:\n        return self.image_names.shape[0]\n\n    def get_labels(self):\n        return list(self.labels)\nfold_number = 0\n\ntrain_dataset = DatasetRetriever(\n    kinds=dataset[dataset['fold'] != fold_number].kind.values,\n    image_names=dataset[dataset['fold'] != fold_number].image_name.values,\n    labels=dataset[dataset['fold'] != fold_number].label.values,\n    transforms=get_train_transforms(),\n)\n\nvalidation_dataset = DatasetRetriever(\n    kinds=dataset[dataset['fold'] == fold_number].kind.values,\n    image_names=dataset[dataset['fold'] == fold_number].image_name.values,\n    labels=dataset[dataset['fold'] == fold_number].label.values,\n    transforms=get_valid_transforms(),\n)\nimage, target = train_dataset[0]\nnumpy_image = image.permute(1,2,0).cpu().numpy()\n\nfig, ax = plt.subplots(1, 1, figsize=(16, 8))\n    \nax.set_axis_off()\nax.imshow(numpy_image);", "bm25Code": "import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nimport matplotlib.gridspec as gridspec\n%matplotlib inline\n\nfrom trackml.dataset import load_event\nfrom trackml.randomize import shuffle_hits\nfrom trackml.score import score_event"}, {"originalMarkdown": "", "originalCode": "import copy\nimport gc\nimport glob\nimport os\nimport time\n\nimport cv2\nimport IPython\nimport IPython.display\nimport librosa\nimport librosa.display\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport tensorflow.keras as keras\nfrom joblib import Parallel, delayed\nfrom sklearn.model_selection import KFold, StratifiedKFold, train_test_split\nfrom tensorflow.keras.applications.resnet50 import ResNet50\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.models import Model\nfrom tqdm import tqdm\n\n%matplotlib inline\npd.options.display.max_columns = 128\npd.options.display.max_rows = 128\nplt.rcParams['figure.figsize'] = (15, 8)", "bm25Code": []}, {"originalMarkdown": "", "originalCode": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import accuracy_score,confusion_matrix,precision_score,f1_score\nfrom sklearn import metrics\nfrom sklearn.calibration import CalibratedClassifierCV\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nfrom sklearn.model_selection import train_test_split\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\ntrain = pd.read_csv('/kaggle/input/cat-in-the-dat-ii/train.csv')\ntest = pd.read_csv('/kaggle/input/cat-in-the-dat-ii/test.csv')\ntrain.describe()\ntrain.head()\nlist(train['ord_0'].unique())\nnp.ceil(train['ord_0'].mean())\ntrain.info()\ndef summary(df):\n    summary = pd.DataFrame(df.dtypes, columns=['dtypes'])\n    summary = summary.reset_index()\n    summary['Name'] = summary['index']\n    summary = summary[['Name', 'dtypes']]\n    summary['Missing'] = df.isnull().sum().values    \n    summary['Uniques'] = df.nunique().values\n    summary['First Value'] = df.loc[0].values\n    summary['Second Value'] = df.loc[1].values\n    summary['Third Value'] = df.loc[2].values\n    return summary\n\n\nsummary(train)\n\n\nset(train['bin_0'].isna())\ncorr = train.corr()\nplt.figure(figsize=(10,10))\nax = sns.heatmap(corr,vmin=-1, vmax=1, center=0,cmap='coolwarm',square=True)", "bm25Code": []}, {"originalMarkdown": "", "originalCode": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nfrom scipy.special import inv_boxcox\nfrom typing import Tuple\nimport lightgbm as lgb\nfrom datetime import timedelta\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\nFIGURE_SIZE = (20, 10)\nplt.rcParams['axes.grid'] = True\n\n%matplotlib inline", "bm25Code": []}, {"originalMarkdown": "", "originalCode": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport os \nfrom sklearn.preprocessing import MinMaxScaler\nSEQ_LEN = 60  # how long of a preceeding sequence to collect for RNN\nFUTURE_PERIOD_PREDICT = 30  # how far into the future are we trying to predict?\nSTOCK_TO_PREDICT = 'GE'\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom keras.layers import Dropout\nfrom keras.layers import BatchNormalization\nfrom keras.layers import Flatten\nfrom keras.layers import GRU\nfrom keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\nfrom keras.optimizers import Adam \nfrom keras.engine.topology import Layer\nfrom keras import initializers, regularizers, constraints\nfrom keras.engine.input_layer import Input\nfrom keras import backend as K\nfrom keras.models import Model", "bm25Code": []}, {"originalMarkdown": "", "originalCode": "from datetime import datetime \nstart_real = datetime.now()\n#Importing libraries\nimport pandas as pd\nimport numpy as np\nimport scipy as sci\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport multiprocessing\n%matplotlib inline\ntrain = pd.read_csv(\"../input/train.tsv\", sep='\\t')\ntest = pd.read_csv(\"../input/test.tsv\", sep='\\t')\n#Getting rid of outliers\ntrain['bigger_than_200'] = train['price'].map(lambda x: 1 if x >200 else 0)\ntrain = train[train['bigger_than_200'] ==0]\ndel train['bigger_than_200']\nprint(train.shape)\nprint(test.shape)\n#Checking any missing values,\nimport missingno as msno\nmsno.bar(train,sort=True,figsize=(10,5))\nmsno.bar(test,sort=True,figsize=(10,5))\n#Getting the length of item description\ntrain['length'] = train['item_description'].map(lambda x: len(str(x)))\ntest['length'] = test['item_description'].map(lambda x: len(str(x)))\n\n#Merging data\ndata = pd.concat([train,test])\n#Defining a variable\ndata['train_or_not'] = data['train_id'].map(lambda x: 1 if x.is_integer() else 0)\n#lowering letters\ndata['brand_name'] = data['brand_name'].map(lambda x: str(x).lower())\ndata['category_name'] = data['category_name'].map(lambda x: str(x).lower())\ndata['item_description'] = data['item_description'].map(lambda x: str(x).lower())\ndata['name'] = data['name'].map(lambda x: str(x).lower())\ndata['no_of_words'] = data['item_description'].map(lambda x: len(str(x).split()))\n#Nan values in brand\n%%time\ndata['brand_nan'] = data['brand_name'].map(lambda x: 1 if x ==\"nan\" else 0)\n##Brand names\n#Number of unique brand names\nprint(len(set(data['brand_name'])))\nprint('brand_name in train',len(set(train['brand_name'])))\nprint('brand_name in test',len(set(test['brand_name'])))\ntrain_cat_names= list(set(train['brand_name']))\ntest_cat_names= list(set(test['brand_name']))\n\nin_test_not_in_train = [x for x in test_cat_names if x not in train_cat_names]\nprint(len(in_test_not_in_train))\n\nin_train_not_in_test = [x for x in train_cat_names if x not in test_cat_names]\nprint(len(in_train_not_in_test))\n#category\ndata['categories'] = data['category_name'].map(lambda x: list(str(x).split('/')))\n#no descriptions\ndata['no_description'] = data['item_description'].map(lambda x: 1 if str(x) =='no description yet' else 0)\nprint(len(data[data['no_description']==1]))\nprint('brand_name = nan & no description',len(data[(data['brand_name']=='nan') & (data['no_description'] ==1)]))\n#No brand name and no desc\nno_desc_no_brand = data[(data['brand_name']=='nan') & (data['no_description'] ==1)]\nno_desc_no_brand['test'] = no_desc_no_brand['test_id'].map(lambda x: 1 if x.is_integer() else 0)\nno_desc_no_brand = no_desc_no_brand[no_desc_no_brand['test'] ==0]\nplt.style.use('fivethirtyeight')\nplt.subplots(figsize=(15,5))\nno_desc_no_brand['price'].hist(bins=150,edgecolor='black',grid=False)\nplt.xticks(list(range(0,100,5)))\nplt.title('Price vs no brand&no_description')\nplt.show() \n#No of rows whose price is bigger than 100\nprint(\"No of rows whose price is bigger than 200 in no_brand&no_description\",len(no_desc_no_brand[no_desc_no_brand['price'] >200]))\n\nno_desc_no_brand['price'].describe()\ndel no_desc_no_brand\nfrom ggplot import *\np = ggplot(aes(x='price'), data=train[train['price']<200]) + geom_histogram(binwidth=10)+ theme_bw() + ggtitle('Histogram of price in train data')\nprint(p)\ndata['price'].describe().apply(lambda x: format(x, 'f'))\n#Length of categories\ndata['len_categories'] = data['categories'].map(lambda x: len(x))\n#Value_counts for item_condition_id\ntemp1=data['item_condition_id'].value_counts()[:5].to_frame()\nsns.barplot(temp1.index,temp1['item_condition_id'],palette='inferno')\nplt.title('Item condition id')\nplt.xlabel('')\nfig=plt.gcf()\nfig.set_size_inches(10,10)\nplt.show()\n#Making binary 'item_condition_id'\nic_list = list(set(data['item_condition_id']))\n\nfor i in ic_list:\n    data['item_condition_id'+str(i)] = data['item_condition_id'].map(lambda x: 1 if x==i else 0)\n\ndel data['item_condition_id']\n#Correlation between no_of_words and price\ncorr = data[['no_of_words','price','shipping','len_categories','length']].corr()\n\n# Set up the matplot figure\nf,ax = plt.subplots(figsize=(12,9))\n\n#Draw the heatmap using seaborn\nsns.heatmap(corr, cmap='inferno', annot=True)", "bm25Code": []}, {"originalMarkdown": "# CORRELATION", "originalCode": "# Reference: https://www.kaggle.com/code/mbkinaci/eda-xgboost-ridge-knn-extratrees-regression\n\ncorr_list = ['percent_atom_al','percent_atom_ga','percent_atom_in','bandgap_energy_ev','formation_energy_ev_natom']\ncorr = data[data['train_or_not'] ==1][corr_list].corr()\nf,ax = plt.subplots(figsize=(12,9))\n\n#Draw the heatmap using seaborn\nsns.heatmap(corr, cmap='winter_r', annot=True)\ndata[data['train_or_not'] ==1]['bandgap_energy_ev'].hist(bins = 100,color = 'green')\ndata[data['train_or_not'] ==1]['formation_energy_ev_natom'].hist(bins = 100,color='red')\ndata.head()\ndef area_calculator(x):\n    a = x['one_to_two']\n    b = x['one_to_three']\n    c = x['two_to_three']\n    p = (a+b+c)/2\n    return np.sqrt(p*(p-a)*(p-b)*(p-c))\ndata['area'] = data.apply(area_calculator,axis=1)\ndata.head()\ndata[data['train_or_not'] ==1][['area','formation_energy_ev_natom','bandgap_energy_ev']].corr()\ndata[data['train_or_not'] ==1].plot(kind = \"scatter\",x='area',y ='formation_energy_ev_natom',marker='.',figsize=(10,5))\ndef area_bucket(x):\n    if (x>10) & (x<65):\n        return 1\n    elif (x>65) & (x<95):\n        return 2\n    elif (x>95) & (x<120):\n        return 3\n    else:\n        return 4\ndata['area_bucket'] = data['area'].map(lambda x: area_bucket(x))\ndata.groupby('area_bucket')['formation_energy_ev_natom'].mean()\ndata['area_bucket_3'] = data['area_bucket'].map(lambda x: 1 if x==3 else 0)\ndata['area_bucket_4'] = data['area_bucket'].map(lambda x: 1 if x==4 else 0)\ndel data['area_bucket']\nfrom pylab import rcParams\nrcParams['figure.figsize'] = 20, 10\ncolors = ['b', 'c', 'y', 'm', 'r','g','k','pink','orange','purple']\n\nc0 = plt.scatter(data[(data['train_or_not'] ==1)&(data['clusters_of_lattices'] == 0)]['bandgap_energy_ev'], data[(data['train_or_not'] ==1)&(data['clusters_of_lattices'] == 0)]['formation_energy_ev_natom'], marker='x', color=colors[0])\nc1 = plt.scatter(data[(data['train_or_not'] ==1)&(data['clusters_of_lattices'] == 1)]['bandgap_energy_ev'], data[(data['train_or_not'] ==1)&(data['clusters_of_lattices'] == 1)]['formation_energy_ev_natom'], marker='o', color=colors[1])\nc2 = plt.scatter(data[(data['train_or_not'] ==1)&(data['clusters_of_lattices'] == 2)]['bandgap_energy_ev'], data[(data['train_or_not'] ==1)&(data['clusters_of_lattices'] == 2)]['formation_energy_ev_natom'], marker='.', color=colors[2])\nc3 = plt.scatter(data[(data['train_or_not'] ==1)&(data['clusters_of_lattices'] == 3)]['bandgap_energy_ev'], data[(data['train_or_not'] ==1)&(data['clusters_of_lattices'] == 3)]['formation_energy_ev_natom'], marker='^', color=colors[3])\nc4 = plt.scatter(data[(data['train_or_not'] ==1)&(data['clusters_of_lattices'] == 4)]['bandgap_energy_ev'], data[(data['train_or_not'] ==1)&(data['clusters_of_lattices'] == 4)]['formation_energy_ev_natom'], marker='+', color=colors[4])\nc5 = plt.scatter(data[(data['train_or_not'] ==1)&(data['clusters_of_lattices'] == 5)]['bandgap_energy_ev'], data[(data['train_or_not'] ==1)&(data['clusters_of_lattices'] == 5)]['formation_energy_ev_natom'], marker='v', color=colors[5])\nc6 = plt.scatter(data[(data['train_or_not'] ==1)&(data['clusters_of_lattices'] == 6)]['bandgap_energy_ev'], data[(data['train_or_not'] ==1)&(data['clusters_of_lattices'] == 6)]['formation_energy_ev_natom'], marker='_', color=colors[6])\nc7 = plt.scatter(data[(data['train_or_not'] ==1)&(data['clusters_of_lattices'] == 7)]['bandgap_energy_ev'], data[(data['train_or_not'] ==1)&(data['clusters_of_lattices'] == 7)]['formation_energy_ev_natom'], marker='*', color=colors[7])\nc8 = plt.scatter(data[(data['train_or_not'] ==1)&(data['clusters_of_lattices'] == 8)]['bandgap_energy_ev'], data[(data['train_or_not'] ==1)&(data['clusters_of_lattices'] == 8)]['formation_energy_ev_natom'], marker='s', color=colors[8])\nc9 = plt.scatter(data[(data['train_or_not'] ==1)&(data['clusters_of_lattices'] == 9)]['bandgap_energy_ev'], data[(data['train_or_not'] ==1)&(data['clusters_of_lattices'] == 9)]['formation_energy_ev_natom'], marker='d', color=colors[9])\n\nplt\nplt.xlabel(\"bandgap_energy_ev\")\nplt.ylabel(\"formation_energy_ev_natom\")\nplt.legend((c0, c1, c2, c3, c4, c5, c6,c7,c8,c9),\n           ('cluster0','cluster1','cluster2','cluster3','cluster4','cluster5','cluster6','cluster7','cluster8','cluster9'),\n           scatterpoints=1,\n           loc='upper right',\n           ncol=2,\n           fontsize=10)\n\nplt.show()\nrcParams['figure.figsize'] = 10, 5\n\nfig, ax = plt.subplots(nrows = 2,ncols=1, sharex=True, sharey=True,)\nax[0].scatter(data[data['train_or_not'] ==1]['lattice_angle_alpha_degree'].values, data[data['train_or_not'] ==1]['formation_energy_ev_natom'].values,\n              color='blue', s=3, label='formation', alpha=1)\nax[1].scatter(data[data['train_or_not'] ==1]['lattice_angle_alpha_degree'].values, data[data['train_or_not'] ==1]['bandgap_energy_ev'].values,\n              color='green', s=3, label='bandgap', alpha=1)\nfig.suptitle('Lattice angle alpha degree vs formation and bandgap energy')\nax[0].legend(loc=0)\nax[0].set_ylabel('energy')\nax[0].set_xlabel('lattice alpha degree')\nax[1].set_ylabel('energy')\nax[1].set_xlabel('lattice alpha degree')\nax[1].legend(loc=0)\n\nplt.show()\nlen(data[(data['lattice_angle_alpha_degree']<92.5) & (data['lattice_angle_alpha_degree']>89.5)])\nrcParams['figure.figsize'] = 10, 5\n\nfig, ax = plt.subplots(nrows = 2,ncols=1, sharex=True, sharey=True,)\nax[0].scatter(data[data['train_or_not'] ==1]['lattice_angle_beta_degree'].values, data[data['train_or_not'] ==1]['formation_energy_ev_natom'].values,\n              color='blue', s=3, label='formation', alpha=1)\nax[1].scatter(data[data['train_or_not'] ==1]['lattice_angle_beta_degree'].values, data[data['train_or_not'] ==1]['bandgap_energy_ev'].values,\n              color='green', s=3, label='bandgap', alpha=1)\nfig.suptitle('Lattice angle beta degree vs formation and bandgap energy')\nax[0].legend(loc=0)\nax[0].set_ylabel('energy')\nax[0].set_xlabel('Lattice angle beta degree')\nax[1].set_ylabel('energy')\nax[1].set_xlabel('Lattice angle beta degree')\nax[1].legend(loc=0)\n\nplt.show()\nprint(data[(data['train_or_not'] ==1) & (data['lattice_angle_beta_degree'] > 102)]['formation_energy_ev_natom'].mean())\nprint(data[(data['train_or_not'] ==1) & (data['lattice_angle_beta_degree'] < 102)]['formation_energy_ev_natom'].mean())\ndata['beta_bigger_102'] = data['lattice_angle_beta_degree'].map(lambda x: 1 if x >102 else 0)\nprint(data[(data['train_or_not'] ==1) & (data['lattice_angle_beta_degree'] > 102)]['bandgap_energy_ev'].mean())\nprint(data[(data['train_or_not'] ==1) & (data['lattice_angle_beta_degree'] < 102)]['bandgap_energy_ev'].mean())\nlen(data[(data['lattice_angle_beta_degree']<90.5) & (data['lattice_angle_beta_degree']>89.5)])\nrcParams['figure.figsize'] = 8, 4\n\nfig, ax = plt.subplots(nrows = 2,ncols=1, sharex=True, sharey=True,)\nax[0].scatter(data[data['train_or_not'] ==1]['lattice_angle_gamma_degree'].values, data[data['train_or_not'] ==1]['formation_energy_ev_natom'].values,\n              color='blue', s=3, label='formation', alpha=1)\nax[1].scatter(data[data['train_or_not'] ==1]['lattice_angle_gamma_degree'].values, data[data['train_or_not'] ==1]['bandgap_energy_ev'].values,\n              color='green', s=3, label='bandgap', alpha=1)\nfig.suptitle('Lattice angle gamma degree vs formation and bandgap energy')\nax[0].legend(loc=0)\nax[0].set_ylabel('energy')\nax[0].set_xlabel('degree')\nax[1].set_ylabel('energy')\nax[1].set_xlabel('degree')\nax[1].legend(loc=0)\n\nplt.show()\ndata['atomic_density'] = data['number_of_total_atoms']/(data['lattice_vector_1_ang']*data['lattice_vector_2_ang']*data['lattice_vector_3_ang'])\nrcParams['figure.figsize'] = 8, 4\n\nfig, ax = plt.subplots(nrows = 2,ncols=1, sharex=True, sharey=True,)\nax[0].scatter(data[data['train_or_not'] ==1]['number_of_total_atoms'].values, data[data['train_or_not'] ==1]['formation_energy_ev_natom'].values,\n              color='blue', s=3, label='formation', alpha=1)\nax[1].scatter(data[data['train_or_not'] ==1]['number_of_total_atoms'].values, data[data['train_or_not'] ==1]['bandgap_energy_ev'].values,\n              color='green', s=3, label='bandgap', alpha=1)\nfig.suptitle('Lattice angle gamma degree vs formation and bandgap energy')\nax[0].legend(loc=0)\nax[0].set_ylabel('energy')\nax[0].set_xlabel('degree')\nax[1].set_ylabel('energy')\nax[1].set_xlabel('degree')\nax[1].legend(loc=0)\n\nplt.show()\ndata[(data['number_of_total_atoms']==10) & (data['formation_energy_ev_natom']>0.3)]\ndata = data.drop([1235,1983])\ndata.shape", "bm25Code": "# Reference: https://www.kaggle.com/code/juanmah/ashrae-degree-hours\n\ntimeseries_cooling = timeseries[\n    [\"building_id\", \"meter\", 'air_temperature']\n    + [cdh for cdh in weather_train.columns.to_list() if \"cooling_degree_hours\" in cdh]\n]\ntimeseries_cooling = pd.wide_to_long(\n    timeseries_cooling,\n    stubnames=\"cooling_degree_hours_\",\n    i=[\"building_id\", \"meter\", 'air_temperature'],\n    j=\"base_temperature\",\n)\ntimeseries_cooling.reset_index(inplace=True)\nfig = px.scatter(\n    timeseries_cooling,\n    x=\"air_temperature\",\n    y=\"cooling_degree_hours_\",\n    color=\"meter\",\n    animation_frame=\"base_temperature\",        \n    opacity=0.7,\n    marginal_x=\"violin\",\n    marginal_y=\"violin\",\n    hover_name=\"building_id\",\n    category_orders=category_orders,\n    color_discrete_map=color_discrete_map_meter,\n)\nfig.update_layout(yaxis=dict(range=[-1, 1]))\nfig.show()"}, {"originalMarkdown": "", "originalCode": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport glob\nglob.glob('..s/input/prostate-cancer-grade-assessment/*')\nlen(glob.glob('../input/prostate-cancer-grade-assessment/train_images/*.tiff'))\n!pip install -qq ../input/efficientnet/efficientnet-1.0.0-py3-none-any.whl\nimport os\nimport cv2\nimport time\nimport skimage.io\nimport numpy as np\nimport pandas as pd\nimport imgaug as ia\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom random import shuffle\nimport matplotlib.pyplot as plt\nimport efficientnet.keras as efn\nfrom imgaug import augmenters as iaa\n\nfrom sklearn.model_selection import train_test_split\n\nfrom keras import Model\nimport keras.backend as K\nfrom keras.layers import *\nfrom keras.optimizers import Adam, RMSprop\nfrom keras.applications.nasnet import  preprocess_input\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau", "bm25Code": []}]}}